Efficient discovery of grid services is essential for the success of grid computing. The standardization of grids based on web services has resulted in the need for scalable web service Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise,to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
MGC '05, November 28- December 2, 2005 Grenoble , France discovery mechanisms to be deployed in grids. Grid discovery services provide the ability to monitor and discover resources and services on grids. They provide the ability to query and subscribe to resource/service information. In addition, threshold traps might be required to indicate specific change in existing conditions. The state of the data needs to be maintained in a soft state so that the most recent information is always available. The information gathered needs to be provided to variety of systems for the purpose of either utilizing the grid or proving summary information. However, the fundamental problem is the need to be scalable to handle huge amounts of data from multiple sources.
The web services community has addressed the need for service discovery, before grids were anticipated, via an industry standard called UDDI. However, even though UDDI has been the de facto industry standard for web-services discovery, imposed requirements of tight-replication among registries and lack of autonomous control, among other things has severely hindered its widespread deployment and usage [7]. With the advent of grid computing the scalability issue with UDDI will become a roadblock that will prevent its deployment in grids.
This paper tackles the scalability issue and a way to find services across multiple registries in UDDI by developing a distributed web services discovery architecture. Distributing UDDI functionality can be achieved in multiple ways and perhaps using different distributed computing infrastructure/platforms (e.g.,
CORBA, DCE, etc.). In this paper we explore how Distributed Hash Table (DHT) technology can be leveraged to develop a scalable distributed web services discovery architecture. A DHT is a peer-to-peer (P2P) distributed system that forms a structured overlay allowing more efficient routing than the underlying network. This crucial design choice is motivated by two factors.
The first motivating factor is the inherent simplicity of the put/get abstraction that DHTs provide, which makes it easy to rapidly build applications on top of DHTs. We recognize that having just this abstraction may not suffice for all distributed applications, but for the objective at hand, works very well as will become clear later. Other distributed computing platforms/middleware while providing more functionality have much higher overhead and complexity. The second motivating factor stems from the fact that DHTs are relatively new tool for building distributed applications and we would like to test its potential by applying it to the problem of distributing UDDI.
In the next section, we provide a brief overview of grid information services, UDDI and its limitations, which is followed by an overview of DHTs in Section 3. Section 4 describes our proposed architecture with details on use cases. In Section 5, we Article 2 describe our current implementation, followed by our findings in Section 6. Section 7 discusses the related work in this area and Section 8 contains our concluding remarks.
Grid computing is based on standards which use web services technology. In the architecture presented in [6], the service discovery function is assigned to a specialized Grid service called Registry. The implementation of the web service version of the Monitoring and Discovery Service (WS MDS), also known as the MDS4 component of the Globus Toolkit version 4 (GT4), includes such a registry in the form of the Index service Resource and service properties are collected and indexed by this service.
Its basic function makes it similar to UDDI registry. To attain scalability, Index services from different Globus containers can register with each other in a hierarchical fashion to aggregate data.
This approach for attaining scalability works best in hierarchical Virtual Organizations (VO), and expanding a search to find sufficient number of matches involves traversing the hierarchy.
Specifically, this approach is not a good match for systems that try to exploit the convergence of grid and peer-to-peer computing [5].
Beyond grid computing, the problem of service discovery needs to be addressed more generally in the web services community.
Again, scalability is a major concern since millions of buyers looking for specific services need to find all the potential sellers of the service who can meet their needs. Although there are different ways of doing this, the web services standards committees address this requirement through a specification called UDDI (Universal Description, Discovery, and Integration).
A UDDI registry enables a business to enter three types of information in a UDDI registry - white pages, yellow pages and green pages. UDDI"s intent is to function as a registry for services just as the yellow pages is a registry for businesses. Just like in Yellow pages, companies register themselves and their services under different categories. In UDDI, White Pages are a listing of the business entities. Green pages represent the technical information that is necessary to invoke a given service. Thus, by browsing a UDDI registry, a developer should be able to locate a service and a company and find out how to invoke the service.
When UDDI was initially offered, it provided a lot of potential.
However, today we find that UDDI has not been widely deployed in the Internet. In fact, the only known uses of UDDI are what are known as private UDDI registries within an enterprise"s boundaries. The readers can refer to [7] for a recent article that discusses the shortcomings of UDDI and the properties of an ideal service registry. Improvement of the UDDI standard is continuing in full force and UDDI version 3 (V3) was recently approved as an OASIS Standard. However, UDDI today has issues that have not been addressed, such as scalability and autonomy of individual registries.
UDDI V3 provides larger support for multi-registry environments based on portability of keys By allowing keys to be re-registered in multiple registries, the ability to link registries in various topologies is effectively enabled. However, no normative description of these topologies is provided in the UDDI specification at this point. The improvements within UDDI V3 that allow support for multi-registry environments are significant and open the possibility for additional research around how multiregistry environments may be deployed. A recommended deployment scenario proposed by the UDDI V3.0.2 Specification is to use the UDDI Business Registries as root registries, and it is possible to enable this using our solution.
A Distributed Hash Table (DHT) is a peer-to-peer (P2P) distributed system that forms a structured overlay allowing more efficient routing than the underlying network. It maintains a collection of key-value pairs on the nodes participating in this graph structure. For our deployment, a key is the hash of a keyword from a service name or description. There will be multiple values for this key, one for each service containing the keyword. Just like any other hash table data structure, it provides a simple interface consisting of put() and get() operations. This has to be done with robustness because of the transient nature of nodes in P2P systems. The value stored in the DHT can be any object or a copy or reference to it. The DHT keys are obtained from a large identifier space. A hash function, such as MD5 or SHA-1, is applied to an object name to obtain its DHT key. Nodes in a DHT are also mapped into the same identifier space by applying the hash function to their identifier, such as IP address and port number, or public key. The identifier space is assigned to the nodes in a distributed and deterministic fashion, so that routing and lookup can be performed efficiently. The nodes of a DHT maintain links to some of the other nodes in the DHT. The pattern of these links is known as the DHT"s geometry. For example, in the Bamboo DHT [11], and in the Pastry DHT [8] on which Bamboo is based, nodes maintain links to neighboring nodes and to other distant nodes found in a routing table. The routing table entry at row i and column j, denoted Ri[j], is another node whose identifier matches its own in first i digits, and whose (i + 1)st digit is j. The routing table allows efficient overlay routing. Bamboo, like all DHTs, specifies algorithms to be followed when a node joins the overlay network, or when a node fails or leaves the network The geometry must be maintained even when this rate is high. To attain consistent routing or lookup, a DHT key must be routed to the node with the numerically closest identifier. For details of how the routing tables are constructed and maintained, the reader is referred to [8, 11].
BASED UDDI REGISTRY HIERARCHIES As mentioned earlier, we propose to build a distributed UDDI system on top of a DHT infrastructure. This choice is primarily motivated by the simplicity of the put/get abstraction that DHTs provide, which is powerful enough for the task at hand, especially since we plan to validate our approach with an implementation running on PlanetLab [9]. A secondary motivation is to understand deployment issues with DHT based systems. Several applications have been built as overlays using DHTs, such as distributed file storage, databases, publish-subscribe systems and content distribution networks. In our case, we are building a DHT based overlay network of UDDI registries, where the DHT acts as a rendezvous network that connects multiple registries. In the grid computing scenario, an overlay network of multiple UDDI registries seems to an interesting alternative to the UDDI public Article 2 registries currently maintained by Microsoft, IBM, SAP and NTT.
In addition, our aim is to not change any of the UDDI interfaces for clients as well as publishers.
Figure 1 highlights the proposed architecture for the DHT based UDDI Registry framework. UDDI nodes are replicated in a UDDI registry as per the current UDDI standard. However, each local registry has a local proxy registry that mediates between the local UDDI registry and the DHT Service. The DHT service is the glue that connects the Proxy Registries together and facilitates searching across registries.
Figure 1: DUDE Architecture Service information can be dispersed to several UDDI registries to promote scalability. The proxy registry publishes, performs queries and deletes information from the dispersed UDDI registries. However, the scope of the queries is limited to relevant registries. The DHT provides information about the relevant registries. The core idea in the architecture is to populate DHT nodes with the necessary information from the proxies which enables easy and ubiquitous searching when queries are made.
When a new service is added to a registry, all potential search terms are hashed by the proxy and used as DHT keys to publish the service in the DHT. The value stored for this service uniquely identifies the service, and includes the URL of a registry and the unique UDDI key of the service in that registry. Similarly when queries arrive, they are parsed and a set of search terms are identified. These search terms are hashed and the values stored with those hash values are retrieved from the DHT. Note that a proxy does not need to know all DHT nodes; it needs to know just one DHT node (this is done as part of the bootstrapping process) and as described in Section 2.3, this DHT node can route the query as necessary to the other nodes on the DHT overlay. We describe three usage scenarios later that deal with adding a new local registry, inserting a new service, and querying for a service.
Furthermore, the DHT optimizes the UDDI query mechanism.
This process becomes a lookup using a UDDI unique key rather than a query using a set of search parameters. This key and the URL of the registry are obtained by searching initially in the DHT. The DHT query can return multiple values for matching services, and in each of the matching registries, the proxy performs lookup operations.
The service name is used as a hash for inserting the service information. The service information contains the query URL and unique UDDI key for the registry containing the service. There could be multiple registries associated with a given service. The service information conforms to the following schema. <xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema" elementFormDefault="qualified" attributeFormDefault="unqualified"> <xs:element name="registries"> <xs:annotation> <xs:documentation>Service Information</xs:documentation> </xs:annotation> <xs:complexType> <xs:sequence> <xs:element name="registry" maxOccurs="unbounded"> <xs:complexType> <xs:sequence> <xs:element name="name"/> <xs:element name="key" maxOccurs="unbounded"/> </xs:sequence> … </xs:schema> There can be multiple proxy UDDI registries in this architecture.
The advantage of this is to introduce distributed interactions between the UDDI clients and registries. Organization can also decide what information is available from the local registries by implementing policies at the proxy registry.
In this section, we demonstrate what the sequence of operations should be for three crucial scenarios - adding a new local registry, inserting a new service and querying a service. Other operations like deleting a registry, deleting a service, etc. are similar and for the sake of brevity are omitted here.
Figure 2: Sequence Diagram- Add New Local Registry Add a New Local UDDI Registry Figure 2 contains a sequence diagram illustrating how a new UDDI registry is added to the network of UDDI registries. The new registry registers itself with its proxy registry. The proxy registry in turn queries the new registry for all services that it has UDDI Local Registry UDDI Local Registry UDDI Local Registry Proxy Registry DHT Based Distribution Proxy Registry Proxy Registry Article 2 stored in its databases and in turn registers each of those entries with the DHT.
Figure 3: Sequence Diagram - Add New Service Add a New Service The use case diagram depicted in Error! Reference source not found. highlights how a client publishes a new service to the UDDI registry. In order to interact with the registry a client has to know how to contact its local proxy registry. It then publishes a service with the proxy registry which in turn publishes the service with the local UDDI registry and receives the UDDI key of the registry entry. Then new key-value pairs are published in the DHT, where each key is obtained by hashing a searchable keyword of the service and the value consists of the query URL of the registry and the UDDI key.
Figure 4: Sequence Diagram - Query for a Service Query a Service Figure 4 shows how a client queries the UDDI registry for a service. Once again, the client needs to know how to contact its local proxy registry and invokes the query service request. The proxy registry in turn contacts one of the DHT nodes to determine DHT queries using the search terms.
As explained earlier in the context of Figure 1, multiple values might be retrieved from the DHT. Each value includes the query URL of a registry, and the unique UDDI key of a matching service in that registry. The proxy then contacts the matching registries and waits for the response of lookup operations using the corresponding UDDI keys. Upon receiving the responses, the proxy registry collates all responses and returns the aggregated set of services to the client.
We will now illustrate these operations using an example.
Consider a client contacting its local proxy to publish a service called Computer Accessories. The proxy follows the steps in Figure 3 to add the service to UDDI 1 registry, and also publishes two entries in the DHT. The keys of these entries are obtained by hashing the words computer and accessories respectively.
Both entries have the same value consisting of the query URL of this registry and the unique UDDI key returned by the registry for this service. Next we consider another client publishing a service called Computer Repair through its proxy to UDDI 2 registry. A similar process results in 2 more entries being added to the DHT.
Recall that our DHT deployment can have multiple entries with the same key. If we follow the steps in Figure 4 for a client sending a query to its proxy using the word computer, we see that the DHT is queried with the hash of the word computer as key. This retrieves the query URL and respective UDDI keys of both services mentioned before in this example. The proxy can then do a simple lookup operation at both UDDI 1 and 2 registries. It is clear that as the number of UDDI registries and clients increases, this process of lookup at only relevant UDDI registries is more scalable that doing a full search using the word computer at all UDDI registries.
In this section, we describe our implementation which is currently deployed on PlanetLab [9]. PlanetLab is an open, globally distributed platform for developing, deploying, and accessing network services. It currently has 527 machines, hosted by 249 sites, spanning over 25 countries. PlanetLab machines are hosted by research/academic institutions as well as industrial companies.
France Telecom and HP are two of the major industry supporters for PlanetLab. Every PlanetLab host machine is connected to the Internet and runs a common software package including a Linux based operating system that supports server virtualization. Thus the users can develop and experiment with new services under real-world conditions. The advantage of using PlanetLab is that we can test the DUDE architecture under real-world conditions with a large scale geographically dispersed node base.
Due to the availability of jUDDI, an open source UDDI V2 registry (http://www.juddi.org) and a lack of existing readily available UDDI V3 registry, a decision to use UDDI V2 was made. The standardization of UDDI V3 is recent and we intend to extend this work to support UDDI V3 and subsequent versions in the future. The proxy registry is implemented by modifying the jUDDI source to enable publishing, querying and deleting service information from a DHT. Furthermore, it also allows querying multiple registries and collating the response using UDDI4j [13].
For the DHT implementation, we use the Bamboo DHT code [11]. The Bamboo DHT allows multiple proxy registries to publish and delete service information from their respective UDDI registries, as well as to query for services from all the registries.
The proxy uses the service name as input to the DHT"s hash Article 2 function to get the DHT key. The value that is stored in the DHT using this key is the URI of the registry along with the UDDI key of the service. This ensures that when the proxy registry queries for services with a certain name, it gets back the URI and UDDI keys for matching entries. Using these returned results, the proxy can do fast lookup operations at the respective UDDI registries.
The UDDI keys make it unnecessary to repeat the search at the UDDI registries with the service name.
We have so far described the process of exact match on service name. However there are additional types of search that must be supported. Firstly, the search requested could be case-insensitive.
To support that, the proxy registry has to publish the same service once using the name exactly as entered in the UDDI registry, and once with the name converted to all lower-case letters. To do a case-insensitive search, the proxy registry simply has to convert the query string into lower-case letters. Secondly, the user could query based on the prefix of a service name. Indeed, this is the default behavior of search in UDDI. In other words, a wildcard is implicit at the end of the service name being searched. To support this efficiently in the DHT, our proxy registries have to take prefixes of the service name of varying length and publish the URI and UDDI key multiple times, once using each prefix. For example, the prefix sizes chosen in one deployment might be 5, 10, 15 and 20 characters. If a search for the first 12 characters of a service name is submitted, the proxy registry will query the DHT with the first 10 characters of the search string, and then refine the search result to ensure that the match extends to the 12th character.
If the search string has less than 5 characters, and the search is for a prefix rather than an exact match, the DHT cannot be of any help, unless every service is published in the DHT with prefix of length 0. Using this null prefix will send a copy of every advertised service to the DHT node to which the hash of the null prefix maps. Since this can lead to load-imbalance, a better solution might be to use the DHT only to get a list of all UDDI registries, and send the search to all of them in the locations to be searched. Thirdly, the service name being searched can be a regular expression, such as one with embedded wildcard characters. For example, a search for Garden%s should match both Garden Supplies and Gardening Tools. This will be treated similarly to the previous case as the DHT has to be queried with the longest available prefix. The results returned have to be refined to ensure that the regular expression matches.
Figure 5 shows the network diagram for our implementation.
There are two proxy UDDI and juddi registry pairs. Consider a client which contacts the UDDI proxy on grouse.hpl.hp.com. The proxy does a lookup of the DHT using the query string or a prefix.
This involves contacting one of the DHT nodes, such as pli1-br3.hpl.hp.com, which serves as the gateway to the DHT for grouse.hpl.hp.com, based on the latter"s configuration file. The DHT node may then route the query to one of the other DHT nodes which is responsible for the DHT key that the query string maps to. The results of the DHT lookup return to pli1-br3.hpl.hp.com, which forwards them to grouse.hpl.hp.com. The results may include a few services from each of the juddi registries. So the proxy registry performs the lookup operations at both planetlab1 and planetlab2.rdfrancetelecom.com for their respective entries listed in the search results. The responses to these lookups are collated by the proxy registry and returned to the client.
Figure 5 Network Diagram
A framework for QoS-based service discovery in grids has been proposed in [18]. UDDIe, an extended UDDI registry for publishing and discovering services based on QoS parameters, is proposed in [19]. Our work is complementary since we focus on how to federate the UDDI registries and address the scalability issue with UDDI. The DUDE proxy can publish the service properties supported by UDDIe in the DHT and support range queries using techniques proposed for such queries on DHTs.
Then we can deliver the scalability benefits of our current solution to both UDDI and UDDIe registries. Discovering services meeting QoS and price requirements has been studied in the context of a grid economy, so that grid schedulers can use various market models such as commodity markets and auctions. The Grid Market Directory [20] was proposed for this purpose.
In [12], the authors present an ontology-based matchmaker.
Resource and request descriptions are expressed in RDF Schema, a semantic markup language. Matchmaking rules are expressed in TRIPLE, a language based on Horn Logic. Although our current implementation focuses on UDDI version 2, in future we will consider semantic extensions to UDDI, WS-Discovery [16] and other Grid computing standards such as Monitoring and Discovery Service (MDS) [10]. So the simplest extension of our work could involve using the DHT to do an initial syntax-based search to identify the local registries that need to be contacted.
Then the Proxy Registry can contact these registries, which do semantic matchmaking to identify their matches, which are then merged at the Proxy Registry and returned to the client.
The convergence of grid and P2P computing has been explored in [5]. GridVine [2] builds a logical semantic overlay on top of a physical layer consisting of P-Grid [1], a structured overlay based on distributed search tree that uses prefix-based routing and changes the overlay paths as part of the network maintenance protocol to adapt to load in different parts of the keyspace. A federated UDDI service [4] has been built on top of the PlanetP [3] publish-subscribe system for unstructured P2P communities.
The focus of this work has been on the manageability of the federated service. The UDDI service is treated as an application Article 2 service to be managed in their framework. So they do not address the issue of scalability in UDDI, and instead use simple replication. In [21], the authors describe a UDDI extension (UX) system that launches a federated query only if locally found results are not adequate. While the UX Server is positioned as an intermediary similarly to the UDDI Proxy described in our DUDE framework, it focuses more on the QoS framework and does not attempt to implement a seamless federation mechanism such as our DHT based approach. In [22] D2HT describes a discovery framework built on top of DHT. However, we have chosen to use UDDI on top of DHT. D2HT have used (Agent Management System) AMS/ (Directory Facilitator) DF on top of DHT.
In this paper, we have described a distributed architecture to support large scale discovery of web-services. Our architecture will enable organizations to maintain autonomous control over their UDDI registries and at the same time allowing clients to query multiple registries simultaneously. The clients are oblivious to the transparent proxy approach we have adopted and get richer and more complete response to their queries. Based on initial prototype testing, we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues. The paper has solved the scalability issues with UDDI but does not preclude the application of this approach to other service discovery mechanisms. An example of another service discovery mechanism that could benefit from such an approach is Globus Toolkit"s MDS. Furthermore, we plan to investigate other aspects of grid service discovery that extend this work. Some of these aspects include the ability to subscribe to resource/service information, the ability to maintain soft states and the ability to provide a variety of views for various different purposes. In addition, we plan to revisit the service APIs for a Grid Service Discovery solution leveraging the available solutions and specifications as well as the work presented in this paper.

Recent advances in computing hardware and software are responsible for the emergence of sensor networks capable of observing the environment, processing the data and making decisions based on the observations. Such a network can be used to monitor the environment, detect, classify and locate specific events, and track targets over a specific region.
Examples of such systems are in surveillance, monitoring of pollution, traffic, agriculture or civil infrastructures [6]. The deployment of sensor networks varies with the application considered. It can be predetermined when the environment is sufficiently known and under control, in which case the sensors can be strategically hand placed. The deployment can also be a priori undetermined when the environment is unknown or hostile in which case the sensors may be airdropped from an aircraft or deployed by other means, generally resulting in a random placement.
This paper investigates deployment strategies for sensor networks performing target detection over a region of interest. In order to detect a target moving through the region, sensors have to make local observations of the environment and collaborate to produce a global decision that reflects the status of the region covered [2]. This collaboration requires local processing of the observations, communication between different nodes, and information fusion [7]. Since the local observations made by the sensors depend on their position, the performance of the detection algorithm is a function of the deployment. One possible measure of the goodness of deployment for target detection is called path exposure. It is a measure of the likelihood of detecting a target traversing the region using a given path. The higher the path exposure, the better the deployment. The set of paths to be considered may be constrained by the environment. For example, if the target is expected to be following a road, only the paths consisting of the roads need to be considered.
In this study, the deployment is assumed to be random which corresponds to many practical applications where the region to be monitored is not accessible for precise placement of sensors. The focus of this paper is to determine the number of sensors to be deployed to carry out target detection in a region of interest. The tradeoffs lie between the network performance, the cost of the sensors deployed, and the cost of deploying the sensors. This paper is organized as follows. In section 2, a definition for path exposure is proposed and a method to evaluate the exposure of a given path is developed. In section 3, the problem of random deployment is formulated and several solutions are presented.
An analytical study of these solutions is given in section 4 and section 5 presents simulation results that are used in section 6 to determine the optimum solution for a given environment. The paper concludes with section 7.
In this section, a model for sensor network target detection is proposed, a definition of path exposure is presented and expressions for evaluating this path exposure are developed.
Consider a rectangular sensor field with n sensors de42 ployed at locations si, i = 1, . . . , n. A target at location u emits a signal which is measured by the sensors. The signal from the target decays as a polynomial of the distance.
If the decay coefficient is k, the signal energy of a target at location u measured by the sensor at si is given by Si(u) = K ||u − si||k (1) where K is the energy emitted by the target and ||u − si|| is the geometric distance between the target and the sensor.
Depending on the environment the value k typically ranges from 2.0 to 5.0 [4].
Energy measurements at a sensor are usually corrupted by noise. If Ni denotes the noise energy at sensor i during a particular measurement, then the total energy measured at sensor i, when the target is at location u, is Ei(u) = Si(u) + Ni = K ||u − si||k + Ni.
The sensors collaborate to arrive at a consensus decision as to whether a target is present in the region. We consider two basic approaches for reaching this consensus: Value fusion and Decision fusion [3]. In value fusion, one of the sensors gathers the energy measurements from the other sensors, totals up the energy and compares the sum to a threshold to decide whether a target is present. If the sum exceeds the threshold, then the consensus decision is that a target is present. In contrast, in decision fusion, each individual sensor compares its energy measurement to a threshold to arrive at a local decision as to whether a target is present.
The local decisions (1 for target present and 0 otherwise) from the sensors are totaled at a sensor and the sum is compared to another threshold to arrive at the consensus decision. In some situations, value fusion outperforms decision fusion and vice versa.
The probability of consensus target detection when the target is at location u is Dv(u) = Prob n i=1 K ||u − si||k + Ni ≥ η = Prob n i=1 Ni ≥ η − n i=1 K ||u − si||k , where η is the value fusion threshold. If the noise processes at the sensors are independent, then the probability density function of n i=1 Ni equals the convolution of the probability density function of Ni, i = 1, . . . , n. In particular, if the noise process at each sensor is Additive White Gaussian Noise (AWGN), then n i=1 Ni has a Chi-square distribution of degree n.
Due to the presence of noise, the sensors may incorrectly decide that a target is present even though there is no target in the field. The probability of a consensus false target detection is Fv = Prob n i=1 Ni ≥ η . (2) As above, if the noise processes at the sensors are independent and AWGN, then the false alarm probability can be computed from the Chi-square distribution of degree n.
For decision fusion, the probability of consensus target detection when the target is located at u is Dd(u) = Prob n i=1 hd,i(u) ≥ η2 = n j=η2 n j · P1 j · P0 (n−j) where P1 = Prob [hd,i(u) = 1] = Prob Ni ≥ η1 − K ||u − si||k and P0 = Prob [hd,i(u) = 0] = 1 − Prob [hd,i(u) = 1] . can be computed from Chi-square distribution of degree 1 for AWGN noise process.
The probability of false target detection at sensor i is Prob[gd,i = 1] = Prob[Ni ≥ η1] and Prob[gd,i = 0] = 1 − Prob[gd,i = 1].
Therefore, the probability of consensus false target detection is Fd = Prob n i=1 gd,i ≥ η2 = n j=η2 n j · (Prob [gd,i = 1])j · (Prob [gd,i = 0])(n−j) The above equations serve as an analytic basis for evaluating exposure as defined in the following subsection.
Note that in value and decision fusion the knowledge of the sensors location can be used to make the detection decision.
For example, a sensor can report values that differ substantially from its neighbors values. This discrepancy can be analyzed to avoid false alarms or misses and therefore improve the detection performance. However, such algorithms are not considered in this paper.
We define exposure to be the probability of detecting the target or an intruder carrying out the unauthorized activity, where the activity depends on the problem under consideration. In this paper, the activity considered is the Unauthorized Traversal (UT) as defined below.
Unauthorized Traversal (UT) Problem: We are given a sensor field with n sensors at locations s1, s2, . . . , sn (see Figure 1). We are also given the stochastic characterization of the noise at each sensor and a tolerable bound, α, on the false alarm probability. Let P denote a path from the west to the east periphery of the sensor field. A target traversing the sensor field using path P is detected if it is detected at some point u ∈ P. The exposure of path P is the net probability of detecting a target that traverses the field using P. The target is assumed to be able to follow any path through the field and the problem is to find the path P with the least exposure. 43 Sensor Figure 1: Example sensor fields for UT problem.
Let P denote a path from the west to the east periphery through the sensor field. A target that traverses the field using P is not detected if and only if it is not detected at any time while it is on that path. Since detection attempts by the sensor network occur at a fixed frequency, we can associate each detection attempt with a point u ∈ P when assuming that the target traverses the field at a constant speed. The detection attempts are based on energy measured over a period of time T during which the target is moving. Therefore, the detection probability associated with each point u reflects the measurements performed during time T. Considering the path, the net probability of not detecting a target traversing the field using P is the product of the probabilities of no detection at each point u ∈ P.
That is, if G(P) denotes the net probability of not detecting a target as it traverses over path P, then, log G(P) = u∈P log(1 − D(u))du, where D(u) is either Dv(u) of Dd(u) depending on whether the sensors use value or decision fusion to arrive at a consensus decision. Since the exposure of P is (1 − G(P)), the problem is to find the path which minimizes (1 − G(P)) or equivalently the path that minimizes | log G(P)|1 .
In general, the path P that minimizes | log G(P)| can be fairly arbitrary in shape. The proposed solution does not exactly compute this path. Instead, we rely on the following approximation. We first divide the sensor field into a fine grid and then assume that the target only moves along this grid. The problem then is to find the path P on this grid that minimizes | log G(P)|. Note that, the finer the grid the closer the approximation. Also, one can use higher order grids such as in [5] instead of the rectangular grid we use in this paper. The higher order grids change the runtime of the algorithm but the approach is the same as with the rectangular grid.
For the target not to be detected at any point u ∈ P, 1 Note that, G(P) lies between 0 and 1 and thus log G(P) is negative.
the west
the east
and b
Dijkstra"s algorithm
exposure equal to 10−w .
Figure 2: Pseudo-code of the proposed solution for the UT problem. it need not be detected at any point u lying between any two adjacent grid points of P. We therefore subdivide any path P as a chain of grid segments. Let us consider two adjacent points, say v1 and v2 on the grid. Let l denote the line segment between v1 and v2. Also, let ml denote the probability of not detecting a target traveling between v1 and v2 on the line segment l. Then, from the discussion above, log ml = u∈l log(1 − D(u))du (3) The probability ml can be evaluated by finding the detection probability D(u) at each point u ∈ l. Note that, ml lies between 0 and 1 and, therefore, log ml is negative.
To find the least exposed path, a non-negative weight equal to | log ml| is assigned to each segment l on this grid.
Also, a fictitious point a is created and a line segment is added from a to each grid point on the west periphery of the sensor field. A weight equal to 0 is assigned to each of these line segments. Similarly, a fictitious point b is created and a line segment is added from b to each grid point on the east periphery of the sensor field. A weight equal to 0 is assigned to each of these line segments.
The problem of finding the least exposed path from west periphery to east periphery is then equivalent to the problem of finding the least weight path from a to b on this grid. Such a path can be efficiently determined using the Dijkstra"s shortest path algorithm [1]. A pseudo-code of the overall algorithm is shown in Figure 2.
Example: Figure 3 shows a sensor field with eight sensors at locations marked by dark circles. Assume the noise process at each sensor is Additive White Gaussian with mean 0 and variance 1. Further assume that the sensors use value fusion to arrive at a consensus decision. Then, from Equation 2, we chose a threshold η = 3.0 to achieve a false alarm probability of 0.187%. The field has been divided into a 10 × 10 grid. The target emits an energy K = 12 and the energy decay factor is 2. The figure shows the weight assigned to each line segment in the grid as described above.
The least exposure path found by the Dijkstra"s algorithm for this weighted grid is highlighted. The probability of de44 Fictitious Fictitious Threshold = 3.0, Detection Probability of the Path = 0.926 Point A Point B0.090.921.651.610.860.08
Sensor
Figure 3: Illustration of the proposed solution for an example UT problem. tecting the target traversing the field using the highlighted path is 0.926.
In this section, the problem of sensor deployment for unauthorized traversal detection is formulated and solutions are identified.
Consider a region to be monitored for unauthorized traversal using a sensor network. The target to traverse the sensor field emits a given energy level K and the stochastic of the noise in the region is known. The sensors are to be deployed over the region in a random fashion where the sensors locations in the region are a priori undetermined and only the number or density of sensors can be chosen. The problem is to find a deployment strategy that results in a desired performance level in unauthorized traversal monitoring of the region.
This performance is measured by the false alarm probability and the path exposure defined in section 2. The false alarm probability does not depend on the sensor placement and is only determined by the number of sensors deployed and the thresholds used in the detection algorithms. It is assumed to be fixed in this study so that the problem consists of maximizing the exposure at constant false alarm rate.
Since targets can traverse the region through any path, the goal of deployment is to maximize the exposure of the least exposed path in the region.
Obviously, the minimum exposure in the region increases (if false alarm rate is kept constant) as more sensors are deployed in the region. However, since the deployment is random, there are no guarantees that the desired exposure level is achieved for a given number of sensors. Indeed some sensor placements can result in very poor detection ability, in particular when the sensors are all deployed in the same vicinity. A study of the statistical distribution of exposure for varying sensor placement for a given number of sensors can provide a confidence level that the desired detection level is achieved. In practical situations, only a limited number of sensors are available for deployment and only a limited detection level with associated confidence level is achievable at a given false alarm rate.
Based on the above discussion, we develop a solution method to the deployment problem when a maximum of M sensors can be used. Deploying the M sensors results in the maximum achievable detection level but is not optimal when considering the cost of sensors. To reduce the number of sensors deployed, only part of the available sensors can be deployed first and the sensors can then report their position.
The random sensor placement obtained can be analyzed to determine if it satisfies the desired performance level. If it does not, additional sensors can be deployed until the desired exposure level is reached or until the all M available sensors are deployed.
The number of sensors used in this strategy can be minimized by deploying one sensor at a time. However, a cost is usually associated with each deployment of sensors and deploying one sensor at a time may not be cost effective if the cost of deployment is sufficiently large with respect to the cost of single sensors. By assigning a cost to both single sensors and deployment, the optimal number of sensors to be deployed at first and thereafter can be determined. In the next section, we develop analytical expressions for finding the optimal solution. In general, the optimal cost solution is neither deploying one sensor at a time nor deploying all the sensors at a time.
In this section, we derive an analytical model for the cost of deployment. Let ed be the desired minimum exposure for the sensor network to be deployed when a maximum of M sensors are available for deployment. The position of sensors are random in the region of interest R and for a given num45 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
Minimum exposure Density 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
Minimum exposure Density 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
Minimum exposure Density 5 sensors 10 sensors 15 sensors Figure 4: Probability density function for the distribution of minimum exposure for deployments of 5, 10 and 15 sensors. ber of sensors n, the least exposure e is a random variable.
Let Fn(x) denote the cumulative distribution function of e, that is the probability that e is less than x, when n sensors are deployed.
As mentioned in the previous section, there is no a priori guarantee that the exposure ed will be obtained when deploying the sensors. If M is the maximum number of sensors available, the confidence of obtaining a least exposure of ed or more is 1−FM (ed). For the proposed solution, we assume that the position of the sensors obtained after a deployment is known so that additional sensors can be deployed if the minimum exposure ed is not reached. To evaluate the probability that the exposure ed is reached after additional sensor deployment, we make the following approximation: the distribution of exposure for n sensors is independent of the exposure corresponding to k of these n sensors, 1 ≤ k ≤ n − 1.
This is an approximation since the exposure obtained with n sensors is always higher than the exposure obtained with only k of these n sensors, 1 ≤ k ≤ n − 1. We observed that the re-deployment of just a few sensors can substantially modify the coverage of the region, for example when filling empty spaces. The approximation is also justified by the loose relation between exposure and sensors positions.
Indeed, a given minimum exposure can correspond to many different deployment positions, some of which can be easily improved by deploying a few additional sensors (e.g. when there is a empty space in the region coverage), some of which can only be improved by deploying many additional sensors (e.g. when the sensors are evenly distributed on the region).
As the minimum exposure e is a random variable, the cost of deploying the sensors in steps until the desired exposure is reached is also a random variable C. We now derive the expression for the expected value of C. Let ni be the total number of sensors deployed after each step i for a maximum number of steps S so that nS = M. Note that ni − ni−1 is the number of sensors deployed at step i. Also let Cd be the cost of deploying the sensors at each step and Cs be the cost of each sensor. If the desired exposure is obtained after the first step, the cost of deployment is Cd + n1Cs, and this 0 5 10 15 20 25 30 35 40 0
1 Number of sensors Probability ed =95% ed =90% ed =85% ed =80% Figure 5: Probability that the minimum exposure is above ed for varying number of sensors and ed=80%,85%,90% and 95%. event happens with probability 1 − Fn1 (ed). Considering all the possible events, the expected cost is given by E{C} = S−1 i=1 (i.Cd + ni.Cs) i−1 j=1 Fnj (ed) (1 − Fni (ed)) + (S.Cd + M.Cs) S−1 j=1 Fnj (ed) (4) Note that a different expression is needed for the cost of step S since no additional sensors are deployed after this step even when the desired exposure is not obtained.
In this section, we present results of simulations that were conducted to collect the exposure distribution function of the number of sensors deployed.
The exposure distribution is obtained by collecting statistics on the exposure when deploying sensors randomly in a predefined region. The random deployment is assumed to be uniformly distributed over the region, which is a local approximation. For every deployment, the minimum exposure is found using a simulator implementing the algorithm presented in section 2. A decay factor of k = 2 and maximum energy of K = 60 are chosen to model the energy emitted by targets (cf Equation 1). The region monitored is of size 20×20 with a noise (AWGN) of variance 1, so that the signal coming from the target is covered by noise when the target is 8 or more units length away from a sensor. The sensors use value fusion to collaborate when making a common decision on the presence of a target in the region. The threshold for detection is chosen as a function of the number of sensors to give a constant false alarm probability. The false alarm probability for each detection attempt is chosen so that the probability to get one or more false alarm along a path of length 20 units (corresponding to 20 detection attempts by the sensors) is 5%. 46 0 20 40 10 15 20 25 30 35 40 Cost for Cd=0 and Cs=1 n Expectedcost 0 20 40 20 30 40 50 60 70 80 Cost for Cd=5 and Cs=1 n Expectedcost 0 20 40 0 200 400 600 800 1000 1200 1400 Cost for Cd=100 and Cs=1 n Expectedcost Figure 6: Expected cost of achieving minimum exposure of 95% as function of the number of sensors for three different cost assignments.
The distribution of minimum exposure were found for the number of sensor deployed varying from 1 to 40. To illustrate our results, the probability density functions for 5, 10 and 15 sensors are shown in Figure 4.
We observe that for 5 sensors deployed, the minimum exposure has zero density for values less than the false alarm probability of .04. The highest density is obtained for values around .07 and then drops exponentially towards zero for higher values of exposure. For deployment of 10 sensors, we find again that the minimum exposure has zero density for values below .04, then increases and has about constant density for values lying between .1 and .98. We also observe a peak of density around 1. For deployment of 15 sensors, densities start at zero for small values and remain very small for most values of minimum exposure. The density slowly increases and has a large peak for minimum exposure of 1.
As expected, the minimum exposure increases on average as the number of sensors deployed increases. When randomly deploying 5 sensors, it is very unlikely to obtain a placement providing a desirable minimum exposure. When deploying 10 sensors, most of the exposure levels are equally likely and only poor confidence is given to obtain a desirable exposure level. When deploying 15 sensors, it is very likely that the sensor placement will give good exposure and this likelihood keeps increasing with the number of sensors deployed.
We use the cumulative distribution function obtained from the statistics collected to evaluate the likelihood that the desired level of exposure ed is obtained for varying number of sensors. The graph of Figure 5 shows the probability that the minimum exposure is above ed as a function of the number of sensors deployed for ed = 80%, 85%, 90% and 95%.
These values can be used to evaluate the cost expressed in Equation 4. The graph shows that the confidence level to obtain a given minimum exposure level ed increases with the number of sensors deployed. The confidence for ed when deploying 40 sensors is above .999, which is sufficient for most applications, and therefore we did not evaluate the distribution of exposure when deploying more than 40 sensors.
In this section, we evaluate the expected cost of deploying sensors using the simulation results. The optimal number of sensor to deploy at first and in the succeeding steps can be derived from these results.
For this cost analysis, the region parameters and signal model are the same as specified in section 5. We further assume that the number of sensors deployed at every step is constant so that ni − ni−1 = n for all 1 ≤ i ≤ S. In this case, Equation 4 reduces to E{C} = (Cd + n.Cs) S−1 i=1 i. i−1 j=1 Fj.n(ed) (1 − Fi.n(ed)) + (S.Cd + M.Cs) S−1 j=1 Fj.n(ed) (5) We evaluated the expected cost as a function of n for three different cost assignments with a desired exposure of ed = 95%. The three corresponding graphs are shown in Figure 6. The first cost assignment is (Cd = 0, Cs = 1) so that the expected cost is the expected number of sensors to be used to achieve an exposure of 95%. Since Cd = 0, the number of steps used to deploy the sensors doesn"t affect the cost and it is therefore optimal to deploy one sensor at a time until the minimum exposure ed is reached, as we observe on the graph. Overall, the expected number of sensor to be 47 deployed increases with n but we observe a local minimum for n = 16 that can be explained by the following analysis.
The expected number of sensors is a weighted sum of i.n, 1 ≤ i ≤ S that are the different number of sensors than can be deployed at a time when deploying n sensors at each step. For n around 16, the probability that the minimum exposure is above ed varies a lot as shown in Figure 5 and the weight associated with the first term of the sum (n) increases rapidly while the weights associated with higher number of sensors decrease. This is the cause of the local minimum and the cost starts to increase again when the increase in n compensates for the decrease in weights.
The second cost assignment is (Cd = 5, Cs = 1) so that the cost of a deployment is equal to the cost of five sensors (note that only the relative cost of Cd/Cs determines the shape of the graphs). In this case, deploying one sensor at a time is prohibited by the cost of deployment and the optimal number of sensors to deploy at every step is 19. Again, we find that the curve presents a local minimum for n = 9 that is due to the variations in weights. The last cost assignment is (Cd = 100, Cs = 1) and the minimum cost is achieved when deploying 27 sensors at every step.
These results are specific to the region and the parameters characterizing the signal emitted by the target that were chosen for the simulation. Similar results can be derived for other parameters, most of the effort residing in finding the exposure distributions through simulation.
This paper addresses the problem of sensor deployment in a region to be monitored for target intrusion. A mechanism for sensor collaboration to perform target detection is proposed and analyzed to evaluate the exposure of paths through the region. The minimum exposure is used as a measure of the goodness of deployment, the goal being to maximize the exposure of the least exposed path in the region.
In the case where sensors are randomly placed in a region to be monitored, a mechanism for sequential deployment in steps is developed. The strategy consists of deploying a limited number of sensors at a time until the desired minimum exposure is achieved. The cost function used in this study depends on the number of sensors deployed in each step and the cost of each deployment. Through simulation, the distribution of minimum exposure obtained by random deployment was evaluated for varying number of sensors deployed.
These results were used to evaluate the cost of deployment for varying number of sensors deployed in each step.
We found that the optimal number of sensors deployed in each step varies with the relative cost assigned to deployment and sensors. The results of this study can be extended to larger regions with different target parameters. The solution proposed in this paper can also be improved by considering deploying variable number of sensors at each step and this multiple variables problem requires further investigation.
This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) and the Air Force research Laboratory, Air Force Material Command, USAF, under agreement number F30602-00-2-055. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.
[1] S. Baase and A. V. Gelder. Computer algorithms: Introduction to design and analysis. Addison-Wesley,
[2] R. R. Brooks and S. S. Iyengar. Multi-Sensor Fusion: Fundamentals and Applications with Software. Prentice Hall, 1998. [3] T. Clouqueur, P. Ramanathan, K. K. Saluja, and K.-C.
Wang. Value-fusion versus decision-fusion for fault-tolerance in collaborative target detection in sensor networks. In Proceedings of Fourth International Conference on Information Fusion, Aug. 2001. [4] M. Hata. Empirical formula for propagation loss in land mobile radio services. IEEE Transactions on Vehicular Technology, 29:317-325, Aug. 1980. [5] S. Meguerdichian, F. Koushanfar, G. Qu, and M. Potkonjak. Exposure in wireless ad-hoc sensor networks. In Proceedings of MOBICOM, pages 139-150,
July 2001. [6] Sensor Information Technology Website,. http://www.darpa.mil/ito/research/sensit/index.html. [7] P. Varshney. Distributed Detection and Data Fusion.

Today's Internet uses the IP protocol suite that was primarily designed for the transport of data and provides best effort data delivery. Delay-constraints and characteristics separate traditional data on the one hand from voice & video applications on the other. Hence, as progressively time-sensitive voice and video applications are deployed on the Internet, the inadequacy of the Internet is exposed. Further, we seek to port telephone services on the Internet. Among them, virtual conference (teleconference) facility is at the cutting edge. Audio and video conferencing on Internet are popular [25] for the several advantages they inhere [3,6]. Clearly, the bandwidth required for a teleconference over the Internet increases rapidly with the number of participants; reducing bandwidth without compromising audio quality is a challenge in Internet Telephony. Additional critical issues are: (a) packet delay, (b) echo, (c) mixing of audio from selected clients, (d) automatic selection of clients to participate in the conference, (e) playout of mixed audio for every client, (f) handling clients not capable of mixing audio streams (such clients are known as dumb clients), and (g) deciding the number of simultaneously active clients in the conference without compromising voice quality.
While all the above requirements are from the technology point of view, the user's perspective and interactions are also essential factors. There is plenty of discussion amongst HCI and CSCW community on the use of Ethnomethodology for design of CSCW applications. The basic approach is to provide larger bandwidth, more facilities and more advanced control mechanisms, looking forward to better quality of interaction. This approach ignores the functional utility of the environment that is used for collaboration.
Eckehard Doerry [4] criticizes this approach by saying "it is keeping form before function". Thus, the need is to take an approach that considers both aspects - the technical and the functional. Regarding the functional aspect, we refer to [15] where it has been dealt with in some detail.
In this work, we do not discuss video conferencing; its inclusion does not significantly benefit conference quality [4]. Our focus is on virtual audio environments.
We first outline the challenges encountered in virtual audio conferences. Then we look into the motivations followed by relevant literature. In Section 5, we explain the architecture of our system. Section 6 comprises description of the various algorithms used in our setup. We address deployment issues. A discussion on Swiss Federal Institute of Technology, Lausanne. Former visitor at CEDT.
PESIT and NIAS, Bangalore, India.iv ii performance follows. We conclude taking alongside some implementation issues.
CONFERENCING Many challenges arise in building a VoIP application. The following are of particular concern in the process: • Ease of use: Conferencing must be simple; users need no domain expertise. Management (addition/removal) of clients and servers must be uncomplicated. Application development should not presuppose specific characteristics of the underlying system or of network layers. Ease of use may include leveraging readily available, technically feasible and economically viable technologies. • Scalability: Conferencing must seem uninterrupted under heavy loads, i.e., when many additional users are added on.
Traffic on WAN should not grow appreciably with the total number of clients; else, this has lead to congestion. So a means to regulate traffic to a minimum is needed for this kind of real-time applications. • Interactivity: In Virtual Conferencing Environments (VCEs), we intend a face-to-face-like conferencing application that mimics a "real" conference, where more vocal participants invite attention. Turn-taking in floor occupation by participants must be adapted gracefully to give a feel of natural transition. • Standardization: The solution must conform to established standards so as to gain interoperability and peer acceptance.
The above requirements are placed in the perspective of observations made in earlier works (vide Sections 3 and 4) and will steer the VCE design.
Ramanathan and Rangan [20] have studied in detail the architectural configurations comparing many conferencing architecture schemes taking into consideration the network delay and computation requirements for mixing. Functional division and object-oriented architecture design that aid in implementation is presented in [1]. An overview of many issues involved in supporting a large conference is dealt in [8]. H. P. Dommel [5] and many others highlight floor control as another pivotal aspect to be taken into account in designing a conferencing tool. Tightly coupled conference control protocols in Internet belong to the ITU-T H.323 family [9]; however, they are mainly for small conferences. The latest IETF draft by Rosenberg and Schulzrinne [23] discusses conferencing models with SIP [22] in the background. Aspects of implementation for centralized SIP conferencing are reported in [26]. A new approach called partial mixing by Radenkovic [18] allows for mixed and non-mixed streams to coexist. In all the above proposals, while there are some very useful suggestions, they share one or more of the following limitations: • In an audio conference, streams from all the clients need not be mixed. Actually, mixing many arbitrary streams [24] from clients degrades the quality of the conference due to the reduction in the volume (spatial aspect of speech). The number of streams mixed varies dynamically depending on the number of active participants. This would lead to fluctuations in the volume of every individual participant causing severe degradation in quality. Customized mixing of streams is not possible when many clients are active. There is a threshold on the number of simultaneous speakers above which increasing the number of speakers becomes counterproductive to conference quality. Fixing the maximum number of simultaneous speakers is dealt in a recent work [15] using Ethnomethodology, and is conjectured to be three. Thus it is advisable to honour that constraint. • There cannot be many intermediate mixers (similarly,
Conference Servers as in [10]) in stages as in [20] because it brings in inordinate delay by increasing the number of hops and is not scalable with interactivity in focus. • Floor Control for an audio conference (even video conference) with explicit turn-taking instructions to participants renders the conference essentially a one-speakerat-a-time affair, not a live and free-to-interrupt one. This way, the conference becomes markedly artificial and its quality degrades. Schulzrinne et al. [24], assume only one participant is speaking at a time. In this case, if applications are implemented with some control [5], the service becomes ‘gagging" for the users. • Partial mixing [18] has a similar problem as that of mixing when more streams are mixed. Moreover, in [18], to allow impromptu speech, mixing is not done when the network can afford high bandwidth requirements for sending/receiving all the streams, but it is unnecessary [15]. • For large conferences [23, 10] a centralized conference cannot scale up. With multicasting, clients will have to parse many streams and traffic on a client"s network increases unnecessarily.
Evidently, different particular issues, all of which are a subset of requirements (defined in [14] and [16]) for a VoIP conferencing support, are tackled. Thus there is a need to address conferencing as a whole with all its requirements considered concurrently.
Towards this goal, the VoIP conferencing system we propose is intended to be scalable and interactive. We make use of the "Loudness Number" for implementing floor control. This permits a participant to freely get into the speaking mode to interrupt the current speaker as in a natural face-to-face meeting. An upper limit on the number of floors (i.e., the number of speakers allowed to speak at the same time) is fixed using a conjecture proposed in [15].
The work presented here is in continuation of our studies into conferencing based on the Session Initiation Protocol in [14] and [16]. SIP, defined in [22] is now the most popular standard for VoIP deployment and has been chosen for its strength, ease of use, extensibility and compatibility. This is the reason it will be in the background of all controlling messages that will implicitly arise between the entities in our architecture. The actual messages are described in [16] and, as such, we do not present a complete description of them here.
The SIP standard defined in RFC 3261 [22] and in later extensions such as [21] does not offer conference control services such as floor control or voting and does not prescribe how a conference is to be managed. However SIP can be used to initiate a session that uses some other conference control protocol.
The core SIP specification supports many models for conferencing [26, 23]. In the server-based models, a server mixes media streams, whereas in a server-less conference, mixing is done at the end systems. SDP [7] can be used to define media capabilities and provide other information about the conference.
We shall now consider a few conference models in SIP that have been proposed recently [23].
First, let us look into server-less models. In End-System Mixing, only one client (SIP UA) handles the signaling and media mixing for all the others, which is clearly not scalable and causes problems when that particular client leaves the conference. In the Users Joining model, a tree grows, as each invited party constitutes a new branch in the distribution path. This leads to an increasing number of hops for the remote leaves and is not scalable. Another option would be to use multicast for conferencing but multicast is not enabled over Internet and only possible on a LAN presently. Among server-based models, in a Dial-In Conference, UAs connect to a central server that handles all the mixing. This model is not scalable as it is limited by the processing power of the server and bandwidth of the network.
Adhoc Centralized Conferences and Dial-Out Conference Servers have similar mechanisms and problems.
Hybrid models involving centralized signaling and distributed media, with the latter using unicast or multicast, raise scalability problems as before. However an advantage is that the conference control can be a third party solution.
Distributed Partial Mixing, presented in [18], proposes that in case of bandwidth limitation, some streams are mixed and some are not, leaving interactivity intact. Loss of spatialism when they mix and the bandwidth increase when they do not are open problems. A related study [19] by the same author proposes conferencing architecture for Collaborative Virtual Environments (CVEs) but does not provide the scalability angle without the availability of multicasting. With the limitations of proposed conferencing systems in mind, we will now detail our proposal.
This section is dedicated to the description of the proposed system architecture. However, as this paper constitutes the continuation of our work started in [14] and furthered in [16], we will not present here all the details about the proposed entities and invite the readers to consult the papers mentioned above for a full and thorough description.
First, we do not restrict our conferencing system to work on small conferences only, but rather on large audio VCEs that have hundreds (or even thousands) of users across a Wide Area Network (WAN) such as the Internet. This view stems from an appraisal that VCEs will gain in importance in the future, as interactive audio conferences will be more popular because of the spread of the media culture around the world.
Two issues must be taken care of when building a VoIP conferencing system: (i) the front-end, consisting of the application program running on the end-users" computers and (ii) the back-end that provides other application programs that facilitate conferencing and conference.
The participating users are grouped into several domains. These domains are Local Area Networks (LANs), such as corporate or educational networks. This distributed assumption asks for distributed controlling and media handling solutions, as centralized systems would not scale for such very large conferences (vide Section 4).
More explicitly, in each domain, we can identify several relevant logical components of a conferencing facility (Fig. 1):  An arbitrary number of end users (clients) that can take part in at most one audio conference at a time. Every user is Fig. 1. Conference example - 3 domains containing the necessary entities so that the conference can take place. included in one and only one domain at a given instant, but can move from domain to domain (nomadism). In our conferencing environment, these clients are regular SIP User Agents (SIP UAs), as defined in [22] so to gain in interoperability with other existing SIP-compatible systems.
These clients are thus not aware of the complex setting that supports the conference and this is highlighted below.  One SIP Server (SIPS) per domain, taking care of all the signaling aspects of the conference (clients joining, leaving, etc.) [16]. In particular, it is considered as a physical implementation encompassing different logical roles, namely a SIP Proxy Server, a SIP Registrar Server, a SIP Redirect Server and a SIP B2BUA (Back-to-Back User Agent) [22].
This physical implementation enables the handling of incoming/outgoing SIP messages by one or another logical entity according to the needs. SIPS is entrusted with maintaining total service and has many advantages such as (a) it works as a centralized entity that can keep track of the activities of the UAs in a conference; (b) it can do all the switching for providing PBX features; (c) it can locate the UAs and invite them for a conference; (d) it can do the billing as well. SIPSs in different domains communicate with each other using SIP messages as described in [16]. If the load on a particular SIPS is too heavy, it can create another SIPS in the same domain so that the load will be shared.  One Master Conference Server (M-CS) (simply a Conference Server (CS)) for each conference is created by the local SIPS when a conference starts. This server will be used for handling media packets for the clients of the domain. Its mechanism will be described in the next section. The M-CS will be able to create a hierarchy of CSs inside a domain by adding one or more Slave CSs (S-CSs) to accommodate all the active clients and prevent its own flooding at the same time. We will see this mechanism in some detail in the sequel.
The entities described here are exhaustive and conform to the SIP philosophy. Thus, the use of SIP makes this architecture more useful and interoperable with any other SIP clients or servers.
Similar to SipConf in [27], a Conference Server (CS) [17] has the function of supporting the conference; it is responsible for handling audio streams using RTP. It can also double to convert audio stream formats for a given client if necessary and can work as Translators/Mixers of RTP specification behind firewalls.
We have based the design of our CS on H.323 Multipoint Processor (MP) [9]. In short, the MP receives audio streams from the endpoints involved in a centralized or hybrid multipoint conference, processes them and returns them to the endpoints. An MP that processes audio prepares NMax audio outputs from M input streams after selection, mixing, or both. Audio mixing requires decoding the input audio to linear signals (PCM or analog), performing a linear combination of the signals and reencoding the result in an appropriate audio format. The MP may eliminate or attenuate some of the input signals in order to reduce noise and unwanted components.
Fig. 2. Schematic diagram of a CS The limitation of H.323 is that it does not address the scalability of a conference. The architecture proposes a cascaded or daisy chain topology [10], which can be shown that it cannot scale up for a large conference.
A CS serves many clients in the same conference. Thus it handles only one conference at a time. Multiple CSs may coexist in a domain, as when there are several conferences under way.
Signaling-related messages of CSs are dealt in [11].
The working of a CS is illustrated on Fig. 2: For each mixing interval, CS 1 chooses the best NMax audio packets out of the M1 (using a criterion termed "Loudness Number, described in the next subsection). It may possibly receive and sends these to CSs 2 to P. The set of packets sent is denoted as ToOtherCSs. In the same mixing interval, it also receives the best NMax audio packets (out of possibly M2) from CS 2, similarly the best NMax (out of possibly MP) from CS P. For simplicity, we ignore propagation delay between CSs which indeed can be taken into account; it is beyond the scope of this presentation. The set of packets received is denoted as FromOtherCSs. Finally, it selects the best NMax packets from the set {ToOtherCSs union FromOtherCSs} and passes these packets to its own group.
It can be seen that the set {ToOtherCSs union FromOtherCSs} is the same at all CSs. This ensures that any client in the conference finally receives the same set of packets for mixing. Hence all clients obtain a common view of the conference.
Similarly, for each time slot (packet time), a subset, F of all clients is selected (using the same criterion) from the pool of packets from all other CSs plus the NMax clients selected locally.
Their packets are mixed and played out at the clients. According to [15], the cardinality of F, |F| is NMax and is fixed at three.
In our conferencing setup, selection is by the Master Conference Server (M-CS), which comes into the picture exclusively for media handling. Note that even if the SIP specification enables direct UA-to-UA media communication in a one-to-one call, it is also possible to use the Conference Server for two-party calls, especially because it is then more functional to create a real conference by adding a third and subsequently more participant(s).
There are cases wherein the processing capacity of an M-CS is exceeded as it may have too many packets - from local domains and from remote domains - to process. In that case, the M-CS will create one or many S-CS (Fig. 6) and transfer its own clients as well as the new clients to them. In this configuration, the algorithm outlined above will be slightly modified, as the audio packets will go from clients to their dedicated S-CS that will select NMax packets to send to the local M-CS, which will then select NMax packets from all its S-CSs in the domain before sending them to the remote domains. The incoming packets from other domains will be received by the M-CS, select NMax of them and send them directly to the domain clients, bypassing the SCSs. This change implies that at most three intermediate entities exist for each audio packet, instead of two in the conventional setup. As the extra hop happens inside the LAN supposed to have a high-speed connectivity, we consider that it should not prevent us from using this hierarchy of CSs when there"s a need to do so.
A basic question to be answered by the CS is the following. In a mixing interval, how should it choose NMax packets out of the M it might possibly receive? One way is to rank the M packets received according to their energies, and choose the top NMax.
However, this is usually found to be inadequate because random fluctuations in packet energies can lead to poor audio quality. This indicates the need for a metric different from mere individual packet energies. The metric should have the following characteristics [12]: • A speaker (floor occupant) should not be cut off by a spike in the packet energy of another speaker. This implies that a speaker"s speech history should be given some weight. This is often referred to as Persistence or Hangover. • A participant who wants to interrupt a speaker will have to (i) speak loudly and (ii) keep trying for a little while. In a face-to-face conference, body language often indicates the intent to interrupt. But in a blind conference under discussion, a participant"s intention to interrupt can be conveyed effectively through LN.
A floor control mechanism empowered to cut off a speaker forcefully must be ensured.
These requirements are met by Loudness Number [12], which changes smoothly with time so that the selection (addition and deletion) of clients for conference is graceful.
LN (= ) is a function of the amplitude of the current audio stream plus the activity and amplitude over a specific window in the past.
Fig. 3. The different windows used for LN computation The Loudness Number is updated on a packet-by-packet basis.
The basic parameter used here is packet amplitude, which is calculated as root mean square (rms) of the energies in audio samples of a packet, and denoted by XK. Three windows are defined as shown in Fig. 3.
The present amplitude level of the speaker is found by calculating the moving average of packet amplitude (XK) within a window called Recent Past Window starting from the present instant to some past time. The past activity of the speaker is found by calculating the moving average of the packet amplitude (XK) within a window called Distant Past Window, which starts at the point where the Recent Past window ends and stretches back in the past for a pre-defined interval. The activity of the speaker in the past is found with a window called Activity Horizon, which spans the recent past window as well as the distant past window and beyond if necessary. Though the contribution of the activity horizon looks similar to the contribution of the recent past and distant past windows, past activity is computed from activity horizon window in a differently.
Define the quantities during these three intervals as L1, L2 and L3.
L1 quantifies the Recent Past speech activity, L2 the Distant Past speech activity and L3 gives a number corresponding to the speech activity in the Activity Horizon window quantifying how active the speaker was in the past few intervals. L3 yields a quantity that is proportional to the fraction of packets having energies above a pre-defined threshold (Eq. 3). The threshold is invariant across clients. ∑ +− = = 1 1 1 RPP P Wt tK K RP X W L (1) ∑ +−− −= = 1 2 1 DPRPP RPP WWt WtK K DP X W L (2) ∑ +− = ≥= 1 }{3 * 1 AHP P K Wt tK X AH I W L θθ (3) Where ifI KX 1}{ =≥θ θ≥KX = otherwise,0 The threshold is a constant. is set at 10-20 percent of the amplitude of the voice samples of a packet in our implementation here. Loudness Number λ for the present time instant (or the present packet) is calculated as, 332211 *L*L*L αααλ ++= (4) Here 1 2 DQG 3 are chosen such that: 0< 1 2  1 2 DQG 3=1- 1 2) Here, 1 is the weight given to the recent past speech, 2 is the weight given to distant past speech and 3 is the weight given to speech activity in the activity horizon window considered.
The λ parameter KDV VRPH PHPRU\ GHSHQGLQJ RQ WKH VSUHDG RI the windows. After one conferee becomes silent, another can take the floor. Also, as there is more than one channel, interruption is enabled. A loud conferee is more likely to be heard because of elevated λ. This ensures fairness to all conferees. After all, even in a face-to-face conference, a more vocal speaker grabs special attention. All these desirable characteristics are embedded into the LN. A comprehensive discussion on selection of the various parameters and the dynamics of LN are beyond the scope of this paper.
Following the developments in subsections 6.1 and 6.2, we present the simple algorithm that runs at each Master-Conference Server (Algorithm. 1). This algorithm is based on the discussions in section 6.1. The globally unique set F is found using this procedure.
Repeat for each time slot at each M-CS {
that belong to it.
PD[LPXP RXW RI 0 &OLHQWV LQ LWV GRPDLQ
Clients in database DB1.
Unicast or Multicast, depending on the configuration).
other M-CSs and store them in database DB2.
WKH EDVLV RI DQG VHOHFW D PD[LPXP RI NMax amongst them (to form set F) that should be played out at each Client.
Clients in its domain.
linearising and send it to dumb Clients in the domain. } Algorithm 1. Selection algorithm The mechanism proposed here is also depicted on Fig. 6, where a single conference takes place between three domains. The shaded clients are the ones selected in their local domains; their audio streams will be sent to other CSs.
We now analyze deployment issues associated with conference management. How are domains to be organized to maximize the number of participants able to join? To address this, we define some useful parameters.  Let d be the number of different domains in which there are active clients in a given conference.  Let Mi be the number of active clients present in domain i ( di ≤≤1 ) in a given conference. The total number of active clients in the conference is thus ∑= = d i iMM 1 .  Let C be the maximum number of audio streams a Conference Server can handle in a packet time, also called capacity. C is set according to the processing power of the weakest CS in the conference but as it cannot be assumed that we know it a-priori, it can be set according to some minimum system requirement a machine must meet in order to take part in a conference.  Let NMax be the number of output streams a CS has to send to other CSs in remote domains (see section 6.1). We will set NMax =3 (=|F|), according to [15].
The optimization problem is now to find the value of d that maximizes the total number of clients Mi served by one CS in a domain with capacity C. We first dispose the case where the capacity is not exceeded (the existing CS is not overloaded), and then proceed to the case where there exists a need to create more CSs when a single CS is overloaded.
We assume that clients are equally distributed amongst the domains, as we may not have information to assume an a-priori distribution of the clients. We can specify no more than an upper bound on the number of clients acceptable, given the number of active domains d.
In this subsection, we consider that we have only one CS, i.e., a unique M-CS in each domain. Thus it cannot be overloaded. We consider that the system works as outlined in section 6.1: The Clients send their audio packets to their local CS, which selects NMax streams, before sending them to other CSs. In parallel, it also receives NMax streams for every other CSs before taking a decision on which NMax streams will be selected, sent and played out at each individual clients.
For system stability, any CS in the conference should be able to handle its local clients in addition to the audio packets from other domains. Clearly then, the following inequality must hold for every domain: )1( −⋅+≥ dN d M C Max (5) The limiting case of (5) (taking the equality) takes the form 2 )( dNdNCM MaxMax ⋅−⋅+= (6) To optimize d with respect to M, we set 0)(2 =+−⋅⋅= ∂ ∂ MaxMax NCdN d M (7) yielding     ⋅ + = Max Max N NC d 2 (8) ([ ]* = Rounding to nearest integer) and hence, M from (6).
C d M 50 9 234 100 17 884 150 26 1950 200 34 3434 250 42 5334 300 51 7650 350 59 10384 400 67 13534 450 76 17100 500 84 21084 Table 1. Values of d and M computed for some values of C with NMax = 3.
In Table 1, we give the values of d and M that were computed using (8) and (6) with NMax = 3. We see that the values of d and M, being dependent of C, are therefore based on the weakest CS.
We see that there is a trade-off between M and d. We could admit more domains in the conference, but at the expense of restricting the total number of clients M in the conference.
While implementing and testing the Conference Servers on a Pentium III 1.4 GHz running Windows NT, we were able to set C=300. But with the advent of faster computers (> 3 GHz), one can easily set C to higher values and determine d and M accordingly.
Fig. 4 shows a contour plot and Fig. 5, a 3D-mesh showing optimized solutions for CSs of different capacities. These lead us to maximize the number of domains, and hence, to maximize the total number of clients based on the capacity of various CSs. In Fig. 4, the individual curves represent the total number of clients targeted, and we select a lower value of d, for capacity C, for targeted M to reduce traffic on WAN. Fig. 5 represents a different perspective of the same data in 3D.
Fig. 4. Contour Plot of Capacity versus Optimum number of domains for various conference sizes
Now considering the case where the number of clients in a particular domain is too large, i.e., d M Mi ≥ (9) one has to avoid the denial of service for new clients due to overloading of Conference Server. This problem can be solved by introducing a second level of CSs inside the given domain, as in Fig. 6. The existing M-CS creates a Slave CS (S-CS) that can handle up to C end-users and to which it transfers all its active clients. Here, the system works differently as outlined in section
which selects NMax streams, before sending them to a local M-CS, which will proceed in the same way, before sending NMax streams to the other domains. Each newly created S-CS must run on a separate machine. The M-CS has to create more S-CSs if the number of active clients exceeds C in the course of the conference after the transfer.
With this mechanism, the M-CS will be able to create utmost     −⋅− = Max Max N dNC U )1( S-CSs, (10) as it must handle 3 (= NMax) packets for each local S-CSs and 3 (= NMax) packets from each other remote domains. We can then calculate the maximum theoretical number of active clients CUMi ⋅= in each domain as well as M, for the whole conference as CUdM ⋅⋅= .
Fig. 5. 3D Plot of Capacity versus Optimum number of domains for various conference sizes Of course, one could further create a third level in the hierarchy, giving the possibility of accommodating even more clients. This may be unnecessary as the number of possible clients is large enough with two levels.
We now analyze the performance of the algorithm presented in subsection 6.3, i.e., the one taking care of the exchange of audio packets between the different domains. Note that the packets that are transiting within the LAN take advantage of the higher capacity (generally coupled with multicast capabilities) and therefore do not require a performance analysis.
Thus we have to look only at the RTP packets over the WAN, i.e., between participating M-CSs. As each M-CS from a domain will be sending only NMax out of d M packets to the other CSs ( MaxN d M >> ), the bandwidth used by the application over a WAN is upper-bounded by the following expression.
The total number of audio packets transiting over the WAN for each time slot is ∑ ∑= ≠=        d i d ijj MaxN 1 ,1 which is quadratic in the number of domains (i.e., O(d2 )).
However, it is independent of the total number of active clients.
This would not have been the case had all packets been sent over the network in each time slot.
The saving is tremendous. Yet, one may contend that sending three packets to and from all domains is a waste of resources, as most of these streams will not be selected. If just one client is active, selecting a subset of clients in that domain is unnecessary.
Pessimistic and optimistic algorithms as presented in the sequel aim at reducing the traffic further by harnessing the slow varying nature of the LN.
Consider a scenario wherein the lowest LN (called LNt) of the three globally selected streams (set F of Section 6.1) exceeds the LN of the most dominant stream of a domain. Evidently, the chances that the next two dominant streams of that domain being selected to F in the next packet period are less. Here, we send this most dominant stream and withhold the other two. There may be an error in unique selection across all domains for one packet period only. As LN varies slowly, the error would get automatically rectified in a subsequent packet period (slot). In this algorithm, there is at least one stream in each period. The net network traffic in a packet period in the best case is )1( −⋅ dd , i.e., )( 2 dO using unicast, instead of MaxNdd ⋅−⋅ )1( .
Considerable valuable bandwidth can be saved using this heuristic. The resulting traffic complexity reduces from O(d2 ) to O(d) in multicast-enabled networks.
Initialize LNt = 0 at an M-CS/S-CS A. In the first time slot (packet time), each CS sends the top NMax streams (based on their LN) to all other CSs.
At each M-CS/S-CS and for each packet time: B. Find the value of lowest LN of the NMax globally selected streams (set F) from the previous time slot. Set LNt with this value.
C. At each CS domain, select the NMax local streams that have maximum value of LN (ToOtherCSs set).
D. Select streams that have LN > LNt.
IF there are >= NMax streams with LN > LNt then send top NMax to other CSs.
ELSE IF there are (NMax-1) streams with LN>LNt then send top (NMax-1) plus the one lower than LNt (i.e., top NMax) to other CSs.
ELSE IF there are (NMax-2) streams with LN>LNt then send top (NMax-2) plus the one lower than LNt (i.e. top (NMax -1)) to other CSs. …… ELSE IF there are NO streams with LN> LNt then send top 1 stream to other CSs.
E. Packets sent in step D form DB1. Packets received from other CSs form DB2.
F. For this time slot, find global NMax streams based on LN from DB1 U DB2 (set F) G. Send set F to the clients in its domain.
Update LNt for the next period.
Algorithm 2. Pessimistic algorithm to reduce the number of packets sent over the Internet.
Fig. 6. Example of a 2-level hierarchy of Conference Servers; the shaded Clients are the one selected by the M-CS and will be sent to other domains" CSs.
In this algorithm the saving in traffic is at the cost of relaxing the condition of formation of globally unique set F. However, the discrepancies in selected streams at different domains remain for a short period of time depending on the transportation delay between any two domains. Even for a total delay of 400ms, for only 10 packet time slots the uniqueness is lost. This duration in a real-time interactive conversation is non-perceivable by the listener. In the case that there is a joke and every one laughs, then there would be sudden rise in the number of packets and it would be upper bounded by MaxN)d(O 2 for a short period.
The traffic can be reduced further. The scheme in the following algorithm (Algorithm. 3) is withholding all the streams that have less value of LN compared to the least of the three in the set F.
We can find the correct and unique three streams after a few time slots depending on the transportation delay between the domains.
As the packet period is of the order of 40ms, the error in the selection is unnoticeable. The number of streams on network in this case is always restricted to NMax (=3). Even without Voice Activity Detection (VAD), there will be no more than three streams in the network in the best case, thus the total traffic is constant. A sudden burst of traffic, as described in 8.1, is a particular case. These advantages are due to exploitation of the characteristics of LN.
Initialize LNt = 0 at an M-CS/S-CS A. In the first time slot (packet time), each CS sends the top NMax streams (based on their LN) to all other CSs.
At each M-CS/S-CS and for each packet time: B. Find the value of lowest LN of the NMax globally selected streams (set F) from the previous time slot. Set LNt with this value.
C. At each CS domain, select the NMax local streams that have maximum value of LN (ToOtherCSs set) D. Select streams that have LN > LNt IF there are >= NMax streams with LN > LNt then send top NMax to other CSs.
ELSE IF there are (NMax-1) streams with LN>LNt then send top (NMax-1) and see E.
ELSE IF there are (NMax-2) streams with LN>LNt then send top (NMax-2) and see E. …… ELSE IF there are NO streams with LN> LNt then don"t send any stream.
E. Exceptions: IF the stream that was in F in the last interval belongs to this CS then select and send that stream even if its LN is now < LNt. (Note this occurs only at that CS which had the stream that was the last of the three in the previous packet period.) F. Packets sent in step D and E form DB1. Packets received from other CSs form DB2.
G. For this time slot, find global NMax streams based on LN from DB1 U DB2 (set F).
H. Send set F to the clients in its domain.
Update LNt for the next period.
Algorithm 3. Optimistic algorithm to reduce the number of packets sent over the Internet Furthermore, when VAD is used [13], it would further reduce the traffic by sending the header part of the RTP packet only and not the whole packet, thus in order to keep updating the LN across.
The traffic here in this case is O(NMax) for multicast and O(d) for unicast.
We see that the above algorithms save bandwidth and computation at each CS, and leads to a scalable architecture with multiple CSs mainly because clients are grouped in domains. The necessary bandwidth is not dependent on the total number of active clients. As the CS always chooses the best three clients out of all the clients assigned to it in the domain, addition of new clients to the existing conference will not cause any scalability problem.
In the architecture that has been proposed, no assumption was made about the availability of multicasting support from the network. The traffic will be further reduced if multicasting is available over WAN. It is simple to show that the order of traffic would tend to become O(d) from O(d2 ). This is an approximation as saving in multicasting depends also on the topology. The analysis was done for the case wherein multicast is not available (a realistic assumption in today"s Internet). The advantage of this set up is that we can use it even if multicasting is partially available. We can instruct CSs during the set-up phase to send unicast packets to those CSs that cannot receive multicast packets whereas CSs on multicast enabled routers can exchange packets on a multicast address. The data structures and conference objects inside a CS is given in [14].
Fig. 7. User Interface for setting the weight for NMax audio streams (setting Self-bar to zero avoids echo).
The observed improvement in the perceived quality of the conference service is due to: (1) limiting the number of concurrent speakers to a low number such as three. Generally, in a conference if more than two participants speak the intelligibility is lost. The conversational analysis demonstrates that there would be a repair mechanism [15] in such a case. (2) Delay: The audio stream between any two clients will pass through at most two CSs thus reducing the end-to-end delay. For a large conference there might be three CSs however, one hop is within the domain incurring negligible delay. (3) As the streams are mixed only at the clients, there can be a customized mix of the streams. The individual tuning of mixing with weights the spatialism is preserved. Fig. 7 shows the user interface for the same. The echo when self-stream is selected can be avoided by reducing the weight. Nonetheless, feedback helps in reassuring speaker that he/she is heard by all.
In this paper, we have presented a discussion on a voice-only virtual conferencing environment. We have argued that the distributed nature of deployment here makes it scalable.
Interactivity is achieved by adapting a recent stream selection scheme based on Loudness Number. Additionally, we incorporate a result from a more recent work [15] where the sufficiency of three simultaneous speakers has been demonstrated. Thus, there is significantly effective utilization of bandwidth. A mixed stream is played out at each client; each client may choose to have a customized mix since mixing is done at the local terminal of each client. These render impromptu speech in a virtual teleconference over VoIP a reality, as in a real face-to-face conference.
Compatibility is assured thanks to the use of SIP, the most soughtafter signaling protocol. To ensure a satisfying performance, we do not demand the availability of multicast, but use it if and when available. The traffic in the WAN (Internet) is upper-bounded by the square of the number of domains, -- further reduced by using heuristic algorithms -- which is far below the total number of clients in the conference. This is due to the use of a Conference Server local to each domain. VAD techniques help further traffic reduction. Using SIP standard for signaling makes this solution highly interoperable.
We have implemented a CS application on a campus-wide network. We believe this new generation of virtual conferencing environments will gain more popularity in the future as their ease of deployment is assured thanks to readily available technologies and scalable frameworks.
[1] L Aguilar et al., Architecture for a Multimedia Teleconferencing System, in Proceedings of the ACM SIGCOMM, Aug 1986, pp. 126-136. [2] Carsten Bormann, Joerg Ott et al., Simple Conference Control Protocol, Internet Draft, Dec. 1996. [3] M. Decina and V. Trecordi, "Voice over Internet Protocol and Human Assisted E-Commerce", IEEE Comm.
Magazine, Sept. 1999, pp. 64-67. [4] Eckehard Doerry, "An Empirical Comparison of Copresent and Technologically-mediated Interaction based on Communicative Breakdown", Phd Thesis, Graduate School of the University of Oregon, 1995. [5] H. P. Dommel and J.J. Garcia-Luna-Aceves, "Floor Control for Multimedia Conferencing and Collaboration", J.
Multimedia Systems, Vol. 5, No. 1, January 1997, pp. 23-38. [6] Amitava Dutta-Roy, "Virtual Meetings with desktop conferencing", IEEE Spectrum, July 1998, pp. 47-56. [7] M. Handley and V. Jacobson, "SDP: Session Description Protocol", RFC 2327, IETF, April 1998. [8] M. Handley, J. Crowcroft et al., "Very large conferences on the Internet: the Internet multimedia conferencing architecture", Journal of Computer Networks, vol. 31, No. 3, Feb 1999, pp. 191-204. [9] ITU-T Rec. H.323, Packet based Multimedia Communications Systems, vol. 2, 1998. [10] P. Koskelainen, H. Schulzrinne and X. Wu, "A SIP-based Conference Control Framework", NOSSDAV"02, May 2002, pp. 53-61. [11] R Venkatesha Prasad et al., Control Protocol for VoIP Audio Conferencing Support, International Conference on Advanced Communication Technology, Mu-Ju, South Korea, Feb 2001, pp. 419-424. [12] R Venkatesha Prasad et al., "Automatic Addition and Deletion of Clients in VoIP Conferencing", 6th IEEE Symposium on Computers and Communications, July 2001,
Hammamet, Tunisia, pp. 386-390. [13] R Venkatesha Prasad, H S Jamadagni, Abjijeet, et al Comparison of Voice Activity Detection Algorithms, 7th IEEE Symposium on Computers and Communications. July 2002, Sicily, Italy, pp. 530-535. [14] R. Venkatesha Prasad, Richard Hurni, H S Jamadagni, A Scalable Distributed VoIP Conferencing using SIP, Proc. of the 8th IEEE Symposium on Computers and Communications, Antalya, Turkey, June 2003. [15] R Venkatesha Prasad, H S Jamadagni and H N Shankar, "On Problem of Specifying Number of Floors in a Voice Only Conference", To appear in IEEE ITRE 2003. [16] R. Venkatesha Prasad, Richard Hurni, H S Jamadagni, "A Proposal for Distributed Conferencing on SIP using Conference Servers", To appear in the Proc. of MMNS 2003, Belfast, UK, September 2003. [17] R. Venkatesha Prasad, H.S. Jamadagni, J. Kuri, R.S.
Varchas, A Distributed VoIP Conferencing Support Using Loudness Number, Tech. Rep. TR-CEDT-TE-03-01 [18] M. Radenkovic et al, "Scaleable and Adaptable Audio Service for Supporting Collaborative Work and Entertainment over the Internet", SSGRR 2002, L'Aquila,
Italy, Jan. 2002. [19] M. Radenkovic, C. Greenhalgh, S. Benford, Deployment Issues for Multi-User Audio Support in CVEs, ACM VRST 2002, Nov. 2002, pp. 179-185. [20] Srinivas Ramanathan, P. Venkata Rangan, Harrick M. Vin,
Designing Communication Architectures for Interorganizational Multimedia Collaboration, Journal of Organizational Computing, 2 (3&4), pp.277-302, 1992. [21] A. B. Roach, " Session Initiation Protocol (SIP)-Specific Event Notification", RFC 3265, IETF, June 2002. [22] J. Rosenberg, H. Schulzrinne et al., "SIP: Session Initiation Protocol", RFC 3261, IETF, June 2002. [23] J. Rosenberg, H. Schulzrinne, Models for Multy Party Conferencing in SIP, Internet Draft, IETF, July 2002. [24] H. Schulzrinne et al., "RTP: a transport protocol for realtime applications", RFC 1889, IETF, Jan 1996. [25] Lisa R. Silverman, "Coming of Age: Conferencing Solutions Cut Corporate Costs", White Paper, www.imcca.org/wpcomingofage.asp [26] Kundan Singh, Gautam Nair and Henning Schulzrinne, "Centralized Conferencing using SIP", Proceedings of the 2nd IP-Telephony Workshop (IPTel), April 2001. [27] D. Thaler, M. Handley and D. Estrin, "The Internet Multicast Address Allocation Architecture", RFC 2908,

In the early morning hours (05:30 GMT) of January 25, 2003 the fastest computer worm in recorded history began spreading throughout the Internet. Within 10 minutes after the first infected host (patient zero), 90 percent of all vulnerable hosts had been compromised creating significant disruption to the global Internet infrastructure. Vern Paxson of the International Computer Science Institute and Lawrence Berkeley National Laboratory in its analysis of Slammer commented: The Slammer worm spread so quickly that human response was ineffective, see [4] The interesting part, from our perspective, about the spread of Slammer is that it was a relatively unsophisticated worm with benign behavior, namely self-reproduction. Since Slammer, researchers have explored the behaviors of fast spreading worms, and have designed countermeasures strategies based primarily on rate detection and limiting algorithms. For example, Zou, et al., [2], proposed a scheme where a Kalman filter is used to detect the early propagation of a worm. Other researchers have proposed the use of detectors where rates of Destination Unreachable messages are monitored by firewalls, and a significant increase beyond normal, alerts the organization to the potential presence of a worm. However, such strategies suffer from the fighting the last War syndrome.
That is, systems are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer.
In the work described here, we put forth the hypothesis that next generation worms will be different, and therefore such techniques may have some significant limitations.
Specifically, we propose to study a new generation of worms called Swarm Worms, whose behavior is predicated on the concept of emergent intelligence. The concept of emergent intelligence was first studied in association with biological systems. In such studies, early researchers discovered a variety of interesting insect or animal behaviors in the wild.
A flock of birds sweeps across the sky. A group of ants forages for food. A school of fish swims, turns, flees together away from a predator, ands so forth. In general, this kind of aggregate motion has been called swarm behavior. Biologists, and computer scientists in the field of artificial intelligence have studied such biological swarms, and 323 attempted to create models that explain how the elements of a swarm interact, achieve goals, and evolve. Moreover, in recent years the study of swarm intelligence has become increasingly important in the fields of robotics, the design of Mobile ad-hoc Networks (MANETS), the design of Intrusion Detection Systems, the study of traffic patterns in transportation systems, in military applications, and other areas, see [3].
The basic concepts that have been developed over the last decade to explain swarms, and swarm behavior include four basic components. These are:
N agents whose intelligence is limited. Agents in the swarm use simple local rules to govern their actions.
Some models called this primitive actions or behaviors;
with other members in the swarm via simple local communication mechanisms. For example, a bird in a flock senses the position of adjacent bird and applies a simple rule of avoidance and follow.
their environment, which probably consists of other agents, but act relatively independently from all other agents. There is no central command or leader, and certainly there is no global plan.
autonomous agents results in complex intelligent behaviors; including self-organization.
In order to understand fully the behavior of such swarms it is necessary to construct a model that explains the behavior of what we will call generic worms. This model, which extends the work by Weaver [5] is presented here in section 2.
In addition, we intend to extend said model in such a way that it clearly explains the behaviors of this new class of potentially dangerous worms called Swarm Worms. Swarm Worms behave very much like biological swarms and exhibit a high degree of learning, communication, and distributed intelligence. Such Swarm Worms are potentially more harmful than their similar generic counterparts. Specifically, the first instance, to our knowledge, of such a learning worm was created, called ZachiK. ZachiK is a simple password cracking swarm worm that incorporates different learning and information sharing strategies. Such a swarm worm was deployed in both a local area network of thirty-(30) hosts, as well as simulated in a 10,000 node topology. Preliminary results showed that such worms are capable of compromising hosts at rates up to two orders of magnitude faster than their generic counterpart. The rest of this manuscript is structure as follows. In section 2 an abstract model of both generic worms as well as swarm worms is presented. This model is used in section 2.6 to described the first instance of a swarm worm, ZachiK. In section 4, preliminary results via both empirical measurements as well as simulation is presented. Finally, in section 5 our conclusions and insights into future work are presented.
In order to study the behavior of swarm worms in general, it is necessary to create a model that realistically reflects the structure of worms and it is not necessarily tied to a specific instance. In this section, we described such a model where a general worm is describe as having four-(4) basic components or subfunctions. By definition, a worm is a selfcontained, self propagating program. Thus, in simple terms, it has two main functions: that which propagates and that which does other things. We propose that there is a third broad functionality of a worm, that of self-preservation. We also propose that the other functionality of a worm may be more appropriately categorized as Goal-Based Actions (GBA), as whatever functionality included in a worm will naturally be dependent on whatever goals (and subgoals) the author has.
The work presented by Weaver et. al. in [5] provides us with mainly an action and technique based taxonomy of computer worms, which we utilize and further extend here.
The propagation function itself may be broken down into three actions: acquire target, send scan, and infect target.
Acquiring the target simply means picking a host to attack next. Sending a scan involves checking to see if that host is receptive to an infection attempt, since IP-space is sparsely populated. This may involve a simple ping to check if the host is alive or a full out vulnerability assessment. Infecting the target is the actual method used to send the worm code to the new host. In algorithm form: propagate() { host := acquire_target() success := send_scan(host) if( success ) then infect(host) endif } In the case of a simple worm which does not first check to see if the host is available or susceptible (such as Slammer), the scan method is dropped: propagate() { host := acquire_target() infect(host) } Each of these actions may have an associated cost to its inclusion and execution, such as increased worm size and CPU or network load. Depending on the authors needs or requirements, these become limiting factors in what may be included in the worm"s actions. This is discussed further after expanding upon these actions below.
The Target Acquisition phase of our worm algorithm is built directly off of the Target Discovery section in [5]. Weaver et. al. taxonomize this task into 5 separate categories. Here, we further extend their work through parameterization.
Scanning: Scanning may be considered an equation-based method for choosing a host. Any type of equation may be used to arrive at an IP address, but there are three main types seen thus far: sequential, random, and local preference. Sequential scanning is exactly as it sounds: start at an IP address and increment through all the IP space. This could carry with it the options of which IP to start with (user chosen value, random, or based on IP of infected host) and 324 how many times to increment (continuous, chosen value, or subnet-based). Random scanning is completely at random (depending on the chosen PRNG method and its seed value).
Local preference scanning is a variance of either Sequential or Random, whereby it has a greater probability of choosing a local IP address over a remote one (for example, the traditional 80/20 split).
Pre-generated Target Lists: Pre-generated Target Lists, or so called hit-lists, could include the options for percentage of total population and percentage wrong, or just number of IPs to include. Implicit to this type is the fact that the list is divided among a parent and its children, avoiding the problem of every instance hitting the exact same machines.
Externally Generated Target Lists: Externally generated target lists depend on one or more external sources that can be queried for host data. This will involve either servers that are normally publicly available, such as gaming meta-servers, or ones explicitly setup by the worm or worm author. The normally available meta-servers could have parameters for rates of change, such as many popping up at night or leaving in the morning. Each server could also have a maximum queries/second that it would be able to handle.
The worm would also need a way of finding these servers, either hard-coded or through scanning.
Internal Target Lists: Internal Target Lists are highly dependent on the infected host. This method could parameterize the choice of how much info is on the host, such as all machines in subnet, all windows boxes in subnet, particular servers, number of internal/external, or some combination.
Passive: Passive methods are determined by normal interactions between hosts. Parameters may include a rate of interaction with particular machines, internal/external rate of interaction, or subnet-based rate of interaction.
Any of these methods may also be combined to produce different types of target acquisition strategies. For example, the a worm may begin with an initial hit-list of 100 different hosts or subnets. Once it has exhausted its search using the hit-list, it may then proceed to perform random scanning with a 50% local bias.
It is important to note, however, that the resource consumption of each method is not the same. Different methods may require the worm to be large, such as the extra bytes required by a hit-list, or to take more processing time, such as by searching the host for addresses of other vulnerable hosts.
Further research and analysis should be performed in this area to determine associated costs for using each method.
The costs could then be used in determining design tradeoffs that worm authors engage at. For example, hit lists provide a high rate of infection, but at a high cost of worm payload size.
The send scan function tests to see if the host is available for infection. This can be as simple as checking if the host is up on the network or as complex as checking if the host is vulnerable to the exploit which will be used. The sending of a scan before attempted infection can increase‘ the scanning rate if the cost for failing an infection is greater than the cost of failing a scan or sending a scan plus infection; and failures are more frequent than successes. One important parameter to this would be the choice of transport protocol (TCP/UDP) or just simply the time for one successful scan and time for one failed scan. Also, whether or not it tests for the host to be up or if it is a full test for the vulnerability (or for multiple vulnerabilities).
The particular infection vector used to access the remote host is mainly dependent on the particular vulnerability chosen to exploit. In a non-specific sense, it is dependent on the transport protocol chosen to use and the message size to be sent. Section 3 of [5] also proposes three particular classes of IV: Self-carried, second channel, and embedded.
The Self Preservation actions of a worm may take many forms. In the wild, worms have been observed to disable anti-virus software or prevent sending itself to certain antivirusknown addresses. They have also been seen to attempt disabling of other worms which may be contending for the same system. We also believe that a time-based throttled scanning may help the worm to slip under the radar. We also propose a decoy method, whereby a worm will release a few children that cause a lot of noise so that the parent is not noticed. It has also been proposed [5] that a worm cause damage to its host if, and only if, it is disturbed in some way. This module could contain parameters for: probability of success in disabling anti-virus or other software updates, probability of being noticed and thus removed, or hardening of the host against other worms.
A worm"s GBA functionality depends on the author"s goal list. The Payloads section of [5] provides some useful suggestions for such a module. The opening of a back-door can make the host susceptible to more attacks. This would involve a probability of the back-door being used and any associated traffic utilization. It could also provide a list of other worms this host is now susceptible to or a list of vulnerabilities this host now has. Spam relays and HTTP-Proxies of course have an associated bandwidth consumption or traffic pattern. Internet DoS attacks would have a set time of activation, a target, and a traffic pattern. Data damage would have an associated probability that the host dies because of the damage.
In Figure 1, this general model of a worm is summarized.
Please note that in this model there is no learning, no, or very little, sharing of information between worm instances, and certainly no coordination of actions. In the next section we expand the model to include such mechanisms, and hence, arrive at the general model of a swarm worm.
As described in section 1, the basic characteristics that distinguished swarm behavior from simply what appears to be collective coordinated behaviors are four basic attributes.
These are:
325 Structure Function/Example Infection, Infection Vector Executable is run Protection & Stealthiness Disable McAfee (Staying Alive) Propagation Send email to everyone in address book Goal Based Action (GBA) DDoS www.sco.com Everything else, often called payload Figure 1: General Worm Model In this work we aggregate all of these attributes under the general title of Learning, Communication, and Distributed Control. The presence of these attributes distinguishes swarm worms from otherwise regular worms, or other types of malware such as Zombies. In figure ??, the generic model of a worm is expanded to included these set of actions.
Within this context then, a worm like Slammer cannot be categorized as a swarm worm due to the fact that new instances of the worm do not coordinate their actions or share information. On the other hand, Zombies and many other forms of DDoS, which at first glance may be consider swarm worms are not. This is simply due to fact that in the case of Zombies, control is not distributed but rather centralized, and no emergent behaviors arise. The latter, the potential emergence of intelligence or new behaviors is what makes swarm worms so potentially dangerous. Finally, when one considers the majority of recent disruptions to the Internet Infrastructure, and in light of our description of swarm attacks, then said disruptions can be easily categorized as precursors to truly swarm behavior. Specifically, • DDOS - Large number of compromised hosts send useless packets requiring processing (Stacheldraht, http : //www.cert.org/ incidentnotes/IN − 99 − 04.html).
DDoS attacks are the early precursors to Swarm Attacks due to the large number of agents involved. • Code Red CrV1, Code Red II, Nimbda - Exhibit early notions of swarm attacks, including a backdoor communication channel. • Staniford & Paxson in How to Own the Internet in Your Spare Time? explore modifications to CrV1,
Code Red I, II with a swarm like type of behavior. For example, they speculate on new worms which employ direct worm-to-worm communication, and employ programmable updates. For example the Warhol worm, and Permutation-Scanning (self coordinating) worms.
In considering the creation of what we believed to be the first Swarm Worm in existence, we wanted to adhere as close as possible to the general model presented in section ?? while at the same time facilitating large scale analysis, both empirical and through simulations, of the behavior of the swarm. For this reason, we selected as the first instance Structure Function/Example Infection, Infection Vector Executable is run Protection & Stealthiness Disable McAfee (Staying Alive) Propagation Send email to everyone in address book Learning, Communication, Pheromones/Flags (Test and Distributed Control if Worm is already present) Time bombs, Learning Algorithms, IRC channel Goal Based Action (GBA) DDoS www.sco.com Everything else, often called payload Figure 2: General Model of a Swarm Worm of the swarm a simple password cracking worm. The objective of this worm simply is to infect a host by sequentially attempting to login into the host using well known passwords (dictionary attack), passwords that have been discovered previously by any member of the swarm, and random passwords. Once, a host is infected, the worm will create communication channels with both its known neighbors at that time, as well as with any offsprings that it successfully generates. In this context a successful generation of an offspring means simply infecting a new host and replicating an exact copy of itself in such a host. We call this swarm worm the ZachiK worm in honor of one of its creators. As it can be seen from this description, the ZachiK worm exhibits all of the elements described before. In the following sections, we described in detail each one of the elements of the ZachiK worm.
The infection vector used for ZachiK worm is the secure shell protocol SSH. A modified client which is capable of receiving passwords from the command line was written, and integrated with a script that supplies it with various passwords: known and random. When a password is found for an appropriate target, the infection process begins. After the root password of a host is discovered, the worm infects the target host and replicates itself. The worm creates a new directory in the target host, copies the modified ssh client, the script, the communications servers, and the updated versions of data files (list of known passwords and a list of current neighbors). It then runs the modified script on the newly infected hosts, which spawns the communications server, notifies the neighbors and starts looking for new targets.
It could be argued, correctly, that the ZachiK worm can be easily defeated by current countermeasure techniques present on most systems today, such as disallowing direct root logins from the network. Within this context ZachiK can quickly be discarded as very simple and harmless worm that does not require further study. However, the reader should consider the following:
infection vectors. For example, it could be programmed to guess common user names and their passwords; gain 326 access to a system, then guess the root password or use other well know vulnerabilities to gain root privileges;
of ZachiK is that it incorporates all of the behaviors of a swarm worm including, but not restricted to, distributed control, communication amongst agents, and learning;
operating independently which lends itself naturally to parallel algorithms such as a parallel search of the IPV4 address space. Within this context, SLAMMER, does incorporate a parallel search capability of potentially susceptible addresses. However, unlike ZachiK, the knowledge discovered by the search is never shared amongst the agents.
For this reasons, and many others, one should not discard the potential of this new class of worms but rather embrace its study.
In the case of ZachiK worm, the main self-preservation techniques used are simply keeping the payload small. In this context, this simply means restricting the number of passwords that an offspring inherits, masquerading worm messages as common http requests, and restricting the number of neighbors to a maximum of five-(5).
Choosing the next target(s) in an efficient matter requires thought. In the past, known and proposed worms, see [5], have applied propagation techniques that varied. These include: strictly random selection of a potential vulnerable host; target lists of vulnerable hosts; locally biased random selection (select a host target at random from a local subnet); and a combination of some or all of the above. In our test and simulation environments, we will apply a combination of locally biased and totally random selection of potential vulnerable hosts. However, due to the fact that the ZachiK worm is a swarm worm, address discovery (that is when non-existent addresses are discovered) information will be shared amongst members of the swarm.
The infection and propagation threads do the following set of activities repeatedly: • Choose an address • Check the validity of the address • Choose a set of passwords • Try infecting the selected host with this set of passwords As described earlier, choosing an address makes use of a combination of random selection, local bias, and target lists.
Specifically, to choose an address, the instance may either: • Generate a new random address • Generate an address on the local network • Pick an address from a handoff list The choice is made randomly among these options, and can be varied to test the dependency of propagation on particular choices. Password are either chosen from the list of known passwords or newly generated. When an infection of a valid address fails, it is added to a list of handoffs, which is sent to the neighbors to try to work on.
Control
The concept of a swarm is based on transfer of information amongst neighbors, which relay their new incoming messages to their neighbors, and so on until every worm instance in the swarm is aware of these messages. There are two classes of messages: data or information messages and commands. The command messages are meant for an external user (a.k.a., hackers and/or crackers) to control the actions of the instances, and are currently not implemented.
The information messages are currently of three kinds: new member, passwords and exploitable addresses (handoffs).
The new member messages are messages that a new instance sends to the neighbors on its (short) list of initial neighbors.
The neighbors then register these instances in their neighbor list. These are messages that form the multi-connectivity of the swarm, and without them, the topology will be a treelike structure, where eliminating a single node would cause the instances beneath it to be inaccessible. The passwords messages inform instances of newly discovered passwords, and by informing all instances, the swarm as whole collects this information, which allows it to infect new instances more effectively. The handoffs messages inform instances of valid addresses that could not be compromised (fail at breaking the password for the root account). Since the address space is rather sparse, it takes a relatively long time (i.e. many trials) to discover a valid address. Therefore, by handing off discovered valid addresses, the swarm is (a) conserving energy by not re-discovering the same addresses (b) attacking more effectively. In a way, this is a simple instance of coordinated activity of a swarm.
When a worm instance is born, it relays its existence to all neighbors on its list. The main thread then spawns a few infection threads, and continues to handle incoming messages (registering neighbors, adding new passwords, receiving addresses and relaying these messages).
Control in the ZachiK worm is distributed in the sense that each instance of the worm performs a set of actions independently of every other instance while at the same time benefiting from the learning achieve by its immediate neighbors.
The first instantiation of the ZachiK worm has two basic goals. These are: (1) propagate, and (2) discover and share with members of th swarm new root passwords.
In order to verify our hypothesis that Swarm Worms are more capable, and therefore dangerous than other well known 327 worms, a network testbed was created, and a simulator, capable of simulating large scale Internet-like topologies (IPV4 space), was developed. The network testbed consisted of a local area network of 30 Linux based computers.
The simulator was written in C++ . The simple swarm worm described in section 2.6 was used to infect patient-zero, and then the swarm worm was allowed to propagate via its own mechanisms of propagation, distributed control, and swarm behaviors.
In the case of a simple local area network of 30 computers, six-(6) different root passwords out of a password space of 4 digits (10000 options) were selected. At the start of the experiment a single known password is known, that of patient-zero. All shared passwords are distributed randomly across all nodes. Similarly, in the case of the simulation, a network topology of 10,000 hosts, whose addresses were selected randomly across the IPV4 space, was constructed.
Within that space, a total of 200 shared passwords were selected and distributed either randomly and/or targeted to specific network topologies subnets. For example, in one of our simulation runs, the network topology consisted of 200 subnets each containing 50 hosts. In such a topology, shared passwords were distributed across subnets where a varying percentage of passwords were shared across subnets. The percentages of shared passwords used was reflective of early empirical studies, where up to 39.7% of common passwords were found to be shared.
In Figure 3, the results comparing Swarm Attack behavior versus that of a typical Malform Worm for a 30 node LAN are presented. In this set of empirical runs, six-(6) shared passwords were distributed at random across all nodes from a possible of 10,000 unknown passwords. The data presented reflects the behaviors of a total of three-(3) distinct classes of worm or swarm worms. The class of worms presented are as follows: • I-NS-NL:= Generic worm, independent (I), no learning/memoryless (NL), and no sharing of information with neighbors or offsprings (NS); • S-L-SP:= Swarm Worm (S), learning (L), keeps list of learned passwords, and sharing of passwords (SP) across nearest neighbors and offsprings; and • S-L-SP&A:= Swarm Worm (S), learning (L), keeps list of learned passwords, and sharing of passwords and existent addresses (SP&A) across nearest neighbors and offsprings.
As it is shown in Figure 3, the results validate our original hypothesis that swarm worms are significantly more efficient and dangerous than generic worms. In this set of experiments, the sharing of passwords provides an order of magnitude improvement over a memoryless random worm.
Similarly, a swarm worm that shares passwords and addresses is approximately two orders of magnitude more efficient than its generic counterpart.
In Figure 3, a series of discontinuities can be observed.
These discontinuities are an artifact of the small sample space used for this experiment. Basically, as soon as a password is broken, all nodes sharing that specific password are infected within a few seconds. Note that it is trivial for a swarm worm to scan and discovered a small shared password space.
In Figure 4, the simulation results comparing Swarm Attack Behavior versus that of a Generic Malform Worm are presented. In this set of simulation runs, a network topology of 10,000 hosts, whose addresses were selected randomly across the IPV4 space, was constructed. Within that space, a total of 200 shared passwords were selected and distributed either randomly and/or targeted to specific network topologies subnets. The data presented reflects the behaviors of three-(3) distinct classes of worm or swarm worms and two(2) different target host selection scanning strategies (random scanning and local bias). The amount of local bias was varied across multiple simulation runs. The results presented are aggregate behaviors. In general the following class of Generic Worms and Swarm Worms were simulated.
Address Scanning: • Random:= addresses are selected at random from a subset of the IPV4 space, namely, a 224 address space; and • Local Bias:= addresses are selected at random from either a local subnet (256 addresses) or from a subset of the IPV4 space, namely, a 224 address space. The percentage of local bias is varied across multiple runs.
Learning, Communication & Distributed Control • I-NL-NS: Generic worm, independent (I), no learning/ memoryless (NL), and no sharing of information with neighbors or offsprings (NS); • I-L-OOS: Generic worm, independent (I), learning/ memoryless (L), and one time sharing of information with offsprings only (OOS); • S-L-SP:= Swarm Worm (S), learning (L), keeps list of learned passwords, and sharing of passwords (SP) across nearest neighbors and offsprings; • S-L-S&AOP:= Swarm Worm (S), learning (L), keeps list of learned passwords, and sharing of addresses with neighbors and offsprings, shares passwords one time only (at creation) with offsprings(SA&OP); • S-L-SP&A:= Swarm Worm (S), learning (L), keeps list of learned passwords, and sharing of passwords and existent addresses (SP&A) across nearest neighbors and offsprings.
As it is shown in Figure 4, the results are consistent with our set of empirical results. In addition, the following observations can be made.
ones. We varied the size of the list allowed in the sharing of addresses; the overhead associated with a long address list is detrimental to the performance of the swarm worm, as well as to its stealthiness;
susceptible host, S-L-S&AOP worm (recall, the S-L-S&AOP swarm shares passwords, one time only, with offsprings 328 at creation time) is more effective than sharing passwords in the case of the S-L-SP swarm. In this case, we can think of the swarm as launching a distributeddictionary attack: different segments of the swarm use different passwords to try to break into susceptible uninfected host. In the local bias mode, early in the life of the swarm, address-sharing is more effective than password-sharing, until most subnets are discovered.
Then the targeting of local addresses assists in discovering the susceptible hosts, while the swarm members need to waste time rediscovering passwords; and
in non-local bias mode. Basically, the shared password list across subnets has been exhausted, and the swarm reverts to simply a random discovery of password algorithm.
Figure 3: Swarm Attack Behavior vs. Malform Worm: Empirical Results, 30node LAN Figure 4: Swarm Attack Behavior vs. Malform Worm: Simulation Results
In this manuscript, we have presented an abstract model, similar in some aspects to that of Weaver [5], that helps explain the generic nature of worms. The model presented in section 2 was extended to incorporate a new class of potentially dangerous worms called Swarm Worms. Swarm Worms behave very much like biological swarms and exhibit a high degree of learning, communication, and distributed intelligence. Such Swarm Worms are potentially more harmful than their generic counterparts.
In addition, the first instance, to our knowledge, of such a learning worm was created, called ZachiK. ZachiK is a simple password cracking swarm worm that incorporates different learning and information sharing strategies. Such a swarm worm was deployed in both a local area network of thirty-(30) hosts, as well as simulated in a 10,000 node topology. Preliminary results showed that such worms is capable of compromising hosts a rates up to 2 orders of magnitude faster than its generic counterpart while retaining stealth capabilities.
This work opens up a new area of interesting problems.
Some of the most interesting and pressing problems to be consider are as follows: • Is it possible to apply some of learning concepts developed over the last ten years in the areas of swarm intelligence, agent systems, and distributed control to the design of sophisticated swarm worms in such a way that true emergent behavior takes place? • Are the current techniques being developed in the design of Intrusion Detection & CounterMeasure Systems and Survivable systems effective against this new class of worms?; and • What techniques, if any, can be developed to create defenses against swarm worms?
This work was conducted as part of a larger effort in the development of next generation Intrusion Detection & CounterMeasure Systems at WSSRL. The work is conducted under the auspices of Grant ACG-2004-06 by the Acumen Consulting Group, Inc., Marlboro, Massachusetts.

Protocol frameworks, such Cactus [5, 2], Appia [1, 16],
Ensemble [12, 17], Eva [3], SDL [8] and Neko[6, 20], are programming tools for developing modular network protocols.
They allow complex protocols to be implemented by decomposing them into several modules cooperating together.
This approach facilitates code reuse and customization of distributed protocols in order to fit the needs of different applications. Moreover, protocol modules can be plugged in to the system dynamically. All these features of protocol frameworks make them an interesting enabling technology for implementing adaptable systems [14] - an important class of applications.
Most protocol frameworks are based on events (all frameworks cited above are based on this abstraction). Events are used for asynchronous communication between different modules on the same machine. However, the use of events raises some problems [4, 13]. For instance, the composition of modules may require connectors to route events, which introduces burden for a protocol composer [4]. Protocol frameworks such as Appia and Eva extend the event-based approach with channels. However, in our opinion, this solution is not satisfactory since composition of complex protocol stacks becomes more difficult.
In this paper, we propose a new approach for building modular protocols, that is based on a service abstraction. We compare this new approach with the common, event-based approach. We show that protocol frameworks based on services have several advantages, e.g. allow for a fairly straightforward protocol composition, clear implementation, and better support of dynamic replacement of distributed protocols. To validate our claims, we have implemented SAMOA - an experimental protocol framework that is purely based on the service-based approach to module composition and implementation. The framework allowed us to compare the service- and event-based implementations of an adaptive group communication middleware.
The paper is organized as follows. Section 2 defines general notions. Section 3 presents the main characteristics of event-based frameworks, and features that are distinct for each framework. Section 4 describes our new approach, which is based on service abstraction. Section 5 discusses the advantages of a service-based protocol framework compared to an event-based protocol framework. The description of our experimental implementation is presented in Section 6.
Finally, we conclude in Section 7.
In this section, we describe notions that are common to all protocol frameworks.
Protocols and Protocol Modules. A protocol is a distributed algorithm that solves a specific problem in a distributed system, e.g. a TCP protocol solves the reliable channel problem. A protocol is implemented as a set of identical protocol modules located on different machines.
Protocol Stacks. A stack is a set of protocol modules (of different protocols) that are located on the same machine.
Note that, despite its name, a stack is not strictly layered, 691 i.e. a protocol module can interact with all other protocol modules in the same stack, not only with the protocol modules directly above and below. In the remainder of this paper, we use the terms machine and stack interchangeably.
Stack 1 S1 Q1 R1 P1 Network Figure 1: Example of a protocol stack In Figure 1, we show an example protocol stack. We represent protocol modules by capital letters indexed with a natural number, e.g. P1, Q1, R1 and S1. We write Pi to denote the protocol module of a protocol P in stack i. We use this notation throughout the paper. Modules are represented as white boxes. Arrows show module interactions.
For instance, protocol module P1 interacts with the protocol module Q1 and conversely (See Fig. 1).
Protocol Module Interactions. Below, we define the different kinds of interaction between protocol modules. • Requests are issued by protocol modules. A request by a protocol module Pi is an asynchronous call by Pi of another protocol module. • Replies are the results of a request. A single request can generate several replies. Only protocol modules belonging to the same protocol as the module that has issued the request are concerned by the corresponding replies. For example, a request by Pi generates replies that concern only protocol modules Pj. • Notifications can be used by a protocol module to inform (possibly many) protocol modules in the same stack about the occurrence of a specific event.
Notifications may also be the results of a request.
FRAMEWORK DESIGN Most existing protocol frameworks are event-based.
Examples are Cactus [5, 2], Appia [1, 16] and Ensemble [12, 17]. In this section, we define the notion of an event in protocol frameworks. We also explain how protocol modules are structured in event-based frameworks.
Events. An event is a special object for indirect communication between protocol modules in the same stack. Events may transport some information, e.g. a network message or some other data. With events, the communication is indirect, i.e. a protocol module that triggers an event is not aware of the module(s) that handle the event. Events enable one-to-many communication within a protocol stack.
Triggering an event can be done either synchronously or asynchronously. In the former case, the thread that triggers an event e is blocked until all protocol modules that handle e have terminated handling of event e. In the latter case, the thread that triggers the event is not blocked.
Protocol Modules. In event-based protocol frameworks, a protocol module consists of a set of handlers. Each handler is dedicated to handling of a specific event. Handlers of the same protocol module may share data. Handlers can be dynamically bound to events. Handlers can also be unbound dynamically. Upon triggering some event e, all handlers bound to e are executed. If no handler is bound, the behavior is usually unspecified.
Stack 1 P1 Q1 R1 S1 Network f e gg deliver send h Figure 2: Example of an event-based protocol stack In Figure 2, we show an example of an event-based stack.
Events are represented by small letters, e.g. e, f, ... The fact that a protocol module can trigger an event is represented by an arrow starting from the module. A white trapezoid inside a module box represents a handler defined by the protocol module. To mark that some handler is bound to event e, we use an arrow pointing to the handler (the label on the arrow represents the event e). For example, the protocol module P1 triggers event e and handles event f (see Fig. 2). Note that the network is represented as a special protocol module that handles the send event (to send a message to another machine) and triggers the deliver event (upon receipt of a message from another machine).
Specific Features. Some protocol frameworks have unique features. Below, we present the features that influence composition and implementation of protocol modules.
In Cactus [5, 2], the programmer can give a priority number to a handler upon binding it to an event. When an event is triggered, all handlers are executed following the order of priority. A handler h is also able to cancel the execution of an event trigger: all handlers that should be executed after h according to the priority are not executed.
Appia [1, 16] and Eva [3] introduce the notion of channels.
Channels allow to build routes of events in protocol stacks.
Each protocol module has to subscribe to one or many channels. All events are triggered by specifying a channel they belong to. When a protocol module triggers an event e specifying channel c, all handlers bound to e that are part of a protocol that subscribes to c are executed (in the order prescribed by the definition of channel c).
FRAMEWORK In this section, we describe our new approach for implementing and composing protocols that is based on services. 692 We show in Section 5 the advantages of service-based protocol frameworks over event-based protocol frameworks.
Service Interface. In our service-based framework, protocol modules in the same stack communicate through objects called service interfaces. Requests, replies and notifications are all issued to service interfaces.
Protocol Modules. A protocol module is a set of executers, listeners and interceptors.
Executers handle requests. An executer can be dynamically bound to a service interface. It can be later unbound.
A request issued to a service interface si leads to the execution of the executer bound to si. If no executer is bound to si, the request is delayed until some executer is bound to si.
Contrary to events, at most one executer at any time can be bound to a service interface on every machine.
Listeners handle replies and notifications. A listener can be dynamically bound and unbound to/from a service interface si. A notification issued to a service interface si is handled by all listeners bound to si in the local stack. A reply issued to a service interface is handled by one single listener. To ensure that one single listener handles a reply, a module Pi has to identify, each time it issues a request, the listener to handle the possible reply. If the request and the reply occur respectively, in stack i and in stack j, the service interface si on i communicates to the service interface si on j the listener that must handle the reply. If the listener that must handle the reply does not exist, the reply is delayed until the listener is created.
Stack 1 P1 Q1 R1 S1 Network t u nt Figure 3: Example of a service-based protocol stack In Figure 3, we show an example of a service-based stack.
We denote a service interface by a small letter (e.g. t, u and nt) in a hexagonal box. The fact that a module Pi can generate a request to a service interface si is represented by a dashed black arrow going from Pi to si. Similarly, a dashed white arrow going from module Pi to service interface si represents the fact that Pi can generate a reply or a notification to si. We represent executers with white boxes inside protocol modules and listeners with white boxes with a gray border. A connecting line between a service interface si and an executer e (resp. a listener l) shows that e (resp. l) is bound to si.
In Figure 3, module Q1 contains an executer bound to service interface t and a listener bound to service interface u.
Module Q1 can generate replies and notifications to service interface t and requests to service interface u. Note that the service interface nt allows to access the network.
P1 Q1 P1 Q1 T1T1 t t t Figure 4: Execution of protocol interactions with interceptors An interceptor plays a special rˆole. Similarly to executers, interceptors can be dynamically bound or unbound to a service interface. They are activated each time a request, a reply or a notification is issued to the service interface they are bound to. This is illustrated in Figure 4. In the right part of the figure, the interceptor of the protocol module T1 is represented by a rounded box. The interceptor is bound to service interface t. The left part of the figure shows that an interceptor can be seen as an executer plus a listener.
When P1 issues a request req to the service interface t, the executer-interceptor of T1 is executed. Then, module T1 may forward a request req to the service interface t, where we can have req = req 1 . When module Q1 issues a reply or a notification, a similar mechanism is used, except that this time the listener-interceptor of T1 is executed. Note that a protocol module Ti, that has an interceptor bound to a service interface, is able to modify requests, replies and notifications.
Upon requests, if several interceptors are bound to the same service interface, they are executed in the order of binding. Upon replies and notifications, the order is reversed.
PROTOCOL FRAMEWORK DESIGN We show in this section the advantages of service-based protocol frameworks over event-based protocol frameworks.
We structure our discussion in three parts. Firstly, we present how protocol interactions are modeled in each of the protocol frameworks. Then, we discuss the composition of protocol modules in each of these frameworks. Finally, we present the problem of dynamic protocol replacement and the advantages of service interfaces in order to implement it. The discussion is summarized in Table 1.
A natural model of protocol interactions (as presented in Section 2) facilitates the implementation of protocol modules. For each protocol interaction, we show how it is modeled in both frameworks. We also explain that an inadequate model may lead to problems.
Requests. In service-based frameworks, a request is generated to a service interface. Each request is handled by at most one executer, since we allow only one executer to be bound to a service interface at any time. On the other hand, in event-based frameworks, a protocol module emulates a request by triggering an event. There is no guarantee 1 The two service interfaces t in the left part of Figure 4 represent the same service interface t. The duplication is only to make the figure readable. 693 that this event is bound to only one handler, which may lead to programming errors.
Replies. When a protocol module generates a reply in a service-based framework, only the correct listener (identified at the time the corresponding request was issued) is executed. This ensures that a request issued by some protocol module Qi, leads to replies handled by protocol modules Qj (i.e. protocol modules of the same protocol).
This is not the case in event-based frameworks, as we now show. Consider protocol module Q1 in Figure 2 that triggers event g to emulate a request. Module S1 handles the request. When modules Si triggers event h to emulate a reply (remember that a reply can occur in many stacks), both modules Qi and Ri will handle the reply (they both contain a handler bound to h). This behavior is not correct: only protocol modules Qi should handle the reply. Moreover, as modules Ri are not necessarily implemented to interact with modules Qi, this behavior may lead to errors.
Solutions to solve this problem exist. However, they introduce an unnecessary burden on the protocol programmers and the stack composer. For instance, channels allow to route events to ensure that modules handle only events concerning them. However, the protocol programmer must take channels into account when implementing protocols.
Moreover, the composition of complex stacks becomes more difficult due to the fact that the composer has to create many channels to ensure that modules handle events correctly. An addition of special protocol modules (named connectors) for routing events is also not satisfactory, since it requires additional work from the composer and introduces overhead.
Notifications. Contrary to requests and replies, notifications are well modeled in event-based frameworks. The reason is that notifications correspond to the one-to-many communication scheme provided by events. In service-based frameworks, notifications are also well modeled. When a module generates a notification to a service interface si, all listeners bound to s are executed. Note that in this case, service interfaces provide the same pattern of communication as events.
Replies (and sometimes notifications) are the results of a request. Thus, there is a semantic link between them.
The composer of protocol modules must preserve this link in order to compose correct stacks. We explain now that service based frameworks provide a mechanism to preserve this link, while in event-based frameworks, the lack of such mechanism leads to error-prone composition.
In service-based frameworks, requests, replies and notifications are issued to a service interface. Thus, a service interface introduces a link between these interactions. To compose a correct stack, the composer has to bound a listener to service interface si for each module that issues a request to si. The same must be done for one executer that is part of a module that issues replies or notifications.
Applying this simple methodology ensures that every request issued to a service interface si eventually results in several replies or notifications issued to the same service interface si.
In event-based frameworks, all protocol interactions are issued through different events: there is no explicit link between an event triggered upon requests and an event triggered upon the corresponding replies. Thus, the composer of a protocol stack must know the meaning of each event in order to preserve the semantic link between replies (and notifications) and requests. Moreover, nothing prevents from binding a handler that should handle a request to an event used to issue a reply. Note that these problems can be partially solved by typing events and handlers. However, it does not prevent from errors if there are several instances of the same event type.
Note that protocol composition is clearer in the protocol frameworks that are based on services, rather than on events. The reason is that several events that are used to model different protocol interactions can be modeled by a single service interface.
Dynamic replacement of protocols consists in switching on-the-fly between protocols that solve the same problem.
Replacement of a protocol P by a new protocol newP means that a protocol module Pi is replaced by newPi in every stack i. This replacement is problematic since the local replacements (within stacks) must be synchronized in order to guarantee protocol correctness [21, 18].
Q1 Q1 R1 P1 1P 1newP 1 Repl−P1 Repl−P1 R newP1 gg h h" g" t Figure 5: Dynamic replacement of protocol P For the synchronization algorithms to work, module interactions are intercepted in order to detect a time when Pi should be replaced by newPi. (Other solutions, e.g. in [11], are more complex.) In Fig. 5, we show how this interception can be implemented in protocol frameworks that are based on services (in the left part of the figure) and events (in the right part of the figure). The two-sided arrows point to the protocol modules P1 and newP1 that are switched.
It can be seen that the approach that uses the Service Interface mechanism has advantages. The intercepting module Repl-P1 has an interceptor bound to service interface t that intercepts every request handled by modules P1 and all replies and notifications issued by P1. The code of the module P1 can therefore remain unchanged.
In event-based frameworks, the solution is to add an intermediate module Repl-P1 that intercepts the requests issued to P1 and also the replies and notifications issued by P1.
Although this ad-hoc solution may seem similar to the servicebased approach, there is an important difference. The eventbased solution requires to slightly modify the module P1 since instead of handling event g and triggering event h, P1 must now handle different events g" and h" (see Fig. 5).
We have implemented an experimental service-based protocol framework (called SAMOA) [7]. Our implementation is light-weight: it consists of approximately 1200 lines of code in Java 1.5 (with generics).
In this section, we describe the main two classes of our implementation: Service (encoding the Service Interface) and 694 service-based event-based Protocol Interaction an adequate an inadequate representation representation Protocol Composition clear and safe complex and error-prone Dynamic Replacement an integrated ad-hoc solutions mechanism Table 1: Service-based vs. event-based Protocol (encoding protocol modules). Finally, we present an example protocol stack that we have implemented to validate the service-based approach.
The Service Class. A Service object is characterized by the arguments of requests and the arguments of responses.
A response is either a reply or a notification. A special argument, called message, determines the kind of interactions modeled by the response. A message represents a piece of information sent over the network. When a protocol module issues a request, it can give a message as an argument. The message can specify the listener that must handle the reply.
When a protocol module issues a response to a service interface, a reply is issued if one of the arguments of the response is a message specifying a listener. Otherwise, a notification is issued.
Executers, listeners and interceptors are encoded as innerclasses of the Service class. This allows to provide type-safe protocol interactions. For instance, executers can only be bound to the Service object, they belong to. Thus, the parameters passed to requests (that are verified statically) always correspond to the parameters accepted by the corresponding executers.
The type of a Service object is determined by the type of the arguments of requests and responses. A Service object t is compatible with another Service object s if the type of the arguments of requests (and responses) of t is a subtype of the arguments of requests (and responses) of s. In practice, if a protocol module Pi can issue a request to a protocol UDP, then it may also issue a request to TCP (compatible with UDP) due to the subtyping relation on parameters of communicating modules.
The Protocol Class. A Protocol object consists of three sets of components, one set for each component type (a listener, an executer, and an interceptor). Protocol objects are characterized by names to retrieve them easily.
Moreover, we have added some features to bind and unbind all executers or interceptors to/from the corresponding Service objects. Protocol objects can be loaded to a stack dynamically. All these features made it easy to implement dynamic replacement of network protocols.
Protocol Stack Implementation. To validate our ideas, we have developed an Adaptive Group Communication (AGC) middleware, adopting both the service- and the event-based approaches. Fig. 6 shows the corresponding stacks of the AGC middleware. Both stacks allow the Consensus and Atomic Broadcast protocols to be dynamically updated.
The architecture of our middleware, shown in Fig. 6, builds on the group communication stack described in [15].
The UDP and RP2P modules provide respectively, unreliable and reliable point-to-point transport. The FD module implements a failure detector; we assume that it ensures the Stack 1 UDP1RP2P1 Repl CT1 1ABc.
Repl CT1 ABc.1 Network FD1 GM1 rp2p nt udp d f abcast consensus Stack 1 Repl CT1 1ABc.
Repl ABc.1 UDP1 FD1 RP2P1 CT1 Network 1GM send deliver Figure 6: Adaptive Group Communication Middleware: service-based (left) vs. event-based (right) properties of the 3S failure detector [9]. The CT module provides a distributed consensus service using the ChandraToueg algorithm [10]. The ABc. module implements atomic broadcast - a group communication primitive that delivers messages to all processes in the same order. The GM module provides a group membership service that maintains consistent membership data among group members (see [19] for details). The Repl ABc. and the Repl CT modules implement the replacement algorithms [18] for, respectively, the ABc. and the CT protocol modules. Note that each arrow in the event-based architecture represents an event. We do not name events in the figure for readability.
The left stack in Figure 6 shows the implementation of AGC with our service-based framework. The right stack shows the same implementation with an event-based framework.
Performance Evaluation. To evaluate the overhead of service interfaces, we compared performance of the serviceand event-based implementations of the AGC middleware.
The latter implementation of AGC uses the Cactus protocol framework [5, 2].
In our experiment, we compared the average latency of Atomic Broadcast (ABcast), which is defined as follows.
Consider a message m sent using ABcast. We denote by ti(m) the time between the moment of sending m and the moment of delivering m on a machine (stack) i. We define the average latency of m as the average of ti(m) for all machines (stacks) i within a group of stacks.
Performance tests have been made using a cluster of PCs running Red Hat Linux 7.2, where each PC has a Pentium III 766 MHz processor and 128MB of RAM. All PCs are interconnected by a 100 Base-TX duplex Ethernet hub. Our experiment has involved 7 machines (stacks) that ABcast messages of 4Mb under a constant load, where a load is a number of messages per second. In Figure 7, we show the results of our experiment for different loads. Latencies are shown on the vertical axis, while message loads are shown on the horizontal axis. The solid line shows the results obtained with our service-based framework. The dashed line shows the results obtained with the Cactus framework. The 695 0 500 1000 1500 2000 10 20 30 40 50 60 70 80 90 100 Averagelatency[ms] Load [msg/s] Service-Based Framework Cactus Figure 7: Comparison between our service-based framework and Cactus overhead of the service-based framework is approximately 10%. This can be explained as follows. Firstly, the servicebased framework provides a higher level abstraction, which has a small cost. Secondly, the AGC middleware was initially implemented and optimized for the event-based Cactus framework. However, it is possible to optimize the AGC middleware for the service-based framework.
In the paper, we proposed a new approach to the protocol composition that is based on the notion of Service Interface, instead of events. We believe that the service-based framework has several advantages over event-based frameworks.
It allows us to: (1) model accurately protocol interactions, (2) reduce the risk of errors during the composition phase, and (3) simply implement dynamic protocol updates. A prototype implementation allowed us to validate our ideas.
[1] The Appia project. Documentation available electronically at http://appia.di.fc.ul.pt/. [2] Nina T. Bhatti, Matti A. Hiltunen, Richard D.
Schlichting, and Wanda Chiu. Coyote: a system for constructing fine-grain configurable communication services. ACM Transactions on Computer Systems, 16(4):321-366, November 1998. [3] Francisco Vilar Brasileiro, Fab´ıola Greve, Frederic Tronel, Michel Hurfin, and Jean-Pierre Le Narzul.
Eva: An event-based framework for developing specialized communication protocols. In Proceedings of the 1st IEEE International Symposium on Network Computing and Applications (NCA "01), 2001. [4] Daniel C. B¨unzli, Sergio Mena, and Uwe Nestmann.
Protocol composition frameworks. A header-driven model. In Proceedings of the 4th IEEE International Symposium on Network Computing and Applications (NCA "05), July 2005. [5] The Cactus project. Documentation available electronically at http://www.cs.arizona.edu/ cactus/. [6] The Neko project. Documentation available electronically at http://lsrwww.epfl.ch/neko/. [7] The SAMOA project. Documentation available electronically at http://lsrwww.epfl.ch/samoa/. [8] The SDL project. Documentation available electronically at http://www.sdl-forum.org/SDL/. [9] Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. The weakest failure detector for solving consensus. Journal of the ACM, 43(4):685-722, 1996. [10] Tushar Deepak Chandra and Sam Toueg. Unreliable failure detectors for reliable distributed systems.
Journal of the ACM, 43(2):225-267, 1996. [11] Wen-Ke Chen, Matti A. Hiltunen, and Richard D.
Schlichting. Constructing adaptive software in distributed systems. In Proceedings of the 21st IEEE International Conference on Distributed Computing System (ICDCS "01), April 2001. [12] The Ensemble project. Documentation available electronically at http://www.cs.cornell.edu/Info/ Projects/Ensemble/. [13] Richard Ekwall, Sergio Mena, Stefan Pleisch, and Andr´e Schiper. Towards flexible finite-state-machine-based protocol composition. In Proceedings of the 3rd IEEE International Symposium on Network Computing and Applications (NCA "04),
August 2004. [14] Philip K. McKinley, Seyed Masoud Sadjadi, Eric P.
Kasten, and Betty H.C. Cheng. Composing adaptive software. IEEE Computer, 37(7):56-64, 2004. [15] Sergio Mena, Andr´e Schiper, and Pawel T.
Wojciechowski. A step towards a new generation of group communication systems. In Proceedings of the 4th ACM/IFIP/USENIX International Middleware Conference (Middleware "03), LNCS 2672, June 2003. [16] Hugo Miranda, Alexandre Pinto, and Lu´ıs Rodrigues.
Appia, a flexible protocol kernel supporting multiple coordinated channels. In Proceedings of the 21st IEEE International Conference on Distributed Computing Systems (ICDCS "01), April 2001. [17] Ohad Rodeh, Kenneth P. Birman, Mark Hayden,
Zhen Xiao, and Danny Dolev. The architecture and performance of security protocols in the Ensemble group communication system. Technical Report TR-98-1703, Computer Science Department, Cornell University, September 1998. [18] Olivier R¨utti, Pawel T. Wojciechowski, and Andr´e Schiper. Dynamic update of distributed agreement protocols. TR IC-2005-12, School of Computer and Communication Sciences, Ecole Polytechnique F´ed´erale de Lausanne (EPFL), March 2005. [19] Andr´e Schiper. Dynamic Group Communication.
Technical Report IC-2003-27, School of Computer and Communication Sciences, Ecole Polytechnique F´ed´erale de Lausanne (EPFL), April 2003. To appear in ACM Distributed Computing. [20] P´eter Urb´an, Xavier D´efago, and Andr´e Schiper.

A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. These concerns are exacerbated by the increased use of the Internet for mission critical business and real-time entertainment applications. A relatively minor outage can disrupt and inconvenience a large number of users.
Today these services are almost exclusively hosted in data centers.
Recent advances in server virtualization technologies [8, 14, 22] allow for the live migration of services within a local area network (LAN) environment. In the LAN environment, these technologies have proven to be a very effective tool to enable data center management in a non-disruptive fashion. Not only can it support planned maintenance events [8], but it can also be used in a more dynamic fashion to automatically balance load between the physical servers in a data center [22]. When using these technologies in a LAN environment, services execute in a virtual server, and the migration services provided by the underlying virtualization framework allows for a virtual server to be migrated from one physical server to another, without any significant downtime for the service or application. In particular, since the virtual server retains the same network address as before, any ongoing network level interactions are not disrupted. Similarly, in a LAN environment, storage requirements are normally met via either network attached storage (NAS) or via a storage area network (SAN) which is still reachable from the new physical server location to allow for continued storage access.
Unfortunately in a wide area environment (WAN), live server migration is not as easily achievable for two reasons: First, live migration requires the virtual server to maintain the same network address so that from a network connectivity viewpoint the migrated server is indistinguishable from the original. While this is fairly easily achieved in a shared LAN environment, no current mechanisms are available to efficiently achieve the same feat in a WAN environment. Second, while fairly sophisticated remote replication mechanisms have been developed in the context of disaster recovery [20, 7, 11], these mechanisms are ill suited to live data center migration, because in general the available technologies are unaware of application/service level semantics.
In this paper we outline a design for live service migration across WANs. Our design makes use of existing server virtualization technologies and propose network and storage mechanisms to facilitate migration across a WAN. The essence of our approach is cooperative, context aware migration, where a migration management system orchestrates the data center migration across all three subsystems involved, namely the server platforms, the wide area network and the disk storage system. While conceptually similar in nature to the LAN based work described above, using migration technologies across a wide area network presents unique challenges and has to our knowledge not been achieved. Our main contribution is the design of a framework that will allow the migration across a WAN of all subsystems involved with enabling data center services. We describe new mechanisms as well as extensions to existing technologies to enable this and outline the cooperative, context aware functionality needed across the different subsystems to enable this. 262
ACROSS WANS Three essential subsystems are involved with hosting services in a data center: First, the servers host the application or service logic.
Second, services are normally hosted in a data center to provide shared access through a network, either the Internet or virtual private networks (VPNs). Finally, most applications require disk storage for storing data and the amount of disk space and the frequency of access varies greatly between different services/applications.
Disruptions, failures, or in general, outages of any kind of any of these components will cause service disruption. For this reason, prior work and current practices have addressed the robustness of individual components. For example, data centers typically have multiple network connections and redundant LAN devices to ensure redundancy at the networking level. Similarly, physical servers are being designed with redundant hot-swappable components (disks, processor blades, power supplies etc). Finally, redundancy at the storage level can be provided through sophisticated data mirroring technologies.
The focus of our work, however, is on the case where such local redundancy mechanisms are not sufficient. Specifically, we are interested in providing service availability when the data center as a whole becomes unavailable, for example because of data center wide maintenance operations, or because of catastrophic events. As such, our basic approach is to migrate services between data centers across the wide are network (WAN).
By necessity, moving or migrating services from one data center to another needs to consider all three of these components.
Historically, such migration has been disruptive in nature, requiring downtime of the actual services involved, or requiring heavy weight replication techniques. In the latter case concurrently running replicas of a service can be made available thus allowing a subset of the service to be migrated or maintained without impacting the service as a whole. We argue that these existing mechanisms are inadequate to meet the needs of network-based services, including real-time services, in terms of continuous availability and operation. Instead, we advocate an approach where server, network and storage subsystems cooperate and coordinate actions, in a manner that is cognizant of the service context in order to realize seamless migration across wide area networks.
In this section we briefly describe the technical building blocks that would enable our approach. As outlined below, some of these building blocks exist, or exist in part, while in other cases we use the desire for high availability of services as the driver for the changes we are proposing.
The main enabler for our approach is the live server migration capabilities that have been developed in the context of server virtualization in recent years [5, 8]. In this approach an entire running operating system (including any active applications) executing as a virtual server is being transfered from one physical machine to another. Since the virtual server is migrated in its entirety, both application and kernel level state gets migrated, including any state associated with ongoing network connections. Assuming that network level reachability to the virtual server"s network addresses are maintained after the migration, the implication is that applications executing in the virtual server experience very little downtime (in the order of tens to hundreds of milliseconds) and ongoing network connections remain intact.
In order to maintain network level reachability, the IP address(es) associated with the virtual server has to be reachable at the physical server where the virtual server is migrated to. In a LAN environment this is achieved either by issuing an unsolicited ARP reply to establish the binding between the new MAC address and the IP address, or by relying on layer-two technologies to allow the virtual server to reuse its (old) MAC address [8].
Because of the difficulty of moving network level (i.e., IP addresses) in a routed non-LAN environment, use of live server migration as a management tool has been limited to the LAN environments [22]. However, virtual server migration across the wide area will also be an attractive tool, specifically to deal with outages, and therefore propose networking mechanisms to enable this.
If disk storage needs are being met with network attached storage (NAS), the storage becomes just another network based application and can therefore be addressed in the same way with LAN based migration [8]. Modern virtualization environments also include support for other forms of (local) storage including storage area networks (SANs) [23]. However, since we propose to use WAN server migration as a means to deal with complete data center outages, these mechanisms are inadequate for our purposes and below we propose extension to remote replication technologies which can work in concert with server migration to minimize service downtime.
From the discussion above, a key requirement for live server migration across a WAN is the ability to have the IP address(es) of the virtual server be reachable at the new data center location immediately after the migration has completed. This presents a significant challenge for a number of reasons. First, despite decades of work in this area, IP address mobility remains an unresolved problem that is typically only addressed at manual configuration time scales. The second challenge comes from the fact that current routing protocols are well known to have convergence issues which is ill suited to the time constraints imposed by live migration. Third, in today"s WAN networking environment connectivity changes are typically initiated, and controlled, by network operators or network management systems. Again this is poorly suited to WAN server migration where it is essential that the migration software, which is closely monitoring the status of the server migration process, initiate this change at the appropriate time.
Our approach to addressing the networking requirements for live WAN migration builds on the observations that not all networking changes in this approach are time critical and further that instantaneous changes are best achieved in a localized manner.
Specifically, in our solution, described in detail in Section 3, we allow the migration software to initiate the necessary networking changes as soon as the need for migration has been identified. We make use of tunneling technologies during this initial phase to preemptively establish connectivity between the data centers involved. Once server migration is complete, the migration software initiates a local change to direct traffic towards the new data center via the tunnel. Slower time scale network changes then phase out this local network connectivity change for a more optimal network wide path to the new data center.
Data availability is typically addressed by replicating business data on a local/primary storage system, to some remote location from where it can be accessed. From a business/usability point of view, such remote replication is driven by two metrics [9]. First 263 is the recovery-point-objective which is the consistent data point to which data can be restored after a disaster. Second is the recoverytime-objective which is the time it takes to recover to that consistent data point after a disaster [13].
Remote replication can be broadly classified into the following two categories: ¡ Synchronous replication: every data block written to a local storage system is replicated to the remote location before the local write operation returns. ¡ Asynchronous replication: in this case the local and remote storage systems are allowed to diverge. The amount of divergence between the local and remote copies is typically bounded by either a certain amount of data, or by a certain amount of time.
Synchronous replication is normally recommended for applications, such as financial databases, where consistency between local and remote storage systems is a high priority. However, these desirable properties come at a price. First, because every data block needs to be replicated remotely, synchronous replication systems can not benefit from any local write coalescing of data if the same data blocks are written repeatedly [16]. Second, because data have to be copied to the remote location before the write operation returns, synchronous replication has a direct performance impact on the application, since both lower throughput and increased latency of the path between the primary and the remote systems are reflected in the time it takes for the local disk write to complete.
An alternative is to use asynchronous replication. However, because the local and remote systems are allowed to diverge, asynchronous replication always involves some data loss in the event of a failure of the primary system. But, because write operations can be batched and pipelined, asynchronous replication systems can move data across the network in a much more efficient manner than synchronous replication systems.
For WAN live server migration we seek a more flexible replication system where the mode can be dictated by the migration semantics. Specifically, to support live server migration we propose a remote replication system where the initial transfer of data between the data centers is performed via asynchronous replication to benefit from the efficiency of that mode of operation. When the bulk of the data have been transfered in this manner, replication switches to synchronous replication in anticipation of the completion of the server migration step. The final server migration step triggers a simultaneous switch-over to the storage system at the new data center. In this manner, when the virtual server starts executing in the new data center, storage requirements can be locally met.
In this section we illustrate how our cooperative, context aware approach can combine the technical building blocks described in the previous section to realize live server migration across a wide area network. We demonstrate how the coordination of server virtualization and migration technologies, the storage replication subsystem and the network can achieve live migration of the entire data center across the WAN. We utilize different scenarios to demonstrate our approach. In Section 3.1 we outline how our approach can be used to achieve the safe live migration of a data center when planned maintenance events are handled. In Section 3.2 we show the use of live server migration to mitigate the effects of unplanned outages or failures.
We deal with maintenance outages in two parts. First, we consider the case where the service has no (or very limited) storage requirements. This might for example be the case with a network element such as a voice-over-IP (VoIP) gateway. Second, we deal with the more general case where the service also requires the migration of data storage to the new data center.
Without Requiring Storage to be Migrated: Without storage to be replicated, the primary components that we need to coordinate are the server migration and network mobility. Figure 1 shows the environment where the application running in a virtual server VS has to be moved from a physical server in data center A to a physical server in data center B.
Prior to the maintenance event, the coordinating migration management system (MMS) would signal to both the server management system as well as the network that a migration is imminent.
The server management system would initiate the migration of the virtual server from physical server a (¢¤£¦¥ ) to physical server b (¢¤£¦§ ). After an initial bulk state transfer as preparation for migration, the server management system will mirror any state changes between the two virtual servers.
Similarly, for the network part, based on the signal received from the MMS, the service provider edge (¢©¨ ) router will initiate a number of steps to prepare for the migration. Specifically, as shown in Figure 1(b), the migration system will cause the network to create a tunnel between ¢©¨ and ¢©¨ which will be used subsequently to transfer data destined to VS to data center B.
When the MMS determines a convenient point to quiesce the VS, another signal is sent to both the server management system and the network. For the server management system, this signal will indicate the final migration of the VS from data center A to data center B, i.e., after this the VS will become active in data center B.
For the network, this second signal enables the network data path to switchover locally at ¢©¨©¥ to the remote data center. Specifically, from this point in time, any traffic destined for the virtual server address that arrives at ¢©¨©¥ will be switched onto the tunnel to ¢©¨©§ for delivery to data center B.
Note that at this point, from a server perspective the migration is complete as the VS is now active in data center B. However, traffic is sub-optimally flowing first to ¢©¨©¥ and then across the tunnel to ¢©¨¤§ . To rectify this situation another networking step is involved. Specifically, ¢©¨©§ starts to advertise a more preferred route to reach VS, than the route currently being advertised by ¢©¨¤¥ . In this manner, as ingress PEs to the network (¢©¨¤ to ¢©¨¤ in Figure 1) receive the more preferred route, traffic will start to flow to ¢©¨©§ directly and the tunnel between ¢©¨©¥ and ¢©¨©§ can be torn down leading to the final state shown in Figure 1(c).
Requiring Storage Migration: When storage has to also be replicated, it is critical that we achieve the right balance between performance (impact on the application) and the recovery point or data loss when the switchover occurs to the remote data center. To achieve this, we allow the storage to be replicated asynchronously, prior to any initiation of the maintenance event, or, assuming the amount of data to be transfered is relatively small, asynchronous replication can be started in anticipation of a migration that is expected to happen shortly. Asynchronous replication during this initial phase allows for the application to see no performance impact. However, when the maintenance event is imminent, the MMS would signal to the replication system to switch from asynchronous replication to synchronous replication to ensure that there is no loss of data during migration. When data is being replicated synchronously, there will be a performance impact on the application. 264 Figure 1: Live server migration across a WAN This requires us to keep the exposure to the amount of time we replicate on a synchronous basis to a minimum.
When the MMS signals to the storage system the requirement to switch to synchronous replication, the storage system completes all the pending asynchronous operations and then proceeds to perform all the subsequent writes by synchronously replicating it to the remote data center. Thus, between the server migration and synchronous replication, both the application state and all the storage operations are mirrored at the two environments in the two data centers. When all the pending write operations are copied over, then as in the previous case, we quiesce the application and the network is signaled to switch traffic over to the remote data center.
From this point on, both storage and server migration operations are complete and activated in data center B. As above, the network state still needs to be updated to ensure optimal data flow directly to data center B.
Note that while we have described the live server migration process as involving the service provider for the networking part, it is possible for a data center provider to perform a similar set of functions without involving the service provider. Specifically, by creating a tunnel between the customer edge (CE) routers in the data center, and performing local switching on the appropriate CE, rather than on the PE, the data center provider can realize the same functionality.
We propose to also use cooperative, context aware migration to deal with unplanned data center outages. There are multiple considerations that go into managing data center operations to plan and overcome failures through migration. Some of these are: (1) amount of overhead under normal operation to overcome anticipated failures; (2) amount of data loss affordable (recovery point objective - RPO); (3) amount of state that has to be migrated; and (4) time available from anticipated failure to occurrence of event.
At the one extreme, one might incur the overhead of completely mirroring the application at the remote site. This has the consequence of both incurring processing and network overhead under normal operation as well as impacting application performance (latency and throughput) throughout. The other extreme is to only ensure data recovery and to start a new copy of the application at the remote site after an outage. In this case, application memory state such as ongoing sessions are lost, but data stored on disk is replicated and available in a consistent state. Neither this hot standby nor the cold standby approach described are desirable due to the overhead or the loss of application memory state.
An intermediate approach is to recover control and essential state of the application, in addition to data stored on disk, to further minimize disruptions to users. A spectrum of approaches are possible.
In a VoIP server, for instance, session-based information can be mirrored without mirroring the data flowing through each session.
More generally, this points to the need to checkpoint some application state in addition to mirroring data on disk. Checkpointing application state involves storing application state either periodically or in an application-aware manner like databases do and then copying it to the remote site. Of course, this has the consequence that the application can be restarted remotely at the checkpoint boundary only. Similarly, for storage one may use asynchronous replication with a periodic snapshot ensuring all writes are up-to-date at the remote site at the time of checkpointing. Some data loss may occur upon an unanticipated, catastrophic failure, but the recovery point may be fairly small, depending on the frequency of checkpointing application and storage state. Coordination between 265 the checkpointing of the application state and the snapshot of storage is key to successful migration while meeting the desired RPOs.
Incremental checkpointing of application and storage is key to efficiency, and we see existing techniques to achieve this [4, 3, 11].
For instance, rather than full application mirroring, a virtualized replica can be maintained as a warm standby-in dormant or hibernating state-enabling a quick switch-over to the previously checkpointed state. To make the switch-over seamless, in addition to replicating data and recovering state, network support is needed.
Specifically, on detecting the unavailability of the primary site, the secondary site is made active, and the same mechanism described in Section 3.1 is used to switch traffic over to reach the secondary site via the pre-established tunnel. Note that for simplicity of exposition we assume here that the PE that performs the local switch over is not affected by the failure. The approach can however, easily be extended to make use of a switchover at a router deeper in the network.
The amount of state and storage that has to be migrated may vary widely from application to application. There may be many situations where, in principle, the server can be stateless. For example, a SIP proxy server may not have any persistent state and the communication between the clients and the proxy server may be using UDP. In such a case, the primary activity to be performed is in the network to move the communication over to the new data center site. Little or no overhead is incurred under normal operation to enable the migration to a new data center. Failure recovery involves no data loss and we can deal with near instantaneous, catastrophic failures.
As more and more state is involved with the server, more overhead is incurred to checkpoint application state and potentially to take storage snapshots, either periodically or upon application prompting. It also means that the RPO is a function of the interval between checkpoints, when we have to deal with instantaneous failures. The more advanced information we have of an impending failure, the more effective we can be in having the state migrated over to the new data center, so that we can still have a tighter RPO when operations are resumed at the new site.
Prior work on this topic falls into several categories: virtual machine migration, storage replication and network support.
At the core of our technique is the ability of encapsulate applications within virtual machines that can be migrated without application downtimes [15]. Most virtual machine software, such as Xen [8] and VMWare [14] support live migration of VMs that involve extremely short downtimes ranging from tens of milliseconds to a second; details of Xen"s live migration techniques are discussed in [8]. As indicated earlier, these techniques assume that migration is being done on a LAN. VM migration has also been studied in the Shirako system [10] and for grid environments [17, 19].
Current virtual machine software support a suspend and resume feature that can be used to support WAN migration, but with downtimes [18, 12]. Recently live WAN migration using IP tunnels was demonstrated in [21], where an IP tunnel is set up from the source to destination server to transparently forward packets to and from the application; we advocate an alternate approach that assumes edge router support.
In the context of storage, there exist numerous commercial products that perform replication, such as IBM Extended Remote Copy,
HP Continuous Access XP, and EMC RepliStor. An excellent description of these and others, as well as a detailed taxonomy of the different approaches for replication can be found in [11]. The Ursa Minor system argues that no single fault model is optimal for all applications and proposed supporting data-type specific selections of fault models and encoding schemes for replication [1]. Recently, we proposed the notion of semantic-aware replication [13] where the system supports both synchronous and asynchronous replication concurrently and use signals from the file system to determine whether to replicate a particular write synchronously and asynchronously.
In the context of network support, our work is related to the RouterFarm approach [2], which makes use of orchestrated network changes to realize near hitless maintenance on provider edge routers. In addition to being in a different application area, our approach differs from the RouterFarm work in two regards. First, we propose to have the required network changes be triggered by functionality outside of the network (as opposed to network management functions inside the network). Second, due to the stringent timing requirements of live migration, we expect that our approach would require new router functionality (as opposed to being realizable via the existing configuration interfaces).
Finally, the recovery oriented computing (ROC) work emphasizes recovery from failures rather than failure avoidance [6]. In a similar spirit to ROC, we advocate using mechanisms from live VM migration to storage replication to support planned and unplanned outages in data centers (rather than full replication to mask such failures).
A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocated a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We sought to achieve high availability of data center services in the face of both planned and incidental outages of data center facilities. We advocated using server virtualization technologies to enable the replication and migration of server functions. We proposed new network functions to enable server migration and replication across wide area networks (such as the Internet or a geographically distributed virtual private network), and finally showed the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.
[1] M. Abd-El-Malek, W. V. Courtright II, C. Cranor, G. R.
Ganger, J. Hendricks, A. J. Klosterman, M. Mesnier,
M. Prasad, B. Salmon, R. R. Sambasivan, S. Sinnamohideen,
J. D. Strunk, E. Thereska, M. Wachs, and J. J. Wylie. Ursa minor: versatile cluster-based storage. USENIX Conference on File and Storage Technologies, December 2005. [2] Mukesh Agrawal, Susan Bailey, Albert Greenberg, Jorge Pastor, Panagiotis Sebos, Srinivasan Seshan, Kobus van der Merwe, and Jennifer Yates. Routerfarm: Towards a dynamic, manageable network edge. SIGCOMM Workshop on Internet Network Management (INM), September 2006. [3] L. Alvisi. Understanding the Message Logging Paradigm for Masking Process Crashes. PhD thesis, Cornell, January
[4] L. Alvisi and K. Marzullo. Message logging: Pessimistic, optimistic, and causal. In Proceedings of the 15th International Conference on Distributed Computing Systems, pages 229-236. IEEE Computer Society, June 1995. 266 [5] Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebar, Ian Pratt, and Andrew Warfield. Xen and the art of virtualization. In the Proceedings of the ACM Symposium on Operating Systems Principles (SOSP), October 2003. [6] A. Brown and D. A. Patterson. Embracing failure: A case for recovery-oriented computing (roc). 2001 High Performance Transaction Processing Symposium, October 2001. [7] K. Brown, J. Katcher, R. Walters, and A. Watson. Snapmirror and snaprestore: Advances in snapshot technology. Network Appliance Technical Report TR3043. www. ne t app. c om/t e c h_ l i br ar y/3043. ht ml . [8] C. Clark, K. Fraser, S. Hand, J. Hanse, E. Jul, C. Limpach,
I. Pratt, and A. Warfiel. Live migration of virtual machines.
In Proceedings of NSDI, May 2005. [9] Disaster Recovery Journal. Business continuity glossary. ht t p: //www. dr j . c om/gl os s ar y/dr j gl os s ar y. ht ml . [10] Laura Grit, David Irwin, , Aydan Yumerefendi, and Jeff Chase. Virtual machine hosting for networked clusters: Building the foundations for autonomic orchestration. In In the First International Workshop on Virtualization Technology in Distributed Computing (VTDC), November
[11] M. Ji, A. Veitch, and J. Wilkes. Seneca: Remote mirroring done write. USENIX 2003 Annual Technical Conference,
June 2003. [12] M. Kozuch and M. Satyanarayanan. Internet suspend and resume. In Proceedings of the Fourth IEEE Workshop on Mobile Computing Systems and Applications, Calicoon, NY,
June 2002. [13] Xiaotao Liu, Gal Niv, K. K. Ramakrishnan, Prashant Shenoy, and Jacobus Van der Merwe. The case for semantic aware remote replication. In Proc. 2nd International Workshop on Storage Security and Survivability (StorageSS 2006),
Alexandria, VA, October 2006. [14] Michael Nelson, Beng-Hong Lim, and Greg Hutchins. Fast Transparent Migration for Virtual Machines. In USENIX Annual Technical Conference, 2005. [15] Mendel Rosenblum and Tal Garfinkel. Virtual machine monitors: Current technology and future trends. Computer, 38(5):39-47, 2005. [16] C. Ruemmler and J. Wilkes. Unix disk access patterns.
Proceedings of Winter 1993 USENIX, Jan 1993. [17] Paul Ruth, Junghwan Rhee, Dongyan Xu, Rick Kennell, and Sebastien Goasguen. Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure. In IEEE International Conference on Autonomic Computing (ICAC), June 2006. [18] Constantine P. Sapuntzakis, Ramesh Chandra, Ben Pfaff, Jim Chow, Monica S. Lam, and Mendel Rosenblum. Optimizing the migration of virtual computers. In Proceedings of the 5th Symposium on Operating Systems Design and Implementation, December 2002. [19] A. Sundararaj, A. Gupta, and P. Dinda. Increasing Application Performance in Virtual Environments through Run-time Inference and Adaptation. In Fourteenth International Symposium on High Performance Distributed Computing (HPDC), July 2005. [20] Symantec Corporation. Veritas Volume Replicator Administrator"s Guide. ht t p: //f t p. s uppor t . ve r i t as . c om/pub/s uppor t / pr oduc t s /Vol ume _ Re pl i c at or /2%83842. pdf ,
[21] F. Travostino, P. Daspit, L. Gommans, C. Jog, C. de Laat,
J. Mambretti, I. Monga, B. van Oudenaarde, S. Raghunath, and P. Wang. Seamless live migration of virtual machines over the man/wan. Elsevier Future Generations Computer Systems, 2006. [22] T. Wood, P. Shenoy, A. Venkataramani, and M. Yousif.
Black-box and gray-box strategies for virtual machine migration. In Proceedings of the Usenix Symposium on Networked System Design and Implementation (NSDI),
Cambridge, MA, April 2007. [23] A xen way to iscsi virtualization? http://www.internetnews.com/dev-news/article.php/3669246,

The different capabilities of mobile devices, plus the varying speed, error rate and disconnection characteristics of mobile networks [1], make it difficult to predict in advance the exact execution environment of mobile applications. One solution which is receiving increasing attention in the research community is application adaptation [2-7], in which applications adjust their behaviour in response to factors such as network, processor, or memory usage.
Effective adaptation requires detailed and up to date information about both the system and the software itself. Metrics related to system wide information (e.g. processor, memory and network load) are referred to as environmental metrics [5], while metrics representing application behaviour are referred as software metrics [8]. Furthermore, the type of metrics required for performing adaptation is dependent upon the type of adaptation required. For example, service-based adaptation, in which service quality or service behaviour is modified in response to changes in the runtime environment, generally requires detailed environmental metrics but only simple software metrics [4]. On the other hand, adaptation via object mobility [6], also requires detailed software metrics [9] since object placement is dependent on the execution characteristics of the mobile objects themselves.
With the exception of MobJeX [6], existing mobile object systems such as Voyager [10], FarGo [11, 12], and JavaParty [13] do not provide automated adaptation, and therefore lack the metrics collection process required to support this process. In the case of MobJeX, although an adaptation engine has been implemented [5], preliminary testing was done using synthetic pre-scripted metrics since there is little prior work on the dynamic collection of software metrics in mobile object frameworks, and no existing means of automatically collecting them.
Consequently, the main contribution of this paper is a solution for dynamic metrics collection to support adaptation via object mobility for mobile applications. This problem is non-trivial since typical mobile object frameworks consist of multiple application and middleware components, and thus metrics collection must be performed at different locations and the results efficiently propagated to the adaptation engine. Furthermore, in some cases the location where each metric should be collected is not fixed (i.e. it could be done in several places) and thus a decision must be made based on the efficiency of the chosen solution (see section 3).
The rest of this paper is organised as follows: Section 2 describes the general structure and implementation of mobile object frameworks in order to understand the challenges related to the collection, propagation and delivery of metrics as described in section 3. Section 4 describes some initial testing and results and section 5 closes with a summary, conclusions and discussion of future work.
In general, an object-oriented application consists of objects collaborating to provide the functionality required by a given problem domain. Mobile object frameworks allow some of these objects to be tagged as mobile objects, providing middleware support for such objects to be moved at runtime to other hosts. At a minimum, a mobile object framework with at least one running mobile application consists of the following components: runtimes, mobile objects, and proxies [14], although the terminology used by individual frameworks can differ [6, 10-13].
A runtime is a container process for the management of mobile objects. For example, in FarGo [15] this component is known as a core and in most systems separate runtimes are required to allow different applications to run independently, although this is not the case with MobJeX, which can run multiple applications in a single runtime using threads. The applications themselves comprise mobile objects, which interact with each other through proxies [14]. Proxies, which have the same method interface as the object itself but add remote communication and object tracking functionality, are required for each target object that a source object communicates with. Upon migration, proxy objects move with the source object.
The Java based system MobJeX, which is used as the implementation platform for the metrics collection solution described in this paper, adds a number of additional middleware components. Firstly, a host manager (known as a service in MobJeX) provides a central point of communication by running on a known port on a per host basis, thus facilitating the enumeration or lookup of components such as runtimes or mobile objects. Secondly, MobJeX has a per-application mobile object container called a transport manager (TM). As such the host and transport managers are considered in the solution provided in the next section but could be omitted in the general case. Finally, depending on adaptation mode, MobJeX can have a centralised system controller incorporating a global adaptation engine for performing system wide optimisation.
This section discusses the design and derivation of a solution for collecting metrics in order to support the adaptation of applications via object migration. The solution, although implemented within the MobJeX framework, is for the most part discussed in generic terms, except where explicitly stated to be MobJeX specific.
The metrics of Ryan and Rossi [9] have been chosen as the basis for this solution, since they are specifically intended for mobile application adaptation as well as having been derived from a series of mathematical models and empirically validated.
Furthermore, the metrics were empirically shown to improve the application performance in a real adaptation scenario following a change in the execution environment.
It would however be beyond the scope of this paper to implement and test the full suite of metrics listed in [9], and thus in order to provide a useful non-random subset, we chose to implement the minimum set of metrics necessary to implement local and global adaptation [9] and thereby satisfy a range of real adaptation scenarios. As such the solution presented in this section is discussed primarily in terms of these metrics, although the structure of the solution is intended to support the implementation of the remaining metrics, as well as other unspecified metrics such as those related to quality and resource utilisation. This subset is listed below and categorised according to metric type. Note that some additional metrics were used for implementation purposes in order to derive core metrics or assist the evaluation, and as such are defined in context where appropriate.
- Number of Invocations (NI), the frequency of invocations on methods of a class.
- Method Execution Time (ET), the time taken to execute a method body (ms). - Method Invocation Time (IT), the time taken to invoke a method, excluding the method execution time (ms).
- Memory Usage (MU), the memory usage of a process (in bytes). - Processor Usage (PU), the percentage of the CPU load of a host. - Network Usage (NU), the network bandwidth between two hosts (in bytes/sec).
Following are brief examples of a number of these metrics in order to demonstrate their usage in an adaptation scenario. As Processor Usage (PU) on a certain host increases, the Execution Time (ET) of a given method executed on that host also increases [9], thus facilitating the decision of whether to move an object with high ET to another host with low PU. Invocation Time (IT) shows the overhead of invoking a certain method, with the invocation overhead of marshalling parameters and transmitting remote data for a remote call being orders of magnitude higher than the cost of pushing and popping data from the method call stack. In other words, remote method invocation is expensive and thus should be avoided unless the gains made by moving an object to a host with more processing power (thereby reducing ET) outweigh the higher IT of the remote call. Finally, Number of Invocations (NI) is used primarily as a weighting factor or multiplier in order to enable the adaptation engine to predict the value over time of a particular adaptation decision.
This subsection discusses how each of the metrics in the subset under investigation can be obtained in terms of either direct measurement or derivation, and where in the mobile object framework such metrics should actually be measured. Of the environmental resource metrics, Processor Usage (PU) and Network Usage (NU) both relate to an individual machine, and thus can be directly measured through the resource monitoring subsystem that is instantiated as part of the MobJeX service.
However, Memory Usage (MU), which represents the memory state of a running process rather than the memory usage of a host, should instead be collected within an individual runtime.
The measurement of Number of Invocations (NI) and Execution Time (ET) metrics can be also be performed via direct measurement, however in this case within the mobile object implementation (mobject) itself.
NI involves simply incrementing a counter value at either the start or end of a method call, depending upon the desired semantics with regard to thrown exceptions, while ET can be measured by starting a timer at the beginning of the method and stopping it at the end of the method, then retrieving the duration recorded by the timer.
In contrast, collecting Invocation Time (IT) is not as straight forward because the time taken to invoke a method can only be measured after the method finishes its execution and returns to the caller. In order to collect IT metrics, another additional metric is needed. Ryan and Rossi [9] define the metric Response Time (RT), as the total time taken for a method call to finish, which is the sum of IT and ET. The Response Time can be measured directly using the same timer based technique used to measure ET, although at the start and end of the proxy call rather than the method implementation. Once the Response Time (RT) is known,
IT can derived by subtracting RT from ET.
Although this derivation appears simple, in practice it is complicated by the fact that the RT and ET values from which the IT is derived are by necessity measured using timer code in different locations i.e. RT measured in the proxy, ET measured in the method body of the object implementation. In addition, the proxies are by definition not part of the MobJeX containment hierarchy, since although proxies have a reference to their target object, it is not efficient for a mobile object (mobject) to have backward references to all of the many proxies which reference it (one per source object). Fortunately, this problem can be solved using the push based propagation mechanism described in section
be derived from the ET value stored there. The derived value of IT is then stored and propagated further as necessary according to the criteria of section 3.6, the structural relationship of which is shown in Figure 1.
The polling approach was identified as the most appropriate method for collecting resource utilisation metrics, such as Processor Usage (PU), Network Usage (NU) and Memory Usage (MU), since they are not part of, or related to, the direct flow of the application. To measure PU or NU, the resource monitor polls the Operating System for the current CPU or network load respectively. In the case of Memory Usage (MU), the Java Virtual Machine (JVM) [16] is polled for the current memory load. Note that in order to minimise the impact on application response time, the polling action should be done asynchronously in a separate thread. Metrics that are suitable for application initiated collection (i.e. as part of a normal method call) are software and performance related metrics, such as Number of Invocations (NI),
Execution Time (ET), and Invocation Time (IT), which are explicitly related to the normal invocation of a method, and thus can be measured directly at this time.
In the solution presented in this paper, all metrics collected in the same location are aggregated in a MetricsContainer with individual containers corresponding to functional components in the mobile object framework. The primary advantage of aggregating metrics in containers is that it allows them to be propagated easily as a cohesive unit through the components of the mobility framework so that they can be delivered to the adaptation engine, as discussed in the following subsection.
Note that this containment captures the different granularity of measurement attributes and their corresponding metrics. Consider the case of measuring memory consumption. At a coarse level of granularity this could be measured for an entire application or even a system, but could also be measured at the level of an individual object; or for an even finer level of granularity, the memory consumption during the execution of a specific method.
As an example of the level of granularity required for mobility based adaptation, the local adaptation algorithm proposed by Ryan and Rossi [9] requires metrics representing both the duration of a method execution and the overhead of a method invocation. The use of metrics containers facilitates the collection of metrics at levels of granularity ranging from a single machine down to the individual method level.
Note that some metrics containers do not contain any Metric objects, since as previously described, the sample implementation uses only a subset of the adaptation metrics from [9]. However, for the sake of consistency and to promote flexibility in terms of adding new metrics in the future, these containers are still considered in the present design for completeness and for future work.
The solution in this paper identifies two stages in the metrics collection and delivery process. Firstly, the propagation of metrics through the components of the mobility framework and secondly, the delivery of those metrics from the host manager/service (or runtime if the host manager is not present) to the adaptation engine.
Regarding propagation, in brief, it is proposed that when a lower level system component detects the arrival of a new metric update (e.g. mobile object), the metric is pushed (possibly along with other relevant metrics) to the next level component (i.e. runtime or transport manager containing the mobile object), which at some later stage, again determined by a configurable criteria (for example when there are a sufficient number of changed mobjects) will get pushed to the next level component (i.e. the host manager or the adaptation engine).
A further incentive for treating propagation separately from delivery is due to the distinction between local and global adaptation [9]. Local adaptation is performed by an engine running on the local host (for example in MobJeX this would occur within the service) and thus in this case the delivery phase would be a local inter-process call. Conversely, global adaptation is handled by a centralised adaptation engine running on a remote host and thus the delivery of metrics is via a remote call, and in the case where multiple runtimes exist without a separate host manager the delivery process would be even more expensive.
Therefore, due to the presence of network communication latency, it is important for the host manager to pass as many metrics as possible to the adaptation engine in one invocation, implying the need to gather these metrics in the host manager, through some form of push or propagation, before sending them to the adaptation engine.
Consequently, an abstract representation or model [17] of the system needs to be maintained. Such a model would contain model entities, corresponding to each of the main system components, connected in a tree like hierarchy, which precisely reflects the structure and containment hierarchy of the actual system. Attaching metrics containers to model entities allows a model entity representing a host manager to be delivered to the adaptation engine enabling it to access all metrics in that component and any of its children (i.e. runtimes, and mobile objects). Furthermore it would generally be expected that an adaptation engine or system controller would already maintain a model of the system that can not only be reused for propagation but also provides an effective means of delivering metrics information from the host manager to the adaptation engine. The relationship between model entities and metrics containers is captured in Figure 1.
This subsection proposes flexible criteria to allow each component to decide when it should propagate its metrics to the next component in line (Figure 1), in order to reduce the overhead incurred when metrics are unnecessarily propagated through the components of the mobility framework and delivered to the adaptation engine.
This paper proposes four different types of criterion that are executed at various stages of the measurement and propagation process in order to determine whether the next action should be taken or not. This approach was designed such that whenever a single criterion is not satisfied, the subsequent criteria are not tested. These four criteria are described in the following subsections.
Measure Metric Criterion - This criterion is attached to individual Metric objects to decide whether a new metric value should be measured or not. This is most useful in the case where it is expensive to measure a particular metric. Furthermore, this criterion can be used as a mechanism for limiting storage requirements and manipulation overhead in the case where metric history is maintained. Simple examples would be either time or frequency based whereas more complex criteria could be domain specific for a particular metric, or based upon information stored in the metrics history.
Notify Metrics Container Criterion - This criterion is also attached to individual Metric objects and is used to determine the circumstances under which the Metric object should notify its MetricsContainer. This is based on the assumption that there may be cases where it is desirable to measure and store a metric in the history for the analysis of temporal behaviour, but is not yet significant enough to notify the MetricsContainer for further processing.
A simple example of this criterion would be threshold based in which the newest metric value is compared with the previously stored value to determine whether the difference is significant enough to be of any interest to the MetricsContainer. A more complex criterion could involve analysis of the history to determine whether a pattern of recent changes is significant enough to warrant further processing and possible metrics delivery.
Notify Model Entity Criterion - Unlike the previous two criteria, this criterion is associated with a MetricsContainer.
Since a MetricsContainer can have multiple Metric objects, of which it has explicit domain knowledge, it is able to determine if, when, and how many of these metrics should be propagated to the ModelEntity and thus become candidates for being part of the hierarchical ModelEntity push process as described below. This decision making is facilitated by the notifications received from individual Metric objects as described above.
A simple implementation would be waiting for a certain number of updates before sending a notification to the model entity. For example, since the MobjectMetricsContainer object contains three metrics, a possible criteria would be to check if two or more of the metrics have changed. A slightly more advanced implementation can be done by giving each metric a weight to indicate how significant it is in the adaptation decision making process.
Push Criterion - The push criterion applies to all of the ModelEntites which are containers, that is the TransportManagerModelEntity,
RuntimeModelEntity and ServiceModelEntity, as well as the special case of the ProxyMetricsContainer.
The purpose of this criterion is twofold. For the TransportManagerModelEntity this serves as a criterion to determine notification since as with the previously described criteria, a local reference is involved. For the other model entities, this serves as an opportunity to determine both when and what metrics should be pushed to the parent container wherein the case of the ServiceModelEntity the parent is the adaptation engine itself or in the case of the ProxyMetricsContainer the target of the push is the MobjectMetricsContainer.
Furthermore, this criterion is evaluated using information from two sources. Firstly, it responds to the notification received from its own MetricsContainer but more importantly it serves to keep track of notifications from its child ModelEntities so as to determine when and what metrics information should be pushed to its parent or target. In the specialised case of the push criterion for the proxy, the decision making is based on both the ProxyMetricsContainer itself, as well as the information accumulated from the individual ProxyMethodMetricsContainers. Note that a push criterion is not required for a mobject since it does not have any containment or aggregating responsibilities since this is already Service Model Entity Service Metrics Container Notify Model Entity Criterion Runtime Model Entity Runtime Metrics Container Notify Model Entity Criterion Transport Manager Model Entity Transport Manager Metrics Container Notify Model Entity Criterion Push Criterion Mobject Model Entity Mobject Method Metrics Notify Model Entity Criterion Push Criterion Push Criterion To adaptation engine Mobject Metrics Container Notify Metrics Container Criterion Measure Metric Criterion Metric 1 NotifyMetrics Container Criterion Notify Metrics Container Criterion Measure Metric CriterionProxyMethod Metrics Containers RT Metric Notify Metrics Container Criterion ProxyMetrics Container Push Criterion Measure Metric Criterion Metric 2 Measure Metric Criterion Metric 1
not currently implemented Notify Metrics Container Criterion Metric 1 Metric 2 Measure Metric Criterion Measure Metric Criterion Notify Metrics Container Criterion MU Metric Measure Metric Criterion Notify Metrics Container Criterion ET Metric IT Metric NI Metric Measure Metric Criterion Measure Metric Criterion Measure Metric Criterion Notify Metrics Container Criterion NU Metric PU Metric Measure Metric Criterion Measure Metric Criterion
Figure 1. Structural overview of the hierarchical and criteriabased notification relationships between Metrics, Metrics Containers, and Model Entities handled by the MobjectMetricsContainer and its individual MobjectMethodMetricsContainers.
Although it is always important to reduce the number of pushes, this is especially so from a service to a centralised global adaptation engine, or from a proxy to a mobject. This is because these relationships involve a remote call [18] which is expensive due to connection setup and data marshalling and unmarshalling overhead, and thus it is more efficient to send a given amount of data in aggregate form rather than sending smaller chunks multiple times.
A simple implementation for reducing the number of pushes can be done using the concept of a process period [19] in which case the model entity accumulates pushes from its child entities until the process period expires at which time it pushes the accumulated metrics to its parent. Alternatively it could be based on frequency using domain knowledge about the type of children for example when a significant number of mobjects in a particular application (i.e. TransportManager) have undergone substantial changes.
For reducing the size of pushed data, two types of pushes were considered: shallow push and deep push. With shallow push, a list of metrics containers that contain updated metrics is pushed.
In a deep push, the model entity itself is pushed, along with its metrics container and its child entities, which also have reference to metrics containers but possibly unchanged metrics. In the case of the proxy, a deep push involves pushing the ProxyMetricsContainer and all of the ProxyMethodMetricsContainers whereas a shallow push means only the ProxyMethodMetricsContainers that meet a certain criterion.
The preliminary tests presented in this section aim to analyse the performance and scalability of the solution and evaluate the impact on application execution in terms of metrics collection overhead. All tests were executed using two Pentium 4 3.0 GHz PCs with 1,024 MB of RAM, running Java 1.4.2_08. The two machines were connected to a router with a third computer acting as a file server and hosting the external adaptation engine implemented within the MobJeX system controller, thereby simulating a global adaptation scenario.
Since only a limited number of tests could be executed, this evaluation chose to measure the worst case scenario in which all metrics collection was initiated in mobjects, wherein the propagation cost is higher than for any other metrics collected in the system. In addition, since exhaustive testing of criteria is beyond the scope of this paper, two different types of criteria were used in the tests. The measure metrics criterion was chosen, since this represents the starting point of the measurement process and can control under what circumstances and how frequently metrics are measured. In addition, the push criterion was also implemented on the service, in order to provide an evaluation of controlling the frequency of metrics delivery to the adaptation engine. All other (update and push) criteria were set to always meaning that they always evaluated to true and thus a notification was posted.
Figure 2 shows the metric collection overhead in the mobject (MMCO), for different numbers of mobjects and methods when all criteria are set to always to provide the maximum measurement and propagation of metrics and thus an absolute worst case performance scenario. It can be seen that the independent factors of increasing the number of mobjects and methods independently are linear. Although combining these together provides an exponential growth that is approximately n-squared, the initial results are not discouraging since delivering all of the metrics associated with 20 mobjects, each having 20 methods (which constitutes quite a large application given that mobjects typically represent coarse grained object clusters) is approximately 400ms, which could reasonably be expected to be offset with adaptation gains. Note that in contrast, the proxy metrics collection overhead (PMCO) was relatively small and constant at < 5ms, since in the absence of a proxy push criterion (this was only implemented on the service) the response time (RT) data for a single method is pushed during every invocation. 50 150 250 350 450 550 1 5 10 15 20 25 Number of Mobjects/Methods MobjectMetricsCollectionOverheadMMCO(ms) Methods Mobjects Both Figure 2. Worst case performance characteristics The next step was to determine the percentage metrics collection overhead compared with execution time in order to provide information about the execution characteristics of objects that would be suitable for adaptation using this metric collection approach. Clearly, it is not practical to measure metrics and perform adaptation on objects with short execution times that cannot benefit from remote execution on hosts with greater processing power, thereby offsetting IT overhead of remote compared with local execution as well as the cost of object migration and the metrics collection process itself.
In addition, to demonstrate the effect of using simple frequency based criteria, the MMCO results as a percentage of method execution time were plotted as a 3-dimensional graph in Figure 3 with the z-axis representing the frequency used in both the measure metrics criterion and the service to adaptation engine push criterion. This means that for a frequency value of 5 (n=5), metrics are only measured on every fifth method call, which then results in a notification through the model entity hierarchy to the service, on this same fifth invocation. Furthermore, the value of n=5 was also applied to the service push criterion so that metrics were only pushed to the adaptation engine after five such notifications, that is for example five different mobjects had updated their metrics.
These results are encouraging since even for the worst case scenario of n=1 the metric collection overhead is an acceptable 20% for a method of 1500ms duration (which is relatively short for a component or service level object in a distributed enterprise class application) with previous work on adaptation showing that such an overhead could easily be recovered by the efficiency gains made by adaptation [5]. Furthermore, the measurement time includes delivering the results synchronously via a remote call to the adaptation engine on a different host, which would normally be done asynchronously, thus further reducing the impact on method execution performance. The graph also demonstrates that even using modest criteria to reduce the metrics measurement to more realistic levels, has a rapid improvement on collection overhead at 20% for 500ms of ET. 0 1000 2000 3000 4000 5000 1 2 3 4 5 6 0 20 40 60 80 100 120 MMCO (%) ET (milliseconds) N (interval) MMCO (%) Figure 3. Performance characteristics with simple criteria
Given the challenges of developing mobile applications that run in dynamic/heterogeneous environments, and the subsequent interest in application adaptation, this paper has proposed and implemented an online metrics collection strategy to assist such adaptation using a mobile object framework and supporting middleware.
Controlled lab studies were conducted to determine worst case performance, as well as show the reduction in collection overhead when applying simple collection criteria. In addition, further testing provided an initial indication of the characteristics of application objects (based on method execution time) that would be good candidates for adaptation using the worst case implementation of the proposed metrics collection strategy.
A key feature of the solution was the specification of multiple configurable criteria to control the propagation of metrics through the system, thereby reducing collection overhead. While the potentially efficacy of this approach was tested using simple criteria, given the flexibility of the approach we believe there are many opportunities to significantly reduce collection overhead through the use of more sophisticated criteria. One such approach could be based on maintaining metrics history in order to determine the temporal behaviour of metrics and thus make more intelligent and conservative decisions regarding whether a change in a particular metric is likely to be of interest to the adaptation engine and should thus serve as a basis for notification for inclusion in the next metrics push. Furthermore, such a temporal history could also facilitate intelligent decisions regarding the collection of metrics since for example a metric that is known to be largely constant need not be frequently measured.
Future work will also involve the evaluation of a broad range of adaptation scenarios on the MobJeX framework to quantity the gains that can be made via adaptation through object mobility and thus demonstrate in practise, the efficacy of the solution described in this paper. Finally, the authors wish to explore applying the metrics collection concepts described in this paper to a more general and reusable context management system [20].
Systems. IEEE Personal Communications, 1994. 1: p. 6-17.
in ICDCS Workshops"04. 2004.
Mobile Devices. in Proceedings of IEEE International Conference on Mobile Data Management 2004. 2004.
Mobility. in Proc. of the 16th ACM Symposium on Operating Systems and Principles SOSP. 1997. Saint-Malo, France.
Local Adaptation for Distributed Mobile Applications. in Proc. of 2005 International Symposium on Distributed Objects and Applications (DOA 2005). 2005. Larnaca, Cyprus: SpringerVerlag.
Transparent and Portable Object Mobility in Java. in International Symposium on Distributed Objects and Applications (DOA 2004). 2004. Larnaca, Cyprus: SpringerVerlag.
Adaptive Distributed Applications: A Framework Overview and Experimental Results. in On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE (LNCS 2888). 2003.
metrics for distributed applications. in Ninth International Software Metrics Symposium. 2003. Sydney: IEEE.
Utilisation Metrics for Context Aware Mobile Applications. in Proceedings of International Software Metrics Symposium IEEE Metrics 2005. 2005. Como, Italy.
http://www.recursionsw.com/voyager.htm. 2005.
Dynamic Layout of Distributed Applications. 1998,
TechinonIsrael Institute of Technology. p. 163 - 173.
Distributed Applications in FarGo. in 21st Int'l Conf. Software Engineering (ICSE'99). 1999: ACM Press.
Objects in Java. Concurrency: Practice and Experience, 1997. 9(11): p. 1225-1242.
the Proxy Principle. in Proc.6th Intl. Conference on Distributed Computing Systems. 1986. Cambridge, Mass. (USA): IEEE.
Dynamic Relocation of Components in Fargo. in Proceedings of the Second International Symposium on Agent Systems and Applications and Fourth International Symposium on Mobile Agents. 2000.
Specification 2nd Edition. 1999: Addison-Wesley.
Development of Large Discrete-Event Simulation Models. in Proceedings of the 31st conference on Winter Simulation. 1999.
Phoenix, Arizona.
Invocation. IEEE Concurrency, 1998. 6(3): p. 5-7.
Application Performance Metrics. in Proceedings of the 1994 Conference of the Centre for Advanced Studies on Collaborative Research. 1994. Toronto, Canada.

Data Grids aggregate distributed resources for solving large-size dataset management problems. Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment. Certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access. The data grid infrastructure integrates data storage devices and data management services into the grid environment, which consists of scattered computing and storage resources, perhaps located in different countries/regions yet accessible to users [12].
Replicating popular content in distributed servers is widely used in practice [14, 17, 19]. Recently, large-scale, data-sharing scientific communities such as those described in [1, 5] used this technology to replicate their large datasets over several sites.
Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity. Bandwidth quality is the most important factor affecting transfers between clients and servers since download speeds are limited by the bandwidth traffic congestion in the links connecting the servers to the clients.
One way to improve download speeds is to determine the best replica locations using replica selection techniques [19]. This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet. Another way is to use co-allocation technology [17] to download data. Co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel. This can improve the performance compared to the single-server cases and alleviate the internet congestion problem [17]. Several co-allocation strategies were provided in previous work [17]. An idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block. Therefore, it is important to reduce the differences in finishing time among replica servers.
In this paper, we propose a dynamic co-allocation scheme based on co-allocation Grid data transfer architecture called RecursiveAdjustment Co-Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance [24]. Experimental results show that our approach is superior to previous methods and achieved the best overall performance. We also discuss combination cost and provide an effective scheme for reducing it.
The remainder of this paper is organized as follows. Related background review and studies are presented in Section 2 and the co-allocation architecture and related work are introduced in Section 3. In Section 4, an efficient replica selection service is proposed by us. Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6. Section 7 concludes this research paper.
The Data Grids enable the sharing, selection, and connection of a wide variety of geographically distributed computational and storage resources for solving large-scale data intensive scientific applications (e.g., high energy physics, bioinformatics applications, and astrophysical virtual observatory). The term Data Grid traditionally represents the network of distributed storage resources, from archival systems to caches and databases, which are linked using a logical name space to create global, persistent identifiers and provide uniform access mechanisms [4].
Data Grids [1, 2, 16] federate a lot of storage resources. Large collections of measured or computed data are emerging as important resources in many data intensive applications.
Replica management involves creating or removing replicas at a data grid site [19]. In other words, the role of a replica manager is to create or delete replicas, within specified storage systems. Most often, these replicas are exact copies of the original files, created only to harness certain performance benefits. A replica manager typically maintains a replica catalog containing replica site addresses and the file instances. The replica management service is responsible for managing the replication of complete and partial copies of datasets, defined as collections of files.
The replica management service is just one component in a Data Grid environment that provides support for high-performance, data-intensive applications. A replica or location is a subset of a collection that is stored on a particular physical storage system.
There may be multiple possibly overlapping subsets of a collection stored on multiple storage systems in a Data Grid.
These Grid storage systems may use a variety of underlying storage technologies and data movement protocols, which are independent of replica management.
As mentioned above, the purpose of the replica catalog is to provide mappings between logical names for files or collections and one or more copies of the objects on physical storage systems.
The replica catalog includes optional entries that describe individual logical files. Logical files are entities with globally unique names that may have one or more physical instances. The catalog may optionally contain one logical file entry in the replica catalog for each logical file in a collection.
A Data Grid may contain multiple replica catalogs. For example, a community of researchers interested in a particular research topic might maintain a replica catalog for a collection of data sets of mutual interest. It is possible to create hierarchies of replica catalogs to impose a directory-like structure on related logical collections. In addition, the replica manager can perform access control on entire catalogs as well as on individual logical files.
The purpose of replica selection [16] is to select a replica from among the sites which constitute a Data Grid [19]. The criteria of selection depend on characteristics of the application. By using this mechanism, users of the Data Grid can easily manage replicas of data sets at their sites, with better performance. Much previous effort has been devoted to the replica selection problem. The common process of replica selection consists of three steps: data preparation, preprocessing and prediction. Then, applications can select a replica according to its specific attributes. Replica selection is important to data-intensive applications, and it can provide location transparency. When a user requests for accessing a data set, the system determines an appropriate way to deliver the replica to the user.
The Globus Project [9, 11, 16] provides software tools collectively called The Globus Toolkit that makes it easier to build computational Grids and Grid-based applications. Many organizations use the Globus Toolkit to build computational Grids to support their applications. The composition of the Globus Toolkit can be pictured as three pillars: Resource Management,
Information Services, and Data Management. Each pillar represents a primary component of the Globus Toolkit and makes use of a common foundation of security. GRAM implements a resource management protocol, MDS implements an information services protocol, and GridFTP implements a data transfer protocol. They all use the GSI security protocol at the connection layer [10, 11, 16, 13]. The Globus alliance proposed a common data transfer and access protocol called GridFTP that provides secure, efficient data movement in Grid environments [3]. This protocol, which extends the standard FTP protocol, provides a superset of the features offered by the various Grid storage systems currently in use.
In order to solve the appearing problems, the Data Grid community tries to develop a secure, efficient data transport mechanism and replica management services. GridFTP is a reliable, secure and efficient data transport protocol which is developed as a part of the Globus project. There is another key technology from Globus project, called replica catalog [16] which is used to register and manage complete and partial copies of data sets. The replica catalog contains the mapping information from a logical file or collection to one or more physical files.
The Network Weather Service (NWS) [22] is a generalized and distributed monitoring system for producing short-term performance forecasts based on historical performance measurements. The goal of the system is to dynamically characterize and forecast the performance deliverable at the application level from a set of network and computational resources. A typical installation involves one nws_nameserver, one or more nws_memory (which may reside on different machines), and an nws_sensor running on each machine with resources which are to be monitored. The system includes sensors for end-to-end TCP/IP performance (bandwidth and latency), available CPU percentage, and available non-paged memory. 798
The Sysstat [15] utilities are a collection of performance monitoring tools for the Linux OS. The Sysstat package incorporates the sar, mpstat, and iostat commands. The sar command collects and reports system activity information, which can also be saved in a system activity file for future inspection. The iostat command reports CPU statistics and I/O statistics for tty devices and disks. The statistics reported by sar concern I/O transfer rates, paging activity, process-related activities, interrupts, network activity, memory and swap space utilization, CPU utilization, kernel activities, and tty statistics, among others. Uniprocessor (UP) and Symmetric multiprocessor (SMP) machines are fully supported.
AND RELATED WORK The co-allocation architecture proposed in [17] consists of three main components: an information service, a broker/co-allocator, and local storage systems. Figure 1 shows the co-allocation of Grid Data transfers, which is an extension of the basic template for resource management [7] provided by Globus Toolkit.
Applications specify the characteristics of desired data and pass the attribute description to a broker. The broker queries available resources and gets replica locations from information services [6] and replica management services [19], and then gets a list of physical locations for the desired files.
Figure 1. Data Grid Co-Allocation Architecture [17] The candidate replica locations are passed to a replica selection service [19], which was presented in a previous work [23]. This replica selection service provides estimates of candidate transfer performance based on a cost model and chooses appropriate amounts to request from the better locations. The co-allocation agent then downloads the data in parallel from the selected servers.
In these researches, GridFTP [1, 11, 16] was used to enable parallel data transfers. GridFTP is a high-performance, secure, reliable data transfer protocol optimized for high-bandwidth widearea networks. Among its many features are security, parallel streams, partial file transfers, third-party transfers, and reusable data channels. Its partial file transfer ability allows files to be retrieved from data servers by specifying the start and end offsets of file sections.
Data grids consist of scattered computing and storage resources located in different countries/regions yet accessible to users [8].
In this study we used the grid middleware Globus Toolkit [16] as the data grid infrastructure. The Globus Toolkit provides solutions for such considerations as security, resource management, data management, and information services. One of its primary components is MDS [6, 11, 16, 25], which is designed to provide a standard mechanism for discovering and publishing resource status and configuration information. It provides a uniform and flexible interface for data collected by lower-level information providers in two modes: static (e.g., OS, CPU types, and system architectures) and dynamic data (e.g., disk availability, memory availability, and loading). And it uses GridFTP [1, 11, 16], a reliable, secure, and efficient data transport protocol to provide efficient management and transfer of terabytes or petabytes of data in a wide-area, distributed-resource environment.
As datasets are replicated within Grid environments for reliability and performance, clients require the abilities to discover existing data replicas, and create and register new replicas. A Replica Location Service (RLS) [4] provides a mechanism for discovering and registering existing replicas. Several prediction metrics have been developed to help replica selection. For instance, Vazhkudai and Schopf [18, 20, 21] used past data transfer histories to estimate current data transfer throughputs.
In our previous work [23, 24], we proposed a replica selection cost model and a replica selection service to perform replica selection. In [17], the author proposes co-allocation architecture for co-allocating Grid data transfers across multiple connections by exploiting the partial copy feature of GridFTP. It also provides Brute-Force, History-Base, and Dynamic Load Balancing for allocating data block.
Brute-Force Co-Allocation: Brute-Force Co-Allocation works by dividing the file size equally among available flows. It does not address the bandwidth differences among the various client-server links.
History-based Co-Allocation: The History-based CoAllocation scheme keeps block sizes per flow proportional to predicted transfer rates.
Conservative Load Balancing: One of their dynamic coallocation is Conservative Load Balancing. The Conservative Load Balancing dynamic co-allocation strategy divides requested datasets into k disjoint blocks of equal size. Available servers are assigned single blocks to deliver in parallel. When a server finishes delivering a block, another is requested, and so on, till the entire file is downloaded. The loadings on the co-allocated flows are automatically adjusted because the faster servers will deliver more quickly providing larger portions of the file.
Aggressive Load Balancing: Another dynamic coallocation strategy, presented in [17], is the Aggressive Load Balancing. The Aggressive Load Balancing dynamic co-allocation strategy presented in [17] adds functions that change block size de-liveries by: (1) progressively increasing the amounts of data requested from faster servers, and (2) reducing the amounts of data requested from slower servers or ceasing to request data from them altogether.
The co-allocation strategies described above do not handle the shortcoming of faster servers having to wait for the slowest server to deliver its final block. In most cases, this wastes much time and decreases overall performance. Thus, we propose an efficient approach called Recursive-Adjustment Co-Allocation and based 799 on a co-allocation architecture. It improves dynamic co-allocation and reduces waiting time, thus improving overall transfer performance.
SERVICE We constructed a replica selection service to enable clients to select the better replica servers in Data Grid environments. See below for a detailed description.
Our proposed replica selection model is illustrated in [23], which shows how a client identifies the best location for a desired replica transfer. The client first logins in at a local site and executes the Data Grid platform application, which checks to see if the files are available at the local site. If they are present at the local site, the application accesses them immediately; otherwise, it passes the logical file names to the replica catalog server, which returns a list of physical locations for all registered copies. The application passes this list of replica locations to a replica selection server, which identifies the storage system destination locations for all candidate data transfer operations.
The replica selection server sends the possible destination locations to the information server, which provides performance measurements and predictions of the three system factors described below. The replica selection server chooses better replica locations according to these estimates and returns location information to the transfer application, which receives the replica through GridFTP. When the application finishes, it returns the results to the user.
Determining the best database from many with the same replications is a significant problem. In our model, we consider three system factors that affect replica selection: Network bandwidth: This is one of the most significant Data Grid factors since data files in Data Grid environments are usually very large. In other words, data file transfer times are tightly dependent on network bandwidth situations. Because network bandwidth is an unstable dynamic factor, we must measure it frequently and predict it as accurately as possible. The Network Weather Service (NWS) is a powerful toolkit for this purpose.
CPU load: Grid platforms consist of numbers of heterogeneous systems, built with different system architectures, e.g., cluster platforms, supercomputers, PCs.
CPU loading is a dynamic system factor, and a heavy system CPU load will certainly affect data file downloads process from the site. The measurement of it is done by the Globus Toolkit / MDS.
I/O state: Data Grid nodes consist of different heterogeneous storage systems. Data files in Data Grids are huge. If the I/O state of a site that we wish to download files from is very busy, it will directly affect data transfer performance. We measure I/O states using sysstat [15] utilities.
The target function of a cost model for distributed and replicated data storage is the information score from the information service.
We listed some influencing factors for our cost model in the preceding section. However, we must express these factors in mathematical notation for further analysis. We assume node i is the local site the user or application logs in on, and node j possesses the replica the user or application wants. The seven system parameters our replica selection cost model considers are: Scorei-j: the score value represents how efficiently a user or application at node i can acquire a replica from node j BW jiP : percentage of bandwidth available from node i to node j; current bandwidth divided by highest theoretical bandwidth BBW : network bandwidth weight defined by the Data Grid administrator CPU jP : percentage of node j CPU idle states WCPU : CPU load weight defined by the Data Grid administrator OI jP / : percentage of node j I/O idle states WI/O : I/O state weight defined by the Data Grid administrator We define the following general formula using these system factors.
OIOI j CPUCPU j BWBW jiji WPWPWPScore // (1) The three influencing factors in this formula: WBW , WCPU , and WI/O describe CPU, I/O, and network bandwidth weights, which can be determined by Data Grid organization administrators according to the various attributes of the storage systems in Data Grid nodes since some storage equipment does not affect CPU loading. After several experimental measurements, we determined that network bandwidth is the most significant factor directly influencing data transfer times. When we performed data transfers using the GridFTP protocol we discovered that CPU and I/O statuses slightly affect data transfer performance. Their respective values in our Data Grid environment are 80%, 10%, and 10%.
When clients download datasets using GridFTP co-allocation technology, three time costs are incurred: the time required for client authentication to the GridFTP server, actual data transmission time, and data block reassembly time.
Authentication Time: Before a transfer, the client must load a Globus proxy and authenticate itself to the GridFTP server with specified user credentials. The client then establishes a control channel, sets up transfer parameters, and requests data channel creation. When the channel has been established, the data begins flowing.
Transmission Time: Transmission time is measured from the time when the client starts transferring to the time when all transmission jobs are finished, and it includes the time 800 required for resetting data channels between transfer requests. Data pathways need be opened only once and may handle many transfers before being closed. This allows the same data pathways to be used for multiple file transfers. However, data channels must be explicitly reset between transfer requests. This is less time-costly.
Combination Time: Co-allocation architecture exploits the partial copy feature of the GridFTP data movement tool to enable data transfers across multiple connections. With partial file transfer, file sections can be retrieved from data servers by specifying only the section start and end offsets.
When these file sections are delivered, they may need to be reassembled; the reassembly operation incurs an additional time cost.
STRATEGY Dynamic co-allocation, described above, is the most efficient approach to reducing the influence of network variations between clients and servers. However, the idle time of faster servers awaiting the slowest server to deliver the last block is still a major factor affecting overall efficiency, which Conservative Load Balancing and Aggressive Load Balancing [17] cannot effectively avoid. The approach proposed in the present paper, a dynamic allocation mechanism called Recursive-Adjustment CoAllocation can overcome this, and thus, improve data transfer performance.
Recursive-Adjustment Co-Allocation works by continuously adjusting each replica server"s workload to correspond to its realtime bandwidth during file transfers. The goal is to make the expected finish time of all servers the same. As Figure 2 shows, when an appropriate file section is first selected, it is divided into proper block sizes according to the respective server bandwidths.
The co-allocator then assigns the blocks to servers for transfer. At this moment, it is expected that the transfer finish time will be consistent at E(T1). However, since server bandwidths may fluctuate during segment deliveries, actual completion time may be dissimilar (solid line, in Figure 2). Once the quickest server finishes its work at time T1, the next section is assigned to the servers again. This allows each server to finish its assigned workload by the expected time at E(T2). These adjustments are repeated until the entire file transfer is finished.
Server 1 Server 2 Server 3 Round 1 Round 2 E(T1) E(T2)T1 File A Section 1 Section 2 ... ... ...
Figure 2. The adjustment process The Recursive-Adjustment Co-Allocation process is illustrated in Figure 3. When a user requests file A, the replica selection service responds with the subset of all available servers defined by the maximum performance matrix. The co-allocation service gets this list of selected replica servers. Assuming n replica servers are selected, Si denotes server i such that 1 i n. A connection for file downloading is then built to each server. The RecursiveAdjustment Co-Allocation process is as follows. A new section of a file to be allocated is first defined. The section size, SEj, is: SEj = UnassignedFileSize , (0 < < 1) (2) where SEj denotes the section j such that 1 j k, assuming we allocate k times for the download process. And thus, there are k sections, while Tj denotes the time section j allocated.
UnassignedFileSize is the portion of file A not yet distributed for downloading; initially, UnassignedFileSize is equal to the total size of file A. is the rate that determines how much of the section remains to be assigned.
Figure 3. The Recursive-Adjustment Co-Allocation process.
In the next step, SEj is divided into several blocks and assigned to n servers. Each server has a real-time transfer rate to the client of Bi, which is measured by the Network Weather Service (NWS) [18]. The block size per flow from SEj for each server i at time Tj is: i n i ii n i iji zeUnFinishSiBBzeUnFinishSiSES -)( 11 (3) where UnFinishSizei denotes the size of unfinished transfer blocks that is assigned in previous rounds at server i.
UnFinishSizei is equal to zero in first round. Ideally, depending to the real time bandwidth at time Tj, every flow is expected to finish its workload in future.
This fulfills our requirement to minimize the time faster servers must wait for the slowest server to finish. If, in some cases, network variations greatly degrade transfer rates, UnFinishSizei may exceed n i ii n i ij BBzeUnFinishSiSE 11 *)( , which is the total block size expected to be transferred after Tj. In such cases, the co-allocator eliminates the servers in advance and assigns SEj to other servers. After allocation, all channels continue transferring data blocks. When a faster channel finishes its assigned data blocks, the co-allocator begins allocating an unassigned section of file A again. The process of allocating data 801 blocks to adjust expected flow finish time continues until the entire file has been allocated.
Adjustment Our approach gets new sections from whole files by dividing unassigned file ranges in each round of allocation. These unassigned portions of the file ranges become smaller after each allocation. Since adjustment is continuous, it would run as an endless loop if not limited by a stop condition.
However, when is it appropriate to stop continuous adjustment?
We provide two monitoring criteria, LeastSize and ExpectFinishedTime, to enable users to define stop thresholds.
When a threshold is reached, the co-allocation server stopped dividing the remainder of the file and assigns that remainder as the final section. The LeastSize criterion specifies the smallest file we want to process, and when the unassigned portion of UnassignedFileSize drops below the LeastSize specification, division stops. ExpectFinishedTime criterion specifies the remaining time transfer is expected to take. When the expected transfer time of the unassigned portion of a file drops below the time specified by ExpectFinishedTime, file division stops. The expected rest time value is determined by: 1 n i iBFileSizeUnAssigned (4) These two criteria determine the final section size allocated.
Higher threshold values will induce fewer divisions and yield lower co-allocation costs, which include establishing connections, negotiation, reassembly, etc. However, although the total coallocation adjustment time may be lower, bandwidth variations may also exert more influence. By contrast, lower threshold values will induce more frequent dynamic server workload adjustments and, in the case of greater network fluctuations, result in fewer differences in server transfer finish time. However, lower values will also increase co-allocation times, and hence, increase co-allocation costs. Therefore, the internet environment, transferred file sizes, and co-allocation costs should all be considered in determining optimum thresholds.
The process of reassembling blocks after data transfers using coallocation technology results in additional overhead and decreases overall performance. The reassembly overhead is related to total block size, and could be reduced by upgrading hardware capabilities or using better software algorithms. We propose an efficient alternative reassembly mechanism to reduce the added combination overhead after all block transmissions are finished. It differs from the conventional method in which the software starts assembly after all blocks have been delivered by starting to assemble blocks once the first deliveries finish. Of course, this makes it necessary to maintain the original splitting order.
Co-allocation strategies such as Conservative Load Balancing and Recursive-Adjustment Co-Allocation produce additional blocks during file transfers and can benefit from enabling reassembly during data transfers. If some blocks are assembled in advance, the time cost for assembling the blocks remaining after all transfers finish can be reduced.
ANALYSIS In this section, we discuss the performance of our RecursiveAdjustment Co-Allocation strategy. We evaluate four coallocation schemes: (1) Brute-Force (Brute), (2) History-based (History), (3) Conservative Load Balancing (Conservative) and (4) Recursive-Adjustment Co-Allocation (Recursive). We analyze the performance of each scheme by comparing their transfer finish time, and the total idle time faster servers spent waiting for the slowest server to finish delivering the last block. We also analyze the overall performances in the various cases.
We performed wide-area data transfer experiments using our GridFTP GUI client tool. We executed our co-allocation client tool on our testbed at Tunghai University (THU), Taichung City,
Taiwan, and fetched files from four selected replica servers: one at Providence University (PU), one at Li-Zen High School (LZ), one at Hsiuping Institute of Technology School (HIT), and one at Da-Li High School (DL). All these institutions are in Taiwan, and each is at least 10 Km from THU. Figure 4 shows our Data Grid testbed. Our servers have Globus 3.0.2 or above installed.
Internet THU Li-Zen High School (LZ) HITCeleron 900 MHz 256 MB RAM 60 GB HD AMD Athlon(tm) XP 2400+ 1024 MB RAM 120 GB HD Pentium 4 2.8 GHz 512 MB RAM 80 GB HD PU Da-Li High School (DL) Athlon MP 2000 MHz *2 1 GB RAM 60 GB HD Pentium 4 1.8 GHZ 128 MB RAM 40 GB HD Pentium 4 2.5 GHZ 512 MB RAM 80 GB HD Figure 4. Our Data Grid testbed In the following experiments, we set = 0.5, the LeastSize threshold to 10MB, and experimented with file sizes of 10 MB, 50MB, 100MB, 500MB, 1000MB, 2000MB, and 4000MB. For comparison, we measured the performance of Conservative Load Balancing on each size using the same block numbers. Figure 5 shows a snapshot of our GridFTP client tool. This client tool is developed by using Java CoG. It allows easier and more rapid application development by encouraging collaborative code reuse and avoiding duplication of effort among problem-solving environments, science portals, Grid middleware, and collaborative pilots. Table 1 shows average transmission rates between THU and each replica server. These numbers were obtained by transferring files of 500MB, 1000MB, and 2000MB from a single replica server using our GridFTP client tool, and each number is an average over several runs.
Table 1. GridFTP end-to-end transmission rate from THU to various servers Server Average transmission rate HIT 61.5 Mbps LZ 59.5 Mbps DL 32.1 Mbps PU 26.7 Mbps 802 Figure 5. Our GridFTP client tool We analyzed the effect of faster servers waiting for the slowest server to deliver the last block for each scheme. Figure 6(a) shows total idle time for various file sizes. Note that our RecursiveAdjustment Co-Allocation scheme achieved significant performance improvements over other schemes for every file size.
These results demonstrate that our approach efficiently reduces the differences in servers finish times. The experimental results shown in Figure 6(b) indicate that our scheme beginning block reassembly as soon as the first blocks have been completely delivered reduces combination time, thus aiding co-allocation strategies like Conservative Load Balancing and RecursiveAdjustment Co-Allocation that produce more blocks during data transfers.
Figure 7 shows total completion time experimental results in a detailed cost structure view. Servers were at PU, DL, and HIT, with the client at THU. The first three bars for each file size denote the time to download the entire file from single server, while the other bars show co-allocated downloads using all three servers. Our co-allocation scheme finished the job faster than the other co-allocation strategies. Thus, we may infer that the main gains our technology offers are lower transmission and combination times than other co-allocation strategies. 0 20 40 60 80 100 120 140 160 180 200 100 500 1000 1500 2000 File Size (MB) WaitTime(Sec) Brute3 History3 Conservative3 Recursive3 0 10 20 30 40 50 60 70 80 90 100 500 1000 1500 2000 File Size (MB) CombinationTime(Sec) Brute3 History3 Conservative3 Recursive3 Figure 6. (a) Idle times for various methods; servers are at PU,
DL, and HIT. (b) Combination times for various methods; servers are at PU, DL, and HIT.
In the next experiment, we used the Recursive-Adjustment CoAllocation strategy with various sets of replica servers and measured overall performances, where overall performance is: Total Performance = File size/Total Completion Time (5) Table 2 lists all experiments we performed and the sets of replica servers used. The results in Figure 8(a) show that using coallocation technologies yielded no improvement for smaller file sizes such as 10MB. They also show that in most cases, overall performance increased as the number of co-allocated flows increased. We observed that for our testbed and our co-allocation technology, overall performance reached its highest value in the REC3_2 case. However, in the REC4 case, when we added one flow to the set of replica servers, the performance did not increase.
On the contrary, it decreased. We can infer that the co-allocation efficiency reached saturation in the REC3_2 case, and that additional flows caused additional overhead and reduced overall performance. This means that more download flows do not necessarily result in higher performance. We must choose appropriate numbers of flows to achieve optimum performance.
We show the detailed cost structure view for the case of REC3_2 and the case of REC4 in Figure 8(b). The detailed cost consists of authentication time, transfer time and combination time. 0 100 200 300 400 500 600 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 500 1000 1500 2000 File Size (MB) CompletionTime(Sec) Authentication Time Transmission Time Combination Time Figure 7. Completion times for various methods; servers are at PU, DL, and HIT.
Table 2. The sets of replica servers for all cases Case Servers PU1 PU DL1 DL REC2 PU, DL REC3_1 PU, DL, LZ REC3_2 PU, DL, HIT REC4 PU, DL, HIT, LZ 0 10 20 30 40 50 60 70 10 50 100 500 1000 1500 2000 File Size (MB) OverallPerformance(Mbits) PU1 DL1 REC2 REC3_1 REC3_2 REC4 0 10 20 30 40 50 60 70 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 10 50 100 500 1000 1500 2000 File Size (MB) OverallPerformance(Mbits) Authentication Time Transmission Time Combination Time Figure 8. (a) Overall performances for various sets of servers. (b) Detailed cost structure view for the case of REC3_2 and the case of REC4.
The co-allocation architecture provides a coordinated agent for assigning data blocks. A previous work showed that the dynamic co-allocation scheme leads to performance improvements.
However, it cannot handle the idle time of faster servers, which must wait for the slowest server to deliver its final block. We proposed the Recursive-Adjustment Co-Allocation scheme to improve data transfer performances using the co-allocation architecture in [17]. In this approach, the workloads of selected replica servers are continuously adjusted during data transfers, and we provide a function that enables users to define a final 803 block threshold, according to their data grid environment.
Experimental results show the effectiveness of our proposed technique in improving transfer time and reducing overall idle time spent waiting for the slowest server. We also discussed the re-combination cost and provided an effective scheme for reducing it.
[1] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster,
C. Kesselman, S. Meder, V. Nefedova, D. Quesnel, and S.
Tuecke, Data Management and Transfer in HighPerformance Computational Grid Environments, Parallel Computing, 28(5):749-771, May 2002. [2] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster,
C. Kesselman, S. Meder, V. Nefedova, D. Quesnel, and S.
Tuecke, Secure, Efficient Data Transport and Replica Management for High-Performance Data-Intensive Computing, Proc. of the Eighteenth IEEE Symposium on Mass Storage Systems and Technologies, pp. 13-28, 2001. [3] B. Allcock, S. Tuecke, I. Foster, A. Chervenak, and C.
Kesselman. Protocols and Services for Distributed DataIntensive Science. ACAT2000 Proceedings, pp. 161-163,
[4] A. Chervenak, E. Deelman, I. Foster, L. Guy, W. Hoschek,
A. Iamnitchi, C. Kesselman, P. Kunszt, and M. Ripeanu,
Giggle: A Framework for Constructing Scalable Replica Location Services, Proc. of SC 2002, Baltimore, MD, 2002. [5] A. Chervenak, I. Foster, C. Kesselman, C. Salisbury, and S.
Tuecke, The Data Grid: Towards an Architecture for the Distributed Management and Analysis of Large Scientific Datasets, Journal of Network and Computer Applications, 23:187-200, 2001. [6] K. Czajkowski, S. Fitzgerald, I. Foster, and C. Kesselman,
Grid Information Services for Distributed Resource Sharing, Proc. of the Tenth IEEE International Symposium on High-Performance Distributed Computing (HPDC-10"01), 181-194, August 2001. [7] K. Czajkowski, I. Foster, and C. Kesselman. Resource CoAllocation in Computational Grids, Proc. of the Eighth IEEE International Symposium on High Performance Distributed Computing (HPDC-8"99), August 1999. [8] F. Donno, L. Gaido, A. Ghiselli, F. Prelz, and M. Sgaravatto,
DataGrid Prototype 1, TERENA Networking Conference,  http://www.terena.nl/conferences/tnc2002/Papers/p5a2ghiselli.pdf, June 2002, [9] I. Foster, C. Kesselman, and S. Tuecke. The Anatomy of the Grid: Enabling Scalable Virtual Organizations. Int. J. of Supercomputer Applications and High Performance Computing, 15(3), pp. 200-222, 2001. [10] I. Foster and C. Kesselman, Globus: A Metacomputing Infrastructure Toolkit, Intl J. Supercomputer Applications, 11(2), pp. 115-128, 1997. [11] Global Grid Forum, http://www.ggf.org/ [12] W. Hoschek, J. Jaen-Martinez, A. Samar, H. Stockinger, and K. Stockinger, Data Management in an International Data Grid Project, Proc. of First IEEE/ACM International Workshop on Grid Computing - Grid 2000, Bangalore, India,
December 2000. [13] IBM Red Books, Introduction to Grid Computing with Globus, IBM Press, www.redbooks.ibm.com/redbooks/pdfs/sg246895.pdf [14] H. Stockinger, A. Samar, B. Allcock, I. Foster, K. Holtman, and B. Tierney, File and Object Replication in Data Grids,
Journal of Cluster Computing, 5(3):305-314, 2002. [15] SYSSTAT utilities home page, http://perso.wanadoo.fr/sebastien.godard/ [16] The Globus Alliance, http://www.globus.org/ [17] S. Vazhkudai, Enabling the Co-Allocation of Grid Data Transfers, Proc. of Fourth International Workshop on Grid Computing, pp. 41-51, November 2003. [18] S. Vazhkudai and J. Schopf, Using Regression Techniques to Predict Large Data Transfers, International Journal of High Performance Computing Applications (IJHPCA), 17:249-268, August 2003. [19] S. Vazhkudai, S. Tuecke, and I. Foster, Replica Selection in the Globus Data Grid, Proc. of the 1st International Symposium on Cluster Computing and the Grid (CCGRID 2001), pp. 106-113, May 2001. [20] S. Vazhkudai, J. Schopf, Predicting Sporadic Grid Data Transfers, Proc. of 11th IEEE International Symposium on High Performance Distributed Computing (HPDC-11 ‘02), pp. 188-196, July 2002. [21] S. Vazhkudai, J. Schopf, and I. Foster, Predicting the Performance of Wide Area Data Transfers, Proc. of the 16th International Parallel and Distributed Processing Symposium (IPDPS 2002), pp.34-43, April 2002, pp. 34 - 43. [22] R. Wolski, N. Spring, and J. Hayes, The Network Weather Service: A Distributed Resource Performance Forecasting Service for Metacomputing, Future Generation Computer Systems, 15(5-6):757-768, 1999. [23] Chao-Tung Yang, Chun-Hsiang Chen, Kuan-Ching Li, and Ching-Hsien Hsu, Performance Analysis of Applying Replica Selection Technology for Data Grid Environments,
PaCT 2005, Lecture Notes in Computer Science, vol. 3603, pp. 278-287, Springer-Verlag, September 2005. [24] Chao-Tung Yang, I-Hsien Yang, Kuan-Ching Li, and ChingHsien Hsu A Recursive-Adjustment Co-Allocation Scheme in Data Grid Environments, ICA3PP 2005 Algorithm and Architecture for Parallel Processing, Lecture Notes in Computer Science, vol. 3719, pp. 40-49, Springer-Verlag,

Recently, wireless sensor network systems have been used in many promising applications including military surveillance, habitat monitoring, wildlife tracking etc. [12] [22] [33] [36]. While many middleware services, to support these applications, have been designed and implemented successfully, localization - finding the position of sensor nodes - remains one of the most difficult research challenges to be solved practically. Since most emerging applications based on networked sensor nodes require location awareness to assist their operations, such as annotating sensed data with location context, it is an indispensable requirement for a sensor node to be able to find its own location.
Many approaches have been proposed in the literature [4] [6] [13] [14] [19] [20] [21] [23] [27] [28], however it is still not clear how these solutions can be practically and economically deployed.
An on-board GPS [23] is a typical high-end solution, which requires sophisticated hardware to achieve high resolution time synchronization with satellites. The constraints on power and cost for tiny sensor nodes preclude this as a viable solution. Other solutions require per node devices that can perform ranging among neighboring nodes. The difficulties of these approaches are twofold. First, under constraints of form factor and power supply, the effective ranges of such devices are very limited. For example the effective range of the ultrasonic transducers used in the Cricket system is less than 2 meters when the sender and receiver are not facing each other [26]. Second, since most sensor nodes are static, i.e. the location is not expected to change, it is not cost-effective to equip these sensors with special circuitry just for a one-time localization. To overcome these limitations, many range-free localization schemes have been proposed. Most of these schemes estimate the location of sensor nodes by exploiting the radio connectivity information among neighboring nodes. These approaches eliminate the need of high-cost specialized hardware, at the cost of a less accurate localization. In addition, the radio propagation characteristics vary over time and are environment dependent, thus imposing high calibration costs for the range-free localizations schemes. With such limitations in mind, this paper addresses the following research challenge: How to reconcile the need for high accuracy in location estimation with the cost to achieve it. Our answer to this challenge is a localization system called Spotlight. This system employs an asymmetric architecture, in which sensor nodes do not need any additional hardware, other than what they currently have. All the sophisticated hardware and computation reside on a single Spotlight device. The Spotlight device uses a steerable laser light source, illuminating the sensor nodes placed within a known terrain. We demonstrate that this localization is much more accurate (i.e., tens of centimeters) than the range-based localization schemes and that it has a much longer effective range (i.e., thousands of meters) than the solutions based on ultra-sound/acoustic ranging. At the same time, since only a single sophisticated device is needed to localize the whole network, the amortized cost is much smaller than the cost to add hardware components to the individual sensors.
In this section, we discuss prior work in localization in two major categories: the range-based localization schemes (which use either expensive, per node, ranging devices for high accuracy, or less accurate ranging solutions, as the Received Signal Strength Indicator (RSSI)), and the range-free schemes, which use only connectivity information (hop-by-hop) as an indication of proximity among the nodes.
The localization problem is a fundamental research problem in many domains. In the field of robotics, it has been studied extensively [9] [10]. The reported localization errors are on the order of tens of centimeters, when using specialized ranging hardware, i.e. laser range finder or ultrasound. Due to the high cost and non-negligible form factor of the ranging hardware, these solutions can not be simply applied to sensor networks.
The RSSI has been an attractive solution for estimating the distance between the sender and the receiver. The RADAR system [2] uses the RSSI to build a centralized repository of signal strengths at various positions with respect to a set of beacon nodes.
The location of a mobile user is estimated within a few meters. In a similar approach, MoteTrack [17] distributes the reference RSSI values to the beacon nodes.
Solutions that use RSSI and do not require beacon nodes have also been proposed [5] [14] [24] [26] [29]. They all share the idea of using a mobile beacon. The sensor nodes that receive the beacons, apply different algorithms for inferring their location. In [29], Sichitiu proposes a solution in which the nodes that receive the beacon construct, based on the RSSI value, a constraint on their position estimate. In [26], Priyantha et al. propose MAL, a localization method in which a mobile node (moving strategically) assists in measuring distances between node pairs, until the constraints on distances generate a rigid graph. In [24], Pathirana et al. formulate the localization problem as an on-line estimation in a nonlinear dynamic system and proposes a Robust Extended Kalman Filter for solving it. Elnahrawy [8] provides strong evidence of inherent limitations of localization accuracy using RSSI, in indoor environments.
A more precise ranging technique uses the time difference between a radio signal and an acoustic wave, to obtain pair wise distances between sensor nodes. This approach produces smaller localization errors, at the cost of additional hardware. The Cricket location-support system [25] can achieve a location granularity of tens of centimeters with short range ultrasound transceivers.
AHLoS, proposed by Savvides et al. [27], employs Time of Arrival (ToA) ranging techniques that require extensive hardware and solving relatively large nonlinear systems of equations. A similar ToA technique is employed in [3].
In [30], Simon et al. implement a distributed system (using acoustic ranging) which locates a sniper in an urban terrain.
Acoustic ranging for localization is also used by Kwon et al. [15].
The reported errors in localization vary from 2.2m to 9.5m, depending on the type (centralized vs. distributed) of the Least Square Scaling algorithm used.
For wireless sensor networks ranging is a difficult option. The hardware cost, the energy expenditure, the form factor, the small range, all are difficult compromises, and it is hard to envision cheap, unreliable and resource-constraint devices make use of range-based localization solutions. However, the high localization accuracy, achievable by these schemes is very desirable.
To overcome the challenges posed by the range-based localization schemes, when applied to sensor networks, a different approach has been proposed and evaluated in the past. This approach is called range-free and it attempts to obtain location information from the proximity to a set of known beacon nodes.
Bulusu et al. propose in [4] a localization scheme, called Centroid, in which each node localizes itself to the centroid of its proximate beacon nodes. In [13], He et al. propose APIT, a scheme in which each node decides its position based on the possibility of being inside or outside of a triangle formed by any three beacon nodes heard by the node. The Global Coordinate System [20], developed at MIT, uses apriori knowledge of the node density in the network, to estimate the average hop distance. The DV-* family of localization schemes [21], uses the hop count from known beacon nodes to the nodes in the network to infer the distance. The majority of range-free localization schemes have been evaluated in simulations, or controlled environments. Several studies [11] [32] [34] have emphasized the challenges that real environments pose. Langendoen and Reijers present a detailed, comparative study of several localization schemes in [16].
To the best of our knowledge, Spotlight is the first range-free localization scheme that works very well in an outdoor environment. Our system requires a line of sight between a single device and the sensor nodes, and the map of the terrain where the sensor field is located. The Spotlight system has a long effective range (1000"s meters) and does not require any infrastructure or additional hardware for sensor nodes. The Spotlight system combines the advantages and does not suffer from the disadvantages of the two localization classes.
The main idea of the Spotlight localization system is to generate controlled events in the field where the sensor nodes were deployed. An event could be, for example, the presence of light in an area. Using the time when an event is perceived by a sensor node and the spatio-temporal properties of the generated events, spatial information (i.e. location) regarding the sensor node can be inferred.
Figure 1. Localization of a sensor network using the Spotlight system We envision, and depict in Figure 1, a sensor network deployment and localization scenario as follows: wireless sensor nodes are randomly deployed from an unmanned aerial vehicle.
After deployment, the sensor nodes self-organize into a network and execute a time-synchronization protocol. An aerial vehicle (e.g. helicopter), equipped with a device, called Spotlight, flies over the network and generates light events. The sensor nodes detect the events and report back to the Spotlight device, through a base station, the timestamps when the events were detected. The Spotlight device computes the location of the sensor nodes.
During the design of our Spotlight system, we made the following assumptions: - the sensor network to be localized is connected and a middleware, able to forward data from the sensor nodes to the Spotlight device, is present. - the aerial vehicle has a very good knowledge about its position and orientation (6 parameters: 3 translation and 3 rigid-body rotation) and it possesses the map of the field where the network was deployed. - a powerful Spotlight device is available and it is able to generate 14 spatially large events that can be detected by the sensor nodes, even in the presence of background noise (daylight). - a line of sight between the Spotlight device and sensor nodes exists.
Our assumptions are simplifying assumptions, meant to reduce the complexity of the presentation, for clarity. We propose solutions that do not rely on these simplifying assumptions, in Section 6.
In order to formally describe and generalize the Spotlight localization system, we introduce the following definitions.
Let"s assume that the space A ⊂R3 contains all sensor nodes N, and that each node Ni is positioned at pi(x, y, z). To obtain pi(x, y, z), a Spotlight localization system needs to support three main functions, namely an Event Distribution Function (EDF) E(t), an Event Detection Function D(e), and a Localization Function L(Ti).
They are formally defined as follows: Definition 1: An event e(t, p) is a detectable phenomenon that occurs at time t and at point p є A. Examples of events are light, heat, smoke, sound, etc. Let Ti={ti1, ti2, …, tin} be a set of n timestamps of events detected by a node i. Let T"={t1", t2", …, tm"} be the set of m timestamps of events generated in the sensor field.
Definition 2: The Event Detection Function D(e) defines a binary detection algorithm. For a given event e: ⎩ ⎨ ⎧ = detectednotisEventfalse, detectedisEventtrue, )( e e eD (1) Definition 3: The Event Distribution Function (EDF) E(t) defines the point distribution of events within A at time t: }{ truepteDApptE =∧∈= )),((|)( (2) Definition 4: The Localization Function L(Ti) defines a localization algorithm with input Ti, a sequence of timestamps of events detected by the node i: I iTt i tETL ∈ = )()( (3) Figure 2. Spotlight system architecture As shown in Figure 2, the Event Detection Function D(e) is supported by the sensor nodes. It is used to determine whether an external event happens or not. It can be implemented through either a simple threshold-based detection algorithm or other advanced digital signal processing techniques. The Event Distribution E(t) and Localization Functions L(Ti) are implemented by a Spotlight device. The Localization function is an aggregation algorithm which calculates the intersection of multiple sets of points. The Event Distribution Function E(t) describes the distribution of events over time. It is the core of the Spotlight system and it is much more sophisticated than the other two functions. Due to the fact that E(t) is realized by the Spotlight device, the hardware requirements for the sensor nodes remain minimal.
With the support of these three functions, the localization process goes as follows: 1) A Spotlight device distributes events in the space A over a period of time. 2) During the event distribution, sensor nodes record the time sequence Ti = {ti1, ti2, …, tin} at which they detect the events. 3) After the event distribution, each sensor node sends the detection time sequence back to the Spotlight device. 4) The Spotlight device estimates the location of a sensor node i, using the time sequence Ti and the known E(t) function.
The Event Distribution Function E(t) is the core technique used in the Spotlight system and we propose three designs for it. These designs have different tradeoffs and the cost comparison is presented in Section 3.5.
To illustrate the basic functionality of a Spotlight system, we start with a simple sensor system where a set of nodes are placed along a straight line (A = [0, l] R). The Spotlight device generates point events (e.g. light spots) along this line with constant speed s.
The set of timestamps of events detected by a node i is Ti={ti1}.
The Event Distribution Function E(t) is: ⊂ }{ stpApptE *)( =∧∈= (4) where t ∈[0, l/s]. The resulting localization function is: }{ sttETL iii ∗== 11 )()( (5) where D(e(ti1, pi)) = true for node i positioned at pi.
The implementation of the Event Distribution Function E(t) is straightforward. As shown in Figure 3(a), when a light source emits a beam of light with the angular speed given by d s dt d S )(cos 2 αα α == , a light spot event with constant speed s is generated along the line situated at distance d.
Figure 3. The implementation of the Point Scan EDF The Point Scan EDF can be generalized to the case where nodes are placed in a two dimensional plane R2 . In this case, the Spotlight system progressively scans the plane to activate the sensor nodes. This scenario is depicted in Figure 3(b).
Some devices, e.g. diode lasers, can generate an entire line of events simultaneously. With these devices, we can support the Line Scan Event Distributed Function easily. We assume that the 15 sensor nodes are placed in a two dimensional plane (A=[l x l] ⊂R2 ) and that the scanning speed is s. The set of timestamps of events detected by a node i is Ti={ti1, ti2}.
Figure 4. The implementation of the Line Scan EDF The Line Scan EDF is defined as follows: ( ){ ks,*tpl][0,kp(t)E kkx =∧∈= } for t ∈[0, l/s] and: ({ ls*tk,pl][0,kp(t)E kky −=∧∈= )} (6) for t ∈[ l/s, 2l/s].
U )()()( tEtEtE yx= We can localize a node by calculating the intersection of the two event lines, as shown in Figure 4. More formally: I )()()( 21 iii tEtETL = (7) where D(e(ti1, pi)) = true, D(e(ti2, pi)) = true for node i positioned at pi.
Other devices, such as light projectors, can generate events that cover an area. This allows the implementation of the Area Cover EDF. The idea of Area Cover EDF is to partition the space A into multiple sections and assign a unique binary identifier, called code, to each section. Let"s suppose that the localization is done within a plane (A R2 ). Each section Sk within A has a unique code k. The Area Cover EDF is then defined as follows: ⊂ ⎩ ⎨ ⎧ = 0iskofbitjthiffalse, 1iskofbitjthiftrue, ),( jkBIT (8) }{ truetkBITSpptE k =∧∈= ),()( and the corresponding localization algorithm is: { ∧∈=∧== )),(()(|)( iki TtiftruetkBITSCOGppTL (9)})`),(( iTTtiffalsetkBIT −∈= where COG(Sk) denotes the center of gravity of Sk.
We illustrate the Area Cover EDF with a simple example. As shown in Figure 5, the plane A is divided in 16 sections. Each section Sk has a unique code k. The Spotlight device distributes the events according to these codes: at time j a section Sk is covered by an event (lit by light), if jth bit of k is 1. A node residing anywhere in the section Sk is localized at the center of gravity of that section.
For example, nodes within section 1010 detect the events at time T = {1, 3}. At t = 4 the section where each node resides can be determined A more accurate localization requires a finer partitioning of the plane, hence the number of bits in the code will increase.
Considering the noise that is present in a real, outdoor environment, it is easy to observe that a relatively small error in detecting the correct bit pattern could result in a large localization error. Returning to the example shown in Figure 5, if a sensor node is located in the section with code 0000, and due to the noise, at time t = 3, it thinks it detected an event, it will incorrectly conclude that its code is 1000, and it positions itself two squares below its correct position. The localization accuracy can deteriorate even further, if multiple errors are present in the transmission of the code.
A natural solution to this problem is to use error-correcting codes, which greatly reduce the probability of an error, without paying the price of a re-transmission, or lengthening the transmission time too much. Several error correction schemes have been proposed in the past. Two of the most notable ones are the Hamming (7, 4) code and the Golay (23, 12) code. Both are perfect linear error correcting codes. The Hamming coding scheme can detect up to 2-bit errors and correct 1-bit errors. In the Hamming (7, 4) scheme, a message having 4 bits of data (e.g. dddd, where d is a data bit) is transmitted as a 7-bit word by adding 3 error control bits (e.g. dddpdpp, where p is a parity bit).
Figure 5. The steps of Area Cover EDF. The events cover the shaded areas.
The steps of the Area Cover technique, when using Hamming (7, 4) scheme are shown in Figure 6. Golay codes can detect up to 6-bit errors and correct up to 3-bit errors. Similar to Hamming (7, 4), Golay constructs a 23-bit codeword from 12-bit data. Golay codes have been used in satellite and spacecraft data transmission and are most suitable in cases where short codeword lengths are desirable.
Figure 6. The steps of Area Cover EDF with Hamming (7, 4) ECC. The events cover the shaded areas.
Let"s assume a 1-bit error probability of 0.01, and a 12-bit message that needs to be transmitted. The probability of a failed transmission is thus: 0.11, if no error detection and correction is used; 0.0061 for the Hamming scheme (i.e. more than 1-bit error); and 0.000076 for the Golay scheme (i.e. more than 3-bit errors).
Golay is thus 80 times more robust that the Hamming scheme, which is 20 times more robust than the no error correction scheme. 16 Considering that a limited number of corrections is possible by any coding scheme, a natural question arises: can we minimize the localization error when there are errors that can not be corrected?
This can be achieved by a clever placement of codes in the grid.
As shown in Figure 7, the placement A, in the presence of a 1-bit error has a smaller average localization error when compared to the placement B. The objective of our code placement strategy is to reduce the total Euclidean distance between all pairs of codes with Hamming distances smaller than K, the largest number of expected 1-bit errors.
Figure 7. Different code placement strategies Formally, a placement is represented by a function P: [0, l]d → C, which assigns a code to every coordinate in the d-dimensional cube of size l (e.g., in the planar case, we place codes in a 2dimensional grid). We denote by dE(i, j) the Euclidean distance and by dH(i, j) the Hamming distance between two codes i and j. In a noisy environment, dH(i,j) determines the crossover probability between the two codes. For the case of independent detections, the higher dH(i, j) is, the lower the crossover probability will be. The objective function is defined as follows: d Kjid E ljiwherejid H ],0[,}),(min{ ),( ∈∑≤ (10) Equation 10 is a non-linear and non-convex programming problem. In general, it is analytically hard to obtain the global minimum. To overcome this, we propose a Greedy Placement method to obtain suboptimal results. In this method we initialize the 2-dimensional grid with codes. Then we swap the codes within the grid repeatedly, to minimize the objective function. For each swap, we greedily chose a pair of codes, which can reduce the objective function (Equation 10) the most. The proposed Greedy Placement method ends when no swap of codes can further minimize the objective function.
For evaluation, we compared the average localization error in the presence of K-bit error for two strategies: the proposed Greedy Placement and the Row-Major Placement (it places the codes consecutively in the array, in row-first order). 0
1
2
3
4 4 9 16 25 36 49 64 81 Grid Size LocalizationError[gridunit] Row-major Consecutive placement Greedy Placement Figure 8. Localization error with code placement and no ECC As Figure 8 shows, if no error detection/correction capability is present and 1-bit errors occur, then our Greedy Placement method can reduce the localization error by an average 23%, when compared to the Row-Major Placement. If error detection and correction schemes are used (e.g. Hamming (12, 8) and if 3-bit errors occur (K=3) then the Greedy Placement method reduces localization error by 12%, when compared to the Row-Major Placement, as shown in Figure 9. If K=1, then there is no benefit in using the Greedy Placement method, since the 1-bit error can be corrected by the Hamming scheme. 0
1
2
3
4
4 9 16 25 36 49 64 81 Grid Size LocalizationError[gridunit] Row-major Consecutive placement Greedy Placement Figure 9. Localization error with code placement and Hamming ECC
Although all three aforementioned techniques are able to localize the sensor nodes, they differ in the localization time, communication overhead and energy consumed by the Event Distribution Function (let"s call it Event Overhead). Let"s assume that all sensor nodes are located in a square with edge size D, and that the Spotlight device can generate N events (e.g. Point, Line and Area Cover events) every second and that the maximum tolerable localization error is r. Table 1 presents the execution cost comparison of the three different Spotlight techniques.
Table 1. Execution Cost Comparison Criterion Point Scan Line Scan Area Cover Localization Time NrD /)/( 22 NrD /)/2( NDlogr / # Detections 1 2 logrD # Time Stamps 1 2 logrD Event Overhead D2 2D2 D2 logrD/2 Table 1 indicates that the Event Overhead for the Point Scan method is the smallest - it requires a one-time coverage of the area, hence the D2 . However the Point Scan takes a much longer time than the Area Cover technique, which finishes in logrD seconds.
The Line Scan method trades the Event Overhead well with the localization time. By doubling the Event Overhead, the Line Scan method takes only r/2D percentage of time to complete, when compared with the Point Scan method. From Table 1, it can be observed that the execution costs do not depend on the number of sensor nodes to be localized.
It is important to remark the ratio Event Overhead per unit time, which is indicative of the power requirement for the Spotlight device. This ratio is constant for the Point Scan (r2 *N) while it grows linearly with area, for the Area Cover (D2 *N/2). If the deployment area is very large, the use of the Area Cover EDF is prohibitively expensive, if not impossible. For practical purposes, the Area Cover is a viable solution for small to medium size networks, while the Line Scan works well for large networks. We discuss the implications of the power requirement for the Spotlight device, and offer a hybrid solution in Section 6.
The accuracy of localization with the Spotlight technique depends on many aspects. The major factors that were considered during the implementation of the system are discussed below: 17 - Time Synchronization: the Spotlight system exchanges time stamps between sensor nodes and the Spotlight device. It is necessary for the system to reach consensus on global time through synchronization. Due to the uncertainty in hardware processing and wireless communication, we can only confine such errors within certain bounds (e.g. one jiffy). An imprecise input to the Localization Function L(T) leads to an error in node localization. - Uncertainty in Detection: the sampling rate of the sensor nodes is finite, consequently, there will be an unpredictable delay between the time when an event is truly present and when the sensor node detects it. Lower sampling rates will generate larger localizations errors. - Size of the Event: the events distributed by the Spotlight device can not be infinitely small. If a node detects one event, it is hard for it to estimate the exact location of itself within the event. - Realization of Event Distribution Function: EDF defines locations of events at time t. Due to the limited accuracy (e.g. mechanical imprecision), a Spotlight device might generate events which locate differently from where these events are supposed to be.
It is important to remark that the localization error is independent of the number of sensor nodes in the network. This independence, as well as the aforementioned independence of the execution cost, indicate the very good scalability properties (with the number of sensor nodes, but not with the area of deployment) that the Spotlight system possesses.
For our performance evaluation we implemented two Spotlight systems. Using these two implementations we were able to investigate the full spectrum of Event Distribution techniques, proposed in Section 3, at a reduced one time cost (less than $1,000).
The first implementation, called μSpotlight, had a short range (10-20 meters), however its capability of generating the entire spectrum of EDFs made it very useful. We used this implementation mainly to investigate the capabilities of the Spotlight system and tune its performance. It was not intended to represent the full solution, but only a scaled down version of the system.
The second implementation, the Spotlight system, had a much longer range (as far as 6500m), but it was limited in the types of EDFs that it can generate. The goal of this implementation was to show how the Spotlight system works in a real, outdoor environment, and show correlations with the experimental results obtained from the μSpotlight system implementation.
In the remaining part of this section, we describe how we implemented the three components (Event Distribution, Event Detection and Localization functions) of the Spotlight architecture, and the time synchronization protocol, a key component of our system.
The first system we built, called μSpotlight, used as the Spotlight device, an Infocus LD530 projector connected to an IBM Thinkpad laptop. The system is shown in Figure 10.
The Event Distribution Function was implemented as a Java GUI. Due to the stringent timing requirements and the delay caused by the buffering in the windowing system of a PC, we used the Full-Screen Exclusive Mode API provided by Java2. This allowed us to bypass the windowing system and more precisely estimate the time when an event is displayed by the projector, hence a higher accuracy of timestamps of events. Because of the 50Hz refresh rate of our projector, there was still an uncertainty in the time stamping of the events of 20msec. We explored the possibility of using and modifying the Linux kernel to expose the vertical synch (VSYNCH) interrupt, generated by the displaying device after each screen refresh, out of the kernel mode. The performance evaluation results showed, however, that this level of accuracy was not needed.
The sensor nodes that we used were Berkeley Mica2 motes equipped with MTS310 multi-sensor boards from Crossbow. This sensor board contains a CdSe photo sensor which can detect the light from the projector.
Figure 10. μSpotlight system implementation With this implementation of the Spotlight system, we were able to generate Point, Line and Area Scan events.
The second Spotlight system we built used, as the Spotlight device, diode lasers, a computerized telescope mount (Celestron CG-5GT, shown in Figure 11), and an IBM Thinkpad laptop. The laptop was connected, through RS232 interfaces, to the telescope mount and to one XSM600CA [7] mote, acting as a base station.
The diode lasers we used ranged in power from 7mW to 35mW.
They emitted at 650nm, close to the point of highest sensitivity for CdSe photosensor. The diode lasers were equipped with lenses that allowed us to control the divergence of the beam.
Figure 11. Spotlight system implementation The telescope mount has worm gears for a smooth motion and high precision angular measurements. The two angular measures that we used were the, so called, Alt (from Altitude) and Az (from Azimuth). In astronomy, the Altitude of a celestial object is its angular distance above or below the celestial horizon, and the Azimuth is the angular distance of an object eastwards of the meridian, along the horizon. 18 The laptop computer, through a Java GUI, controls the motion of the telescope mount, orienting it such that a full Point Scan of an area is performed, similar to the one described in Figure 3(b).
For each turning point i, the 3-tuple (Alti and Azi angles and the timestamp ti) is recorded. The Spotlight system uses the timestamp received from a sensor node j, to obtain the angular measures Altj and Azj for its location.
For the sensor nodes, we used XSM motes, mainly because of their longer communication range. The XSM mote has the photo sensor embedded in its main board. We had to make minor adjustments to the plastic housing, in order to expose the photo sensor to the outside. The same mote code, written in nesC, for TinyOS, was used for both µSpotlight and Spotlight system implementations.
The Event Detection Function aims to detect the beginning of an event and record the time when the event was observed. We implemented a very simple detection function based on the observed maximum value. An event i will be time stamped with time ti, if the reading from the photo sensor dti, fulfills the condition: itdd <Δ+max where dmax is the maximum value reported by the photo sensor before ti and Δ is a constant which ensures that the first large detection gives the timestamp of the event (i.e. small variations around the first large signal are not considered). Hence Δ guarantees that only sharp changes in the detected value generate an observed event.
The Localization Function is implemented in the Java GUI. It matches the timestamps created by the Event Distribution Function with those reported by the sensor nodes.
The Localization Function for the Point Scan EDF has as input a time sequence Ti = {t1}, as reported by node i. The function performs a simple search for the event with a timestamp closest to t1. If t1 is constrained by: 11 + << nn ee ttt where en and en+1 are two consecutive events, then the obtained location for node i is: 11 , ++ == nn ee yyxx The case for the Line Scan is treated similarly. The input to the Localization Function is the time sequence Ti = {t1, t2} as reported by node i. If the reported timestamps are constrained by: 11 + << nn ee ttt , and 12 + << mm ee ttt where en and en+1 are two consecutive events on the horizontal scan and em and em+1 are two consecutive events on vertical scan, then the inferred location for node i is: 11 , ++ == mn ee yyxx The Localization Function for the Area Cover EDF has as input a timestamp set Ti={ti1, ti2, …, tin} of the n events, detected by node i. We recall the notation for the set of m timestamps of events generated by the Spotlight device, T"={t1", t2", …, tm"}. A code di=di1di2…dim is then constructed for each node i, such that dij=1 if tj" ∈Ti and dij=0 if tj" ∉ Ti. The function performs a search for an event with an identical code. If the following condition is true: nei dd = where en is an event with code den, then the inferred location for node i is: nn ee yyxx == ,
The time synchronization in the Spotlight system consists of two parts: - Synchronization between sensor nodes: This is achieved through the Flooding Time Synchronization Protocol [18]. In this protocol, synchronized nodes (the root node is the only synchronized node at the beginning) send time synchronization message to unsynchronized nodes. The sender puts the time stamp into the synchronization message right before the bytes containing the time stamp are transmitted. Once a receiver gets the message, it follows the sender's time and performs the necessary calculations to compensate for the clock drift. - Synchronization between the sensor nodes and the Spotlight device: We implemented this part through a two-way handshaking between the Spotlight device and one node, used as the base station. The sensor node is attached to the Spotlight device through a serial interface.
Figure 12. Two-way synchronization As shown in Figure 12, let"s assume that the Spotlight device sends a synchronization message (SYNC) at local time T1, the sensor node receives it at its local time T2 and acknowledges it at local time T3 (both T2 and T3 are sent back through ACK). After the Spotlight device receives the ACK, at its local time T4, the time synchronization can be achieved as follows: 2 )()( 4312 TTTT Offset −+− = (11) OffsetTTT spotlightnodeglobal +== We note that Equation 11 assumes that the one trip delays are the same in both directions. In practice this does not hold well enough. To improve the performance, we separate the handshaking process from the timestamp exchanges. The handshaking is done fast, through a 2 byte exchange between the Spotlight device and the sensor node (the timestamps are still recorded, but not sent).
After this fast handshaking, the recorded time stamps are exchanged. The result indicates that this approach can significantly improve the accuracy of time synchronization.
In this section we present the performance evaluation of the Spotlight systems when using the three event distribution functions, i.e. Point Scan, Line Scan and Area Cover, described in Section 3. 19 For the µSpotlight system we used 10 Mica2 motes. The sensor nodes were attached to a vertically positioned Veltex board. By projecting the light to the sensor nodes, we are able to generate well controlled Point, Line and Area events. The Spotlight device was able to generate events, i.e. project light patterns, covering an area of approximate size 180x140cm2 . The screen resolution for the projector was 1024x768, and the movement of the Point Scan and Line Scan techniques was done through increments (in the appropriate direction) of 10 pixels between events.
Each experimental point was obtained from 10 successive runs of the localization procedure. Each set of 10 runs was preceded by a calibration phase, aimed at estimating the total delays (between the Spotlight device and each sensor node) in detecting an event.
During the calibration, we created an event covering the entire sensor field (illuminated the entire area). The timestamp reported by each sensor node, in conjunction with the timestamp created by the Spotlight device were used to obtain the time offset, for each sensor node. More sophisticated calibration procedures have been reported previously [35]. In addition to the time offset, we added a manually configurable parameter, called bias. It was used to best estimate the center of an event.
Figure 13. Deployment site for the Spotlight system For the Spotlight system evaluation, we deployed 10 XSM motes in a football field. The site is shown in Figure 13 (laser beams are depicted with red arrows and sensor nodes with white dots). Two sets of experiments were run, with the Spotlight device positioned at 46m and at 170m from the sensor field. The sensor nodes were aligned and the Spotlight device executed a Point Scan. The localization system computed the coordinates of the sensor nodes, and the Spotlight device was oriented, through a GoTo command sent to the telescope mount, towards the computed location. In the initial stages of the experiments, we manually measured the localization error.
For our experimental evaluation, the metrics of interest were as follows: - Localization error, defined as the distance, between the real location and the one obtained from the Spotlight system. - Localization duration, defined as the time span between the first and last event. - Localization range, defined as the maximum distance between the Spotlight device and the sensor nodes. - A Localization Cost function Cost:{{localization accuracy}, {localization duration}} → [0,1] quantifies the trade-off between the accuracy in localization and the localization duration. The objective is to minimize the Localization Cost function. By denoting with ei the localization error for the ith scenario, with di the localization duration for the ith scenario, with max(e) the maximum localization error, with max(d) the maximum localization duration, and with α the importance factor, the Localization Cost function is formally defined as: )max( )1( )max( ),( d d e e deCost ii ii ∗−+∗= αα - Localization Bias. This metric is used to investigate the effectiveness of the calibration procedure. If, for example, all computed locations have a bias in the west direction, a calibration factor can be used to compensate for the difference.
The parameters that we varied during the performance evaluation of our system were: the type of scanning (Point, Line and Area), the size of the event, the duration of the event (for Area Cover), the scanning speed, the power of the laser and the distance between the Spotlight device and sensor field, to estimate the range of the system.
In this experiment, we investigated how the size of the event and the scanning speed affect the localization error. Figure 14 shows the mean localization errors with their standard deviations.
It can be observed, that while the scanning speed, varying between 35cm/sec and 87cm/sec has a minor influence on the localization accuracy, the size of the event has a dramatic effect. 0 2 4 6 8 10 12 14
Event Size [cm] Locationerror[cm] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 14. Localization Error vs. Event Size for the Point Scan EDF The obtained localization error varied from as little as 2cm to over 11cm for the largest event. This dependence can be explained by our Event Detection algorithm: the first detection above a threshold gave the timestamp for the event.
The duration of the localization scheme is shown in Figure 15.
The dependency of the localization duration on the size of the event and scanning speed is natural. A bigger event allows a reduction in the total duration of up to 70%. The localization duration is directly proportional to the scanning speed, as expected, and depicted in Figure 15. 0 20 40 60 80 100 120
Event Size [cm] LocalizationDuration[sec] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 15. Localization Duration vs. Event Size for the Point Scan EDF 20 An interesting trade-off is between the localization accuracy (usually the most important factor), and the localization time (important in environments where stealthiness is paramount).
Figure 16 shows the Localization Cost function, for α = 0.5 (accuracy and duration are equally important).
As shown in Figure 16, it can be observed that an event size of approximately 10-15cm (depending on the scanning speed) minimizes our Cost function. For α = 1, the same graph would be a monotonically increasing function, while for α = 0, it would be monotonically decreasing function.
Event Size [cm] LocalizationCost[%] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 16. Localization Cost vs. Event Size for the Point Scan EDF
In a similar manner to the Point Scan EDF, for the Line Scan EDF we were interested in the dependency of the localization error and duration on the size of the event and scanning speed.
We represent in Figure 17 the localization error for different event sizes. It is interesting to observe the dependency (concave shape) of the localization error vs. the event size. Moreover, a question that should arise is why the same dependency was not observed in the case of Point Scan EDF. 0 1 2 3 4 5 6 7 8 9 10
Event Size [cm] Locationerror[cm] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 17. Localization Error vs. Event Size for the Line Scan EDF The explanation for this concave dependency is the existence of a bias in location estimation. As a reminder, a bias factor was introduced in order to best estimate the central point of events that have a large size. What Figure 17 shows is the fact that the bias factor was optimal for an event size of approximately 7cm. For events smaller and larger than this, the bias factor was too large, and too small, respectively. Thus, it introduced biased errors in the position estimation.
The reason why we did not observe the same dependency in the case of the Point Scan EDF was that we did not experiment with event sizes below 7cm, due to the long time it would have taken to scan the entire field with events as small as 1.7cm.
The results for the localization duration as a function of the size of the event are shown in Figure 18. As shown, the localization duration is directly proportional to the scanning speed. The size of the event has a smaller influence on the localization duration. One can remark the average localization duration of about 10sec, much shorter then the duration obtained in the Point Scan experiment.
The Localization Cost function dependency on the event size and scanning speed, for α=0.5, is shown in Figure 19. The dependency on the scanning speed is very small (the Cost Function achieves a minimum in the same 4-6cm range). It is interesting to note that this 4-6cm optimal event size is smaller than the one observed in the case of Point Scan EDF. The explanation for this is that the smaller localization duration observed in the Line Scan EDF, allowed a shift (towards smaller event sizes) in the total Localization Cost Function. 0 5 10 15 20
Event Size [cm] LocalizationDuration[sec] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 18. Localization Duration vs. Event Size for the Line Scan EDF
Event Size [cm] LocalizationCost[%] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 19. Cost Function vs. Event Size for the Line Scan EDF During our experiments with the Line Scan EDF, we observed evidence of a bias in location estimation. The estimated locations for all sensor nodes exhibited different biases, for different event sizes. For example, for an event size of 17.5cm, the estimated location for sensor nodes was to the upper-left size of the actual location. This was equivalent to an early detection, since our scanning was done from left to right and from top to bottom. The scanning speed did not influence the bias.
In order to better understand the observed phenomena, we analyzed our data. Figure 20 shows the bias in the horizontal direction, for different event sizes (the vertical bias was almost identical, and we omit it, due to space constraints).
From Figure 20, one can observe that the smallest observed bias, and hence the most accurate positioning, was for an event of size 7cm. These results are consistent with the observed localization error, shown in Figure 17.
We also adjusted the measured localization error (shown in Figure 17) for the observed bias (shown in Figure 20). The results of an ideal case of Spotlight Localization system with Line Scan EDF are shown in Figure 21. The errors are remarkably small, varying between 0.1cm and 0.8cm, with a general trend of higher localization errors for larger event sizes. 21 -6 -5 -4 -3 -2 -1 0 1 2 3
Event Size [cm] HorizontalBias[cm] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 20. Position Estimation Bias for the Line Scan EDF
Event Size [cm] LocalizationErrorw/oBias[cm] 87cm/sec 58cm/sec 43cm/sec 35cm/sec Figure 21. Position Estimation w/o Bias (ideal), for the Line Scan EDF
In this experiment, we investigated how the number of bits used to quantify the entire sensor field, affected the localization accuracy.
In our first experiment we did not use error correcting codes. The results are shown in Figure 22.
6 8 10 12 Number of Bits Locationerror[cm] 20ms/event 40ms/event 60ms/event 80ms/event 100ms/event Figure 22. Localization Error vs. Event Size for the Area Cover EDF One can observe a remarkable accuracy, with localization error on the order of 0.3-0.6cm. What is important to observe is the variance in the localization error. In the scenario where 12 bits were used, while the average error was very small, there were a couple of cases, where an incorrect event detection generated a larger than expected error. An example of how this error can occur was described in Section 3.4. The experimental results, presented in Figure 22, emphasize the need for error correction of the bit patterns observed and reported by the sensor nodes.
The localization duration results are shown in Figure 23. It can be observed that the duration is directly proportional with the number of bits used, with total durations ranging from 3sec, for the least accurate method, to 6-7sec for the most accurate. The duration of an event had a small influence on the total localization time, when considering the same scenario (same number of bits for the code).
The Cost Function dependency on the number of bits in the code, for α=0.5, is shown in Figure 24. Generally, since the localization duration for the Area Scan can be extremely small, a higher accuracy in the localization is desired. While the Cost function achieves a minimum when 10 bits are used, we attribute the slight increase observed when 12 bits were used to the two 12bit scenarios where larger than the expected errors were observed, namely 6-7mm (as shown in Figure 22). 0 1 2 3 4 5 6 7 8 9 10 6 8 10 12 Number of Bits LocalizationDuration[sec] 20ms/event 40ms/event 60ms/event 80ms/event 100ms/event Figure 23. Localization Duration vs. Event Size for the Area Cover EDF
4 6 8 10 12 14 Number of Bits CostFunction[%] 20ms/event 40ms/event 60ms/event 80ms/event 100ms/event Figure 24. Cost Function vs. Event Size for the Area Cover EDF -0.4 -0.1
20 40 60 80 100 Event Duration [ms/event] Locationerror[cm] w/o ECC w/ ECC Figure 25. Localization Error w/ and w/o Error Correction The two problematic scenarios (shown in Figure 22, where for 12-bit codes we observed errors larger than the event size, due to errors in detection) were further explored by using error correction codes. As described in Section 3.3, we implemented an extended Golay (24, 12) error correction mechanism in our location estimation algorithm.
The experimental results are depicted in Figure 25, and show a consistent accuracy. The scenario without error correction codes, is simply the same 12-bit code scenario, shown in Figure 22. We only investigated the 12-bit scenario, due to its match with the 12bit data required by the Golay encoding scheme (extended Golay producing 24-bit codewords). 22
In this section we describe the experiments performed at a football stadium, using our Spotlight system. The hardware that we had available allowed us to evaluate the Point Scan technique of the Spotlight system. In our evaluation, we were interested to see the performance of the system at different ranges. Figures 26 and 27 show the localization error versus the event size at two different ranges: 46m and 170m.
Figure 26 shows a remarkable accuracy in localization. The errors are in the centimeter range. Our initial, manual measurements of the localization error were most of the time difficult to make, since the spot of the laser was almost perfectly covering the XSM mote. We are able to achieve localization errors of a few centimeters, which only range-based localization schemes are able to achieve [25]. The observed dependency on the size of the event is similar to the one observed in the μSpotlight system evaluation, and shown in Figure 14. This proved that the μSpotlight system is a viable alternative for investigating complex EDFs, without incurring the costs for the necessary hardware. 0 5 10 15 20 25 0 5 10 15 20 25 30 Event Size [cm] LocalizationError[cm]
Figure 26. Localization Error vs. Event Size for Spotlight system at 46m In the experiments performed over a much longer distance between the Spotlight device and sensor network, the average localization error remains very small. Localization errors of 510cm were measured, as Figure 27 shows. We were simply amazed by the accuracy that the system is capable of, when considering that the Spotlight system operated over the length of a football stadium. Throughout our experimentation with the Spotlight system, we have observed localization errors that were simply offsets of real locations. Since the same phenomenon was observed when experimenting with the μSpotlight system, we believe that with auto-calibration, the localization error can be further reduced. 0 5 10 15 20 25 6 12 18 Event Size [cm] LocalizationError[cm]
3m/sec Figure 27. Localization Error vs. Event Size for Spotlight system at 170m The time required for localization using the Spotlight system with a Point Scan EDF, is given by: t=(L*l)/(s*Es), where L and l are the dimensions of the sensor network field, s is the scanning speed, and Es is the size of the event. Figure 28 shows the time for localizing a sensor network deployed in an area of size of a football field using the Spotlight system. Here we ignore the message propagation time, from the sensor nodes to the Spotlight device.
From Figure 28 it can be observed that the very small localization errors are prohibitively expensive in the case of the Point Scan. When localization errors of up to 1m are tolerable, localization duration can be as low as 4 minutes. Localization durations of 5-10 minutes, and localization errors of 1m are currently state of art in the realm of range-free localization schemes. And these results are achieved by using the Point Scan scheme, which required the highest Localization Time, as it was shown in Table 1. 0 5 10 15 20 25 30 35 40 0 25 50 75 100 125 150 Event Size [cm] LocalizationTime[min] 3m/sec 6m/sec 9m/sec Figure 28. Localization Time vs. Event Size for Spotlight system One important characteristic of the Spotlight system is its range.
The two most important factors are the sensitivity of the photosensor and the power of the Spotlight source. We were interested in measuring the range of our Spotlight system, considering our capabilities (MTS310 sensor board and inexpensive, $12-$85, diode laser). As a result, we measured the intensity of the laser beam, having the same focus, at different distances. The results are shown in Figure 29. 950 1000 1050 1100 0 50 100 150 200 Distance [m] Intensity[ADCcount] 35mW 7mW Figure 29. Localization Range for the Spotlight system From Figure 29, it can be observed that only a minor decrease in the intensity occurs, due to absorption and possibly our imperfect focusing of the laser beam. A linear fit of the experimental data shows that distances of up to 6500m can be achieved. While we do not expect atmospheric conditions, over large distances, to be similar to our 200m evaluation, there is strong evidence that distances (i.e. altitude) of 1000-2000m can easily be achieved. The angle between the laser beam and the vertical should be minimized (less than 45°), as it reduces the difference between the beam cross-section (event size) and the actual projection of the beam on the ground.
In a similar manner, we were interested in finding out the maximum size of an event, that can be generated by a COTS laser and that is detectable by the existing photosensor. For this, we 23 varied the divergence of the laser beam and measured the light intensity, as given by the ADC count. The results are shown in Figure 30. It can be observed that for the less powerful laser, an event size of 1.5m is the limit. For the more powerful laser, the event size can be as high as 4m.
Through our extensive performance evaluation results, we have shown that the Spotlight system is a feasible, highly accurate, low cost solution for localization of wireless sensor networks. From our experience with sources of laser radiation, we believe that for small and medium size sensor network deployments, in areas of less than 20,000m2 , the Area Cover scheme is a viable solution.
For large size sensor network deployments, the Line Scan, or an incremental use of the Area Cover are very good options. 0 200 400 600 800 1000 1200 0 50 100 150 200 Event Size [cm] Intensity[ADCcount] 35mW 7mW Figure 30. Detectable Event Sizes that can be generated by COTS lasers
The proposed design and the implementation of the Spotlight system can be considered centralized, due to the gathering of the sensor data and the execution of the Localization Function L(t) by the Spotlight device. We show that this design can easily be transformed into a distributed one, by offering two solutions.
One idea is to disseminate in the network, information about the path of events, generated by the EDF (similar to an equation, describing a path), and let the sensor nodes execute the Localization Function. For example, in the Line Scan scenario, if the starting and ending points for the horizontal and vertical scans, and the times they were reached, are propagated in the network, then any sensor in the network can obtain its location (assuming a constant scanning speed).
A second solution is to use anchor nodes which know their positions. In the case of Line Scan, if three anchors are present, after detecting the presence of the two events, the anchors flood the network with their locations and times of detection. Using the same simple formulas as in the previous scheme, all sensor nodes can infer their positions.
Another requirement imposed by the Spotlight system design, is the use of a time synchronization protocol between the Spotlight device and the sensor network. Relaxing this requirement and imposing only a time synchronization protocol among sensor nodes is a very desirable objective. The idea is to use the knowledge that the Spotlight device has about the speed with which the scanning of the sensor field takes place. If the scanning speed is constant (let"s call it s), then the time difference (let"s call it Δt) between the event detections of two sensor nodes is, in fact, an accurate measure of the range between them: d=s*Δt. Hence, the Spotlight system can be used for accurate ranging of the distance between any pair of sensor nodes. An important observation is that this ranging technique does not suffer from limitations of others: small range and directionality for ultrasound, or irregularity, fading and multipath for Received Signal Strength Indicator (RSSI). After the ranges between nodes have been determined (either in a centralized or distributed manner) graph embedding algorithms can be used for a realization of a rigid graph, describing the sensor network topology.
Another system optimization is for environments where the sensor node density is not uniform. One disadvantage of the Line Scan technique, when compared to the Area Cover, is the localization time.
An idea is to use two scans: one which uses a large event size (hence larger localization errors), followed by a second scan in which the event size changes dynamically. The first scan is used for identifying the areas with a higher density of sensor nodes. The second scan uses a larger event in areas where the sensor node density is low and a smaller event in areas with a higher sensor node density.
A dynamic EDF can also be used when it is very difficult to meet the power requirements for the Spotlight device (imposed by the use of the Area Cover scheme in a very large area). In this scenario, a hybrid scheme can be used: the first scan (Point Scan) is performed quickly, with a very large event size, and it is meant to identify, roughly, the location of the sensor network.
Subsequent Area Cover scans will be executed on smaller portions of the network, until the entire deployment area is localized.
Our implementation of the Spotlight system used visible light for creating events. Using the system during the daylight or in a room well lit, poses challenges due to the solar or fluorescent lamp radiation, which generate a strong background noise. The alternative, which we used in our performance evaluations, was to use the system in a dark room (μSpotlight system) or during the night (Spotlight system). While using the Spotlight system during the night is a good solution for environments where stealthiness is not important (e.g. environmental sciences) for others (e.g. military applications), divulging the presence and location of a sensor field, could seriously compromise the efficacy of the system.
Figure 31. Fluorescent Light Spectra (top), Spectral Response for CdSe cells (bottom) A solution to this problem, which we experimented with in the µSpotlight system, was to use an optical filter on top of the light 24 sensor. The spectral response of a CdSe photo sensor spans almost the entire visible domain [37], with a peak at about 700nm (Figure 31-bottom). As shown in Figure 31-top, the fluorescent light has no significant components above 700nm. Hence, a simple red filter (Schott RG-630), which transmits all light with wavelength approximately above 630nm, coupled with an Event Distribution Function that generates events with wavelengths above the same threshold, would allow the use of the system when a fluorescent light is present.
A solution for the Spotlight system to be stealthy at night, is to use a source of infra-red radiation (i.e. laser) emitting in the range [750, 1000]nm. For a daylight use of the Spotlight system, the challenge is to overcome the strong background of the natural light. A solution we are considering is the use of a narrow-band optical filter, centered at the wavelength of the laser radiation. The feasibility and the cost-effectiveness of this solution remain to be proven.
A further generalization is when the map of the terrain where the sensor network was deployed is unknown. While this is highly unlikely for many civil applications of wireless sensor network technologies, it is not difficult to imagine military applications where the sensor network is deployed in a hostile and unknown terrain. A solution to this problem is a system that uses two Spotlight devices, or equivalently, the use of the same device from two distinct positions, executing, from each of them, a complete localization procedure. In this scheme, the position of the sensor node is uniquely determined by the intersection of the two location directions obtained by the system. The relative localization (for each pair of Spotlight devices) will require an accurate knowledge of the 3 translation and 3 rigid-body rotation parameters for Spotlight"s position and orientation (as mentioned in Section 3).
This generalization is also applicable to scenarios where, due to terrain variations, there is no single aerial point with a direct line of sight to all sensor nodes, e.g. hilly terrain. By executing the localization procedure from different aerial points, the probability of establishing a line of sight with all the nodes, increases. For some military scenarios [1] [12], where open terrain is prevalent, the existence of a line of sight is not a limiting factor. In light of this, the Spotlight system can not be used in forests or indoor environments.
In this paper we presented the design, implementation and evaluation of a localization system for wireless sensor networks, called Spotlight. Our localization solution does not require any additional hardware for the sensor nodes, other than what already exists. All the complexity of the system is encapsulated into a single Spotlight device. Our localization system is reusable, i.e. the costs can be amortized through several deployments, and its performance is not affected by the number of sensor nodes in the network. Our experimental results, obtained from a real system deployed outdoors, show that the localization error is less than 20cm. This error is currently state of art, even for range-based localization systems and it is 75% smaller than the error obtained when using GPS devices or when the manual deployment of sensor nodes is a feasible option [31].
As future work, we would like to explore the self-calibration and self-tuning of the Spotlight system. The accuracy of the system can be further improved if the distribution of the event, instead of a single timestamp, is reported. A generalization could be obtained by reformulating the problem as an angular estimation problem that provides the building blocks for more general localization techniques.
This work was supported by the DARPA IXO office, under the NEST project (grant number F336616-01-C-1905) and by the NSF grant CCR-0098269. We would like to thank S. Cornwell for allowing us to run experiments in the stadium, M. Klopf for his assistance with optics, and anonymous reviewers and our shepherd, Koen Langendoen, for their valuable feedback.
[1] A. Arora, P. Dutta, S. Bapat, V. Kulathumani, H. Zhang, V.
Naik, V. Mittal, H. Cao, M. Demirbas, M. Gouda, Y. Choi, T.
Herman, S. Kulharni, U. Arumugam, M. Nesterenko, A.
Vora, M. Miyashita, A Line in the Sand: A Wireless Sensor Network for Target Detection, Classification and Tracking, in Computer Networks 46(5), 2004. [2] P. Bahl, V.N. Padmanabhan, RADAR: An In-Building RFbased User Location and Tracking System, in Proceedings of Infocom, 2000 [3] M. Broxton, J. Lifton, J. Paradiso, Localizing a Sensor Network via Collaborative Processing of Global Stimuli, in Proceedings of EWSN, 2005. [4] N. Bulusu, J. Heidemann, D. Estrin, GPS-less Low Cost Outdoor Localization for Very Small Devices, in IEEE Personal Communications Magazine, 2000. [5] P. Corke, R. Peterson, D. Rus, Networked Robots: Flying Robot Navigation Using a Sensor Net, in ISSR, 2003. [6] L. Doherty, L. E. Ghaoui, K. Pister, Convex Position Estimation in Wireless Sensor Networks, in Proceedings of Infocom, 2001 [7] P. Dutta, M. Grimmer, A. Arora, S. Bibyk, D. Culler, Design of a Wireless Sensor Network Platform for Detecting Rare,
Random, and Ephemeral Events, in Proceedings of IPSN,
[8] E. Elnahrawy, X. Li, R. Martin, The Limits of Localization using RSSI, in Proceedings of SECON, 2004. [9] D. Fox, W. Burgard, S. Thrun, Markov Localization for Mobile Robots in Dynamic Environments, in Journal of Artificial Intelligence Research, 1999. [10] D. Fox, W. Burgard, F. Dellaert, S. Thrun, Monte Carlo Localization: Efficient Position Estimation for Mobile Robots, in Conference on Artificial Intelligence, 2000. [11] D. Ganesan, B. Krishnamachari, A. Woo, D. Culler, D.
Estrin, S. Wicker, Complex Behaviour at Scale: An Experimental Study of Low Power Wireless Sensor Networks, in Technical Report, UCLA-TR 01-0013, 2001. [12] T. He, S. Krishnamurthy, J. A. Stankovic, T. Abdelzaher, L.
Luo, R. Stoleru, T. Yan, L. Gu, J. Hui, B. Krogh, An Energy-Efficient Surveillance System Using Wireless Sensor Networks, in Proceedings of Mobisys, 2004. [13] T. He, C. Huang, B. Blum, J. A. Stankovic, T. Abdelzaher,
Range-Free Localization Schemes for Large Scale Sensor Networks in Proceedings of Mobicom, 2003. [14] L. Hu, D. Evans, Localization for Mobile Sensor Networks, in Proceedings of Mobicom, 2004. [15] Y. Kwon, K. Mechitov, S. Sundresh, W. Kim, G. Agha,
Resilient Localization for Sensor Networks in Outdoor Environments, UIUC Technical Report, 2004. 25 [16] K. Langendoen, N. Reijers, Distributed Localization in Wireless Sensor Networks, A Comparative Study, in Computer Networks vol. 43, 2003. [17] K. Lorincz, M. Welsh, MoteTrack: A Robust, Decentralized Approach to RF-Based Location Tracking, in Proceedings of Intl. Workshop on Location and Context-Awareness, 2005. [18] M. Maroti, B. Kusy, G. Simon, A. Ledeczi, The Flooding Time Synchronization Protocol, in Proceedings of Sensys,
[19] D. Moore, J. Leonard, D. Rus, S. Teller, Robust Distributed Network Localization with Noisy Range Measurements in Proceedings of Sensys, 2004. [20] R. Nagpal, H. Shrobe, J. Bachrach, Organizing a Global Coordinate System for Local Information on an Adhoc Sensor Network, in A.I Memo 1666. MIT A.I. Laboratory, 1999. [21] D. Niculescu, B. Nath, DV-based Positioning in Adhoc Networks in Telecommunication Systems, vol. 22, 2003. [22] E. Osterweil, T. Schoellhammer, M. Rahimi, M. Wimbrow,
T. Stathopoulos, L.Girod, M. Mysore, A.Wu, D. Estrin, The Extensible Sensing System, CENS-UCLA poster, 2004. [23] B.W. Parkinson, J. Spilker, Global Positioning System: theory and applications, in Progress in Aeronautics and Astronautics, vol. 163, 1996. [24] P.N. Pathirana, N. Bulusu, A. Savkin, S. Jha, Node Localization Using Mobile Robots in Delay-Tolerant Sensor Networks, in Transactions on Mobile Computing, 2004. [25] N. Priyantha, A. Chakaborty, H. Balakrishnan, The Cricket Location-support System, in Proceedings of MobiCom,
[26] N. Priyantha, H. Balakrishnan, E. Demaine, S. Teller,
Mobile-Assisted Topology Generation for Auto-Localization in Sensor Networks, in Proceedings of Infocom, 2005. [27] A. Savvides, C. Han, M. Srivastava, Dynamic Fine-grained localization in Adhoc Networks of Sensors, in Proceedings of MobiCom, 2001. [28] Y. Shang, W. Ruml, Improved MDS-Based Localization, in Proceedings of Infocom, 2004. [29] M. Sichitiu, V. Ramadurai,Localization of Wireless Sensor Networks with a Mobile Beacon, in Proceedings of MASS,
[30] G. Simon, M. Maroti, A. Ledeczi, G. Balogh, B. Kusy, A.
Nadas, G. Pap, J. Sallai, Sensor Network-Base Countersniper System, in Proceedings of Sensys, 2004. [31] R. Stoleru, T. He, J.A. Stankovic, Walking GPS: A Practical Solution for Localization in Manually Deployed Wireless Sensor Networks, in Proceedings of EmNetS, 2004. [32] R. Stoleru, J.A. Stankovic, Probability Grid: A Location Estimation Scheme for Wireless Sensor Networks, in Proceedings of SECON, 2004. [33] R. Szewczyk, A. Mainwaring, J. Polastre, J. Anderson, D.
Culler, An Analysis of a Large Scale Habitat Monitoring Application, in Proceedings of Sensys, 2004. [34] K. Whitehouse, A. Woo, C. Karlof, F. Jiang, D. Culler, The Effects of Ranging Noise on Multi-hop Localization: An Empirical Study, in Proceedings of IPSN, 2005. [35] K. Whitehouse, D. Culler, Calibration as Parameter Estimation in Sensor Networks, in Proceedings of WSNA,
[36] P. Zhang, C. Sadler, S. A. Lyon, M. Martonosi, Hardware Design Experiences in ZebraNet, in Proceedings of Sensys,

Biological sequence comparison (or sequence alignment) is one of the most important problems in computational biology, given the number and diversity of the sequences and the frequency on which it is needed to be solved daily.
SW [14] is an exact algorithm that finds the best local alignment between two sequences of size n in quadratic time and space. In genome projects, the size of the sequences to be compared is constantly increasing, thus an O(n2 ) solution is expensive. For this reason, heuristics like BLAST [3] were proposed to reduce execution time.
The popularity of the Internet made possible the interconnection of millions of powerful machines in a global scale.
This led to the idea of grid computing, which involves cooperative and secure sharing of non-dedicated and heterogeneous resources that are geographically distributed [5].
Resource scheduling is one of the most important components of a grid system. The choice of the best resources for a particular application is called task allocation, which is an NP-Complete problem. Grid applications usually do not have high communication rates and many of them follow the master/slave model [13]. In order to schedule master/slave applications many task allocation policies were proposed such as Self Scheduling [15] and FAC2 [8]. The choice of the best allocation policy depends on the application access pattern and on the environment in which it runs [13].
In this paper, we propose PackageBLAST, an adaptive multi-policy grid service to run BLAST searches in grids composed by segmented genetic databases. PackageBLAST executes on Globus 3 [4] and, by now, provides five allocation policies. Also, we propose an adaptive mechanism to assign weights to the grid nodes, taking into account their current workload. As far as we know, this is the first grid service that runs BLAST with multiple task policies with a segmented database in a heterogeneous non-dedicated platform.
This paper is organized as follows. Section 2 presents the sequence comparison problem and the BLAST algorithm.
Section 3 describes allocation policies for grids. Section 4 discusses related work. Section 5 presents the design of PackageBLAST. Experimental results are discussed in section 6. Section 7 concludes the paper.
To compare two sequences, we must find the best alignment, which is to place one sequence above the other making clear the correspondence between similar characters [7].
Given an alignment between two sequences, a score is usually associated for it as follows (figure 1). For each column, we associate, for instance, +1 if the two characters are identical, -1 if the characters are different and -2 if one of them is a space. The score is the sum of all the values and the maximal score is the similarity between the sequences.
To compute exact local sequence alignments, [14] proposed an algorithm (SW), based on dynamic programming, with quadratic time and space complexity.
Usually, one given biological sequence is compared against thousands or even millions of sequences that compose genetic data banks. By now, there are millions of entries composed of billions of nucleotides at GenBank, which is one of the most important public gene repositories. Due to the 156 G A C G G A T T A G G A T C G G A A T A G +1 +1 −2 +1 +1 +1 +1 −1 +1 +1 +1 Σ = 6 Figure 1: Example of an alignment with score 6 current growth rate, these databases will soon achieve terabytes.
In this scenario, the use of exact methods such as SW is prohibitive. For this reason, faster heuristic methods are proposed which do not guarantee that the best alignment will be produced. Usually, these heuristic methods are evaluated using the concepts of sensitivity and sensibility.
Sensitivity is the rate at which the method fails to identify similar sequences whereas sensibility is the rate at which the method identifies sequences that are not similar [7]. BLAST [1] is the most widely used heuristic method for sequence comparison.
BLAST (Basic Local Alignment Tool) [1] is a set of programs used to search DNA and protein databases for similarities between sequences. It is designed to obtain high performance with low impact in terms of sensibility. BLAST provides programs to compare many combinations of query and database sequence types (table 1).
Table 1: Some of the BLAST family programs Program Database Query Translation BLASTN Nucleotide Nucleotide None BLASTP Protein Protein None BLASTX Protein Nucleotide Query The first version of BLAST searched for local similarities without taking spaces (gaps) into account. In 1996-1997, two gapped versions of BLAST emerged: NCBI-BLAST [3] and WU-BLAST [6].
Basically, the algorithm proceeds in three steps: seeding, extension and evaluation. In the seeding step, a query sequence is split in portions called words of size W. These words are matched to database sequences and used as alignment seeds if their scores are higher than a threshold T.
In the extension step, alignments are generated from seeds.
A parameter X maintains the recent alignment history and controls this step. Once seeds are extended, the last step begins. The alignments are evaluated to determine if they are statistically significant. The significant ones are termed HSPs (High-scoring Segment Pairs). A new parameter, S, is used to sort alignments. The combination of parameters W, T, X and S is used to determine the sensitivity and speed of BLAST searches.
Grid Computing was initially developed to enable resource sharing between scientific institutions who needed to share data, software and computational power. The Globus Toolkit [4] emerged as an open source project and quickly became a de facto standard for grid computing infrastructure. Globus implements a set of protocols, APIs and services used by hundreds of grid applications all over the world.
In 2002, the Open Grid Services Architecture (OGSA) was introduced by the Global Grid Forum (GGF) to expand standardization. OGSA provided a new architecture for grid applications based on web services in order to achieve interoperability using industry standards. Many OGSA architecture implementations were developed, including one for Globus. The work carried out in this paper is deployed on a grid based on Globus (GT3).
Usually, grid applications are modelled as master/slave, where one problem is divided in many independent work units (tasks) of smaller size that can be distributed to slave nodes for parallel processing.
A very important problem to be solved in this context is task allocation. The task allocation problem consists of assigning tasks to processors in order to maximize system performance [13]. In this problem, it is assumed that no precedence relations exist among the tasks.
Given a master/slave application composed by a master m and S slaves, the allocation function allocate(m, si, N, S) determines how many tasks out of N must be assigned to a slave si (equation 1), where A(N, S) represents an allocation policy. WeightFactor(m, si, S) was defined by [13] (equation 2) and provides weights for each slave si, based on its statically known processing rate (WorkerRate). allocate(m, si, N, S) = A(N, S) ∗ W eightF actor(m, si, S) (1) W eightF actor(m, si, S) = P ∗ W orkerRate(m, si) P i=1 W orkerRate(m, si) (2) The following subsections present some work allocation policies, which are instances A(N, S) of equation 1.
The Fixed [13] strategy distributes all work units uniformly to slaves nodes. This strategy is appropriate for homogeneous systems with dedicated resources (equation 3).
A(N, S) = N S (3)
Self Scheduling (SS) [15] distributes a single work unit to every slave node (equation 4).
A(N, S) = 1, while work units are still left to allocate (4) In SS, the maximum imbalance is limited by the processing time of a work unit in the slowest node. Nevertheless,
SS usually demands a lot of communication, since each work unit retrieval requires one interaction with the master.
Trapezoidal Self-Scheduling (TSS) [16] allocates work units in groups with a linearly decreasing size. This strategy uses two variables, steps and δ, that represent the total number of allocation steps and the block reduction factor, respectively (equations 5 and 6). steps = 4NS N + 2S (5) 157 δ = N − 2S 2S(steps − 1) (6) TSS calculates the length of the sth block using the difference between the length of the first block and total reduction from the last s − 1 blocks (equation 7).
A(s, N, S) = max N 2S − [(s − 1) ∗ δ] , 1 (7)
Guided Self-Scheduling (GSS) [11] allocates work units in groups whose length decrease exponentially. Its goal is to create a tradeoff between the number of the work units processed and the imbalance in finishing times (equation 8).
A(s, N, S) = max N 1 − 1 S s−1 S , 1 , s > 0 (8)
FAC2 allocates work units in cycles consisting of S allocation sequences. Equation 9 shows the function that defines the cycle number of an iteration s. In FAC2, half of the remaining work units are allocated in each round (equation 10). round(s) = (s − 1) S + 1 (9) A(s, N, S) = max N S ∗ 2round(s) , 1 (10)
MpiBLAST [2] was proposed for clusters and has two phases. First, the genetic database is segmented. Then, the queries are evenly distributed among the nodes. If the node does not have a database fragment, a local copy is made. A method is proposed that associates data fragments to nodes, trying to minimize the number of copies.
BLAST++ [10] groups multiple sequences to reduce the number of database accesses. A master/slave approach is used that allocates the queries to the slaves according to the fixed policy (section 3.3). Each worker executes BLAST++ independently and, finally, the results are collected and combined by the master.
GridBlast [9] is a master/slave grid application that uses Globus 2. It distributes sequences among the grid nodes using two allocation policies: FCFS and minmax. Of those, only the last one takes into account the current load and the heterogeneity of the environment. However, to use minmax, the total execution time of each BLAST task must be known.
Having decided which sequences will be compared by each node, GridBlast sends the sequences, the executable files and the whole database to the chosen node. When the search finishes, the results are compacted and sent to the master.
Grid Blast Toolkit (GBTK) [12] is a web portal to execute BLAST searches in Globus 3. All genetic databases are statically placed on the grid nodes (without replication). GBTK is a master/slave application that receives the sequences and the name of the genetic database. It then verifies if the node that contains the database is available. If so, it is selected.
If the node is not available, the less loaded node is chosen and the database is copied to it.
Master SlaveSlaveSlave Internet database segment but only part of it is processed in each node The database is replicated in the nodes,
Figure 2: PackageBLAST segmentation and distribution mechanism.
We propose an adaptive task allocation framework which is a grid service to perform BLAST searches against sequence database segments. The framework, called PackageBLAST, provides an infrastructure to choose or incorporate allocation strategies in a master/slave application. We also propose a strategy to compute grid nodes execution weight which distributes work units (database segments) to grid nodes according to their current computational power.
Segmentation consists in the division of a database archive in many portions of smaller size, called segments, that can be processed independently. It enables grid nodes to search smaller parts of a sequence database, reducing the number of disk accesses and hence improving BLAST performance.
Also, a single query sequence can be compared against all segments in parallel. Just as in mpiBLAST (section 4), we decided to use database segmentation in PackageBLAST with an NCBI tool called formatdb, which was modified to generate more database segments of smaller size.
We opted to replicate the segmented database in every slave grid node to improve data accesses times and to provide a potential for fault tolerance. Figure 2 illustrates this.
As [13], we think that no allocation policy will produce the best results for every situation. Thus, we propose the use of a framework where many allocation policies can be incorporated. By now, our framework contains five allocation policies: Fixed, SS, GSS, TSS, FAC2, all described in section 3. So, the user can choose or even create the allocation policy which is the most appropriate to his/her environment and his/her BLAST parameters.
Besides that, we propose PSS (Package Weighted Adaptive Self-Scheduling), a new strategy that adapts the chosen allocation policy to a grid with local workload. Considering the heterogeneity and dynamic characteristics of the grid,
PSS is able to modify the length of the work units during execution, based on average processing time of each node.
The expression used for work unit allocation is shown in equation 11, where A(N, P) is the allocation policy for a system with N workload units and P nodes and Φ(m, pi, P) is the weight calculated by PSS. A(N, P) can be a pre-defined allocation policy or a user-defined one. 158 allocate(m, pi, N, P ) = A(N, P ) ∗ Φ(m, pi, P ) (11) To distribute database segments to nodes, the master analyzes periodic slave notifications. The expression used is Φ(m, pi, P) (equation 12), defined as the weighted mean from the last Ω notifications sent by each pi slave node.
Φ(m, pi, P ) = P ∗ P i=1 Γ(m,pi,Ω) Γ(m,pi,Ω) P i=1 P i=1 Γ(m,pi,Ω) Γ(m,pi,Ω) (12) Γ(m, pi, Ω) (equation 13) specifies the average computing time of a segment in a node pi, considering the last Ω notifications of TE(m, pi, τ), which is the average computation time of τ work units (database segments) assigned by the master m to a slave pi. At the moment of computation of Γ, if there is not enough notifications of TE, the calculation is done with total k notifications already received.
Γ(m, pi, Ω) = min(Ω,k) j=1 T E(m, pi, τ) min(Ω, k) (13)
PackageBLAST was designed as a grid service over Globus 3, based on Web Services and Java. Figure 3 presents the PackageBLAST architecture.
BLAST receives MASTER Strategies Allocation Work units Generate Work Units Distribute Reports Generate work units (to slaves)reports searches Figure 3: PackageBLAST architecture.
The module Allocation Strategies contains implementations for the pre-defined allocation policies (Fixed, SS, GSS,
TSS and FAC2) and also makes possible the creation of new allocation strategies.
The module Generate Work Units is the core of the PSS mechanism. It calculates the weight of each slave node and decides how many work units will be assigned to a particular slave node, according to the chosen allocation policy.
Distribute Work Units is the module responsible for the communication between the master and slaves nodes. It distributes the work units generated by the previous module and collects the notifications.
Finally, the module Generate Reports obtains the intermediary outputs sent by the slave nodes through file transfer and merges them into a single BLAST output report.
In general, the following execution flow is executed. The user specifies the sequence to be compared and chooses the allocation strategy. The master node starts execution and waits for slave connections. To start processing, a minimum number of slaves must register into the master node, by calling a master grid service. After receiving connections from the slaves, the master notifies them about their initial segments to compare. The slave processes τ database segments and notifies the master, which uses the last Ω notifications to compute the next allocation block size based on the selected allocation strategy and the weight provided by PSS.
Then, the master sends a XML message to the slave informing its new segments to process. This flow continues until all segments are processed.
PackageBLAST was evaluated in a 16-node grid testbed, composed by two laboratories, interconnected by a localarea network. Eleven desktops (P01-11) and a notebook (NB) were used in LABPOS and four desktops (L01-04) were used in LAICO (table 2). All grid nodes used Linux with Globus 3.2.1, NCBI BLAST 2.2.10 and Java VM 1.4.2.
Table 2: Characteristics of the grid testbed.
Node Names CPU Main Memory HD NB 3200 MHz 512 MB 80 GB L01-L03 1700 MHz 256 MB 30 GB L04 350 MHz 160 MB 6 GB P01-P10 1000 MHz 256 MB 20 GB P11 900 MHz 128 MB 20 GB To investigate the performance gains of PackageBLAST, we executed BLASTX in 2, 4, 8 and 16 grid nodes. Each BLAST search compared a 10KBP real DNA sequence against the 1.2GB nr genetic database segmented in 167 parts of 5MB each. Fixed, SS, TSS, GSS and FAC2 allocation strategies were employed in the tests. Execution times for all allocation strategies are presented in table 3.
Table 3: Execution times for BLASTX.
Strategy 2 nodes 4 nodes 8 nodes 16 nodes FIXED 2037 999 491 252 SS 1112 514 246 134 TSS 1296 570 259 143 GSS 1115 535 250 127 FAC2 1187 514 266 142 Table 4 presents execution times in a single machine and absolute speedups for 2, 4, 8 and 16 nodes, considering the best execution time for a given number of nodes. To calculate the absolute speedups, the BLAST sequential version was executed with the nr unsegmented database.
Table 4: Sequential execution times and speedups.
Node SeqTime 2 nodes 4 nodes 8 nodes 16 nodes NB 1432 1.29 2.79 5.82 11.28 L01 1585 1.43 3.08 6.44 12.48 P01 1853 1.67 3.61 7.53 14.59 P11 2004 1.80 3.90 8.15 15.78 L04 3810 3.43 7.41 15.49 30.00 PackageBLAST achieved very good speedups.
Considering the worst (L04), average (P01) and best (NB) node in the grid, the speedups obtained were superlinear, close to linear and sublinear, respectively.
In table 3, it can also be noticed that there is no allocation strategy that always reaches the best execution time. This variation justifies the allocation framework provided.
To evaluate PSS, we executed PackageBLAST with 16 grid nodes, introducing local workload in nodes L01, L02,
P01 and P02. The load was started simultaneously 30 seconds after the beginning of BLAST and consisted of the 159 execution of formatdb on the nr database. Three scenarios were simulated (table 5): 1) with PSS strategy, but without workload; 2) with PSS strategy and workload (PSS 2x), to use grid environment knowledge obtained in the preceeding iteration; and 3) Execution without PSS and with workload.
Table 5: PSS evaluation with local workload. Gain is the comparison of without PSS with PSS 2x Strategy with PSS PSS 2x without PSS Gain Fixed 316 184 393 113.59% SS 186 177 179 1.13% TSS 160 162 171 5.56% GSS 149 159 339 113.21% FAC2 156 165 153 -7.27% As expected, the allocation strategies that assign a large amount of work to the nodes (fixed and GSS) obtained great benefit from using PSS. This is due to the fact that a slow node can easily become a bottleneck in these strategies. TSS also obtained a reduction of 5.56% in its execution time.
PSS uses two parameters: τ and Ω (section 5.2). We varied these parameters in order to evaluate the PSS behavior in two scenarios. In both cases, we used a four-node (NB,
L01, P01, L04) grid. In the first experiment, a local workload (formatdb) was introduced when the last task of the first TSS allocation starts and was stopped immediately after the processing of one segment. The goal was to evaluate the impact of short-lived local tasks in PSS. In the second case, local workload was introduced at the same time of the previous case, but continued until the end. The goal was to evaluate long-lived local tasks. Figure 4 presents the gains.
Figure 4: Percentual gain obtained by PSS varying τ and Ω parameters.
In scenario 1, when a very recent history is considered (τ=1 and Ω=1), PSS tries to adapt to a situation that will shortly disappear. For τ=5 and Ω=4, PSS takes longer to notice modification and short-lived tasks have low impact.
On the other hand, in scenario 2, τ=1,Ω=1 presents better results than τ=5, Ω=4, because it changes weights faster.
In this article, we proposed and evaluated PackageBLAST, an adaptive multi-policy grid service to execute master/slave BLAST searches. PackageBLAST contains a framework where the user can choose or incorporate allocation policies.
We also defined a strategy, PSS, that adapts the chosen policy to a heterogeneous non-dedicated grid environment.
The results collected by running PackageBLAST with 5 allocation policies in a grid testbed were very good. In order to compare a 10KBP real DNA sequence against the nr genetic database, we were able to reduce execution time from 30.88 min to 2.11 min. Also, we showed that, in our testbed, there is no allocation policy that always achieves the best performance and that makes evident the importance of providing multiple policies. Moreover, we showed that the introduction of PSS led to very good performance gains for some policies.
As future work, we intend to run PackageBLAST in a geographically dispersed grid, to evaluate the impact of high network latencies in the allocation policies and in PSS. Also, we intend to provide support for genomic database synchronization and dynamic join/leave operations for slaves.
[1] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J. Lipman. A basic local alignment search tool.
Journal of Molecular Biology, 215:403-410, 1990. [2] A. Darling, L. Carey, and W. Feng. The design, implementation, and evaluation of mpiblast. 4th International Conference on Linux Clusters, 2003. [3] S. F. A. et al. Gapped blast and psi-blast: a new generation of protein database search programs.
Nucleic Acids Research, 25(17):3389-3402, 1997. [4] I. Foster and C. Kesselman. Globus: A metacomputing infrastructure toolkit. International Journal of Supercomputer Applications, 11(2):115-128, 1997. [5] I. Foster and C. Kesselman. The Grid: Blueprint of a Future Computing Infrastructure. Morgan-Kauffman,
[6] W. Gish. Washington university blast. http://blast.wustl.edu, 1996-2002. [7] D. Gusfield. Algorithms on Strings, Trees and Sequences. Cambridge University Press, 1997. [8] S. F. Hummel, E. Schonberg, and L. E. Flynn.
Factoring: A method for scheduling parallel loops.
Communications of the ACM, 35(8):90-101, 1992. [9] A. Krishnan. Gridblast: High throughput blast on the grid. Symposium on Biocomputing, January 2003. [10] D. Peng., W. Yan, and Z. Lei. Parallelization of blast++. Technical report, Singapore-MIT, 2004. [11] C. D. Polychronopoulos and D. J. Kuck. Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. IEEE Transactions on Computers, 36(12):1425-1439, Dec. 1987. [12] M. K. Satish and R. R. Joshi. Gbtk: A toolkit for grid implementation of blast. 7th International Conference HPCAsia, pages 378-382, 2004. [13] G. Shao. Adaptive Scheduling of Master/Worker Applications on Distributed Computational Resources.
PhD thesis, Univ. California at San Diego, 2001. [14] T. Smith and M. Waterman. Identification of common molecular subsequences. J. Mol. Biol., 147:195-197,

Elucidation of the stable conformations and the folding process of proteins is one of the most fundamental and challenging goals in life science. While some of the most common secondary structures (e.g., certain types of helix, the beta-strand, and the coil) are well known, precise analysis of the thousands of chemically important conformers and pico-second-order analysis of their conformational interconversions via the transition states on the potential energy surface are required for the microsecond-order investigation of the folding process toward the tertiary structure formations.
Recently, the concept of the computational grid has begun to attract significant interest in the field of high-performance network computing. Rapid advances in wide-area networking technology and infrastructure have made it possible to construct large-scale, high-performance distributed computing environments, or computational grids, that provide dependable, consistent and pervasive access to enormous computational resources.
CONFLEX is one of the most efficient and reliable conformational space search programs[1]. We have applied this 154 program to parallelization using global computing. The performance of the parallelized CONFLEX enables exploration of the lower-energy region of the conformational space of small peptides within an available elapsed time using a local PC cluster. Since trial structure optimization in CONFLEX is calculated via molecular mechanics, conformational space search can be performed quickly compared to that using molecular orbital calculation.
Although the parallelized version of CONFLEX was used to calculate in parallel the structure optimization, which takes up over 90% of the processing in the molecular conformation search, sufficient improvement in the speedup could not be achieved by this method alone. Therefore, for high polymers from live organisms, such as HIV protease, the use one PC cluster is insufficient due to the requirement for optimization of a huge number of trial structures. This requires the vast computer resources of a grid computing environment.
In this paper, we describe CONFLEX-G, a grid-enabled molecular conformational search program, using OmniRPC and report its performance in a grid of several PC clusters which are geographically distributed. The prototype CONFLEX-G allocates calculation trial structures optimization, which is a very time-consuming task, to worker nodes in the grid environment in order to obtain high throughput.
In addition, we compare the performance of CONFLEX-G in a local PC cluster to that in a grid testbed.
OmniRPC[2, 3, 4] is a thread-safe implementation of Ninf RPC[5, 6] which is a Grid RPC facility for grid environment computing. Several systems adopt the concept of the RPC as the basic model for grid environment computing, including Ninf-G[7], NetSolve[8] and CORBA[9]. The RPCstyle system provides an easy-to-use, intuitive programming interface, allowing users of the grid system to easily create grid-enabled applications. In order to support parallel programming, an RPC client can issue asynchronous call requests to a different remote computer to exploit networkwide parallelism via OmniRPC.
In this paper, we propose the OmniRPC persistence model to a Grid RPC system and demonstrate its effectiveness. In order to support a typical application for a grid environment, such as a parametric search application, in which the same function is executed with different input parameters on the same data set. In the current GridRPC system[10], the data set by the previous call cannot be used by subsequent calls. In the OmniRPC system, once a remote executable is invoked, the client attempts to use the invoked remote executable and its initialized state for subsequent RPC calls to the same remote functions in order to eliminate the invocation cost of each call.
This paper demonstrates that CONFLEX-G is able to exploit the huge computer resources of a grid environment and search large-scale molecular conformers. We demonstrate CONFLEX-G on our grid testbed using the actual protein as a sample molecule. The OmniRPC facility of the automatic initializable module (AIM) allows the system to efficiently calculate numerous conformers. Furthermore, by using OmniRPC, the user can grid-parallelize the existing application, and move from the cluster to the grid environment without modifying program code and compiling the program. In addition, the user can easily build a private grid environment.
The rest of this paper is organized as follows. An overview Selection of Initial Structure Conformations Database Local Perturbation Geometry Optimization Comparison and Registration Figure 1: Algorithm of conformational space search in the original CONFLEX. of the CONFLEX system is presented in Section2, and the implementation and design of CONFLEX-G are described in Section 3. We report experimental results obtained using CONFLEX-G and discuss its performance in Section 4. In Section 6, we present conclusions and discuss subjects for future study.
CONFLEX [1] is an efficient conformational space search program, which can predominately and exhaustively search the conformers in the lower-energy region. Applications of CONFLEX include the elucidation of the reactivity and selectivity of drugs and possible drug materials with regard to their conformational flexibility.
The basic strategy of CONFLEX is an exhaustive search of only the low-energy regions. The original CONFLEX performs the following four major steps:
discovered unique conformers sorted in a conformational database. (An input structure is used as the first initial structure at the beginning of a search execution only.)
to the selected initial structure.
structures.
structures with the other conformers stored in a conformation database, and preservation of newly discovered unique conformers in the database.
Figure 1 shows the outline of CONFLEX, the original conformational space search algorithm.
These procedures incorporate two unique strategies.
Figure 2 shows the strategies for generating local perturbations in CONFLEX. The first strategy involves both corner flapping and edge flipping for the ring atoms and stepwise rotation for side-chains or backbone chains. These methods provide a highly efficient way to produce several good trial structures. These perturbations can be considered to mimic 155 Stepwise Rotation Corner Flap Edge Flip Figure 2: Strategies used to generate the local perturbations. a barrier-crossing step in the elementary process of the thermal conformational inter-conversion. Actually, the perturbations of an initial structure correspond to the precise performance around the space of the initial structure because of localization and weakness of the perturbation.
The selection rule of the initial structure, the LowestConformer-First rule, is the second strategy for directing the conformation search expanded to the low-energy regions.
The initial structure is selected as the set of lowest energy conformers stored in the conformation database. This rule is effective in moving down the search space toward lower energy regions, like water from a stream running into an empty reservoir, while filling local depressions along the way. Therefore, these tactical procedures of the CONFLEX search are referred to as the Reservoir Filling Algorithm.
In order to remain in the low-energy region and perform an exhaustive search, the search limit (SEL), which determines the maximum energy of the initial structures, is pre-defined. Gradually increasing SEL allows only the lowenergy conformers to be searched and avoids straying into unnecessarily high-energy regions.
For application to over 100 atoms, CONFLEX was improved using high-performance parallel computing techniques.
In the CONFLEX search algorithm, the geometry optimization procedures always take 95% of the elapsed time of the search execution. Therefore, we parallelized this optimization using the Master/Worker parallelization technique.
We modified the search procedures as follows. After trial structures are generated (step 2), they are temporarily stored in a task pool on the master node. Then, each worker node is dynamically supplied with one trial structure from the master node. After an optimization on a worker node is finished, the worker is immediately supplied with another trial structure. When all of the trial structures related to a given initial structure are optimized, only the master procedure is used in comparison. By parallelizing CONFLEX, the speedup of searching molecular conformers obtained is as reported in[11].
Originally, CONFLEX was intended for use in exploring the conformers of the large bio-molecules, such HIV protease. In such molecules, the number of trial structures increases and the time required for optimization of RPC Selection of Initial Structure Conformations Database Local Perturbation Comparison and Registration Client program Task Pool of Geometry Optimization RPC RPC Grid environment Cluster B Cluster A Cluster C Trial structureTrial structure Trial structure Trial structure Figure 3: Procedure of CONFLEX-G. agent rexrex rex Client jones.tsukuba.ac.jp hpc-serv.hpcc.jp hpc1 hpc2 hpc3 Agent invocation communicationNetwork Figure 4: Overview of the OmniRPC system for the remote cluster having a private IP address. the trial structure becomes immense. We implemented the parallelized version of CONFLEX, which cannot treat such molecules using only a local PC cluster.
In order to exploit the vast computing resources of a grid environment, we designed and implemented CONFLEX-G, which is a grid-enabled version of CONFLEX, with the OmniRPC system. CONFLEX-G allocates jobs to optimize a trial structure to the computational nodes of each cluster in the grid environment. Figure 3 shows the process of CONFLEX-G. The worker programs are initialized by the initialize method, which is provided by the OmniRPC AIM facility at worker invocation. At each RPC call, the initialized state is reused on the remote host. In other words, the client program can eliminate the initialization for each RPC call, and can therefore optimize trial structures efficiently.
OmniRPC is a Grid RPC system which allows seamless parallel programming from a PC cluster to a grid environment. OmniRPC inherits its API and basic architecture from Ninf. A client and the remote computational hosts which execute the remote procedures may be connected via a network. The remote libraries are implemented as an executable program which contains a network stub routine as its main routine. We call this executable program a remote executable program (rex).
When the OmniRPC client program starts, the initialization function of OmniRPC system invokes the OmniRPC agent program omrpc-agent in the remote hosts listed in the host file. To invoke the agent, the user can use the remote shell command rsh in a local-area network, the GRAM (Globus Resource Allocation Manager) API of the Globus 156 toolkit[12] in a grid environment, or the secure remote shell command ssh. The user can switch the configurations only by changing the host file.
OmniRpcCall is a simple client programming interface for calling remote functions. When OmniRpcCall makes a remote procedure call, the call is allocated to an appropriate remote host. When the client issues the RPC request, it requests that the agent in the selected host submit the job of the remote executable with the local job scheduler specified in the host file. If the job scheduler is not specified, the agent executes the remote executable in the same node by the fork system call. The client sends the data of the input arguments to the invoked remote executable, and receives the results upon return of the remote function. Once a remote executable is invoked, the client attempts to use the invoked remote executable for subsequent RPC calls in order to eliminate the cost of invoking the same remote executable again.
When the agent and the remote executables are invoked, the remote programs obtain the client address and port from the argument list and connect back to the client by direct TCP/IP or Globus-IO for data transmission. Because the OmniRPC system does not use any fixed service ports, the client program allocates unused ports dynamically to wait for connection from the remote executables. This avoids possible security problems, and allows the user to install the OmniRPC system without requiring a privileged account.
Herein, a typical grid resource is regarded as a cluster of geographically distributed PC clusters. For PC clusters on a private network, an OmniRPC agent process on the server host functions as a proxy to relay communications between the client and the remote executables by multiplexing the communications using a single connection.
This feature, called multiplex IO (MXIO), allows a single client to use up to 1,000 remote computing hosts. When the PC cluster is inside a firewall, the port forwarding of SSH enables the node to communicate to the outside with MXIO.
Figure 4 shows the overview of the OmniRPC system for a remote cluster with a private IP address.
For parallel programming, the programmer can use asynchronous remote procedure calls, allowing the client to issue several requests while continuing with other computations.
The requests are dispatched to different remote hosts to be executed in parallel, and the client waits or polls the completed request. In such a programming model with asynchronous remote procedure calls, the programmer should handle outstanding requests explicitly. Because OmniRPC is a thread-safe system, a number of remote procedure calls may be outstanding at any time for multi-threaded programs written in OpenMP.
initializable module OmniRPC efficiently supports typical Master/Worker parallel applications such as parametric execution programs.
For parametric search applications, which often require large amount of identical data for each call, OmniRPC supports a limited persistence model, which is implemented by the automatic initializable module. The user can define an initialization procedure in the remote executable in order to send and store data automatically in advance of actual remote procedure calls. Since the remote executable may accept requests for subsequent calls, the data set which has been set by the initialization procedure can be re-used. As a result, the worker program can execute efficiently and reduce the amount of data transmitted for initialization.
Once a remote executable is invoked, the client attempts to use the invoked remote executable for subsequent RPC calls. However, OmniRPC does not guarantee persistence of the remote executable, so that the data set by the previous call cannot be used by subsequent calls. This is because a remote call by OmniRpcCall may be scheduled to any remote host dynamically, and remote executables may be terminated accidentally due to dynamic re-scheduling or host faults. However, persistence of the remote executable can be exploited in certain applications. An example is a parametric search application: in such an application, it would be efficient if a large set of data could be pre-loaded by the first call, and subsequent calls could be performed on the same data, but with different parameters. This is the case for CONFLEX.
OmniRPC provides a restricted persistence model through the automatic initializable module (AIM) in order to support this type of application. If the initialization procedure is defined in the module, the module is automatically initialized at invocation by calling the initialization procedure. When the remote executable is re-scheduled in different hosts, the initialization is called to initialize the newly allocated remote module. This can eliminate unnecessary communications when RPC calls use the same data.
To reveal more about the difference in progress between the cases with OmniRPC AIM and without OmniRPC AIM, we present two figures. Figure 5 illustrates the time chart of the progress of a typical OmniRPC application using the OmniRPC AIM facility, and Figure 6 illustrates the time chart of the same application without the OmniRPC AIM facility. In both figures, the lines between diamonds represent the processes of initialization, and the lines between points represent the calculation. The bold line indicates the time when the client program sends the data to a worker program. It is necessary for the application without the OmniRPC AIM facility to initialize at each RPC. The application using the OmniRPC AIM facility can re-use the initialized data once the data set is initialized. This can reduce the initialization at each RPC. The workers of the application with the AIM can calculate efficiently compared to the application without the OmniRPC AIM facility.
OmniRPC Figure 3 shows an overview of the process used in CONFLEXG. Using RPCs, CONFLEX-G allocates the processes of trial structure optimization, which are performed by the computation nodes of a PC cluster in the MPI version of CONFLEX, to the computational nodes of each cluster in a grid environment. There are two computations which are performed by the worker programs in CONFLEX-G. One is the initialization of a worker program, and another is the calculation of trial structure optimization.
First, the OmniRPC facility of the AIM is adapted for initialization of a worker program. This facility automatically calls the initialization function, which is contained in the worker program, once the client program invokes the worker program in a remote node. It is necessary for the common RPC system including GridRPC to initialize a program for every RPC call, since data persistence of worker programs 157 time Client Program Worker Program 1 Worker Program 2 initialization initialization calculation calculation calculation calculation calculation Parallelized using asynchronous RPCs Figure 5: Time chart of applications using the OmniRPC facility of the automatic initializable module. time Client Program Worker Program 1 Worker Program 2 initialization initializationcalculation calculation calculation calculation initialization initialization initialization Parallelized using asynchronous RPCs calculation Figure 6: Time chart of applications without the OmniRPC facility of the automatic initializable module.
Table 1: Machine configurations in the grid testbed.
Site Cluster Name Machine Network Authentication # of Nodes # of CPUs Univ. of Tsukuba Dennis Dual Xeon 2.4GHz 1Gb Ethernet Globus, SSH 14 28 Alice Dual Athlon 1800+ 100Mb Ethernet Globus, SSH 18 36 TUT Toyo Dual Athlon 2600+ 100Mb Ethernet SSH 8 16 AIST Ume Dual Pentium3 1.4GHz 1Gb Ethernet Globus, SSH 32 64 is not supported. In OmniRPC, however, when the Initialize remote function is defined in the worker program and a new worker program, corresponding to the other RPC, is assigned to execute, an Initialize function is called automatically. Therefore, after the Initialize function call to set up common initialization data, a worker program can re-use this data and increase the efficiency of it"s processes. Thus, the higher the set-up cost, the greater the potential benefit.
We implemented the worker program of CONFLEX-G to receive data, such as evaluation parameters of energy, from a client program and to be initialized by the Initialize function. We arranged the client program of CONFLEX-G to transfer the parameter file at the time of worker initialization. This enables execution to be performed by modify only the client setting if the user wants to run CONFLEX-G with a different data set.
Second, in order to calculate trial structure optimization in a worker program, the worker program must receive the data, such as the atom arrangement of the trial structure and the internal energy state. The result is returned to the client program after the worker has Optimized the trial structure.
Since the calculation portion of the structure optimization in this worker program can be calculated independently using different parameters, we parallelized this portion using asynchronous RPCs on the client side. To call the structure optimization function in a worker program from the client program, we use the OmniRpcCallAsync API, which is intended for asynchronous RPC. In addition, OmniRpcCallWaitAll API which waits until all asynchronous RPCs are used in order to perform synchronization with all of the asynchronous RPCs completed so as to optimize the trial structure. The client program which assigns trial structure optimization to the calculation node of a PC cluster using RPC is outlined as follows.
OmniRpcInit() OmniRpcModuleInit("conflex_search",...); ... while( <new conformers> ) { foreach( <trial structures> ) OmniRpcCallAsync("conflex_search_worker", ...); OmniRpcWaitAll(); ...
Note that OmniRpcModuleInit API returns only the arguments needed for initialization and will not actually execute the Initialization function. As described above, the actual Initialization is performed at the first remote call.
Since the OmniRPC system has an easy round-robin scheduler, we do not have to explicitly write the code for load balance. Therefore, RPCs are allocated automatically to idle workers. 158 Table 2: Network performance between the master node of the Dennis cluster and the master node of each PC cluster.
Round-Trip Throughput Cluster Time (ms) (Mbps) Dennis 0.23 879.31 Alice 0.18 94.12 Toyo 11.27 1.53 Ume 1.07 373.33
The grid testbed was constructed by computing resources at the University of Tsukuba, the Toyohashi University of Technology (TUT) and the National Institute of Advanced Industrial Science and Technology (AIST). Table 1 shows the computing resources used for the grid of the present study.
The University of Tsukuba and AIST are connected by a 1-Gbps Tsukuba WAN, and the other PC clusters are connected by SINET, which is wide-area network dedicated to academic research in Japan. Table 2 shows the performance of the measured network between the master node of the Dennis cluster and the master node of each PC cluster in the grid testbed. The communication throughput was measured using netperf, and the round-trip time was measured by ping.
In all of the CONFLEX-G experiments, the client program was executed on the master node of the Dennis cluster at the University of Tsukuba. The built-in Round-Robin scheduler of OmniRPC was used as a job scheduler.
SSH was used for an authentication system, the OminRPC"s MXIO, which relays the I/O communication between client program and worker programs by port forwarding of SSH was, not used. Note that one worker program is assigned and performed on one CPU of the calculation node in a PC cluster. That is, the number of workers is equal to the number of CPUs.
These programs were compiled by the Intel Fortran Compiler 7.0 and gcc 2.95. MPICH, Version 1.2.5, was used to compare the performance between CONFLEX MPI and CONFLEX-G. In order to demonstrate the usability of the OmniRPC facility of the AIM, we implemented another version of CONFLEX-G which did not utilize the OmniRPC facility. The worker program in this version of CONFLEXG must be initialized at each RPC because the worker does not hold the previous data set.
In order to examine the performance of CONFLEX-G, we selected two peptides and two small protein as test molecules: • N-acetyl tetra-alanine methylester (AlaX04) • N-acetyl hexdeca-alanine methylester (AlaX16) • TRP-cage miniprotein construct TC5B (1L2Y) • PTH receptor N-terminus fragment (1BL1) Table 3 lists the characteristics of these sample molecules.
The column trial structure / loops in this table shows the Figure 7: Performances of CONFLEX-G,
CONFLEX MPI and Original CONFLEX in the Dennis cluster.
Figure 8: Speedup ratio, which is based on the elapsed time of CONFLEX-G using one worker in the Dennis cluster.
Figure 9: Performance of CONFLEX-G with and without the OmniRPC facility of automatic initializable module for AlaX16. 159 Table 3: Characteristics of molecules and data transmission for optimizing trial molecular structures in each molecular code.
Molecular # of # of total trial trial structure Data transfer to Data transfer code atoms structures / loop initialize a worker / trial structure AlaX04 181 360 45 2033KB 17.00KB AlaX16 191 480 160 2063KB 18.14KB 1L2Y 315 331 331 2099KB 29.58KB 1BL1 519 519 519 2150KB 48.67KB Table 4: Elapsed search time for the molecular conformation of AlaX04.
Total Total Optimization Cluster # of Structures optimization time Elapsed Speed (# of workers) workers / worker time (s) / structure (s) time (s) up Dennis (sequential) 1 320.0 1786.21 4.96 1786.21 1.00 Toyo (16) 16 20.0 1497.08 4.16 196.32 9.10 Dennis (28) 28 11.4 1905.51 5.29 97.00 18.41 Alice (36) 36 8.9 2055.25 5.71 87.09 20.51 Ume (56) 56 5.7 2196.77 6.10 120.69 14.80 Dennis (28) + Toyo (16) 44 7.3 1630.09 4.53 162.35 11.00 Alice (36) + Toyo (16) 52 6.2 1774.53 4.93 178.24 10.02 Dennis (28) + Alice (36) 64 5.0 1999.02 5.55 81.52 21.91 Dennis (28) + Ume (56) 84 3.8 2085.84 5.79 92.22 19.37 Alice (36) + Ume (56) 92 3.5 2120.87 5.89 101.25 17.64 Table 5: Elapsed search time for the molecular conformation of AlaX16 Total Total Optimization Cluster # of Structures optimization time Elapsed Speed (# of workers) workers / worker time (s) / structure (s) time (s) up Dennis (1) 1 480.0 74027.80 154.22 74027.80 1.00 Toyo (16) 16 30.0 70414.21 146.70 4699.15 15.75 Dennis (28) 28 17.1 74027.80 154.22 3375.60 21.93 Alice (36) 36 13.3 90047.27 187.60 3260.41 22.71 Ume (56) 56 8.6 123399.38 257.08 2913.63 25.41 Dennis (28) + Toyo (16) 44 10.9 76747.74 159.89 2762.10 26.80 Alice (36) + Toyo (16) 52 9.2 82700.44 172.29 2246.73 32.95 Dennis (28) + Alice (36) 64 7.5 87571.30 182.44 2051.50 36.08 Toyo (16) + Ume (56) 72 6.7 109671.32 228.48 2617.85 28.28 Dennis (28) + Ume (56) 84 5.7 102817.90 214.20 2478.93 29.86 Dennis(28)+Ume(56)+Toyo(16) 100 4.8 98238.07 204.66 2478.93 29.86 Table 6: Elapsed time of the search for the trial structure of 1L2Y.
Cluster Total # of Structures Optimization time Elapsed Elapsed Speed (# of workers) workers / worker / structure (s) time (s) time (H) up Toyo MPI (1) 1 331.0 867 286,967 79.71 1.00 Toyo MPI (16) 16 20.7 867 18,696 5.19 15.34 Dennis (28) 28 11.8 803 14,101 3.91 20.35 Dennis (28) + Ume(56) 84 3.9 1,064 8,316 2.31 34.50 Table 7: Elapsed time of the search for the trial structure of 1BL1.
Cluster Total # of Structures Optimization time Elapsed Elapsed Speed (# of workers) workers / worker / structure (s) time (s) time (H) up Toyo MPI (1) 1 519.0 3,646 1892,210 525.61 1.00 Toyo MPI (16) 16 32.4 3,646 120,028 33.34 15.76 Dennis (28) 28 18.5 3,154 61,803 17.16 30.61 Dennis (28) + Ume (56) 84 6.1 4,497 33,502 9.30 56.48 160 number of trial structures generated in each iteration, indicating the degree of parallelism. Figure 3 also summarizes the amount of data transmission required for initialization of a worker program and for optimization of each trial structure. Note that the amount of data transmission, which is required in order to initialize a worker program and optimize a trial structure in the MPI version of CONFLEX, is equal to that of CONFLEX-G. We used an improvement version of MM2 force field to assign a potential energy function to various geometric properties of a group of atoms.
We first compared the performance of CONFLEX-G, the MPI version of CONFLEX, and the original sequential version of CONFLEX-G using a local cluster. We investigated performance by varying the number of workers using the Dennis cluster. We chose AlaX04 as a test molecule for this experiment. Figure 7 compares the results for the CONFLEX MPI and CONFLEX-G in a local PC cluster.
The result of this experiment shows that CONFLEX-G can reduce the execution time as the number of workers increases, as in the MPI version of CONFLEX. We found that CONFLEX-G achieved efficiencies comparable to the MPI version. With 28 workers, CONFLEX-G achieved an
sequential version. The performance of CONFLEX-G without the OmniRPC AIM facility is worse than that of CONFLEXG using the facility, based on the increase in the number of workers. This indicates that the OmniRPC AIM enables the worker to calculate efficiently without other calculations, such initialization or invocation of worker programs.
As the number of workers is increased, the performance of CONFLEX-G is a slightly lower than that of the MPI version. This performance degradation is caused by differences in the worker initialization processes of CONFLEX-G and CONFLEX MPI. In the case of CONFLEX MPI, all workers are initialized in advance of the optimization phase. In the case of OminRPC, the worker is invoked on-demand when the RPC call is actually issued. Therefore, the initialization incurs this overhead.
Since the objective of CONFLEX-G is to explore the conformations of large bio-molecules, the number of trial structures and the time to optimize the trial structure might be large. In such cases, the overhead to invoke and initialize the worker program can be small compared to the entire elapsed time.
First, the sample molecules (AlaX04 and AlaX16) were used to examine the CONFLEX-G performance in a grid environment. Figure 8 shows the speedup achieved by using multiple clusters compared to using one worker in the Dennis cluster. Detailed results are shown in Table 4 and Table 5.
In both cases, the best performance was obtained using 64 workers of the combination of the Dennis and Alice clusters. CONFLEX-G achieved a maximum speedup of 36.08 times for AlaX04 and a maximum speedup of 21.91 times for AlaX16.
In the case of AlaX04, the performance is improved only when the network performance between clusters is high.
However, even if two or more clusters are used in a wide area network environment, the performance improvement was slight because the optimization time of one trial structure generated from AlaX04, a small molecule, is short. In addition, the overhead required for invocation of a worker program and network data transmission consume a large portion of the remaining processing time. In particular, the data transmission required for the initialization of a worker program is 2 MB. In the case of Toyo cluster, where the network performance between the client program and the worker programs is poor, the time of data transmission to the worker program required approximately 6.7 seconds.
Since this transmission time was longer than the processing time of one structure optimization in CONFLEX-G, most of the time was spent for this data transmission.
Therefore, even if CONFLEX-G uses a large number of calculation nodes in a wide area network environment, the benefit of using a grid resource is not obtained.
In the case of AlaX16, CONFLEX-G achieved a speedup by using two or more PC clusters in our grid testbed. This was because the calculation time on the worker program was long and the overhead, such as network latency and the invoking of worker programs, became relatively small and could be hidden. The best performance was obtained using 64 workers in the Dennis and Alice clusters. In the case of AaX16, the achieved performance was a speedup of 36.08 times.
Figure 9 reveals the effect of using the facility of the OmniRPC AIM on CONFLEX-G performance. In most cases,
CONFLEX-G with the OmniRPC AIM facility archived better performance than CONFLEX-G without the facility. In particular, the OmniRPC AIM facility was advantageous when using two clusters connected by a low-performance network. The results indicate that the OmniRPC AIM facility can improve performance in the grid environment.
Finally, we explored the molecular conformation using CONFLEX-G for large molecules. In a grid environment, this experiment was conducted using the Dennis and Ume clusters. In this experiment, we used two proteins, 1L2Y and 1BL1. Table 6 and Table 7 show the performance of CONFLEX-G in the grid environment and that of CONFLEX MPI in the Toyo cluster, respectively. The speedups in these tables were computed respectively based on the performance of one worker and 16 workers of the Toyo cluster using CONFLEX MPI.
CONFLEX-G with 84 workers in Dennis and Ume clusters obtained maximum speedups of 56.5 times for 1L2Y and 34.5 times for 1L2Y. Since the calculation time for structure optimization required a great deal of time, the ratio of overhead, including tasks such as the invocation of a worker program and data transmission for initialization, became very small, so that the performance of CONFLEX-G was improved.
We found that the load imbalance in the processing time of optimization for each trial structure caused performance degradation. When we obtained the best performance for 1L2Y using the Dennis and Ume clusters, the time for each structure optimization varied from 190 to 27,887 seconds, and the ratio between the longest and shortest times was
time was 190. In addition, in order that the worker program wait until the completion of optimization of all trial structures, all worker programs were found to wait in an idle state for approximately 6 hours. This has caused the performance degradation of CONFLEX-G. 161
In this subsection, we discuss the improvement of the performance reflected in our experiments.
Exploiting parallelism - In order to exploit more computational resources, it is necessary to increase the degree of parallelism. In this experiment, the degree of parallelism was not so large in the case of the sample molecules. When using a set of over 500 computing nodes for 1BL1, the number of one trial structures assigned to each worker will be only one or two. If over 100 trial structures are assigned to each worker program, calculation can be performed more efficiently due to the reduction of the overhead for worker invocation and initialization via the facility of the OmniRPC AIM.
One idea for increasing parallelism is to overlap the execution of two or more sets of trial structures. In the current algorithm, a set of trial structures is generated from one initial structure and computed until optimizations for all structures in this set are calculated. Furthermore, this will help to improve load imbalance. By having other sets of trial structures overlap, even if some optimizations require a long time, the optimization for the structures in other sets can be executed to compensate for the idle workers for other optimizations. It is however unclear how such modification of the algorithm might affect the quality of the final results in terms of a conformation search.
Improvement in load imbalance when optimizing each trial structure - Table 8 lists the statistics for optimization times of trial structures generated for each sample molecule measured using 28 workers in the Dennis cluster. When two or more sets of PC clusters are used, the speedup in performance is hampered by the load imbalance of the optimization of the trial structures. The longest time for optimizing a trial structure was nearly 24 times longer than the shortest time. Furthermore, other workers must wait until the longest job has Finished, so that the entire execution time cannot be reduced. When CONFLEX-G searched the conformers of 1BL1 by the Dennis cluster, the longest calculation time of the trial structure optimization made up approximately 80% of the elapsed time.
Therefore, there are two possible solutions for the load Imbalance. • It is necessary to refine the algorithm used to generate the trial structure, which suppresses the time variation for optimizing a trial structure in CONFLEX. This enables CONFLEX-G to achieve high-throughput by using many computer resources. • One of the solutions is to overlap the executions for two or more sets of trial structures. In the current algorithms, a set of trial structures is generated from one initial structure and calculation continues until all structures in this set are calculated. By having other sets of trial structures, even if a structure search takes a long time, a job can be executed in order to compensate the load imbalance by other jobs. However, how such modification of the algorithms might affect the efficiency is not clear. • In this experiment, we used a simple build-in roundrobbin scheduler of OmniRPC, which is necessary in order to apply the scheduler that allocates structures with long optimization times to a high-performance Table 8: Statistics of elapsed time of trial structure optimization using 28 workers in the Dennis cluster.
Molecular Min Max Average Variance code (s) (s) (s) AlaX04 2.0 11.3 5.3 3 AlaX16 47.6 920.0 154.2 5404 1L2Y 114.2 13331.4 803.2 636782 1BL1 121.0 29641.8 3153.5 2734811 node and structures with short optimization times to low-performance nodes. In general, however, it might be difficult to predict the time required for trial structure optimization.
Parallelization of the worker program for speedup to optimize a trial structure - In the current implementation, we do not parallelize the worker program. In order to speed up trial structures, hybrid programming using OmniRPC and OpenMP in an SMP (Symmetric Multiple Processor) machine may be one of the alternative methods by which to improve overall performance.
Recently, an algorithm has been developed that solves the problems of parallelization and communication in poorly connected processors to be used for simulation. The Folding@home project[13] simulates timescales thousands to millions of times longer than previously achieved. This has allowed us to simulate folding for the first time and to directly examine folding related diseases.
SETI@home[14] is a program to search for alien life by analyzing radio telescope signals using Fourier transform radio telescope data from telescopes from different sites.
SETI@home tackles immensely parallel problems, in which calculation can easily be divided among several computers.
Radio telescope data chunks can easily be assigned to different computers.
Most of these efforts explicitly develop a docking application as a parallel application using a special purpose parallel programming language and middleware, such as MPI, which requires development skills and effort. However, the skills and effort required to develop a grid application may not be required for OmniRPC.
Nimrod/G[15] is a tool for distributed parametric modeling and implements a parallel task farm for simulations that require several varying input parameters. Nimrod incorporates a distributed scheduling component that can manage the scheduling of individual experiments to idle computers in a local area network. Nimrod has been applied to applications including bio-informatics, operations research, and molecular modeling for drug design.
NetSolve[8] is an RPC facility similar to OmniRPC and Ninf, providing a similar programming interface and automatic load balancing mechanism. Ninf-G[7] is a grid-enabled implementation of Ninf and provides a GridRPC[10] system that uses LDAP to manage the database of remote executables, but does not support clusters involving private IP addresses or addresses inside a firewall. Matsuoka et al.[16] has also discussed several design issues related to grid RPC systems. 162
We have designed and implemented CONFLEX-G using OmniRPC. We reported its performance in a grid testbed of several geographically distributed PC clusters. In order to explore the conformation of large bio-molecules,
CONFLEXG was used to generate trial structures of the molecules, and allocate jobs to optimize them by molecular mechanics in the grid. OmniRPC provides a restricted persistence model so that the module is automatically initialized at invocation by calling the initialization procedure. This can eliminate unnecessary communication and the initialization at each call in CONFLEX-G.
CONFLEX-G can achieves performance comparable to CONFLEX MPI and exploits more computing resources by allowing the use of multiple PC clusters in the grid. The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times for the 1BL1 molecule, where the molecule consists of a large number of atoms and each trial structure optimization requires a great deal of time. The load imbalance of the trial structure optimizations may cause performance degradation. We need to refine the algorithm used to generate the trial structure in order to improve the load balance optimization for trial structures in CONFLEX.
Future studies will include development of deployment tools and an examination of fault tolerance. In the current OmniRPC, the registration of an execution program to remote hosts and deployments of worker programs are manually set. Deployment tools will be required as the number of remote hosts is increased. In grid environments in which the environment changes dynamically, it is also necessary to support fault tolerance. This feature is especially important in large-scale applications which require lengthy calculation in a grid environment.
We plan to refine the conformational optimization algorithm in CONFLEX to explore the conformation space search of larger bio-molecules such HIV protease using up to 1000 workers in a grid environment.
This research was supported in part by a Grant-in-Aid from the Ministry of Education, Culture, Sports, Science and Technology in Japan, No. 14019011, 2002, and as part of the Program of Research and Development for Applying Advanced Computational Science and Technology by the Japan Science and Technology Corporation (Research on the grid computing platform for drug design). We would like to thank grid technology research center, AIST, Japan for providing computing resources for our experiment.
[1] H. Goto and E. Osawa. An efficient algorithm for searching low-energy conformers of cyclic and acyclic molecules. J. Chem. Soc., Perkin Trans, 2:187-198,
[2] M. Sato, T. Boku, and D. Takahashi. OmniRPC: a Grid RPC System for Parallel Programming in Cluster and Grid Environment. In Proc. of CCGrid2003, pages 219-229, 2003. [3] M. Sato, M. Hirano, Y. Tanaka, and S. Sekiguchi.
OmniRPC: a Grid RPC facility for Cluster and Global Computing in OpenMP. In Proc. of Workshop on OpenMP Applications and Tools 2001(LNCS 2104 ), pages 130-135, 2001. [4] OmniRPC Project. http://www.omni.hpcc.jp/omnirpc/. [5] M. Sato, H. Nakada, S. Sekiguchi, S. Matsuoka,
U. Nagashima, and H. Takagi. Ninf: A Network Based Information Library for Global World-Wide Computing Infrastructure. In HPCN Europe, pages 491-502, 1997. [6] Ninf Project. http://ninf.apgrid.org/. [7] Y. Tanaka, H. Nakada, S. Sekiguchi, T. Suzumura, and S. Matsuoka. Ninf-G: A Reference Implementation of RPC-based Programming Middleware for Grid Computing . Journal of Grid Computing, 1(1):41-51, 2003. [8] D. Arnold, S. Agrawal, S. Blackford, J. Dongarra,
M. Miller, K. Seymour, K. Sagi, Z. Shi, and S. Vadhiyar. Users" Guide to NetSolve V1.4.1.
Innovative Computing Dept. Technical Report ICL-UT-02-05, University of Tennessee, Knoxville,
TN, June 2002. [9] Object management group. http://www.omg.org/. [10] K. Seymour, H. Nakada, S. Matsuoka, J. Dongarra,
C. Lee, and H. Casanova. GridRPC: A Remote Procedure Call API for Grid Computing. [11] H.Goto, T. Takahashi, Y. Takata, K. Ohta, and U Nagashima. Conflex: Conformational behaviors of polypeptides as predicted by a conformational space search. In Nanotech2003, volume 1, pages 32-35, 2003. [12] I. Foster and C. Kesselman. Globus: A metacomputing infrastructure toolkit. The International Journal of Supercomputer Applications and High Performanc e Computing, 11(2):115-128, Summer 1997. [13] Stefan M. Larson, Christopher D. Snow, Michael Shirts, and Vijay S. Pande. Folding@home and genome@home: Using distributed computing to tackle prev iously intractable problems in computational biology. Computational Genomics, 2002. [14] seti@home project. http://setiathome.ssl.berkeley.edu/. [15] R. Buyya, K. Branson, J. Giddy, and D. Abramson.
The virtual laboratory: a toolset to enable distributed molecular modelling for drug design on the world-wide grid. Concurrency and Computation: Practice and Experience, 15(1):1-25, January 2003. [16] S. Matsuoka, H. Nakada, M. Sato, and S. Sekiguchi.

In recent years, grid computing has become a real alternative to traditional parallel computing. A grid provides much computational power, and thus offers the possibility to solve very large problems, especially if applications can run on multiple sites at the same time (7; 15; 20). However, the complexity of Grid environments also is many times larger than that of traditional parallel machines like clusters and supercomputers. One important problem is resource selection - selecting a set of compute nodes such that the application achieves good performance. Even in traditional, homogeneous parallel environments, finding the optimal number of nodes is a hard problem and is often solved in a trial-and-error fashion.
In a grid environment this problem is even more difficult, because of the heterogeneity of resources: the compute nodes have various speeds and the quality of network connections between them varies from low-latency and high-bandwidth local-area networks (LANs) to high-latency and possibly low-bandwidth wide-area networks (WANs). Another important problem is that the performance and availability of grid resources varies over time: the network links or compute nodes may become overloaded, or the compute nodes may become unavailable because of crashes or because they have been claimed by a higher priority application. Also, new, better resources may become available. To maintain a reasonable performance level, the application therefore needs to adapt to the changing conditions.
The adaptation problem can be reduced to the resource selection problem: the resource selection phase can be repeated during application execution, either at regular intervals, or when a performance problem is detected, or when new resources become available. This approach has been adopted by a number of systems (5; 14; 18). For resource selection, the application runtime is estimated for some resource sets and the set that yields the shortest runtime is selected for execution. Predicting the application runtime on a given set of resources, however, requires knowledge about the application.
Typically, an analytical performance model is used, but constructing such a model is inherently difficult and requires an expertise which application programmers may not have.
In this paper, we introduce and evaluate an alternative approach to application adaptation and resource selection which does not need a performance model. We start an application on any set of resources. During the application run, we periodically collect information about the communication times and idle times of the processors. We use these statistics to automatically estimate the resource requirements of the application. Next, we adjust the resource set the application is running on by adding or removing compute nodes or even entire clusters. Our adaptation strategy uses the work by Eager et al. (10) to determine the efficiency and tries to keep the efficiency of the application between a lower and upper threshold derived from their theory. Processors are added or deleted to stay between the thresholds, thus adapting automatically to the changing environment.
A major advantage of our approach is that it improves application performance in many different situations that are typical for grid computing. It handles all of the following cases: • automatically adapting the number of processors to the degree of parallelism in the application, even when this degree changes dynamically during the computation • migrating (part of) a computation away from overloaded resources • removing resources with poor communication links that slow down the computation • adding new resources to replace resources that have crashed Our work assumes the application is malleable and can run (efficiently) on multiple sites of a grid (i.e., using co-allocation (15)).
It should not use static load balancing or be very sensitive to wide121 area latencies. We have applied our ideas to divide-and-conquer applications, which satisfy these requirements. Divide-and-conquer has been shown to be an attractive paradigm for programming grid applications (4; 20). We believe that our approach can be extended to other classes of applications with the given assumptions.
We implemented our strategy in Satin, which is a Java-centric framework for writing grid-enabled divide-and-conquer applications (20). We evaluate the performance of our approach on the DAS-2 wide-area system and we will show that our approach yields major performance improvements (roughly 10-60 %) in the above scenarios.
The rest of this paper is structured as follows. In Section 2, we explain what assumptions we are making about the applications and grid resources. In Section 3, we present our resource selection and adaptation strategy. In Section 4, we describe its implementation in the Satin framework. In Section 5, we evaluate our approach in a number of grid scenarios. In Section 6, we compare our approach with the related work. Finally, in Section 7, we conclude and describe future work.
In this section, we describe our assumptions about the applications and their resources. We assume the following resource model. The applications are running on multiple sites at the same time, where sites are clusters or supercomputers. We also assume that the processors of the sites are accessible using a grid scheduling system, such as Koala (15), Zorilla (9) or GRMS (3). Processors belonging to one site are connected by a fast LAN with a low latency and high bandwidth. The different sites are connected by a WAN.
Communication between sites suffers from high latencies. We assume that the links connecting the sites with the Internet backbone might become bottlenecks causing the inter-site communication to suffer from low bandwidths.
We studied the adaptation problem in the context of divide-andconquer applications. However, we believe that our methodology can be used for other types of applications as well. In this section we summarize the assumptions about applications that are important to our approach.
The first assumption we make is that the application is malleable, i.e., it is able to handle processors joining and leaving the on-going computation. In (23), we showed how divide-andconquer applications can be made fault tolerant and malleable.
Processors can be added or removed at any point in the computation with little overhead. The second assumption is that the application can efficiently run on processors with different speeds.
This can be achieved by using a dynamic load balancing strategy, such as work stealing used by divide-and-conquer applications (19). Also, master-worker applications typically use dynamic load-balancing strategies (e.g., MW - a framework for writing gridenabled master-worker applications (12)). We find it a reasonable assumption for a grid application, since applications for which the slowest processor becomes a bottleneck will not be able to efficiently utilize grid resources. Finally, the application should be insensitive to wide-area latencies, so it can run efficiently on a widearea grid (16; 17).
In this section we will explain how we use application malleability to find a suitable set of resources for a given application and to adapt to changing conditions in the grid environment. In order to monitor the application performance and guide the adaptation, we added an extra process to the computation which we call adaptation coordinator. The adaptation coordinator periodically collects performance statistics from the application processors. We introduce a new application performance metric: weighted average efficiency which describes the application performance on a heterogeneous set of resources. The coordinator uses statistics from application processors to compute the weighted average efficiency. If the efficiency falls above or below certain thresholds, the coordinator decides on adding or removing processors. A heuristic formula is used to decide which processors have to be removed. During this process the coordinator learns the application requirements by remembering the characteristics of the removed processors. These requirements are then used to guide the adding of new processors.
In traditional parallel computing, a standard metric describing the performance of a parallel application is efficiency. Efficiency is defined as the average utilization of the processors, that is, the fraction of time the processors spend doing useful work rather than being idle or communicating with other processors (10). efficiency = 1 n ∗ n i=0 (1 − overheadi ) where n is the number of processors and overheadi is the fraction of time the ith processor spends being idle or communicating.
Efficiency indicates the benefit of using multiple processors.
Typically, the efficiency drops as new processors are added to the computation. Therefore, achieving a high speedup (and thus a low execution time) and achieving a high system utilization are conflicting goals (10). The optimal number of processors is the number for which the ratio of efficiency to execution time is maximized. Adding processors beyond this number yields little benefit.
This number is typically hard to find, but in (10) it was theoretically proven that if the optimal number of processors is used, the efficiency is at least 50%. Therefore, adding processors when efficiency is smaller or equal to 50% will only decrease the system utilization without significant performance gains.
For heterogeneous environments with different processor speeds, we extended the notion of efficiency and introduced weighted average efficiency. wa efficiency = 1 n ∗ n i=0 speedi ∗ (1 − overheadi ) The useful work done by a processor (1 − overheadi) is weighted by multiplying it by the speed of this processor relative to the fastest processor. The fastest processor has speed = 1, for others holds: 0 < speed ≤ 1. Therefore, slower processors are modeled as fast ones that spend a large fraction of the time being idle. Weighted average efficiency reflects the fact that adding slow processors yields less benefit than adding fast processors.
In the heterogeneous world, it is hardly beneficial to add processors if the efficiency is lower than 50% unless the added processor is faster than some of the currently used processors. Adding faster processors might be beneficial regardless of the efficiency.
Each processor measures the time it spends communicating or being idle. The computation is divided into monitoring periods.
After each monitoring period, the processors compute their overhead over this period as the percentage of the time they spent being idle or communicating in this period. Apart from total overhead, each processor also computes the overhead of inter-cluster and intracluster communication.
To calculate weighted average efficiency, we need to know the relative speeds of the processors, which depend on the application and the problem size used. Since it is impractical to run the 122 whole application on each processor separately, we use applicationspecific benchmarks. Currently we use the same application with a small problem size as a benchmark and we require the application programmer to specify this problem size. This approach requires extra effort from the programmer to find the right problem size and possibly to produce input files for this problem size, which may be hard. In the future, we are planning to generate benchmarks automatically by choosing a random subset of the task graph of the original application.
Benchmarks have to be re-run periodically because the speed of a processor might change if it becomes overloaded by another application (for time-shared machines). There is a trade-off between the accuracy of speed measurements and the overhead it incurs. The longer the benchmark, the greater the accuracy of the measurement.
The more often it is run, the faster changes in processor speed are detected. In our current implementation, the application programmer specifies the length of the benchmark (by specifying its problem size) and the maximal overhead it is allowed to cause.
Processors run the benchmark at such frequency so as not to exceed the specified overhead. In the future, we plan to combine benchmarking with monitoring the load of the processor which would allow us to avoid running the benchmark if no change in processor load is detected. This optimization will further reduce the benchmarking overhead.
Note that the benchmarking overhead could be avoided completely for more regular applications, for example, for masterworker applications with tasks of equal or similar size. The processor speed could then be measured by counting the tasks processed by this processor within one monitoring period.
Unfortunately, divide-and-conquer applications typically exhibit a very irregular structure. The sizes of tasks can vary by many orders of magnitude.
At the end of each monitoring period, the processors send the overhead statistics and processor speeds to the coordinator.
Periodically, the coordinator computes the weighted average efficiency and other statistics, such as average inter-cluster overhead or overheads in each cluster. The clocks of the processors are not synchronized with each other or with the clock of the coordinator. Each processor decides separately when it is time to send data. Occasionally, the coordinator may miss data at the end of a monitoring period, so it has to use data from the previous monitoring period for these processors. This causes small inaccuracies in the calculations of the coordinator, but does not influence the performance of adaptation.
The adaptation coordinator tries to keep the weighted average efficiency between Emin and Emax. When it exceeds Emax, the coordinator requests new processors from the scheduler. The number of requested processors depends on the current efficiency: the higher the efficiency, the more processors are requested. The coordinator starts removing processors when the weighted average efficiency drops below Emin. The number of nodes that are removed again depends on the weighted average efficiency. The lower the efficiency, the more nodes are removed. The thresholds we use are Emax = 50%, because we know that adding processors when efficiency is lower does not make sense, and Emin = 30%.
Efficiency of 30% or lower might indicate performance problems such as low bandwidth or overloaded processors. In that case, removing bad processors will be beneficial for the application. Such low efficiency might also indicate that we simply have too many processors. In that case, removing some processors may not be beneficial but it will not harm the application. The coordinator always tries to remove the worst processors. The badness of a processor is determined by the following formula: proc badnessi = α ∗ 1 speedi + β ∗ ic overheadi + γ ∗ inW orstCluster(i) The processor is considered bad if it has low speed ( 1 speed is big) and high inter-cluster overhead (ic overhead). High intercluster overhead indicates that the bandwidth to this processor"s cluster is insufficient. Removing processors located in a single cluster is desirable since it decreases the amount of wide-area communication. Therefore, processors belonging to the worst cluster are preferred. Function inW orstCluster(i) returns 1 for processors belonging to the worst cluster and 0 otherwise. The badness of clusters is computed similarly to the badness of processors: cluster badnessi = α ∗ 1 speedi + β ∗ ic overheadi The speed of a cluster is the sum of processor speeds normalized to the speed of the fastest cluster. The ic overhead of a cluster is an average of processor inter-cluster overheads. The α, β and γ coefficients determine the relative importance of the terms. Those coefficients are established empirically. Currently we are using the following values: α = 1, β = 100 and γ = 10, based on the observation that ic overhead > 0.2 indicates bandwidth problems and processors with speed < 0.05 do not contribute to the computation.
Additionally, when one of the clusters has an exceptionally high inter-cluster overhead (larger than 0.25), we conclude that the bandwidth on the link between this cluster and the Internet backbone is insufficient for the application. In that case, we simply remove the whole cluster instead of computing node badness and removing the worst nodes. After deciding which nodes are removed, the coordinator sends a message to these nodes and the nodes leave the computation. Figure 1 shows a schematic view of the adaptation strategy. Dashed lines indicate a part that is not supported yet, as will be explained below.
This simple adaptation strategy allows us to improve application performance in several situations typical for the Grid: • If an application is started on fewer processors than its degree of parallelism allows, it will automatically expand to more processors (as soon as there are extra resources available). Conversely, if an application is started on more processors than it can efficiently use, a part of the processors will be released. • If an application is running on an appropriate set of resources but after a while some of the resources (processors and/or network links) become overloaded and slow down the computation, the overloaded resources will be removed. After removing the overloaded resources, the weighted average efficiency will increase to above the Emax threshold and the adaptation coordinator will try to add new resources. Therefore, the application will be migrated from overloaded resources. • If some of the original resources chosen by the user are inappropriate for the application, for example the bandwidth to one of the clusters is too small, the inappropriate resources will be removed. If necessary, the adaptation component will try to add other resources. • If during the computation a substantial part of the processors will crash, the adaptation component will try to add new resources to replace the crashed processors. 123 0 2000 4000 6000 runtime(secs.) Scenario 0 a b c Scenario 1 Scenario 2 Scenario 3 Scenario 4 Scenario 5 without monitoring and adaptation (runtime 1) with monitoring and adaptation (runtime 2) with monitoring but no adaptation (runtime 3) Figure 2. The runtimes of the Barnes-Hut application, scenarios 0-5 add nodes faster nodes available if compute weighted average efficiency E wa wait & collect statistics rank nodes remove worst nodes waE Ewa Y N N Y above if below if Emin maxE Figure 1. Adaptation strategy • If the application degree of parallelism is changing during the computation, the number of nodes the application is running on will be automatically adjusted.
Further improvements are possible, but require extra functionality from the grid scheduler and/or integration with monitoring services such as NWS (22). For example, adding nodes to a computation can be improved. Currently, we add any nodes the scheduler gives us. However, it would be more efficient to ask for the fastest processors among the available ones. This could be done, for example, by passing a benchmark to the grid scheduler, so that it can measure processor speeds in an application specific way.
Typically, it would be enough to measure the speed of one processor per site, since clusters and supercomputers are usually homogeneous. An alternative approach would be ranking the processors based on parameters such as clock speed and cache size. This approach is sometimes used for resource selection for sequential applications (14). However, it is less accurate than using an application specific benchmark.
Also, during application execution, we can learn some application requirements and pass them to the scheduler. One example is minimal bandwidth required by the application. The lower bound on minimal required bandwidth is tightened each time a cluster with high inter-cluster overhead is removed. The bandwidth between each pair of clusters is estimated during the computation by measuring data transfer times, and the bandwidth to the removed cluster is set as a minimum. Alternatively, information from a grid monitoring system can be used. Such bounds can be passed to the scheduler to avoid adding inappropriate resources. It is especially important when migrating from resources that cause performance problems: we have to be careful not to add the resources we have just removed. Currently we use blacklisting - we simply do not allow adding resources we removed before. This means, however, that we cannot use these resources even if the cause of the performance problem disappears, e.g. the bandwidth of a link might improve if the background traffic diminishes.
We are currently not able to perform opportunistic migration - migrating to better resources when they are discovered. If an application runs with efficiency between Emin and Emax, the adaptation component will not undertake any action, even if better resources become available. Enabling opportunistic migration requires, again, the ability to specify to the scheduler what better resources are (faster, with a certain minimal bandwidth) and receiving notifications when such resources become available.
Existing grid schedulers such as GRAM from the Globus Toolkit (11) do not support such functionality. The developers of the KOALA metascheduler (15) have recently started a project whose goal is to provide support for adaptive applications. We are currently discussing with them the possibility of providing the functionalities required by us, aiming to extend our adaptivity strat124 egy to support opportunistic migration and to improve the initial resource selection.
We incorporated our adaptation mechanism into Satin - a Java framework for creating grid-enabled divide-and-conquer applications. With Satin, the programmer annotates the sequential code with divide-and-conquer primitives and compiles the annotated code with a special Satin compiler that generates the necessary communication and load balancing code. Satin uses a very efficient, grid-aware load balancing algorithm - Cluster-aware Random Work Stealing (CRS) (19), which hides wide-area latencies by overlapping local and remote stealing. Satin also provides transparent fault tolerance and malleability (23). With Satin, removing and adding processors from/to an ongoing computation incurs little overhead.
We instrumented the Satin runtime system to collect runtime statistics and send them to the adaptation coordinator. The coordinator is implemented as a separate process. Both coordinator and Satin are implemented entirely in Java on top of the Ibis communication library (21). The core of Ibis is also implemented in Java.
The resulting system therefore is highly portable (due to Java"s write once, run anywhere property) allowing the software to run unmodified on a heterogeneous grid.
Ibis also provides the Ibis Registry. The Registry provides, among others, a membership service to the processors taking part in the computation. The adaptation coordinator uses the Registry to discover the application processes, and the application processes use this service to discover each other. The Registry also offers fault detection (additional to the fault detection provided by the communication channels). Finally, the Registry provides the possibility to send signals to application processes. The coordinator uses this functionality to notify the processors that they need to leave the computation. Currently the Registry is implemented as a centralized server.
For requesting new nodes, the Zorilla (9) system is used - a peer-to-peer supercomputing middleware which allows straightforward allocation of processors in multiple clusters and/or supercomputers. Zorilla provides locality-aware scheduling, which tries to allocate processors that are located close to each other in terms of communication latency. In the future, Zorilla will also support bandwidth-aware scheduling, which tries to maximize the total bandwidth in the system. Zorilla can be easily replaced with another grid scheduler. In the future, we are planning to integrate our adaptation component with GAT (3) which is becoming a standard in the grid community and KOALA (15) a scheduler that provides co-allocation on top of standard grid middleware, such as the Globus Toolkit (11).
In this section, we will evaluate our approach. We will demonstrate the performance of our mechanism in a few scenarios. The first scenario is an ideal situation: the application runs on a reasonable set of nodes (i.e., such that the efficiency is around 50%) and no problems such as overloaded networks and processors, crashing processors etc. occur. This scenario allows us to measure the overhead of the adaptation support. The remaining scenarios are typical for grid environments and demonstrate that with our adaptation support the application can avoid serious performance bottlenecks such as overloaded processors or network links. For each scenario, we compare the performance of an application with adaptation support to a non-adaptive version. In the non-adaptive version, the coordinator does not collect statistics and no benchmarking (for measuring processor speeds) is performed. In the ideal scenario, 0 5 10 15 iteration number 0 200 400 600 iterationduration(secs.) starting on 8 nodes starting on 16 nodes starting on 24 nodes starting on 8 nodes starting on 16 nodes starting on 24 nodes }no adaptation }with adaptation Figure 3. Barnes-Hut iteration durations with/without adaptation, too few processors 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation CPU load introduced overloaded nodes removed started adding nodes 36 nodes reached Figure 4. Barnes-Hut iteration durations with/without adaptation, overloaded CPUs we additionally measure the performance of an application with collecting statistics and benchmarking turned on but without doing adaptation, that is, without allowing it to change the number of nodes. This allows us to measure the overhead of benchmarking and collecting statistics. In all experiments we used a monitoring period of 3 minutes for the adaptive versions of the applications.
All the experiments were carried out on the DAS-2 wide-area system (8), which consists of five clusters located at five Dutch uni125 versities. One of the clusters consists of 72 nodes, the others of 32 nodes. Each node contains two 1 GHz Pentium processors. Within a cluster, the nodes are connected by Fast Ethernet. The clusters are connected by the Dutch university Internet backbone. In our experiments, we used the Barnes-Hut N-body simulation.
BarnesHut simulates the evolution of a large set of bodies under influence of (gravitational or electrostatic) forces. The evolution of N bodies is simulated in iterations of discrete time steps.
In this scenario, the application is started on 36 nodes. The nodes are equally divided over 3 clusters (12 nodes in each cluster). On this number of nodes, the application runs with 50% efficiency, so we consider it a reasonable number of nodes. As mentioned above, in this scenario we measured three runtimes: the runtime of the application without adaptation support (runtime 1), the runtime with adaptation support (runtime 2) and the runtime with monitoring (i.e., collection of statistics and benchmarking) turned on but without allowing it to change the number of nodes (runtime 3). Those runtimes are shown in Figure 2, first group of bars. The comparison between runtime 3 and 1 shows the overhead of adaptation support.
In this experiment it is around 15%. Almost all overhead comes from benchmarking. The benchmark is run 1-2 times per monitoring period. This overhead can be made smaller by increasing the length of the monitoring period and decreasing the benchmarking frequency. The monitoring period we used (3 minutes) is relatively short, because the runtime of the application was also relatively short (30-60 minutes). Using longer running applications would not allow us to finish the experimentation in a reasonable time.
However, real-world grid applications typically need hours, days or even weeks to complete. For such applications, a much longer monitoring period can be used and the adaptation overhead can be kept much lower. For example, with the Barnes-Hut application, if the monitoring period is extended to 10 minutes, the overhead drops to 6%. Note that combining benchmarking with monitoring processor load (as described in Section 3.2) would reduce the benchmarking overhead to almost zero: since the processor load is not changing, the benchmarks would only need to be run at the beginning of the computation.
In this scenario, the application is started on fewer nodes than the application can efficiently use. This may happen because the user does not know the right number of nodes or because insufficient nodes were available at the moment the application was started. We tried 3 initial numbers of nodes: 8 (Scenario 1a), 16 (Scenario 1b) and 24 (Scenario 1c). The nodes were located in 1 or 2 clusters. In each of the three sub-scenarios, the application gradually expanded to 36-40 nodes located in 4 clusters. This allowed to reduce the application runtimes by 50% (Scenario 1a), 35% (Scenario 1b) and 12% (Scenario 1c) with respect to the non-adaptive version. Those runtimes are shown in Figure 2. Since Barnes-Hut is an iterative application, we also measured the time of each iteration, as shown in Figure 3. Adaptation reduces the iteration time by a factor of 3 (Scenario 1a), 1.7 (Scenario 1b) and 1.2 (Scenario 1c) which allows us to conclude that the gains in the total runtime would be even bigger if the application were run longer than for 15 iterations.
In this scenario, we started the application on 36 nodes in 3 clusters.
After 200 seconds, we introduced a heavy, artificial load on the processors in one of the clusters. Such a situation may happen when an application with a higher priority is started on some of the resources. Figure 4 shows the iteration durations of both the adaptive and non-adaptive versions. After introducing the load, the iteration 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation one cluster is badly connected badly connected cluster removed started adding nodes 36 nodes reached Figure 5. Barnes-Hut iteration durations with/without adaptation, overloaded network link 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation one cluster is badly connected 12 nodes lightly overloaded removed badly connected cluster removed 2 lightly overloaded nodes Figure 6. Barnes-Hut iteration durations with/without adaptation, overloaded CPUs and an overloaded network link duration increased by a factor of 2 to 3. Also, the iteration times became very variable. The adaptive version reacted by removing the overloaded nodes. After removing these nodes, the weighted average efficiency rose to around 35% which triggered adding new nodes and the application expanded back to 38 nodes. So, the overloaded nodes were replaced by better nodes, which brought the iteration duration back to the initial values. This reduced the total runtime by 14%. The runtimes are shown in Figure 2. 126
In this scenario, we ran the application on 36 nodes in 3 clusters.
We simulated that the uplink to one of the clusters was overloaded and the bandwidth on this uplink was reduced to approximately 100 KB/s. To simulate low bandwidth we use the traffic-shaping techniques described in (6). The iteration durations in this experiment are shown in Figure 5. The iteration durations of the nonadaptive version exhibit enormous variation: from 170 to 890 seconds. The adaptive version removed the badly connected cluster after the first monitoring period. As a result, the weighted average efficiency rose to around 35% and new nodes were gradually added until their number reached 38. This brought the iteration times down to around 100 seconds. The total runtime was reduced by 60% (Figure 2).
network link In this scenario, we ran the application on 36 nodes in 3 clusters.
Again, we simulated an overloaded uplink to one of the clusters.
Additionally, we simulated processors with heterogeneous speeds by inserting a relatively light artificial load on the processors in one of the remaining clusters. The iteration durations are shown in Figure 6. Again, the non-adaptive version exhibits a great variation in iteration durations: from 200 to 1150 seconds. The adaptive version removes the badly connected cluster after the first monitoring period which brings the iteration duration down to 210 seconds on average. After removing one of the clusters, since some of the processors are slower (approximately 5 times), the weighted average efficiency raises only to around 40%. Since this value lies between Emin and Emax, no nodes are added or removed. This example illustrates what the advantages of opportunistic migration would be.
There were faster nodes available in the system. If these nodes were added to the application (which could trigger removing the slower nodes) the iteration duration could be reduced even further. Still, the adaptation reduced the total runtime by 30% (Figure 2).
In the last scenario, we also run the application on 36 nodes in 3 clusters. After 500 seconds, 2 out of 3 clusters crash. The iteration durations are shown in Figure 7. After the crash, the iteration duration raised from 100 to 200 seconds. The weighted efficiency rose to around 30% which triggered adding new nodes in the adaptive version. The number of nodes gradually went back to 35 which brought the iteration duration back to around 100 seconds.
The total runtime was reduced by 13% (Figure 2).
A number of Grid projects address the question of resource selection and adaptation. In GrADS (18) and ASSIST (1), resource selection and adaptation requires a performance model that allows predicting application runtimes. In the resource selection phase, a number of possible resource sets is examined and the set of resources with the shortest predicted runtime is selected. If performance degradation is detected during the computation, the resource selection phase is repeated. GrADS uses the ratio of the predicted execution times (of certain application phases) to the real execution times as an indicator of application performance. ASSIST uses the number of iterations per time unit (for iterative applications) or the number of tasks per time unit (for regular master-worker applications) as a performance indicator. The main difference between these approaches and our approach is the use of performance models. The main advantage is that once the performance model is known, the system is able to take more accurate migration decisions than with our approach. However, even if the performance 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation 2 out of 3 clusters crash started adding nodes 36 nodes reached Figure 7. Barnes-Hut iteration durations with/without adaptation, crashing CPUs model is known, the problem of finding an optimal resource set (i.e. the resource set with the minimal execution time) is NP-complete.
Currently, both GrADS and ASSIST examine only a subset of all possible resource sets and therefore there is no guarantee that the resulting resource set will be optimal. As the number of available grid resources increases, the accuracy of this approach diminishes, as the subset of possible resource sets that can be examined in a reasonable time becomes smaller. Another disadvantage of these systems is that the performance degradation detection is suitable only for iterative or regular applications.
Cactus (2) and GridWay (14) do not use performance models.
However, these frameworks are only suitable for sequential (GridWay) or single-site applications (Cactus). In that case, the resource selection problem boils down to selecting the fastest machine or cluster. Processor clock speed, average load and a number of processors in a cluster (Cactus) are used to rank resources and the resource with the highest rank is selected. The application is migrated if performance degradation is detected or better resources are discovered. Both Cactus and GridWay use the number of iterations per time unit as the performance indicator. The main limitation of this methodology is that it is suitable only for sequential or single-site applications. Moreover, resource selection based on clock speed is not always accurate. Finally, performance degradation detection is suitable only for iterative applications and cannot be used for irregular computations such as search and optimization problems.
The resource selection problem was also studied by the AppLeS project (5). In the context of this project, a number of applications were studied and performance models for these applications were created. Based on such a model a scheduling agent is built that uses the performance model to select the best resource set and the best application schedule on this set. AppLeS scheduling agents are written on a case-by-case basis and cannot be reused for another application. Two reusable templates were also developed for specific classes of applications, namely master-worker (AMWAT template) and parameter sweep (APST template) applications. Migration is not supported by the AppLeS software. 127 In (13), the problem of scheduling master-worker applications is studied. The authors assume homogeneous processors (i.e., with the same speed) and do not take communication costs into account.
Therefore, the problem is reduced to finding the right number of workers. The approach here is similar to ours in that no performance model is used. Instead, the system tries to deduce the application requirements at runtime and adjusts the number of workers to approach the ideal number.
In this paper, we investigated the problem of resource selection and adaptation in grid environments. Existing approaches to these problems typically assume the existence of a performance model that allows predicting application runtimes on various sets of resources.
However, creating performance models is inherently difficult and requires knowledge about the application. We propose an approach that does not require in-depth knowledge about the application. We start the application on an arbitrary set of resources and monitor its performance. The performance monitoring allows us to learn certain application requirements such as the number of processors needed by the application or the application"s bandwidth requirements. We use this knowledge to gradually refine the resource set by removing inadequate nodes or adding new nodes if necessary.
This approach does not result in the optimal resource set, but in a reasonable resource set, i.e. a set free from various performance bottlenecks such as slow network connections or overloaded processors. Our approach also allows the application to adapt to the changing grid conditions.
The adaptation decisions are based on the weighted average efficiency - an extension of the concept of parallel efficiency defined for traditional, homogeneous parallel machines. If the weighted average efficiency drops below a certain level, the adaptation coordinator starts removing worst nodes. The badness of the nodes is defined by a heuristic formula. If the weighted average efficiency raises above a certain level, new nodes are added. Our simple adaptation strategy allows us to handle multiple scenarios typical for grid environments: expand to more nodes or shrink to fewer nodes if the application was started on an inappropriate number of processors, remove inadequate nodes and replace them with better ones, replace crashed processors, etc. The application adapts fully automatically to changing conditions. We implemented our approach in the Satin divide-and-conquer framework and evaluated it on the DAS-2 distributed supercomputer and demonstrate that our approach can yield significant performance improvements (up to 60% in our experiments).
Future work will involve extending our adaptation strategy to support opportunistic migration. This, however, requires grid schedulers with more sophisticated functionality than currently exists. Further research is also needed to decrease the benchmarking overhead. For example, the information about CPU load could be used to decrease the benchmarking frequency. Another line of research that we wish to investigate is using feedback control to refine the adaptation strategy during the application run. For example, the node badness formula could be refined at runtime based on the effectiveness of the previous adaptation decisions. Finally, the centralized implementation of the adaptation coordinator might become a bottleneck for applications which are running on very large numbers of nodes (hundreds or thousands). This problem can be solved by implementing a hierarchy of coordinators: one subcoordinator per cluster which collects and processes statistics from its cluster and one main coordinator which collects the information from the sub-coordinators.
Acknowledgments This work was carried out in the context of Virtual Laboratory for e-Science project (ww.vl-e.nl). This project is supported by a BSIK grant from the Dutch Ministry of Education, Culture and Science (OC&W) and is part of the ICT innovation program of the Ministry of Economic Affairs (EZ).
References [1] M. Aldinucci, F. Andre, J. Buisson, S. Campa, M. Coppola,
M. Danelutto, and C. Zoccolo. Parallel program/component adaptivity management. In ParCo 2005, Sept. 2005. [2] G. Allen, D. Angulo, I. Foster, G. Lanfermann, C. Liu,
T. Radke, E. Seidel, and J. Shalf. The cactus worm: Experiments with resource discovery and allocation in a grid environment. Int"l Journal of High Performance Computing Applications, 15(4):345-358, 2001. [3] G. Allen, K. Davis, K. N. Dolkas, N. D. Doulamis, T. Goodale,
T. Kielmann, A. Merzky, J. Nabrzyski, J. Pukacki, T. Radke,
M. Russell, E. Seidel, J. Shalf, and I. Taylor. Enabling applications on the grid - a gridlab overview. Int"l Journal of High-Performance Computing Applications, 17(4):449-466,
Aug. 2003. [4] J. E. Baldeschwieler, R. D. Blumofe, and E. A. Brewer.
ATLAS: An Infrastructure for Global Computing. In 7th ACM SIGOPS European Workshop on System Support for Worldwide Applications, pages 165-172, Sept. 1996. [5] F. Berman, R. Wolski, H. Casanova, W. Cirne, H. Dail,
M. Faerman, S. Figueira, J. Hayes, G. Obertelli, J. Schopf,
G. Shao, S. Smallen, N. Spring, A. Su, and D.
Zagorodnov. Adaptive Computing on the Grid Using AppLeS. IEEE Trans. on Parallel and Distributed Systems, 14(4):369-382,
Apr. 2003. [6] D.-M. Chiu, M. Kadansky, J. Provino, and J. Wesley.
Experiences in programming a traffic shaper. In 5th IEEE Symp. on Computers and Communications, pages 470-476, 2000. [7] W. Chrabakh and R. Wolski. GridSAT: A Chaff-based Distributed SAT Solver for the Grid. In 2003 ACM/IEEE conference on Supercomputing, page 37, 2003. [8] The Distributed ASCI Supercomputer (DAS). http://www.cs.vu.nl/das2/. [9] N. Drost, R. V. van Nieuwport, and H. E. Bal. Simple localityaware co-allocation in peer-to-peer supercomputing. In 6th Int"l Workshop on Global Peer-2-Peer Computing, May 2005. [10] D. L. Eager, J. Zahorjan, and E. D. Lazowska. Speedup versus efficiency in parallel systems. IEEE Transactions on Computers, 38(3):408-423, Mar. 1989. [11] I. Foster. Globus toolkit version 4: Software for serviceoriented systems. In IFIP International Conference on Network and Parallel Computing, pages 2-13. Springer-Verlag LNCS 3779, 2005. [12] J.-P. Goux, S. Kulkarni, M. Yoder, and J. Linderoth. An Enabling Framework for Master-Worker Applications on the Computational Grid. In 9th IEEE Int"l Symp. on High Performance Distributed Computing, pages 43-50, Aug. 2000. [13] E. Heymann, M. A. Senar, E. Luque, and M. Livny.
Adaptive scheduling for master-worker applications on the computational grid. In 1st IEEE/ACM International Workshop on Grid Computing, pages 214-227. Springer Verlag LNCS 1971, 2000. 128 [14] E. Huedo, R. S. Montero, and I. M. Llorente. A framework for adaptive execution in grids. Software - Practice & Experience, 34(7):631-651, 2004. [15] H. H. Mohamed and D. H. Epema. Experiences with the KOALA Co-Allocating Scheduler in Multiclusters. In 5th IEEE/ACM Int"l Symp. on Cluster Computing and the GRID, pages 640-650, May 2005. [16] A. Plaat, H. E. Bal, and R. F. H. Hofman. Sensitivity of parallel applications to large differences in bandwidth and latency in two-layer interconnects. In 5th Int"l Symp. On High Performance Computer Architecture, pages 244-253,
Jan. 1999. [17] J. W. Romein, H. E. Bal, J. Schaeffer, and A. Plaat. A performance analysis of transposition-table-driven work scheduling in distributed search. IEEE Trans. on Parallel and Distributed Systems, 13(5):447-459, May 2002. [18] S. S. Vadhiyar and J. J. Dongarra. Self adaptivity in Grid computing. Concurrency and Computation: Practice and Experience, 17(2-4):235-257, 2005. [19] R. V. van Nieuwpoort, T. Kielmann, and H. E. Bal. Efficient load balancing for wide-area divide-and-conquer applications.
In 8th ACM SIGPLAN Symp. on Principles and Practices of Parallel Programming, pages 34-43, 2001. [20] R. V. van Nieuwpoort, J. Maassen, T. Kielmann, and H. E. Bal.
Satin: Simple and Efficient Java-based Grid Programming.
Scalable Computing: Practice and Experience, 6(3):19-32,
Sept. 2004. [21] R. V. van Nieuwpoort, J. Maassen, G. Wrzesinska, R.
Hofman, C. Jacobs, T. Kielmann, and H. E. Bal. Ibis: a Flexible and Efficient Java-based Grid Programming Environment.
Concurrency & Computation: Practice & Experience, 17(78):1079-1107, 2005. [22] R. Wolski, N. Spring, and J. Hayes. The network weather service: A distributed resource performance forecasting service for metacomputing. Journal of Future Generation Computing Systems, 15(5-6):757-768, Oct. 1999. [23] G. Wrzesinska, R. V. van Nieuwport, J. Maassen, and H. E.

In this paper, we consider the following general problem.
Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery.
Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable (and hence extensible) end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications.
However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree [2, 6]. However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node"s single parent in the tree.
Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically 282 transmit disjoint data sets to various points in the network.
Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the last object, which may only be available at a few nodes. One hypothesis of this work is that, relative to a tree, this model will result in higher bandwidth-leveraging the bandwidth from simultaneous parallel downloads from multiple sources rather than a single parent-and higher reliability-retrieving data from multiple peers reduces the potential damage from a single node failure.
To illustrate Bullet"s behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available (TCP-friendly) bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data.
Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly [15].
No flow should consume more than its fair share of the bottleneck bandwidth and each flow must respond to congestion signals (losses) by reducing its transmission rate. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to peer with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure high up in a multicast overlay tree.
In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties.
Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques [1, 18, 21, 24, 34]. Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Depending on the type of data being transmitted, Bullet can optionally employ a variety of encoding schemes, for instance Erasure codes [7, 26, 25] or Multiple Description Coding (MDC) [17], to efficiently disseminate data, adapt to variable bandwidth, and recover from losses. Finally, we use TFRC [15] to transfer data both down the overlay tree and among peers. This ensures that the entire overlay behaves in a congestion-friendly manner, adjusting its transmission rate on a per-connection basis based on prevailing network conditions.
One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source (its parent). Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research [20, 35], accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system.
Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh.
We have completed a prototype of Bullet running on top of a number of overlay trees. Our evaluation of a 1000-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree (using an oﬄine algorithm and global network topology information), all while remaining TCP friendly. We also deployed our prototype across the PlanetLab [31] wide-area testbed.
For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios.
The remainder of this paper is organized as follows.
Section 2 presents Bullet"s system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet"s performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions.
Our approach to high bandwidth data dissemination centers around the techniques depicted in Figure 1. First, we split the target data stream into blocks which are further subdivided into individual (typically packet-sized) objects.
Depending on the requirements of the target applications, objects may be encoded [17, 26] to make data recovery more efficient. Next, we purposefully disseminate disjoint objects 283 S A C Original data stream: 1 2 3 4 5 6 B 1 2 3 5 1 3 4 6 2 4 5 6 TFRC to determine available BW D E 1 2 5 1 3 4 Figure 1: High-level view of Bullet"s operation. to different clients at a rate determined by the available bandwidth to each client. We use the equation-based TFRC protocol to communicate among all nodes in the overlay in a congestion responsive and TCP friendly manner.
Given the above techniques, data is spread across the overlay tree at a rate commensurate with the available bandwidth in the overlay tree. Our overall goal however is to deliver more bandwidth than would otherwise be available through any tree. Thus, at this point, nodes require a scalable technique for locating and retrieving disjoint data from their peers. In essence, these perpendicular links across the overlay form a mesh to augment the bandwidth available through the tree. In Figure 1, node D only has sufficient bandwidth to receive 3 objects per time unit from its parent. However, it is able to locate two peers, C and E, who are able to transmit missing data objects, in this example increasing delivered bandwidth from 3 objects per time unit to 6 data objects per time unit. Locating appropriate remote peers cannot require global state or global communication. Thus, we propose the periodic dissemination of changing, uniformly random subsets of global state to each overlay node once per configurable time period. This random subset contains summary tickets of the objects available at a subset of the nodes in the system. Each node uses this information to request data objects from remote nodes that have significant divergence in object membership. It then attempts to establish a number of these peering relationships with the goals of minimizing overlap in the objects received from each peer and maximizing the total useful bandwidth delivered to it.
In the remainder of this section, we provide brief background on each of the techniques that we employ as fundamental building blocks for our work. Section 3 then presents the details of the entire Bullet architecture.
Depending on the type of data being distributed through the system, a number of data encoding schemes can improve system efficiency. For instance, if multimedia data is being distributed to a set of heterogeneous receivers with variable bandwidth, MDC [17] allows receivers obtaining different subsets of the data to still maintain a usable multimedia stream. For dissemination of a large file among a set of receivers, Erasure codes enable receivers not to focus on retrieving every transmitted data packet. Rather, after obtaining a threshold minimum number of packets, receivers are able to decode the original data stream. Of course,
Bullet is amenable to a variety of other encoding schemes or even the null encoding scheme, where the original data stream is transmitted best-effort through the system.
In this paper, we focus on the benefits of a special class of erasure-correcting codes used to implement the digital fountain [7] approach. Redundant Tornado [26] codes are created by performing XOR operations on a selected number of original data packets, and then transmitted along with the original data packets. Tornado codes require any (1+ )k correctly received packets to reconstruct the original k data packets, with the typically low reception overhead ( ) of 0.03 − 0.05. In return, they provide significantly faster encoding and decoding times. Additionally, the decoding algorithm can run in real-time, and the reconstruction process can start as soon as sufficiently many packets have arrived. Tornado codes require a predetermined stretch factor (n/k, where n is the total number of encoded packets), and their encoding time is proportional to n. LT codes [25] remove these two limitations, while maintaining a low reception overhead of 0.05.
To address the challenge of locating disjoint content within the system, we use RanSub [24], a scalable approach to distributing changing, uniform random subsets of global state to all nodes of an overlay tree. RanSub assumes the presence of some scalable mechanism for efficiently building and maintaining the underlying tree. A number of such techniques are described in [1, 18, 21, 24, 34].
RanSub distributes random subsets of participating nodes throughout the tree using collect and distribute messages.
Collect messages start at the leaves and propagate up the tree, leaving state at each node along the path to the root.
Distribute messages start at the root and travel down the tree, using the information left at the nodes during the previous collect round to distribute uniformly random subsets to all participants. Using the collect and distribute messages,
RanSub distributes a random subset of participants to each node once per epoch. The lower bound on the length of an epoch is determined by the time it takes to propagate data up then back down the tree, or roughly twice the height of the tree. For appropriately constructed trees, the minimum epoch length will grow with the logarithm of the number of participants, though this is not required for correctness.
As part of the distribute message, each participant sends a uniformly random subset of remote nodes, called a distribute set, down to its children. The contents of the distribute set are constructed using the collect set gathered during the previous collect phase. During this phase, each participant sends a collect set consisting of a random subset of its descendant nodes up the tree to the root along with an estimate of its total number of descendants. After the root receives all collect sets and the collect phase completes, the distribute phase begins again in a new epoch.
One of the key features of RanSub is the Compact operation. This is the process used to ensure that membership in a collect set propagated by a node to its parent is both random and uniformly representative of all members of the sub-tree rooted at that node. Compact takes multiple fixedsize subsets and the total population represented by each subset as input, and generates a new fixed-size subset. The 284 A CSC={Cs},
CSD={Ds} CSF={Fs},
CSG={Gs} CSB={Bs,Cs,Ds},
CSE={Es,Fs,Gs} B C E D GF B C A E D GF DSE={As,Bs,Cs,
Ds} DSB={As,Es,Fs,Gs} DSG={As,Bs,Cs,
Ds,Es,Fs} DSD={As,Bs,
Cs,Es,Fs,Gs} DSF={As,Bs,Cs,
Ds,Es,Gs} DSC={As,Bs,
Ds,Es,Fs,Gs} Figure 2: This example shows the two phases of the RanSub protocol that occur in one epoch. The collect phase is shown on the left, where the collect sets are traveling up the overlay to the root. The distribute phase on the right shows the distribute sets traveling down the overlay to the leaf nodes. members of the resulting set are uniformly random representatives of the input subset members.
RanSub offers several ways of constructing distribute sets.
For our system, we choose the RanSub-nondescendants option. In this case, each node receives a random subset consisting of all nodes excluding its descendants. This is appropriate for our download structure where descendants are expected to have less content than an ancestor node in most cases.
A parent creates RanSub-nondescendants distribute sets for each child by compacting collect sets from that child"s siblings and its own distribute set. The result is a distribute set that contains a random subset representing all nodes in the tree except for those rooted at that particular child. We depict an example of RanSub"s collect-distribute process in Figure 2. In the figure, AS stands for node A"s state.
Assuming we can enable a node to locate a peer with disjoint content using RanSub, we need a method for reconciling the differences in the data. Additionally, we require a bandwidth-efficient method with low computational overhead. We chose to implement the approximate reconciliation techniques proposed in [6] for these tasks in Bullet.
To describe the content, nodes maintain working sets. The working set contains sequence numbers of packets that have been successfully received by each node over some period of time. We need the ability to quickly discern the resemblance between working sets from two nodes and decide whether a fine-grained reconciliation is beneficial. Summary tickets, or min-wise sketches [5], serve this purpose. The main idea is to create a summary ticket that is an unbiased random sample of the working set. A summary ticket is a small fixed-size array. Each entry in this array is maintained by a specific permutation function. The goal is to have each entry populated by the element with the smallest permuted value. To insert a new element into the summary ticket, we apply the permutation functions in order and update array values as appropriate.
The permutation function can be thought of as a specialized hash function. The choice of permutation functions is important as the quality of the summary ticket depends directly on the randomness properties of the permutation functions. Since we require them to have a low computational overhead, we use simple permutation functions, such as Pj(x) = (ax+b)mod|U|, where U is the universe size (dependant on the data encoding scheme). To compute the resemblance between two working sets, we compute the number of summary ticket entries that have the same value, and divide it by the total number of entries in the summary tickets. Figure 3 shows the way the permutation functions are used to populate the summary ticket. 12 10 2 27 7 2 18 19 40 1 Workingset 14 42 17 33 38 15 12 P1 33 29 28 44 57 15 P2 22 28 45 61 14 51 Pn… … Summary ticket minminmin 10 2 Figure 3: Example showing a sample summary ticket being constructed from the working set.
To perform approximate fine-grain reconciliation, a peer A sends its digest to peer B and expects to receive packets not described in the digest. For this purpose, we use a Bloom filter [4], a bit array of size m with k independent associated hash functions. An element s from the set of received keys S = {so, s2, . . . , sn−1} is inserted into the filter by computing the hash values h0, h1, . . . , hk−1 of s and setting the bits in the array that correspond to the hashed 285 values. To check whether an element x is in the Bloom filter, we hash it using the hash functions and check whether all positions in the bit array are set. If at least one is not set, we know that the Bloom filter does not contain x.
When using Bloom filters, the insertion of different elements might cause all the positions in the bit array corresponding to an element that is not in the set to be nonzero.
In this case, we have a false positive. Therefore, it is possible that peer B will not send a packet to peer A even though A is missing it. On the other hand, a node will never send a packet that is described in the Bloom filter, i.e. there are no false negatives. The probability of getting a false positive pf on the membership query can be expressed as a function of the ratio m n and the number of hash functions k: pf = (1 − e−kn/m )k . We can therefore choose the size of the Bloom filter and the number of hash functions that will yield a desired false positive ratio.
Although most traffic in the Internet today is best served by TCP, applications that require a smooth sending rate and that have a higher tolerance for loss often find TCP"s reaction to a single dropped packet to be unnecessarily severe. TCP Friendly Rate Control, or TFRC, targets unicast streaming multimedia applications with a need for less drastic responses to single packet losses [15]. TCP halves the sending rate as soon as one packet loss is detected.
Alternatively, TFRC is an equation-based congestion control protocol that is based on loss events, which consist of multiple packets being dropped within one round-trip time. Unlike TCP, the goal of TFRC is not to find and use all available bandwidth, but instead to maintain a relatively steady sending rate while still being responsive to congestion.
To guarantee fairness with TCP, TFRC uses the response function that describes the steady-state sending rate of TCP to determine the transmission rate in TFRC. The formula of the TCP response function [27] used in TFRC to describe the sending rate is: T = s R Õ2p 3 +tRT O(3 Õ3p 8 )p(1+32p2) This is the expression for the sending rate T in bytes/second, as a function of the round-trip time R in seconds, loss event rate p, packet size s in bytes, and TCP retransmit value tRT O in seconds.
TFRC senders and receivers must cooperate to achieve a smooth transmission rate. The sender is responsible for computing the weighted round-trip time estimate R between sender and receiver, as well as determining a reasonable retransmit timeout value tRT O. In most cases, using the simple formula tRT O = 4R provides the necessary fairness with TCP. The sender is also responsible for adjusting the sending rate T in response to new values of the loss event rate p reported by the receiver. The sender obtains a new measure for the loss event rate each time a feedback packet is received from the receiver. Until the first loss is reported, the sender doubles its transmission rate each time it receives feedback just as TCP does during slow-start.
The main role of the receiver is to send feedback to the sender once per round-trip time and to calculate the loss event rate included in the feedback packets. To obtain the loss event rate, the receiver maintains a loss interval array that contains values for the last eight loss intervals. A loss interval is defined as the number of packets received correctly between two loss events. The array is continually updated as losses are detected. A weighted average is computed based on the sum of the loss interval values, and the inverse of the sum is the reported loss event rate, p.
When implementing Bullet, we used an unreliable version of TFRC. We wanted a transport protocol that was congestion aware and TCP friendly. Lost packets were more easily recovered from other sources rather than waiting for a retransmission from the initial sender. Hence, we eliminate retransmissions from TFRC. Further, TFRC does not aggressively seek newly available bandwidth like TCP, a desirable trait in an overlay tree where there might be multiple competing flows sharing the same links. For example, if a leaf node in the tree tried to aggressively seek out new bandwidth, it could create congestion all the way up to the root of the tree. By using TFRC we were able to avoid these scenarios.
Bullet is an efficient data distribution system for bandwidth intensive applications. While many current overlay network distribution algorithms use a distribution tree to deliver data from the tree"s root to all other nodes,
Bullet layers a mesh on top of an original overlay tree to increase overall bandwidth to all nodes in the tree. Hence, each node receives a parent stream from its parent in the tree and some number of perpendicular streams from chosen peers in the overlay. This has significant bandwidth impact when a single node in the overlay is unable to deliver adequate bandwidth to a receiving node.
Bullet requires an underlying overlay tree for RanSub to deliver random subsets of participants"s state to nodes in the overlay, informing them of a set of nodes that may be good candidates for retrieving data not available from any of the node"s current peers and parent. While we also use the underlying tree for baseline streaming, this is not critical to Bullet"s ability to efficiently deliver data to nodes in the overlay. As a result, Bullet is capable of functioning on top of essentially any overlay tree. In our experiments, we have run Bullet over random and bandwidth-optimized trees created oﬄine (with global topological knowledge). Bullet registers itself with the underlying overlay tree so that it is informed when the overlay changes as nodes come and go or make performance transformations in the overlay.
As with streaming overlays trees, Bullet can use standard transports such as TCP and UDP as well as our implementation of TFRC. For the remainder of this paper, we assume the use of TFRC since we primarily target streaming highbandwidth content and we do not require reliable or in-order delivery. For simplicity, we assume that packets originate at the root of the tree and are tagged with increasing sequence numbers. Each node receiving a packet will optionally forward it to each of its children, depending on a number of factors relating to the child"s bandwidth and its relative position in the tree.
RanSub periodically delivers subsets of uniformly random selected nodes to each participant in the overlay. Bullet receivers use these lists to locate remote peers able to transmit missing data items with good bandwidth. RanSub messages contain a set of summary tickets that include a small (120 286 bytes) summary of the data that each node contains.
RanSub delivers subsets of these summary tickets to nodes every configurable epoch (5 seconds by default). Each node in the tree maintains a working set of the packets it has received thus far, indexed by sequence numbers. Nodes associate each working set with a Bloom filter that maintains a summary of the packets received thus far. Since the Bloom filter does not exceed a specific size (m) and we would like to limit the rate of false positives, Bullet periodically cleans up the Bloom filter by removing lower sequence numbers from it.
This allows us to keep the Bloom filter population n from growing at an unbounded rate. The net effect is that a node will attempt to recover packets for a finite amount of time depending on the packet arrival rate. Similarly, Bullet removes older items that are not needed for data reconstruction from its working set and summary ticket.
We use the collect and distribute phases of RanSub to carry Bullet summary tickets up and down the tree. In our current implementation, we use a set size of 10 summary tickets, allowing each collect and distribute to fit well within the size of a non-fragmented IP packet. Though Bullet supports larger set sizes, we expect this parameter to be tunable to specific applications" needs. In practice, our default size of 10 yields favorable results for a variety of overlays and network topologies. In essence, during an epoch a node receives a summarized partial view of the system"s state at that time. Upon receiving a random subset each epoch, a Bullet node may choose to peer with the node having the lowest similarity ratio when compared to its own summary ticket. This is done only when the node has sufficient space in its sender list to accept another sender (senders with lackluster performance are removed from the current sender list as described in section 3.4). Once a node has chosen the best node it sends it a peering request containing the requesting node"s Bloom filter. Such a request is accepted by the potential sender if it has sufficient space in its receiver list for the incoming receiver. Otherwise, the send request is rejected (space is periodically created in the receiver lists as further described in section 3.4).
Assuming it has space for the new peer, a recipient of the peering request installs the received Bloom filter and will periodically transmit keys not present in the Bloom filter to the requesting node. The requesting node will refresh its installed Bloom filters at each of its sending peers periodically. Along with the fresh filter, a receiving node will also assign a portion of the sequence space to each of its senders.
In this way, a node is able the reduce the likelihood that two peers simultaneously transmit the same key to it, wasting network resources. A node divides the sequence space in its current working set among each of its senders uniformly.
As illustrated in Figure 4, a Bullet receiver views the data space as a matrix of packet sequences containing s rows, where s is its current number of sending peers. A receiver periodically (every 5 seconds by default) updates each sender with its current Bloom filter and the range of sequences covered in its Bloom filter. This identifies the range of packets that the receiver is currently interested in recovering. Over time, this range shifts as depicted in Figure 4-b). In addition, the receiving node assigns to each sender a row from the matrix, labeled mod. A sender will forward packets to b) Mod = 3 00000000000000000000000000000000001111111111111111111111111111111111 7 1 2 8 a) Senders = 7Mod = 2 Low High Time 00000000000000000000000000000000001111111111111111111111111111111111 Figure 4: A Bullet receiver views data as a matrix of sequenced packets with rows equal to the number of peer senders it currently has. It requests data within the range (Low, High) of sequence numbers based on what it has received. a) The receiver requests a specific row in the sequence matrix from each sender. b) As it receives more data, the range of sequences advances and the receiver requests different rows from senders. the receiver that have a sequence number x such that x modulo s equals the mod number. In this fashion, receivers register to receive disjoint data from their sending peers.
By specifying ranges and matrix rows, a receiver is unlikely to receive duplicate data items, which would result in wasted bandwidth. A duplicate packet, however, may be received when a parent recovers a packet from one of its peers and relays the packet to its children (and descendants). In this case, a descendant would receive the packet out of order and may have already recovered it from one of its peers. In practice, this wasteful reception of duplicate packets is tolerable; less than 10% of all received packets are duplicates in our experiments.
We now provide details of Bullet"s mechanisms to increase the ease by which nodes can find disjoint data not provided by parents. We operate on the premise that the main challenge in recovering lost data packets transmitted over an overlay distribution tree lies in finding the peer node housing the data to recover. Many systems take a hierarchical approach to this problem, propagating repair requests up the distribution tree until the request can be satisfied.
This ultimately leads to scalability issues at higher levels in the hierarchy particularly when overlay links are bandwidthconstrained.
On the other hand, Bullet attempts to recover lost data from any non-descendant node, not just ancestors, thereby increasing overall system scalability. In traditional overlay distribution trees, packets are lost by the transmission transport and/or the network. Nodes attempt to stream data as fast as possible to each child and have essentially no control over which portions of the data stream are dropped by the transport or network. As a result, the streaming subsystem has no control over how many nodes in the system will ultimately receive a particular portion of the data. If few nodes receive a particular range of packets, recovering these pieces of data becomes more difficult, requiring increased communication costs, and leading to scalability problems.
In contrast, Bullet nodes are aware of the bandwidth achievable to each of its children using the underlying transport. If 287 a child is unable to receive the streaming rate that the parent receives, the parent consciously decides which portion of the data stream to forward to the constrained child. In addition, because nodes recover data from participants chosen uniformly at random from the set of non-descendants, it is advantageous to make each transmitted packet recoverable from approximately the same number of participant nodes.
That is, given a randomly chosen subset of peer nodes, it is with the same probability that each node has a particular data packet. While not explicitly proven here, we believe that this approach maximizes the probability that a lost data packet can be recovered, regardless of which packet is lost. To this end, Bullet distributes incoming packets among one or more children in hopes that the expected number of nodes receiving each packet is approximately the same.
A node p maintains for each child, i, a limiting and sending factor, lfi and sfi. These factors determine the proportion of p"s received data rate that it will forward to each child.
The sending factor sfi is the portion of the parent stream (rate) that each child should own based on the number of descendants the child has. The more descendants a child has, the larger the portion of received data it should own.
The limiting factor lfi represents the proportion of the parent rate beyond the sending factor that each child can handle. For example, a child with one descendant, but high bandwidth would have a low sending factor, but a very high limiting factor. Though the child is responsible for owning a small portion of the received data, it actually can receive a large portion of it.
Because RanSub collects descendant counts di for each child i, Bullet simply makes a call into RanSub when sending data to determine the current sending factors of its children.
For each child i out of k total, we set the sending factor to be: sfi = diÈk j=1 dj .
In addition, a node tracks the data successfully transmitted via the transport. That is, Bullet data transport sockets are non-blocking; successful transmissions are send attempts that are accepted by the non-blocking transport. If the transport would block on a send (i.e., transmission of the packet would exceed the TCP-friendly fair share of network resources), the send fails and is counted as an unsuccessful send attempt. When a data packet is received by a parent, it calculates the proportion of the total data stream that has been sent to each child, thus far, in this epoch. It then assigns ownership of the current packet to the child with sending proportion farthest away from its sfi as illustrated in Figure 5.
Having chosen the target of a particular packet, the parent attempts to forward the packet to the child. If the send is not successful, the node must find an alternate child to own the packet. This occurs when a child"s bandwidth is not adequate to fulfill its responsibilities based on its descendants (sfi). To compensate, the node attempts to deterministically find a child that can own the packet (as evidenced by its transport accepting the packet). The net result is that children with more than adequate bandwidth will own more of their share of packets than those with inadequate bandwidth. In the event that no child can accept a packet, it must be dropped, corresponding to the case where the sum of all children bandwidths is inadequate to serve the received foreach child in children { if ( (child->sent / total_sent) < child->sending_factor) target_child = child; } if (!senddata( target_child->addr, msg, size, key)) { // send succeeded target_child->sent++; target_child->child_filter.insert(got_key); sent_packet = 1; } foreach child in children { should_send = 0; if (!sent_packet) // transfer ownership should_send = 1; else // test for available bandwidth if ( key % (1.0/child->limiting_factor) == 0 ) should_send = 1; if (should_send) { if (!senddata( child->addr, msg, size, key)) { if (!sent_packet) // i received ownership child->sent++; else increase(child->limiting_factor); child->child_filter.insert(got_key); sent_packet = 1; } else // send failed if (sent_packet) // was for extra bw decrease(child->limiting_factor); } } Figure 5: Pseudo code for Bullet"s disjoint data send routine stream. While making data more difficult to recover, Bullet still allows for recovery of such data to its children. The sending node will cache the data packet and serve it to its requesting peers. This process allows its children to potentially recover the packet from one of their own peers, to whom additional bandwidth may be available.
Once a packet has been successfully sent to the owning child, the node attempts to send the packet to all other children depending on the limiting factors lfi. For each child i, a node attempts to forward the packet deterministically if the packet"s sequence modulo 1/lfi is zero. Essentially, this identifies which lfi fraction of packets of the received data stream should be forwarded to each child to make use of the available bandwidth to each. If the packet transmission is successful, lfi is increased such that one more packet is to be sent per epoch. If the transmission fails, lfi is decreased by the same amount. This allows children limiting factors to be continuously adjusted in response to changing network conditions.
It is important to realize that by maintaining limiting factors, we are essentially using feedback from children (by observing transport behavior) to determine the best data to stop sending during times when a child cannot handle the entire parent stream. In one extreme, if the sum of children bandwidths is not enough to receive the entire parent stream, each child will receive a completely disjoint data stream of packets it owns. In the other extreme, if each 288 child has ample bandwidth, it will receive the entire parent stream as each lfi would settle on 1.0. In the general case, our owning strategy attempts to make data disjoint among children subtrees with the guiding premise that, as much as possible, the expected number of nodes receiving a packet is the same across all packets.
Bullet allows a maximum number of peering relationships.
That is, a node can have up to a certain number of receivers and a certain number of senders (each defaults to 10 in our implementation). A number of considerations can make the current peering relationships sub-optimal at any given time: i) the probabilistic nature of RanSub means that a node may not have been exposed to a sufficiently appropriate peer, ii) receivers greedily choose peers, and iii) network conditions are constantly changing. For example, a sender node may wind up being unable to provide a node with very much useful (non-duplicate) data. In such a case, it would be advantageous to remove that sender as a peer and find some other peer that offers better utility.
Each node periodically (every few RanSub epochs) evaluates the bandwidth performance it is receiving from its sending peers. A node will drop a peer if it is sending too many duplicate packets when compared to the total number of packets received. This threshold is set to 50% by default.
If no such wasteful sender is found, a node will drop the sender that is delivering the least amount of useful data to it. It will replace this sender with some other sending peer candidate, essentially reserving a trial slot in its sender list.
In this way, we are assured of keeping the best senders seen so far and will eliminate senders whose performance deteriorates with changing network conditions.
Likewise, a Bullet sender will periodically evaluate its receivers. Each receiver updates senders of the total received bandwidth. The sender, knowing the amount of data it has sent to each receiver, can determine which receiver is benefiting the least by peering with this sender. This corresponds to the one receiver acquiring the least portion of its bandwidth through this sender. The sender drops this receiver, creating an empty slot for some other trial receiver. This is similar to the concept of weans presented in [24].
We have evaluated Bullet"s performance in real Internet environments as well as the ModelNet [37] IP emulation framework. While the bulk of our experiments use ModelNet, we also report on our experience with Bullet on the PlanetLab Internet testbed [31]. In addition, we have implemented a number of underlying overlay network trees upon which Bullet can execute. Because Bullet performs well over a randomly created overlay tree, we present results with Bullet running over such a tree compared against an oﬄine greedy bottleneck bandwidth tree algorithm using global topological information described in Section 4.1.
All of our implementations leverage a common development infrastructure called MACEDON [33] that allows for the specification of overlay algorithms in a simple domainspecific language. It enables the reuse of the majority of common functionality in these distributed systems, including probing infrastructures, thread management, message passing, and debugging environment. As a result, we believe that our comparisons qualitatively show algorithmic differences rather than implementation intricacies. Our implementation of the core Bullet logic is under 1000 lines of code in this infrastructure.
Our ModelNet experiments make use of 50 2Ghz Pentium4"s running Linux 2.4.20 and interconnected with 100 Mbps and 1 Gbps Ethernet switches. For the majority of these experiments, we multiplex one thousand instances (overlay participants) of our overlay applications across the 50 Linux nodes (20 per machine). In ModelNet, packet transmissions are routed through emulators responsible for accurately emulating the hop-by-hop delay, bandwidth, and congestion of a network topology. In our evaluations, we used four 1.4Ghz Pentium III"s running FreeBSD-4.7 as emulators. This platform supports approximately 2-3 Gbps of aggregate simultaneous communication among end hosts. For most of our ModelNet experiments, we use 20,000-node INET-generated topologies [10]. We randomly assign our participant nodes to act as clients connected to one-degree stub nodes in the topology. We randomly select one of these participants to act as the source of the data stream.
Propagation delays in the network topology are calculated based on the relative placement of the network nodes in the plane by INET. Based on the classification in [8], we classify network links as being Client-Stub, Stub-Stub,
TransitStub, and Transit-Transit depending on their location in the network. We restrict topological bandwidth by setting the bandwidth for each link depending on its type. Each type of link has an associated bandwidth range from which the bandwidth is chosen uniformly at random. By changing these ranges, we vary bandwidth constraints in our topologies. For our experiments, we created three different ranges corresponding to low, medium, and high bandwidths relative to our typical streaming rates of 600-1000 Kbps as specified in Table 1. While the presented ModelNet results are restricted to two topologies with varying bandwidth constraints, the results of experiments with additional topologies all show qualitatively similar behavior.
We do not implement any particular coding scheme for our experiments. Rather, we assume that either each sequence number directly specifies a particular data block and the block offset for each packet, or we are distributing data within the same block for LT Codes, e.g., when distributing a file.
One of our goals is to determine Bullet"s performance relative to the best possible bandwidth-optimized tree for a given network topology. This allows us to quantify the possible improvements of an overlay mesh constructed using Bullet relative to the best possible tree. While we have not yet proven this, we believe that this problem is NP-hard.
Thus, in this section we present a simple greedy oﬄine algorithm to determine the connectivity of a tree likely to deliver a high level of bandwidth. In practice, we are not aware of any scalable online algorithms that are able to deliver the bandwidth of an oﬄine algorithm. At the same time, trees constructed by our algorithm tend to be long and skinny making them less resilient to failures and inappropriate for delay sensitive applications (such as multimedia streaming).
In addition to any performance comparisons, a Bullet mesh has much lower depth than the bottleneck tree and is more resilient to failure, as discussed in Section 4.6. 289 Topology classification Client-Stub Stub-Stub Transit-Stub Transit-Transit Low bandwidth 300-600 500-1000 1000-2000 2000-4000 Medium bandwidth 800-2800 1000-4000 1000-4000 5000-10000 High bandwidth 1600-5600 2000-8000 2000-8000 10000-20000 Table 1: Bandwidth ranges for link types used in our topologies expressed in Kbps.
Specifically, we consider the following problem: given complete knowledge of the topology (individual link latencies, bandwidth, and packet loss rates), what is the overlay tree that will deliver the highest bandwidth to a set of predetermined overlay nodes? We assume that the throughput of the slowest overlay link (the bottleneck link) determines the throughput of the entire tree. We are, therefore, trying to find the directed overlay tree with the maximum bottleneck link. Accordingly, we refer to this problem as the overlay maximum bottleneck tree (OMBT). In a simplified case, assuming that congestion only exists on access links and there are no lossy links, there exists an optimal algorithm [23].
In the more general case of contention on any physical link, and when the system is allowed to choose the routing path between the two endpoints, this problem is known to be NP-hard [12], even in the absence of link losses. For the purposes of this paper, our goal is to determine a good overlay streaming tree that provides each overlay participant with substantial bandwidth, while avoiding overlay links with high end-to-end loss rates.
We make the following assumptions:
is fixed. This closely models the existing overlay network model with IP for unicast routing.
connections to transfer data point-to-point.
throughput of a TCP-friendly flow using a steady-state formula [27].
each flow can achieve throughput of at most c n , where c is the physical capacity of the link.
Given these assumptions, we concentrate on estimating the throughput available between two participants in the overlay. We start by calculating the throughput using the steady-state formula. We then route the flow in the network, and consider the physical links one at a time. On each physical link, we compute the fair-share for each of the competing flows. The throughput of an overlay link is then approximated by the minimum of the fair-shares along the routing path, and the formula rate. If some flow does not require the same share of the bottleneck link as other competing flows (i.e., its throughput might be limited by losses elsewhere in the network), then the other flows might end up with a greater share than the one we compute. We do not account for this, as the major goal of this estimate is simply to avoid lossy and highly congested physical links.
More formally, we define the problem as follows: Overlay Maximum Bottleneck Tree (OMBT).
Given a physical network represented as a graph G = (V, E), set of overlay participants P ⊂ V , source node (s ∈ P), bandwidth B : E → R+ , loss rate L : E → [0, 1], propagation delay D : E → R+ of each link, set of possible overlay links O = {(v, w) | v, w ∈ P, v = w}, routing table RT : O × E → {0, 1}, find the overlay tree T = {o | o ∈ O} (|T| = |P| − 1, ∀v ∈ P there exists a path ov = s ❀ v) that maximizes min o|o∈T (min(f(o), min e|e∈o b(e) |{p | p ∈ T, e ∈ p}| )) where f(o) is the TCP steady-state sending rate, computed from round-trip time d(o) = Èe∈o d(e) + Èe∈o d(e) (given overlay link o = (v, w), o = (w, v)), and loss rate l(o) = 1 − Ée∈o (1 − l(e)). We write e ∈ o to express that link e is included in the o"s routing path (RT(o, e) = 1).
Assuming that we can estimate the throughput of a flow, we proceed to formulate a greedy OMBT algorithm. This algorithm is non-optimal, but a similar approach was found to perform well [12].
Our algorithm is similar to the Widest Path Heuristic (WPH) [12], and more generally to Prim"s MST algorithm [32].
During its execution, we maintain the set of nodes already in the tree, and the set of remaining nodes. To grow the tree, we consider all the overlay links leading from the nodes in the tree to the remaining nodes. We greedily pick the node with the highest throughput overlay link. Using this overlay link might cause us to route traffic over physical links traversed by some other tree flows. Since we do not re-examine the throughput of nodes that are already in the tree, they might end up being connected to the tree with slower overlay links than initially estimated. However, by attaching the node with the highest residual bandwidth at every step, we hope to lessen the effects of after-the-fact physical link sharing. With the synthetic topologies we use for our emulation environment, we have not found this inaccuracy to severely impact the quality of the tree.
We have implemented a simple streaming application that is capable of streaming data over any specified tree. In our implementation, we are able to stream data through overlay trees using UDP, TFRC, or TCP. Figure 6 shows average bandwidth that each of 1000 nodes receives via this streaming as time progresses on the x-axis. In this example, we use TFRC to stream 600 Kbps over our oﬄine bottleneck bandwidth tree and a random tree (other random trees exhibit qualitatively similar behavior). In these experiments, streaming begins 100 seconds into each run. While the random tree delivers an achieved bandwidth of under 100 Kbps, our oﬄine algorithm overlay delivers approximately 400 Kbps of data. For this experiment, bandwidths were set to the medium range from Table 1. We believe that any degree-constrained online bandwidth overlay tree algorithm would exhibit similar (or lower) behavior to our bandwidth290 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bottleneck bandwidth tree Random tree Figure 6: Achieved bandwidth over time for TFRC streaming over the bottleneck bandwidth tree and a random tree. optimized overlay. Hence, Bullet"s goal is to overcome this bandwidth limit by allowing for the perpendicular reception of data and by utilizing disjoint data flows in an attempt to match or exceed the performance of our oﬄine algorithm.
To evaluate Bullet"s ability to exceed the bandwidth achievable via tree distribution overlays, we compare Bullet running over a random overlay tree to the streaming behavior shown in Figure 6. Figure 7 shows the average bandwidth received by each node (labeled Useful total) with standard deviation. The graph also plots the total amount of data received and the amount of data a node receives from its parent. For this topology and bandwidth setting, Bullet was able to achieve an average bandwidth of 500 Kbps, fives times that achieved by the random tree and more than 25% higher than the oﬄine bottleneck bandwidth algorithm.
Further, the total bandwidth (including redundant data) received by each node is only slightly higher than the useful content, meaning that Bullet is able to achieve high bandwidth while wasting little network resources. Bullet"s use of TFRC in this example ensures that the overlay is TCP friendly throughout. The average per-node control overhead is approximately 30 Kbps. By tracing certain packets as they move through the system, we are able to acquire link stress estimates of our system. Though the link stress can be different for each packet since each can take a different path through the overlay mesh, we average link stress due to each traced packet. For this experiment, Bullet has an average link stress of approximately 1.5 with an absolute maximum link stress of 22.
The standard deviation in most of our runs is fairly high because of the limited bandwidth randomly assigned to some Client-Stub and Stub-Stub links. We feel that this is consistent with real Internet behavior where clients have widely varying network connectivity. A time slice is shown in Figure 8 that plots the CDF of instantaneous bandwidths that each node receives. The graph shows that few client nodes receive inadequate bandwidth even though they are bandwidth constrained. The distribution rises sharply starting at approximately 500 Kbps. The vast majority of nodes receive a stream of 500-600 Kbps.
We have evaluated Bullet under a number of bandwidth constraints to determine how Bullet performs relative to the 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 7: Achieved bandwidth over time for Bullet over a random tree. 0
1 0 100 200 300 400 500 600 700 800 Percentageofnodes Bandwidth(Kbps) Figure 8: CDF of instantaneous achieved bandwidth at time 430 seconds. available bandwidth of the underlying topology. Table 1 describes representative bandwidth settings for our streaming rate of 600 Kbps. The intent of these settings is to show a scenario where more than enough bandwidth is available to achieve a target rate even with traditional tree streaming, an example of where it is slightly not sufficient, and one in which the available bandwidth is quite restricted. Figure 9 shows achieved bandwidths for Bullet and the bottleneck bandwidth tree over time generated from topologies with bandwidths in each range.
In all of our experiments, Bullet outperforms the bottleneck bandwidth tree by a factor of up to 100%, depending on how much bandwidth is constrained in the underlying topology. In one extreme, having more than ample bandwidth,
Bullet and the bottleneck bandwidth tree are both able to stream at the requested rate (600 Kbps in our example). In the other extreme, heavily constrained topologies allow Bullet to achieve twice the bandwidth achievable via the bottleneck bandwidth tree. For all other topologies, Bullet"s benefits are somewhere in between. In our example, Bullet running over our medium-constrained bandwidth topology is able to outperform the bottleneck bandwidth tree by a factor of 25%. Further, we stress that we believe it would 291 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bullet - High Bandwidth Bottleneck tree - High Bandwidth Bullet - Medium Bandwidth Bottleneck tree - Medium Bandwidth Bullet - Low Bandwidth Bottleneck tree - Low Bandwidth Figure 9: Achieved bandwidth for Bullet and bottleneck tree over time for high, medium, and low bandwidth topologies. be extremely difficult for any online tree-based algorithm to exceed the bandwidth achievable by our oﬄine bottleneck algorithm that makes use of global topological information.
For instance, we built a simple bandwidth optimizing overlay tree construction based on Overcast [21]. The resulting dynamically constructed trees never achieved more than 75% of the bandwidth of our own oﬄine algorithm.
Bullet"s ability to deliver high bandwidth levels to nodes depends on its disjoint transmission strategy. That is, when bandwidth to a child is limited, Bullet attempts to send the correct portions of data so that recovery of the lost data is facilitated. A Bullet parent sends different data to its children in hopes that each data item will be readily available to nodes spread throughout its subtree. It does so by assigning ownership of data objects to children in a manner that makes the expected number of nodes holding a particular data object equal for all data objects it transmits. Figure 10 shows the resulting bandwidth over time for the non-disjoint strategy in which a node (and more importantly, the root of the tree) attempts to send all data to each of its children (subject to independent losses at individual child links). Because the children transports throttle the sending rate at each parent, some data is inherently sent disjointly (by chance). By not explicitly choosing which data to send its child, this approach deprives Bullet of 25% of its bandwidth capability, when compared to the case when our disjoint strategy is enabled in Figure 7.
In this section, we explore how Bullet compares to data dissemination approaches that use some form of epidemic routing. We implemented a form of gossiping, where a node forwards non-duplicate packets to a randomly chosen number of nodes in its local view. This technique does not use a tree for dissemination, and is similar to lpbcast [14] (recently improved to incorporate retrieval of data objects [13]).
We do not disseminate packets every T seconds; instead we forward them as soon as they arrive. 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 10: Achieved bandwidth over time using nondisjoint data transmission.
We also implemented a pbcast-like [2] approach for retrieving data missing from a data distribution tree. The idea here is that nodes are expected to obtain most of their data from their parent. Nodes then attempt to retrieve any missing data items through gossiping with random peers.
Instead of using gossiping with a fixed number of rounds for each packet, we use anti-entropy with a FIFO Bloom filter to attempt to locate peers that hold any locally missing data items.
To make our evaluation conservative, we assume that nodes employing gossip and anti-entropy recovery are able to maintain full group membership. While this might be difficult in practice, we assume that RanSub [24] could also be applied to these ideas, specifically in the case of anti-entropy recovery that employs an underlying tree. Further, we also allow both techniques to reuse other aspects of our implementation: Bloom filters, TFRC transport, etc. To reduce the number of duplicate packets, we use less peers in each round (5) than Bullet (10). For our configuration, we experimentally found that 5 peers results in the best performance with the lowest overhead. In our experiments, increasing the number of peers did not improve the average bandwidth achieved throughout the system. To allow TFRC enough time to ramp up to the appropriate TCP-friendly sending rate, we set the epoch length for anti-entropy recovery to 20 seconds.
For these experiments, we use a 5000-node INET topology with no explicit physical link losses. We set link bandwidths according to the medium range from Table 1, and randomly assign 100 overlay participants. The randomly chosen root either streams at 900 Kbps (over a random tree for Bullet and greedy bottleneck tree for anti-entropy recovery), or sends packets at that rate to randomly chosen nodes for gossiping. Figure 11 shows the resulting bandwidth over time achieved by Bullet and the two epidemic approaches. As expected, Bullet comes close to providing the target bandwidth to all participants, achieving approximately 60 percent more then gossiping and streaming with anti-entropy. The two epidemic techniques send an excessive number of duplicates, effectively reducing the useful bandwidth provided to each node. More importantly, both approaches assign equal significance to other peers, regardless of the available band292 0 500 1000 1500 2000 0 50 100 150 200 250 300 Bandwidth(Kbps) Time (s) Push gossiping raw Streaming w/AE raw Bullet raw Bullet useful Push gossiping useful Streaming w/AE useful Figure 11: Achieved bandwidth over time for Bullet and epidemic approaches. width and the similarity ratio. Bullet, on the other hand, establishes long-term connections with peers that provide good bandwidth and disjoint content, and avoids most of the duplicates by requesting disjoint data from each node"s peers.
To evaluate Bullet"s performance under more lossy network conditions, we have modified our 20,000-node topologies used in our previous experiments to include random packet losses. ModelNet allows the specification of a packet loss rate in the description of a network link. Our goal by modifying these loss rates is to simulate queuing behavior when the network is under load due to background network traffic.
To effect this behavior, we first modify all non-transit links in each topology to have a packet loss rate chosen uniformly random from [0, 0.003] resulting in a maximum loss rate of
maximum loss rate of 0.1%. Similar to the approach in [28], we randomly designated 5% of the links in the topologies as overloaded and set their loss rates uniformly random from [0.05, 0.1] resulting in a maximum packet loss rate of 10%.
Figure 12 shows achieved bandwidths for streaming over Bullet and using our greedy oﬄine bottleneck bandwidth tree. Because losses adversely affect the bandwidth achievable over TCP-friendly transport and since bandwidths are strictly monotonically decreasing over a streaming tree, treebased algorithms perform considerably worse than Bullet when used on a lossy network. In all cases, Bullet delivers at least twice as much bandwidth than the bottleneck bandwidth tree. Additionally, losses in the low bandwidth topology essentially keep the bottleneck bandwidth tree from delivering any data, an artifact that is avoided by Bullet.
In this section, we discuss Bullet"s behavior in the face of node failure. In contrast to streaming distribution trees that must quickly detect and make tree transformations to overcome failure, Bullet"s failure resilience rests on its ability to maintain a higher level of achieved bandwidth by virtue of perpendicular (peer) streaming. While all nodes under a failed node in a distribution tree will experience a temporary 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bullet - High Bandwidth Bullet - Medium Bandwidth Bottleneck tree - High Bandwidth Bottleneck tree - Medium Bandwidth Bullet - Low Bandwidth Bottleneck tree - Low Bandwidth Figure 12: Achieved bandwidths for Bullet and bottleneck bandwidth tree over a lossy network topology. disruption in service, Bullet nodes are able compensate for this by receiving data from peers throughout the outage.
Because Bullet, and, more importantly, RanSub makes use of an underlying tree overlay, part of Bullet"s failure recovery properties will depend on the failure recovery behavior of the underlying tree. For the purposes of this discussion, we simply assume the worst-case scenario where an underlying tree has no failure recovery. In our failure experiments, we fail one of root"s children (with 110 of the total 1000 nodes as descendants) 250 seconds after data streaming is started. By failing one of root"s children, we are able to show Bullet"s worst-case performance under a single node failure.
In our first scenario, we disable failure detection in RanSub so that after a failure occurs, Bullet nodes request data only from their current peers. That is, at this point,
RanSub stops functioning and no new peer relationships are created for the remainder of the run. Figure 13 shows Bullet"s achieved bandwidth over time for this case. While the average achieved rate drops from 500 Kbps to 350 Kbps, most nodes (including the descendants of the failed root child) are able to recover a large portion of the data rate.
Next, we enable RanSub failure detection that recognizes a node"s failure when a RanSub epoch has lasted longer than the predetermined maximum (5 seconds for this test).
In this case, the root simply initiates the next distribute phase upon RanSub timeout. The net result is that nodes that are not descendants of the failed node will continue to receive updated random subsets allowing them to peer with appropriate nodes reflecting the new network conditions. As shown in Figure 14, the failure causes a negligible disruption in performance. With RanSub failure detection enabled, nodes quickly learn of other nodes from which to receive data. Once such recovery completes, the descendants of the failed node use their already established peer relationships to compensate for their ancestor"s failure. Hence, because Bullet is an overlay mesh, its reliability characteristics far exceed that of typical overlay distribution trees.
This section contains results from the deployment of Bullet over the PlanetLab [31] wide-area network testbed. For 293 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 13: Bandwidth over time with a worst-case node failure and no RanSub recovery. 0 200 400 600 800 1000 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 14: Bandwidth over time with a worst-case node failure and RanSub recovery enabled. our first experiment, we chose 47 nodes for our deployment, with no two machines being deployed at the same site. Since there is currently ample bandwidth available throughout the PlanetLab overlay (a characteristic not necessarily representative of the Internet at large), we designed this experiment to show that Bullet can achieve higher bandwidth than an overlay tree when the source is constrained, for instance in cases of congestion on its outbound access link, or of overload by a flash-crowd.
We did this by choosing a root in Europe connected to PlanetLab with fairly low bandwidth. The node we selected was in Italy (cs.unibo.it) and we had 10 other overlay nodes in Europe. Without global knowledge of the topology in PlanetLab (and the Internet), we are, of course, unable to produce our greedy bottleneck bandwidth tree for comparison. We ran Bullet over a random overlay tree for 300 seconds while attempting to stream at a rate of 1.5 Mbps.
We waited 50 seconds before starting to stream data to allow nodes to successfully join the tree. We compare the performance of Bullet to data streaming over multiple handcrafted trees. Figure 15 shows our results for two such trees.
The good tree has all nodes in Europe located high in the tree, close to the root. We used pathload [20] to measure the 0 200 400 600 800 1000 1200 0 50 100 150 200 250 Bandwidth(Kbps) Time (s) Bullet Good Tree Worst Tree Figure 15: Achieved bandwidth over time for Bullet and TFRC streaming over different trees on PlanetLab with a root in Europe. available bandwidth between the root and all other nodes.
Nodes with high bandwidth measurements were placed close to the root. In this case, we are able to achieve a bandwidth of approximately 300 Kbps. The worst tree was created by setting the root"s children to be the three nodes with the worst bandwidth characteristics from the root as measured by pathload. All subsequent levels in the tree were set in this fashion.
For comparison, we replaced all nodes in Europe from our topology with nodes in the US, creating a topology that only included US nodes with high bandwidth characteristics.
As expected, Bullet was able to achieve the full 1.5 Mbps rate in this case. A well constructed tree over this highbandwidth topology yielded slightly lower than 1.5 Mbps, verifying that our approach does not sacrifice performance under high bandwidth conditions and improves performance under constrained bandwidth scenarios.
Snoeren et al. [36] use an overlay mesh to achieve reliable and timely delivery of mission-critical data. In this system, every node chooses n parents from which to receive duplicate packet streams. Since its foremost emphasis is reliability, the system does not attempt to improve the bandwidth delivered to the overlay participants by sending disjoint data at each level. Further, during recovery from parent failure, it limits an overlay router"s choice of parents to nodes with a level number that is less than its own level number.
The power of perpendicular downloads is perhaps best illustrated by Kazaa [22], the popular peer-to-peer file swapping network. Kazaa nodes are organized into a scalable, hierarchical structure. Individual users search for desired content in the structure and proceed to simultaneously download potentially disjoint pieces from nodes that already have it. Since Kazaa does not address the multicast communication model, a large fraction of users downloading the same file would consume more bandwidth than nodes organized into the Bullet overlay structure. Kazaa does not use erasure coding; therefore it may take considerable time to locate the last few bytes. 294 BitTorrent [3] is another example of a file distribution system currently deployed on the Internet. It utilizes trackers that direct downloaders to random subsets of machines that already have portions of the file. The tracker poses a scalability limit, as it continuously updates the systemwide distribution of the file. Lowering the tracker communication rate could hurt the overall system performance, as information might be out of date. Further, BitTorrent does not employ any strategy to disseminate data to different regions of the network, potentially making it more difficult to recover data depending on client access patterns. Similar to Bullet,
BitTorrent incorporates the notion of choking at each node with the goal of identifying receivers that benefit the most by downloading from that particular source.
FastReplica [11] addresses the problem of reliable and efficient file distribution in content distribution networks (CDNs). In the basic algorithm, nodes are organized into groups of fixed size (n), with full group membership information at each node. To distribute the file, a node splits it into n equal-sized portions, sends the portions to other group members, and instructs them to download the missing pieces in parallel from other group members. Since only a fixed portion of the file is transmitted along each of the overlay links, the impact of congestion is smaller than in the case of tree distribution. However, since it treats all paths equally, FastReplica does not take full advantage of highbandwidth overlay links in the system. Since it requires file store-and-forward logic at each level of the hierarchy necessary for scaling the system, it may not be applicable to high-bandwidth streaming.
There are numerous protocols that aim to add reliability to IP multicast. In Scalable Reliable Multicast (SRM) [16], nodes multicast retransmission requests for missed packets.
Two techniques attempt to improve the scalability of this approach: probabilistic choice of retransmission timeouts, and organization of receivers into hierarchical local recovery groups. However, it is difficult to find appropriate timer values and local scoping settings (via the TTL field) for a wide range of topologies, number of receivers, etc. even when adaptive techniques are used. One recent study [2] shows that SRM may have significant overhead due to retransmission requests.
Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. In pbcast [2], a node has global group membership, and periodically chooses a random subset of peers to send a digest of its received packets. A node that receives the digest responds to the sender with the missing packets in a last-in, first-out fashion. Lbpcast [14] addresses pbcast"s scalability issues (associated with global knowledge) by constructing, in a decentralized fashion, a partial group membership view at each node. The average size of the views is engineered to allow a message to reach all participants with high probability. Since lbpcast does not require an underlying tree for data distribution and relies on the push-gossiping model, its network overhead can be quite high.
Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not blindly request retransmissions from their peers.
Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Further, a Bullet node splits the retransmission load between all of its peers. We note that pbcast nodes contain a mechanism to rate-limit retransmitted packets and to send different packets in response to the same digest. However, this does not guarantee that packets received in parallel from multiple peers will not be duplicates. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree.
Narada [19] builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. It then runs a standard routing protocol on top of the overlay mesh to construct forwarding trees using each node as a possible source. Narada nodes maintain global knowledge about all group participants, limiting system scalability to several tens of nodes.
Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent.
On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers.
Overcast [21] is an example of a bandwidth-efficient overlay tree construction algorithm. In this system, all nodes join at the root and migrate down to the point in the tree where they are still able to maintain some minimum level of bandwidth. Bullet is expected to be more resilient to node departures than any tree, including Overcast. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers.
This transition is seamless, as the node that is disconnected from its parent will start demanding more missing packets from its peers during the standard round of refreshing its filters. Overcast convergence time is limited by probes to immediate siblings and ancestors. Bullet is able to provide approximately a target bandwidth without having a fully converged tree.
In parallel to our own work, SplitStream [9] also has the goal of achieving high bandwidth data dissemination. It operates by splitting the multicast stream into k stripes, transmitting each stripe along a separate multicast tree built using Scribe [34]. The key design goal of the tree construction mechanism is to have each node be an intermediate node in at most one tree (while observing both inbound and outbound node bandwidth constraints), thereby reducing the impact of a single node"s sudden departure on the rest of the system. The join procedure can potentially sacrifice the interior-node-disjointness achieved by Scribe. Perhaps more importantly, SplitStream assumes that there is enough available bandwidth to carry each stripe on every link of the tree, including the links between the data source and the roots of individual stripe trees independently chosen by Scribe.
To some extent, Bullet and SplitStream are complementary.
For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe.
CoopNet [29] considers live content streaming in a peerto-peer environment, subject to high node churn.
Consequently, the system favors resilience over network efficiency.
It uses a centralized approach for constructing either random or deterministic node-disjoint (similar to SplitStream) trees, and it includes an MDC [17] adaptation framework based on scalable receiver feedback that attempts to maximize the signal-to-noise ratio perceived by receivers. In the case of on-demand streaming, CoopNet [30] addresses 295 the flash-crowd problem at the central server by redirecting incoming clients to a fixed number of nodes that have previously retrieved portions of the same content. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file.
Typically, high bandwidth overlay data streaming takes place over a distribution tree. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures. Specifically, this paper makes the following contributions: • We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques. • We provide a technique for recovering missing data from peers in a scalable and efficient manner.
RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants. • We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes. • A large-scale evaluation of 1000 overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree.
Acknowledgments We would like to thank David Becker for his invaluable help with our ModelNet experiments and Ken Yocum for his help with ModelNet emulation optimizations. In addition, we thank our shepherd Barbara Liskov and our anonymous reviewers who provided excellent feedback.

The Peer-to-Peer (P2P) computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files.
There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. While the utilization of P2P clients is already a gigantic step forward compared to downloading files off websites, using such programs are not without their problems.
The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier.
Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user"s machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations.
The rest of this paper is organized as follows. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8.
Apocrita is a distributed peer-to-peer file sharing system, and has been designed to make finding documents easier in an Intranet environment. Currently, it is possible for documents to be located on a user's machine or on a remote machine. It is even possible that different revisions could reside on each node on the Intranet.
This means there must be a manual process to maintain document versions. Apocrita solves this problem using two approaches.
First, due to the inherent nature of Apocrita, the document will only reside on a single logical location. Second, Apocrita provides a method of reverting to previous document versions. Apocrita Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
ACMSE"07, MARCH 23-24, 2007, WINSTON-SALEM, NC, USA.
COPYRIGHT 2007 ACM 978-1-59593-629-5/07/0003 …$5.00. 174 will also distribute documents across multiple machines to ensure high availability of important documents. For example, if a machine contains an important document and the machine is currently inaccessible, the system is capable of maintaining availability of the document through this distribution mechanism.
It provides a simple interface for searching and accessing files that may exist either locally or remotely. The distributed nature of the documents is transparent to the user. Apocrita supports a decentralized network model where the peers use a discovery protocol to determine peers.
Apocrita is intended for network users on an Intranet. The main focus is organizations that may not have a network large enough to require a file server and supporting infrastructure. It eliminates the need for documents to be manually shared between users while being edited and reduces the possibility of conflicting versions being distributed. The system also provides some redundancy and in the event of a single machine failure, no important documents will be lost. It is operating system independent, and easy to access through a web browser or through a standalone application. To decrease the time required for indexing a large number of documents, the indexing process is distributed across available idle nodes. Local and remote files should be easily accessible through a virtual mountable file system, providing transparency for users.
Apocrita uses a distributed index for all the documents that are available on the Intranet. Each node will contain part of the full index, and be aware of what part of the index each other node has.
A node will be able to contact each node that contains a unique portion of the index. In addition, each node has a separate local index of its own documents. But as discussed later, in the current implementation, each node has a copy of the entire index.
Indexing of the documents is distributed. Therefore, if a node is in the process of indexing many documents, it will break up the work over the nodes. Once a node"s local index is updated with the new documents, the distributed index will then be updated.
The current distributed indexing system consists of three separate modules: NodeController, FileSender, and NodeIndexer. The responsibility of each module is discussed later in this section.
The protocol we have designed for the distributed indexing is depicted in Figure 1.
Figure 1. Apocrita distributed indexing protocol.
IDLE QUERY: The IDLE QUERY is sent out from the initiating node to determine which other nodes may be able to help with the overall indexing process. There are no parameters sent with the command. The receiving node will respond with either a BUSY or IDLE command. If the IDLE command is received, the initiating node will add the responding node to a list of available distributed indexing helpers. In the case of a BUSY command being received, the responding node is ignored.
BUSY: Once a node received an IDL QUERY, it will determine whether it can be considered a candidate for distributed indexing.
This determination is based on the overall CPU usage of the node.
If the node is using most of its CPU for other processes, the node will respond to the IDLE QUERY with a BUSY command.
IDLE: As with the case of the BUSY response, the node receiving the IDLE QUERY will determine its eligibility for distributed indexing. To be considered a candidate for distributed indexing, the overall CPU usage must be at a minimum to all for dedicated indexing of the distributed documents. If this is the case, the node will respond with an IDLE command.
INCOMING FILE: Once the initiating node assembles a set of idle nodes to assist with the distributed indexing, it will divide the documents to be sent to the nodes. To do this, it sends an INCOMING FILE message, which contains the name of the file as well as the size in bytes. After the INCOMING FILE command has been sent, the initiating node will begin to stream the file to the other node. The initiating node will loop through the files that are to be sent to the other node; each file stream being preceded by the INCOMING FILE command with the appropriate parameters.
INDEX FILE: Once the indexing node has completed the indexing process of the set of files, it must send the resultant index back to the initiating node. The index is comprised of multiple files, which exist on the file system of the indexing node.
As with the INCOMING FILE command, the indexing node streams each index file after sending an INDEX FILE command.
The INDEX FILE command has two parameters: the first being the name of the index, and the second is the size of the file in bytes.
SEND COMPLETE: When sending the sets of files for both the index and the files to be indexed, the node must notify the corresponding node when the process is complete. Once the initiating node is finished sending the set of documents to be indexed, it will then send a SEND COMPLETE command indicating to the indexing node that there are no more files and the node can proceed with indexing the files. In the case of the initiating node sending the index files, the indexing node will complete the transfer with the SEND COMPLETE command indicating to the initiating node that there are no more index files to be sent and the initiating node can then assemble those index files into the main index.
The NodeController is responsible for setting up connections with nodes in the idle state to distribute the indexing process. Using JXTA [5], the node controller will obtain a set of nodes. This set of nodes is iterated and each one is sent the IDLE QUERY command. The nodes that respond with idle are then collected.
The set of idle nodes includes the node initiating the distributed indexing process, referred to as the local node. Once the collection of idle nodes is obtained, the node updates the set of controllers and evenly divides the set of documents that are to be indexed. For example, if there are 100 documents and 10 nodes (including the local node) then each node will have 10 documents to index. For each indexing node an instance of the FileSender object is created. The FileSender is aware of the set of documents that node is responsible for. Once a FileSender object has been created for each node, the NodeController waits for each FileSender to complete. When the FileSender objects have completed the NodeController will take the resultant indexes from 175 each node and pass them to an instance of the IndexCompiler, which maintains the index and the list of FileSenders. Once the IndexCompiler has completed it will return to the idle state and activate the directory scanner to monitor the locally owned set of documents for changes that may require reindexing.
The NodeIndexer is responsible for receiving documents sent to it by the initiating node and then indexing them using the Lucene engine [7]. Once the indexing is complete the resulting index is streamed back to the initiating node as well as compiled in the indexer nodes own local index. Before initiating the indexing process it must be sent an IDLE QUERY message. This is the first command that sets off the indexing process. The indexer node will determine whether it is considered idle based on the current CPU usage. As outlined in the protocol section if the node is not being used and has a low overall CPU usage percentage it will return IDLE to the IDLE QUERY command. If the indexer nodes CPU usage is above 50% for a specified amount of time it is then considered to be busy and will respond to the IDLE QUERY command with BUSY. If a node is determined busy it returns to its listening state waiting for another IDLE QUERY from another initiating node. If the node is determined to be idle it will enter the state where it will receive files from the initiating node that it is responsible for indexing. Once all of the files are received by the initiating node, indicated by a SEND COMPLETE message, it starts an instance of the Lucene indexing engine. The files are stored in a temporary directory separate from the nodes local documents that it is responsible for maintaining an index of. The Lucene index writer then indexes all of the transferred files. The index is stored on the drive within a temporary directory separate from the current index. After the indexing of the files completes the indexer node enters the state where the index files are sent back to the initiating node. The indexer node loops through all of the files created by Lucene"s IndexWriter and streams them to the initiating node. Once these files are sent back that index is then merged into the indexer nodes own full index of the existing files. It then enters the idle state where it will then listen for any other nodes that required distributing the indexing process.
The FileSender object is the initiating node equivalent of the indexer node. It initiates the communication between the initiating node and the node that will assist in the distributed indexing. The initiating node runs many instances of the FileSender node one for each other node it has determined to be idle. Upon instantiation of the FileSender it is passed the node that it is responsible for contacting and the set of files that must be sent.
The FileSender"s first job is to send the files that are to be indexed by the other idle node. The files are streamed one at a time to the other node. It sends each file using the INCOMING FILE command. With that command it sends the name of the file being sent and the size in bytes. Once all files have been sent the FileSender sends the SEND COMPLETE command. The FileSender creates an instance of Lucene"s IndexWriter and prepares to create the index in a temporary directory on the file system. The FileSender will begin to receive the files that are to be saved within the index. It receives an INDEX FILE command with the name of the files and the size in bytes. This file is then streamed into the temporary index directory on the FileSender node. After the transfer of the index files has been completed the FileSender notifies the instance of the index compiler that it is ready to combine the index. Each instance of the FileSender has its own unique section of temporary space to store the index that has been transferred back from the indexing node. When notifying the IndexCompiler it will also pass the location of the particular FileSenders directory location of that index.
Apocrita uses a peer-to-peer distribution model in order to distribute files. Files are distributed solely from a serving node to a client node without regard for the availability of file pieces from other clients in the network. This means that the file transfers will be fast and efficient and should not severely affect the usability of serving nodes from the point of view of a local user. The JXTA framework [5] is used in order to implement peer-to-peer functionality. This has been decided due to the extremely shorttimeline of the project which allows us to take advantage of over five years of testing and development and support from many large organizations employing JXTA in their own products. We are not concerned with any potential quality problems because JXTA is considered to be the most mature and stable peer-to-peer framework available.
Using JXTA terminology, there are three types of peers used in node classification.
Edge peers are typically low-bandwidth, non-dedicated nodes.
Due to these characteristics, edge peers are not used with Apocrita.
Relay peers are typically higher-bandwidth, dedicated nodes.
This is the classification of all nodes in the Apocrita network, and, as such, are the default classification used.
Rendezvous peers are used to coordinate message passing between nodes in the Apocrita network. This means that a minimum of one rendezvous peer per subnet is required.
The Apocrita server subsystem uses the JXTA Peer Discovery Protocol (PDP) in order to find participating peers within the network as shown in Figure 2.
Figure 2. Apocrita peer discovery process. 176 The PDP listens for peer advertisements from other nodes in the Apocrita swarm. If a peer advertisement is detected, the server will attempt to join the peer group and start actively contributing to the network. If no peers are found by the discovery service, the server will create a new peer group and start advertising this peer group. This new peer group will be periodically advertised on the network; any new peers joining the network will attach to this peer group. A distinct advantage of using the JXTA PDP is that Apocrita does not have to be sensitive to particular networking nuances such as Maximum Transmission Unit (MTU). In addition, Apocrita does not have to support one-to-many packet delivery methods such as multicast and instead can rely on JXTA for this support.
All nodes in the Apocrita swarm have a complete and up-to-date copy of the network index stored locally. This makes querying the index for search results trivial. Unlike the Gnutella protocol, a query does not have to propagate throughout the network. This also means that the time to return query results is very fast - much faster than protocols that rely on nodes in the network to pass the query throughout the network and then wait for results. This is demonstrated in Figure 3.
Figure 3. Apocrita query operation.
Each document in the swarm has a unique document identification number (ID). A node will query the index and a result will be returned with both the document ID number as well as a list of peers with a copy of the matched document ID. It is then the responsibility of the searching peer to contact the peers in the list to negotiate file transfer between the client and server.
Apocrita uses the Lucene framework [7], which is a project under development by the Apache Software Foundation. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. In the current implementation,
Apocrita is only capable of indexing plain text documents.
Apocrita uses the JXTA framework [5] as a peer-to-peer transport library between nodes. JXTA is used to pass both messages and files between nodes in the search network. By using JXTA,
Apocrita takes advantage of a reliable, and proven peer-to-peer transport mechanism. It uses the pipe facility in order to pass messages and files between nodes. The pipe facility provides many different types of pipe advertisements. This includes an unsecured unicast pipe, a secured unicast pipe, and a propagated unsecured pipe.
Message passing is used to pass status messages between nodes in order to aid in indexing, searching, and retrieval. For example, a node attempting to find an idle node to participate in indexing will query nodes via the message facility. Idle nodes will reply with a status message to indicate they are available to start indexing.
File passing is used within Apocrita for file transfer. After a file has been searched for and located within the peer group, a JXTA socket will be opened and file transfer will take place. A JXTA socket is similar to a standard Java socket, however a JXTA socket uses JXTA pipes in underlying network transport. File passing uses an unsecured unicast pipe in order to transfer data.
File passing is also used within Apocrita for index transfer. Index transfer works exactly like a file transfer. In fact, the index transfer actually passes the index as a file. However, there is one key difference between file transfer and index transfer. In the case of file transfer, a socket is created between only two nodes. In the case of index transfer, a socket must be created between all nodes in the network in order to pass the index, which allows for all nodes to have a full and complete index of the entire network. In order to facilitate this transfer efficiently, index transfer will use an unsecured propagated pipe to communicate with all nodes in the Apocrita network.
It is difficult to objectively benchmark the results obtained through Apocrita because there is no other system currently available with the same goals as Apocrita. We have, however, evaluated the performance of the critical sections of the system.
The critical sections were determined to be the processes that are the most time intensive. The evaluation was completed on standard lab computers on a 100Mb/s Ethernet LAN; the machines run Windows XP with a Pentium 4 CPU running at
The indexing time has been run against both: the Time Magazine collection [8], which contains 432 documents and 83 queries and their most relevant results, and the NPL collection [8] that has a total of 11,429 documents and 93 queries with expected results.
Each document ranges in size between 4KB and 8KB. As Figure 4 demonstrates, the number of nodes involved in the indexing process affects the time taken to complete the indexing processsometimes even drastically.
Figure 4. Node vs. index time.
The difference in going from one indexing node to two indexing nodes is the most drastic and equates to an indexing time 37% faster than a single indexing node. The different between two 177 indexing nodes and three indexing nodes is still significant and represents a 16% faster time than two indexing nodes. As the number of indexing nodes increases the results are less dramatic.
This can be attributed to the time overhead associated with having many nodes perform indexing. The time needed to communicate with a node is constant, so as the number of nodes increases, this constant becomes more prevalent. Also, the complexity of joining the indexing results is a complex operation and is complicated further as the number of indexing nodes increases.
Socket performance is also a very important part of Apocrita.
Benchmarks were performed using a 65MB file on a system with both the client and server running locally. This was done to isolate possible network issues. Although less drastic, similar results were shown when the client and server run on independent hardware. In order to mitigate possible unexpected errors, each test was run 10 times.
Figure 5. Java sockets vs. JXTA sockets.
As Figure 5 demonstrates, the performance of JXTA sockets is abysmal as compared to the performance of standard Java sockets.
The minimum transfer rate obtained using Java sockets is 81,945KB/s while the minimum transfer rater obtained using JXTA sockets is much lower at 3, 805KB/s. The maximum transfer rater obtain using Java sockets is 97,412KB/s while the maximum transfer rate obtained using JXTA sockets is 5,530KB/s. Finally, the average transfer rate using Java sockets is 87,540KB/s while the average transfer rate using JXTA sockets is 4,293KB/s.
The major problem found in these benchmarks is that the underlying network transport mechanism does not perform as quickly or efficiently as expected. In order to garner a performance increase, the JXTA framework needs to be substituted with a more traditional approach. The indexing time is also a bottleneck and will need to be improved for the overall quality of Apocrita to be improved.
Several decentralized P2P systems [1, 2, 3] exist today that Apocrita features some of their functionality. However, Apocrita also has unique novel searching and indexing features that make this system unique. For example, Majestic-12 [4] is a distributed search and indexing project designed for searching the Internet.
Each user would install a client, which is responsible for indexing a portion of the web. A central area for querying the index is available on the Majestic-12 web page. The index itself is not distributed, only the act of indexing is distributed. The distributed indexing aspect of this project most closely relates Apocrita goals.
YaCy [6] is a peer-to-peer web search application. YaCy consists of a web crawler, an indexer, a built-in database engine, and a p2p index exchange protocol. YaCy is designed to maintain a distributed index of the Internet. It used a distributed hash table (DHT) to maintain the index. The local node is used to query but all results that are returned are accessible on the Internet. YaCy used many peers and DHT to maintain a distributed index.
Apocrita will also use a distributed index in future implementations and may benefit from using an implementation of a DHT. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita.
We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet. It can help organizations with no network file server or necessary network infrastructure to share documents. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed. A proof of concept prototype has been constructed, but the results from measuring the network transport mechanism and the indexing time were not as impressive as initially envisioned. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems.
For future work, Apocrita will have a smart content distribution model in which a single instance of a file can intelligently and transparently replicate throughout the network to ensure a copy of every important file will always be available regardless of the availability of specific nodes in the network. In addition, we plan to integrate a revision control system into the content distribution portion of Apocrita so that users could have the ability to update an existing file that they found and have the old revision maintained and the new revision propagated. Finally, the current implementation has some overhead and redundancy due to the fact that the entire index is maintained on each individual node, we plan to design a distributed index.

Improvements in network connectivity erode the distinction between local and wide-area computing and, increasingly, users expect their work environment to follow them wherever they go. Nevertheless, distributed applications may perform poorly in wide-area network environments.
Network bandwidth problems will improve in the foreseeable future, but improvement in network latency is fundamentally limited. BuddyCache is a new object caching technique that addresses the network latency problem for collaborative applications in wide-area network environment.
Collaborative applications provide a shared work environment for groups of networked users collaborating on a common task, for example a team of engineers jointly overseeing a construction project. Strong-consistency collaborative applications, for example CAD systems, use client/server transactional object storage systems to ensure consistent access to shared persistent data. Up to now however, users have rarely considered running consistent network storage systems over wide-area networks as performance would be unacceptable [24]. For transactional storage systems, the high cost of wide-area network interactions to maintain data consistency is the main cost limiting the performance and therefore, in wide-area network environments, collaborative applications have been adapted to use weaker consistency storage systems [22]. Adapting an application to use weak consistency storage system requires significant effort since the application needs to be rewritten to deal with a different storage system semantics. If shared persistent objects could be accessed with low-latency, a new field of distributed strong-consistency applications could be opened.
Cooperative web caching [10, 11, 15] is a well-known approach to reducing client interaction with a server by allowing one client to obtain missing objects from a another client instead of the server. Collaborative applications seem a particularly good match to benefit from this approach since one of the hard problems, namely determining what objects are cached where, becomes easy in small groups typical of collaborative settings. However, cooperative web caching techniques do not provide two important properties needed by collaborative applications, strong consistency and efficient 26 access to fine-grained objects. Cooperative object caching systems [2] provide these properties. However, they rely on interaction with the server to provide fine-grain cache coherence that avoids the problem of false sharing when accesses to unrelated objects appear to conflict because they occur on the same physical page. Interaction with the server increases latency. The contribution of this work is extending cooperative caching techniques to provide strong consistency and efficient access to fine-grain objects in wide-area environments.
Consider a team of engineers employed by a construction company overseeing a remote project and working in a shed at the construction site. The engineers use a collaborative CAD application to revise and update complex project design documents. The shared documents are stored in transactional repository servers at the company home site. The engineers use workstations running repository clients. The workstations are interconnected by a fast local Ethernet but the network connection to the home repository servers is slow. To improve access latency, clients fetch objects from repository servers and cache and access them locally. A coherence protocol ensures that client caches remain consistent when objects are modified. The performance problem facing the collaborative application is coordinating with the servers consistent access to shared objects.
With BuddyCache, a group of close-by collaborating clients, connected to storage repository via a high-latency link, can avoid interactions with the server if needed objects, updates or coherency information are available in some client in the group.
BuddyCache presents two main technical challenges. One challenge is how to provide efficient access to shared finegrained objects in the collaborative group without imposing performance overhead on the entire caching system. The other challenge is to support fine-grain cache coherence in the presence of slow and failed nodes.
BuddyCache uses a redirection approach similar to one used in cooperative web caching systems [11]. A redirector server, interposed between the clients and the remote servers, runs on the same network as the collaborating group and, when possible, replaces the function of the remote servers. If the client request can not be served locally, the redirector forwards it to a remote server. When one of the clients in the group fetches a shared object from the repository, the object is likely to be needed by other clients.
BuddyCache redirects subsequent requests for this object to the caching client. Similarly, when a client creates or modifies a shared object, the new data is likely to be of potential interest to all group members. BuddyCache uses redirection to support peer update, a lightweight application-level multicast technique that provides group members with consistent access to the new data committed within the collaborating group without imposing extra overhead outside the group.
Nevertheless, in a transactional system, redirection interferes with shared object availability. Solo commit, is a validation technique used by BuddyCache to avoid the undesirable client dependencies that reduce object availability when some client nodes in the group are slow, or clients fail independently. A salient feature of solo commit is supporting fine-grained validation using inexpensive coarse-grained coherence information.
Since redirection supports the performance benefits of reducing interaction with the server but introduces extra processing cost due to availability mechanisms and request forwarding, this raises the question is the cure worse than the disease? We designed and implemented a BuddyCache prototype and studied its performance benefits and costs using analytical modeling and system measurements. We compared the storage system performance with and without BuddyCache and considered how the cost-benefit balance is affected by network latency.
Analytical results, supported by measurements based on the multi-user 007 benchmark, indicate that for typical Internet latencies BuddyCache provides significant performance benefits, e.g. for latencies ranging from 40 to 80 milliseconds round trip time, clients using the BuddyCache can reduce by up to 50% the latency of access to shared objects compared to the clients accessing the repository directly. These strong performance gains could make transactional object storage systems more attractive for collaborative applications in wide-area environments.
Cooperative caching techniques [20, 16, 13, 2, 28] provide access to client caches to avoid high disk access latency in an environment where servers and clients run on a fast local area network. These techniques use the server to provide redirection and do not consider issues of high network latency.
Multiprocessor systems and distributed shared memory systems [14, 4, 17, 18, 5] use fine-grain coherence techniques to avoid the performance penalty of false sharing but do not address issues of availability when nodes fail.
Cooperative Web caching techniques, (e.g. [11, 15]) investigate issues of maintaining a directory of objects cached in nearby proxy caches in wide-area environment, using distributed directory protocols for tracking cache changes. This work does not consider issues of consistent concurrent updates to shared fine-grained objects.
Cheriton and Li propose MMO [12] a hybrid web coherence protocol that combines invalidations with updates using multicast delivery channels and receiver-reliable protocol, exploiting locality in a way similar to BuddyCache.
This multicast transport level solution is geared to the single writer semantics of web objects. In contrast, BuddyCache uses application level multicast and a sender-reliable coherence protocol to provide similar access latency improvements for transactional objects. Application level multicast solution in a middle-ware system was described by Pendarakis, Shi and Verma in [27]. The schema supports small multi-sender groups appropriate for collaborative applications and considers coherence issues in the presence of failures but does not support strong consistency or fine-grained sharing.
Yin, Alvisi, Dahlin and Lin [32, 31] present a hierarchical WAN cache coherence scheme. The protocol uses leases to provide fault-tolerant call-backs and takes advantage of nearby caches to reduce the cost of lease extensions.
The study uses simulation to investigate latency and fault tolerance issues in hierarchical avoidance-based coherence scheme. In contrast, our work uses implementation and analysis to evaluate the costs and benefits of redirection and fine grained updates in an optimistic system.
Anderson, Eastham and Vahdat in WebFS [29] present a global file system coherence protocol that allows clients to choose 27 on per file basis between receiving updates or invalidations.
Updates and invalidations are multicast on separate channels and clients subscribe to one of the channels. The protocol exploits application specific methods e.g. last-writer-wins policy for broadcast applications, to deal with concurrent updates but is limited to file systems.
Mazieres studies a bandwidth saving technique [24] to detect and avoid repeated file fragment transfers across a WAN when fragments are available in a local cache. BuddyCache provides similar bandwidth improvements when objects are available in the group cache.
High network latency imposes performance penalty for transactional applications accessing shared persistent objects in wide-area network environment. This section describes the BuddyCache approach for reducing the network latency penalty in collaborative applications and explains the main design decisions.
We consider a system in which a distributed transactional object repository stores objects in highly reliable servers, perhaps outsourced in data-centers connected via high-bandwidth reliable networks. Collaborating clients interconnected via a fast local network, connect via high-latency, possibly satellite, links to the servers at the data-centers to access shared persistent objects. The servers provide disk storage for the persistent objects. A persistent object is owned by a single server. Objects may be small (order of 100 bytes for programming language objects [23]). To amortize the cost of disk and network transfer objects are grouped into physical pages.
To improve object access latency, clients fetch the objects from the servers and cache and access them locally. A transactional cache coherence protocol runs at clients and servers to ensure that client caches remain consistent when objects are modified. The performance problem facing the collaborating client group is the high latency of coordinating consistent access to the shared objects.
BuddyCache architecture is based on a request redirection server, interposed between the clients and the remote servers. The interposed server (the redirector) runs on the same network as the collaborative group and, when possible, replaces the function of the remote servers. If the client request can be served locally, the interaction with the server is avoided. If the client request can not be served locally, redirector forwards it to a remote server. Redirection approach has been used to improve the performance of web caching protocols. BuddyCache redirector supports the correctness, availability and fault-tolerance properties of transactional caching protocol [19]. The correctness property ensures onecopy serializability of the objects committed by the client transactions. The availability and fault-tolerance properties ensure that a crashed or slow client does not disrupt any other client"s access to persistent objects.
The three types of client server interactions in a transactional caching protocol are the commit of a transaction, the fetch of an object missing in a client cache, and the exchange of cache coherence information. BuddyCache avoids interactions with the server when a missing object, or cache coherence information needed by a client is available within the collaborating group. The redirector always interacts with the servers at commit time because only storage servers provide transaction durability in a way that ensures committed Client Redirector Client Client Buddy Group Client Redirector Client Client Buddy Group Servers Figure 1: BuddyCache. data remains available in the presence of client or redirector failures. Figure 1 shows the overall BuddyCache architecture.
The redirector maintains a directory of pages cached at each client to provide cooperative caching [20, 16, 13, 2, 28], redirecting a client fetch request to another client that caches the requested object. In addition, redirector manages cache coherence.
Several efficient transactional cache coherence protocols [19] exist for persistent object storage systems. Protocols make different choices in granularity of data transfers and granularity of cache consistency. The current best-performing protocols use page granularity transfers when clients fetch missing objects from a server and object granularity coherence to avoid false (page-level) conflicts. The transactional caching taxonomy [19] proposed by Carey, Franklin and Livny classifies the coherence protocols into two main categories according to whether a protocol avoids or detects access to stale objects in the client cache. The BuddyCache approach could be applied to both categories with different performance costs and benefits in each category.
We chose to investigate BuddyCache in the context of OCC [3], the current best performing detection-based protocol. We chose OCC because it is simple, performs well in high-latency networks, has been implemented and we had access to the implementation. We are investigating BuddyCache with PSAA [33], the best performing avoidancebased protocol. Below we outline the OCC protocol [3]. The OCC protocol uses object-level coherence. When a client requests a missing object, the server transfers the containing page. Transaction can read and update locally cached objects without server intervention. However, before a transaction commits it must be validated; the server must make sure the validating transaction has not read a stale version of some object that was updated by a successfully committed or validated transaction. If validation fails, the transaction is aborted. To reduce the number and cost of aborts, 28 Helper Requester A:p Fetch pPeer fetch p Page p Redirector Figure 2: Peer fetch a server sends background object invalidation messages to clients caching the containing pages. When clients receive invalidations they remove stale objects from the cache and send background acknowledgments to let server know about this.
Since invalidations remove stale objects from the client cache, invalidation acknowledgment indicates to the server that a client with no outstanding invalidations has read upto-date objects. An unacknowledged invalidation indicates a stale object may have been accessed in the client cache.
The validation procedure at the server aborts a client transaction if a client reads an object while an invalidation is outstanding.
The acknowledged invalidation mechanism supports object-level cache coherence without object-based directories or per-object version numbers. Avoiding per-object overheads is very important to reduce performance penalties [3] of managing many small objects, since typical objects are small. An important BuddyCache design goal is to maintain this benefit.
Since in BuddyCache a page can be fetched into a client cache without server intervention (as illustrated in figure 2), cache directories at the servers keep track of pages cached in each collaborating group rather than each client. Redirector keeps track of pages cached in each client in a group. Servers send to the redirector invalidations for pages cached in the entire group. The redirector propagates invalidations from servers to affected clients. When all affected clients acknowledge invalidations, redirector can propagate the group acknowledgment to the server.
When one of the clients in the collaborative group creates or modifies shared objects, the copies cached by any other client become stale but the new data is likely to be of potential interest to the group members. The goal in BuddyCache is to provide group members with efficient and consistent access to updates committed within the group without imposing extra overhead on other parts of the storage system.
The two possible approaches to deal with stale data are cache invalidations and cache updates. Cache coherence studies in web systems (e.g. [7]) DSM systems (e.g. [5]), and transactional object systems (e.g. [19]) compare the benefits of update and invalidation. The studies show the Committing Client Server Redirector x2. Store x
Figure 3: Peer update. benefits are strongly workload-dependent. In general, invalidation-based coherence protocols are efficient since invalidations are small, batched and piggybacked on other messages. Moreover, invalidation protocols match the current hardware trend for increasing client cache sizes. Larger caches are likely to contain much more data than is actively used. Update-based protocols that propagate updates to low-interest objects in a wide-area network would be wasteful. Nevertheless, invalidation-based coherence protocols can perform poorly in high-latency networks [12] if the object"s new value is likely to be of interest to another group member. With an invalidation-based protocol, one member"s update will invalidate another member"s cached copy, causing the latter to perform a high-latency fetch of the new value from the server.
BuddyCache circumvents this well-known bandwidth vs. latency trade-off imposed by update and invalidation protocols in wide-area network environments. It avoids the latency penalty of invalidations by using the redirector to retain and propagate updates committed by one client to other clients within the group. This avoids the bandwidth penalty of updates because servers propagate invalidations to the redirectors. As far as we know, this use of localized multicast in BuddyCache redirector is new and has not been used in earlier caching systems.
The peer update works as follows. An update commit request from a client arriving at the redirector contains the object updates. Redirector retains the updates and propagates the request to the coordinating server. After the transaction commits, the coordinator server sends a commit reply to the redirector of the committing client group. The redirector forwards the reply to the committing client, and also propagates the retained committed updates to the clients caching the modified pages (see figure 3). Since the groups outside the BuddyCache propagate invalidations, there is no extra overhead outside the committing group.
In the OCC protocol, clients acknowledge server invalidations (or updates) to indicate removal of stale data. The straightforward group acknowledgement protocol where redirector collects and propagates a collective acknowledge29 Redirector commit ok ABORT Client 1 Client 2 Server commit (P(x)) commit (P(x)) ok + inv(P(x)) inv(P(x)) commit(P(x)) commit(P(x)) ack(P(x)) ack(P(x)) Figure 4: Validation with Slow Peers ment to the server, interferes with the availability property of the transactional caching protocol [19] since a client that is slow to acknowledge an invalidation or has failed can delay a group acknowledgement and prevent another client in the group from committing a transaction. E.g. an engineer that commits a repeated revision to the same shared design object (and therefore holds the latest version of the object) may need to abort if the group acknowledgement has not propagated to the server.
Consider a situation depicted in figure 4 where Client1 commits a transaction T that reads the latest version of an object x on page P recently modified by Client1. If the commit request for T reaches the server before the collective acknowledgement from Client2 for the last modification of x arrives at the server, the OCC validation procedure considers x to be stale and aborts T (because, as explained above, an invalidation unacknowledged by a client, acts as indication to the server that the cached object value is stale at the client).
Note that while invalidations are not required for the correctness of the OCC protocol, they are very important for the performance since they reduce the performance penalties of aborts and false sharing. The asynchronous invalidations are an important part of the reason OCC has competitive performance with PSAA [33], the best performing avoidance-based protocol [3].
Nevertheless, since invalidations are sent and processed asynchronously, invalidation processing may be arbitrarily delayed at a client. Lease-based schemes (time-out based) have been proposed to improve the availability of hierarchical callback-based coherence protocols [32] but the asynchronous nature of invalidations makes the lease-based approaches inappropriate for asynchronous invalidations.
The Solo commit validation protocol allows a client with up-to-date objects to commit a transaction even if the group acknowledgement is delayed due to slow or crashed peers.
The protocol requires clients to include extra information with the transaction read sets in the commit message, to indicate to the server the objects read by the transaction are up-to-date.
Object version numbers could provide a simple way to track up-to-date objects but, as mentioned above, maintaining per object version numbers imposes unacceptably high overheads (in disk storage, I/O costs and directory size) on the entire object system when objects are small [23].
Instead, solo commit uses coarse-grain page version numbers to identify fine-grain object versions. A page version number is incremented at a server when at transaction that modifies objects on the page commits. Updates committed by a single transaction and corresponding invalidations are therefore uniquely identified by the modified page version number.
Page version numbers are propagated to clients in fetch replies, commit replies and with invalidations, and clients include page version numbers in commit requests sent to the servers. If a transaction fails validation due to missing group acknowledgement, the server checks page version numbers of the objects in the transaction read set and allows the transaction to commit if the client has read from the latest page version.
The page version numbers enable independent commits but page version checks only detect page-level conflicts. To detect object-level conflicts and avoid the problem of false sharing we need the acknowledged invalidations. Section 4 describes the details of the implementation of solo commit support for fine-grain sharing.
The BuddyCache architecture supports multiple concurrent peer groups. Potentially, it may be faster to access data cached in another peer group than to access a remote server. In such case extending BuddyCache protocols to support multi-level peer caching could be worthwhile. We have not pursued this possibility for several reasons.
In web caching workloads, simply increasing the population of clients in a proxy cache often increases the overall cache hit rate [30]. In BuddyCache applications, however, we expect sharing to result mainly from explicit client interaction and collaboration, suggesting that inter-group fetching is unlikely to occur. Moreover, measurements from multi-level web caching systems [9] indicate that a multilevel system may not be advantageous unless the network connection between the peer groups is very fast. We are primarily interested in environments where closely collaborating peers have fast close-range connectivity, but the connection between peer groups may be slow. As a result, we decided that support for inter-group fetching in BuddyCache is not a high priority right now.
To support heterogenous resource-rich and resource-poor peers, the BuddyCache redirector can be configured to run either in one of the peer nodes or, when available, in a separate node within the site infrastructure. Moreover, in a resource-rich infrastructure node, the redirector can be configured as a stand-by peer cache to receive pages fetched by other peers, emulating a central cache somewhat similar to a regional web proxy cache. From the BuddyCache cache coherence protocol point of view, however, such a stand-by peer cache is equivalent to a regular peer cache and therefore we do not consider this case separately in the discussion in this paper.
In this section we provide the details of the BuddyCache implementation. We have implemented BuddyCache in the Thor client/server object-oriented database [23]. Thor supports high performance access to distributed objects and therefore provides a good test platform to investigate BuddyCache performance. 30
Thor servers provide persistent storage for objects and clients cache copies of these objects. Applications run at the clients and interact with the system by making calls on methods of cached objects. All method calls occur within atomic transactions. Clients communicate with servers to fetch pages or to commit a transaction.
The servers have a disk for storing persistent objects, a stable transaction log, and volatile memory. The disk is organized as a collection of pages which are the units of disk access. The stable log holds commit information and object modifications for committed transactions. The server memory contains cache directory and a recoverable modified object cache called the MOB. The directory keeps track of which pages are cached by which clients. The MOB holds recently modified objects that have not yet been written back to their pages on disk. As MOB fills up, a background process propagates modified objects to the disk [21, 26].
Transactions are serialized using optimistic concurrency control OCC [3] described in Section 3.1. We provide some of the relevant OCC protocol implementation details. The client keeps track of objects that are read and modified by its transaction; it sends this information, along with new copies of modified objects, to the servers when it tries to commit the transaction. The servers determine whether the commit is possible, using a two-phase commit protocol if the transaction used objects at multiple servers. If the transaction commits, the new copies of modified objects are appended to the log and also inserted in the MOB. The MOB is recoverable, i.e. if the server crashes, the MOB is reconstructed at recovery by scanning the log.
Since objects are not locked before being used, a transaction commit can cause caches to contain obsolete objects.
Servers will abort a transaction that used obsolete objects.
However, to reduce the probability of aborts, servers notify clients when their objects become obsolete by sending them invalidation messages; a server uses its directory and the information about the committing transaction to determine what invalidation messages to send. Invalidation messages are small because they simply identify obsolete objects.
Furthermore, they are sent in the background, batched and piggybacked on other messages.
When a client receives an invalidation message, it removes obsolete objects from its cache and aborts the current transaction if it used them. The client continues to retain pages containing invalidated objects; these pages are now incomplete with holes in place of the invalidated objects.
Performing invalidation on an object basis means that false sharing does not cause unnecessary aborts; keeping incomplete pages in the client cache means that false sharing does not lead to unnecessary cache misses. Clients acknowledge invalidations to indicate removal of stale data as explained in Section 3.1. Invalidation messages prevent some aborts, and accelerate those that must happen - thus wasting less work and oﬄoading detection of aborts from servers to clients.
When a transaction aborts, its client restores the cached copies of modified objects to the state they had before the transaction started; this is possible because a client makes a copy of an object the first time it is modified by a transaction.
The redirector runs on the same local network as the peer group, in one of the peer nodes, or in a special node within the infrastructure. It maintains a directory of pages available in the peer group and provides fast centralized fetch redirection (see figure 2) between the peer caches. To improve performance, clients inform the redirector when they evict pages or objects by piggybacking that information on messages sent to the redirector.
To ensure up-to-date objects are fetched from the group cache the redirector tracks the status of the pages. A cached page is either complete in which case it contains consistent values for all the objects, or incomplete, in which case some of the objects on a page are marked invalid. Only complete pages are used by the peer fetch. The protocol for maintaining page status when pages are updated and invalidated is described in Section 4.4.
When a client request has to be processed at the servers, e.g., a complete requested page is unavailable in the peer group or a peer needs to commit a transaction, the redirector acts as a server proxy: it forwards the request to the server, and then forwards the reply back to the client. In addition, in response to invalidations sent by a server, the redirector distributes the update or invalidation information to clients caching the modified page and, after all clients acknowledge, propagates the group acknowledgment back to the server (see figure 3). The redirector-server protocol is, in effect, the client-server protocol used in the base Thor storage system, where the combined peer group cache is playing the role of a single client cache in the base system.
The peer update is implemented as follows. An update commit request from a client arriving at the redirector contains the object updates. Redirector retains the updates and propagates the request to the coordinator server. After a transaction commits, using a two phase commit if needed, the coordinator server sends a commit reply to the redirector of the committing client group. The redirector forwards the reply to the committing client. It waits for the invalidations to arrive to propagate corresponding retained (committed) updates to the clients caching the modified pages (see figure 3.) Participating servers that are home to objects modified by the transaction generate object invalidations for each cache group that caches pages containing the modified objects (including the committing group). The invalidations are sent lazily to the redirectors to ensure that all the clients in the groups caching the modified objects get rid of the stale data.
In cache groups other than the committing group, redirectors propagates the invalidations to all the clients caching the modified pages, collect the client acknowledgments and after completing the collection, propagate collective acknowledgments back to the server.
Within the committing client group, the arriving invalidations are not propagated. Instead, updates are sent to clients caching those objects" pages, the updates are acknowledged by the client, and the collective acknowledgment is propagated to the server.
An invalidation renders a cached page unavailable for peer fetch changing the status of a complete page p into an incomplete. In contrast, an update of a complete page preserves the complete page status. As shown by studies of the 31 fragment reconstruction [2], such update propagation allows to avoid the performance penalties of false sharing. That is, when clients within a group modify different objects on the same page, the page retains its complete status and remains available for peer fetch. Therefore, the effect of peer update is similar to eager fragment reconstruction [2].
We have also considered the possibility of allowing a peer to fetch an incomplete page (with invalid objects marked accordingly) but decided against this possibility because of the extra complexity involved in tracking invalid objects.
The solo commit validation protocol allows clients with up-to-date objects to commit independently of slower (or failed) group members. As explained in Section 3.3, the solo commit protocol allows a transaction T to pass validation if extra coherence information supplied by the client indicates that transaction T has read up-to-date objects. Clients use page version numbers to provide this extra coherence information. That is, a client includes the page version number corresponding to each object in the read object set sent in the commit request to the server. Since a unique page version number corresponds to each committed object update, the page version number associated with an object allows the validation procedure at the server to check if the client transaction has read up-to-date objects.
The use of coarse-grain page versions to identify object versions avoids the high penalty of maintaining persistent object versions for small objects, but requires an extra protocol at the client to maintain the mapping from a cached object to the identifying page version (ObjectToVersion). The main implementation issue is concerned with maintaining this mapping efficiently.
At the server side, when modifications commit, servers associate page version numbers with the invalidations. At validation time, if an unacknowledged invalidation is pending for an object x read by a transaction T, the validation procedure checks if the version number for x in T"s read set matches the version number for highest pending invalidation for x, in which case the object value is current, otherwise T fails validation.
We note again that the page version number-based checks, and the invalidation acknowledgment-based checks are complimentary in the solo commit validation and both are needed.
The page version number check allows the validation to proceed before invalidation acknowledgments arrive but by itself a page version number check detects page-level conflicts and is not sufficient to support fine-grain coherence without the object-level invalidations.
We now describe how the client manages the mapping ObjectToVersion. The client maintains a page version number for each cached page. The version number satisfies the following invariant V P about the state of objects on a page: if a cached page P has a version number v, then the value of an object o on a cached page P is either invalid or it reflects at least the modifications committed by transactions preceding the transaction that set P"s version number to v.
New object values and new page version numbers arrive when a client fetches a page or when a commit reply or invalidations arrive for this page. The new object values modify the page and, therefore, the page version number needs to be updated to maintain the invariant V P. A page version number that arrives when a client fetches a page, replaces Object Version x 8 Redirector Server 1Client 1 com(P(x,6),Q(y,9)) com(P(x,6),Q(y,9)) ok(P(x,8),Q(y,10)) ok(P(x,8),Q(y,10)) inv(Q(s,11)) inv(Q(s,11)) inv(P(r,7) inv(P(r,7) Server 2 Figure 5: Reordered Invalidations the page version number for this page. Such an update preserves the invariant V P. Similarly, an in-sequence page version number arriving at the client in a commit or invalidation message advances the version number for the entire cached page, without violating V P. However, invalidations or updates and their corresponding page version numbers can also arrive at the client out of sequence, in which case updating the page version number could violate V P. For example, a commit reply for a transaction that updates object x on page P in server S1, and object y on page Q in server S2, may deliver a new version number for P from the transaction coordinator S1 before an invalidation generated for an earlier transaction that has modified object r on page P arrives from S1 (as shown in figure 5).
The cache update protocol ensures that the value of any object o in a cached page P reflects the update or invalidation with the highest observed version number. That is, obsolete updates or invalidations received out of sequence do not affect the value of an object.
To maintain the ObjectToVersion mapping and the invariant V P in the presence of out-of-sequence arrival of page version numbers, the client manages a small version number cache vcache that maintains the mapping from an object into its corresponding page version number for all reordered version number updates until a complete page version number sequence is assembled. When the missing version numbers for the page arrive and complete a sequence, the version number for the entire page is advanced.
The ObjectToVersion mapping, including the vcache and page version numbers, is used at transaction commit time to provide version numbers for the read object set as follows.
If the read object has an entry in the vcache, its version number is equal to the highest version number in the vcache for this object. If the object is not present in the vcache, its version number is equal the version number of its containing cached page. Figure 6 shows the ObjectToVersion mapping in the client cache, including the page version numbers for pages and the vcache.
Client can limit vcache size as needed since re-fetching a page removes all reordered page version numbers from the vcache. However, we expect version number reordering to be uncommon and therefore expect the vcache to be very small.
A client group contains multiple client nodes and a redi32 VersionPageObject Version VCache Client Cache Client Page Cache Figure 6: ObjectToVersion map with vcache rector that can fail independently. The goal of the failover protocol is to reconfigure the BuddyCache in the case of a node failure, so that the failure of one node does not disrupt other clients from accessing shared objects. Moreover, the failure of the redirector should allow unaffected clients to keep their caches intact.
We have designed a failover protocols for BuddyCache but have not implemented it yet. The appendix outlines the protocol.
BuddyCache redirection supports the performance benefits of avoiding communication with the servers but introduces extra processing cost due to availability mechanisms and request forwarding. Is the cure worse then the disease? To answer the question, we have implemented a BuddyCache prototype for the OCC protocol and conducted experiments to analyze the performance benefits and costs over a range of network latencies.
The performance benefits of peer fetch and peer update are due to avoided server interactions. This section presents a simple analytical performance model for this benefit. The avoided server interactions correspond to different types of client cache misses. These can be cold misses, invalidation misses and capacity misses. Our analysis focuses on cold misses and invalidation misses, since the benefit of avoiding capacity misses can be derived from the cold misses.
Moreover, technology trends indicate that memory and storage capacity will continue to grow and therefore a typical BuddyCache configuration is likely not to be cache limited.
The client cache misses are determined by several variables, including the workload and the cache configuration.
Our analysis tries, as much as possible, to separate these variables so they can be controlled in the validation experiments.
To study the benefit of avoiding cold misses, we consider cold cache performance in a read-only workload (no invalidation misses). We expect peer fetch to improve the latency cost for client cold cache misses by fetching objects from nearby cache. We evaluate how the redirection cost affects this benefit by comparing and analyzing the performance of an application running in a storage system with BuddyCache and without (called Base).
To study the benefit of avoiding invalidation misses, we consider hot cache performance in a workload with modifications (with no cold misses). In hot caches we expect BuddyCache to provide two complementary benefits, both of which reduce the latency of access to shared modified objects. Peer update lets a client access an object modified by a nearby collaborating peer without the delay imposed by invalidation-only protocols. In groups where peers share a read-only interest in the modified objects, peer fetch allows a client to access a modified object as soon as a collaborating peer has it, which avoids the delay of server fetch without the high cost imposed by the update-only protocols.
Technology trends indicate that both benefits will remain important in the foreseeable future. The trend toward increase in available network bandwidth decreases the cost of the update-only protocols. However, the trend toward increasingly large caches, that are updated when cached objects are modified, makes invalidation-base protocols more attractive.
To evaluate these two benefits we consider the performance of an application running without BuddyCache with an application running BuddyCache in two configurations.
One, where a peer in the group modifies the objects, and another where the objects are modified by a peer outside the group.
Peer update can also avoid invalidation misses due to false-sharing, introduced when multiple peers update different objects on the same page concurrently. We do not analyze this benefit (demonstrated by earlier work [2]) because our benchmarks do not allow us to control object layout, and also because this benefit can be derived given the cache hit rate and workload contention.
The model considers how the time to complete an execution with and without BuddyCache is affected by invalidation misses and cold misses.
Consider k clients running concurrently accessing uniformly a shared set of N pages in BuddyCache (BC) and Base. Let tfetch(S), tredirect(S), tcommit(S), and tcompute(S) be the time it takes a client to, respectively, fetch from server, peer fetch, commit a transaction and compute in a transaction, in a system S, where S is either a system with BuddyCache (BC) or without (Base). For simplicity, our model assumes the fetch and commit times are constant. In general they may vary with the server load, e.g. they depend on the total number of clients in the system.
The number of misses avoided by peer fetch depends on k, the number of clients in the BuddyCache, and on the client co-interest in the shared data. In a specific BuddyCache execution it is modeled by the variable r, defined as a number of fetches arriving at the redirector for a given version of page P (i.e. until an object on the page is invalidated).
Consider an execution with cold misses. A client starts with a cold cache and runs read-only workload until it accesses all N pages while committing l transactions. We assume there are no capacity misses, i.e. the client cache is large enough to hold N pages. In BC, r cold misses for page P reach the redirector. The first of the misses fetches P from the server, and the subsequent r − 1 misses are redirected. Since each client accesses the entire shared set r = k.
Let Tcold(Base) and Tcold(BC) be the time it takes to complete the l transactions in Base and BC. 33 Tcold(Base) = N ∗ tfetch(Base) +(tcompute + tcommit(Base)) ∗ l (1) Tcold(BC) = N ∗ 1 k ∗ tfetch(BC) + (1 − 1 k ) ∗ tredirect +(tcompute + tcommit(BC)) ∗ l (2) Consider next an execution with invalidation misses. A client starts with a hot cache containing the working set of N pages. We focus on a simple case where one client (writer) runs a workload with modifications, and the other clients (readers) run a read-only workload.
In a group containing the writer (BCW ), peer update eliminates all invalidation misses. In a group containing only readers (BCR), during a steady state execution with uniform updates, a client transaction has missinv invalidation misses. Consider the sequence of r client misses on page P that arrive at the redirector in BCR between two consequent invalidations of page P. The first miss goes to the server, and the r − 1 subsequent misses are redirected.
Unlike with cold misses, r ≤ k because the second invalidation disables redirection for P until the next miss on P causes a server fetch.
Assuming uniform access, a client invalidation miss has a chance of 1/r to be the first miss (resulting in server fetch), and a chance of (1 − 1/r) to be redirected.
Let Tinval(Base), Tinval(BCR) and Tinval(BCW ) be the time it takes to complete a single transaction in the Base,
BCR and BCW systems.
Tinval(Base) = missinv ∗ tfetch(Base) +tcompute + tcommit(Base) (3) Tinval(BCR) = missinv ∗ ( 1 r ∗ tfetch(BCR) +(1 − 1 r ) ∗ tredirect(BCR)) +tcompute + tcommit(BCR) (4) Tinval(BCW ) = tcompute + tcommit(BCW ) (5) In the experiments described below, we measure the parameters N, r, missinv, tfetch(S), tredirect(S), tcommit(S), and tcompute(S). We compute the completion times derived using the above model and derive the benefits. We then validate the model by comparing the derived values to the completion times and benefits measured directly in the experiments.
Before presenting our results we describe our experimental setup. We use two systems in our experiments. The Base system runs Thor distributed object storage system [23] with clients connecting directly to the servers. The Buddy system runs our implementation of BuddyCache prototype in Thor, supporting peer fetch, peer update, and solo commit, but not the failover.
Our workloads are based on the multi-user OO7 benchmark [8]; this benchmark is intended to capture the characteristics of many different multi-user CAD/CAM/CASE applications, but does not model any specific application. We use OO7 because it is a standard benchmark for measuring object storage system performance. The OO7 database contains a tree of assembly objects with leaves pointing to three composite parts chosen randomly from among 500 such objects. Each composite part contains a graph of atomic parts linked by connection objects; each atomic part has 3 outgoing connections. We use a medium database that has 200 atomic parts per composite part. The multi-user database allocates for each client a private module consisting of one tree of assembly objects, and adds an extra shared module that scales proportionally to the number of clients.
We expect a typical BuddyCache configuration not to be cache limited and therefore focus on workloads where the objects in the client working set fit in the cache. Since the goal of our study is to evaluate how effectively our techniques deal with access to shared objects, in our study we limit client access to shared data only. This allows us to study the effect our techniques have on cold cache and cache consistency misses and isolate as much as possible the effect of cache capacity misses.
To keep the length of our experiments reasonable, we use small caches. The OO7 benchmark generates database modules of predefined size. In our implementation of OO7, the private module size is about 38MB. To make sure that the entire working set fits into the cache we use a single private module and choose a cache size of 40MB for each client. The OO7 database is generated with modules for 3 clients, only one of which is used in our experiments as we explain above.
The objects in the database are clustered in 8K pages, which are also the unit of transfer in the fetch requests.
We consider two types of transaction workloads in our analysis, read-only and read-write. In OO7 benchmark, read-only transactions use the T1 traversal that performs a depth-first traversal of entire composite part graph. Write transactions use the T2b traversal that is identical to T1 except that it modifies all the atomic parts in a single composite. A single transaction includes one traversal and there is no sleep time between transactions. Both read-only and read-write transactions always work with data from the same module. Clients running read-write transactions don"t modify in every transaction, instead they have a 50% probability of running read-only transactions.
The database was stored by a server on a 40GB IBM 7200RPM hard drive, with a 8.5 average seek time and 40 MB/sec data transfer rates. In Base system clients connect directly to the database. In Buddy system clients connect to the redirector that connects to the database. We run the experiments with 1-10 clients in Base, and one or two 1-10 client groups in Buddy. The server, the clients and the redirectors ran on a 850MHz Intel Pentium III processor based PC, 512MB of memory, and Linux Red Hat
server was configured with a 50MB cache (of which 6MB were used for the modified object buffer), the client had a 40MB cache. The experiments ran in Utah experimental testbed emulab.net [1]. 34 Latency [ms] Base Buddy 3 group 5 group 3 group 5 group Fetch 1.3 1.4 2.4 2.6 Commit 2.5 5.5 2.4 5.7 Table 1: Commit and Server fetch Operation Latency [ms] PeerFetch 1.8 - 5.5 −AlertHelper 0.3 - 4.6 −CopyUnswizzle 0.24 −CrossRedirector 0.16 Table 2: Peer fetch
This section analyzes the basic cost of the requests in the Buddy system during the OO7 runs.
Fetch and commit requests in the BuddyCache cross the redirector, a cost not incurred in the Base system. For a request redirected to the server (server fetch) the extra cost of redirection includes a local request from the client to redirector on the way to and from the server. We evaluate this latency overhead indirectly by comparing the measured latency of the Buddy system server fetch or commit request with the measured latency of the corresponding request in the Base system.
Table 1 shows the latency for the commit and server fetch requests in the Base and Buddy system for 3 client and 5 client groups in a fast local area network. All the numbers were computed by averaging measured request latency over 1000 requests. The measurements show that the redirection cost of crossing the redirector in not very high even in a local area network. The commit cost increases with the number of clients since commits are processed sequentially.
The fetch cost does not increase as much because the server cache reduces this cost. In a large system with many groups, however, the server cache becomes less efficient.
To evaluate the overheads of the peer fetch, we measure the peer fetch latency (PeerFetch) at the requesting client and break down its component costs. In peer fetch, the cost of the redirection includes, in addition to the local network request cost, the CPU processing latency of crossing the redirector and crossing the helper, the latter including the time to process the help request and the time to copy, and unswizzle the requested page.
We directly measured the time to copy and unswizzle the requested page at the helper, (CopyUnswizzle), and timed the crossing times using a null crossing request. Table 2 summarizes the latencies that allows us to break down the peer fetch costs. CrossRedirector, includes the CPU latency of crossing the redirector plus a local network round-trip and is measured by timing a round-trip null request issued by a client to the redirector. AlertHelper, includes the time for the helper to notice the request plus a network roundtrip, and is measured by timing a round-trip null request issued from an auxiliary client to the helper client. The local network latency is fixed and less than 0.1 ms.
The AlertHelper latency which includes the elapsed time from the help request arrival until the start of help request processing is highly variable and therefore contributes to the high variability of the PeerFetch time. This is because the client in Buddy system is currently single threaded and therefore only starts processing a help request when blocked waiting for a fetch- or commit reply. This overhead is not inherent to the BuddyCache architecture and could be mitigated by a multi-threaded implementation in a system with pre-emptive scheduling.
The solo commit allows a fast client modifying an object to commit independently of a slow peer. The solo commit mechanism introduces extra processing at the server at transaction validation time, and extra processing at the client at transaction commit time and at update or invalidation processing time.
The server side overheads are minimal and consist of a page version number update at commit time, and a version number comparison at transaction validation time.
The version cache has an entry only when invalidations or updates arrive out of order. This may happen when a transaction accesses objects in multiple servers. Our experiments run in a single server system and therefore, the commit time overhead of version cache management at the client does not contribute in the results presented in the section below. To gauge these client side overheads in a multiple server system, we instrumented the version cache implementation to run with a workload trace that included reordered invalidations and timed the basic operations.
The extra client commit time processing includes a version cache lookup operation for each object read by the transaction at commit request preparation time, and a version cache insert operation for each object updated by a transaction at commit reply processing time, but only if the updated page is missing some earlier invalidations or updates.
It is important that the extra commit time costs are kept to a minimum since client is synchronously waiting for the commit completion. The measurements show that in the worst case, when a large number of invalidations arrive out of order, and about half of the objects modified by T2a (200 objects) reside on reordered pages, the cost of updating the version cache is 0.6 ms. The invalidation time cost are comparable, but since invalidations and updates are processed in the background this cost is less important for the overall performance. We are currently working on optimizing the version cache implementation to further reduce these costs.
This section examines the performance gains seen by an application running OO7 benchmark with a BuddyCache in a wide area network.
To evaluate the performance gains from avoiding cold misses we compare the cold cache performance of OO7 benchmark running read-only workload in the Buddy and Base systems. We derive the times by timing the execution of the systems in the local area network environment and substituting 40 ms and 80 ms delays for the requests crossing the redirector and the server to estimate the performance in the wide-area-network. Figures 7 and 8 show the overall time to complete 1000 cold cache transactions. The numbers were 35 0 5 0 100 150 200 250 Base Buddy Base Buddy Base Buddy 3 Clients 5 Clients 10 Clients [ms] CPU Commit Server Fetch Peer Fetch Figure 7: Breakdown for cold read-only 40ms RTT 0 5 0 100 150 200 250 300 350 400 Base Buddy Base Buddy Base Buddy 3 Clients 5 Clients 10 Clients [ms] CPU Commit Server Fetch Peer Fetch Figure 8: Breakdown for cold read-only 80ms RTT obtained by averaging the overall time of each client in the group.
The results show that in a 40 ms network Buddy system reduces significantly the overall time compared to the Base system, providing a 39% improvement in a three client group, 46% improvement in the five client group and 56% improvement in the ten client case.
The overall time includes time spent performing client computation, direct fetch requests, peer fetches, and commit requests.
In the three client group, Buddy and Base incur almost the same commit cost and therefore the entire performance benefit of Buddy is due to peer fetch avoiding direct fetches.
In the five and ten client group the server fetch cost for individual client decreases because with more clients faulting in a fixed size shared module into BuddyCache, each client needs to perform less server fetches.
Figure 8 shows the overall time and cost break down in the 80 ms network. The BuddyCache provides similar performance improvements as with the 40ms network. Higher network latency increases the relative performance advantage provided by peer fetch relative to direct fetch but this benefit is offset by the increased commit times.
Figure 9 shows the relative latency improvement provided by BuddyCache (computed as the overall measured time difference between Buddy and Base relative to Base) as a -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 Latency [ms] 3 Clients 3 Clients (Perf model) 5 Clients 5 Clients (Perf model) 10 Clients 10 FEs (perf model) Figure 9: Cold miss benefit 0 2 0 4 0 6 0 8 0 100 120 140 Base Buddy Reader Buddy Writer [ms] CPU Commit Server Fetch Peer Fetch Figure 10: Breakdown for hot read-write 40ms RTT function of network latency, with a fixed server load. The cost of the extra mechanism dominates BuddyCache benefit when network latency is low. At typical Internet latencies 20ms-60ms the benefit increases with latency and levels off around 60ms with significant (up to 62% for ten clients) improvement.
Figure 9 includes both the measured improvement and the improvement derived using the analytical model.Remarkably, the analytical results predict the measured improvement very closely, albeit being somewhat higher than the empirical values. The main reason why the simplified model works well is it captures the dominant performance component, network latency cost.
To evaluate the performance benefits provided by BuddyCache due to avoided invalidation misses, we compared the hot cache performance of the Base system with two different Buddy system configurations. One of the Buddy system configurations represents a collaborating peer group modifying shared objects (Writer group), the other represents a group where the peers share a read-only interest in the modified objects (Reader group) and the writer resides outside the BuddyCache group.
In each of the three systems, a single client runs a readwrite workload (writer) and three other clients run read-only workload (readers). Buddy system with one group contain36 0 5 0 100 150 200 250 300 Base Buddy Reader Buddy Writer [ms] CPU Commit Server Fetch Peer Fetch Figure 11: Breakdown for hot read-write 80ms RTT ing a single reader and another group containing two readers and one writer models the Writer group. Buddy system with one group containing a single writer and another group running three readers models the Reader group. In Base, one writer and three readers access the server directly. This simple configuration is sufficient to show the impact of BuddyCache techniques.
Figures 10 and 11 show the overall time to complete 1000 hot cache OO7 read-only transactions. We obtain the numbers by running 2000 transactions to filter out cold misses and then time the next 1000 transactions. Here again, the reported numbers are derived from the local area network experiment results.
The results show that the BuddyCache reduces significantly the completion time compared to the Base system.
In a 40 ms network, the overall time in the Writer group improves by 62% compared to Base. This benefit is due to peer update that avoids all misses due to updates. The overall time in the Reader group improves by 30% and is due to peer fetch that allows a client to access an invalidated object at the cost of a local fetch avoiding the delay of fetching from the server. The latter is an important benefit because it shows that on workloads with updates, peer fetch allows an invalidation-based protocol to provide some of the benefits of update-based protocol.
Note that the performance benefit delivered by the peer fetch in the Reader group is approximately 50% less than the performance benefit delivered by peer update in the Writer group. This difference is similar in 80ms network.
Figure 12 shows the relative latency improvement provided by BuddyCache in Buddy Reader and Buddy Writer configurations (computed as the overall time difference between BuddyReader and Base relative to Base, and Buddy Writer and Base relative to Base) in a hot cache experiment as a function of increasing network latency, for fixed server load.
The peer update benefit dominates overhead in Writer configuration even in low-latency network (peer update incurs minimal overhead) and offers significant 44-64% improvement for entire latency range.
The figure includes both the measured improvement and the improvement derived using the analytical model. As in cold cache experiments, here the analytical results predict the measured improvement closely. The difference is -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 Latency [ms] Benefits[%] Buddy Reader Buddy Reader (perf model) Buddy Writer Buddy Writer (perf model) Figure 12: Invalidation miss benefit minimal in the "writer group", and somewhat higher in the "reader group" (consistent with the results in the cold cache experiments). As in cold cache case, the reason why the simplified analytical model works well is because it captures the costs of network latency, the dominant performance cost.
Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area network because of high network latency.
This paper described BuddyCache, a new transactional cooperative caching [20, 16, 13, 2, 28] technique that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The technique improves performance yet provides strong correctness and availability properties in the presence of node failures and slow clients.
BuddyCache uses redirection to fetch missing objects directly from group members caches, and to support peer update, a new lightweight application-level multicast technique that gives group members consistent access to the new data committed within the collaborating group without imposing extra overhead outside the group. Redirection, however, can interfere with object availability. Solo commit, is a new validation technique that allows a client in a group to commit independently of slow or failed peers. It provides fine-grained validation using inexpensive coarse-grain version information.
We have designed and implemented BuddyCache prototype in Thor distributed transactional object storage system [23] and evaluated the benefits and costs of the system over a range of network latencies. Analytical results, supported by the system measurements using the multi-user 007 benchmark indicate, that for typical Internet latencies BuddyCache provides significant performance benefits, e.g. for latencies ranging from 40 to 80 milliseconds round trip time, clients using the BuddyCache can reduce by up to 50% the latency of access to shared objects compared to the clients accessing the repository directly.
The main contributions of the paper are:
37 fine-grain strong-consistency access in high-latency environments,
strong performance gains over the base system,
evaluation of the costs and benefits of the new techniques capturing the dominant performance cost, high network latency.
We are grateful to Jay Lepreau and the staff of Utah experimental testbed emulab.net [1], especially Leigh Stoller, for hosting the experiments and the help with the testbed.
We also thank Jeff Chase, Maurice Herlihy, Butler Lampson and the OOPSLA reviewers for the useful comments that improved this paper.
[1] "emulab.net", the Utah Network Emulation Facility. http://www.emulab.net. [2] A. Adya, M. Castro, B. Liskov, U. Maheshwari, and L. Shrira. Fragment Reconstruction: Providing Global Cache Coherence in a Transactional Storage System.
Proceedings of the International Conference on Distributed Computing Systems, May 1997. [3] A. Adya, R. Gruber, B. Liskov, and U. Maheshwari.
Efficient optimistic concurrencty control using loosely synchronized clocks. In Proceedings of the ACM SIGMOD International Conference on Management of Data, May 1995. [4] C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu,
R. Rajamony, W. Yu, and W. Zwaenepoel.
Treadmarks: Shared memory computing on networks of workstations. IEEE Computer, 29(2), February
[5] C. Anderson and A. Karlin. Two Adaptive Hybrid Cache Coherency Protocols. In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture (HPCA "96), February 1996. [6] M. Baker. Fast Crash Recovery in Distributed File Systems. PhD thesis, University of California at Berkeley, 1994. [7] P. Cao and C. Liu. Maintaining Strong Cache Consistency in the World Wide Web. In 17th International Conference on Distributed Computing Systems., April 1998. [8] M. Carey, D. J. Dewitt, C. Kant, and J. F. Naughton.
A Status Report on the OO7 OODBMS Benchmarking Effort. In Proceedings of OOPSLA, October 1994. [9] A. Chankhunthod, M. Schwartz, P. Danzig,
K. Worrell, and C. Neerdaels. A Hierarchical Internet Object Cache. In USENIX Annual Technical Conference, January 1995. [10] J. Chase, S. Gadde, and M. Rabinovich. Directory Structures for Scalable Internet Caches. Technical Report CS-1997-18, Dept. of Computer Science, Duke University, November 1997. [11] J. Chase, S. Gadde, and M. Rabinovich. Not All Hits Are Created Equal: Cooperative Proxy Caching Over a Wide-Area Network. In Third International WWW Caching Workshop, June 1998. [12] D. R. Cheriton and D. Li. Scalable Web Caching of Frequently Updated Objects using Reliable Multicast. 2nd USENIX Symposium on Internet Technologies and Systems, October 1999. [13] M. D. Dahlin, R. Y. Wang, T. E. Anderson, and D. A.
Patterson. Cooperative caching: Using remote client memory to improve file system performance.
Proceedings of the USENIX Conference on Operating Systems Design and Implementation, November 1994. [14] S. Dwarkadas, H. Lu, A.L. Cox, R. Rajamony, and W. Zwaenepoel. Combining Compile-Time and Run-Time Support for Efficient Software Distributed Shared Memory. In Proceedings of IEEE, Special Issue on Distributed Shared Memory, March 1999. [15] Li Fan, Pei Cao, Jussara Almeida, and Andrei Broder.
Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol. In Proceedings of ACM SIGCOMM,
September 1998. [16] M. Feeley, W. Morgan, F. Pighin, A. Karlin, and H. Levy. Implementing Global Memory Management in a Workstation Cluster. Proceedings of the 15th ACM Symposium on Operating Systems Principles,
December 1995. [17] M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M.
Levy. Integrating Coherency and Recoverablity in Distributed Systems. In Proceedings of the First Usenix Symposium on Operating sustems Design and Implementation, May 1994. [18] P. Ferreira and M. Shapiro et al. PerDiS: Design,
Implementation, and Use of a PERsistent DIstributed Store. In Recent Advances in Distributed Systems,
LNCS 1752, Springer-Verlag, 1999. [19] M. J. Franklin, M. Carey, and M. Livny. Transactional Client-Server Cache Consistency: Alternatives and Performance. In ACM Transactions on Database Systems, volume 22, pages 315-363, September 1997. [20] Michael Franklin, Michael Carey, and Miron Livny.
Global Memory Management for Client-Server DBMS Architectures. In Proceedings of the 19th Intl.
Conference on Very Large Data Bases (VLDB),
August 1992. [21] S. Ghemawat. The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases. PhD thesis, Massachusetts Institute of Technology, 1997. [22] L. Kawell, S. Beckhardt, T. Halvorsen, R. Ozzie, and I. Greif. Replicated document management in a group communication system. In Proceedings of the ACM CSCW Conference, September 1988. [23] B. Liskov, M. Castro, L. Shrira, and A. Adya.
Providing Persistent Objects in Distributed Systems.
In Proceedings of the 13th European Conference on Object-Oriented Programming (ECOOP "99), June
[24] A. Muthitacharoen, B. Chen, and D. Mazieres. A Low-bandwidth Network File System. In 18th ACM Symposium on Operating Systems Principles, October
[25] B. Oki and B. Liskov. Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems. In Proc. of ACM Symposium on Principles of Distributed 38 Computing, August 1988. [26] J. O"Toole and L. Shrira. Opportunistic Log: Efficient Installation Reads in a Reliable Object Server. In Usenix Symposium on Operation Systems Design and Implementation, November 1994. [27] D. Pendarakis, S. Shi, and D. Verma. ALMI: An Application Level Multicast Infrastructure. In 3rd USENIX Symposium on Internet Technologies and Systems, March 2001. [28] P. Sarkar and J. Hartman. Efficient Cooperative Caching Using Hints. In Usenix Symposium on Operation Systems Design and Implementation,
October 1996. [29] A. M. Vahdat, P. C. Eastham, and T. E Anderson.
WebFS: A Global Cache Coherent File System.
Technical report, University of California, Berkeley,
[30] A. Wolman, G. Voelker, N. Sharma, N. Cardwell,
A. Karlin, and H. Levy. On the Scale and Performance of Cooperative Web Proxy Caching. In 17th ACM Symposium on Operating Systems Principles,
December 1999. [31] J. Yin, L. Alvisi, M. Dahlin, and C. Lin. Hierarchical Cache Consistency in a WAN. In USENIX Symposium on Internet Technologies and Systems, October 1999. [32] J. Yin, L. Alvisi, M. Dahlin, and C. Lin. Volume Leases for Consistency in Large-Scale Systems. IEEE Transactions on Knowledge and Data Engineering, 11(4), July/August 1999. [33] M. Zaharioudakis, M. J. Carey, and M. J. Franklin.
Adaptive, Fine-Grained Sharing in a Client-Server OODBMS: A Callback-Based Approach. ACM Transactions on Database Systems, 22:570-627,
December 1997.
This appendix outlines the BuddyCache failover protocol.
To accommodate heterogeneous clients including resourcepoor hand-helds we do not require the availability of persistent storage in the BuddyCache peer group. The BuddyCache design assumes that the client caches and the redirector data structures do not survive node failures.
A failure of a client or a redirector is detected by a membership protocol that exchanges periodic I am alive messages between group members and initiates a failover protocol. The failover determines the active group participants, re-elects a redirector if needed, reinitializes the BuddyCache data structures in the new configuration and restarts the protocol. The group reconfiguration protocol is similar to the one presented in [25]. Here we describe how the failover manages the BuddyCache state.
To restart the BuddyCache protocol, the failover needs to resynchronize the redirector page directory and clientserver request forwarding so that active clients can continue running transactions using their caches. In the case of a client failure, the failover removes the crashed client pages from the directory. Any response to an earlier request initiated by the failed client is ignored except a commit reply, in which case the redirector distributes the retained committed updates to active clients caching the modified pages.
In the case of a redirector failure, the failover protocol reinitializes sessions with the servers and clients, and rebuilds the page directory using a protocol similar to one in [6]. The newly restarted redirector asks the active group members for the list of pages they are caching and the status of these pages, i.e. whether the pages are complete or incomplete.
Requests outstanding at the redirector at the time of the crash may be lost. A lost fetch request will time out at the client and will be retransmitted. A transaction running at the client during a failover and committing after the failover is treated as a regular transaction, a transaction trying to commit during a failover is aborted by the failover protocol. A client will restart the transaction and the commit request will be retransmitted after the failover.
Invalidations, updates or collected update acknowledgements lost at the crashed redirector could prevent the garbage collection of pending invalidations at the servers or the vcache in the clients. Therefore, servers detecting a redirector crash retransmit unacknowledged invalidations and commit replies.
Unique version numbers in invalidations and updates ensure that duplicate retransmitted requests are detected and discarded.
Since the transaction validation procedure depends on the cache coherence protocol to ensure that transactions do not read stale data, we now need to argue that BuddyCache failover protocol does not compromise the correctness of the validation procedure. Recall that BuddyCache transaction validation uses two complementary mechanisms, page version numbers and invalidation acknowledgements from the clients, to check that a transaction has read up-to-date data.
The redirector-based invalidation (and update) acknowledgement propagation ensures the following invariant. When a server receives an acknowledgement for an object o modification (invalidation or update) from a client group, any client in the group caching the object o has either installed the latest value of object o, or has invalidated o.
Therefore, if a server receives a commit request from a client for a transaction T reading an object o after a failover in the client group, and the server has no unacknowledged invalidation for o pending for this group, the version of the object read by the transaction T is up-to-date independently of client or redirector failures.
Now consider the validation using version numbers. The transaction commit record contains a version number for each object read by the transaction. The version number protocol maintains the invariant V P that ensures that the value of object o read by the transaction corresponds to the highest version number for o received by the client. The invariant holds since the client never applies an earlier modification after a later modification has been received.
Retransmition of invalidations and updates maintains this invariant.
The validation procedure checks that the version number in the commit record matches the version number in the unacknowledged outstanding invalidation. It is straightforward to see that since this check is an end-to-end client-server check it is unaffected by client or redirector failure.

Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates [1]. Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers (sensors or other data sources of context information) with different quality levels. For example, in a smart home, thermometer A"s measurement precision is 0.1 ◦ C, and thermometer B"s measurement precision is
C. Thus A could provide more precise context information about temperature than B. Moreover, sometimes different context providers may provide conflictive context information. For example, different sensors report that the same person is in different places at the same time.
Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications.
In pervasive environments, context providers considered as relatively independent entities, have their own interests.
They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized arbitrator provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers" interests. Moreover, when such arbitrator is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide good context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages:
middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware.
decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized arbitrator.
proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems.
automatically. It does not need any applications and users" intervention.
The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process (i.e. rewards).
Negotiator may find current offer and reward worth more than counter-offer which will delay the agreement, and accepts current offer and reward. Without the reward, it may find current offer worth less than the counter-offer, and proposes its counter-offer. It will cost more time to reach agreement.
It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement.
The remainder of this paper is organized as follows.
Section 2 presents some assumptions. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers" strategies. Section 4 evaluates our approach. In section 5 we introduce some related work and conclude in section 6.
Before introducing our approach, we would like to give some assumptions:
During the negotiation process, they exchange information honestly. Rewards confirmed in this negotiation process will be fulfilled in the next negotiation process.
They should provide appropriate context information for current applications. After guaranteeing the system"s interest, they can try to maximize their own personal interests. The assumption is reasonable, because when an inappropriate context provider gets the right to provide bad context, as a punishment, its reputation will decrease, and the proceeds is also very small.
influence their negotiation stance and behavior are private and not available to their opponents. Their utility functions are also private.
environments, time is a critical factors. The current application often hopes to get context information as quickly as possible, so the time cost to reach agreement should be as short as possible. Context providers often have strict deadline by when the negotiation must be completed.
After presenting these assumptions, we will propose our approach based on negotiation with rewards in the next section.
In the beginning, we introduce the concepts of reputation and Quality of Context (QoC) attributes. Both will be used in our approach. Reputation of an agent is a perception regarding its behavior norms, which is held by other agents, based on experiences and observation of its past actions [7].
Here agent means context provider. Each provider"s reputation indicates its historical ability to provide appropriate context information. Quality of Context (QoC) attributes characterize the quality of context information. When applications require context information, they should specify their QoC requirements which express constraints of QoC attributes. Context providers can specify QoC attributes for the context information they deliver. Although we can decide who provides appropriate context according to QoC requirements and context providers" QoC information, applications" QoC requirements might not reflect the actual quality requirements. Thus, in addition to QoC, reputation information of context providers is another factor affecting the decision who can provide context information.
Negotiation is a process by which a joint decision is made by two or more parties. The parties first verbalize contradictory demands and then move towards agreement by a process of concession making or search for new alternatives [2].
In pervasive environments, all available context providers negotiate with each other to decide who can provide context information. This process will be repeated because a kind of context is needed more than one time.
Negotiation using persuasive arguments (such as threats, promises of future rewards, and appeals) allows negotiation parties to influence each others" preferences to reach better deals effectively and efficiently [9]. This pervasive negotiation is effective in repeated interaction because arguments can be constructed to directly impact future encounters. In this paper, for simplicity, we let negotiation take place between two providers. We extend Raiffa"s basic model for bilateral negotiation [8], and allow negotiators to negotiate with each other by exchanging arguments in the form of promises of future rewards or requests for future rewards. Rewards mean some extra proceeds in the next negotiation process. They can influence outcomes of current and future negotiation.
In our approach, as described by Figure 1, the current application requires Context Manager to provide a specific type of context information satisfying QoC requirements.
Context Manager finds that provider A and B can provide such kind of context with different quality levels. Then the manager tells A and B to negotiate to reach agreement on who can provide the context information and how they will allocate the proceeds. Both providers get reputation information from the database Reputation of Context Providers and QoC requirements, and then negotiate with each other according to our negotiation model. When negotiation is completed, the chosen provider will provide the context information to Context Manager, and then Context Manager delivers such information to the application and also stores it in Context Knowledge Base where current and historical context information is stored. The current application gives the feedback information about the provided context, and then Context Manager will update the chosen provider"s reputation information according to the feedback information.
Context Manager also provides the proceeds to providers according to the feedback information and the time cost on negotiation. In the following parts of this section, we describe our negotiation model in detail, including context providers" utility functions to evaluate offers and rewards, negotiation protocol, and strategies to generate offers and rewards.
Context Knowledge Base Reputation of Context Providers Context provider A Context Manager Negotiate Application"s QoC requirements and feedback Provide QoC requirements and proceeds Manage Context Provide Context Getreputation Getreputation Update reputation information according to feedback Context provider B Figure 1: Negotiate to provide appropriate context information.
During the negotiation process, one provider proposes an offer and a reward to the other provider. An offer is noted as o = (c, p): c indicates the chosen context provider and its domain is Dc (i.e. the two context providers participating in the negotiation); p means the proposer"s portion of the proceeds, and its domain is Dp = [0,1]. Its opponent"s portion of the proceeds is 1−p. The reward ep"s domain is Dep = [-1,1], and |ep| means the extra portion of proceeds the proposer promises to provide or requests in the next negotiation process. ep < 0 means the proposer promises to provide reward, ep > 0 means the proposer requests reward and ep =0 means no reward. The opponent evaluates the offer and reward to decide to accept them or propose a counter-offer and a reward. Thus context providers should have utility functions to evaluate offers and rewards.
Time is a critical factor, and only at times in the set T = {0, 1, 2, . . . tdeadline}, context providers can propose their offers. The set O include all available offers.
Context provider A"s utility function of the offer and reward at time t UA : O × Dep × T → [−1, 1] is defined as: UA(o,ep,t)=(wA 1 ·UA c (c)+wA 2 ·UA p (p)+wA 3 ·UA ep(ep))·δA(t) (1) Similarly, the utility function of A"s opponent (i.e. B) can be defined as: UB(o,ep,t)=(wB 1 ·UB c (c)+wB 2 ·UB p (1−p)+wB 3 ·UB ep(−ep))·δB(t) In (1), wA 1 , wA 2 and wA 3 are weights given to c, p and ep respectively, and wA 1 + wA 2 + wA 3 =1. Usually, the context provider pays the most attention to the system"s interests, pays the least attention to the reward, thus wA 1 > wA 2 > wA 3 .
UA c : Dc → [−1, 1] is the utility function of the issue who provides context. This function is determined by two factors: the distance between c"s QoC and current application"s QoC requirements, and c"s reputation. The two negotiators acquire c"s QoC information from c, and we use the approach proposed in [4] to calculate the distance between c"s QoC and the application"s Qoc requirements. The required context has n QoC attributes and let the application"s wishes for this context be a = (a1, a2 . . . an) (where ai = means the application"s indifference to the i-th QoC attribute), c"s QoC attributes cp = (cp1, cp2 . . . cpn) (where cpi = means c"s inability to provide a quantitative value for the i-th QoC attribute). Because numerical distance values of different properties are combined, e.g. location precision in metres with refresh rate in Hz, thus a standard scale for all dimension is needed. The scaling factors for the QoC attributes are s = (s1, s2 . . . sn). In addition, different QoC attributes may have different weights: w = (w1, w2 . . . wn).
Then d = (d1, d2 . . . dn) di = (cpi − ai) · si · wi where cpi−ai = 0 for ai = and cpi−ai = o(ai) for cpi = ( o(.) determines the application"s satisfaction or dissatisfaction when c is unable to provide an estimate of a QoC attribute, given the value wished for by the application). The distance can be linear distance (1-norm), Euclidean distance (2-norm), or the maximum distance (max-norm): |d| = |d1| + |d2| + . . . + |dn| (1 − norm) ||d||2 = |d1|2 + |d2|2 + . . . + |dn|2 (2 − norm) ||d||∞ = max{|d1|, |d2| . . . |dn|} (max − norm) The detail description of this calculation can be found in [4].
Reputation of c can be acquired from the database Reputation of Context Providers. UA c (c) : R × Drep → [−1, 1] can be defined as: UA c (c) = wA c1 · UA d (d) + wA c2 · UA rep(rep) wA c1 and wA c2 are weights given to the distance and reputation respectively, and wA c1 + wA c2 = 1. Drep is the domain of reputation information. UA d : R → [0, 1] is a monotonedecreasing function and UA rep : Drep → [−1, 1] is a monotoneincreasing function. UA p : Dp → [0, 1] is the utility function of the portion of proceeds A will receive and it is also a monotone-increasing function. A"s utility function of reward ep UA ep : Dep → [−1, 1] is also a monotone-increasing function and UA ep(0) = 0. δA : T → [0, 1] is the time discount function. It is also a monotone-decreasing function. When time t cost on negotiation increases, δA(t) will decrease, and the utility will also decrease. Thus both negotiators want to reach agreement as quickly as possible to avoid loss of utility.
When provider A and B have got QoC requirements and reputation information, they begin to negotiate. They first set their reserved (the lowest acceptable) utility which can guarantee the system"s interests and their personal interests. When the context provider finds the utility of an offer and a reward is lower than its reserved utility, it will reject this proposal and terminate the negotiation process. The provider who starts the negotiation is chosen randomly. We assume A starts the negotiation, and it proposes offer o and reward ep to B according to its strategy (see subsection 3.3).
When B receives the proposal from A, it uses its utility function to evaluate it. If it is lower than its reserved utility, the provider terminates the negotiation. Otherwise, if UB(o, ep, t) ≥ UB(o , ep , t + 1) i.e. the utility of o and ep proposed by A at time t is greater than the utility of offer o" and reward ep" which B will propose to A at time t + 1, B will accept this offer and reward.
The negotiation is completed. However, if UB(o, ep, t) < UB(o , ep , t + 1) then B will reject A"s proposal, and propose its counter-offer and reward to A. When A receives B"s counter-offer and reward, A evaluates them using its utility function, and compares the utility with the utility of offer and reward it wants to propose to B at time t+2, decides to accept it or give its counter-offer and reward. This negotiation process continues and in each negotiation round, context providers concede in order to reach agreement. The negotiation will be successfully finished when agreement is reached, or be terminated forcibly due to deadline or the utility lower than reserved utility. When negotiation is forced to be terminated,
Context manager will ask A and B to calculate UA c (A), UA c (B),
UB c (A) and UB c (B) respectively. If UA c (A) + UB c (A) > UA c (B) + UB c (B) Context Manager let A provide context. If UA c (A) + UB c (A) < UA c (B) + UB c (B) then B will get the right to provide context information.
When UA c (A) + UB c (A) = UA c (B) + UB c (B) Context Manager will select a provider from A and B randomly. In addition, Context Manager allocates the proceeds between the two providers. Although we can select one provider when negotiation is terminated forcibly, however, this may lead to the unfair allocation of the proceeds.
Moreover, more time negotiators cost on negotiation, less proceeds will be given. Thus negotiators will try to reach agreement as soon as possible in order to avoid unnecessary loss.
When the negotiation is finished, the chosen provider provides the context information to Context Manager which will deliver the information to current application.
According to the application"s feedback information about this context, Context Manager updates the provider"s reputation stored in Reputation of Context Providers. The provider"s reputation may be enhanced or decreased. In addition, according to the feedback and the negotiation time,
Context Manager will give proceeds to the provider. Then the provider will share the proceeds with its opponent according to the negotiation outcome and the reward confirmed in the last negotiation process. For example, in the last negotiation process A promised to give reward ep (0 ≤ ep < 1) to B, and A"s portion of the proceeds is p in current negotiation. Then A"s actual portion of the proceeds is p · (1 − ep), and its opponent B"s portion of the proceeds is 1−p+p·ep.
The context provider might want to pursue the right to provide context information blindly in order to enhance its reputation. However when it finally provides bad context information, its reputation will be decreased and the proceeds is also very small. Thus the context provider should take action according to its strategy. The aim of provider"s negotiation strategy is to determine the best course of action which will result in a negotiation outcome maximizing its utility function (i.e how to generate an offer and a reward).
In our negotiation model, the context provider generates its offer and reward according to its pervious offer and reward and the last one sent by its opponent.
At the beginning of the negotiation, context providers initialize their offers and rewards according to their beliefs and their reserved utility. If context provider A considers that it can provide good context and wants to enhance reputation, then it will propose that A provides the context information, shares some proceeds with its opponent B, and even promises to give reward. However, if A considers that it may provide bad context, A will propose that its opponent B provide the context, and require B to share some proceeds and provide reward.
During the negotiation process, we assume that at time t A proposes offer ot and reward ept to B, at time t + 1, B proposes counter-offer ot+1 and reward ept+1 to A. Then at time t + 2, when the utility of B"s proposal is greater than A"s reserved utility, A gives its response. Now we calculate the expected utility to be conceded at time t +2, we use Cu to express the conceded utility.
Cu = (UA(ot, ept, t) − UA(ot+1, ept+1, t + 1)) · cA(t + 2) (UA(ot, ept, t) > UA(ot+1, ept+1, t + 1), otherwise, A will accept B"s proposal) where cA : T → [0, 1] is a monotoneincreasing function. cA(t) indicates A"s utility concession rate1 . A concedes a little in the beginning before conceding significantly towards the deadline. Then A generates its offer ot+2 = (ct+2, pt+2) and reward ept+2 at time t + 2. The expected utility of A at time t + 2 is: UA(ot+2, ept+2, t + 2) = UA(ot, ept, t + 2) − Cu If UA(ot+2, ept+2, t + 2) ≤ UA(ot+1, ept+1, t + 1) then A will accept B"s proposal (i.e. ot+1 and ept+1).
Otherwise, A will propose its counter-offer and reward based on Cu. We assume that Cu is distributed evenly on c, p and ep (i.e. the utility to be conceded on c, p and ep is 1 3 Cu respectively). If |UA c (ct)−(UA c (ct)− 1 3 Cu δA(t+2) )| ≤ |UA c (ct+1)−(UA c (ct)− 1 3 Cu δA(t+2) )| i.e. the expected utility of c at time t+2 is UA c (ct)− 1 3 Cu δA(t+2) and it is closer to the utility of A"s proposal ct at time t, then at time t + 2, ct+2 = ct, else the utility is closer to B"proposal ct+1 and ct+2 = ct+1. When ct+2 is equal to ct, the actual conceded utility of c is 0, and the total concession of p and ep is Cu. We divide the total concession of p and ep evenly, and get the conceded utility of p and ep respectively.
We calculate pt+2 and ept+2 as follows: pt+2 = (UA p )−1 (UA p (pt) − 1 2 Cu δA(t + 2) ) ept+2 = (UA ep)−1 (UA ep(ept) − 1 2 Cu δA(t + 2) ) When ct+2 is equal to ct+1, the actual conceded utility of c is |UA c (ct+2) − UA c (ct)|, the total concession of p and ep is Cu δA(t+2) − |UA c (ct+2) − UA c (ct)|, then: pt+2 = (UA p )−1 (UA p (pt)− 1 2 ( Cu δA(t + 2) −|UA c (ct+2)−UA c (ct)|)) ept+2 = (UA ep)−1 (UA ep(ept)−1 2 ( Cu δA(t+2) −|UA c (ct+2)−UA c (ct)|)) Now, we have generated the offer and reward A will propose at time t + 2. Similarly, B also can generate its offer and reward. 1 For example, cA(t) = ( t tdeadline ) 1 β (0 < β < 1) Utility function and weight of c, p and ep Uc, w1 Up, w2 Uep, w3 A 0.5(1 − dA 500 ) + 0.5repA 1000 , 0.6 0.9p, 0.3 0.9ep, 0.1 B 0.52(1 − dB 500 ) + 0.48repB 1000 , 0.5 0.9p, 0.45 0.8ep, 0.05 Table 1: Utility functions and weights of c, p and ep for each provider
In this section, we evaluate the effectiveness of our approach by simulated experiments. Context providers A and B negotiate to reach agreement. They get QoC requirements and calculate the distance between Qoc requirements and their QoC. For simplicity, in our experiments, we assume that the distance has been calculated, and dA represents distance between QoC requirements and A"s QoC, dB represents distance between QoC requirements and B"s QoC.
The domain of dA and dB is [0,500]. We assume reputation value is a real number and its domain is [-1000, 1000], repA represents A"s reputation value and repB represents B"s reputation value. We assume that both providers pay the most attention to the system"s interests, and pay the least attention to the reward, thus w1 > w2 > w3, and the weight of Ud approximates the weight of Urep. A and B"s utility functions and weights of c, p and ep are defined in Table 1. We set deadline tdeadline = 100, and define time discount function δ(t) and concession rate function c(t) of A and B as follows: δA(t) = 0.9t δB(t) = 0.88t cA(t) = ( t tdeadline ) 1
cB(t) = ( t tdeadline ) 1
Given different values of dA, dB, repA and repB, A and B negotiate to reach agreement. The provider that starts the negotiation is chosen at random. We hope that when dA dB and repA repB, A will get the right to provide context and get a major portion of the proceeds, and when ∆d = dA − dB is in a small range (e.g. [-50,50]) and ∆rep = repA − repB is in a small range (e.g. [-50,50]), A and B will get approximately equal opportunities to provide context, and allocate the proceeds evenly. When dA−dB 500  approximates to dA−dB 1000 (i.e. the two providers" abilities to provide context information are approximately equal), we also hope that A and B get equal opportunities to provide context and allocate the proceeds evenly.
According to the three situations above, we make three experiments as follows: Experiment 1 : In this experiment, A and B negotiate with each other for 50 times, and at each time, we assign different values to dA, dB, repA, repB (satisfying dA dB and repA repB) and the reserved utilities of A and B. When the experiment is completed, we find 3 negotiation games are terminated due to the utility lower than the reserved utility. A gets the right to provide context for 47 times.
The average portion of proceeds A get is about 0.683, and B"s average portion of proceeds is 0.317. The average time cost to reach agreement is 8.4. We also find that when B asks A to provide context in its first offer, B can require and get more portion of the proceeds because of its goodwill.
Experiment 2 : A and B also negotiate with each other for 50 times in this experiment given different values of dA, dB, repA, repB (satisfying −50 ≤ ∆d = dA − dB ≤ 50 and −50 ≤ ∆rep = drep −drep ≤ 50) and the reserved utilities of A and B. After the experiment, we find that there are 8 negotiation games terminated due to the utility lower than the reserved utility. A and B get the right to provide context for 20 times and 22 times respectively. The average portion of proceeds A get is 0.528 and B"s average portion of the proceeds is 0.472. The average time cost on negotiation is
Experiment 3 : In this experiment, A and B also negotiate with each other for 50 times given dA, dB, repA, repB (satisfying −0.2 ≤ dA−dB 500 − dA−dB 1000 ≤ 0.2) and the reserved utilities of A and B. There are 6 negotiation games terminated forcibly. A and B get the right to provide context for 21 times and 23 times respectively. The average portion of proceeds A get is 0.481 and B"s average portion of the proceeds is 0.519. The average time cost on negotiation is
One thing should be mentioned is that except for d, rep, p and ep, other factors (e.g. weights, time discount function δ(t) and concession rate function c(t)) could also affect the negotiation outcome. These factors should be adjusted according to providers" beliefs at the beginning of each negotiation process. In our experiments, for similarity, we assign values to them without any particularity in advance. These experiments" results prove that our approach can choose an appropriate context provider and can provide a relatively fair proceeds allocation. When one provider is obviously more appropriate than the other provider, the provider will get the right to provide context and get a major portion of the proceeds. When both providers have the approximately same abilities to provide context, their opportunities to provide context are equal and they can get about a half portion of the proceeds respectively.
In [4], Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. Their adaptive middleware uses utility functions to choose the best context provider (given the QoC requirements of applications and the QoC of alternative means of context acquisition). In our negotiation model, the calculation of utility function Uc was inspired by this approach. Henricksen and Indulska propose an approach to modelling and using imperfect information in [3]. They characterize various types and sources of imperfect context information and present a set of novel context modelling constructs. They also outline a software infrastructure that supports the management and use of imperfect context information. Judd and Steenkiste in [5] describe a generic interface to query context services allowing clients to specify their quality requirements as bounds on accuracy, confidence, update time and sample interval.
In [6], Lei et al. present a context service which accepts freshness and confidence meta-data from context sources, and passes this along to clients so that they can adjust their level of trust accordingly. [10] presents a framework for realizing dynamic context consistency management. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources.
Most approaches to provide appropriate context utilize a centralized arbitrator. In our approach, we let distributed context providers themselves decide who can provide appropriate context information. Our approach can reduce the burden of the middleware, because we do not need the middleware to provide a context selection mechanism. It can avoid the serious consequences caused by a breakdown of the arbitrator. Also, it can guarantee context providers" interests.
How to provide the appropriate context information is a challenging problem in pervasive computing. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers" interests by a relatively fair proceeds allocation.
In this paper, we only consider how to choose an appropriate context provider from two providers. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem. We assume that the context provider will fulfill its promise of reward in the next negotiation process. In fact, the context provider might deceive its opponent and provide illusive promise. We should solve this problem in the future. We also should deal with interactions which are interrupted by failing communication links in the future work.
The work is funded by 973 Project of China(2002CB312002, 2006CB303000), NSFC(60403014) and NSFJ(BK2006712).
[1] J. Coutaz, J. L. Crowley, S. Dobson, and D. Garlan.
Context is key. Commun. ACM, 48(3):49 - 53, March
[2] D.G.Pruitt. Negotiation behavior. Academic Press,
[3] K. Henricksen and J. Indulska. Modelling and using imperfect context information. In Proceedings of the Second IEEE Annual Conference on Pervasive Computing and Communications Workshops, pages 33-37, 2004. [4] M. C. Huebscher and J. A. McCann. Adaptive middleware for context-aware applications in smart-homes. In Proceedings of the 2nd workshop on Middleware for pervasive and ad-hoc computing MPAC "04, pages 111-116, October 2004. [5] G. Judd and P. Steenkiste. Providing contextual information to pervasive computing applications. In Proceedings of the First IEEE International Conference on Pervasive Computing and Communications, pages 133-142, 2003. [6] H. Lei, D. M. Sow, J. S. Davis, G. Banavar, and M. R.
Ebling. The design and applications of a context service. ACM SIGMOBILE Mobile Computing and Communications Review, 6(4):45-55, 2002. [7] J. Liu and V. Issarny. Enhanced reputation mechanism for mobile ad-hoc networks. In Trust Management: Second International Conference, iTrust, 2004. [8] H. Raiffa. The Art and Science of Negotiation.
Harvard University Press, 1982. [9] S. D. Ramchurn, N. R. Jennings, and C. Sierra.
Persuasive negotiation for autonomous agents: A rhetorical approach. In C. Reed, editor, Workshop on the Computational Models of Natural Argument,

Security communication is an important requirement in many sensor network applications, so shared secret keys are used between communicating nodes to encrypt data. As one of the most fundamental security services, pairwise key establishment enables the sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the sensor nodes' limited computational capabilities, battery energy, and available memory, it is not feasible for them to use traditional pairwise key establishment techniques such as public key cryptography and key distribution center (KDC). Several alternative approaches have been developed recently to perform pairwise key establishment on resource-constrained sensor networks without involving the use of traditional cryptography [14].
Eschenauer and Gligor proposed a basic probabilistic key predistribution scheme for pairwise key establishment [1]. In the scheme, each sensor node randomly picks a set of keys from a key pool before the deployment so that any two of the sensor nodes have a certain probability to share at least one common key. Chan et al. further extended this idea and presented two key predistribution schemes: a q-composite key pre-distribution scheme and a random pairwise keys scheme. The q-composite scheme requires any two sensors share at least q pre-distributed keys. The random scheme randomly picks pair of sensors and assigns each pair a unique random key [2]. Inspired by the studies above and the polynomial-based key pre-distribution protocol [3], Liu et al. further developed the idea addressed in the previous works and proposed a general framework of polynomial pool-based key predistribution [4]. The basic idea can be considered as the combination of the polynomial-based key pre-distribution and the key pool idea used in [1]] and [2]. Based on such a framework, they presented two pairwise key pre-distribution schemes: a random subset assignment scheme and a grid-based scheme. A polynomial pool is used in those schemes, instead of using a key pool in the previous techniques. The random subset assignment scheme assigns each sensor node the secrets generated from a random subset of polynomials in the polynomial pool. The gridbased scheme associates polynomials with the rows and the columns of an artificial grid, assigns each sensor node to a unique coordinate in the grid, and gives the node the secrets generated from the corresponding row and column polynomials. Based on this grid, each sensor node can then identify whether it can directly establish a pairwise key with another node, and if not, what intermediate nodes it can contact to indirectly establish the pairwise key.
A similar approach to those schemes described by Liu et al was independently developed by Du et a. [5]. Rather than on Blundo's scheme their approach is based on Blom's scheme [6]. In some cases, it is essentially equivalent to the one in [4]. All of those schemes above improve the security over the basic probabilistic key pre-distribution scheme. However, the pairwise key establishment problem in sensor networks is still not well solved.
For the basic probabilistic and the q-composite key predistribution schemes, as the number of compromised nodes increases, the fraction of affected pairwise keys increases quickly.
As a result, a small number of compromised nodes may affect a large fraction of pairwise keys [3]. Though the random pairwise keys scheme doses not suffer from the above security problem, it incurs a high memory overhead, which increases linearly with the number of nodes in the network if the level of security is kept constant [2][4]. For the random subset assignment scheme, it suffers higher communication and computation overheads.
In 2004, Liu proposed a new hypercube-based pairwise key predistribution scheme [7], which extends the grid-based scheme from a two dimensional grid to a multi-dimensional hypercube.
The analysis shows that hypercube-based scheme keeps some attractive properties of the grid-based scheme, including the guarantee of establishing pairwise keys and the resilience to node compromises. Also, when perfect security against node compromise is required, the hypercube-based scheme can support a larger network by adding more dimensions instead of increasing the storage overhead on sensor nodes. Though hypercube-based scheme (we consider the grid-based scheme is a special case of hypercube-based scheme) has many attractive properties, it requires any two nodes in sensor networks can communication directly with each other. This strong assumption is impractical in most of the actual applications of the sensor networks.
In this paper, we present a kind of new cluster-based distribution model of sensor networks, and for which, we propose a new pairwise key pre-distribution scheme. The main contributions of this paper are as follows: Combining the deployment knowledge of sensor networks and the polynomial pool-based key pre-distribution, we setup a clusterbased topology that is practical with the real deployment of sensor networks. Based on the topology, we propose a novel cluster distribution based hierarchical hypercube model to establish the pairwise key. The key contribution is that our scheme dose not require the assumption of all nodes can directly communicate with each other as the previous schemes do, and it still maintains high probability of key establishment, low memory overhead and good security performance. We develop a kind of new pairwise key establishment algorithm with our hierarchical hypercube model.
The structure of this paper is arranged as follows: In section 3, a new distribution model of cluster deployed sensor networks is presented. In section 4, a new Hierarchical Hypercube model is proposed. In section 5, the mapping relationship between the clusters deployed sensor network and Hierarchical Hypercube model is discussed. In section 6 and section 7, new pairwise key establishment algorithm are designed based on the Hierarchical Hypercube model and detailed analyses are described. Finally, section 8 presents a conclusion.
Definition 1 (Key Predistribution): The procedure, which is used to encode the corresponding encryption and decryption algorithms in sensor nodes before distribution, is called Key Predistribution.
Definition 2 (Pairwise Key): For any two nodes A and B, if they have a common key E, then the key E is called a pairwise key between them.
Definition 3 (Key Path): For any two nodes A0 and Ak, when there has not a pairwise key between them, if there exists a path A0,A1,A2,……,Ak-1,Ak, and there exists at least one pairwise key between the nodes Ai and Aj for 0≤i≤k-1 and 1≤j≤k, then the path consisted of A0,A1,A2,……,Ak-1,Ak is called a Key Path between A0 and Ak.
Definition 4 (n-dimensional Hypercube): An n-dimensional Hypercube (or n−cube) H(v,n) is a topology with the following properties: (1) It is consisted of n·vn-1 edges, (2) Each node can be coded as a string with n positions such as b1b2…bn, where 0≤b1,b2,…,bn≤v-1, (3) Any two nodes are called neighbors, which means that there is an edge between them, iff there is just one position different between their node codes.
SENSOR NETWORKS In some actual applications of sensor networks, sensors can be deployed through airplanes. Supposing that the deployment rounds of sensors are k, and the communication radius of any sensors is r, then the sensors deployed in the same round can be regarded as belonging to a same Cluster. We assign a unique cluster number l (1 ≤ l ≤ k) for each cluster. Supposing that the sensors form a connected graph in any cluster after deployment through airplanes, and then the Fig.1 presents an actual model of clusters deployed sensor networks.
Figure.1 An actual model of clusters deployed sensor networks.
From Figure.1, it is easy to know that, for a given node A, there exist lots of nodes in the same cluster of A, which can be communicated directly with A, since the nodes are deployed densely in a cluster. But there exist much less nodes in a cluster neighboring to the cluster of A, which can be communicated directly with A. since the two clusters are not deployed at the same time.
Definition 5 (k-levels Hierarchical Hypercube): Let there are N nodes totally, then a k-levels Hierarchical Hypercube named H(k,u,m,v,n) can be constructed as follows: 1) The N nodes are divided into k clusters averagely, and the [N/k] nodes in any cluster are connected into an n-dimensional Hypercube: In the n-dimensional Hypercube, any node is encoded 55 as i1i2…in, which are called In-Cluster-Hypercube-Node-Codes, where 0 ≤ i1,i2,…in ≤ v-1,v=[ n kN / ],[j] equals to an integer not less than j. So we can obtain k such kind of different hypercubes. 2) The k different hypercubes obtained above are encoded as j1j2…jm, which are called Out-Cluster-Hypercube-Node-Codes, where 0 ≤ j1,j2,…jm ≤ u-1,u=[ m k ]. And the nodes in the k different hypercubes are connected into m-dimensional hypercubes according to the following rules: The nodes with same In-Cluster-Hypercube-Node-Codes and different Out-ClusterHypercube-Node-Codes are connected into an m-dimensional hypercube. (The graph constructed through above steps is called a k-levels Hierarchical Hypercube abbreviated as H(k,u,m,v,n).) 3) Any node A in H(k,u,m,v,n) can be encoded as (i, j), where i(i=i1i2…in, 0 ≤ i1,i2,…in ≤ v-1) is the In-Cluster-HypercubeNode-Code of node A, and j(j=j1j2…jm, 0 ≤ j1,j2,…jm ≤ u-1) is the Out-Cluster-Hypercube-Node-Code of node A.
Obviously, the H(k,u,m,v,n) model has the following good properties: Property 1: The diameter of H(k,u,m,v,n) model is m+n.
Proof: Since the diameter of n-dimensional hypercube is n, and the diameter of m-dimensional hypercube is m, so it is easy to know that the diameter of H(k,u,m,v,n) model is m+n from the definition 5.
Property 2: The distance between any two nodes A(i1, j1) and B(i2, j2) in H(k,u,m,v,n) model is d(A,B)= dh(i1, i2)+dh(j1, j2), where dh represents the Hamming distance.
Proof: Since the distance between any two nodes in hypercube equals to the Hamming distance between them, so it is obvious that the theorem 2"s conclusion stands from definition 5.
SENSOR NETWORKS TO H(K,U,M,V,N) Obviously, from the description in section 3 and 4, we can know that the clusters deployed sensor network can be mapped into a klevels- hierarchical hypercube model as follows: At first, the k clusters in the sensor network can be mapped into k different levels (or hypercubes) in the k-levels- hierarchical hypercube model. Then, the sensor nodes in each cluster can be encoded with the In-Cluster-Hypercube-Node-Codes, and the sensor nodes in the k different clusters with the same In-ClusterHypercube-Node-Codes can be encoded with the Out-ClusterHypercube-Node-Codes according to the definition 5 respectively.
Consequently, the whole sensor network has been mapped into a k-levels- hierarchical hypercube model.
PAIRWISE KEY PREDISTRIBUTION ALGORITHM FOR SENSOR NETWORKS In order to overcome the drawbacks of polynomial-based and polynomial pool-based key predistribution algorithms, this paper proposed an innovative H(k,u,m,v,n) model-based key predistribution scheme and pairwise key establishment algorithm, which combines the advantages of polynomial-based and key pool-based encryption schemes, and is based on the KDC and polynomials pool-based key predistribution models.
The new H(k,u,m,v,n) model-based pairwise key establishment algorithm includes three main steps: (1) Generation of the polynomials pool and key predistribution, (2) Direct pairwise key discovery, (3) Path key discovery.
Predistribution Supposing that, the sensor network includes N nodes, and is deployed through k different rounds. Then we can predistribute keys for each sensor node on the basis of the H(k,u,m,v,n) model as follows: Step 1: Key setup server randomly generates a bivariate polynomials pool such as the following: F={ f i iiil n >< −121 ,...,,, (x,y), f j jjjinii m >< −121 ,...,,,,...,2,1 (x,y) | 0 ≤ iii n 121 ... −≤≤≤ ≤ v-1, 1 ≤ i ≤ n, 1 ≤ l ≤ k; 0 ≤ jjj m 121 ... −≤≤≤ ≤ u-1 , 1 ≤ j ≤ m} with vn *m*um-1 +[N/vn ]*n*vn-1 different t-degree bivariate polynomials over a finite field Fq, and then assigns a unique polynomial ID to each bivariate polynomial in F.
Step 2: In each round, key setup server assigns a unique node ID: (i1i2…in,j1j2…jm) to each sensor node from small to big, where 0 ≤ i1,i2,…in ≤ v-1, 0 ≤ j1,j2,…jm ≤ u-1.
Step 3: key setup server assigns a unique cluster ID: l to all the sensor nodes deployed in the same round, where 1 ≤ l ≤ k.
Step 4: key setup server predistributes m+n bivariate polynomials { f iiil n 1 ,...,,, 32 >< (i1,y),…, f n iiil n >< −121 ,...,,, (in,y); f jjinii m 1 ,...,,,...,2,1 2 >< ( j1,y),…, f m jjinii m >< −11 ,...,,,...,2,1 ( jm,y) } and the corresponding polynomial IDs to the sensor node deployed in the lth round and with ID (i1i2…in, j1j2…jm).
If the node A(i1i2…in,j1j2…jm) in the sensor network wants to establish pairwise key with a node B (i'1i'2…i'n,j'1j'2…j'm), then node A can establish pairwise key with the node B trough the following methods.
Firstly, node A computes out the distance between itself and node B: d= d1+ d2, where d1=dh(i1i2…in, i'1i'2…i'n) and d2=dh(j1j2…jm, j'1j'2…j'm). If d=1, then node A obtains the direct pairwise key between itself and node B according to the following theorem 1: Theorem 1: For any two sensor nodes A(i1i2…in,j1j2…jm) and B (i'1i'2…i'n,j'1j'2…j'm) in the sensor network, supposing that the 56 distance between nodes A and B is d= d1+ d2, where d1=dh(i1i2…in, i'1i'2…i'n) and d2=dh(j1j2…jm, j'1j'2…j'm). If d=1, then there exists a direct pairwise key between nodes A and B.
Poof: Since d=1, then there is d1=1, d2=0, or d1=0, d2=1. 1) If d1=1, d2=0: From d2=0, there is nodes A, B belong to the same cluster. Supposing that nodes A, B belong to the same cluster l, then from d1=1 ⇒ There is only one position different between i1i2…in and i"1i"2…i"n. Let it=i"t, when 1 ≤ t ≤ n-1, and in ≠ i"n ⇒ f n iiil n >< −121 ,...,,, (in,i"n)= f n iiil n >′′′< −121 ,...,,, (i"n,in). So, there exists a direct pairwise key f n iiil n >< −121 ,...,,, (in,i"n) between nodes A and B. 2) If d1=0, d2=1: From d2=1 ⇒ There is only one position different between j1j2…jm and j"1j"2…j"m. Let jt=j"t, when 1 ≤ t ≤ m1, and jm ≠ j"m. Since d1=0 ⇒ i1i2…in equals to i"1i"2…i"n ⇒ f m jjjinii m >< −121 ,...,,,,...,2,1 (jm, j"m)= f m jjji nii m >′′′′′′< −121 ,...,,,,...,2,1 (j"m,jm). So, there exists a direct pairwise key f m jjjinii m >< −121 ,...,,,,...,2,1 (jm, j"m) between nodes A and B.
According to theorem 1, we present the detailed description of the direct pairwise key discovery algorithm as follows: Step 1: Obtain the node IDs and cluster IDs of the source node A and destination node B; Step 2: Compute out the distance between nodes A and B: d= d1+ d2; Step 3: If d1=1, d2=0, then select out a common polynomial share of nodes A and B from { f iiil n 1 ,...,,, 32 >< ,..., f n iiil n >< −121 ,...,,, } to establish direct pairwise key; Step 4: If d1=0, d2=1, then select out a common polynomial share of nodes A and B from { f jjinii m 1 ,...,,,...,2,1 2 >< ,..., f m jjjinii m >< −121 ,...,,,,...,2,1 } to establish direct pairwise key; Step 5: Otherwise, there exists no direct pairwise key between nodes A and B. And then turn to the following path key discovery process.
If d>1, then node A can establish path key with node B according to the following theorem 2: Theorem 2: For any two sensor nodes A(i1i2…in,j1j2…jm) and B (i'1i'2…i'n,j'1j'2…j'm) in the sensor network, supposing that the distance between nodes A and B is d= d1+ d2, where d1=dh(i1i2…in, i'1i'2…i'n) and d2=dh(j1j2…jm, j'1j'2…j'm). If d>1, then there exists a path key between nodes A and B.
Proof: Let d1=a, d2=b, then we can think that it ≠ i't, when 1 ≤ t ≤ a; but it=i't, when t>a; and jt ≠ j't, when 1 ≤ t ≤ b; but jt=j't, when t>b. Obviously, nodes A(i1i2…in, j1j2…jm) ,(i'1i2 i3…in, j1j2…jm),(i'1i'2 i3…in, j1j2…jm),…,(i'1i'2…i'n, j1j2…jm) belong to the same cluster. So, according to the supposing condition of The nodes in the same cluster form a connected graph, there is a route among those nodes. In addition, in those nodes, the distance between any two neighboring nodes is 1, so from theorem 1, it is easy to know that there exists direct pairwise key between any two neighboring nodes among those nodes.
For nodes (i'1i'2…i'n,j1j2…jm), (i'1i'2…i'n,j'1 j2 j3…jm), (i'1i'2…i'n,j'1j'2 j3…jm-1 jm),…, (i'1i'2…i'n,j'1j'2…j'm-1jm), since they have the same Out-Cluster-Hypercube-Node-Codes with the node B(i'1i'2…i'n,j'1j'2…j'm), so nodes (i'1i'2…i'n,j1j2…jm), (i'1i'2…i'n,j'1 j2 j3…jm), (i'1i'2…i'n,j'1j'2 j3…jm-1 jm),…, (i'1i'2…i'n,j'1j'2…j'm-1 jm) and node B belong to a same logical hypercube. Obviously, from the supposing condition of The whole sensor network forms a connected graph, there is a route among those nodes. In addition, in those nodes, the distance between any two neighboring nodes is 1, so from theorem 1, it is easy to know that there exists direct pairwise key between any two neighboring nodes among those nodes.
So, it is obvious that there exists a path key between nodes A and B.
According to theorem 2, we present the detailed description of the path key discovery algorithm as follows: Step 1: Compute out the intermediate nodes (i'1i2 i3…in,j1j2…jm), (i'1i'2 i3…in,j1j2…jm),…,(i'1i'2…i'n, j1j2…jm) and (i'1i'2…i'n,j1'j2 j3…jm), (i'1i'2…i'n,j'1j'2 j3…j'm-1 jm),…,(i'1i'2…i'n,j'1j'2…j'm-1 jm) from the source node A(i1i2…in,j1j2…jm) and the destination node B (i'1i'2…i'n,j'1j'2…j'm).
Step 2: In those nodes series A(i1i2…in,j1j2…jm), (i'1i2 i3…in,j1j2…jm), (i'1i'2 i3…in,j1j2…jm),…,(i'1i'2…i'n,j1j2…jm) and (i'1i'2…i'n,j1'j2 j3…jm), (i'1i'2…i'n,j'1j'2 j3…j'm-1 jm),…, (i'1i'2…i'n, j'1j'2…j'm-1 jm), B (i'1i'2…i'n,j'1j'2…j'm), the neighboring nodes select their common polynomial share to establish direct pairwise key.
From theorem 2, it is easy to know that any source node A can compute out a key path P to the destination node B according to the above algorithm, when there are no compromised nodes in the sensor network. Once the key path P is computed out, then node A can send messages to B along the path P to establish indirect pairwise key with node B. Fig.2 presents a example of key path establishment.
Figure.2 Key path establishment example.
For example: In the above Figure.2, node A((012),(1234)) can establish pairwise key with node B((121),(2334)) through the following key path: A((012),(1234)) → C((112),(1234)) → D((122),(1234)) → E((121),(1234)) → F((121),(2234)) → B((121),(2334)), where node F shall route through nodes G, H, I,
J to establish direct pairwise key with node B. 57 According to the properties of H(k,u,m,v,n) model, we can prove that the following theorem by combing the proof of theorem 2: Theorem 3: Supposing that there exist no compromised nodes in the sensor network, and the distance between node A and B, then there exists a shortest key path with k distance between node A and B logically. That is to say, node A can establish indirect pairwise key with node B through t-1 intermediate nodes.
Proof: Supposing that the distance between node A(i1i2…in, j1j2…jm) and B (i'1i'2…i'n, j'1j'2…j'm) is d=d1+ d2, where d1=dh(i1i2…in, i'1i'2…i'n), d2=dh(j1j2…jm, j'1j'2…j'm). Since d=t, according to the construction properties of H(k,u,m,v,n), it is easy to know that there exist t-1 intermediate nodes I1,…,It-1, in the logical space H(k,u,m,v,n), which satisfy that the distance between any two neighboring nodes in the nodes series A,
I1,…,It1, B equals to 1. So according to the theorem 1, we can know that nodes A, I1,…,It-1, B form a correct key path between node A and B. If any two neighboring nodes in the nodes series A, I1,…,It-1, B can communicate directly, then node A can establish indirect pairwise key with node B through those t-1 intermediate nodes.
The path key discovery algorithm proposed in the above section can establish a key path correctly, only when there exist no compromised nodes in the whole sensor network, since the key path is computed out beforehand. And the proposed algorithm cannot find an alternative key path when there exist some compromised nodes or some intermediate nodes not in the communication radius, even that there exists other alternative key paths in the sensor network. From the following example we can know that there are many parallel paths in the H(k,u,m,v,n) model for any two given source and destination nodes, since the H(k,u,m,v,n) model is high fault-tolerant[9,10] .
Figure.3 Alternative key path establishment example.
For example: Considering the key path establishment example given in the above section based on Figure.2: A((012),(1234)) → C((112),(1234)) → D((122),(1234)) → E((121),(1234)) → F((121),(2234)) → B((121),(2334)), supposing that node F((121),(2234)) has compromised, then from Figure.3, we can know that there exists another alternative key path as A((012),(1234)) → C((112),(1234)) → D((122),(1234)) →E((121),(1234)) → M((121),(1334)) → B((121),(2334)), which can be used to establish the indirect pairwise key between node A and B, where node E shall route through nodes D and K to establish direct pairwise key with node M, and node M shall route through nodes N, O, G, H, I, J to establish direct pairwise key with node B.
Since the sensors are source limited, so they are easy to die or out of the communication radius, therefore the algorithm proposed in the above section cannot guarantee to establish correct key path efficiently. In this section, we will propose a dynamic path key discovery algorithm as follows, which can improve the probability of key path effectively: Algorithm I: Dynamic key path establishment algorithm based on H(k,u,m,v,n) model for cluster deployed sensor networks.
Input: Sub-sensor network H(k,u,m,v,n), which has some compromised /fault sensors and fault links, And two reachable nodes A(a1…an,a"1…a"m) and B(b1…bn,b"1…b"m) in H(k,u,m,v,n), where a"t ≠ b"t, t∈[1,s], a"t=b"t, t >s.
Output: A correct key path from node A to B in H(k,u,m,v,n).
Step 1: Obtain the code strings of node A and B: A ← (a1…an,a"1…a"m), B ← (b1…bn,b"1…b"m), where aj, bj [0,∈ u-1], a"j, b"j [0,∈ v-1].
Step 2: If a"1…a"m = b"1…b"m, then node A can find a route to B according to the routing algorithms of hypercube [9-10].
Step 3: Otherwise, node A can find a route to C(b1…bn, a"1…a"m) according to the Algorithm I or Algorithm II. Then let I0=C(b1…bn,a"1…a"m), I1=(b1…bn,b"1 a"2…a"m),…,
Is=B(b1…bn,b"1 b"2…b"s a"s+1…a"m), and each node It in the above nodes series find a route to its neighboring node It+1 on the basis of the location information (Detailed routing algorithms based on location information can see the references[11-14]).
Step 4: Algorithm exits. If such kind of a correct key path exists, then through which node A can establish an indirect pairwise key with node B. Otherwise, node A fails to establish an indirect pairwise key with node B. And node A will tries again to establish an indirect pairwise key with node B some time later.
According to the former description and analyses, it is easy to know that the above newly proposed algorithm has the following properties: Property 3: When there exist no fault and compromised nodes, by using new pairwise key predistribution scheme based on H(k,u,m,v,n) model, the probability of direct pairwise key establishment between any two nodes can be estimated as P=(m(u-1)+n(v-1))/(N-1), where N is the total number of nodes in the sensor network, and N=um * vn .
Proof: Since the predistributed pairwise keys for any node FA ={ f iiil n 1 ,...,,, 32 >< (i1,y),…, f n iiil n >< −121 ,...,,, (in,y); f jjinii m 1 ,...,,,...,2,1 2 >< (j1 ,y),…, f m jjinii m >< −11 ,...,,,...,2,1 ( jm,y) } in the newly proposed algorithm. Obviously, in the logical hypercube formed by the nodes in the same cluster of node A, there are n(v-1) nodes, which 58 have direct pairwise key with node A. And in the logical hypercube formed by the nodes in different clusters from that of node A, there are m(u-1) nodes, which have direct pairwise key with node A. Therefore, there are totally m(u-1)+n(v-1) nodes, which have direct pairwise key with node A. So, the probability of pairwise key establishment between any two nodes can be estimated as P=(m(u-1)+n(v-1))/(N-1), since the whole sensor network has N sensor nodes in all.
Figure.4 presents the comparision between the probability of direct pairwise key establishment between any two nodes and the dimension n, when the sensor network has different total nodes, and use the new pairwise key predistribution scheme based on H(8,2,3,v,n) model. 2 3 4 5 6 7 8 9 10 0
Number of Dimension ProbabilitytoEstablishDirectKey N = 8000 N=10000 N=20000 N=30000 Figure.4 Comparision between the probability of direct pairwise key establishment between any two nodes and the dimension n, when the sensor network has different total nodes, and use the new pairwise key predistribution scheme based on H(8,2,3,v,n) model.
From Figure.4, it is easy to know that by using new pairwise key predistribution scheme based on H(k,u,m,v,n) model, the probability of direct pairwise key establishment between any two nodes decreases with the increasing of the scale of the sensor networks, and in addition, the probability of direct pairwise key establishment between any two nodes decreases with the increasing of the dimension n, when the scale of the sensor network is fixed.
Theorem 4: Supposing that the total sensors is N in the sensor network, then when u ≥ v2 , the probability of direct pairwise key establishment between any two nodes, when using the key distribution scheme based on the hypercube model H(v,p), is smaller than that when using the key distribution scheme based on the H(k,u,m,v,n) model.
Proof: Since u ≥ v, then we can let u=vt , where t ≥ 2. Since the total number of nodes in H(v,p) is vp =N, the total number of nodes in H(k,u,m,v,n) is um * vn =N. Let p=x+n, then there is um *vn = vx * vn ⇒ um =vx ⇒ x=tm.
From the property 3, it is easy to know that the probability of direct pairwise key establishment between any two nodes can be estimated as P=(m(u-1)+n(v-1))/(N-1). According to the description in [7], it is well know that the probability of direct pairwise key establishment between any two nodes can be estimated as P"= p(v-1)/(N-1)= (x(v-1)+n(v-1))/(N-1).
Next, we will prove that m(u-1) ≥ x(v-1): m(u-1)= m(vt -1), x(v-1)= tm(v-1). Construct a function as f(t)= vt -1- t(v-1), where t ≥ 2. When t=2, it is obvious that there is f(t)= vt -2v+1=( v-1)2 ≥ 0 and f"(t)=t vt-1 - v+1 ≥ 2v- v+1= v+1>0.
So, there is f(t) ≥ 0 ⇒ vt -1 ≥ t(v-1) ⇒ m(vt -1) ≥ tm(v-1) ⇒ m(u1) ≥ x(v-1).
Therefore, the conclusion of the theorem stands.
As for the conclusion of theorem 4, we give an example to illustrate.
Supposing that the total number of nodes in the sensor network is N=214 , and H(k,u,m,v,n)=H(16,4,2,2,10), H(v,p)= H(10,14), then the probability of direct pairwise key establishment between any two nodes based on the H(k,u,m,v,n) model is P= (m(u-1)+n(v1))/(N-1)= (2(4-1)+10(2-1))/(214 -1)=16/(214 -1), but the probability of direct pairwise key establishment between any two nodes based on the H(v,p) model is P"= p(v-1)/(N-1)=14(2-1)/(214 -1)= 14/(214  1).
Supposing that the total number of nodes in the sensor network is N, Figure.5 illustrates the comparison between the probability of direct pairwise key establishment between any two nodes based on the H(k,u,m,v,n) model and the probability of direct pairwise key establishment between any two nodes based on the H(v,p) model, when u=4 and v=2. 1 2 3 4 5 6 7 8 9 10 0
1
x 10 -3 scaleofthesensornetwork ProbabilitytoEstablishDirectKey H(k,u,m,v,n)model-based H(v,p)model-based Figure.5 Comparison between the probability of direct pairwise key establishment between H(v,n) and H(k,u,m,v,n) models.
From Figure.5, it is easy to know that the theorem 5 stands.
Theorem 5: Supposing that the total sensors is N in the sensor network, then the pairwise key distribution scheme based on the hypercube model H(v,p), is only a special case of the pairwise key distribution scheme based on the H(k,u,m,v,n) model.
Proof: As for the pairwise key distribution scheme based on the H(k,u,m,v,n) model, let k=1 (u=1, m=0), which means that the total sensor network includes only one cluster. Then obviously, the H(k,u,m,v,n) model will degrade into the H(v,n) model.
According to the former anayses in this paper and the definition of the pairwise key distribution scheme based on the hypercube model H(v,p) in [7], it is easy to know that the conclusion of the theorem stands. 59
By using the pairwise key establishment algorithm based on the H(k,u,m,v,n) model, the intruders can launch two kinds of attacks: 1) The attackers may target the pairwise key between two particular sensor node, in order to compromise the pairwise key between them, or prevent them to establish pairwise key. 2) The attackers may attack against the whole sensor network, inorder to decrease the probability of the pairwise key establishment, or increase the cost of the pairwise key establishment.
Attacks against a Pair of sensor nodes
nodes u,v, where u,v are all not compromised nodes, but the intruders want to compromise the pairwise key between them. 1) If u,v can establish direct pairwise key, then the only way to compromise the key is to compromise the common bivariate polynomial f(x,y) between u,v. Since the degree of the bivariate polynomial f(x,y) is t, so the intruders need to compromise at least t+1 sensor nodes that have a share of the bivariate polynomial f(x,y). 2) If u,v can establish indirect pairwise key through intermediate nodes, then the intruders need to compromise at least one intermediate node, or compromise the common bivariate polynomial f(x,y) between two neighboring intermediate nodes. But even if the intruders succeed to do that, node u and v can still reestablish indirect pairwise key through alternative intermediate nodes.
nodes u,v, where u,v are all not compromised nodes, but the intruders want to prevent them to establish the pairwise key.
Then, the intruders need to compromise all of the m+n bivariate polynomials of node u or v. Since the degree of the bivariate polynomial f(x,y) is t, so for bivariate polynomial, the intruders need to compromise at least t+1 sensor nodes that have a share of the given bivariate polynomial. Therefore, the intruders need to compromise (m+n)(t+1) sensor nodes altogether to prevent u,v to establish the pairwise key.
Attacks against the sensor network Supposing that the Attackers know the distribution of the polynomials over sensor nodes, it may systematically attack the network by compromising the polynomials in F one by one in order to compromise the entire network. Assume the fraction of the compromised polynomials is pc, then there are up to N"=pc × { vn v N umv n n mn ××+×× ][ }= pc ××N (m+n) Sensor nodes that have at least one compromised polynomial share.
Among all of the remaining N- N" sensor nodes, none of them includes a compromised polynomial share. So, the remaining N- N" sensor nodes can establish direct pairwise key by using any one of their polynomial shares. However, the indirect pairwise keys in the remaining N- N" sensor nodes may be affected. And they may need to re-establish a new indirect pairwise key between them by select alternative intermediate nodes that do not belong to the N" compromised nodes.
Supposing that the scale of the sensor network is N=10000, Figure.6 presents the comparison between pc and the number of sensor nodes with at least one compromised polynomial share in sensor networks based on different H(k,u,m,v,n) distribution models.
From Figure.6, it is easy to know that, when the scale of the sensor network is fixed, the number of the affected sensor nodes in the sensor network increases with the increasing of the number of compromised nodes. 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 1000 2000 3000 4000 5000 6000 7000 8000 F rac tion of C om prom is ed B ivariate P oly nom ialsSensorNodeswithatleastoneCompromisedPolynomialShare H (1,0,0,100,2) H (2,2,1,71,2) H (4,2,2,50,2) H (8,2,3,36,2) Figure.6 the comparison between pc and the number of sensor nodes with at least one compromised polynomial share in sensor networks based on different H(k,u,m,v,n) distribution models.
Theorem 6: Supposing that the total sensors is N in the sensor network, and the fraction of compromised nodes is pc, then when u>v, the number of affected nodes of the H(v,p) model based key predistribution scheme, is bigger than that of the H(k,u,m,v,n) model based key predistribution scheme.
Proof: Since the number of affected nodes of the H(k,u,m,v,n) model based key predistribution scheme is pc ××N (m+n), and it is proved in [7] that the number of affected nodes of the H(v,p) model based key predistribution scheme is pc ××N p. Let p=x+n, then there is um * vn = vx * vn ⇒ um =vx . Since u>v ⇒ x>m ⇒ pc ××N (m+n)< pc ××N (x+n)= pc ××N p.
Supposing that the scale of the sensor network is N=10000, Figure.7 presents the comparison between pc and the number of sensor nodes with at least one compromised polynomial share in sensor networks based on H(9,3,2,2,n) and H(2,p) distribution models.
From Figure.7, it is easy to know that the conclusion of theorem 9 is correct, and the number of the affected sensor nodes in the sensor network increases with the increasing of the number of compromised nodes, when the scale of the sensor network is fixed. 60
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Fraction of Compromised Bivariate Polynomials SensorNodeswithatleastoneCompromisedPolynomialShare H(9,3,2,34,2) H(16,4,2,25,2) H(225,15,2,7,2) H(1296,36,2,3,2) H(2,14) Figure.7 the comparison between pc and the number of sensor nodes with at least one compromised polynomial share in sensor networks based on H(9,3,2,2,n) and H(2,p) distribution models.
A new hierarchical hypercube model named H(k,u,m,v,n) is proposed, which can be used for pairwise key predistribution for cluster deployed sensor networks. And Based on the H(k,u,m,v,n) model, an innovative pairwise key predistribution scheme and algorithm are designed respectively, by combing the good properties of the Polynomial Key and Key Pool encryption schemes.
The new algorithm uses the good characteristics of node codes and high fault-tolerance of H(k,u,m,v,n) model to route and predistribute pairwise keys, in which nodes are not needed to be able to communicate with each other directly such as that the algorithms proposed by [7] shall need. So, the traditional pairwise key predistribution algorithm based on hypercube model [7] is only a special case of the new algorithm proposed in this paper.
Theoretical and experimental analyses show that the newly proposed algorithm is an efficient pairwise key establishment algorithm that is suitable for the cluster deployed sensor networks.
Our thanks to ACM SIGCHI for allowing us to modify templates they had developed, and to nature science fund of Fujian province of PR.China under grant No.A0510024.
[1] L. Eschenauer and V. Gligor. A key-management scheme for distribute sensor networks. In Proceedings of the 9th ACM Conference on Computer and Communication Security.
ACM Press, Washington DC, USA, 2002, 41-47. [2] H. Chan, A. Perrig, and D. Song. Random key predistribution schemes for sensor networks. In IEEE Symposium on Security and Privacy. IEEE Computer Society, California, USA, 2003, 197-213. [3] C. Blundo, A. D. Santis, A. Herzberg, S. Kutten, U. Vaccaro, and M. Yung. Perfectly-secure key distribution for dynamic conferences. Lecture Notes in Computer Science. 1993, 740, 471-486. [4] D. Liu and P. Ning. Establishing pairwise keys in distributed sensor networks. In Proceedings of the 10th ACM Conference on Computer and Communications Security.
ACM Press, Washingtion, DC, USA, 2003, 52-61. [5] W. Du, J. Deng, Y. Han, and P. Varshney. A pairwise key pre-distribution scheme for wireless sensor networks. In Proceedings of the Tenth ACM Conference on Computer and Communications Security. Washingtion, DC, USA,2003,
[6] R. Blom. An optimal class of symmetric key generation systems. Advances in Cryptology: Proceedings of EUROCRYPT 84. Lecture Notes in Computer Science. 1985, 209, :335-338. [7] Donggang Liu, Peng Ning, Rongfang Li, Establishing Pairwise Keys in Distributed Sensor Networks. ACM Journal Name, 2004, 20, 1-35. [8] L. Fang, W. Du, and N. Peng. A Beacon-Less Location Discovery Scheme for Wireless Sensor Networks,
INFOCOM 2005. [9] Wang Lei, Lin Ya-ping, Maximum safety path matrix based fault-tolerant routing algorithm for hypercube interconnection network. Journal of software. 2004,15(7), 994-1004. [10] Wang Lei, Lin Ya-ping, Maximum safety path vector based fault-tolerant routing algorithm for hypercube interconnection network. Journal of China Institute of Communications. 2004, 16(4), 130-137. [11] Lin Ya-ping, Wang Lei, Location information based hierarchical data congregation routing algorithm for sensor networks. Chinese Journal of electronics. 2004, 32(11), 1801-1805. [12] W. Heinzelman, J. Kulik, and H. Balakrishnan, Negotiation Based Protocols for Disseminating Information in Wireless Sensor Networks. ACM Wireless Networks. 2002, 8,

Publish/subscribe is well suited as a communication mechanism for building Internet-scale distributed event-driven applications. Much of its capacity for scale in the number of participants comes from its decoupling of publishers and subscribers by placing an asynchronous event delivery service between them. In truly Internet-scale publish/subscribe systems, the event delivery service will include a large set of interconnected broker nodes spanning a wide geographic (and thus network) area.
However, publish/subscribe systems that do span a wide geographic area are likely to also span multiple administrative domains, be they independent administrative domains inside a single organisation, multiple independent organisations, or a combination of the two.
While the communication capabilities of publish/subscribe systems are well proved, spanning multiple administrative domains is likely to require addressing security considerations. As security and access control are almost the antithesis of decoupling, relatively little publish/subscribe research has focused on security so far.
Our overall research aim is to develop Internet-scale publish/subscribe networks that provide secure, efficient delivery of events, fault-tolerance and self-healing in the delivery infrastructure, and a convenient event interface.
In [12] Pesonen et al. propose a multi-domain, capabilitybased access control architecture for publish/subscribe systems. The architecture provides a mechanism for authorising event clients to publish and subscribe to event types.
The privileges of the client are checked by the local broker that the client connects to in order to access the publish/ subscribe system. The approach implements access control at the edge of the broker network and assumes that all brokers can be trusted to enforce the access control policies correctly. Any malicious, compromised or unauthorised broker is free to read and write any events that pass through it on their way from the publishers to the subscribers. This might be acceptable in a relatively small system deployed inside a single organisation, but it is not appropriate in a multi-domain environment in which organisations share a common infrastructure.
We propose enforcing access control within the broker network by encrypting event content, and that policy dictate controls over the necessary encryption keys. With encrypted event content only those brokers that are authorised to ac104 cess the encryption keys are able to access the event content (i.e. publish, subscribe to, or filter). We effectively move the enforcement of access control from the brokers to the encryption key managers.
We expect that access control would need to be enforced in a multi-domain publish/subscribe system when multiple organisations form a shared publish/subscribe system yet run multiple independent applications. Access control might also be needed when a single organisation consists of multiple sub-domains that deliver confidential data over the organisation-wide publish/subscribe system. Both cases require access control because event delivery in a dynamic publish/subscribe infrastructure based on a shared broker network may well lead to events being routed through unauthorised domains along their paths from publishers to subscribers.
There are two particular benefits to sharing the publish/ subscribe infrastructure, both of which relate to the broker network. First, sharing brokers will create a physically larger network that will provide greater geographic reach.
Second, increasing the inter-connectivity of brokers will allow the publish/subscribe system to provide higher faulttolerance.
Figure 1 shows the multi-domain publish/subscribe network we use as an example throughout this paper. It is based on the United Kingdom Police Forces, and we show three particular sub-domains: Metropolitan Police Domain. This domain contains a set of CCTV cameras that publish information about the movements of vehicles around the London area.
We have included Detective Smith as a subscriber in this domain.
Congestion Charge Service Domain. The charges that are levied on the vehicles that have passed through the London Congestion Charge zone each day are issued by systems within this domain. The source numberplate recognition data comes from the cameras in the Metropolitan Police Domain. The fact that the CCS are only authorised to read a subset of the vehicle event data will exercise some of the key features of the enforceable publish/subscribe system access control presented in this paper.
PITO Domain. The Police Information Technology Organisation (PITO) is the centre from which Police data standards are managed. It is the event type owner in this particular scenario.
Encryption protects the confidentiality of events should they be transported through unauthorised domains. However encrypting whole events means unauthorised brokers cannot make efficient routing decisions.
Our approach is to apply encryption to the individual attributes of events. This way our multi-domain access control policy works at a finer granularity - publishers and subscribers may be authorised access to a subset of the available attributes. In cases where non-encrypted events are used for routing, we can reduce the total number of events sent through the system without revealing the values of sensitive attributes.
In our example scenario, the Congestion Charge Service would only be authorised to read the numberplate field of vehicle sightings - the location attribute would not be decrypted. We thus preserve the privacy of motorists while still allowing the CCS to do its job using the shared publish/subscribe infrastructure.
Let us assume that a Metropolitan Police Service detective is investigating a crime and she is interested in sightings of a specific vehicle. The detective gets a court order that authorises her to subscribe to numberplate events of the specific numberplate related to her case.
Current publish/subscribe access control systems enforce security at the edge of the broker network where clients connect to it. However this approach will often not be acceptable in Internet-scale systems. We propose enforcing security within the broker network as well as at the edges that event clients connect to, by encrypting event content.
Publications will be encrypted with their event type specific encryption keys. By controlling access to the encryption keys, we can control access to the event types. The proposed approach allows event brokers to route events even when they have access only to a subset of the potential encryption keys.
We introduce decentralised publish/subscribe systems and relevant cryptography in Section 2. In Section 3 we present our model for encrypting event content on both the event and the attribute level. Section 4 discusses managing encryption keys in multi-domain publish/subscribe systems.
We analytically evaluate the performance of our proposal in Section 5. Finally Section 6 discusses related work in securing publish/subscribe systems and Section 7 provides concluding remarks.
In this section we provide a brief introduction to decentralised publish/subscribe systems. We indicate our assumptions about multi-domain publish/subscribe systems, and describe how these assumptions influence the developments we have made from our previously published work.
A publish/subscribe system includes publishers, subscribers, and an event service. Publishers publish events, subscribers subscribe to events of interest to them, and the event service is responsible for delivering published events to all subscribers whose interests match the given event.
The event service in a decentralised publish/subscribe system is distributed over a number of broker nodes. Together these brokers form a network that is responsible for maintaining the necessary routing paths from publishers to subscribers. Clients (publishers and subscribers) connect to a local broker, which is fully trusted by the client. In our discussion we refer to the client hosting brokers as publisher hosting brokers (PHB) or subscriber hosting brokers (SHB) depending on whether the connected client is a publisher or 105 IB SHB Sub Pub Pub Sub Sub IB PHB IB IB PHB IB IB IB IB SHB SHB IBIB IB IB IB IB IB IBIB IB TO IB IB IB Metropolitan Police Domain Congestion Charge Service Domain PITO Domain Detective Smith Camera 1 Camera 2 Billing Office Statistics Office Sub Subscriber SHB Subscriber Hosting Broker Pub Publisher PHB Publisher Hosting Broker TO Type Owner IB Intermediate Broker KEY Figure 1: An overall view of our multi-domain publish/subscribe deployment a subscriber, respectively. A local broker is usually either part of the same domain as the client, or it is owned by a service provider trusted by the client.
A broker network can have a static topology (e.g. Siena [3] and Gryphon [14]) or a dynamic topology (e.g. Scribe [4] and Hermes [13]). Our proposed approach will work in both cases. A static topology enables the system administrator to build trusted domains and in that way improve the efficiency of routing by avoiding unnecessary encryptions (see Sect. 3.4), which is very difficult with a dynamic topology.
On the other hand, a dynamic topology allows the broker network to dynamically re-balance itself when brokers join or leave the network either in a controlled fashion or as a result of a network or node failure.
Our work is based on the Hermes system. Hermes is a content-based publish/subscribe middleware that includes strong event type support. In other words, each publication is an instance of a particular predefined event type.
Publications are type checked at the local broker of each publisher.
Our attribute level encryption scheme assumes that events are typed. Hermes uses a structured overlay network as a transport and therefore has a dynamic topology.
A Hermes publication consists of an event type identifier and a set of attribute value pairs. The type identifier is the SHA-1 hash of the name of the event type. It is used to route the publication through the event broker network. It conveniently hides the type of the publication, i.e. brokers are prevented from seeing which events are flowing through them unless they are aware of the specific event type name and identifier.
Pesonen et al. introduced secure event types in [11], which can have their integrity and authenticity confirmed by checking their digital signatures. A useful side effect of secure event types are their globally unique event type and attribute names. These names can be referred to by access control policies. In this paper we use the secure name of the event type or attribute to refer to the encryption key used to encrypt the event or attribute.
Pesonen et al. proposed a capability-based access control architecture for multi-domain publish/subscribe systems in [12]. The model treats event types as resources that publishers, subscribers, and event brokers want to access. The event type owner is responsible for managing access control for an event type by issuing Simple Public Key Infrastructure (SPKI) authorisation certificates that grant the holder access to the specified event type. For example, authorised publishers will have been issued an authorisation certificate that specifies that the publisher, identified by public key, is authorised to publish instances of the event type specified in the certificate.
We leverage the above mentioned access control mechanism in this paper by controlling access to encryption keys using the same authorisation certificates. That is, a publisher who is authorised to publish a given event type, is also authorised 106 to access the encryption keys used to protect events of that type. We discuss this in more detail in Sect. 4.
The goal of the proposed mechanism is to enforce access control for authorised participants in the system. In our case the first level of access control is applied when the participant tries to join the publish/subscribe network.
Unauthorised event brokers are not allowed to join the broker network. Similarly unauthorised event clients are not allowed to connect to an event broker. All the connections in the broker network between event brokers and event clients utilise Transport Layer Security (TLS) [5] in order to prevent unauthorised access on the transport layer.
The architecture of the publish/subscribe system means that event clients must connect to event brokers in order to be able to access the publish/subscribe system. Thus we assume that these clients are not a threat. The event client relies completely on the local event broker for access to the broker network. Therefore the event client is unable to access any events without the assistance of the local broker.
The brokers on the other hand are able to analyse all events in the system that pass through them. A broker can analyse both the event traffic as well as the number and names of attributes that are populated in an event (in the case of attribute level encryption). There are viable approaches to preventing traffic analysis by inserting random events into the event stream in order to produce a uniform traffic pattern. Similarly attribute content can be padded to a standard length in order to avoid leaking information to the adversary.
While traffic analysis is an important concern we have not addressed it further in this paper.
We propose enforcing access control in a decentralised broker network by encrypting the contents of published events and controlling access to the encryption keys. Effectively we move the responsibility for access control from the broker network to the key managers.
It is assumed that all clients have access to a broker that they can trust and that is authorised to access the event content required by the client. This allows us to implement the event content encryption within the broker network without involving the clients. By delegating the encryption tasks to the brokers, we lower the number of nodes required to have access to a given encryption key1 . The benefits are three-fold: i) fewer nodes handle the confidential encryption key so there is a smaller chance of the key being disclosed; ii) key refreshes involve fewer nodes which means that the key management algorithm will incur smaller communication and processing overheads to the publish/subscribe system; and iii) the local broker will decrypt an event once and deliver it to all subscribers, instead of each subscriber 1 The encryption keys are changed over time in response to brokers joining or leaving the network, and periodically to reduce the amount of time any single key is used. This is discussed in Sect. 4.2 having to decrypt the same event. Delegating encryption tasks to the local broker is appropriate, because encryption is a middleware feature used to enforce access control within the middleware system. If applications need to handle encrypted data in the application layer, they are free to publish encrypted data over the publish/subscribe system.
We can implement encryption either at the event level or the attribute level. Event encryption is simpler, requires fewer keys, fewer independent cryptographic operations, and thus is usually faster. Attribute encryption enables access control at the attribute level, which means that we have a more expressive and powerful access control mechanism, while usually incurring a larger performance penalty.
In this section we discuss encrypting event content both at the event level and the attribute level; avoiding leaking information to unauthorised brokers by encrypting subscription filters; avoiding unnecessary encryptions between authorised brokers; and finally, how event content encryption was implemented in our prototype. Note that since no publish/subscribe client is ever given access to encryption keys, any encryption performed by the brokers is necessarily completely transparent to all clients.
In event encryption all the event attributes are encrypted as a single block of plaintext. The event type identifier is left intact (i.e. in plaintext) in order to facilitate event routing in the broker network.
The globally unique event type identifier specifies the encryption key used to encrypt the event content. Each event type in the system will have its own individual encryption key. Keys are refreshed, as discussed in Sect. 4.2.
While in transit the event will consist of a tuple containing the type identifier, a publication timestamp, ciphertext, and a message authentication tag: <type id, timestamp, cipher text, authentication tag>.
Event brokers that are authorised to access the event, and thus have access to the encryption key, can decrypt the event and implement content-based routing. Event brokers that do not have access to the encryption key will be forced to route the event based only on its type. That is, they will not be able to make intelligent decisions about whether events need not be transmitted down their outgoing links.
Event encryption results in one encryption at the publisher hosting broker, and one decryption at each filtering intermediate broker and subscriber hosting broker that the event passes through, regardless of the number of attributes. This results in a significant performance advantage compared to attribute encryption.
In attribute encryption each attribute value in an event is encrypted separately with its own encryption key. The encryption key is identified by the attribute"s globally unique identifier (the globally unique event identifier defines a namespace inside which the attribute identifier is a fully qualified name). 107 The event type identifier is left intact to facilitate event routing for unauthorised brokers. The attribute identifiers are also left intact to allow authorised brokers to decrypt the attribute values with the correct keys. Brokers that are authorised to access some of the attributes in an event, can implement content-based routing over the attributes that are accessible to them.
An attribute encrypted event in transit consists of the event type identifier, a publication timestamp, and a set of attribute tuples: <type id, timestamp, attributes >.
Attribute tuples consist of an attribute identifier, ciphertext, and a message authentication tag: <attr id, ciphertext, authentication tag>. The attribute identifier is the SHA-1 hash of the attribute name used in the event type definition.
Using the attribute identifier in the published event instead of the attribute name prevents unauthorised parties from learning which attributes are included in the publication.
Compared with event encryption, attribute encryption usually results in larger processing overheads, because each attribute is encrypted separately. In the encryption process the initialisation of the encryption algorithm takes a significant portion of the total running time of the algorithm.
Once the algorithm is initialised, increasing the amount of data to be encrypted does not affect the running time very much. This disparity is emphasised in attribute encryption, where an encryption algorithm must be initialised for each attribute separately, and the amount of data encrypted is relatively small. As a result attribute encryption incurs larger processing overheads when compared with event encryption which can be clearly seen from the performance results in Sect. 5.
The advantage of attribute encryption is that the type owner is able to control access to the event type at the attribute level. The event type owner can therefore allow clients to have different levels of access to the same event type. Also, attribute level encryption enables content-based routing in cases where an intermediate broker has access only to some of the attributes of the event, thus reducing the overall impact of event delivery on the broker network. Therefore the choice between event and attribute encryption is a trade-off between expressiveness and performance, and depends on the requirements of the distributed application.
The expressiveness provided by attribute encryption can be emulated by introducing a new event type for each group of subscribers with the same authorisation. The publisher would then publish an instance of each of these types instead of publishing just a combined event. For example, in our London police network, the congestion control cameras would have to publish one event for the CCS and another for the detective. This approach could become difficult to manage if the attributes have a variety of security properties, since a large number of event types would be required and policies and subscriptions may change dynamically. This approach creates a large number of extra events that must be routed through the network, as is shown in Sect. 5.3.
In order to fully protect the confidentiality of event content we must also encrypt subscriptions. Encrypted subscriptions guarantee: i) that only authorised brokers are able to submit subscriptions to the broker network, and ii) that unauthorised brokers do not gain information about event content by monitoring which subscriptions a given event matches. For example, in the first case an unauthorised broker can create subscriptions with appropriately chosen filters, route them towards the root of the event dissemination tree, and monitor which events were delivered to it as matching the subscription. The fact that the event matched the subscription would leak information to the broker about the event content even if the event was still encrypted. In the second case, even if an unauthorised broker was unable to create subscriptions itself, it could still look at subscriptions that were routed through it, take note of the filters on those subscriptions, and monitor which events are delivered to it by upstream brokers as matching the subscription filters. This would again reveal information about the event content to the unauthorised broker.
In the case of encrypting complete events, we also encrypt the complete subscription filter. The event type identifier in the subscription must be left intact to allow brokers to route events based on their topic when they are not authorised to access the filter. In such cases the unauthorised broker is required to assume that events of such a type match all filter expressions.
Each attribute filter is encrypted individually, much as when encrypting a publication. In addition to the event type identifier the attribute identifiers are also left intact to allow authorised brokers to decrypt those filters that they have access to, and route the event based on its matching the decrypted filters.
Operations Encrypting the event content is not necessary if the current broker and the next broker down the event dissemination tree have the same credentials with respect to the event type at hand. For example, one can assume that all brokers inside an organisation would share the same credentials and therefore, as long as the next broker is a member of the same domain, the event can be routed to it in plaintext.
With attribute encryption it is possible that the neighbouring broker is authorised to access a subset of the decrypted attributes, in which case those attributes that the broker is not authorised to access would be passed to it encrypted.
In order to know when it is safe to pass the event in plaintext form, the brokers exchange credentials as part of a handshake when they connect to each other. In cases when the brokers are able to verify each others" credentials, they will add them to the routing table for future reference. If a broker acquires new credentials after the initial handshake, it will present these new credentials to its neighbours while in session.
Regardless of its neighbouring brokers, the PHB will always encrypt the event content, because it is cheaper to encrypt the event once at the root of the event dissemination tree.
In Hermes the rendezvous node for each event type is selected uniformly randomly (the event type name is hashed with the SHA-1 hash algorithm to produce the event type 108 PHB IBIB IB SHB RN IB SHB Figure 2: Node addressing is evenly distributed across the network, thus rendezvous nodes may lie outside the domain that owns an event type IB IB SHBPHBP S Encrypts Filters from cache Decrypts, delivers Decrypts, filters Plaintext Cached Plaintext (most data) Cached Plaintext (some data) Different domains Cyphertext KEY Figure 3: Caching decrypted data to increase efficiency when delivering to peers with equivalent security privileges identifier, then the identifier is used to select the rendezvous node in the structured overlay network). Therefore it is probable that the rendezvous node will reside outside the current domain. This situation is illustrated in the event dissemination tree in Fig. 2. So even with domain internal applications, where the event can be routed from the publisher to all subscribers in plaintext form, the event content will in most cases have to be encrypted for it to be routed to the rendezvous node.
To avoid unnecessary decryptions, we attach a plaintext content cache to encrypted events. A broker fills the cache with content that it has decrypted, for example, in order to filter on the content. The cache is accessed by the broker when it delivers an event to a local subscriber after first seeing if the event matches the subscription filter, but the broker also sends the cache to the next broker with the encrypted event.
The next broker can look the attribute up from the cache instead of having to decrypt it. If the event is being sent to an unauthorised broker, the cache will be discarded before the event is sent. Obviously sending the cache with the encrypted event will add to the communication cost, but this is outweighed by the saving in encryption/decryption processing. In Fig. 3 we see two separate cached plaintext streams accompanying an event depending on the inter-broker relationships in two different domains.
We show in Sect. 5.2 that the overhead of sending encrypted messages with a full plaintext cache incurs almost no overhead compared to sending plaintext messages.
In our implementation we have used the EAX mode [2] of operation when encrypting events, attributes, and subscription filters. EAX is a mode of operation for block ciphers, also called an Authenticated Encryption with Associated Data (AEAD) algorithm that provides simultaneously both data confidentiality and integrity protection. The algorithm implements a two-pass scheme where during the first pass the plain text is encrypted, and on the second pass a message authentication code (MAC) is generated for the encrypted data.
The EAX mode is compatible with any block cipher. We decided to use the Advanced Encryption Standard (AES) [9] algorithm in our implementation, because of its standard status and the fact that the algorithm has gone through thorough cryptanalysis during its existence and no serious vulnerabilities have been found thus far.
In addition to providing both confidentiality and integrity protection, the EAX mode uses the underlying block cipher in counter mode (CTR mode) [21]. A block cipher in counter mode is used to produce a stream of key bits that are then XORed with the plaintext. Effectively CTR mode transforms a block cipher into a stream cipher. The advantage of stream ciphers is that the ciphertext is the same length as the plaintext, whereas with block ciphers the plaintext must be padded to a multiple of the block cipher"s block length (e.g. the AES block size is 128 bits). Avoiding padding is very important in attribute encryption, because the padding might increase the size of the attribute disproportionally.
For example, a single integer might be 32 bits in length, which would be padded to 128 bits if we used a block cipher. With event encryption the message expansion is not that relevant, since the length of padding required to reach the next 16 byte multiple will probably be a small proportion of the overall plaintext length.
In encryption mode the EAX algorithm takes as input a nonce (a number used once), an encryption key and the plaintext, and it returns the ciphertext and an authentication tag. In decryption mode the algorithm takes as input the encryption key, the ciphertext and the authentication tag, and it returns either the plaintext, or an error if the authentication check failed.
The nonce is expanded to the block length of the underlying block cipher by passing it through an OMAC construct (see [7]). It is important that particular nonce values are not reused, otherwise the block cipher in CTR mode would produce an identical key stream. In our implementation we used the PHB defined event timestamp (64-bit value counting the milliseconds since January 1, 1970 UTC) appended by the PHB"s identity (i.e. public key) as the nonce. The broker is responsible for ensuring that the timestamps increase monotonically.
The authentication tag is appended to the produced cipher text to create a two-tuple. With event encryption a single tag is created for the encrypted event. With attribute 109 encryption each attribute is encrypted and authenticated separately, and they all have their individual tags. The tag length is configurable in EAX without restrictions, which allows the user to make a trade-off between the authenticity guarantees provided by EAX and the added communication overhead. We used a tag length of 16 bytes in our implementation, but one could make the tag length a publisher/subscriber defined parameter for each publication/subscription or include it in the event type definition to make it a type specific parameter.
EAX also supports including unencrypted associated data in the tag calculation. The integrity of this data is protected, but it is still readable by everyone. This feature could be used with event encryption in cases where some of the event content is public and thus would be useful for content-based routing. The integrity of the data would still be protected against changes, but unauthorised brokers would be able to apply filters. We have included the event type identifier as associated data in order to protect its integrity.
Other AEAD algorithms include the offset codebook mode (OCB) [17] and the counter with CBC-MAC mode (CCM) [22]. Contrarily to the EAX mode the OCB mode requires only one pass over the plaintext, which makes it roughly twice as fast as EAX. Unfortunately the OCB mode has a patent application in place in the USA, which restricts its use. The CCM mode is the predecessor of the EAX mode.
It was developed in order to provide a free alternative to OCB. The EAX was developed later to address some issues with CCM [18]. Similarly to EAX, CCM is also a two-pass mode.
In both encryption approaches the encrypted event content has a globally unique identifier (i.e. the event type or the attribute identifier). That identifier is used to determine the encryption key to use when encrypting or decrypting the content. Each event type, in event encryption, and attribute, in attribute encryption, has its own individual encryption key. By controlling access to the encryption key we effectively control access to the encrypted event content.
In order to control access to the encryption keys we form a key group of brokers for each individual encryption key. The key group is used to refresh the key when necessary and to deliver the new key to all current members of the key group.
The key group manager is responsible for verifying that a new member requesting to join the key group is authorised to do so. Therefore the key group manager must be trusted by the type owner to enforce the access control policy. We assume that the key group manager is either a trusted third party or alternatively a member of the type owner"s domain.
In [12] Pesonen et al. proposed a capability-based access control architecture for multi-domain publish/subscribe systems. The approach uses capabilities to decentralise the access control policy amongst the publish/subscribe nodes (i.e. clients and brokers): each node holds a set of capabilities that define the authority granted to that node.
Authority to access a given event type is granted by the owner of that type issuing a capability to a node. The capability defines the event type, the action, and the attributes that Type Owner ACS Broker Key Manager
for Number Platekey
join Number Plate key group
all checks,they will begin receiving appropriate keys.
broker"s credentials at the Access Control Service
check that the Type Owner permits access Figure 4: The steps involved for a broker to be successful in joining a key group the node is authorised to access. For example, a tuple <NP, subscribe, *> would authorise the owner to subscribe to Numberplate events with access to all attributes in the published events. The sequence of events required for a broker to successfully join a key group is shown in Fig. 4.
Both the client hosting broker and the client must be authorised to make the client"s request. That is, if the client makes a subscription request for Numberplate events, both the client and the local broker must be authorised to subscribe to Numberplate events. This is because from the perspective of the broker network, the local broker acts as a proxy for the client.
We use the same capabilities to authorise membership in a key group that are used to authorise publish/subscribe requests. Not doing so could lead to the inconsistent situation where a SHB is authorised to make a subscription on behalf of its clients, but is not able to decrypt incoming event content for them. In the Numberplate example above, the local broker holding the above capability is authorised to join the Numberplate key group as well as the key groups for all the attributes in the Numberplate event type.
Event content encryption in a decentralised multi-domain publish/subscribe system can be seen as a sub-category of secure group communication. In both cases the key management system must scale well with the number of clients, clients might be spread over large geographic areas, there might be high rates of churn in group membership, and all members must be synchronised with each other in time in order to use the same encryption key at the same time.
There are a number of scalable key management protocols for secure group communication [15]. We have implemented the One-Way Function Tree (OFT) [8] protocol as a proof of concept. We chose to implement OFT, because of its relatively simplicity and good performance. Our implementation uses the same structured overlay network used by the broker network as a transport. The OFT protocol is based on a binary tree where the participants are at the leaves of the tree. It scales in log2n in processing and communication costs, as well as in the size of the state stored at each participant, which we have verified in our simulations.
Traditionally in group key management schemes the encryption key is refreshed when a new member joins the group, an 110 existing member leaves the group, or a timer expires.
Refreshing the key when a new member joins provides backward secrecy, i.e. the new member is prevented from accessing old messages. Similarly refreshing the key when an existing member leaves provides forward secrecy, i.e. the old member is prevented from accessing future messages. Timer triggered refreshes are issued periodically in order to limit the damage caused by the current key being compromised.
Even though the state-of-the-art key management protocols are efficient, refreshing the key unnecessarily introduces extra traffic and processing amongst the key group members.
In our case key group membership is based on the broker holding a capability that authorises it to join the key group.
The capability has a set of validity conditions that in their simplest form define a time period when the certificate is valid, and in more complex cases involve on-line checks back towards the issuer. In order to avoid unnecessary key refreshes the key manager looks at the certificate validity conditions of the joining or leaving member. In case of a joining member, if the manager can ascertain that the certificate was valid at the time of the previous key refresh, a new key refresh can be avoided. Similarly, instead of refreshing the key immediately when a member leaves the key group, the key manager can cache their credentials and refresh the key only when the credentials expire. These situations are both illustrated in Fig.5. It can be assumed that the credentials granted to brokers are relatively static, i.e. once a domain is authorised to access an event type, the authority will be delegated to all brokers of that domain, and they will have the authority for the foreseeable future. More fine grained and dynamic access control would be implemented at the edge of the broker network between the clients and the client hosting brokers.
When an encryption key is refreshed the new key is tagged with a timestamp. The encryption key to use for a given event is selected based on the event"s publication timestamp.
The old keys will be kept for a reasonable amount of time in order to allow for some clock drift. Setting this value is part of the key management protocol, although exactly how long this time should be will depend on the nature of the application and possibly the size of the network. It can be configured independently per key group if necessary.
In order to evaluate the performance of event content encryption we have implemented both encryption approaches running over our implementation of the Hermes publish/ subscribe middleware. The implementation supports three modes: plaintext content, event encryption, and attribute encryption, in a single publish/subscribe system.
We ran three performance tests in a discrete event simulator.
The simulator was run on an Intel P4 3.2GHz workstation with 1GB of main memory. We decided to run the tests on an event simulator instead of an actual deployed system in order to be able to measure to aggregate time it takes to handle all messages in the system.
The following sections describe the specific test setups and the results in more detail.
The end-to-end overhead test shows how much the overall message throughput of the simulator was affected by event content encryption. We formed a broker network with two brokers, attached a publisher to one of them and a subscriber to the other one. The subscriber subscribed to the advertised event type without any filters, i.e. each publication matched the subscriber"s publication and thus was delivered to the subscriber. The test measures the combined time it takes to publish and deliver 100,000 events. If the content is encrypted this includes both encrypting the content at the PHB and decrypting it at the SHB.
In the test the number of attributes in the event type is increased from 1 to 25 (the x-axis). Each attribute is set to a 30 character string. For each number of attributes in the event type the publisher publishes 100,000 events, and the elapsed time is measured to derive the message throughput. The test was repeated five times for each number of attributes and we use the average of all iterations in the graph, but the results were highly consistent so the standard deviation is not shown. The same tests were run with no content encryption, event encryption, and attribute encryption.
As can be seen in Fig. 6, event content encryption introduces a large overhead compared to not using encryption. The throughput when using attribute encryption with an event type with one attribute is 46% of the throughput achieved when events are sent in plaintext. When the number of attributes increases the performance gap increases as well: with ten attributes the performance with attribute encryption has decreased to 11.7% of plaintext performance.
Event encryption fares better, because of fewer encryption operations. The increase in the amount of encrypted data does not affect the performance as much as the number of individual encryption operations does. The difference in performance with event encryption and attribute encryption with only one attribute is caused by the Java object serialisation mechanism: in the event encryption case the whole attribute structure is serialised, which results in more objects than serialising a single attribute value. A more efficient implementation would provide its own marshalling mechanism.
Note that the EAX implementation we use runs the nonce (i.e. initialisation vector) through an OMAC construct to increase its randomness. Since the nonce is not required to be kept secret (just unique), there is a potential time/space trade-off we have not yet investigated in attaching extra nonce attributes that have already had this OMAC construct applied to them.
We explained in Sect. 3.4 that event content decryption and encryption can be avoided if both brokers are authorised to access the event content. This test was designed to show that the use of the encrypted event content mechanism between two authorised brokers incurs only a small performance overhead.
In this test we again form a broker network with two brokers. 111 Key refresh schedule Broker 1 joining and leaving the key group Broker 2 joining and leaving the key group Actual key refresh times Time One day Broker"s key group credentials are valid Actual join time Actual leave time One day One day Figure 5: How the key refresh schedule is affected by brokers joining and leaving key groups 0 5000 10000 15000 20000 25000 30000 35000 0 5 10 15 20 25 MessagesperSecond Number of Attributes No Encryption Attribute Encryption Whole-content Encryption Figure 6: Throughput of Events in a Simulator Both brokers are configured with the same credentials. The publisher is attached to one of the brokers and the subscriber to the other, and again the subscriber does not specify any filters in its subscription.
The publisher publishes 100,000 events and the test measures the elapsed time in order to derive the system"s message throughput. The event content is encrypted outside the timing measurement, i.e. the encryption cost is not included in the measurements. The goal is to model an environment where a broker has received a message from another authorised broker, and it routes the event to a third authorised broker. In this scenario the middle broker does not need to decrypt nor encrypt the event content.
As shown in Fig. 2, the elapsed time was measured as the number of attributes in the published event was increased from 1 to 25. The attribute values in each case are 30 character strings. Each test is repeated five times, and we use the average of all iterations in the graph. The same test was then repeated with no encryption, event encryption and attribute encryption turned on.
The encrypted modes follow each other very closely.
Predictably, the plaintext mode performs a little better for all attribute counts. The difference can be explained partially by the encrypted events being larger in size, because they include both the plaintext and the encrypted content in this test. The difference in performance is 3.7% with one attribute and 2.5% with 25 attributes.
We believe that the roughness of the graphs can be explained by the Java garbage collector interfering with the simulation.
The fact that all three graphs show the same irregularities supports this theory. 112 50000 55000 60000 65000 70000 75000 80000 85000 90000 95000 100000 0 5 10 15 20 25 MessagesperSecond Number of Attributes No Encryption Attribute Encryption Whole-content Encryption Figure 7: Throughput of Domain Internal Events
Through the definition of multiple event types, it is possible to emulate the expressiveness of attribute encryption using only event content encryption. The last test we ran was to show the communication overhead caused by this emulation technique, compared to using real attribute encryption.
In the test we form a broker network of 2000 brokers. We attach one publisher to one of the brokers, and an increasing number of subscribers to the remaining brokers. Each subscriber simulates a group of subscribers that all have the same access rights to the published event. Each subscriber group has its own event type in the test.
The outcome of this test is shown in Fig. 8. The number of subscriber groups is increased from 1 to 50 (the x-axis).
For each n subscriber groups the publisher publishes one event to represent the use of attribute encryption and n events representing the events for each subscriber group. We count the number of hops each publication makes through the broker network (y-axis).
Note that Fig. 8 shows workloads beyond what we would expect in common usage, in which many event types are likely to contain fewer than ten attributes. The subscriber groups used in this test represent disjoint permission sets over such event attributes. The number of these sets can be determined from the particular access control policy in use, but will be a value less than or equal to the factorial of the number of attributes in a given event type.
The graphs indicate that attribute encryption performs better than event encryption even for small numbers of subscriber groups. Indeed, with only two subscriber groups (e.g. the case with Numberplate events) the hop count increases from 7.2 hops for attribute encryption to 16.6 hops for event encryption. With 10 subscriber groups the corresponding numbers are 24.2 and 251.0, i.e. an order of magnitude difference.
Wang et al. have categorised the various security issues that need to be addressed in publish/subscribe systems in the future in [20]. The paper is a comprehensive overview of security issues in publish/subscribe systems and as such tries to draw attention to the issues rather than providing solutions.
Bacon et al. in [1] examine the use of role-based access control in multi-domain, distributed publish/subscribe systems.
Their work is complementary to this paper: distributed RBAC is one potential policy formalism that might use the enforcement mechanisms we have presented.
Opyrchal and Prakash address the problem of event confidentiality at the last link between the subscriber and the SHB in [10]. They correctly state that a secure group communication approach is infeasible in an environment like publish/subscribe that has highly dynamic group memberships. As a solution they propose a scheme utilising key caching and subscriber grouping in order to minimise the number of required encryptions when delivering a publication from a SHB to a set of matching subscribers. We assume in our work that the SHB is powerful enough to man113 1 10 100 1000 10000 0 5 10 15 20 25 30 35 40 45 50 NumberofHopsinTotal Number of Subscription Groups Attribute Encryption Whole-content Encryption Figure 8: Hop Counts When Emulating Attribute Encryption age a TLS secured connection for each local subscriber.
Both Srivatsa et al. [19] and Raiciu et al. [16] present mechanisms for protecting the confidentiality of messages in decentralised publish/subscribe infrastructures. Compared to our work both papers aim to provide the means for protecting the integrity and confidentiality of messages whereas the goal for our work is to enforce access control inside the broker network. Raiciu et al. assume in their work that none of the brokers in the network are trusted and therefore all events are encrypted from publisher to subscriber and that all matching is based on encrypted events. In contrast, we assume that some of the brokers on the path of a publication are trusted to access that publication and are therefore able to implement event matching. We also assume that the publisher and subscriber hosting brokers are always trusted to access the publication. The contributions of Srivatsa et al. and Raiciu et al. are complementary to the contributions in this paper.
Finally, Fiege et al. address the related topic of event visibility in [6]. While the work concentrated on using scopes as mechanism for structuring large-scale event-based systems, the notion of event visibility does resonate with access control to some extent.
Event content encryption can be used to enforce an access control policy while events are in transit in the broker network of a multi-domain publish/subscribe system.
Encryption causes an overhead, but i) there may be no alternative when access control is required, and ii) the performance penalty can be lessened with implementation optimisations, such as passing cached plaintext content alongside encrypted content between brokers with identical security credentials.
This is particularly appropriate if broker-to-broker connections are secured by default so that wire-sniffing is not an issue.
Attribute level encryption can be implemented in order to enforce fine-grained access control policies. In addition to providing attribute-level access control, attribute encryption enables partially authorised brokers to implement contentbased routing based on the attributes that are accessible to them.
Our experiments show that i) by caching plaintext and ciphertext content when possible, we are able to deliver comparable performance to plaintext events, and ii) that attribute encryption within an event incurs far less overhead than defining separate event types for the attributes that need different levels of protection.
In environments comprising multiple domains, where eventbrokers have different security credentials, we have quantified how a trade-off can be made between performance and expressiveness.
Acknowledgements We would like to thank the anonymous reviewers for their very helpful comments. Lauri Pesonen is supported by EPSRC (GR/T28164) and the Nokia Foundation. David Eyers is supported by EPSRC (GR/S94919). 114
[1] J. Bacon, D. M. Eyers, K. Moody, and L. I. W.
Pesonen. Securing publish/subscribe for multi-domain systems. In G. Alonso, editor, Middleware, volume 3790 of Lecture Notes in Computer Science, pages 1-20. Springer, 2005. [2] M. Bellare, P. Rogaway, and D. Wagner. Eax: A conventional authenticated-encryption mode.
Cryptology ePrint Archive, Report 2003/069, 2003. http://eprint.iacr.org/. [3] A. Carzaniga, D. S. Rosenblum, and A. L. Wolf.
Design and evaluation of a wide-area event notification service. ACM Transactions on Computer Systems, 19(3):332-383, Aug. 2001. [4] M. Castro, P. Druschel, A. Kermarrec, and A. Rowstron. SCRIBE: A large-scale and decentralized application-level multicast infrastructure. IEEE Journal on Selected Areas in communications (JSAC), 20(8):1489-1499, Oct. 2002. [5] T. Dierks and C. Allen. The TLS protocol, version
[6] L. Fiege, M. Mezini, G. M uhl, and A. P. Buchmann.
Engineering event-based systems with scopes. In ECOOP "02: Proceedings of the 16th European Conference on Object-Oriented Programming, pages 309-333, London, UK, 2002. Springer-Verlag. [7] T. Iwata and I. A. Iurosawa. OMAC: One-key CBC MAC, Jan. 14 2002. [8] D. A. McGrew and A. T. Sherman. Key establishment in large dynamic groups using one-way function trees.
Technical Report 0755, TIS Labs at Network Associates, Inc., Glenwood, MD, May 1998. [9] National Institute of Standards and Technology (NIST). Advanced Encryption Standard (AES).
Federal Information Processing Standards Publication (FIPS PUB) 197, Nov. 2001. [10] L. Opyrchal and A. Prakash. Secure distribution of events in content-based publish subscribe systems. In Proc. of the 10th USENIX Security Symposium.
USENIX, Aug. 2001. [11] L. I. W. Pesonen and J. Bacon. Secure event types in content-based, multi-domain publish/subscribe systems. In SEM "05: Proceedings of the 5th international workshop on Software engineering and middleware, pages 98-105, New York, NY, USA, Sept.
[12] L. I. W. Pesonen, D. M. Eyers, and J. Bacon. A capabilities-based access control architecture for multi-domain publish/subscribe systems. In Proceedings of the Symposium on Applications and the Internet (SAINT 2006), pages 222-228, Phoenix, AZ,
Jan. 2006. IEEE. [13] P. R. Pietzuch and J. M. Bacon. Hermes: A distributed event-based middleware architecture. In Proc. of the 1st International Workshop on Distributed Event-Based Systems (DEBS"02), pages 611-618,
Vienna, Austria, July 2002. IEEE. [14] P. R. Pietzuch and S. Bhola. Congestion control in a reliable scalable message-oriented middleware. In M. Endler and D. Schmidt, editors, Proc. of the 4th Int. Conf. on Middleware (Middleware "03), pages 202-221, Rio de Janeiro, Brazil, June 2003. Springer. [15] S. Rafaeli and D. Hutchison. A survey of key management for secure group communication. ACM Computing Surveys, 35(3):309-329, 2003. [16] C. Raiciu and D. S. Rosenblum. Enabling confidentiality in content-based publish/subscribe infrastructures. In Securecomm "06: Proceedings of the Second IEEE/CreatNet International Conference on Security and Privacy in Communication Networks,
[17] P. Rogaway, M. Bellare, J. Black, and T. Krovetz.
OCB: a block-cipher mode of operation for efficient authenticated encryption. In ACM Conference on Computer and Communications Security, pages 196-205, 2001. [18] P. Rogaway and D. Wagner. A critique of CCM, Feb.
[19] M. Srivatsa and L. Liu. Securing publish-subscribe overlay services with eventguard. In CCS "05: Proceedings of the 12th ACM conference on Computer and communications security, pages 289-298, New York, NY, USA, 2005. ACM Press. [20] C. Wang, A. Carzaniga, D. Evans, and A. L. Wolf.
Security issues and requirements in internet-scale publish-subscribe systems. In Proc. of the 35th Annual Hawaii International Conference on System Sciences (HICSS"02), Big Island, HI, USA, 2002. IEEE. [21] D. Whitfield and M. Hellman. Privacy and authentication: An introduction to cryptography. In Proceedings of the IEEE, volume 67, pages 397-427,

Limited deployment of IP multicast has motivated a new distribution paradigm over the Internet based on overlay networks where a group of participating end-systems (or peers) form an overlay structure and actively participate in distribution of content without any special support from the network (e.g., [7]). Since overlay structures are layered over the best-effort Internet, any approach for constructing overlay should address the following fundamental challenges: (i) Scalability with the number of participating peers, (ii) Robustness to dynamics of peer participation, (iii) Adaptation to variations of network bandwidth, and (iv) Accommodating heterogeneity and asymmetry of bandwidth connectivity among participating peers[19]. Coping with bandwidth variations, heterogeneity and asymmetry are particularly important in design of peer-to-peer overlay for streaming applications because delivered quality to each peer is directly determined by its bandwidth connectivity to (other peer(s) on) the overlay.
This paper presents a simple framework for architecting Peer-to-peer Receiver-driven Overlay, called PRO. PRO can accommodate a spectrum of non-interactive streaming applications ranging from playback to lecture-mode live sessions. The main design philosophy in PRO is that each peer should be allowed to independently and selfishly determine the best way to connect to the overlay in order to maximize its own delivered quality. Toward this end, each peer can connect to the overlay topology at multiple points (i.e., receive content through multiple parent peers).
Therefore, participating peers form an unstructured overlay that can gracefully cope with high churn rate[5]. Furthermore, having multiple parent peers accommodates bandwidth heterogeneity and asymmetry while improves resiliency against dynamics of peer participation.
PRO consists of two key components: (i) Gossip-based Peer Discovery: Each peer periodically exchanges message (i.e., gossips) with other known peers to progressively learn about a subset of participating peers in the overlay that are likely to be good parents. Gossiping provides a scalable and efficient approach to peer discovery in unstructured peer-to-peer networks that can be customized to guide direction of discovery towards peers with desired properties (e.g., peers with shorter distance or higher bandwidth). (ii) Receiver-driven Parent Selection: Given the collected information about other participating peers by gossiping mechanism, each peer (or receiver) gradually improves its own delivered quality by dynamically selecting a proper subset of parent peers that collectively maximize provided bandwidth to the receiver. Since the available bandwidth from different participating peers to a receiver (and possible correlation among them) can be measured only at that receiver, a receiver-driven approach is the natural solution to maximize available bandwidth to heterogeneous peers. Furthermore, the available bandwidth from parent peers serves as an implicit signal for a receiver to detect and react to changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers leads to an efficient overlay that maximizes delivered quality to each peer. PRO incorporates 42 several damping functions to ensure stability of the overlay despite uncoordinated actions by different peers.
PRO is part of a larger architecture that we have developed for peer-to-peer streaming. In our earlier work, we developed a mechanism called PALS [18] that enables a receiver to stream layered structured content from a given set of congestion controlled senders. Thus, PRO and PALS are both receiver-driven but complement each other. More specifically, PRO determines a proper subset of parent peers that collectively maximize delivered bandwidth to each receiver whereas PALS coordinates in-time streaming of different segments of multimedia content from these parents despite unpredictable variations in their available bandwidth.
This division of functionality provides a great deal of flexibility because it decouples overlay construction from delivery mechanism. In this paper, we primarily focus on the overlay construction mechanism, or PRO.
The rest of this paper is organized as follows: In Section 2, we revisit the problem of overlay construction for peerto-peer streaming and identify its two key components and explore their design space. We illustrate the differences between PRO and previous solutions, and justify our design choices. We present our proposed framework in Section 3.
In Sections 4 and 5, the key components of our framework are described in further detail. Finally, Section 6 concludes the paper and presents our future plans.
Constructing a peer-to-peer overlay for streaming applications should not only accommodate global design goals such as scalability and resilience but also satisfy the local design goal of maximizing delivered quality to individual peers 1 .
More specifically, delivered quality of streaming content to each peer should be proportional to its incoming access link bandwidth. Achieving these goals is particularly challenging because participating peers often exhibit heterogeneity and asymmetry in their bandwidth connectivity.
Solutions for constructing peer-to-peer overlays often require two key mechanisms to be implemented at each peer: Peer Discovery (PD) and Parent Selection (PS). The PD mechanism enables each peer to learn about other participating peers in the overlay. Information about other peers are used by the PS mechanism at each peer to determine proper parent peers through which it should connect to the overlay. The collective behavior of PD and PS mechanisms at all participating peers leads to an overlay structure that achieves the above design goals. There has been a wealth of previous research that explored design space of the PD and PS mechanisms as follows: Peer Discovery: In structured peer-to-peer networks, the existing structure enables each peer to find other participating peers in a scalable fashion (e.g., [4]). However, structured peer-to-peer networks may not be robust against high churn rate [5]. In contrast, unstructured peer-to-peer networks can gracefully accommodate high churn rate [5] but require a separate peer discovery mechanism.
Meshfirst approaches (e.g., [7, 6]) that require each peer to know about all other participating peers as well as centralized approaches (e.g., [16]) to peer discovery exhibit limited scalability. NICE [2] leverages a hierarchal structure to achieve 1 It is worth clarifying that our design goal is different from common goals in building application-level multicast trees [7] (i.e., minimizing stretch and stress). scalability but each peer only knows about a group of closeby peers who may not be good parents (i.e., may not provide sufficient bandwidth).
Parent Selection: We examine two key aspects of parent selections: (i) Selection Criteria: There are two main criteria for parent selections: relative delay and available bandwidth between two peers. Relative delay between any two peers can be estimated in a scalable fashion with one of the existing landmark-based solutions such as Global Network Positioning (GNP) [15]. However, estimating available bandwidth between two peers requires end-to-end measurement.
Using available bandwidth as criteria for parent selection does not scale for two reasons: First, to cope with dynamics of bandwidth variations, each peer requires to periodically estimate the available bandwidth from all other peers through measurement (e.g., [6]). Second, the probability of interference among different measurements grows with the number of peers in an overlay (similar to joint experiment in RLM [13]).
Most of the previous solutions adopted the idea of application level multicast and used delay as the main selection criteria. Participating peers cooperatively run a distributed algorithm to organize themselves into a source-rooted tree structure in order to minimize either overall delay across all branches of the tree (e.g., [7]), or delay between source and each receiver peer (e.g., [20]). While these parent selection strategies minimize associated network load, they may not provide sufficient bandwidth to individual peers because delay is often not a good indicator for available bandwidth between two peers [12, 14]. The key issue is that minimizing overall delay (global design goal) and maximizing delivered bandwidth to each peer (local design goal) could easily be in conflict. More specifically, parent peers with longer relative distance may provide higher bandwidth than close-by parents. This suggests that there might exist a tradeoff between maximizing provided bandwidth to each peer and minimizing overall delay across the overlay. (ii) Single vs Multiple Parents: A single tree structure for the overlay (where each peer has a single parent) is inherently unable to accommodate peers with heterogeneous and asymmetric bandwidth. A common approach to accommodating bandwidth heterogeneity is to use layer structured content (either layered or multiple description encodings) and allow each receiver to have multiple parents. This approach could accommodate heterogeneity but it introduces several new challenges. First, parent selection strategy should be determined based on location of a bottleneck.
If the bottleneck is at the (outgoing) access links of parent peers 2 , then a receiver should simply look for more parents.
However, when the bottleneck is else where in the network, a receiver should select parents with a diverse set of paths (i.e., utilize different network paths). In practice, a combination of these cases might simultaneously exist among participating peers [1]. Second, streaming a single content from multiple senders is challenging for two reasons: 1) This requires tight coordination among senders to determine overall delivered quality (e.g., number of layers) and decide which sender is responsible for delivery of each segment. 2) Delivered segments from different senders should arrive before their playout times despite uncorrelated vari2 if bottleneck is at the receiver"s access link, then provided bandwidth to the receiver is already maximized. 43 ations in (congestion controlled) bandwidth from different senders. This also implies that those solutions that build multi-parent overlay structure but do not explicitly ensure in-time delivery of individual segments (e.g., [3, 11]) may not be able to support streaming applications.
One approach to build a multi-parent overlay is to organize participating peers into different trees where each layer of the stream is sent to a separate tree (e.g., [4, 16]). Each peer can maximize its quality by participating in a proper number of trees. This approach raises several issues: 1) the provided bandwidth to peers in each tree is limited by minimum uplink bandwidth among upstream peers on that tree.
In the presence of bandwidth asymmetry, this could easily limit delivered bandwidth on each tree below the required bandwidth for a single layer, 2) it is not feasible to build separate trees that are all optimal for a single selection criteria (e.g., overall delay),. 3) connections across different trees are likely to compete for available bandwidth on a single bottleneck3 . We conclude that a practical solution for peer-topeer streaming applications should incorporate the following design properties: (i) it should use an unstructured, multiparent peer-to-peer overlay, (ii) it should provide a scalable peer discovery mechanism that enables each peer to find its good parents efficiently, (iii) it should detect (and possibly avoid) any shared bottleneck among different connections in the overlay, and (iv) it should deploy congestion controlled connections but ensure in-time arrival of delivered segments to each receiver. In the next section, we explain how PRO incorporates all the above design properties.
Assumptions: We assume that each peer can estimate the relative distance between any two peers using the GNP mechanism [15]. Furthermore, each peer knows the incoming and outgoing bandwidth of its access link. Each peer uses the PALS mechanism to stream content from multiple parent peers. All connections are congestion controlled by senders (e.g., [17]). To accommodate peer bandwidth heterogeneity, we assume that the content has a layered representation. In other words, with proper adjustment, the framework should work with both layered and multipledescription encodings. Participating peers have heterogeneous and asymmetric bandwidth connectivity.
Furthermore, peers may join and leave in an arbitrary fashion.
Overview: In PRO, each peer (or receiver) progressively searches for a subset of parents that collectively maximize delivered bandwidth and minimize overall delay from all parents to the receiver. Such a subset of parents may change over time as some parents join (or leave) the overlay, or available bandwidth from current parents significantly changes.
Note that each peer can be both receiver and parent at the same time 4 . Each receiver periodically exchanges messages (i.e., gossips) with other peers in the overlay to learn about those participating peers that are potentially good parents. Potentially good parents for a receiver are identified based on their relative utility for the receiver. The utility of a parent peer pi for a receiver pj is a function of their relative network distance (delij) and the outgoing access link bandwidth of the parent (outbwi), (i.e., U(pi, pj) 3 These multi-tree approaches often do not use congestion control for each connection. 4 Throughout this paper we use receiver and parent as short form for receiver peer and parent peer. = f(delij, outbwi)). Using parents" access link bandwidth instead of available bandwidth has several advantages: (i) outgoing bandwidth is an upper bound for available bandwidth from a parent. Therefore, it enables the receiver to roughly classify different parents. (ii) estimating available bandwidth requires end-to-end measurement and such a solution does not scale with the number of peers, and more importantly, (iii) given a utility function, this approach enables any peer in the overlay to estimate relative utility of any other two peers. Each receiver only maintains information about a fixed (and relatively small) number of promising parent peers in its local image. The local image at each receiver is dynamically updated with new gossip messages as other peers join/leave the overlay. Each peer selects a new parent in a demand-driven fashion in order to minimize the number of end-to-end bandwidth measurements, and thus improve scalability. When a receiver needs a new parent, its PS mechanism randomly selects a peer from its local image where probability of selecting a peer directly depends on its utility. Then, the actual properties (i.e., available bandwidth and delay) of the selected parent are verified through passive measurement. Toward this end, the selected parent is added to the parent list which triggers PALS to request content from this parent. Figure 1 depicts the interactions between PD and PS mechanisms.
In PRO, each receiver leverages congestion controlled bandwidth from its parents as an implicit signal to detect two events: (i) any measurable shared bottleneck among connections from different parents, and (ii) any change in network or overlay conditions (e.g., departure or arrival of other close-by peers). Figure 2 shows part of an overlay to illustrate this feature. Each receiver continuously monitors available bandwidth from all its parents. Receiver p0 initially has only p1 as a parent. When p0 adds a new parent (p2), the receiver examines the smoothed available bandwidth from p1 and p2 and any measurable correlation between them. If the available bandwidth from p1 decreases after p2 is added, the receiver can conclude that these two parents are behind the same bottleneck (i.e., link L0). We note that paths from two parents might have some overlap that does not include any bottleneck. Assume another receiver p3 selects p1 as a parent and thus competes with receiver p0 for available bandwidth on link L1. Suppose that L1 becomes a bottleneck and the connection between p1 to p3 obtains a significantly higher share of L1"s bandwidth than connection between p1 to p0. This change in available bandwidth from p1 serves as a signal for p0. Whenever a receiver detects such a drop in bandwidth, it waits for a random period of time (proportional to the available bandwidth) and then drops Source Peer Disc.
Peer Selec. gossip Exam. a New Parent Criteriafor PeerDiscovery Update LocalImage oftheOverlay Unknown peers in the overlay Known peers in the overlay Select Internal Components of Receiver Peer Receiver Peer Figure 1: Interactions between PD and PS mechanisms through local image 44 P1 P3 P0 L0 L1 L3 Overlay connection Network Path P2 L2 P1 P3 P0 L0 L1 L3 P2 L2 Initial Overlay Reshaped Overlay Figure 2: Using congestion controlled bandwidth as signal to reshape the overlay the corresponding parent if its bandwidth remains low [8].
Therefore, the receiver with a higher bandwidth connectivity (p3) is more likely to keep p1 as parent whereas p0 may examine other parents with higher bandwidth including p3.
The congestion controlled bandwidth signals the receiver to properly reshape the overlay. We present a summary of key features and limitations of PRO in the next two sections.
Table 1 summarizes our notation throughout this paper.
Main Features: Gossiping provides a scalable approach to peer discovery because each peer does not require global knowledge about all group members, and its generated traffic can be controlled. The PD mechanism actively participates in peer selection by identifying peers for the local image which limits the possible choices of parents for the PS mechanism. PRO constructs a multi-parent, unstructured overlay. But PRO does not have the same limitations that exist in multi-tree approaches because it allows each receiver to independently micro-manage its parents to maximize its overall bandwidth based on local information. PRO conducts passive measurement not only to determine available bandwidth from a parent but also to detect any shared bottleneck between paths from different parents.
Furthermore, by selecting a new parent from the local image, PRO increases the probability of finding a good parent in each selection, and thus significantly decreases number of required measurements which in turn improves scalability.
PRO can gracefully accommodate bandwidth heterogeneity and asymmetry among peers since PALS is able to manage delivery of content from a group of parents with different bandwidth.
Limitations and Challenges: The main hypothesis in our framework is that the best subset of parents for each receiver are likely to be part of its local image i.e., PD mechanism can find the best parents. Whenever this condition is not satisfied, either a receiver may not be able to maximize its overall bandwidth or resulting overlay may not be efficient.
Table 1: Notation used throughout the paper Symbol Definition. pi Peer i inbwi Incoming access link BW for pi outbwi Outgoing access link BW for pi min nopi Min. No of parents for pi max nopi Max. No of parents for pi nopi(t) No of active parents for pi at time t img sz Size of local image at each peer sgm Size of gossip message delij Estimated delay between pi and pj Clearly, properties of the selected utility function as well as accuracy of estimated parameters (in particular using outgoing bandwidth instead of available bandwidth) determine properties of the local image at each peer which in turn affects performance of the framework in some scenarios. In these cases, the utility value may not effectively guide the search process in identifying good parents which increases the average convergence time until each peer finds a good subset of parents. Similar to many other adaptive mechanisms (e.g., [13]), the parent selection mechanism should address the fundamental tradeoff between responsiveness and stability. Finally, the congestion controlled bandwidth from parent peers may not provide a measurable signal to detect a shared bottleneck when level of multiplexing is high at the bottleneck link. However, this is not a major limitation since the negative impact of a shared bottleneck in these cases is minimal. All the above limitations are in part due to the simplicity of our framework and would adversely affect its performance. However, we believe that this is a reasonable design tradeoff since simplicity is one of our key design goals. In the following sections, we describe the two key components of our framework in further details.
Peer discovery at each receiver is basically a search among all participating peers in the overlay for a certain number (img sz) of peers with the highest relative utility. PRO adopts a gossip-like [10] approach to peer discovery.
Gossiping (or rumor spreading) has been frequently used as a scalable alternative to flooding that gradually spreads information among a group of peers.However, we use gossiping as a search mechanism [9] for finding promising parents since it has two appealing properties (i) the volume of exchanged messages can be controlled, and (ii) the gossip-based information exchange can be customized to leverage relative utility values to improve search efficiency.
The gossip mechanism works as follow: each peer maintains a local image that contains up to img sz records where each record represents the following information for a previously discovered peer pi in the overlay: 1) IP address, 2) GNP coordinates, 3) number of received layers, 4) timestamp when the record was last generated by a peer, 5) outbwi and 6) inbwi. To bootstrap the discovery process, a new receiver needs to learn about a handful of other participating peers in the overlay. This information can be obtained from the original server (or a well-known rendezvous point). The server should implement a strategy for selecting the initial peers that are provided to each new receiver. We call this the initial parent selection mechanism. Once the initial set of peers are known, each peer pi periodically invokes a target selection mechanism to determine a target peer (pj) from its local image for gossip. Given a utility function, peer pi uses a content selection strategy to select sgm records (or smaller number when sgm records are not available) from its local image that are most useful for pj and send those records to pj. In response, pj follows the same steps and replies with a gossip message that includes sgm records from its local image that are most useful for pi, i.e., bidirectional gossip. When a gossip message arrives at each peer, an image maintenance scheme integrates new records into the current local image and discards excess records such that certain property of the local image is improved (e.g., increase overall utility of peers in the image) Aggregate performance of 45 a gossip mechanism can be presented by two average metrics and their distribution among peers: (i) Average Convergence Time: average number of gossip messages until all peers in an overlay reach their final images, and (ii) Average Efficiency Ratio: average ratio of unique records to the total number of received records by each peer.
We have been exploring the design space of four key components of the gossip mechanism. Frequency and size of gossip messages determine average freshness of local images.
Currently, the server randomly selects the initial parents from its local image for each new peer.
Target Selection: Target selection randomly picks a peer from the current image to evenly obtain information from different areas of the overlay and speed up discovery.
Content Selection: peer pk determines relative utility of all the peers (pj) in its local image for target peer pi, and then randomly selects sgm peers to prepare a gossip message for pi. However, probability of selecting a peer directly depends on its utility. This approach is biased towards peers with higher utility but its randomness tend to reduce number of duplicate records in different gossip message from one peer (i.e., improves efficiency). A potential drawback of this approach is the increase in convergence time. We plan to examine more efficient information sharing schemes such as bloom filters [3] in our future work. PRO uses joint-ranking [15] to determine relative utility of a parent for a receiver.
Given a collection of peers in a local image of pk, the jointranking scheme ranks all the peers once based on their outgoing bandwidth, and then based on their estimated delay from a target peer pi. The utility of peer pj (U(pj, pi)) is inversely proportional to the sum of pj"s ranks in both rankings. Values for each property (i.e., bandwidth and delay) of various peers are divided into multiple ranges (i.e., bins) where all peers within each range are assumed to have the same value for that property. This binning scheme minimizes the sensitivity to minor differences in delay or bandwidth among different peers.
Image maintenance: Image maintenance mechanism evicts extra records (beyond img sz) that satisfy one of the following conditions: (i) represent peers with the lower utility, (ii) represent peers that were already dropped by the PS mechanism due to poor performance and (iii) have a timestamp older than a threshold. This approach attempts to balance image quality (in terms of overall utility of existing peers) and its freshness.
Note that the gossip mechanism can discover any peer in the overlay as long as reachability is provided through overlap among local images at different peers. The higher the amount of overlap, the higher the efficiency of discovery, and the higher the robustness of the overlay to dynamics of peer participations. The amount of overlap among images depends on both the size and shape of the local images at each peer. The shape of the local image is a function of the deployed utility function. Joint-ranking utility gives the same weight to delay and bandwidth. Delay tends to bias selection towards near-by peers whereas outgoing bandwidth introduces some degree of randomness in location of selected peers. Therefore, the resulting local images should exhibit a sufficient degree of overlap.
The PS mechanism at each peer is essentially a progressive search within the local image for a subset of parent peers such that the following design goals are achieved: (i) maximizing delivered bandwidth 5 , (ii) minimizing the total delay from all parents to the receiver, and (iii) maximizing diversity of paths from parents (whenever it is feasible).
Whenever these goals are in conflict, a receiver optimizes the goal with the highest priority. Currently, our framework does not directly consider diversity of paths from different parents as a criteria for parent selection. However, the indirect effect of shared path among parents is addressed because of its potential impact on available bandwidth from a parent when two or more parents are behind the same bottleneck.
The number of active parents (nopi(t)) for each receiver should be within a configured range [min nop, max nop].
Each receiver tries to maximize its delivered bandwidth with the minimum number of parents. If this goal can not be achieved after evaluation of a certain number of new parents, the receiver will gradually increase its number of parents. This flexibility is important in order to utilize available bandwidth from low bandwidth parents, i.e., cope with bandwidth heterogeneity. min nop determines minimum degree of resilience to parent departure, and minimum level of path diversity (whenever diverse paths are available). The number of children for each peer should not be limited.
Instead, each peer only limits maximum outgoing bandwidth that it is able (or willing) to provide to its children. This allows child peers to compete for congestion controlled bandwidth from a parent which motivates child peers with poor bandwidth connectivity to look for other parents (i.e., properly reshape the overlay).
Design of a PS mechanism should address three main questions as follows: 1) When should a new parent be selected?
There is a fundamental tradeoff between responsiveness of a receiver to changes in network conditions (or convergence time after a change) and stability of the overlay. PRO adopts a conservative approach where each peer selects a new parent in a demand-driven fashion. This should significantly reduce number of new parent selections, which in turn improves scalability (by minimizing the interference caused by new connections) and stability of the overlay structure. A new parent is selected in the following scenarios: (i) Initial Phase: when a new peer joins the overlay, it periodically adds a new parent until it has min nop parents. (ii) Replacing a Poorly-Performing Parent: when available bandwidth from an existing parent is significantly reduced for a long time or a parent leaves the session, the receiver can select another peer after a random delay. Each receiver selects a random delay proportional to its available bandwidth from the parent peer [8]. This approach dampens potential oscillation in the overlay while increasing the chance for receivers with higher bandwidth connectivity to keep a parent (i.e., properly reshapes the overlay). (iii) Improvement in Performance: when it is likely that a new parent would significantly improve a non-optimized aspect of performance (increase the bandwidth or decrease the delay). This strategy allows gradual improvement of the parent subset as new peers are discovered (or joined) the overlay. The available information for each peer in the image is used as a heuristic to predict performance of a new peer. Such an improvement should be examined infrequently. A hysteresis mechanism 5 The target bandwidth is the lower value between maximum stream bandwidth and receiver"s incoming bandwidth. 46 is implemented in scenario (ii) and (iii) to dampen any potential oscillation in the overlay. 2) Which peer should be selected as a new parent?
At any point of time, peers in the local image are the best known candidate peers to serve as parent. In PRO, each receiver randomly selects a parent from its current image where the probability of selecting a parent is proportional to its utility. Deploying this selection strategy by all peers lead to proportional utilization of outgoing bandwidth of all peers without making the selection heavily biased towards high bandwidth peers. This approach (similar to [5]) leverages heterogeneity among peers since number of children for each peer is proportional to its outgoing bandwidth. 3) How should a new parent be examined?
Each receiver continuously monitors available bandwidth from all parents and potential correlation between bandwidth of two or more connections as signal for shared bottleneck. The degree of such correlation also reveals the level of multiplexing at the bottleneck link, and could serve as an indicator for separating remote bottlenecks from a local one. Such a monitoring should use average bandwidth of each flow over a relatively long time scale (e.g., hundreds of RTT) to filter out any transient variations in bandwidth.
To avoid selecting a poorly-performing parent in the near future, the receiver associates a timer to each parent and exponentially backs off the timer after each failed experience [13].
After the initial phase, each receiver maintains a fixed number of parents at any point of time (nopi(t)). Thus, a new parent should replace one of the active parents.
However, to ensure monotonic improvement in overall performance of active parents, a new parent is always added before one of the existing parents is dropped (i.e., a receiver can temporarily have one extra parent). Given the available bandwidth from all parents (including the new one) and possible correlation among them, a receiver can use one of the following criteria to drop a parent: (i) to maximize the bandwidth, the receiver can drop the parent that contributes minimum bandwidth, (ii) to maximize path diversity among connections from parents, the receiver should drop the parent that is located behind the same bottleneck with the largest number of active parents and contributes minimum bandwidth among them. Finally, if the aggregate bandwidth from all parents remains below the required bandwidth after examining certain number of new parents (and nopi(t) < max nop), the receiver can increase the total number of parents by one.
In this paper, we presented a simple receiver-driven framework for architecting peer-to-pee overlay structures called PRO. PRO allows each peer to selfishly and independently determine the best way to connect to the overlay to maximize its performance. Therefore, PRO should be able to maximize delivered quality to peers with heterogeneous and asymmetric bandwidth connectivity. Both peer discovery and peer selection in this framework are scalable.
Furthermore, PRO uses congestion controlled bandwidth as an implicit signal to detect shared bottleneck among existing parents as well as changes in network or overlay conditions to properly reshape the structure. We described the basic framework and its key components, and sketched our strawman solutions.
This is a starting point for our work on PRO. We are currently evaluating various aspects of this framework via simulation, and exploring the design space of key components. We are also prototyping this framework to conduct real-world experiments on the Planet-Lab in a near future.

Considering that a real-time flow may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity: Temporal sensitivity: Loss of ADUs" which is correlated in time may lead to disruptions in the service.
Note that this effect is further aggravated by some interdependence between ADUs (i.e., that one ADU can only be decoded when a previous ADU before has successfully been received and decoded). For voice, as a single packet contains typically several ADUs (voice frames) this effect is thus more significant than e.g. for video. It translates basically to isolated packet losses versus losses that occur in bursts.
Sensitivity due to ADU heterogeneity: Certain ADUs might contain parts of the encoded signal which are ‘Application Data Unit: the unit of data emitted by a source coder such as a video or voice frame. 441 Figure 1: Schematic utility functions dependent on the loss of more (+l) and less (-1) important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance (we assume same size, frequency and no interdependence between the frames). Under the loss of 50% of the packets, the perceptual quality varies hugely between the case where the 50% of the frames with high perceptual importance are received and the tax where the 50% less important frames are received.
Network support for real-time multimedia flows can on one hand aim at offering a lossless service, which, however, to be implemented within a pa&et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. It is our strong belief that this needs to be done in a generic way, i.e., no application-specific knowledge (about particular coding schemes e.g.) should be necessary within the network and, vice versa, no knowledge about network specifics should be necessary within an application. Let us now consider the case that 50% of packets of a flow are identified as more important (designated by +l) or less important (-1) due to any of the above sensitivity constraints. Figure 1 a) shows a generic utility function describing the applicationlevel Quality of Service (QoS) dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the +l and -1 packets are equal. Due to the over-proportional sensitivity of the +l packets to loss as well as the dependence of the end-t* end loss recovery performance on the fl packets, the utility function is decreasing significantly in a non-linear way (approximated in the figure by piece-wise linear functions) with an increasing loss rate. Figure 1 b) presents the CBS~ where all +l packets are protected at the expense of -1 pa&&.
The decay of the utility function (for loss rates < 50%) is reduced, because the +l packets are protected and the endto-end loss recovery can thus operate properly over a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application"s utility. Note that the higher the non-linearity of the utility contribution of the fl packets is (deviation from the dotted curve in Fig. 1 a), the higher is the potential gain in utility when the protection for +l packets is enabled. Results for actual perceived quality as utility for multimedia applications exhibit such a non-linear behavior*.
To describe this effect and provide a taxonomy for different QoS enhancement approaches, we introduce a novel terminology: we designate mechanisms which influence QoS parameters between Bows (thus decrease the loss rate of one flow at the expense of other flows) as inter-flow QoS.
Schemes which, in the presence of loss, differentiate between pa&& within a flow as demonstrated in Figure 1 above, provide intra-flow QoS enhancement. As QoS mechanisms have to be implemented within the network (hopby-hop) and/or in the end systems (end-to-end), we have another axis of classification.
The adaptation of the sender"s bitrate to the current network congestion state as an intraflow QoS scheme (loss avoidance, [IS]) is difficult to apply to voice. Considering that voice flows have a very low bitrate, the relative cost of transmitting the feedback information is high (when compared e.g. to a video flow). To reduce this cost the feedback interval would need to be increased, then leading to a higher probability of wrong adaptation decisions. The major di6culty, however, is the lack of a codec which is truly scalable in terms of its output bitrate and corresponding perceptual quality. Currently standardized voice codecs ([17]) usually only have a 6xed output bitrate. While it has been proposed to switch between voice codeca ([2]), the MOS (subjective quality) values for the codecs employed do not differ much: e.g., theITU codecs G.723.1, G.729, G.728, G.726and G.711 cover a bitrate range from 5.3 kbitjs to 64 kbitjs while the subjective quality differs by less than 0.25 on a l-to-5 MOS scale ([4], 1: bad, 5: excellent quality). So when the availability of sutticient computing power is assumed, the lowest bitrate codec can be chosen permanently without actually decreasing the perceptual quality.
For loss recovery on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction (FEC) have been proposed ([2]). While such schemes are attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. This adaptation is crucial especially when the fraction of traflic using redundancy schemes is large (181). If the redundancy is a source coding itself, like it has often been proposed ([7]), the comments from above on adaptation also apply. Using redundancy has also implications to the playout delay adaptation ([lo]) employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to ap‘While we have obtained results which confirm the shape of the overall utility curve shown in Fig. 1, clearly the utility functions of the +1/-l sub.flows and their relationship are more complex and only approximately additive. 442 Table 1: State and transition probabilities computed for an end-to-end Internet trace using a general Markov model (third order) by Yajnik et. al. [9] state 000 001 010 011 100 101 110 111 Probability of Probability Probability being in the state of I(s)=0 of l(s)=1
0 . 0 2 0 8 0 . 6 1 1 2 0 . 3 8 8 8 0 . 0 1 4 2 0 . 8 8 1 9 0.1181 0 . 0 1 0 2 0 . 2 7 1 0 0 . 7 2 9 0 0 . 0 2 0 8 0 . 9 2 7 8 0 . 0 7 2 2 0 . 0 0 3 6 0 . 4 1 9 8 0 . 5 8 0 2 0 . 0 1 0 2 0 . 8 1 0 9 0.1891
plications which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but cannot come close to eliminate them.
Therefore in this work we assume that the lowest possible bitrate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual codecs and measure their utility in the presence of packet loss using objective speech quality measurement.
The paper is structured as follows: Section 2 introduces packet- and user-level metrics. We employ these metrics to describe the sensitivity of VoIP traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and hopby-hop level. Section 6 concludes the paper.
A general Markov model ([19, S]) which describes the loss process is defined as follows: Let P( Z(s) ] Z(s - m),... ,Z(s - 2),Z(s - 1) ) be the state transition probability of a general Markov model of order m, where Z(s) is the loss indicator function for the packet with the sequence number s. All combinations for the values (0 and 1) of the sequence Z(s-m), . . . , Z(s-2),Z(s-1) appear in the state space. As an example P( Z(s) = 1 ] Z(s - 2),Z(s1) = 01 ) gives the state transition probability when the current packet s is lost, the previous packet s - 1 has also been lost and packet s - 2 has not been lost. The number of states of the model is 2. Two state transitions can take place from any of the states. Thus, the number of parameters which have to be computed is 2m+1. Even for relatively small m this amount of parameters is difficult to be evaluated and compared. Table 1 shows some values for the state and transition probabilities for a general Markov model of third order measured end-to-end in the Internet by Yajnik et. al. ([19]). It is interesting to note that for all states with Z(s - 1) = 0 the probability for the next packet not to be lost (Z(s) = 0) is generally very high (> 0.8, in bold typeface) whereas when Z(s- 1) = 1 the state transition probabilities to that event cover the range of 0.15 to 0.61.
That means that past no loss events do not affect the loss process as much as past loss events. Intuitively this seems to make sense, because a successfully arriving packet can be seen as a indicator for congestion relief. Andren et. al. ([l]) as well as Yajnik et. al. ([20]) both confirmed this by measuring the cross correlation of the loss- and no-loss-runlengths. They came to the result that such correlation is very weak. This implies that patterns of short loss bursts interspersed with short periods of successful packet arrivals occur rarely (note in this context that in ‘&able 1 the pattern 101 has by far the lowest state probability).
Thus, in the following we employ a model ([12]) which only considers the past loss events for the state transition probability. The number of states of the model can be reduced from 2m to m + 1. This means that we only consider the state transition probability P( Z(s) ] Z(s - k), . . . ,Z(s - 1) ) with Z(s - Ic + i) = 1 V i E [0, Ic - 11, where Ic (0 < k 5 m) is a variable parameter. We define a loss run length lc for a sequence of Ic consecutively lost packets detected at sj (sj > Ic > 0) with Z(sj - k - 1) = O,Z(sj) = 0 and Z(sj - k + i) = 1 V i E [0, Ic - 11, j being the j-th ‘<burst loss event. Note that the parameters of the model become independent of the sequence number s and can now rather be described by the occurence ok of a loss run length k.
We define the random variable X as follows: X = 0: no packet lost, X = k (0 < lc < m): ezactly lc consecutive packets lost, and X 2 k (0 < k < m): at least k consecutive packets lost. With this definition, we establish a loss run-length model with a finite number of states (m + 1) which gives loss probabilities dependent on the burst length.
In the model, for every additional lost packet which adds to the length of a loss burst a state transition takes place. If a packet is successfully received, the state returns to X = 0.
Thus, the state probability of the system for 0 < Ic < m is P(X 2 Zc). Due to the limited memory of the system, the last state X = m is just defined as crm consecutive packets lost, with P(X = m) being the state probability. Given the case of a finite number of packets a for a flow, which experiences d = cpC1 ]cok packet drops, we have the relative frequency pL,k = 2 (P(X = k) for a + 00) for the occurence of a loss burst of length lc. An approximation for the expectation of the random variable X can be computed as pi = cr==, #+L,k and identified with the mean loss rate. We can also approximate the state probabilities of the model by the cumulative loss rate p~,~,,~(k) = CrSkp~,,, (0 < k < m) and p~,~ = Cr.p=, (n--mzl)ot (k = m). The transition probabilities for 1 < lc < m can be computed easily as: p(k--l)(k) = P(X>klX>k-l)= P(X>knX>k-1) P(X>k-1) ZZ P ( X > k ) P(X>k-1) These conditional loss probabilities again can be approxPzl,cum(k) _ Cnm,k on i m a t e d by PL,cond@) = PL,cum(k-l) - C;Ek--l 0,. For 443 Figure 2: Loss run-length model with m + 1 states P(X = m]X = m) we have: PL,,,d(rn) = %Lf;:P Fig. 2 shows the Markov chain for the model. (1) Additionally, we also define a random variable Y which de scribes the distribution of burst loss lengths with respect to the bunt loss events j (and not to packet events like in the definition of X). We have the burst loss length rate ga = * as an estimate for P(Y = k). Thus, the mean burst loss length is g = h = e = CT==, kgb corresponding to E[Y] (average loss gap, [5]).
The run-length model implies a geometric distribution for residing in state X = m. For the probability of a burst loss length of k packets we thus can compute estimates for performance parameters in a higher order model representation (note that here Y represent the random variable used in the higher-order models). For a three state model we have e.g. for P(Y = k): P(Y = k) = P(Y=k)=l-pm: k=l pn p;;" (1 -pm): k 2 2 c-4 For the special case of a system with a memory of only the previous packet (m = l), we can use the runlength distribution for a simple computation of the parameters of the commonly-used Gilbert model (Fig. 3) to characterize the loss process (X being the associated random variable with X = 0: no packet lost, X = 1 a packet lost). Then the probability of being in state m can be seen BS the uncow ditional loss probability ulp and approximated by the mean loss (pr. = pi,,,,). Only one conditional loss probability clp for the transition 1 --t 1 exists: p~,E-d = CFL(~ - lbd (3) If losses of one flow are correlated (i.e., the loss probability of an arriving packet is itiuenced by the contribution to the state of the queue by a previous packet of the same flow and/or both the previous and the current packet see busty arrivals of other traffic, [15]) we have pal < clp and thus ulp 5 clp. For pal = elp the Gilbert model is equivalent to a l-state (Bernotilli) model with zllp = elp (no loss correlation).
As in equation 2 we can compute an estimate for the probability of a burst loss length of k packets for a higher order model representation: B(Y = k) = clpk-‘(1 - clp) (4) Figure 3: Loss run-length (Gilbert model) model with two states Figure 4: Components of the end-to-end loss recovery/control measurement setup.
Unlike the conventional methods like the Signal-t-Noise Ratio (SNR), novel objective quality measures attempt to estimate the subjective quality as closely as possible by modeling the human auditory system. In our evaluation we use two objective quality measures: the Enhanced Mod&d Bark Spectral Distortion (EMBSD, [21]) and the Measw ing Normalizing Blocks (MNB) described in the Appendix II of the ITU-T Recommendation P.861 ([18]). These two objective quality measure.~ are reported to have a very high correlation with subjective tests, their relation to the range of subjective test result values (MOS) is close to being linear and they are recommended as being suitable for the evaluation of speech degraded by transmission errors in real network environments such as bit errors and frame erasures ([18, 211). Both metrics are distance measures, i.e., a IB suit value of 0 implies perfect quality, the larger the value, the worse the speech quality is (in Fig. 5 we show an axis with approximate corresponding MOS values). For all simulations in this paper we employed both schemes. As they yielded very similar results (though MNB results generally exhibited less variability) we only present EMBSD results.
Figure 4 shows the components of the measurement setup which we will use to evaluate our approach to combined end-t-end and hopby-hop loss recovery and control. The 444 shaded boxes show the components in the data path where mechanisms of loss recovery are located. Together with the parameters of the network model (section 2.1) and the perceptual model we obtain a measurement setup which allows us to map a specific PCM signal input to a speech quality measure. While using a simple end-to-end loss characterization, we generate a large number of loss patterns by using different seeds for the pseudcerandom number generator (for the results presented here we used 300 patterns for each simulated condition for a single speech sample). This procedure takes thus into account that the impact of loss on an input signal may not be homogenous (i.e., a loss burst within one segment of that signal can have a different perceptual impact than a loss burst of the same size within another segment).
By averaging the result of the objective quality measure for several loss patterns, we have a reliable indication for the performance of the codec operating under a certain network loss condition. We employed a Gilbert model (Fig. 3) as the network model for the simulations, as we have found that here higher order models do not provide much additional information.
We first analyze the behaviour for ~-law PCM flows (64 kbit/s) with and without loss concealment, where the loss concealment repairs isolated losses only (speech stationarity can only be assumed for small time intervals). Results are shown for the AP/C concealment algorithm ([ll]). Similar results were obtained with other concealment algorithms.
Figure 5 shows the case without loss concealment enabled where Gilbert model parameters are varied. The resulting speech quality is insensitive to the loss distribution parameter (elp). The results are even slightly decressing for an increasing clp, pointing to a significant variability of theresuits. In Figure 6 the results with loss concealment are depicted. When the loss correlation (dp) is low, loss concealment provides a significant performance improvement. The relative improvement increases with increasing loss (pulp).
For higher clp values the cases with and without concealment become increasingly similar and show the same performance at clp x 0.3. Figures 5 and 6 respectively also contain a curve showing the performance under the assumption of random losses (Bernouilli model, ulp = elp). Thus, considering a given ulp, a worst case loss pattern of alternating losses (I(s mod 2) = l,l([s + l] mod 2) = 0) would enable a considerable performance improvement (with ok = OVk > 1: p~,cond = 0, Eq. 3).
As we found by visual inspection that the distributions of the perceptual distortion values for one loss condition seem to approximately follow a normal distribution we employ mean and standard deviation to describe the statistical variability of the measured values. Figure 7 presents the perceptual distortion as in the previous figure but also give the standard deviation as error bars for the respective loss condition. It shows the increasing variability of the results with increasing loss correlation (clp), while the variability does not seem to change much with an increasing amount of loss (alp). On one hand this points to some, though weak, sensitivity with regard to heterogeneity (i.e., it matters which area of the speech signal (voiced/unvoiced) experiences a burst loss).
On the other hand it shows, that a large number of different Figure 5: Utility curve for PCM without loss conc e a l m e n t (EMBSD) Figure 6: Utility curve for PCM with loss concealment (EMBSD) Figure 7: Variability of the perceptual distortion with 10s~ concealment (EMBSD) 445 Figure 8: Utility curve for the G.729 codec (EMBSD) loss patterns is necessary for a certain speech sample when using objective speech quality measurement to assess the impact of loss corwlation on user perception.
G.729 ([17]) uses the Conjugate Structure Algebraic Code Excited Linear Prediction (CS-ACELP) coding scheme and ooerates at 8 kbit/s. In G.729. a speech fmme is 10 ms in d&ion. For each frame, the"G.7i9 encoder analyzes the input data and extracts the parameters of the Code Excited Linear Prediction (CELP) model such as linear prediction filter coefficients and excitation vectors. When a frame is lost or corrupted, the G.729 decoder uses the parameters of the previous frame to interpolate those of the lost frame.
The line spectral pair coefficients (LSP3) of the last good frame are repeated and the gain coefficients are taken from the previous frame but they are damped to gradually reduce their impact. When a frame loss occurs, the decoder cannot update its state, resulting in a divergence of encoder and decoder state. Thus, errors are not only introduced during the time period represented by the current frame but also in the following ones. In addition to the impact of the missing codewords, distortion is increased by the missing update of the predictor filter memories for the line speo tral pairs and the linear prediction synthesis filter memo ries. Figure 8 shows that for similar network conditions the output quality of the G.729* is worse than PCM with loss concealment, demonstrating the compression versus quality tradeoff under packet loss. Interestingly the loss correlation (dp parameter) has some impact on the speech quality, however, the effect is weak pointing to a certain robustness of the G.729 codec with regard to the resiliency to consecutive packet losses due to the internal loss concealment.
Rosenberg has done a similar experiment ([9]), showing that the difference between the original and the concealed signal with increasing loss burstiness in terms of a mean-squared error is significant, however. This demonstrates the importance of perceptual metrics which are able to include concealment ‘LSPs are another representation of the linear prediction coefficients. ‘Two G.729 frames are contained in a packet.
Figure 9: Resynchronization time (in frames) of the G.729 decoder after the loss of k consecutive frames (k E [1,4]) as a function of frame position. (and not only reconstruction) operations in the quality assessment.
PCM is a memoryless encoding. Therefore the ADU content is only weakly heterogeneous (Figure 7). Thus, in this section we concentrate on the G.729 coda. The experiment we carry out is to meawre the resynchronization time of the d* coder after k consecutive frames are lost. The G.729 decoder is said to have resynchronized with the encoder when the energy of the error signal falls below one percent of the energy of the decoded signal without frame loss (this is equivalent to a signal-t-noise ratio (SNR) threshold of 2OdB). The error signal energy (and thus the SNR) is computed on a per-frame basis. Figure 9 shows the resynchronization time (expressed in the number of frames needed until the threshold is exceeded) plotted against the position of the loss (i.e., the index of the first lost frame) for d&rent values of k.
The speech sample is produced by a male speaker where an unvoiced/voiced (au) transition occurs in the eighth frame.
We can see from Figure 9 that the position of a frame loss has a signilicant inlluence cm the resulting signal degradation", while the degradation is not that sensitive to the length of the frame loss burst k. The loss of unvoiced frames seems to have a rather small impact on the signal degradation and the decoder recovers the state information fast thereafter. The loss of voiced frames causes a larger degradation of the speech signal and the decoder needs more time to resyncbronize with the sender. However, the loss of voiced frames at an unvoiced/voiced transition leads to a significant degradation of the signal. We have repeated the experiment for different male and female speakers and obtained similar results. lsking into account the wed coding scheme, the above phenomenon could be explained as follows: Because voiced sounds have a higher energy than unvoiced sounds, the loss of voiced frames causes a larger signal degradation than the loss of unvoiced frames. However, due to the periodic property of voiced sounds, the decoder can conceal ‘While on one hand we see that SNR n~easu~es often do not correlate well with subjective speech quality, on the other hand the large differences in the SNR-threshold-based resynchronization time clearly point to a significant impact on subjective speech quality. 446 Figure 10: Differential RED drop probabilities as a function of average queue sizes the loss of voiced frames well once it has obtained suflicient information on them. The decoder fails to conceal the loss of voiced frames at an unvoiced/voiced transition because it attempts to conceal the loss of voiced frames using the filter coefficients and the excitation for an unvoiced sound.
Moreover, because the G.729 encoder uses a moving average filter to predict the values of the line spectral pairs and only transmits the difference between the real and predicted vaues, it takes a lot of time for the decoder to resynchronize with the encoder once it has failed to build the appropriate linear prediction filter.
INTRAFLOW LOSS CONTROL While we have highlighted the sensitivity of VoIP traffic to the distribution of loss in the last sections, we now want to briefly introduce a queue management mechanism ([13]) which is able to enforce the relative preferences of the ap plication with regard to loss.
We consider flows with packets marked with +l and ‘I1 (as described in the introduction) BS foreground traffic (FT) and other (best effort) flows as backaound traffic (BT). Packet marking, in addition to keeping the desirable property of state aggregration within the network core as proposed by the IETF Differentiated Services architecture, is exploited here to convey the intrbflow requirements of a llow. As it should be avoided to introduce reordering of the packets of a flow in the network we consider mechanisms for the management of a single queue with different priority levels. One approach to realize inter-flow service differentiation using a single queue is RIO (‘RED with IN and OUT", [3]). With RIO, two average queue sizes as congestion indicators are computed: one just for the IN (high priority) packets and another for both IN and OUT (low priority) packets. Packets marked as OUT are dropped earlier (in terms of the average queue size) than IN packets. RIO has been designed to decrease the clip seen by particular Bows at the expense of other flows. In this work, however, we want to keep the ulp as given by other parameters while modifying the loss distribution for the foreground traffic (FT).
This amounts to trading the loss of a +l packet against a -1 packet of the same flow (in a statistical sense). Fig. 10 shows the conventional RED drop probability curve (po as a function of the average queue size for all arrivals avg), which is applied to all unmarked (0) traffic (background traffic: BT).
The necessary relationship between the drop probabilities for packets marked as -1 and +l can be derived 85 follows (note that this relationship is valid both at the end-tend level and every individual hop): Let a = a0 + a+, +a-~ be the overall number of emitted packets by an FT flow and a,, + E [-l,O, +l] be the number of packets belonging to a certain class (where the 0 class corresponds to (unmarked) best effort traflic). Then, with a+, = a-1 = a,~, and considering that the resulting service has to be best effort in the long term, we have: aopo + a+lp+l + LIP-1 A ape Qlll@+l fP-1) = (a--oo)m P - l = zpo-p+1 (5) Due to this relationship between the drop probability for +l and -1 packets, we designate this queue management algorithm as Differential RED (DiERED). Figure 10 shows the corresponding drop probability curves. Due to the condition of a+, = (I-L = all, in addition to the conventional RED behaviour, the DiffRED implementation should also monitor the +l and -1 arrival processes. If the ratio of +l to -1 packets at a gateway is not 1 (either due to misbehaving flows or a significant number of flows which have already experienced loss at earlier hops) the -1 loss prob ability is decreased and the +l probability is increased at the same time thus degrading the service for all users. The shaded areas above and below the po(avg) curve (Fig. 10) show the operating area when this correction is added.
In [13] it has been shown that using only the conventional RED average queue size avg for DSRED operation is not sufficient. This is due to the potentially missing correlation of the computed aug value between consecutive +l and -1 arrivals, especially when the share of the FT traftic is low. As this might result in a unfair distribution of losses between the FT and BT fractions, a specific avgl value is computed by sampling the queue size only at FT arrival instants. Thus, a service differentiation for foreground traffic is possible which does not differ from conventional RED behaviour in the long term average (i.e., in the ulp).
CONTROL
Considering a flow with temporal loss sensitivity, paragraph
the performance of the end-to-end loss recovery. The pattern is not tied to particular packets, therefore a per-flow characterization with the introduced metrics is applicable. In this paragraph we assume that a flow expressed its temporal se*sitivity by marking its flow with an alternating pattern of ,c+l,>,‘c~l,,, 447 Figure 11: Comparison of actual and estimated Figure 12: Comparison of actual and estimated burst loss length rates as a function of burst length burst, loss length rates as a function of burst length k: three state run-length-based model k: two state run-length-based model (Gilbert.) Figures 11 and 12 show the rates for the actual and the estimated burst loss lengths for a three-state (m = 2) and a two=state (m = 1, Gilbert) model respectively6. We can observe that DiffRED shapes the burst probability curve in the desired way. Most of the probability mass is concentrated at, isolated losses (the ideal behaviour would be the occurence of only isolated losses (Ic = 1) which can be expressed with clp = 0 in terms of Gilbert model parameters).
With Drop Tail an approximately geometrically decreasing burst loss probability with increasing burst length (Eq. 4) is obtainable, where the clp parameter is relatively large though. Thus, considering voice with temporal loss sensitivity as the foreground traffic of interest, with DifFRED a large number of short annoying bursts can be traded against a larger number of isolated losses and few long loss bursts (which occur when the queue is under temporary overload, i.e., awg > maxth, Fig. 10).
We can see that the three-state model estimation (Eq. 2) reflects the two areas of the DifFRED operation (the sharp drop of the burst loss length rate for k = 2 and the decrease along a geometrically decreasing asymptote for k > 2). This effect cannot be captured by the two state model (Eq. 4) which thus overestimates the burst loss length rate for Ic = 2 and then hugely underestimates it for k > 2. Interestingly, for Drop Tail, while both models capture the shape of the actual curve, the lower order model is more accurate in the estimation. This can be explained as follows: if the burst loss length probabilities are in fact close to a geometrical distribution, the estimate is more robust if all data is included (note that the run-length based approximation of the conditional loss probability P(X = mlX = m) only includes loss run-length occurences larger or equal to m: Eq. 1). sWe only discuss the results qualitatively here to give an example how an intra-flow loss control algorithm performs and to show how loss models can capture this performance.
Details on the simulation scenario and parameters can be found in [12].
In paragraph 3.1.2 we have seen that sensitivity to ADU heterogeneity results in a certain non-periodic loss pattern.
Thus, a mechanism at (or near) the sender is necessary which derives that pattern from the voice data.
Furthermore, an explicit cooperation between end-to-end and hop by-hop mechanisms is necessary (Fig. 4).
We use the result of paragraph 3.2 to develop a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking (SPB-DIFFMARK). The DIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder"s concealment for other frames.
Figure 13 shows the simple algorithm written in a pseudocode that is used to detect an unvoiced/voiced (uw) transition and protect the voiced frames at, the beginning of a voiced signal. The procedure analysis0 is used to classify a block of Ic frames as voiced, unvoiced, or uv transition. send0 is used to send a block of Ic frames as a single packet with the appropriate priority (either fl, 0 or -1).
As the core algorithm gives only a binary marking decision (protect the packet or not), we employ a simple algorithm to send the necessary -1 packets for compensation (Eq. 5): after a burst of +l packets has been sent, a corresponding number of -1 packets is sent immediately. State about the necessary number of to-be-sent -1 packets is kept in the event that the SPB algorithm triggers the next +l burst before all -1 packets necessary for compensation are sent.
Thus, seen over time intervals which are long compared to the +1/-l burst times, the mean loss for the flow will be equal to the best effort case (Eq. 5). N is a pre-defined value and defines how many frames at the beginning of a voiced signal are to be protected. Our simulations (Fig. 9) have shown that the range from 10 to 20 are appropriate values for N (depending on the network loss condition). In the simulations presented below, we choose Ic = 2, a typical 448 protect = 0 fcm?ach (k frames) classify = analysis(k frames)] if (protect > 0) if (classify == unvoiced) protect = 0 if (compensation > 0) compensation = compensation-k send(k frames, -1) else send(k frames, 0) endif else send(k frames, +l) protect = protect-k compensation = compensation+k endif else if (classify == uvfransition) send(k frames, +l) protect = N - k compensation = compensation+k else if (compensation > 0) compensation = compensation-k send(k frames, -1) else send(k frames, 0) endif endif endif endfor Figure 13: SPB-DIFFMARK Pseudo Code value for interactive speech transmissions over the Internet (20ms of audio data per packet). A larger number of Ic would help to reduce the relative overhead of the protocol header but also increases the packetization delay and makes sender classification and receiver concealment in case of packet loss (due to a larger loss gap) more difficult.
Due to the non-periodic loss pattern, we need to explicitly associate a drop probability with a single packet within an end-to-end model. Therefore we use a separate one-state Markov model (Bernouilli model) to describe the network behaviour as seen by each class of packets. Best effort packets (designated by 0 in Fig. 14) are dropped with the probability pc, whereas packets marked with +l and -1 are dropped with probabilities of p+i and p-1 respectively.
This is a reasonable assumption" with regard to the interdependence of the different classes in fact, as it has been shown that DiffRED (Figs. 11 and 12) achieves a fair amount of decorrelation of +l and -1 packet losses. Nevertheless to include some correlation between the classes we have set p+i = 10m3 pc for the subsequent simulations. This should ‘The appropriateness of the simple end-to-end modeling used has been investigated in [12] with discrete event simulation using a multi-hop topology and detailed modeling of foreground and background traffic sources.
Marking Scheme Network Model N O M A R K lolo/ 0~0~0~ 01 F U L L M A R K +I +I +I +I +l +I SPB MARK 1 0 1+I j +I I 0 I 0 / 0 ALT MARK o/+11 Ol+ll o/+1 +I Pcl 330 po S P B D I F F M A R K 1 IJ / +l I +l I -1 j-1 / 0 I ALT DIFFMARK I-1 / +I 1-1 1+I /-I 1+I 1 Figure 14: Marking schemes and corresponding network models. also allow a reasonable evaluation of how losses in the fl class affect the performance of the SPB-algorithms.
For a direct comparison with SPB-DIFFMARK, we evaluate a scheme where packets are alternatingly marked as being either -1 or +l (ALT-DIFFMARK, Figure 14). We furthermore include related inter-flow loss protection schemes.
The first scheme uses full protection (FULL MARK, all packets are marked as +l). The SPB-MARK scheme operates similarly to SPB-DIFFMARK, but no -1 packets are sent for compensation (those packets are also marked as 0). For comparison we again use a scheme where packets are alternatingly marked as being either 0 or +l (ALT-MARK). Finally, packets of pure best effort flows are dropped with the probability po (NO MARK case in Fig. 14). For the SPB marking schemes the percentage of +l- and -l-marked packets respectively is 40.4% for the speech material used. We obtained similar marking percentages for other speech samples. The ALT marking schemes mark exactly 50% of their packets as being fl.
Figure 15 shows the perceptual distortion for the marking schemes dependent on the drop probability pc. The unprotected case (NO MARK) has the highest perceptual distortion and thus the worst speech quality*. The differential marking scheme (SPB-DIFFMARK) offers a significantly better speech quality even when only using a network service which amounts to best effort in the long term. Note that the ALT-DIFFMARK marking strategy does not differ from the best effort case (which confirms the result of paragraph 3.1.2). SPB-DIFFMARK is also even better than the inter-flow QoS ALT-MARK scheme, especially for higher values of pe. These results validate the strategy of our SPB marking schemes that do not equally mark all packets with a higher priority but rather protect a subset of frames that are essential to the speech quality. The SPB-FEC scheme ([12]), *We have also perfo rmed informal listening tests which confirmed the results using the objective metrics. 449 Figure 15: Perceptual Distortion (EMBSD) for the marking schemes and SPB-FEC which uses redundancy9 piggybacked on the main payload packets (RFC 2198, [7]) to protect a subset of the packets, enables a very good output speech quality for low loss rates.
However, it should be noted that the amount of data sent over the network is increased by 40.4%. Note that the simulation presumes that this additionally consumed bandwidth itself does not contribute significantly to congestion. This assumption is only valid if a small fraction of trafhc is voice ([S]). The SPB-FEC curve is convex with increasing po, as due to the increasing loss correlation an increasing number of consecutive packets carrying redundancy are lost leading to unrecoverable losses. The curve for SPB-DIFFMARK is concave, however, yielding better performance for pe & 0.22.
The inter-flow QoS ALT-MARK scheme (50% of the packets are marked) enhances the perceptual quality. However, the auditory distance and the perceptual distortion of the SPB-MARK scheme (with 40.4% of all packets marked) is significantly lower and very close to the quality of the decoded signal when all packets are marked (FULL MARK).
This also shows that by protecting the entire flow only a minor improvement in the perceptual quality is obtained.
The results for the FULL MARK scheme also show that, while the loss of some of the +l packets has some measurable impact, the impact on perceptual quality can still be considered to be very low.
In this paper we have characterized the behaviour of a samplebased codec (PCM) and a frame-based codec (G.729) in the presence of packet loss. We have then developed intraflow loss recovery and control mechanisms to increase the perceptual quality. While we have tested other codecs only informally, we think that our results reflect the fundamental difference between codecs which either encode the speech wave‘We also used the G.729 encoder for the redundant source coding. form directly or which are based on linear prediction. For PCM without loss concealment we have found that it neither exhibits significant temporal sensitivity nor sensitivity to payload heterogeneity. With loss concealment, however, the speech quality is increased but the amount of increase exhibits strong temporal sensitivity. Frame-based codecs amplify on one hand the impact of loss by error propagation, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state.
Contrary to sample-based codecs we have shown that the concealment performance of the G.729 decoder may break at transitions within the speech signal however, thus showing strong sensitivity to payload heterogeneity.
We have briefly introduced a queue management algorithm which is able to control loss patterns without changing the amount of loss and characterized its performance for the loss control of a flow exhibiting temporal sensitivity. Then we developed a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking for an efficient protection of frame-based codecs. The SPBDIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder"s concealment for other frames. We have also evaluated the mapping of an end-to-end algorithm to interflow protection. We have found that the selective marking scheme performs almost as good as the protection of the entire flow at a significantly lower number of necessary highpriority packets.
Thus, combined intra-flow end-to-end / hopby-hop schemes seem to be well-suited for heavily-loaded networks with a relatively large fraction of voice traffic. This is the case because they neither need the addition of redundancy nor feedback (which would incur additional data and delay overhead) and thus yield stable voice quality also for higher loss rates due to absence of FEC and feedback loss. Such schemes can better accomodate codecs with fixed output bitrates, which are difficult to integrate into FEC schemes requiring adaptivity of both the codec and the redundancy generator. Also, it is useful for adaptive codecs running at the lowest possible bitrate. Avoiding redundancy and feedback is also interesting in multicast conferencing scenarios where the end-to-end loss characteristics of the different paths leading to members of the session are largely different. Our work has clearly focused on linking simple end-to-end models which can be easily parametrized with the known characteristic of hopby-hop loss control to user-level metrics. An analysis of a large scale deployment of non-adaptive or adaptive FEC as compared to a deployment of our combined scheme requires clearly further study.
We would like to thank Wonho Yang and Robert Yantorno,
Temple University, for providing the EMBSD software for the objective speech quality measurements. Michael Zander, GMD Fokus, helped with the simulations of the queue management schemes.
Additional author: Georg Carle (GMD Fokus, email: carle@fokus.gmd.de). 450
PI PI [31 PI [51 PI [71 PI PI WI WI w REFERENCES J. Andren, M. Hilding, and D. Veitch. Understanding end-to-end internet traffic dynamics. In Proceedings IEEE GLOBECOM, Sydney, Australia, November
J.-C. Bolot, S. Fosse-Parisis, and D. Towsley.
Adaptive FEC-based error control for interactive audio in the Internet. In Proceedings IEEE INFOCOM, New York, NY, March 1999.
D. Clark and W. Fang. Explicit allocation of best effort packet delivery service. Technical Report, MIT LCS, 1997. http://diffserv.lcs.mit.edu/Papers/expallot-ddc-wf.pdf.
R. Cox and P. Kroon. Low bit-rate speech coders for multimedia communication. IEEE Communications Magazine, pages 34-41, December 1996.
J. Ferrandiz and A. Lazar. Consecutive packet loss in real-time packet traffic. In Proceedings of the Fourth International Conference on Data Communications Systems, IFIP TC6, pages 306-324, Barcelona, June
W. Jiang and H. Schulzrinne. QoS measurement of Internet real-time multimedia services. In Proceedings NOSSDAV, Chapel Hill, NC, June 2000.
C. Perkins, I. Kouvelas, 0. Hodson, M. Handley, and J. Bolot. RTP payload for redundant audio data. RFC 2198, IETF, September 1997. ftp://ftp.ietf.org/rfc/rfc2198.txt.
M. Podolsky, C. Romer, and S. McCanne. Simulation of FEC-based error control for packet audio on the Internet. In Proceedings IEEE INFOCOM, pages 48-52, San Francisco, CA, March 1998.
J. Rosenberg. G. 729 error recovery for Internet Telephony. Project report, Columbia University, 1997.
J. Rosenberg, L. Qiu, and H. Schulzrinne. Integrating packet FEC into adaptive voice playout buffer algorithms on the Internet. In Proceedings IEEE INFOCOM, Tel Aviv, Israel, March 2000.
H. Sanneck. Concealment of lost speech packets using adaptive packetization. In Proceedings IEEE Multimedia Systems, pages 140-149, Austin, TX, June
ftp://ftp.fokus.gmd.de/pub/glone/papers/Sann9806: Adaptive.ps.gz.
H. Sanneck. Packet Loss Recovery and Control for Voice tinsmission over the Internet. PhD thesis,
GMD Fokus / Telecommunication Networks Group,
Technical University of Berlin, October 2000.
P31 PI P51 WI P71 P31 w PI PI http://sanneck.net/research/publications/thesis/ SannOOlOLoss.pdf.
H. Sanneck and G. Carle. A queue management algorithm for intracflow service differentiation in the best effort Internet. In Proceedings of the Eighth Conference on Computer Communications and Networks (ICCCN), pages 419426, Natick, MA,
October 1999. ftp://ftp.fokus.gmd.de/pub/glone/papers/Sann9910: Intra-Flow.ps.gz.
H. Sanneck, N. Le, and A. Wolisz. Efficient QoS support for Voice-over-IP applications using selective packet marking. In Special Session on Error Control Techniques for Real-time Delivery of Multimedia data,
First Intenzational Workshop on Intelligent Multimedia Computing (IMMCN), pages 553-556,
Atlantic City, NJ, February 2000. ftp://ftp.fokus.gmd.de/pub/glone/papers/Sann0002: VoIP-marking.ps.gz.
H. Schulzrinne, J. Kurose, and D. Towsley. Loss correlation for queues with bursty input streams. In Proceedings ICC, pages 219-224, Chicago, IL, 1992.
D. Sisalem and A. Wolisz. LDAf TCP-friendly adaptation: A measurement and comparison study. In Proceedings NOSSDAV, Chapel Hill, NC, June 2000.
International Telecommunication Union. Coding of speech at 8 kbit/s using conjugate-structure algebraic-code-excited linear-prediction (CS-ACELP).
Recommendation G.729, ITU-T, March 1996.
International Telecommunication Union. Objective oualitv measurement of telephone-band (300-3400 Hz) speech codecs. Recommendation P.861, ITU-T,
February 1998.
M. Yajnik, J. Kurose, and D. Towsley. Packet loss correlation in the MBone multicast network: Experimental measurements and markov chain models. Technical Report 95-115, Department of Computer Science, University of Massachusetts,
Amherst, 1995.
M. Yajnik, S. Moon, J. Kurose, and D. Towsley.
Measurement and modelling of the temporal dependence in packet loss. Technical Report 98-78,
Department of Computer Science, University of Massachusetts, Amherst, 1998.
W. Yang and R. Yantorno. Improvement of MBSD scaling noise masking threshold and correlation analysis with MOS difference instead of MOS. In Proceedings ICASSP, pages 673-676, Phoenix, AZ,

Recently, Massively Multiplayer Online Games (MMOGs) have been studied as a framework for next-generation virtual environments. Many MMOG applications, however, still limit themselves to a traditional design approach where their 3D scene complexity is carefully controlled in advance to meet real-time rendering constraints at the client console side.
To enable a virtual landscape in next-generation environments that is seamless, endless, and limitless, Marshall et al. [1] identified four new requirements2 : dynamic extensibility (a system allows the addition or the change of components at run time); scalability (although the number of concurrent users increases, the system continues to function effectively); interactibility; and interoperability. In this paper, we mainly focus on the first two requirements.
Dynamic extensibility allows regular game-users to deploy their own created content. This is a powerful concept, but unfortunately, user-created content tends to create imbalances among the existing scene complexity, causing system-wide performance problems.
Full support for dynamic extensibility will, thus, continue to be one of the biggest challenges for game developers.
Another important requirement is scalability. Although MMOG developers proclaim that their systems can support hundreds of thousands of concurrent users, it usually does not mean that all the users can interact with each other in the same world. By carefully partitioning the world into multiple sub-worlds or replicating worlds at geographically dispersed locations, massive numbers of concurrent users can be supported. Typically, the maximum number of users in the same world managed by a single server or a server-cluster is limited to several thousands, assuming a rather stationary world [2, 3].
Second Life [4] is the first successfully deployed MMOG system that meets both requirements. To mitigate the dynamics of the game world, where a large number of autonomous objects are continuously moving, it partitions the space in a grid-like manner and 2 Originally, these requirements were specified for their dedicated platform. But we acknowledge that these requirements are also valid for new virtual environments. 402 Avatar Object PoppingAutonomous Entities (a) At time t (b) At time t+Δ Figure 1: Object popping occurred as a user moves forward (screenshots from Second Life) where Δ = 2 seconds. employs a client/server based 3D object streaming model [5]. In this model, a server continuously transmits both update events and geometry data to every connected user. As a result, this extensible gaming environment has accelerated the deployment of usercreated content and provides users with unlimited freedom to pursue a navigational experience in its space.
One of the main operations in MMOG applications that stream 3D objects is to accurately calculate all objects that are visible to a user. The traditional visibility determination approach, however, has an object popping problem. For example, a house outside a user"s visible range is not drawn at time t, illustrated in Figure 1(a).
As the user moves forward, the house will suddenly appear at time (t + Δ) as shown in Figure 1(b). If Δ is small, or the house is large enough to collide with the user, it will disrupt the user"s navigational experience.
The visibility calculation for each user not only needs to be accurate, but also fast. This challenge is illustrated by the fact that the maximum number of concurrent users per server of Second Life is still an order of magnitude smaller than for stationary worlds.
To address these challenges, we propose a method that identifies the most relevant visible objects from a given geometry database (view model) and then put forth a fast indexing method that computes the visible objects for each user (spatial indexing). Our two novel methods represent the main contributions of this work.
The organization of this paper is as follows. Section 2 presents related work. Section 3 describes our new view method. In Section 4, we present assumptions on our target application and introduce a new spatial indexing method designed to support real-time visibility computations. We also discuss its optimization issues.
Section 5 reports on the quantitative analysis and Section 6 presents preliminary results of our simulation based experiments. Finally, we conclude and address future research directions in Section 7.
Visibility determination has been widely explored in the field of 3D graphics. Various local rendering algorithms have been proposed to eliminate unnecessary objects before rendering or at any stage in the rendering pipeline. View-frustum culling, back-face culling, and occlusion culling are some of the well-known visibility culling techniques [6]. However, these algorithms assume that all the candidate visible objects have been stored locally.
If the target objects are stored on remote servers, the clients receive the geometry items that are necessary for rendering from the server databases. Teller et al. described a geometry data scheduling algorithm that maximizes the quality of the frame rate over time in remote walkthroughs of complex 3D scenes from a user"s navigational path [5]. Funkhouser et al. showed that multi-resolutional representation, such as Levels Of Detail (LOD), can be used to improve rendering frame rates and memory utilization during interactive visualization [7]. However, these online optimization algorithms fail to address performance issue at the server in highly crowded environments. On the other hand, our visibility computation model, a representative of this category, is based on different assumptions on the data representation of virtual entities.
In the graphics area, there has been little work on supporting real-time visibility computations for a massive number of moving objects and users. Here we recognize that such graphics related issues have a very close similarity to spatial database problems.
Recently, a number of publications have addressed the scalability issue on how to support massive numbers of objects and queries in highly dynamic environments. To support frequent updates, two partitioning policies have been studied in depth: (1) R-tree based spatial indexing, and (2) grid-based spatial indexing. The R-tree is a well-known spatial index structure that allows overlapping between the regions in different branches which are represented by Minimum Bounding Rectangles (MBR). The grid-based partitioning model is a special case of fixed partitioning. Recently, it has been re-discovered since it can be efficient in highly dynamic environments.
Many studies have reported that the R-tree and its variants (R+  tree, R∗ -tree) suffer from unacceptable performance degradation in a highly dynamic environment, primarily due to the computational complexity of the split algorithm [8, 9, 10, 11, 12]. A bottom-up update strategy proposed for R-trees [9] optimizes update operations of the index while maintaining a top down query processing mechanism. Instead of traversing a tree from the root node for frequent update requests (top-down approach), it directly accesses the leaf node of the object to be updated via an object hash table.
Q-Index [13, 11] is one of the earlier work that re-discovers the usefulness of grid-based space partitioning for emerging moving object environments. In contrast to traditional spatial indexing methods that construct an index on the moving objects, it builds an index on the continuous range queries, assuming that the queries move infrequently while the objects move freely. The basic idea of the Q+Rtree [14] is to separate indexing structures for quasistationary objects and moving objects: fast-moving objects are indexed in a Quadtree and quasi-stationary objects are stored in an R∗ -tree. SINA [10] was proposed to provide efficient query evaluations for any combination of stationary/moving objects and stationary/moving queries. Specifically, this approach only detects newly discovered (positive) or no longer relevant (negative) object updates efficiently. Unlike other spatial indexing methods that focus on reducing the query evaluation cost, Hu et al. [12] proposed a general framework that minimizes the communication cost for location updates by maintaining a rectangular area called a safe region around moving objects. As long as any object resides in this region, all the query results are guaranteed to be valid in the system. If objects move out of their region, location update requests should be delivered to the database server and the affected queries are re-evaluated on the fly. Our indexing method is very similar to the above approaches. The major difference is that we are more concentrating on real-time visibility determination while others assume loose timing constraints.
In this section we illustrate how the object popping problem can be associated with a typical view decision model. We then propose our own model, and finally we discuss its strengths and limitations. To begin with, we define the terminologies commonly used throughout this paper.
Entities in a virtual space can be categorized into three types 403 based on their role - autonomous entities, spectator entities, and avatars. The term autonomous entity refers to an ordinary moving or stationary geometric object that can be visible to other entities.
The spectator entity corresponds to a player"s viewpoint, but is invisible to other entities. It has no shape and is represented only by a point location. It is designed to allow a game participant to see from a third-person viewpoint. It functions similar to a camera control in the 3D graphics field. It also has a higher degree of mobility than other entities. The avatar represents a normal game user who can freely navigate in the space and interact with other entities. It possesses both features: its own viewpoint and visibility. For the remainder we use the term object entity to refer to an autonomous entity or an avatar while we use user entity to denote an avatar or a spectator entity.
The visible range of an entity refers to the spatial extent within which any other entity can recognize its existence. It is based on the assumptions that there always exists an optimal visible distance between a user and an object at any given time and every user possesses equal visibility. Thus, the user and the object, only when their current distance is smaller than or equal to the optimal, can see each other. To specify the visible range, much literature in the graphics area [5, 6] uses a circular Area Of Interest (AOI) whose center is the location of an entity. Its omnidirectional nature allows rapid directional changes without any display disruptions at the periphery of the viewable area. However, we employ a squareshaped AOI at the expense of accuracy because the square-shaped spatial extension is very simple and efficient to be indexed in a grid partitioned world.
The traditional view model, which we call user-initiated view model, assumes that a user entity has an AOI while an object entity does not. As the user navigates, she continuously searches for all the entities within her AOI. Due to its simple design and its low indexing overhead, many Location Based Services (LBSs) and game applications use this model.
However, the user-initiated model has a serious object popping problem during navigation. Recall, as shown in Figure 1, that the house that will have appeared at time t + Δ does not appear at time t because the user cannot recognize objects that are outside of her AOI at time t. In fact, it turned out that the side length of her AOI was smaller than the optimal distance between the user and the house at the time t. Therefore, there is no other way but to increase the visible range of the user in this model to make such an experience unlikely. A large AOI, however, may lead to a significant system degradation.
To overcome the object popping problem, we propose a new view model which we call object-initiated view model. All object entities have their own AOI centered at their current location while all spectator entities have no AOI. Every user entity recognizes the objects whose AOIs cover its point location.
The main strengths of the new model are that (1) it has no object popping problem as long as the underlying system can manage the optimal visible range of all object entities correctly and that (2) the content creators can produce an enriched expressiveness of various behavioral and temporal changes. A huge object may have a farther visible range than a small one; an object has a broader visible range during day-time than at night; even during the night the visible range of an object that owns a light source will have a much wider visible area than a non-illuminated object; if an object is located inside a building, its visible range would be constrained by the surrounding structure.
One of the potential arguments against the object-initiated view is that indexing of the spatial extension of an object is too complex to be practical, compared with the user-initiated view. We agree E2 E1 A S Client S Client A Sub-world Server Figure 2: Target system in a 4 × 4 grid partition. that existing spatial indexing methods are inefficient in supporting our view model. To refute this argument, we propose a novel spatial indexing solution detailed in Section 4.4. Our spatial indexing solution offers a very promising performance even with a large number of mobile entities and visibility calculations in real-time.
For the rest of the paper our design scope is limited to a 2D space, although our application is targeted for 3D environments3 .
Note that our view model is not intended to rival a sophisticated visibility decision algorithm such as visibility culling [6], but to efficiently filter out unnecessary entities that do not contribute to the final image. In Section 6.1 we evaluate both models through quantitatively measures such as the degree of expressiveness and the quality of the two view models and we discuss simulation results.
In Section 4.1 we introduce our target application model. Next,
Section 4.2 presents an abstraction of our node and edge structures whose detailed indexing and cell evaluation methods are explained later in Sections 4.3 and 4.4. Several optimization issues for edge indexing follow in Section 4.5.
Our target application assumes both 3D object streaming and sub-world hosting. The sub-world hosting is a collaborative virtual environment where every server hosts one sub-world, thus constructing a single world. Second Life is the classic example of such an approach.
A virtual space is partitioned into equal-sized sub-worlds. The sample sub-world separated with bold-dashed lines in Figure 2 contains four virtual entities: two autonomous entities (E1, E2); one spectator entity S; and one avatar A. As mentioned in Section 3, all object entities (E1, E2, A) have their own square-shaped AOI. Two user entities (S, A) are associated with individual client machines, (client S and client A in the figure). The spatial condition that the point location of S resides inside the AOI of E2 can be symbolized as S.P ∈ E2.R.
Every sub-world is managed by its dedicated server machine.
Each server indexes all the entities, delivers any new events (i.e., a new user enters into the sub-world or an object moves from one place to another) to clients, and resolves any inconsistencies among the entities. For efficient management of moving entities, a server further divides its sub-world into smaller partitions, called grid cells. Figure 2 shows the 4 × 4 grid enclosed by the dashed lines.
Instead of indexing the object entities with a user entity structure, our system indexes their visible regions on the grid cells. Retrieval of the indexed objects for a given user includes the search and de3 A better indexing method for a 3D space is work in progress. 404 Tokens: IT(E1) IT(E2) IT(A) IT(S) Tokens: AT(E1) DT(E1) AT(E2) DT(E2) AT(A) DT(A) IT(S) (a) node indexing (b) edge indexing (c) edge indexing with row-wise cell evaluation Figure 3: Illustration of different data structures for node indexing and edge indexing for the sample space in Figure 2. There are three object entities, {E1, E2, A}, and two user entities, {S, A} in the world. livery of the indices stored on the cell it is located in. This retrieval process is interchangeably called a user (or query) evaluation.
Our application only considers the efficient indexing of virtual entities and the search for the most relevant entities - that is, how many entities per sub-world are indexed and how quickly index updates are recognized and retrieved. Efficient delivery of retrieved real geometry data is out of the scope of this paper.
We define a token as an abstraction of a virtual entity that satisfies a specific spatial relationship with a given cell. In our application, we use three types of tokens: Inclusion Token (IT) indicates that its entity overlaps with or is covered by the given cell.
Appearance Token (AT) denotes that its entity is an IT for the given cell, but not for the previously adjacent cell.
Disappearance Token (DT) is the opposite of AT, meaning that while its entity does not satisfy the IT relationship with the given cell, it does so with the previously adjacent cell.
We also define two data structures for storing and retrieving the tokens: a node and an edge. A node is a data structure that stores ITs of a cell. Thus, the node for cell i is defined as a set of IT entities and formally expressed as Ni = {o|o.R∩i.R = ∅}, where R is either an AOI or a cell region. An edge is another data structure for two adjacent cells that stores their ATs or DTs. If the edge only stores the AT entities, it is termed an Appearance Edge (AE); otherwise, if it stores DTs, it is termed a Disappearance Edge (DE).
The AE for two adjacent cells i and j is defined as a set of ATs and expressed as E+(i, j) = Nj − (Ni ∩ Nj ) (1) where Ni and Nj are the node structures for the cells i and j. The DE for two adjacent cells i, j is defined as a set of DTs, satisfying: E−(i, j) = Ni − (Ni ∩ Nj ) (2) In a 2D map, depending on the adjacency relationship between two neighboring cells, edges are further classified as either rowwise, if two neighbors are adjacent horizontally (Er ), or columnwise, if they are adjacent vertically (Ec ). Consequently, edges are of four different types, according to their token type and adjacency: Er +(i, j), Er −(i, j), Ec +(i, j), and Ec −(i, j).
Grid partitioning is a popular space subdivision method that has recently gained popularity for indexing moving entities in highly dynamic virtual environments [12, 8, 13, 10]. To highlight the difference to our newly proposed method, we term all existing grid partitioning-based indexing methods node indexing. Node indexing partitions the space into equi-sized subspaces (grid cells), indexes entities on each cell, and searches for entities that satisfy a spatial condition with a given query.
In many LBS applications, node indexing maintains a node structure per cell and stores an index of entities whose spatial extent is a point location. For a given range query, a search is performed from the node structures of the cells whose region intersects with the spatial extent of the range query. Due to the use of a simple point geometry for entities, this allows for lightweight index updates. Much of the existing work falls into this category.
However, if the spatial extent of an entity is a complex geometry such as rectangle, node indexing will suffer from significant system degradation due to expensive update overhead. For example, a single movement of an entity whose spatial extent overlaps with 100 grid cells requires 100 token deletions and 100 token insertions, in the worst case. One of the popular node indexing methods, Query Indexing, has been reported to have such performance degradation during the update of rectangle-shaped range queries [13].
For the sample space shown in Figure 2, the concept of node indexing is illustrated in Figure 3(a). Every cell stores IT entities that intersect with its region. Query processing for the spectator S means to search the node structure whose cell region intersects with S. In Figure 3(a), E2 is indexed on the same cell, thus being delivered to the client S after the query evaluation.
Our new indexing method, edge indexing, is designed to provide an efficient indexing method for the specific spatial extension (square) of the entities in a grid. Its features are (1) an edge structure and (2) periodic entity update and cell evaluation.
Edge Structure The main characteristic of our approach is that it maintains edge structures instead of using node structures. With this approach, redundant ITs between two adjacent cells (Ni ∩Nj ) are eliminated.
In a 2D M × M grid map, each cell i is surrounded by four neighboring cells (i− 1), (i+ 1), (i− M), (i+ M) (except for the 405 outermost cells) and eight different edge structures. If the first two neighbor cells are horizontally adjacent to i and the last two cells (i−M), (i+M) are vertically nearby, the eight edge structures are Ec +(i−M, i), Ec −(i−M, i), Er +(i−1, i), Er −(i−1, i), Er +(i, i+ 1), Er −(i, i + 1), Ec +(i, i + M), and Ec −(i, i + M).
Figure 3(b) illustrates how edge structures are constructed from node structures, using Equations 1 and 2. Inversely, the cell evaluation process with edge indexing derives node structures from the edge structures. If any node structure and all the edge structures are known a priori, we can derive all the node structures as defined by Lemma 1. The proof of Lemma 1 is trivial as it is easily induced from Equations 1 and 2.
Lemma 1. Nj , a set of ITs of a given cell j can be derived from a set of ITs of its neighbor cell i, Ni and its edges E+(i, j) − E−(i, j): Nj = Ni + E+(i, j) − E−(i, j) Row-wise and column-wise edge structures, however, capture some redundant information. Thus, na¨ıve edge indexing stores more tokens than node indexing - the total number of edge tokens shown in Figure 3(b) is 35 (17 ATs + 17 DTs + 1 IT); for node indexing in Figure 3(a) the number is 25. To reduce such redundancy, a subsequent two-step algorithm can be applied to the original edge indexing.
Periodic Entity Update and Cell Evaluation Many objects are continuously moving and hence index structures must be regularly updated. Generally, this is done through a twostep algorithm [13] that works as follows. The algorithm begins by updating all the corresponding indices of newly moved entities (the entity update step) and then computes the node structures of every cell (the cell evaluation step). After one cell evaluation, the indexed user entities are retrieved and the computed node structure is delivered for every client that is associated with a user. After all the cells are evaluated, the algorithm starts over.
The two-step algorithm can also be used for our edge indexing by updating the edge structures of the entities that moved during the previous time period and by applying Lemma 1 during the cell evaluations. In addition to this adaptability, the Lemma also reveals another important property of cell evaluations: either row edges or column edges are enough to obtain all the node structures.
Let us assume that the system maintains the row-wise edges.
The leftmost node structures are assumed to be obtained in advance. Once we know the node structure of the leftmost cell per row, we can compute that of its right-hand cell from the leftmost node structure and the row-wise edges. We repeat this computation until we reach the rightmost cell. Hence, without any column-wise edges we can obtain all the node structures successfully. As a result, we reduce the complexity of the index construction and update by a factor of two.
Figure 3(c) illustrates the concept of our row-wise edge indexing method. The total number of tokens is reduced to 17 (8 ATs + 8 DTs + 1 IT). The detailed analysis of its indexing complexity is presented in Section 5.
Figure 4 illustrates how to construct edge structures from two nearby cells. In the figure, two row-wise adjacent cells 3 and 4 have two row-wise edge transitions between them, E+(3, 4), E−(3, 4); two point entities P1, P2; and two polygonal entities R1, R2.
As shown in the figure, N3 indexes {P2, R1, R2} and N4 maintains the indices of {P1, R2}. E+(3, 4) is obtained from Equation 1: N4 − (N3 ∩ N4) = {P1}. Similarly, E−(3, 4) = N3 − Cell 3 Cell 4 E+(3, 4)={P1} E_(3, 4)={P2,R1} P2 P1 R2 R1 Figure 4: Example of edge indexing of two point entities {P1, P2} and two polygonal entities {R1, R2} between two row-wise adjacent cells. (N3 ∩ N4) = {P2, R1}. If we know N3, E+(3, 4), and E−(3, 4), we can compute N4 according to Lemma 1, N4 = N3+E+(3, 4)− E−(3, 4) = {P1, R2}.
The above calculation also corresponds to our intuition. P2, R1,
R2 overlap with cell 3 while P1, R2 overlap with cell 4. When transiting from cell 3 to 4, the algorithm will recognize that P2, R1 disappear and P1 is newly appearing while the spatial condition of R2 is unchanged. Thus, we can insert P2, R1 in the disappearance edge set and insert P1 in the appearance edge set.
Obviously, edge indexing is inefficient for indexing a point geometry. Node indexing has one IT per point entity and requires one token removal and one insertion upon any location movement.
Edge indexing, however, requires one AT and one DT per point entity and two token removals and two insertions during the update, in the worst case. In such a situation, we take advantage of using both according to the spatial property of entity extension. In summary, as shown in Figure 3(c), our edge indexing method uses edge structures for the AOI enabled entities (A, E1, E2) while it uses node structures for the point entity (S).
In this section, we describe several optimization techniques for edge indexing, which reduces the algorithm complexity significantly.
Typically, there exist two practical policies for a region update: Full Update simply removes every token of the previous entity region and re-inserts newly updated tokens into newly positioned areas.
Incremental Update only removes the tokens whose spatial relationship with the cells changed upon an update and inserts them into new edge structures that satisfy the new spatial conditions.
Entities from Stationary Entities So far, we have not addressed any side-effect of token removals during the update operation. Let us assume that an edge index is realized with a hash table. Inserting a token is implemented by inserting it at the head of the corresponding hash bucket, hence the processing time becomes constant. However, the token removal time depends on the expected number of tokens per hash bucket.
Therefore, the hash implementation may suffer from a significant system penalty when used with a huge number of populated entities.
Two-table edge indexing is designed to make the token removal overhead constant. First, we split a single edge structure that indexes both stationary and moving entities into two separate edge 406 Table 1: Summary of notations for virtual entities and their properties.
Symbol Meaning U set of populated object entities O set of moving object entities, O ⊆ U Uq set of populated user entities Q set of moving user entities, Q ⊆ Uq A set of avatars, A = {a|a ∈ U ∩ Uq} i.P location of entity i where i ∈ (U ∪ Uq) i.R AOI of entity i where i ∈ (U ∪ Uq) mi side length of entity i where i ∈ (U ∪ Uq). It is represented by the number of cell units. m average side length of the AOI of entities V ar(mi) variance of random variable mi v maximum reachable distance. It is represented by the number of cell units. structures. If an entity is not moving, its tokens will be placed in a stationary edge structure. Otherwise, it will be placed with a moving edge.
Second, all moving edge structures are periodically reconstructed.
After the reconstruction, all grid cells are evaluated to compute their visible sets. Once all the cells are evaluated, the moving edges are destroyed and the reconstruction step follows. As a result, search operations on the moving edge structures are no longer necessary and the system becomes insensitive to any underlying distribution pattern and moving speed of the entities. A singly linked list implementation is used for the moving edge structure.
We analyze three indexing schemes quantitatively (node indexing, edge indexing, and two-table edge indexing) in terms of memory utilization and processing time. In this analysis, we assume that node and edge structures are implemented with hash tables.
For hash table manipulations we assume three memory-access functions: token insertion, token removal, and token scan. Their processing costs are denoted by Ta, Td, and Ts, respectively. A token scan operation reads the tokens in a hash bucket sequentially. It is extensively used during cell evaluations. Ts and Td are a function of the number of tokens in the bucket while Ta is constant.
For the purpose of analysis, we define two random variables.
One variable, denoted by mo, represents the side length of the AOI of an entity o. The side lengths are uniformly distributed in the range of [mmin, mmax]. The average value of mo is denoted by m. The second random variable v denotes the x-directional or ydirectional maximum distance of a moving entity during a time interval. The simulated movement of an entity during the given time is also uniformly distributed in the range of [0, v]. For a simple calculation, both random variables are expressed as the number of cell units.
Table 1 summarizes the symbolic notations and their meaning.
Let the token size be denoted by s. Node indexing uses s · |Uq| memory units for user entities and s · Èo∈U (mo + 1)2 ≈ s(m2 + 2m + 1 + V ar(mo))|U| units for object entities. Single-table edge indexing consumes s · |Uq| storage units for the user entities and s · Èo∈U 2(mo + 1) ≈ 2s(m + 1)|U| for the object entities.
Two-table edge indexing occupies s · |Uq| units for the users and s{ Èi∈O 2(mi+1)+ Èj∈(U−O) 2(mj +1)} ≈ 2s(m+1)|U| units for the objects. Table 2 summarizes these results. In our target apTable 2: Memory requirements of different indexing methods. indexing method user entities object entities node indexing s · |Uq| s((m + 1)2 + V ar(mo))|U| single-table edge s · |Uq| 2s(m + 1)|U| two-table edge s · |Uq| 2s(m + 1)|U| plication, our edge indexing methods consume approximately m+1 2 times less memory space than node indexing.
Different grid cell partitioning with edge methods will lead to different memory requirements. For example, here are two grids: a M × M grid and a 2M × 2M grid. The memory requirement for the user entities is unchanged because it depends only on the total number of user entities. The memory requirements for the object entities are approximately 2s(m + 1)|U| in the M × M grid case and 2s(2m + 1)|U| for the (2M) × (2M) grid. Thus, a four times larger cell size will lead to an approximately two times smaller number of tokens.
In this section, we focus on the cost analysis of update operations and cell evaluations. For a fair comparison of the different methods, we only analyze the run-time complexity of moving objects and moving users.
We assume that a set of moving objects O and a set of moving users Q are known in advance.
Similar to edge indexing, node indexing has two update policies: full update and incremental update. Full update, implemented in Q-Index [13] and SINA [10], removes all the old tokens from the old cell node structures and inserts all the new tokens into the new cell nodes. The incremental update policy, implemented by no existing work, removes and inserts all the tokens whose spatial condition changed during a period. In this analysis, we only consider incremental node indexing.
To analyze the update cost of node indexing, we introduce the maximum reachable distance (v), where the next location of a moving entity, whose previous location was at cell(0,0), is uniformly distributed over the (±v, ±v) grid cell space as illustrated in Figure 5. We also assume that the given maximum reachable distance is less than any side length of the AOI of the objects in the system; that is, v < mo where o ∈ O. As seen in Figure 5, the next location may fall into three categories: areas A, B, and the center cell area (0,0). If an object resides in the same cell, there will be no update. If the object moves into the area A, there will be (i + j)(mo + 1) − ij token insertions and removals, where 1 ≤ i, j ≤ v. Otherwise, there will be k(mo + 1) token insertions and removals, where 1 ≤ k ≤ v. Thus, the expected processing time of an object update for node indexing is the summation of three different movement types T node per update(o) = 4 · (A) + 4 · (B) (2v + 1)2 · (Ta + Td) = v(v + 1){v(4mo + 3 − v) + 2(mo + 1)} (2v + 1)2 · (Ta + Td) (3) and the expected processing time of any object for node indexing is obtained by T node per update = Èo∈O,v<mo T node per update(o) |O| = v(v + 1){v(4m + 3 − v) + 2m + 1)} (2v + 1)2 · (Ta + Td) (4) . 407 Table 3: Update time cost for any single update event where v < mq, mo and q ∈ Q. indexing method queries ×(Ta + Td) (seconds) objects ×(Ta + Td) (seconds) node indexing with incremental update |Q| |O| · v(v+1){v(4m+3−v)+2(m+1)} (2v+1)2 single-table edge indexing with full update |Q| |O| · 2(m + 1) single-table edge indexing with incremental update |Q| |O| · v(4m(1+2v)+9v+5) (2v+1)2 two-table edge indexing |Q| · Ta Ta+Td |O| · 2(m + 1) Ta Ta+Td Maximum Reachable Distance (v) (0,0) i j (i,j) A A AA B B B B Figure 5: Illustration of next cell location, cell(i, j), of a moving entity whose initial location was at cell (0, 0).
The expected time of any single entity update for edge indexing with full update is: T edgefull per update = Èo∈O T edgefull per update (o) |O| = 2(m + 1)(Ta + Td) (5) The analysis of the expected time of any single entity update for edge indexing with incremental update becomes complicated because the time cost depends both on the side length of the entity AOI and on the moving speed. Roughly speaking, its worst-case processing cost is the same as Tedgefull per update . Due to space limitations we only show the analysis result of the expected processing time when v of any object o ∈ O is smaller than mo: Tedgeincremental per update = Èo∈O,v<mo Tedgeincremental per update (o) |O| = v(4m(1 + 2v) + 9v + 5) (2v + 1)2 · (Ta + Td) = v(4m(1 + 2v) + 9v + 5) (2v + 1)2 · 2(m + 1) · Tedgefull per update (6) All update complexities are summarized in Table 3. In this table, it is evident that while the update cost of the worst-case edge indexing (single-table edge indexing with full update policy) depends only on m, that of the best-case node indexing (node indexing with incremental update policy) is still proportional to two variables, v and m. For a smaller value of v (v = 1), the update cost of node indexing slightly outperforms that of edge indexing (i.e., 12m+8 9 vs. 2(m + 1)). However, as v increases, the performance gain is then immediately reversed (i.e., 60m+24 25 versus 2(m + 1), where v = 2).
Another interesting result is that two-table edge indexing depends only on the token insertion cost, Ta. Typically, Td is slightly 1 2 3 4 5 0 5 10 15 20 25 30 35 40 Maximum Reachable Distance (v) (%) #ofAffectedTokens Two−table Edge Indexing Incremental Edge Indexing Full Edge Indexing Incremental Node Indexing Figure 6: Simulation results of update complexity of different indexing methods. The update complexity, the expected number of token removals and insertions per object update, is drawn as a function of maximum reachable distance (v). The average side length of object AOIs is 10% of the side length of a given 2-D map. greater than Ta because Td requires at least one token lookup operation. After the lookup, Td executes the reverse operation of Ta.
Thus, Td may well be expressed as (Ta + Tlookup) and can be simplified as (Ta + |E| 2·b ·Ts) where |E| is the size of the edge structure and b is the number of its hash buckets. From this observation, we can infer that full update of single-table edge indexing takes at least twice as long as the update for two-table edge indexing.
Figure 6 shows that full update of edge indexing when the maximum reachable distance is less than the side length of any moving entities takes constant time to update the corresponding edge structures, which mainly depends on the side length of the given AOI.
In this figure we assume that the average side length of the AOI is
only on the side length but also on the reachable distance. Thus the entity update in node indexing is much heavier than the full update for edge indexing.
As expected, these simulation results validate a common belief that in less dynamic environments, incremental updates reduce the amount of token insertions and removals noticeably while in extremely dynamic environments the reduction ratio becomes negligible.
Node indexing scans all entities and then collects the user entities indexed on every cell node. Therefore, it would take |Q|×Ts to scan all user entities. If every node stores (m2 +2m+1+V ar(mo))|O| M2 object entities on average, the expected completion time of one cell evaluation will then be Èo∈O (m2 +2m+1+V ar(mo))|O| M2 · Ts. If every cell has at most one user entity, the expected completion time of all cell evaluations will be |Q| · (m2 +2m+1+V ar(mo))|O| M2 · Ts.
The runtime complexity of the single-table cell evaluation can 408 Table 4: Summary of cell evaluation cost. indexing method expected elapsed time node indexing Ts · |Q| · (m2 +2m+1+V ar(mo))|O| M2 single-table edge Ts · (|Q| + |O| · 2(m + 1)) two-table edge (Ts + Td) · (|Q| + |O| · 2(m + 1)) be simplified as Ts ·|O|·2(m +1). In this analysis, we do not consider any data delivery overhead after a cell evaluation. Note that in single-table edge indexing we need to scan all the tokens for cell evaluations. Two-table edge indexing executes in Td to remove the evaluated tokens after a cell evaluation. Unlike the Td operation, the Td operation is much lighter because it does not require any lookup operation.
Table 4 shows the expected complexities of different cell evaluation scenarios. If previously computed result sets are re-used during the next evaluation round, the expected elapsed time of node indexing will be bound by the total number of cell evaluations (i.e.,
Ts(m2 + 2m + 1 + V ar(mo))|O|). However, in the worst case, the cell evaluation of node indexing is still m+1 2 times longer than that of any edge indexing method.
As we saw in Section 5.2.1, edge indexing methods outperform node indexing in terms of updates and cell evaluations. In this section we focus on evaluating the performance difference between single-table edge indexing and two-table edge indexing.
The total elapsed time of full update based single-table edge indexing for a given set of moving entities is the summation of the elapsed time of updates and cell evaluations: (Ta + Td + Ts) · {|Q| + |O|2(m + 1)} (7) Similarly, the total elapsed time of two-table edge indexing is as follows: (Ta + Td + Ts) · {|Q| + |O|2(m + 1)} (8) From Equation 7 and 8 we conclude that two-table edge indexing, even though it represents a minor optimization of single-table edge indexing by replacing unpredictable Td with predictable Td, achieves a significant performance improvement. First of all, Td is very predictable and a more lightweight procedure than Td. All the data structure manipulation overheads, such as Ta, Ts, and Td can be easily profiled and all become constant. In addition, twotable indexing is guaranteed to outperform single-table full update edge indexing. Another novelty of the two-table approach is that it is highly resilient to the underlying data distribution, regardless of whether it is highly skewed or uniform.
Equation 8 also reveals the minimum time interval that satisfies the given input parameters, Ta, Ts, Td, |Q|, |O|, and m. While Ta,
Ts and Td are system-specific parameters, |O|, |Q|, and m are all application-specific. The latter can be configured by the former and any given real-time constraint T. Thus, the system throughput - how many moving objects and users are supported by the given system - is obtained from Equation 9.
Maximum System Throughput = |Q| + |O|2(m + 1) = T Ts + Ta + Td (9) For example, if a given sub-world is only filled with moving avatars, A = Q = O, whose average side length is 10% of the map side length, then Ts + Td takes 0.42 microseconds per token evaluation, and Ta takes 0.78 microseconds, and the system will handle about 36,231 avatars per second. Every avatar can navigate in the sub-world freely and the same number of remotely connected clients receive the latest update events continuously.
This section presents two simulation setups and their performance results. Section 6.1 examines whether our new view approach is superior to existing view models, in spite of its higher indexing complexity. Section 6.2 discusses the degree of practicality and scalability of our indexing method that is designed for our new view model.
To quantify the quality of the retrieved results of query processing, we use two widely known evaluation metrics, Precision (P) and Recall (R), that estimate the degree of accuracy and comprehensiveness of a given result set [15]. P is the ratio of relevant, retrieved items to all retrieved items. A lower value of P implies that the query result set contains a large number of unnecessary objects that do not have to be delivered to a client. A higher P value means a higher network traffic load than required. R is the ratio of relevant, retrieved items to all relevant items. A lower R value means that more objects that should be recognized are ignored. From the R measure, we can quantitatively estimate the occurrence of object popping.
In addition to the P and R metrics, we use a standardized singlevalued query evaluation metric that combines P and R, called Emeasure [15]. The E-measure is defined as: E = 1 − (β2 + 1)PR β2P + R where β is the relative importance of P or R. If β is equal to 1, P and R are equally important. If β is less than 1, P becomes more important. Otherwise, R will affect the E-measure significantly.
A lower E-measure value implies that the tested view model has a higher quality. The best E-measure value is zero, where the best values for P and R are both ones.
We tested four query processing schemes, which use either a user-initiated or an object-initiated view model: • User-initiated visibility computation - RQ-OP: Region Query - Object Point • Object-oriented visibility computation - PQ-OR: Point Query - Object Region - RQ-OR: Region Query - Object Region - ACQ-OR: Approximate Cell Query - Object Region RQ-OP is the typical computation scheme that collects all objects whose location is inside a user defined AOI. PQ-OR collects a set of objects whose AOI intersects with a given user point, formally {o|q.P ∈ o.R}. RQ-OR, an imaginary computation scheme, is the combination of RQ-OP and PQ-OR where the AOI of an object intersects with that of a user, {o|o.R ∩ q.R = ∅}.
Lastly, ACQ-OR, an approximate visibility computation model, is a special scheme designed for grid-based space partitioning, which is our choice of cell evaluation methodology for edge indexing. If a virtual space is partitioned into tiled cells and a user point belongs to one of the cells, the ACQ-OR searches the objects whose AOI 409 Table 5: P and R computations of different visibility determination schemes.
Scheme P R RQ-OP |{o|o.P ∈q.R∧q.P ∈o.R)}| |{o|o.P ∈q.R}| |{o|o.P ∈q.R∧q.P ∈o.R)}| |{o|q.P ∈o.R}| PQ-OR |{o|q.P ∈o.R}| |{o|q.P ∈o.R}| = 1 |{o|q.P ∈o.R}| |{o|q.P ∈o.R}| = 1 RQ-OR |{o|q.P ∈o.R}| |{o|q.R∩o.R=∅}| |{o|q.P ∈o.R}| |{o|q.P ∈o.R}| = 1 ACQ-OR |{o|q.P ∈o.R}| |{o|c.R∩o.R=∅,q.P ∈c.R}| |{o|q.P ∈o.R}| |{o|q.P ∈o.R}| = 1 would intersect with the region of the corresponding grid cell. Of course, it exhibits similar properties as RQ-OR while the result set of its query is not a subset of the RQ-OR query result. It identifies any object o satisfying the condition c.R ∩ o.R = ∅ where the cell c satisfies q.P ∈ c.R as well.
Our simulation program populated 100K object entities and 10K user entities in a 2D unit space, [0, 1) × [0, 1). The populated entities are uniformly located in the unit space. The side length of their AOI is also uniformly assigned in the range of [0.05, 0.14], meaning 5% to 14% of the side length of the unit space. The program performs intersection tests between all user and all object entities exhaustively and computes the P, R, and E-measure values (shown in Table 5).
Distribution of P and R measure: Figure 7 shows the distribution of P and R for RQ-OP. We can observe that P and R are roughly inversely proportional to each other when varying a user AOI range. A smaller side length leads to higher accuracy but lower comprehensiveness. For example, 5% of the side length of a user AOI detects all objects whose side length of the AOI is at least 5%. Thus, every object retrieved by RQ-OP is guaranteed to be all rendered at the client. But RQ-OP cannot detect the objects outside the AOI of the user, thus suffering from too many missing objects that should be rendered. Similarly, the user whose AOI is wider than any other AOI cannot miss any objects that should be rendered, but detects too many unnecessary objects. To remove any object popping problem, the side length of any AOI should be greater than or equal to the maximum visible distance of any object in the system, which may incur significant system degradation.
E-measure Distribution: Figure 8 reveals two trends. First, the precision values of RQ-OP lie in between those of ACQ-OR (100 × 100 grid) and RQ-OR. Second, the tendency curve of the Precision-to-E-measure plot of RQ-OR shows resemblance to that of ACQ-OR. It looks as if the two curves lie on the same imaginary curve, which conveys that ACQ-OR inherits the properties of RQ-OR.
Effect of Different Grid Size: Figure 9 shows the statistical difference of E-measure values of seven different grid partitioning schemes (using ACQ-OR) and one RQ-OP model. We use a boxand-whisker plot to show both median values and the variances of E-measure distributions and the outliers of each scheme. We also draw the median value of the RQ-OP E-measures (green line) for comparison purposes. While the ACQ-OR schemes have some outliers, their E-measure values are heavily concentrated around the median values, thus, they are less sensitive to object AOI. As expected, fine-grained grid partitioning showed a smaller E-measure value. The RQ-OP scheme showed a wider variance of its quality than other schemes, which is largely attributable to different user side lengths. As the R measure becomes more important, the query quality of ACQ-OR is improved more evidently than that of RQOP. From Figure 9, the 20×20 grid scheme had a better E-measure Table 6: Measured elapsed time (seconds) of 100K moving objects and 10K moving users in a slowly moving environment (v = 1). indexing Update Time Evaluation Time Total Single-tableF ull
Single-tableIncr
Two-table 1.74 0.93 2.67 Table 7: Measured elapsed time (seconds) of 100K moving objects and 10K moving users in a highly dynamic environment (v = 15). indexing Update Time Evaluation Time Total Single-tableF ull
Single-tableIncr
Two-table 1.75 0.93 2.68 value in a prioritized environment than in an equal-prioritized environment. As a result, we can roughly anticipate that at least the 20×20 grid cell partitioning retrieves a higher quality of visible sets than the RQ-OP.
In this section, we present the preliminary results of the simulations that examine the applicability of our edge indexing implementation. To estimate the degree of real-time support of our indexing method, we used the total elapsed time of updating all moving entities and computing visible sets for every cell. We also experimented with different grid partitioning policies and compared them with exhaustive search solutions.
We implemented edge indexing algorithms in C and ran the experiments on a 64-bit 900MHz Itanium processor with 8 GBs of memory. We implemented a generalized hash table mechanism to store node and edge structures.
Periodic Monitoring Cost: Tables 6 and 7 show the performance numbers of different edge indexing methods by varying v.
The moving speed of entities was also uniformly assigned between 0 and v. In a slowly moving environment (Table 6), the incremental edge indexing method outperforms full update edge indexing, due to reduced index updates; the two-table approach surpasses the performance of single-table schemes, mainly due to the lack of token lookup during an update. However, the two-table method showed a slightly higher evaluation time than the two single-table methods because of its sequential token removal.
Table 7 exemplified the elapsed time of index updates and cell evaluations in a highly dynamic environment where slowly moving and dynamically moving objects co-exist. Compared with the results shown in Table 6, the two-table approach produced similar performance numbers regardless of the underlying moving environments. However, the performance gain obtained by the incremental policy of the single-table is decreased compared with that in the slowly moving environment.
Effect of Different Grid Size: How many object updates and cell evaluations can be supported in a given time period is an important performance metric to quantify system throughput. In this section, we evaluate the performance results of three different visibility computation models: two computation-driven exhaustive search methods; and one two-table edge indexing method with different grid sizes. 410 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
1 Recall (R) Precision(P) 5% Range Query 6% Range Query 7% Range Query 8% Range Query 9% Range Query 10% Range Query 11% Range Query 12% Range Query 13% Range Query 14% Range Query Optimality 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
1 Precision (P) E−measure Optimality RQ−OP RQ−OR ACQ−OR (100x100 grid cells) 10x10 20x20 50x50 100x100 200x200 500x500 1Kx1K RQ−OP 0
1 E−Measure Visibility Test Scheme Median of RQ−OP Figure 7: Distribution of P and R measured by RQ-OP.
Figure 8: E-measure value as a function of Precision value P when β = 1.
Figure 9: E-measure value as a function of ACQ-QR grid partitioning scheme when β = 2. 10K 50K 100K 10 −1 10 0 10 1 10 2 # of Object Updates (10K queries) TotalElapsedTime(seconds) Population Size = 100K, visible range = 5−15%, mobility = 1% Exhaustive Search (Intersection Test) Exhaustive Search (Euclidean Distance Measure) 200x200 Two−table Edge Indexing 100x100 Two−table Edge Indexing 50x50 Two−table Edge Indexing Figure 10: Total elapsed time of different indexing schemes.
Exhaustive search methods do not maintain any intermediate results. They simply compute whether a given user point is inside a given object AOI. They can tolerate unpredictable behavior of object movement. In spite of their simple design and extensibility, they suffer from lengthy computational delays to complete the visibility determination. Figure 10 reveals the performance difference between the exhaustive solutions and the two-table methods, a difference of up to two orders of magnitude.
As shown in Section 5, the total elapsed time of object updates and cell evaluations is linear with respect to the average side length of object AOI. Because the side length is represented by cell units, an increase in the number of cells increases the side lengths proportionally. Figure 10 illustrates that the measured simulation results roughly match the expected performance gain computed from the analysis.
To support dynamic extensibility and scalability in highly dynamic environments, we proposed a new view paradigm, the object-initiated view model, and its efficient indexing method, edge indexing.
Compared with the traditional view model, our new view model promises to eliminate any object popping problem that can easily be observed in existing virtual environments at the expense of increased indexing complexity. Our edge indexing model, however, can overcome such higher indexing complexity by indexing spatial extensions at edge-level not at node-level in a grid partitioned sub-world and was validated through quantitative analyses and simulations.
However, for now our edge indexing still retains a higher complexity, even in a two-dimensional domain. Currently, we are developing another edge indexing method to make the indexing complexity constant. Once indexing complexity becomes constant, we plan to index 3D spatial extensions and multi-resolutional geometry data. We expect that our edge indexing can contribute to successful deployment of next-generation gaming environments.
[1] D. Marshall, D. Delaney, S. McLoone, and T. Ward, Challeges in modern distributed interactive application design, Tech. Rep., Department of Computer Science, National University of Ireland, Maynooth, Maynooth, Col.
Kildare, Ireland, 2004. [2] Kuan-Ta Chen, Polly Huang, Chun-Ying Huang, and Chin-Laung Lei, Game traffic analysis: An MMORPG perspective, in NOSSDAV"05. 2005, pp. 19-24, ACM Press. [3] Jaecheol Kim, Jaeyoung Choi, Dukhyun Chang, Taekyoung Kwon, and Yanghee Choi, Traffic charateristics of a massively multiplayer online role playing game and its implications, in NetGames "05, Oct 2005. [4] Philip Rosedale and Cory Ondrejka, Enabling player-created online worlds with grid computing and streaming, Gamastutra Magazine, http://www.gamasutra.com/resource guide/20030916/rosedale 01.shtml,
September 2003. [5] Eyal Teler and Dani Lischinski, Streaming of complex 3d scenes for remote walkthroughs., Comput. Graph. Forum, vol. 20, no. 3, 2001. [6] Gerd Hesina and Dieter Schmalstieg, A network architecture for remote rendering, in Second International Workshop on Distributed Interactive Simulation and Real-Time Applications, 1998. [7] Thomas A. Funkhouser and Carlo H. Sequin, Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments, in SIGGRAPH "93, New York, NY, USA, 1993, pp. 247-254, ACM Press. [8] Jussi Myllymaki and James Kaufman, High-performance spatial indexing for location-based services, in WWW "03, New York, NY, USA, 2003, pp. 112-117, ACM Press. [9] M. Lee, W. Hsu, C. Jensen, B. Cui, and K. Teo, Supporting frequent updates in r-trees: A bottom-up approach, in VLDB, pages 608-619, 2003, 2003. [10] Mohamed F. Mokbel, Xiaopeing Xiong, and Walid G. Aref, Sina: scalable incremental processing of continuous queries in spatio-temporal databases, in ACM SIGMOD "04. 2004, pp. 623-634, ACM Press. [11] Dmitri V. Kalashnikov, Sunil Prabhakar, and Susanne E. Hambrusch, Main memory evaluation of monitoring queries over moving objects, Distrib.

Achieving end-to-end real-time quality of service (QoS) is particularly important for open distributed real-time and embedded (DRE) systems that face resource constraints, such as limited computing power and network bandwidth.
Overutilization of these system resources can yield unpredictable and unstable behavior, whereas under-utilization can yield excessive system cost. A promising approach to meeting these end-to-end QoS requirements effectively, therefore, is to develop and apply adaptive middleware [10, 15], which is software whose functional and QoS-related properties can be modified either statically or dynamically. Static modifications are carried out to reduce footprint, leverage capabilities that exist in specific platforms, enable functional subsetting, and/or minimize hardware/software infrastructure dependencies. Objectives of dynamic modifications include optimizing system responses to changing environments or requirements, such as changing component interconnections, power-levels, CPU and network bandwidth availability, latency/jitter, and workload.
In open DRE systems, adaptive middleware must make such modifications dependably, i.e., while meeting stringent end-to-end QoS requirements, which requires the specification and enforcement of upper and lower bounds on system resource utilization to ensure effective use of system resources. To meet these requirements, we have developed the Hybrid Adaptive Resource-management Middleware (HyARM), which is an open-source1 distributed resource management middleware.
HyARM is based on hybrid control theoretic techniques [8], which provide a theoretical framework for designing control of complex system with both continuous and discrete dynamics. In our case study, which involves a distributed real-time video distribution system, the task of adaptive resource management is to control the utilization of the different resources, whose utilizations are described by continuous variables. We achieve this by adapting the resolution of the transmitted video, which is modeled as a continuous variable, and by changing the frame-rate and the compression, which are modeled by discrete actions. We have implemented HyARM atop The ACE ORB (TAO) [13], which is an implementation of the Real-time CORBA specification [12]. Our results show that (1) HyARM ensures effective system resource utilization and (2) end-to-end QoS requirements of higher priority applications are met, even in the face of fluctuations in workload.
The remainder of the paper is organized as follows: Section 2 describes the architecture, functionality, and resource utilization model of our DRE multimedia system case study; Section 3 explains the structure and functionality of HyARM; Section 4 evaluates the adaptive behavior of HyARM via experiments on our multimedia system case study; Section 5 compares our research on HyARM with related work; and Section 6 presents concluding remarks. 1 The code and examples for HyARM are available at www. dre.vanderbilt.edu/∼nshankar/HyARM/.
Article 7
SYSTEM This section describes the architecture and QoS requirements of our DRE multimedia system.
Wireless Link Wireless Link Wireless Link ` ` ` Physical Link Physical Link Physical Link Base Station End Receiver End Receiver End Receiver` Physical Link End Receiver UAV Camera Video Encoder Camera Video Encoder Camera Video Encoder UAV Camera Video Encoder Camera Video Encoder Camera Video Encoder UAV Camera Video Encoder Camera Video Encoder Camera Video Encoder Figure 1: DRE Multimedia System Architecture The architecture for our DRE multimedia system is shown in Figure 1 and consists of the following entities: (1)Data source (video capture by UAV), where video is captured (related to subject of interest) by camera(s) on each UAV, followed by encoding of raw video using a specific encoding scheme and transmitting the video to the next stage in the pipeline. (2)Data distributor (base station), where the video is processed to remove noise, followed by retransmission of the processed video to the next stage in the pipeline. (3) Sinks (command and control center), where the received video is again processed to remove noise, then decoded and finally rendered to end user via graphical displays.
Significant improvements in video encoding/decoding and (de)compression techniques have been made as a result of recent advances in video encoding and compression techniques [14]. Common video compression schemes are MPEG1, MPEG-2, Real Video, and MPEG-4. Each compression scheme is characterized by its resource requirement, e.g., the computational power to (de)compress the video signal and the network bandwidth required to transmit the compressed video signal. Properties of the compressed video, such as resolution and frame-rate determine both the quality and the resource requirements of the video.
Our multimedia system case study has the following endto-end real-time QoS requirements: (1) latency, (2) interframe delay (also know as jitter), (3) frame rate, and (4) picture resolution. These QoS requirements can be classified as being either hard or soft. Hard QoS requirements should be met by the underlying system at all times, whereas soft QoS requirements can be missed occasionally.2 For our case study, we treat QoS requirements such as latency and jitter as harder QoS requirements and strive to meet these requirements at all times. In contrast, we treat QoS requirements such as video frame rate and picture resolution as softer QoS requirements and modify these video properties adaptively to handle dynamic changes in resource availabil2 Although hard and soft are often portrayed as two discrete requirement sets, in practice they are usually two ends of a continuum ranging from softer to harder rather than two disjoint points. ity effectively.
There are two primary types of resources in our DRE multimedia system: (1) processors that provide computational power available at the UAVs, base stations, and end receivers and (2) network links that provide communication bandwidth between UAVs, base stations, and end receivers.
The computing power required by the video capture and encoding tasks depends on dynamic factors, such as speed of the UAV, speed of the subject (if the subject is mobile), and distance between UAV and the subject. The wireless network bandwidth available to transmit video captured by UAVs to base stations also depends on the wireless connectivity between the UAVs and the base station, which in-turn depend on dynamic factors such as the speed of the UAVs and the relative distance between UAVs and base stations.
The bandwidth of the link between the base station and the end receiver is limited, but more stable than the bandwidth of the wireless network. Resource requirements and availability of resources are subjected to dynamic changes.
Two classes of applications - QoS-enabled and best-effort - use the multimedia system infrastructure described above to transmit video to their respective receivers. QoS-enabled class of applications have higher priority over best-effort class of application. In our study, emergency response applications belong to QoS-enabled and surveillance applications belong to best-effort class. For example, since a stream from an emergency response application is of higher importance than a video stream from a surveillance application, it receives more resources end-to-end.
Since resource availability significantly affects QoS, we use current resource utilization as the primary indicator of system performance. We refer to the current level of system resource utilization as the system condition. Based on this definition, we can classify system conditions as being either under, over, or effectively utilized.
Under-utilization of system resources occurs when the current resource utilization is lower than the desired lower bound on resource utilization. In this system condition, residual system resources (i.e., network bandwidth and computational power) are available in large amounts after meeting end-to-end QoS requirements of applications. These residual resources can be used to increase the QoS of the applications. For example, residual CPU and network bandwidth can be used to deliver better quality video (e.g., with greater resolution and higher frame rate) to end receivers.
Over-utilization of system resources occurs when the current resource utilization is higher than the desired upper bound on resource utilization. This condition can arise from loss of resources - network bandwidth and/or computing power at base station, end receiver or at UAV - or may be due to an increase in resource demands by applications. Over-utilization is generally undesirable since the quality of the received video (such as resolution and frame rate) and timeliness properties (such as latency and jitter) are degraded and may result in an unstable (and thus ineffective) system.
Effective resource utilization is the desired system condition since it ensures that end-to-end QoS requirements of the UAV-based multimedia system are met and utilization of both system resources, i.e., network bandwidth and computational power, are within their desired utilization bounds.
Article 7 Section 3 describes techniques we applied to achieve effective utilization, even in the face of fluctuating resource availability and/or demand.
This section describes the architecture of the Hybrid Adaptive Resource-management Middleware (HyARM). HyARM ensures efficient and predictable system performance by providing adaptive resource management, including monitoring of system resources and enforcing bounds on application resource utilization.
Resource Utilization Legend Resource Allocation Application Parameters Figure 2: HyARM Architecture HyARM is composed of three types of entities shown in Figure 2 and described below: Resource monitors observe the overall resource utilization for each type of resource and resource utilization per application. In our multimedia system, there are resource monitors for CPU utilization and network bandwidth. CPU monitors observe the CPU resource utilization of UAVs, base station, and end receivers. Network bandwidth monitors observe the network resource utilization of (1) wireless network link between UAVs and the base station and (2) wired network link between the base station and end receivers.
The central controller maintains the system resource utilization below a desired bound by (1) processing periodic updates it receives from resource monitors and (2) modifying the execution of applications accordingly, e.g., by using different execution algorithms or operating the application with increased/decreased QoS. This adaptation process ensures that system resources are utilized efficiently and end-to-end application QoS requirements are met. In our multimedia system, the HyARM controller determines the value of application parameters such as (1) video compression schemes, such as Real Video and MPEG-4, and/or (2) frame rate, and (3) picture resolution. From the perspective of hybrid control theoretic techniques [8], the different video compression schemes and frame rate form the discrete variables of application execution and picture resolution forms the continuous variables.
Application adapters modify application execution according to parameters recommended by the controller and ensures that the operation of the application is in accordance with the recommended parameters. In the current mplementation of HyARM, the application adapter modifies the input parameters to the application that affect application QoS and resource utilization - compression scheme, frame rate, and picture resolution. In our future implementations, we plan to use resource reservation mechanisms such as Differentiated Service [7, 3] and Class-based Kernel Resource Management [4] to provision/reserve network and CPU resources. In our multimedia system, the application adapter ensures that the video is encoded at the recommended frame rate and resolution using the specified compression scheme.
System Case Study HyARM is built atop TAO [13], a widely used open-source implementation of Real-time CORBA [12]. HyARM can be applied to ensure efficient, predictable and adaptive resource management of any DRE system where resource availability and requirements are subject to dynamic change.
Figure 3 shows the interaction of various parts of the DRE multimedia system developed with HyARM, TAO, and TAO"s A/V Streaming Service. TAO"s A/V Streaming service is an implementation of the CORBA A/V Streaming Service specification. TAO"s A/V Streaming Service is a QoS-enabled video distribution service that can transfer video in real-time to one or more receivers. We use the A/V Streaming Service to transmit the video from the UAVs to the end receivers via the base station. Three entities of Receiver UAV TAO Resource Utilization HyARM Central Controller A/V Streaming Service : Sender MPEG1 MPEG4 Real Video HyARM Resource Monitor A/V Streaming Service : Receiver Compressed Video Compressed Video Application HyARM Application Adapter Remote Object Call Control Inputs Resource Utilization Resource Utilization / Control Inputs Control Inputs Legend Figure 3: Developing the DRE Multimedia System with HyARM HyARM, namely the resource monitors, central controller, and application adapters are built as CORBA servants, so they can be distributed throughout a DRE system.
Resource monitors are remote CORBA objects that update the central controller periodically with the current resource utilization. Application adapters are collocated with applications since the two interact closely.
As shown in Figure 3, UAVs compress the data using various compression schemes, such as MPEG1, MPEG4, and Real Video, and uses TAO"s A/V streaming service to transmit the video to end receivers. HyARM"s resource monitors continuously observe the system resource utilization and notify the central controller with the current utilization. 3 The interaction between the controller and the resource monitors uses the Observer pattern [5]. When the controller receives resource utilization updates from monitors, it computes the necessary modifications to application(s) parameters and notifies application adapter(s) via a remote operation call. Application adapter(s), that are collocated with the application, modify the input parameters to the application - in our case video encoder - to modify the application resource utilization and QoS. 3 The base station is not included in the figure since it only retransmits the video received from UAVs to end receivers.
Article 7
ANALYSIS This section first describes the testbed that provides the infrastructure for our DRE multimedia system, which was used to evaluate the performance of HyARM. We then describe our experiments and analyze the results obtained to empirically evaluate how HyARM behaves during underand over-utilization of system resources.
Testbed Our experiments were performed on the Emulab testbed at University of Utah. The hardware configuration consists of two nodes acting as UAVs, one acting as base station, and one as end receiver. Video from the two UAVs were transmitted to a base station via a LAN configured with the following properties: average packet loss ratio of 0.3 and bandwidth 1 Mbps. The network bandwidth was chosen to be 1 Mbps since each UAV in the DRE multimedia system is allocated 250 Kbps. These parameters were chosen to emulate an unreliable wireless network with limited bandwidth between the UAVs and the base station. From the base station, the video was retransmitted to the end receiver via a reliable wireline link of 10 Mbps bandwidth with no packet loss.
The hardware configuration of all the nodes was chosen as follows: 600 MHz Intel Pentium III processor, 256 MB physical memory, 4 Intel EtherExpress Pro 10/100 Mbps Ethernet ports, and 13 GB hard drive. A real-time version of Linux - TimeSys Linux/NET 3.1.214 based on RedHat Linux 9was used as the operating system for all nodes. The following software packages were also used for our experiments: (1) Ffmpeg 0.4.9-pre1, which is an open-source library (http: //www.ffmpeg.sourceforge.net/download.php) that compresses video into MPEG-2, MPEG-4, Real Video, and many other video formats. (2) Iftop 0.16, which is an opensource library (http://www.ex-parrot.com/∼pdw/iftop/) we used for monitoring network activity and bandwidth utilization. (3) ACE 5.4.3 + TAO 1.4.3, which is an opensource (http://www.dre.vanderbilt.edu/TAO) implementation of the Real-time CORBA [12] specification upon which HyARM is built. TAO provides the CORBA Audio/Video (A/V) Streaming Service that we use to transmit the video from the UAVs to end receivers via the base station.
Our experiment consisted of two (emulated) UAVs that simultaneously send video to the base station using the experimentation setup described in Section 4.1. At the base station, video was retransmitted to the end receivers (without any modifications), where it was stored to a file. Each UAV hosted two applications, one QoS-enabled application (emergency response), and one best-effort application (surveillance). Within each UAV, computational power is shared between the applications, while the network bandwidth is shared among all applications.
To evaluate the QoS provided by HyARM, we monitored CPU utilization at the two UAVs, and network bandwidth utilization between the UAV and the base station. CPU resource utilization was not monitored at the base station and the end receiver since they performed no computationallyintensive operations. The resource utilization of the 10 Mpbs physical link between the base station and the end receiver does not affect QoS of applications and is not monitored by HyARM since it is nearly 10 times the 1 MB bandwidth of the LAN between the UAVs and the base station. The experiment also monitors properties of the video that affect the QoS of the applications, such as latency, jitter, frame rate, and resolution.
The set point on resource utilization for each resource was specified at 0.69, which is the upper bound typically recommended by scheduling techniques, such as rate monotonic algorithm [9]. Since studies [6] have shown that human eyes can perceive delays more than 200ms, we use this as the upper bound on jitter of the received video. QoS requirements for each class of application is specified during system initialization and is shown in Table 1.
This section presents the results obtained from running the experiment described in Section 4.2 on our DRE multimedia system testbed. We used system resource utilization as a metric to evaluate the adaptive resource management capabilities of HyARM under varying input work loads. We also used application QoS as a metric to evaluate HyARM"s capabilities to support end-to-end QoS requirements of the various classes of applications in the DRE multimedia system. We analyze these results to explain the significant differences in system performance and application QoS.
Comparison of system performance is decomposed into comparison of resource utilization and application QoS. For system resource utilization, we compare (1) network bandwidth utilization of the local area network and (2) CPU utilization at the two UAV nodes. For application QoS, we compare mean values of video parameters, including (1) picture resolution, (2) frame rate, (3) latency, and (4) jitter.
Comparison of resource utilization. Over-utilization of system resources in DRE systems can yield an unstable system. In contrast, under-utilization of system resources increases system cost. Figure 4 and Figure 5 compare the system resource utilization with and without HyARM.
Figure 4 shows that HyARM maintains system utilization close to the desired utilization set point during fluctuation in input work load by transmitting video of higher (or lower) QoS for QoS-enabled (or best-effort) class of applications during over (or under) utilization of system resources.
Figure 5 shows that without HyARM, network utilization was as high as 0.9 during increase in workload conditions, which is greater than the utilization set point of 0.7 by 0.2. As a result of over-utilization of resources, QoS of the received video, such as average latency and jitter, was affected significantly. Without HyARM, system resources were either under-utilized or over-utilized, both of which are undesirable. In contrast, with HyARM, system resource utilization is always close to the desired set point, even during fluctuations in application workload. During sudden fluctuation in application workload, system conditions may be temporarily undesirable, but are restored to the desired condition within several sampling periods. Temporary over-utilization of resources is permissible in our multimedia system since the quality of the video may be degraded for a short period of time, though application QoS will be degraded significantly if poor quality video is transmitted for a longer period of time.
Comparison of application QoS. Figures 6, Figure 7, and Table 2 compare latency, jitter, resolution, and frameArticle 7 Class Resolution Frame Rate Latency (msec ) Jitter (msec) QoS Enabled 1024 x 768 25 200 200 Best-effort 320 x 240 15 300 250 Table 1: Application QoS Requirements Figure 4: Resource utilization with HyARM Figure 5: Resource utilization without HyARM rate of the received video, respectively. Table 2 shows that HyARM increases the resolution and frame video of QoSenabled applications, but decreases the resolution and frame rate of best effort applications. During over utilization of system resources, resolution and frame rate of lower priority applications are reduced to adapt to fluctuations in application workload and to maintain the utilization of resources at the specified set point.
It can be seen from Figure 6 and Figure 7 that HyARM reduces the latency and jitter of the received video significantly. These figures show that the QoS of QoS-enabled applications is greatly improved by HyARM. Although application parameters, such as frame rate and resolutions, which affect the soft QoS requirements of best-effort applications may be compromised, the hard QoS requirements, such as latency and jitter, of all applications are met.
HyARM responds to fluctuation in resource availability and/or demand by constant monitoring of resource utilization. As shown in Figure 4, when resources utilization increases above the desired set point, HyARM lowers the utilization by reducing the QoS of best-effort applications. This adaptation ensures that enough resources are available for QoS-enabled applications to meet their QoS needs.
Figures 6 and 7 show that the values of latency and jitter of the received video of the system with HyARM are nearly half of the corresponding value of the system without HyARM.
With HyARM, values of these parameters are well below the specified bounds, whereas without HyARM, these value are significantly above the specified bounds due to overutilization of the network bandwidth, which leads to network congestion and results in packet loss. HyARM avoids this by reducing video parameters such as resolution, frame-rate, and/or modifying the compression scheme used to compress the video.
Our conclusions from analyzing the results described above are that applying adaptive middleware via hybrid control to DRE system helps to (1) improve application QoS, (2) increase system resource utilization, and (3) provide better predictability (lower latency and inter-frame delay) to QoSenabled applications. These improvements are achieved largely due to monitoring of system resource utilization, efficient system workload management, and adaptive resource provisioning by means of HyARM"s network/CPU resource monitors, application adapter, and central controller, respectively.
A number of control theoretic approaches have been applied to DRE systems recently. These techniques aid in overcoming limitations with traditional scheduling approaches that handle dynamic changes in resource availability poorly and result in a rigidly scheduled system that adapts poorly to change. A survey of these techniques is presented in [1].
One such approach is feedback control scheduling (FCS) [2, 11]. FCS algorithms dynamically adjust resource allocation by means of software feedback control loops. FCS algorithms are modeled and designed using rigorous controltheoretic methodologies. These algorithms provide robust and analytical performance assurances despite uncertainties in resource availability and/or demand. Although existing FCS algorithms have shown promise, these algorithms often assume that the system has continuous control variable(s) that can continuously be adjusted. While this assumption holds for certain classes of systems, there are many classes of DRE systems, such as avionics and total-ship computing environments that only support a finite a priori set of discrete configurations. The control variables in such systems are therefore intrinsically discrete.
HyARM handles both continuous control variables, such as picture resolution, and discrete control variable, such as discrete set of frame rates. HyARM can therefore be applied to system that support continuous and/or discrete set of control variables. The DRE multimedia system as described in Section 2 is an example DRE system that offers both continuous (picture resolution) and discrete set (frame-rate) of control variables. These variables are modified by HyARM to achieve efficient resource utilization and improved application QoS.
Article 7 Figure 6: Comparison of Video Latency Figure 7: Comparison of Video Jitter Source Picture Size / Frame Rate With HyARM Without HyARM UAV1 QoS Enabled Application 1122 X 1496 / 25 960 X 720 / 20 UAV1 Best-effort Application 288 X 384 / 15 640 X 480 / 20 UAV2 QoS Enabled Application 1126 X 1496 / 25 960 X 720 / 20 UAV2 Best-effort Application 288 X 384 / 15 640 X 480 / 20 Table 2: Comparison of Video Quality Many distributed real-time and embedded (DRE) systems demand end-to-end quality of service (QoS) enforcement from their underlying platforms to operate correctly. These systems increasingly run in open environments, where resource availability is subject to dynamic change. To meet end-to-end QoS in dynamic environments, DRE systems can benefit from an adaptive middleware that monitors system resources, performs efficient application workload management, and enables efficient resource provisioning for executing applications.
This paper described HyARM, an adaptive middleware, that provides effective resource management to DRE systems. HyARM employs hybrid control techniques to provide the adaptive middleware capabilities, such as resource monitoring and application adaptation that are key to providing the dynamic resource management capabilities for open DRE systems. We employed HyARM to a representative DRE multimedia system that is implemented using Real-time CORBA and CORBA A/V Streaming Service.
We evaluated the performance of HyARM in a system composed of three distributed resources and two classes of applications with two applications each. Our empirical results indicate that HyARM ensures (1) efficient resource utilization by maintaining the resource utilization of system resources within the specified utilization bounds, (2) QoS requirements of QoS-enabled applications are met at all times.
Overall, HyARM ensures efficient, predictable, and adaptive resource management for DRE systems.

Grid computing [1] is an emerging collaborative computing paradigm to extend institution/organization specific high performance computing (HPC) capabilities greatly beyond local resources. Its importance stems from the fact that ground breaking research in strategic application areas such as bioscience and medicine, energy exploration and environmental modeling involve strong interdisciplinary components and often require intercampus collaborations and computational capabilities beyond institutional limitations.
The Texas Internet Grid for Research and Education (TIGRE) [2,3] is a state funded cyberinfrastructure development project carried out by five (Rice, A&M, TTU,
UH and UT Austin) major university systems - collectively called TIGRE Institutions. The purpose of TIGRE is to create a higher education Grid to sustain and extend research and educational opportunities across Texas.
TIGRE is a project of the High Performance Computing across Texas (HiPCAT) [4] consortium. The goal of HiPCAT is to support advanced computational technologies to enhance research, development, and educational activities.
The primary goal of TIGRE is to design and deploy state-of-the-art Grid middleware that enables integration of computing systems, storage systems and databases, visualization laboratories and displays, and even instruments and sensors across Texas. The secondary goal is to demonstrate the TIGRE capabilities to enhance research and educational opportunities in strategic application areas of interest to the State of Texas. These are bioscience and medicine, energy exploration and air quality modeling. Vision of the TIGRE project is to foster interdisciplinary and intercampus collaborations, identify novel approaches to extend academic-government-private partnerships, and become a competitive model for external funding opportunities. The overall goal of TIGRE is to support local, campus and regional user interests and offer avenues to connect with national Grid projects such as Open Science Grid [5], and TeraGrid [6].
Within the energy exploration strategic application area, we have Grid-enabled the ensemble Kalman Filter (EnKF) [7] approach for data assimilation in reservoir modeling and demonstrated the extensibility of the application using the TIGRE environment and the GridWay [8] metascheduler.
Section 2 provides an overview of the TIGRE environment and capabilities. Application description and the need for Grid-enabling EnKF methodology is provided in Section 3.
The implementation details and merits of our approach are discussed in Section 4. Conclusions are provided in Section
in Section 6.
The TIGRE Grid middleware consists of minimal set of components derived from a subset of the Virtual Data Toolkit (VDT) [9] which supports a variety of operating systems. The purpose of choosing a minimal software stack is to support applications at hand, and to simplify installation and distribution of client/server stacks across TIGRE sites. Additional components will be added as they become necessary. The PacMan [10] packaging and distribution mechanism is employed for TIGRE client/server installation and management. The PacMan distribution mechanism involves retrieval, installation, and often configuration of the packaged software. This approach allows the clients to keep current, consistent versions of TIGRE software. It also helps TIGRE sites to install the needed components on resources distributed throughout the participating sites. The TIGRE client/server stack consists of an authentication and authorization layer,
Globus GRAM4-based job submission via web services (pre-web services installations are available up on request).
The tools for handling Grid proxy generation, Grid-enabled file transfer and Grid-enabled remote login are supported.
The pertinent details of TIGRE services and tools for job scheduling and management are provided below.
The TIGRE security infrastructure includes a certificate authority (CA) accredited by the International Grid Trust Federation (IGTF) for issuing X. 509 user and resource Grid certificates [11]. The Texas Advanced Computing Center (TACC), University of Texas at Austin is the TIGRE"s shared CA. The TIGRE Institutions serve as Registration Authorities (RA) for their respective local user base. For up-to-date information on securing user and resource certificates and their installation instructions see ref [2]. The users and hosts on TIGRE are identified by their distinguished name (DN) in their X.509 certificate provided by the CA. A native Grid-mapfile that contains a list of authorized DNs is used to authenticate and authorize user job scheduling and management on TIGRE site resources. At Texas Tech University, the users are dynamically allocated one of the many generic pool accounts. This is accomplished through the Grid User Management System (GUMS) [12].
The TIGRE environment supports GRAM4-based job submission via web services. The job submission scripts are generated using XML. The web services GRAM translates the XML scripts into target cluster specific batch schedulers such as LSF, PBS, or SGE. The high bandwidth file transfer protocols such as GridFTP are utilized for staging files in and out of the target machine. The login to remote hosts for compilation and debugging is only through GSISSH service which requires resource authentication through X.509 certificates. The authentication and authorization of Grid jobs are managed by issuing Grid certificates to both users and hosts. The certificate revocation lists (CRL) are updated on a daily basis to maintain high security standards of the TIGRE Grid services. The TIGRE portal [2] documentation area provides a quick start tutorial on running jobs on TIGRE.
The metascheduler interoperates with the cluster level batch schedulers (such as LSF, PBS) in the overall Grid workflow management. In the present work, we have employed GridWay [8] metascheduler - a Globus incubator project - to schedule and manage jobs across TIGRE.
The GridWay is a light-weight metascheduler that fully utilizes Globus functionalities. It is designed to provide efficient use of dynamic Grid resources by multiple users for Grid infrastructures built on top of Globus services. The TIGRE site administrator can control the resource sharing through a powerful built-in scheduler provided by GridWay or by extending GridWay"s external scheduling module to provide their own scheduling policies. Application users can write job descriptions using GridWay"s simple and direct job template format (see Section 4 for details) or standard Job Submission Description Language (JSDL).
See section 4 for implementation details.
A TIGRE portal [2] was designed and deployed to interface users and resource providers. It was designed using GridPort [13] and is maintained by TACC. The TIGRE environment is supported by open source tools such as the Open Ticket Request System (OTRS) [14] for servicing trouble tickets, and MoinMoin [15] Wiki for TIGRE content and knowledge management for education, outreach and training. The links for OTRS and Wiki are consumed by the TIGRE portal [2] - the gateway for users and resource providers. The TIGRE resource status and loads are monitored by the Grid Port Information Repository (GPIR) service of the GridPort toolkit [13] which interfaces with local cluster load monitoring service such as Ganglia.
The GPIR utilizes cron jobs on each resource to gather site specific resource characteristics such as jobs that are running, queued and waiting for resource allocation.
APPLICATION The main goal of hydrocarbon reservoir simulations is to forecast the production behavior of oil and gas field (denoted as field hereafter) for its development and optimal management. In reservoir modeling, the field is divided into several geological models as shown in Figure 1. For accurate performance forecasting of the field, it is necessary to reconcile several geological models to the dynamic response of the field through history matching [16-20].
Figure 1. Cross-sectional view of the Field. Vertical layers correspond to different geological models and the nails are oil wells whose historical information will be used for forecasting the production behavior. (Figure Ref:http://faculty.smu.edu/zchen/research.html).
The EnKF is a Monte Carlo method that works with an ensemble of reservoir models. This method utilizes crosscovariances [21] between the field measurements and the reservoir model parameters (derived from several models) to estimate prediction uncertainties. The geological model parameters in the ensemble are sequentially updated with a goal to minimize the prediction uncertainties. Historical production response of the field for over 50 years is used in these simulations. The main advantage of EnKF is that it can be readily linked to any reservoir simulator, and can assimilate latest production data without the need to re-run the simulator from initial conditions. Researchers in Texas are large subscribers of the Schlumberger ECLIPSE [22] package for reservoir simulations. In the reservoir modeling, each geological model checks out an ECLIPSE license. The simulation runtime of the EnKF methodology depends on the number of geological models used, number of ECLIPSE licenses available, production history of the field, and propagated uncertainties in history matching.
The overall EnKF workflow is shown Figure 2.
Figure 2. Ensemble Kaman Filter Data Assimilation Workflow. Each site has L licenses.
At START, the master/control process (EnKF main program) reads the simulation configuration file for number (N) of models, and model-specific input files. Then, N working directories are created to store the output files. At the end of iteration, the master/control process collects the output files from N models and post processes crosscovariances [21] to estimate the prediction uncertainties.
This information will be used to update models (or input files) for the next iteration. The simulation continues until the production histories are exhausted.
Typical EnKF simulation with N=50 and field histories of 50-60 years, in time steps ranging from three months to a year, takes about three weeks on a serial computing environment.
In parallel computing environment, there is no interprocess communication between the geological models in the ensemble. However, at the end of each simulation time-step, model-specific output files are to be collected for analyzing cross covariances [21] and to prepare next set of input files. Therefore, master-slave model in messagepassing (MPI) environment is a suitable paradigm. In this approach, the geological models are treated as slaves and are distributed across the available processors. The master Cluster or (TIGRE/GridWay) START Read Configuration File Create N Working Directories Create N Input files Model l Model 2 Model N. . .
ECLIPSE on site A ECLIPSE on Site B ECLIPSE on Site Z Collect N Model Outputs,
Post-process Output files END . . . process collects model-specific output files, analyzes and prepares next set of input files for the simulation. Since each geological model checks out an ECLIPSE license, parallelizability of the simulation depends on the number of licenses available. When the available number of licenses is less than the number of models in the ensemble, one or more of the nodes in the MPI group have to handle more than one model in a serial fashion and therefore, it takes longer to complete the simulation.
A Petroleum Engineering Department usually procures 10-15 ECLIPSE licenses while at least ten-fold increase in the number of licenses would be necessary for industry standard simulations. The number of licenses can be increased by involving several Petroleum Engineering Departments that support ECLIPSE package.
Since MPI does not scale very well for applications that involve remote compute clusters, and to get around the firewall issues with license servers across administrative domains, Grid-enabling the EnKF workflow seems to be necessary. With this motivation, we have implemented Grid-enabled EnKF workflow for the TIGRE environment and demonstrated parallelizability of the application across TIGRE using GridWay metascheduler. Further details are provided in the next section.
To Grid-enable the EnKF approach, we have eliminated the MPI code for parallel processing and replaced with N single processor jobs (or sub-jobs) where, N is the number of geological models in the ensemble. These model-specific sub-jobs were distributed across TIGRE sites that support ECLIPSE package using the GridWay [8] metascheduler.
For each sub-job, we have constructed a GridWay job template that specifies the executable, input and output files, and resource requirements. Since the TIGRE compute resources are not expected to change frequently, we have used static resource discovery policy for GridWay and the sub-jobs were scheduled dynamically across the TIGRE resources using GridWay. Figure 3 represents the sub-job template file for the GridWay metascheduler.
Figure 3. GridWay Sub-Job Template In Figure 3, REQUIREMENTS flag is set to choose the resources that satisfy the application requirements. In the case of EnKF application, for example, we need resources that support ECLIPSE package. ARGUMENTS flag specifies the model in the ensemble that will invoke ECLIPSE at a remote site. INPUT_FILES is prepared by the EnKF main program (or master/control process) and is transferred by GridWay to the remote site where it is untared and is prepared for execution. Finally,
OUTPUT_FILES specifies the name and location where the output files are to be written.
The command-line features of GridWay were used to collect and process the model-specific outputs to prepare new set of input files. This step mimics MPI process synchronization in master-slave model. At the end of each iteration, the compute resources and licenses are committed back to the pool. Table 1 shows the sub-jobs in TIGRE Grid via GridWay using gwps command and for clarity, only selected columns were shown .
USER JID DM EM NAME HOST pingluo 88 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF pingluo 89 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF pingluo 90 wrap actv enkf.jt minigar.hpcc.ttu.edu/LSF pingluo 91 wrap pend enkf.jt minigar.hpcc.ttu.edu/LSF pingluo 92 wrap done enkf.jt cosmos.tamu.edu/PBS pingluo 93 wrap epil enkf.jt cosmos.tamu.edu/PBS Table 1. Job scheduling across TIGRE using GridWay Metascheduler. DM: Dispatch state, EM: Execution state,
JID is the job id and HOST corresponds to site specific cluster and its local batch scheduler.
When a job is submitted to GridWay, it will go through a series of dispatch (DM) and execution (EM) states. For DM, the states include pend(ing), prol(og), wrap(per), epil(og), and done. DM=prol means the job has been scheduled to a resource and the remote working directory is in preparation. DM=warp implies that GridWay is executing the wrapper which in turn executes the application. DM=epil implies the job has finished running at the remote site and results are being transferred back to the GridWay server. Similarly, when EM=pend implies the job is waiting in the queue for resource and the job is running when EM=actv. For complete list of message flags and their descriptions, see the documentation in ref [8].
We have demonstrated the Grid-enabled EnKF runs using GridWay for TIGRE environment. The jobs are so chosen that the runtime doesn"t exceed more than a half hour. The simulation runs involved up to 20 jobs between A&M and TTU sites with TTU serving 10 licenses. For resource information, see Table I.
One of the main advantages of Grid-enabled EnKF simulation is that both the resources and licenses are released back to the pool at the end of each simulation time step unlike in the case of MPI implementation where licenses and nodes are locked until the completion of entire simulation. However, the fact that each sub-job gets scheduled independently via GridWay could possibly incur another time delay caused by waiting in queue for execution in each simulation time step. Such delays are not expected EXECUTABLE=runFORWARD REQUIREMENTS=HOSTNAME=cosmos.tamu.edu | HOSTNAME=antaeus.hpcc.ttu.edu | HOSTNAME=minigar.hpcc.ttu.edu | ARGUMENTS=001 INPUT_FILES=001.in.tar OUTPUT_FILES=001.out.tar in MPI implementation where the node is blocked for processing sub-jobs (model-specific calculation) until the end of the simulation. There are two main scenarios for comparing Grid and cluster computing approaches.
Scenario I: The cluster is heavily loaded. The conceived average waiting time of job requesting large number of CPUs is usually longer than waiting time of jobs requesting single CPU. Therefore, overall waiting time could be shorter in Grid approach which requests single CPU for each sub-job many times compared to MPI implementation that requests large number of CPUs at a single time. It is apparent that Grid scheduling is beneficial especially when cluster is heavily loaded and requested number of CPUs for the MPI job is not readily available.
Scenario II: The cluster is relatively less loaded or largely available. It appears the MPI implementation is favorable compared to the Grid scheduling. However, parallelizability of the EnKF application depends on the number of ECLIPSE licenses and ideally, the number of licenses should be equal to the number of models in the ensemble. Therefore, if a single institution does not have sufficient number of licenses, the cluster availability doesn"t help as much as it is expected.
Since the collaborative environment such as TIGRE can address both compute and software resource requirements for the EnKF application, Grid-enabled approach is still advantageous over the conventional MPI implementation in any of the above scenarios.
TIGRE is a higher education Grid development project and its purpose is to sustain and extend research and educational opportunities across Texas. Within the energy exploration application area, we have Grid-enabled the MPI implementation of the ensemble Kalman filter data assimilation methodology for reservoir characterization.
This task was accomplished by removing MPI code for parallel processing and replacing with single processor jobs one for each geological model in the ensemble. These single processor jobs were scheduled across TIGRE via GridWay metascheduler. We have demonstrated that by pooling licenses across TIGRE sites, more geological models can be handled in parallel and therefore conceivably better simulation accuracy. This approach has several advantages over MPI implementation especially when a site specific cluster is heavily loaded and/or the number licenses required for the simulation is more than those available at a single site.
Towards the future work, it would be interesting to compare the runtime between MPI, and Grid implementations for the EnKF application. This effort could shed light on quality of service (QoS) of Grid environments in comparison with cluster computing.
Another aspect of interest in the near future would be managing both compute and license resources to address the job (or processor)-to-license ratio management.
LEARNED The Grid-enabling efforts for EnKF application have provided ample opportunities to gather insights on the visibility and promise of Grid computing environments for application development and support. The main issues are industry standard data security and QoS comparable to cluster computing.
Since the reservoir modeling research involves proprietary data of the field, we had to invest substantial efforts initially in educating the application researchers on the ability of Grid services in supporting the industry standard data security through role- and privilege-based access using X.509 standard.
With respect to QoS, application researchers expect cluster level QoS with Grid environments. Also, there is a steep learning curve in Grid computing compared to the conventional cluster computing. Since Grid computing is still an emerging technology, and it spans over several administrative domains, Grid computing is still premature especially in terms of the level of QoS although, it offers better data security standards compared to commodity clusters.
It is our observation that training and outreach programs that compare and contrast the Grid and cluster computing environments would be a suitable approach for enhancing user participation in Grid computing. This approach also helps users to match their applications and abilities Grids can offer.
In summary, our efforts through TIGRE in Grid-enabling the EnKF data assimilation methodology showed substantial promise in engaging Petroleum Engineering researchers through intercampus collaborations. Efforts are under way to involve more schools in this effort. These efforts may result in increased collaborative research, educational opportunities, and workforce development through graduate/faculty research programs across TIGRE Institutions.
The authors acknowledge the State of Texas for supporting the TIGRE project through the Texas Enterprise Fund, and TIGRE Institutions for providing the mechanism, in which the authors (Ravi Vadapalli, Taesung Kim, and Ping Luo) are also participating. The authors thank the application researchers Prof. Akhil Datta-Gupta of Texas A&M University and Prof. Lloyd Heinze of Texas Tech University for their discussions and interest to exploit the TIGRE environment to extend opportunities in research and development.
[1] Foster, I. and Kesselman, C. (eds.) 2004. The Grid: Blueprint for a new computing infrastructure (The Elsevier series in Grid computing) [2] TIGRE Portal: http://tigreportal.hipcat.net [3] Vadapalli, R. Sill, A., Dooley, R., Murray, M., Luo, P., Kim,
T., Huang, M., Thyagaraja, K., and Chaffin, D. 2007.
Demonstration of TIGRE environment for Grid enabled/suitable applications. 8th IEEE/ACM Int. Conf. on Grid Computing, Sept 19-21, Austin [4] The High Performance Computing across Texas Consortium http://www.hipcat.net [5] Pordes, R. Petravick, D. Kramer, B. Olson, D. Livny, M.
Roy, A. Avery, P. Blackburn, K. Wenaus, T. Würthwein, F.
Foster, I. Gardner, R. Wilde, M. Blatecky, A. McGee, J. and Quick, R. 2007. The Open Science Grid, J. Phys Conf Series http://www.iop.org/EJ/abstract/1742-6596/78/1/012057 and http://www.opensciencegrid.org [6] Reed, D.A. 2003. Grids, the TeraGrid and Beyond,
Computer, vol 30, no. 1 and http://www.teragrid.org [7] Evensen, G. 2006. Data Assimilation: The Ensemble Kalman Filter, Springer [8] Herrera, J. Huedo, E. Montero, R. S. and Llorente, I. M.
[9] Avery, P. and Foster, I. 2001. The GriPhyN project: Towards petascale virtual data grids, technical report GriPhyN-200115 and http://vdt.cs.wisc.edu [10] The PacMan documentation and installation guide http://physics.bu.edu/pacman/htmls [11] Caskey, P. Murray, M. Perez, J. and Sill, A. 2007. Case studies in identify management for virtual organizations,
EDUCAUSE Southwest Reg. Conf., Feb 21-23, Austin, TX. http://www.educause.edu/ir/library/pdf/SWR07058.pdf [12] The Grid User Management System (GUMS) https://www.racf.bnl.gov/Facility/GUMS/index.html [13] Thomas, M. and Boisseau, J. 2003. Building grid computing portals: The NPACI grid portal toolkit, Grid computing: making the global infrastructure a reality, Chapter 28,
Berman, F. Fox, G. Thomas, M. Boisseau, J. and Hey, T. (eds), John Wiley and Sons, Ltd, Chichester [14] Open Ticket Request System http://otrs.org [15] The MoinMoin Wiki Engine http://moinmoin.wikiwikiweb.de [16] Vasco, D.W. Yoon, S. and Datta-Gupta, A. 1999. Integrating dynamic data into high resolution reservoir models using streamline-based analytic sensitivity coefficients, Society of Petroleum Engineers (SPE) Journal, 4 (4). [17] Emanuel, A. S. and Milliken, W. J. 1998. History matching finite difference models with 3D streamlines, SPE 49000,
Proc of the Annual Technical Conf and Exhibition, Sept 2730, New Orleans, LA. [18] Nævdal, G. Johnsen, L.M. Aanonsen, S.I. and Vefring, E.H.
using Ensemble Kalman Filter, SPE 84372, Proc of the Annual Technical Conf and Exhibition, Oct 5-8, Denver,
CO. [19] Jafarpour B. and McLaughlin, D.B. 2007. History matching with an ensemble Kalman filter and discrete cosine parameterization, SPE 108761, Proc of the Annual Technical Conf and Exhibition, Nov 11-14, Anaheim, CA [20] Li, G. and Reynolds, A. C. 2007. An iterative ensemble Kalman filter for data assimilation, SPE 109808, Proc of the SPE Annual Technical Conf and Exhibition, Nov 11-14,
Anaheim, CA [21] Arroyo-Negrete, E. Devagowda, D. Datta-Gupta, A. 2006.

Although Wireless Sensor Networks (WSN) have shown promising prospects in various applications [5], researchers still face several challenges for massive deployment of such networks. One of these is to identify the location of individual sensor nodes in outdoor environments. Because of unpredictable flow dynamics in airborne scenarios, it is not currently feasible to localize sensor nodes during massive UVA-based deployment. On the other hand, geometric information is indispensable in these networks, since users need to know where events of interest occur (e.g., the location of intruders or of a bomb explosion).
Previous research on node localization falls into two categories: range-based approaches and range-free approaches.
Range-based approaches [13, 17, 19, 24] compute per-node location information iteratively or recursively based on measured distances among target nodes and a few anchors which precisely know their locations. These approaches generally require costly hardware (e.g., GPS) and have limited effective range due to energy constraints (e.g., ultrasound-based TDOA [3, 17]). Although range-based solutions can be suitably used in small-scale indoor environments, they are considered less cost-effective for large-scale deployments. On the other hand, range-free approaches [4, 8, 10, 13, 14, 15] do not require accurate distance measurements, but localize the node based on network connectivity (proximity) information.
Unfortunately, since wireless connectivity is highly influenced by the environment and hardware calibration, existing solutions fail to deliver encouraging empirical results, or require substantial survey [2] and calibration [24] on a case-by-case basis.
Realizing the impracticality of existing solutions for the large-scale outdoor environment, researchers have recently proposed solutions (e.g., Spotlight [20] and Lighthouse [18]) for sensor node localization using the spatiotemporal correlation of controlled events (i.e., inferring nodes" locations based on the detection time of controlled events). These solutions demonstrate that long range and high accuracy localization can be achieved simultaneously with little additional cost at sensor nodes. These benefits, however, come along with an implicit assumption that the controlled events can be precisely distributed to a specified location at a specified time. We argue that precise event distribution is difficult to achieve, especially at large scale when terrain is uneven, the event distribution device is not well calibrated and its position is difficult to maintain (e.g., the helicopter-mounted scenario in [20]).
To address these limitations in current approaches, in this paper we present a multi-sequence positioning (MSP) method 15 for large-scale stationary sensor node localization, in deployments where an event source has line-of-sight to all sensors.
The novel idea behind MSP is to estimate each sensor node"s two-dimensional location by processing multiple easy-to-get one-dimensional node sequences (e.g., event detection order) obtained through loosely-guided event distribution.
This design offers several benefits. First, compared to a range-based approach, MSP does not require additional costly hardware. It works using sensors typically used by sensor network applications, such as light and acoustic sensors, both of which we specifically consider in this work. Second, compared to a range-free approach, MSP needs only a small number of anchors (theoretically, as few as two), so high accuracy can be achieved economically by introducing more events instead of more anchors. And third, compared to Spotlight, MSP does not require precise and sophisticated event distribution, an advantage that significantly simplifies the system design and reduces calibration cost.
This paper offers the following additional intellectual contributions: • We are the first to localize sensor nodes using the concept of node sequence, an ordered list of sensor nodes, sorted by the detection time of a disseminated event. We demonstrate that making full use of the information embedded in one-dimensional node sequences can significantly improve localization accuracy. Interestingly, we discover that repeated reprocessing of one-dimensional node sequences can further increase localization accuracy. • We propose a distribution-based location estimation strategy that obtains the final location of sensor nodes using the marginal probability of joint distribution among adjacent nodes within the sequence. This new algorithm outperforms the widely adopted Centroid estimation [4, 8]. • To the best of our knowledge, this is the first work to improve the localization accuracy of nodes by adaptive events. The generation of later events is guided by localization results from previous events. • We evaluate line-based MSP on our new Mirage test-bed, and wave-based MSP in outdoor environments. Through system implementation, we discover and address several interesting issues such as partial sequence and sequence flips. To reveal MSP performance at scale, we provide analytic results as well as a complete simulation study.
All the simulation and implementation code is available online at http://www.cs.umn.edu/∼zhong/MSP.
The rest of the paper is organized as follows. Section 2 briefly surveys the related work. Section 3 presents an overview of the MSP localization system. In sections 4 and 5, basic MSP and four advanced processing methods are introduced. Section 6 describes how MSP can be applied in a wave propagation scenario. Section 7 discusses several implementation issues. Section 8 presents simulation results, and Section 9 reports an evaluation of MSP on the Mirage test-bed and an outdoor test-bed. Section 10 concludes the paper.
Many methods have been proposed to localize wireless sensor devices in the open air. Most of these can be classified into two categories: range-based and range-free localization. Range-based localization systems, such as GPS [23],
Cricket [17], AHLoS [19], AOA [16], Robust Quadrilaterals [13] and Sweeps [7], are based on fine-grained point-topoint distance estimation or angle estimation to identify pernode location. Constraints on the cost, energy and hardware footprint of each sensor node make these range-based methods undesirable for massive outdoor deployment. In addition, ranging signals generated by sensor nodes have a very limited effective range because of energy and form factor concerns.
For example, ultrasound signals usually effectively propagate 20-30 feet using an on-board transmitter [17]. Consequently, these range-based solutions require an undesirably high deployment density. Although the received signal strength indicator (RSSI) related [2, 24] methods were once considered an ideal low-cost solution, the irregularity of radio propagation [26] seriously limits the accuracy of such systems. The recently proposed RIPS localization system [11] superimposes two RF waves together, creating a low-frequency envelope that can be accurately measured. This ranging technique performs very well as long as antennas are well oriented and environmental factors such as multi-path effects and background noise are sufficiently addressed.
Range-free methods don"t need to estimate or measure accurate distances or angles. Instead, anchors or controlled-event distributions are used for node localization. Range-free methods can be generally classified into two types: anchor-based and anchor-free solutions. • For anchor-based solutions such as Centroid [4], APIT [8], SeRLoc [10], Gradient [13] , and APS [15], the main idea is that the location of each node is estimated based on the known locations of the anchor nodes. Different anchor combinations narrow the areas in which the target nodes can possibly be located. Anchor-based solutions normally require a high density of anchor nodes so as to achieve good accuracy. In practice, it is desirable to have as few anchor nodes as possible so as to lower the system cost. • Anchor-free solutions require no anchor nodes. Instead, external event generators and data processing platforms are used. The main idea is to correlate the event detection time at a sensor node with the known space-time relationship of controlled events at the generator so that detection time-stamps can be mapped into the locations of sensors.
Spotlight [20] and Lighthouse [18] work in this fashion.
In Spotlight [20], the event distribution needs to be precise in both time and space. Precise event distribution is difficult to achieve without careful calibration, especially when the event-generating devices require certain mechanical maneuvers (e.g., the telescope mount used in Spotlight). All these increase system cost and reduce localization speed. StarDust [21], which works much faster, uses label relaxation algorithms to match light spots reflected by corner-cube retro-reflectors (CCR) with sensor nodes using various constraints. Label relaxation algorithms converge only when a sufficient number of robust constraints are obtained. Due to the environmental impact on RF connectivity constraints, however, StarDust is less accurate than Spotlight.
In this paper, we propose a balanced solution that avoids the limitations of both anchor-based and anchor-free solutions.
Unlike anchor-based solutions [4, 8], MSP allows a flexible tradeoff between the physical cost (anchor nodes) with the soft 16 1 A B 2 3 4 5 Target nodeAnchor node 1A 5 3 B2 4
1A25B4 3
1 2 3 5 4 (b) (c)(d) (a) Event 1 Node Sequence generated by event 1 Event 3 Node Sequence generated by event 2 Node Sequence generated by event 3 Node Sequence generated by event 4 Event 2 Event 4 Figure 1. The MSP System Overview cost (localization events). MSP uses only a small number of anchors (theoretically, as few as two). Unlike anchor-free solutions, MSP doesn"t need to maintain rigid time-space relationships while distributing events, which makes system design simpler, more flexible and more robust to calibration errors.
MSP works by extracting relative location information from multiple simple one-dimensional orderings of nodes.
Figure 1(a) shows a layout of a sensor network with anchor nodes and target nodes. Target nodes are defined as the nodes to be localized. Briefly, the MSP system works as follows. First, events are generated one at a time in the network area (e.g., ultrasound propagations from different locations, laser scans with diverse angles). As each event propagates, as shown in Figure 1(a), each node detects it at some particular time instance. For a single event, we call the ordering of nodes, which is based on the sequential detection of the event, a node sequence. Each node sequence includes both the targets and the anchors as shown in Figure 1(b). Second, a multi-sequence processing algorithm helps to narrow the possible location of each node to a small area (Figure 1(c)). Finally, a distributionbased estimation method estimates the exact location of each sensor node, as shown in Figure 1(d).
Figure 1 shows that the node sequences can be obtained much more economically than accurate pair-wise distance measurements between target nodes and anchor nodes via ranging methods. In addition, this system does not require a rigid time-space relationship for the localization events, which is critical but hard to achieve in controlled event distribution scenarios (e.g., Spotlight [20]).
For the sake of clarity in presentation, we present our system in two cases: • Ideal Case, in which all the node sequences obtained from the network are complete and correct, and nodes are time-synchronized [12, 9]. • Realistic Deployment, in which (i) node sequences can be partial (incomplete), (ii) elements in sequences could flip (i.e., the order obtained is reversed from reality), and (iii) nodes are not time-synchronized.
To introduce the MSP algorithm, we first consider a simple straight-line scan scenario. Then, we describe how to implement straight-line scans as well as other event types, such as sound wave propagation. 1 A 2 3 4 5 B C 6 7 8 9 Straight-line Scan 1 Straight-lineScan2 8 1
6 C 4 3 7 2 B 9 3 1 C 5 9
B
Target node Anchor node Figure 2. Obtaining Multiple Node Sequences
Let us consider a sensor network with N target nodes and M anchor nodes randomly deployed in an area of size S. The top-level idea for basic MSP is to split the whole sensor network area into small pieces by processing node sequences.
Because the exact locations of all the anchors in a node sequence are known, all the nodes in this sequence can be divided into O(M +1) parts in the area.
In Figure 2, we use numbered circles to denote target nodes and numbered hexagons to denote anchor nodes. Basic MSP uses two straight lines to scan the area from different directions, treating each scan as an event. All the nodes react to the event sequentially generating two node sequences. For vertical scan 1, the node sequence is (8,1,5,A,6,C,4,3,7,2,B,9), as shown outside the right boundary of the area in Figure 2; for horizontal scan 2, the node sequence is (3,1,C,5,9,2,A,4,6,B,7,8), as shown under the bottom boundary of the area in Figure 2.
Since the locations of the anchor nodes are available, the anchor nodes in the two node sequences actually split the area vertically and horizontally into 16 parts, as shown in Figure 2.
To extend this process, suppose we have M anchor nodes and perform d scans from different angles, obtaining d node sequences and dividing the area into many small parts.
Obviously, the number of parts is a function of the number of anchors M, the number of scans d, the anchors" location as well as the slop k for each scan line. According to the pie-cutting theorem [22], the area can be divided into O(M2d2) parts. When M and d are appropriately large, the polygon for each target node may become sufficiently small so that accurate estimation can be achieved. We emphasize that accuracy is affected not only by the number of anchors M, but also by the number of events d. In other words, MSP provides a tradeoff between the physical cost of anchors and the soft cost of events.
Algorithm 1 depicts the computing architecture of basic MSP. Each node sequence is processed within line 1 to 8. For each node, GetBoundaries() in line 5 searches for the predecessor and successor anchors in the sequence so as to determine the boundaries of this node. Then in line 6 UpdateMap() shrinks the location area of this node according to the newly obtained boundaries. After processing all sequences, Centroid Estimation (line 11) set the center of gravity of the final polygon as the estimated location of the target node.
Basic MSP only makes use of the order information between a target node and the anchor nodes in each sequence.
Actually, we can extract much more location information from 17 Algorithm 1 Basic MSP Process Output: The estimated location of each node. 1: repeat 2: GetOneUnprocessedSeqence(); 3: repeat 4: GetOneNodeFromSequenceInOrder(); 5: GetBoundaries(); 6: UpdateMap(); 7: until All the target nodes are updated; 8: until All the node sequences are processed; 9: repeat 10: GetOneUnestimatedNode(); 11: CentroidEstimation(); 12: until All the target nodes are estimated; each sequence. Section 5 will introduce advanced MSP, in which four novel optimizations are proposed to improve the performance of MSP significantly.
Four improvements to basic MSP are proposed in this section. The first three improvements do not need additional sensing and communication in the networks but require only slightly more off-line computation. The objective of all these improvements is to make full use of the information embedded in the node sequences. The results we have obtained empirically indicate that the implementation of the first two methods can dramatically reduce the localization error, and that the third and fourth methods are helpful for some system deployments.
As shown in Figure 2, each scan line and M anchors, splits the whole area into M + 1 parts. Each target node falls into one polygon shaped by scan lines. We noted that in basic MSP, only the anchors are used to narrow down the polygon of each target node, but actually there is more information in the node sequence that we can made use of.
Let"s first look at a simple example shown in Figure 3. The previous scans narrow the locations of target node 1 and node
Then a new scan generates a new sequence (1, 2). With knowledge of the scan"s direction, it is easy to tell that node 1 is located to the left of node 2. Thus, we can further narrow the location area of node 2 by eliminating the shaded part of node 2"s rectangle. This is because node 2 is located on the right of node 1 while the shaded area is outside the lower boundary of node 1. Similarly, the location area of node 1 can be narrowed by eliminating the shaded part out of node 2"s right boundary.
We call this procedure sequence-based MSP which means that the whole node sequence needs to be processed node by node in order. Specifically, sequence-based MSP follows this exact processing rule: 1 2
1 2 Lower boundary of 1 Upper boundary of 1 Lower boundary of 2 Upper boundary of 2 New sequence New upper boundary of 1 New Lower boundary of 2 EventPropagation Figure 3. Rule Illustration in Sequence Based MSP Algorithm 2 Sequence-Based MSP Process Output: The estimated location of each node. 1: repeat 2: GetOneUnprocessedSeqence(); 3: repeat 4: GetOneNodeByIncreasingOrder(); 5: ComputeLowbound(); 6: UpdateMap(); 7: until The last target node in the sequence; 8: repeat 9: GetOneNodeByDecreasingOrder(); 10: ComputeUpbound(); 11: UpdateMap(); 12: until The last target node in the sequence; 13: until All the node sequences are processed; 14: repeat 15: GetOneUnestimatedNode(); 16: CentroidEstimation(); 17: until All the target nodes are estimated; Elimination Rule: Along a scanning direction, the lower boundary of the successor"s area must be equal to or larger than the lower boundary of the predecessor"s area, and the upper boundary of the predecessor"s area must be equal to or smaller than the upper boundary of the successor"s area.
In the case of Figure 3, node 2 is the successor of node 1, and node 1 is the predecessor of node 2. According to the elimination rule, node 2"s lower boundary cannot be smaller than that of node 1 and node 1"s upper boundary cannot exceed node 2"s upper boundary.
Algorithm 2 illustrates the pseudo code of sequence-based MSP. Each node sequence is processed within line 3 to 13. The sequence processing contains two steps: Step 1 (line 3 to 7): Compute and modify the lower boundary for each target node by increasing order in the node sequence. Each node"s lower boundary is determined by the lower boundary of its predecessor node in the sequence, thus the processing must start from the first node in the sequence and by increasing order. Then update the map according to the new lower boundary.
Step 2 (line 8 to 12): Compute and modify the upper boundary for each node by decreasing order in the node sequence.
Each node"s upper boundary is determined by the upper boundary of its successor node in the sequence, thus the processing must start from the last node in the sequence and by decreasing order. Then update the map according to the new upper boundary.
After processing all the sequences, for each node, a polygon bounding its possible location has been found. Then, center-ofgravity-based estimation is applied to compute the exact location of each node (line 14 to 17).
An example of this process is shown in Figure 4. The third scan generates the node sequence (B,9,2,7,4,6,3,8,C,A,5,1). In addition to the anchor split lines, because nodes 4 and 7 come after node 2 in the sequence, node 4 and 7"s polygons could be narrowed according to node 2"s lower boundary (the lower right-shaded area); similarly, the shaded area in node 2"s rectangle could be eliminated since this part is beyond node 7"s upper boundary indicated by the dotted line. Similar eliminating can be performed for node 3 as shown in the figure. 18 1 A 2 3 4 5 B C 6 7 8 9 Straight-line Scan 1 Straight-lineScan2 Straight-line Scan 3 Target node Anchor node Figure 4. Sequence-Based MSP Example 1 A 2 3 4 5 B C 6 7 8 9 Straight-line Scan 1 Straight-lineScan2 Straight-line Scan 3 Reprocessing Scan 1 Target node Anchor node Figure 5. Iterative MSP: Reprocessing Scan 1 From above, we can see that the sequence-based MSP makes use of the information embedded in every sequential node pair in the node sequence. The polygon boundaries of the target nodes obtained in prior could be used to further split other target nodes" areas. Our evaluation in Sections 8 and 9 shows that sequence-based MSP considerably enhances system accuracy.
Sequence-based MSP is preferable to basic MSP because it extracts more information from the node sequence. In fact, further useful information still remains! In sequence-based MSP, a sequence processed later benefits from information produced by previously processed sequences (e.g., the third scan in Figure 5). However, the first several sequences can hardly benefit from other scans in this way. Inspired by this phenomenon, we propose iterative MSP. The basic idea of iterative MSP is to process all the sequences iteratively several times so that the processing of each single sequence can benefit from the results of other sequences.
To illustrate the idea more clearly, Figure 4 shows the results of three scans that have provided three sequences. Now if we process the sequence (8,1,5,A,6,C,4,3,7,2,B,9) obtained from scan 1 again, we can make progress, as shown in Figure 5.
The reprocessing of the node sequence 1 provides information in the way an additional vertical scan would. From sequencebased MSP, we know that the upper boundaries of nodes 3 and
boundary of node 7, therefore the grid parts can be eliminated (a) Central of Gravity (b) Joint Distribution
2
2 1
1
2
2 Figure 6. Example of Joint Distribution Estimation …... vm ap[0] vm ap[1] vm ap[2] vm ap[3] Combine m ap Figure 7. Idea of DBE MSP for Each Node for the nodes 3 and node 4, respectively, as shown in Figure 5.
From this example, we can see that iterative processing of the sequence could help further shrink the polygon of each target node, and thus enhance the accuracy of the system.
The implementation of iterative MSP is straightforward: process all the sequences multiple times using sequence-based MSP. Like sequence-based MSP, iterative MSP introduces no additional event cost. In other words, reprocessing does not actually repeat the scan physically. Evaluation results in Section 8 will show that iterative MSP contributes noticeably to a lower localization error. Empirical results show that after 5 iterations, improvements become less significant. In summary, iterative processing can achieve better performance with only a small computation overhead.
After determining the location area polygon for each node, estimation is needed for a final decision. Previous research mostly applied the Center of Gravity (COG) method [4] [8] [10] which minimizes average error. If every node is independent of all others, COG is the statistically best solution. In MSP, however, each node may not be independent. For example, two neighboring nodes in a certain sequence could have overlapping polygon areas. In this case, if the marginal probability of joint distribution is used for estimation, better statistical results are achieved.
Figure 6 shows an example in which node 1 and node 2 are located in the same polygon. If COG is used, both nodes are localized at the same position (Figure 6(a)). However, the node sequences obtained from two scans indicate that node 1 should be to the left of and above node 2, as shown in Figure 6(b).
The high-level idea of distribution-based estimation proposed for MSP, which we call DBE MSP, is illustrated in Figure 7. The distributions of each node under the ith scan (for the ith node sequence) are estimated in node.vmap[i], which is a data structure for remembering the marginal distribution over scan i. Then all the vmaps are combined to get a single map and weighted estimation is used to obtain the final location.
For each scan, all the nodes are sorted according to the gap, which is the diameter of the polygon along the direction of the scan, to produce a second, gap-based node sequence. Then, the estimation starts from the node with the smallest gap. This is because it is statistically more accurate to assume a uniform distribution of the node with smaller gap. For each node processed in order from the gap-based node sequence, either if 19 Pred. node"s area Predecessor node exists: conditional distribution based on pred. node"s area Alone: Uniformly Distributed Succ. node"s area Successor node exists: conditional distribution based on succ. node"s area Succ. node"s area Both predecessor and successor nodes exist: conditional distribution based on both of them Pred. node"s area Figure 8. Four Cases in DBE Process no neighbor node in the original event-based node sequence shares an overlapping area, or if the neighbors have not been processed due to bigger gaps, a uniform distribution Uniform() is applied to this isolated node (the Alone case in Figure 8).
If the distribution of its neighbors sharing overlapped areas has been processed, we calculate the joint distribution for the node.
As shown in Figure 8, there are three possible cases depending on whether the distribution of the overlapping predecessor and/or successor nodes have/has already been estimated.
The estimation"s strategy of starting from the most accurate node (smallest gap node) reduces the problem of estimation error propagation. The results in the evaluation section indicate that applying distribution-based estimation could give statistically better results.
So far, all the enhancements to basic MSP focus on improving the multi-sequence processing algorithm given a fixed set of scan directions. All these enhancements require only more computing time without any overhead to the sensor nodes.
Obviously, it is possible to have some choice and optimization on how events are generated. For example, in military situations, artillery or rocket-launched mini-ultrasound bombs can be used for event generation at some selected locations. In adaptive MSP, we carefully generate each new localization event so as to maximize the contribution of the new event to the refinement of localization, based on feedback from previous events.
Figure 9 depicts the basic architecture of adaptive MSP.
Through previous localization events, the whole map has been partitioned into many small location areas. The idea of adaptive MSP is to generate the next localization event to achieve best-effort elimination, which ideally could shrink the location area of individual node as much as possible.
We use a weighted voting mechanism to evaluate candidate localization events. Every node wants the next event to split its area evenly, which would shrink the area fast. Therefore, every node votes for the parameters of the next event (e.g., the scan angle k of the straight-line scan). Since the area map is maintained centrally, the vote is virtually done and there is no need for the real sensor nodes to participate in it. After gathering all the voting results, the event parameters with the most votes win the election. There are two factors that determine the weight of each vote: • The vote for each candidate event is weighted according to the diameter D of the node"s location area. Nodes with bigger location areas speak louder in the voting, because Map Partitioned by the Localization Events Diameter of Each Area Candidate Localization Events Evaluation Trigger Next Localization Evet Figure 9. Basic Architecture of Adaptive MSP 2 3 Diameter D3 1 1 3k 2 3k 3 3k 4 3k 5 3k 6 3k 1 3k 2 3k 3 3k 6 3k4 3k 5 3k Weight el small i opt i j ii j i S S DkkDfkWeight arg ),(,()( ⋅=∆= 1 3 opt k Target node Anchor node Center of Gravity Node 3's area Figure 10. Candidate Slops for Node 3 at Anchor 1 overall system error is reduced mostly by splitting the larger areas. • The vote for each candidate event is also weighted according to its elimination efficiency for a location area, which is defined as how equally in size (or in diameter) an event can cut an area. In other words, an optimal scan event cuts an area in the middle, since this cut shrinks the area quickly and thus reduces localization uncertainty quickly.
Combining the above two aspects, the weight for each vote is computed according to the following equation (1): Weight(k j i ) = f(Di,△(k j i ,k opt i )) (1) k j i is node i"s jth supporting parameter for next event generation; Di is diameter of node i"s location area; △(k j i ,k opt i ) is the distance between k j i and the optimal parameter k opt i for node i, which should be defined to fit the specific application.
Figure 10 presents an example for node 1"s voting for the slopes of the next straight-line scan. In the system, there are a fixed number of candidate slopes for each scan (e.g., k1,k2,k3,k4...). The location area of target node 3 is shown in the figure. The candidate events k1 3,k2 3,k3 3,k4 3,k5 3,k6
evaluated according to their effectiveness compared to the optimal ideal event which is shown as a dotted line with appropriate weights computed according to equation (1). For this specific example, as is illustrated in the right part of Figure 10, f(Di,△(k j i ,kopt i )) is defined as the following equation (2): Weight(kj i ) = f(Di,△(kj i ,kopt i )) = Di · Ssmall Slarge (2) Ssmall and Slarge are the sizes of the smaller part and larger part of the area cut by the candidate line respectively. In this case, node 3 votes 0 for the candidate lines that do not cross its area since Ssmall = 0.
We show later that adaptive MSP improves localization accuracy in WSNs with irregularly shaped deployment areas. 20
This section provides a complexity analysis of the MSP design. We emphasize that MSP adopts an asymmetric design in which sensor nodes need only to detect and report the events.
They are blissfully oblivious to the processing methods proposed in previous sections. In this section, we analyze the computational cost on the node sequence processing side, where resources are plentiful.
According to Algorithm 1, the computational complexity of Basic MSP is O(d · N · S), and the storage space required is O(N · S), where d is the number of events, N is the number of target nodes, and S is the area size.
According to Algorithm 2, the computational complexity of both sequence-based MSP and iterative MSP is O(c·d ·N ·S), where c is the number of iterations and c = 1 for sequencebased MSP, and the storage space required is O(N ·S). Both the computational complexity and storage space are equal within a constant factor to those of basic MSP.
The computational complexity of the distribution-based estimation (DBE MSP) is greater. The major overhead comes from the computation of joint distributions when both predecessor and successor nodes exit. In order to compute the marginal probability, MSP needs to enumerate the locations of the predecessor node and the successor node. For example, if node A has predecessor node B and successor node C, then the marginal probability PA(x,y) of node A"s being at location (x,y) is: PA(x,y) = ∑ i ∑ j ∑ m ∑ n 1 NB,A,C ·PB(i, j)·PC(m,n) (3) NB,A,C is the number of valid locations for A satisfying the sequence (B, A, C) when B is at (i, j) and C is at (m,n); PB(i, j) is the available probability of node B"s being located at (i, j); PC(m,n) is the available probability of node C"s being located at (m,n). A naive algorithm to compute equation (3) has complexity O(d · N · S3). However, since the marginal probability indeed comes from only one dimension along the scanning direction (e.g., a line), the complexity can be reduced to O(d · N · S1.5) after algorithm optimization. In addition, the final location areas for every node are much smaller than the original field S; therefore, in practice, DBE MSP can be computed much faster than O(d ·N ·S1.5).
So far, the description of MSP has been solely in the context of straight-line scan. However, we note that MSP is conceptually independent of how the event is propagated as long as node sequences can be obtained. Clearly, we can also support wave-propagation-based events (e.g., ultrasound propagation, air blast propagation), which are polar coordinate equivalences of the line scans in the Cartesian coordinate system.
This section illustrates the effects of MSP"s implementation in the wave propagation-based situation. For easy modelling, we have made the following assumptions: • The wave propagates uniformly in all directions, therefore the propagation has a circle frontier surface. Since MSP does not rely on an accurate space-time relationship, a certain distortion in wave propagation is tolerable. If any directional wave is used, the propagation frontier surface can be modified accordingly. 1 3 5 9 Target node Anchor node Previous Event location A 2 Center of Gravity 4 8 7 B 6 C A line of preferred locations for next event Figure 11. Example of Wave Propagation Situation • Under the situation of line-of-sight, we allow obstacles to reflect or deflect the wave. Reflection and deflection are not problems because each node reacts only to the first detected event. Those reflected or deflected waves come later than the line-of-sight waves. The only thing the system needs to maintain is an appropriate time interval between two successive localization events. • We assume that background noise exists, and therefore we run a band-pass filter to listen to a particular wave frequency. This reduces the chances of false detection.
The parameter that affects the localization event generation here is the source location of the event. The different distances between each node and the event source determine the rank of each node in the node sequence. Using the node sequences, the MSP algorithm divides the whole area into many non-rectangular areas as shown in Figure 11. In this figure, the stars represent two previous event sources. The previous two propagations split the whole map into many areas by those dashed circles that pass one of the anchors. Each node is located in one of the small areas. Since sequence-based MSP, iterative MSP and DBE MSP make no assumptions about the type of localization events and the shape of the area, all three optimization algorithms can be applied for the wave propagation scenario.
However, adaptive MSP needs more explanation. Figure 11 illustrates an example of nodes" voting for next event source locations. Unlike the straight-line scan, the critical parameter now is the location of the event source, because the distance between each node and the event source determines the rank of the node in the sequence. In Figure 11, if the next event breaks out along/near the solid thick gray line, which perpendicularly bisects the solid dark line between anchor C and the center of gravity of node 9"s area (the gray area), the wave would reach anchor C and the center of gravity of node 9"s area at roughly the same time, which would relatively equally divide node 9"s area. Therefore, node 9 prefers to vote for the positions around the thick gray line.
For the sake of presentation, until now we have described MSP in an ideal case where a complete node sequence can be obtained with accurate time synchronization. In this section we describe how to make MSP work well under more realistic conditions. 21
For diverse reasons, such as sensor malfunction or natural obstacles, the nodes in the network could fail to detect localization events. In such cases, the node sequence will not be complete. This problem has two versions: • Anchor nodes are missing in the node sequence If some anchor nodes fail to respond to the localization events, then the system has fewer anchors. In this case, the solution is to generate more events to compensate for the loss of anchors so as to achieve the desired accuracy requirements. • Target nodes are missing in the node sequence There are two consequences when target nodes are missing. First, if these nodes are still be useful to sensing applications, they need to use other backup localization approaches (e.g., Centroid) to localize themselves with help from their neighbors who have already learned their own locations from MSP. Secondly, since in advanced MSP each node in the sequence may contribute to the overall system accuracy, dropping of target nodes from sequences could also reduce the accuracy of the localization. Thus, proper compensation procedures such as adding more localization events need to be launched.
In a sensor network without time synchronization support, nodes cannot be ordered into a sequence using timestamps. For such cases, we propose a listen-detect-assemble-report protocol, which is able to function independently without time synchronization. listen-detect-assemble-report requires that every node listens to the channel for the node sequence transmitted from its neighbors. Then, when the node detects the localization event, it assembles itself into the newest node sequence it has heard and reports the updated sequence to other nodes. Figure 12 (a) illustrates an example for the listen-detect-assemble-report protocol. For simplicity, in this figure we did not differentiate the target nodes from anchor nodes. A solid line between two nodes stands for a communication link. Suppose a straight line scans from left to right. Node 1 detects the event, and then it broadcasts the sequence (1) into the network. Node 2 and node
sequence propagates in the same direction with the scan as shown in Figure 12 (a). Finally, node 6 obtains a complete sequence (1,2,3,5,7,4,6).
In the case of ultrasound propagation, because the event propagation speed is much slower than that of radio, the listendetect-assemble-report protocol can work well in a situation where the node density is not very high. For instance, if the distance between two nodes along one direction is 10 meters, the 340m/s sound needs 29.4ms to propagate from one node to the other. While normally the communication data rate is 250Kbps in the WSN (e.g., CC2420 [1]), it takes only about
One problem that may occur using the listen-detectassemble-report protocol is multiple partial sequences as shown in Figure 12 (b). Two separate paths in the network may result in two sequences that could not be further combined. In this case, since the two sequences can only be processed as separate sequences, some order information is lost. Therefore the 1,2,5,4 1,3,7,4 1,2,3,5 1,2,3,5,7,4 1,2,3,5,7 1,2,3,5 1,3 1,2 1 2 3 5 7 4 6 1 1 1,3 1,2,3,5,7,4,6 1,2,5 1,3,7 1,3 1,2 1 2 3 5 7 4 6 1 1 1,3,7,4,6 1,2,5,4,6 (a) (b) (c) 1,3,2,5 1,3,2,5,7,4 1,3,2,5,7 1,3,2,5 1,3 1,2 1 2 3 5 7 4 6 1 1 1,3 1,3,2,5,7,4,6 Event Propagation Event Propagation Event Propagation Figure 12. Node Sequence without Time Synchronization accuracy of the system would decrease.
The other problem is the sequence flip problem. As shown in Figure 12 (c), because node 2 and node 3 are too close to each other along the scan direction, they detect the scan almost simultaneously. Due to the uncertainty such as media access delay, two messages could be transmitted out of order.
For example, if node 3 sends out its report first, then the order of node 2 and node 3 gets flipped in the final node sequence.
The sequence flip problem would appear even in an accurately synchronized system due to random jitter in node detection if an event arrives at multiple nodes almost simultaneously. A method addressing the sequence flip is presented in the next section.
Sequence flip problems can be solved with and without time synchronization. We firstly start with a scenario applying time synchronization. Existing solutions for time synchronization [12, 6] can easily achieve sub-millisecond-level accuracy. For example, FTSP [12] achieves 16.9µs (microsecond) average error for a two-node single-hop case. Therefore, we can comfortably assume that the network is synchronized with maximum error of 1000µs. However, when multiple nodes are located very near to each other along the event propagation direction, even when time synchronization with less than 1ms error is achieved in the network, sequence flip may still occur.
For example, in the sound wave propagation case, if two nodes are less than 0.34 meters apart, the difference between their detection timestamp would be smaller than 1 millisecond.
We find that sequence flip could not only damage system accuracy, but also might cause a fatal error in the MSP algorithm.
Figure 13 illustrates both detrimental results. In the left side of Figure 13(a), suppose node 1 and node 2 are so close to each other that it takes less than 0.5ms for the localization event to propagate from node 1 to node 2. Now unfortunately, the node sequence is mistaken to be (2,1). So node 1 is expected to be located to the right of node 2, such as at the position of the dashed node 1. According to the elimination rule in sequencebased MSP, the left part of node 1"s area is cut off as shown in the right part of Figure 13(a). This is a potentially fatal error, because node 1 is actually located in the dashed area which has been eliminated by mistake. During the subsequent eliminations introduced by other events, node 1"s area might be cut off completely, thus node 1 could consequently be erased from the map! Even in cases where node 1 still survives, its area actually does not cover its real location. 22 1 2 12 2 Lower boundary of 1 Upper boundary of 1 Flipped Sequence Fatal Elimination Error EventPropagation
Fatal Error 1 1 2 12 2 Lower boundary of 1 Upper boundary of 1 Flipped Sequence Safe Elimination EventPropagation
New lower boundary of 1 1 B (a) (b) B: Protection band Figure 13. Sequence Flip and Protection Band Another problem is not fatal but lowers the localization accuracy. If we get the right node sequence (1,2), node 1 has a new upper boundary which can narrow the area of node 1 as in Figure 3. Due to the sequence flip, node 1 loses this new upper boundary.
In order to address the sequence flip problem, especially to prevent nodes from being erased from the map, we propose a protection band compensation approach. The basic idea of protection band is to extend the boundary of the location area a little bit so as to make sure that the node will never be erased from the map. This solution is based on the fact that nodes have a high probability of flipping in the sequence if they are near to each other along the event propagation direction. If two nodes are apart from each other more than some distance, say, B, they rarely flip unless the nodes are faulty. The width of a protection band B, is largely determined by the maximum error in system time synchronization and the localization event propagation speed.
Figure 13(b) presents the application of the protection band.
Instead of eliminating the dashed part in Figure 13(a) for node 1, the new lower boundary of node 1 is set by shifting the original lower boundary of node 2 to the left by distance B. In this case, the location area still covers node 1 and protects it from being erased. In a practical implementation, supposing that the ultrasound event is used, if the maximum error of system time synchronization is 1ms, two nodes might flip with high probability if the timestamp difference between the two nodes is smaller than or equal to 1ms. Accordingly, we set the protection band B as 0.34m (the distance sound can propagate within
chances of fatal errors, although at the cost of localization accuracy. Empirical results obtained from our physical test-bed verified this conclusion.
In the case of using the listen-detect-assemble-report protocol, the only change we need to make is to select the protection band according to the maximum delay uncertainty introduced by the MAC operation and the event propagation speed. To bound MAC delay at the node side, a node can drop its report message if it experiences excessive MAC delay. This converts the sequence flip problem to the incomplete sequence problem, which can be more easily addressed by the method proposed in Section 7.1.
Our evaluation of MSP was conducted on three platforms: (i) an indoor system with 46 MICAz motes using straight-line scan, (ii) an outdoor system with 20 MICAz motes using sound wave propagation, and (iii) an extensive simulation under various kinds of physical settings.
In order to understand the behavior of MSP under numerous settings, we start our evaluation with simulations.
Then, we implemented basic MSP and all the advanced MSP methods for the case where time synchronization is available in the network. The simulation and implementation details are omitted in this paper due to space constraints, but related documents [25] are provided online at http://www.cs.umn.edu/∼zhong/MSP. Full implementation and evaluation of system without time synchronization are yet to be completed in the near future.
In simulation, we assume all the node sequences are perfect so as to reveal the performance of MSP achievable in the absence of incomplete node sequences or sequence flips. In our simulations, all the anchor nodes and target nodes are assumed to be deployed uniformly. The mean and maximum errors are averaged over 50 runs to obtain high confidence. For legibility reasons, we do not plot the confidence intervals in this paper.
All the simulations are based on the straight-line scan example.
We implement three scan strategies: • Random Scan: The slope of the scan line is randomly chosen at each time. • Regular Scan: The slope is predetermined to rotate uniformly from 0 degree to 180 degrees. For example, if the system scans 6 times, then the scan angles would be: 0, 30, 60, 90, 120, and 150. • Adaptive Scan: The slope of each scan is determined based on the localization results from previous scans.
We start with basic MSP and then demonstrate the performance improvements one step at a time by adding (i) sequencebased MSP, (ii) iterative MSP, (iii) DBE MSP and (iv) adaptive MSP.
The evaluation starts with basic MSP, where we compare the performance of random scan and regular scan under different configurations. We intend to illustrate the impact of the number of anchors M, the number of scans d, and target node density (number of target nodes N in a fixed-size region) on the localization error. Table 1 shows the default simulation parameters.
The error of each node is defined as the distance between the estimated location and the real position. We note that by default we only use three anchors, which is considerably fewer than existing range-free solutions [8, 4].
Impact of the Number of Scans: In this experiment, we compare regular scan with random scan under a different number of scans from 3 to 30 in steps of 3. The number of anchors Table 1. Default Configuration Parameters Parameter Description Field Area 200×200 (Grid Unit) Scan Type Regular (Default)/Random Scan Anchor Number 3 (Default) Scan Times 6 (Default) Target Node Number 100 (Default) Statistics Error Mean/Max Random Seeds 50 runs 23
0 10 20 30 40 50 60 70 80 90 Mean Error and Max Error VS Scan Time Scan Time Error Max Error of Random Scan Max Error of Regular Scan Mean Error of Random Scan Mean Error of Regular Scan (a) Error vs. Number of Scans
0 10 20 30 40 50 60 Mean Error and Max Error VS Anchor Number Anchor Number Error Max Error of Random Scan Max Error of Regular Scan Mean Error of Random Scan Mean Error of Regular Scan (b) Error vs. Anchor Number
10 20 30 40 50 60 70 Mean Error and Max Error VS Target Node Number Target Node Number Error Max Error of Random Scan Max Error of Regular Scan Mean Error of Random Scan Mean Error of Regular Scan (c) Error vs. Number of Target Nodes Figure 14. Evaluation of Basic MSP under Random and Regular Scans
0 10 20 30 40 50 60 70 Basic MSP VS Sequence Based MSP II Scan Time Error Max Error of Basic MSP Max Error of Seq MSP Mean Error of Basic MSP Mean Error of Seq MSP (a) Error vs. Number of Scans
0 5 10 15 20 25 30 35 40 45 50 Basic MSP VS Sequence Based MSP I Anchor Number Error Max Error of Basic MSP Max Error of Seq MSP Mean Error of Basic MSP Mean Error of Seq MSP (b) Error vs. Anchor Number
5 10 15 20 25 30 35 40 45 50 55 Basic MSP VS Sequence Based MSP III Target Node Number Error Max Error of Basic MSP Max Error of Seq MSP Mean Error of Basic MSP Mean Error of Seq MSP (c) Error vs. Number of Target Nodes Figure 15. Improvements of Sequence-Based MSP over Basic MSP is 3 by default. Figure 14(a) indicates the following: (i) as the number of scans increases, the localization error decreases significantly; for example, localization errors drop more than 60% from 3 scans to 30 scans; (ii) statistically, regular scan achieves better performance than random scan under identical number of scans. However, the performance gap reduces as the number of scans increases. This is expected since a large number of random numbers converges to a uniform distribution. Figure 14(a) also demonstrates that MSP requires only a small number of anchors to perform very well, compared to existing range-free solutions [8, 4].
Impact of the Number of Anchors: In this experiment, we compare regular scan with random scan under different number of anchors from 3 to 30 in steps of 3. The results shown in Figure 14(b) indicate that (i) as the number of anchor nodes increases, the localization error decreases, and (ii) statistically, regular scan obtains better results than random scan with identical number of anchors. By combining Figures 14(a) and 14(b), we can conclude that MSP allows a flexible tradeoff between physical cost (anchor nodes) and soft cost (localization events).
Impact of the Target Node Density: In this experiment, we confirm that the density of target nodes has no impact on the accuracy, which motivated the design of sequence-based MSP.
In this experiment, we compare regular scan with random scan under different number of target nodes from 10 to 190 in steps of 20. Results in Figure 14(c) show that mean localization errors remain constant across different node densities. However, when the number of target nodes increases, the average maximum error increases.
Summary: From the above experiments, we can conclude that in basic MSP, regular scan are better than random scan under different numbers of anchors and scan events. This is because regular scans uniformly eliminate the map from different directions, while random scans would obtain sequences with redundant overlapping information, if two scans choose two similar scanning slopes.
This section evaluates the benefits of exploiting the order information among target nodes by comparing sequence-based MSP with basic MSP. In this and the following sections, regular scan is used for straight-line scan event generation. The purpose of using regular scan is to keep the scan events and the node sequences identical for both sequence-based MSP and basic MSP, so that the only difference between them is the sequence processing procedure.
Impact of the Number of Scans: In this experiment, we compare sequence-based MSP with basic MSP under different number of scans from 3 to 30 in steps of 3. Figure 15(a) indicates significant performance improvement in sequence-based MSP over basic MSP across all scan settings, especially when the number of scans is large. For example, when the number of scans is 30, errors in sequence-based MSP are about 20% of that of basic MSP. We conclude that sequence-based MSP performs extremely well when there are many scan events.
Impact of the Number of Anchors: In this experiment, we use different number of anchors from 3 to 30 in steps of 3. As seen in Figure 15(b), the mean error and maximum error of sequence-based MSP is much smaller than that of basic MSP.
Especially when there is limited number of anchors in the system, e.g., 3 anchors, the error rate was almost halved by using sequence-based MSP. This phenomenon has an interesting explanation: the cutting lines created by anchor nodes are exploited by both basic MSP and sequence-based MSP, so as the 24
0 5 10 15 20 25 30 35 40 45 50 Basic MSP VS Iterative MSP Iterative Times Error Max Error of Iterative Seq MSP Mean Error of Iterative Seq MSP Max Error of Basic MSP Mean Error of Basic MSP Figure 16. Improvements of Iterative MSP
0
1 DBE VS Non−DBE Error CumulativeDistrubutioinFunctions(CDF) Mean Error CDF of DBE MSP Mean Error CDF of Non−DBE MSP Max Error CDF of DBE MSP Max Error CDF of Non−DBE MSP Figure 17. Improvements of DBE MSP
0 10 20 30 40 50 60 70 Adaptive MSP for 500by80 Target Node Number Error Max Error of Regualr Scan Max Error of Adaptive Scan Mean Error of Regualr Scan Mean Error of Adaptive Scan (a) Adaptive MSP for 500 by 80 field
0
1 Mean Error CDF at Different Angle Steps in Adaptive Scan Mean Error CumulativeDistrubutioinFunctions(CDF)
(b) Impact of the Number of Candidate Events Figure 18. The Improvements of Adaptive MSP number of anchor nodes increases, anchors tend to dominate the contribution. Therefore the performance gaps lessens.
Impact of the Target Node Density: Figure 15(c) demonstrates the benefits of exploiting order information among target nodes. Since sequence-based MSP makes use of the information among the target nodes, having more target nodes contributes to the overall system accuracy. As the number of target nodes increases, the mean error and maximum error of sequence-based MSP decreases. Clearly the mean error in basic MSP is not affected by the number of target nodes, as shown in Figure 15(c).
Summary: From the above experiments, we can conclude that exploiting order information among target nodes can improve accuracy significantly, especially when the number of events is large but with few anchors.
In this experiment, the same node sequences were processed iteratively multiple times. In Figure 16, the two single marks are results from basic MSP, since basic MSP doesn"t perform iterations. The two curves present the performance of iterative MSP under different numbers of iterations c. We note that when only a single iteration is used, this method degrades to sequence-based MSP. Therefore, Figure 16 compares the three methods to one another.
Figure 16 shows that the second iteration can reduce the mean error and maximum error dramatically. After that, the performance gain gradually reduces, especially when c > 5.
This is because the second iteration allows earlier scans to exploit the new boundaries created by later scans in the first iteration. Such exploitation decays quickly over iterations.
Figure 17, in which we augment iterative MSP with distribution-based estimation (DBE MSP), shows that DBE MSP could bring about statistically better performance.
Figure 17 presents cumulative distribution localization errors. In general, the two curves of the DBE MSP lay slightly to the left of that of non-DBE MSP, which indicates that DBE MSP has a smaller statistical mean error and averaged maximum error than non-DBE MSP. We note that because DBE is augmented on top of the best solution so far, the performance improvement is not significant. When we apply DBE on basic MSP methods, the improvement is much more significant. We omit these results because of space constraints.
This section illustrates the performance of adaptive MSP over non-adaptive MSP. We note that feedback-based adaptation can be applied to all MSP methods, since it affects only the scanning angles but not the sequence processing. In this experiment, we evaluated how adaptive MSP can improve the best solution so far. The default angle granularity (step) for adaptive searching is 5 degrees.
Impact of Area Shape: First, if system settings are regular, the adaptive method hardly contributes to the results. For a square area (regular), the performance of adaptive MSP and regular scans are very close. However, if the shape of the area is not regular, adaptive MSP helps to choose the appropriate localization events to compensate. Therefore, adaptive MSP can achieve a better mean error and maximum error as shown in Figure 18(a). For example, adaptive MSP improves localization accuracy by 30% when the number of target nodes is
Impact of the Target Node Density: Figure 18(a) shows that when the node density is low, adaptive MSP brings more benefit than when node density is high. This phenomenon makes statistical sense, because the law of large numbers tells us that node placement approaches a truly uniform distribution when the number of nodes is increased. Adaptive MSP has an edge 25 Figure 19. The Mirage Test-bed (Line Scan) Figure 20. The 20-node Outdoor Experiments (Wave) when layout is not uniform.
Impact of Candidate Angle Density: Figure 18(b) shows that the smaller the candidate scan angle step, the better the statistical performance in terms of mean error. The rationale is clear, as wider candidate scan angles provide adaptive MSP more opportunity to choose the one approaching the optimal angle.
Starting from basic MSP, we have demonstrated step-bystep how four optimizations can be applied on top of each other to improve localization performance. In other words, these optimizations are compatible with each other and can jointly improve the overall performance. We note that our simulations were done under assumption that the complete node sequence can be obtained without sequence flips. In the next section, we present two real-system implementations that reveal and address these practical issues.
In this section, we present a system implementation of MSP on two physical test-beds. The first one is called Mirage, a large indoor test-bed composed of six 4-foot by 8-foot boards, illustrated in Figure 19. Each board in the system can be used as an individual sub-system, which is powered, controlled and metered separately. Three Hitachi CP-X1250 projectors, connected through a Matorx Triplehead2go graphics expansion box, are used to create an ultra-wide integrated display on six boards. Figure 19 shows that a long tilted line is generated by the projectors. We have implemented all five versions of MSP on the Mirage test-bed, running 46 MICAz motes. Unless mentioned otherwise, the default setting is 3 anchors and 6 scans at the scanning line speed of 8.6 feet/s. In all of our graphs, each data point represents the average value of 50 trials. In the outdoor system, a Dell A525 speaker is used to generate 4.7KHz sound as shown in Figure 20. We place 20 MICAz motes in the backyard of a house. Since the location is not completely open, sound waves are reflected, scattered and absorbed by various objects in the vicinity, causing a multi-path effect. In the system evaluation, simple time synchronization mechanisms are applied on each node.
During indoor experiments, we encountered several realworld problems that are not revealed in the simulation. First, sequences obtained were partial due to misdetection and message losses. Second, elements in the sequences could flip due to detection delay, uncertainty in media access, or error in time synchronization. We show that these issues can be addressed by using the protection band method described in Section 7.3.
In this experiment, we studied the impact of the scanning speed and the length of protection band on the performance of the system. In general, with increasing scanning speed, nodes have less time to respond to the event and the time gap between two adjacent nodes shrinks, leading to an increasing number of partial sequences and sequence flips.
Figure 21 shows the node flip situations for six scans with distinct angles under different scan speeds. The x-axis shows the distance between the flipped nodes in the correct node sequence. y-axis shows the total number of flips in the six scans.
This figure tells us that faster scan brings in not only increasing number of flips, but also longer-distance flips that require wider protection band to prevent from fatal errors.
Figure 22(a) shows the effectiveness of the protection band in terms of reducing the number of unlocalized nodes. When we use a moderate scan speed (4.3feet/s), the chance of flipping is rare, therefore we can achieve 0.45 feet mean accuracy (Figure 22(b)) with 1.6 feet maximum error (Figure 22(c)). With increasing speeds, the protection band needs to be set to a larger value to deal with flipping. Interesting phenomena can be observed in Figures 22: on one hand, the protection band can sharply reduce the number of unlocalized nodes; on the other hand, protection bands enlarge the area in which a target would potentially reside, introducing more uncertainty. Thus there is a concave curve for both mean and maximum error when the scan speed is at 8.6 feet/s.
In this experiment, we show the improvements resulting from three different methods. Figure 23(a) shows that a protection band of 0.35 feet is sufficient for the scan speed of
MSP (with adaptation) achieves best performance. For example, Figures 23(b) shows that when we set the protection band at 0.05 feet, iterative MSP achieves 0.7 feet accuracy, which is 42% more accurate than the basic design. Similarly,
Figures 23(b) and 23(c) show the double-edged effects of protection band on the localization accuracy.
0 20 40 (3) Flip Distribution for 6 Scans at Line Speed of 14.6feet/s Flips Node Distance in the Ideal Node Sequence
0 20 40 (2) Flip Distribution for 6 Scans at Line Speed of 8.6feet/s Flips
0 20 40 (1) Flip Distribution for 6 Scans at Line Speed of 4.3feet/s Flips Figure 21. Number of Flips for Different Scan Speeds 26
0 2 4 6 8 10 12 14 16 18 20 Unlocalized Node Number(Line Scan at Different Speed) Protection Band (in feet) UnlocalizedNodeNumber Scan Line Speed: 14.6feet/s Scan Line Speed: 8.6feet/s Scan Line Speed: 4.3feet/s (a) Number of Unlocalized Nodes
1
Mean Error(Line Scan at Different Speed) Protection Band (in feet) Error(infeet) Scan Line Speed:14.6feet/s Scan Line Speed: 8.6feet/s Scan Line Speed: 4.3feet/s (b) Mean Localization Error
2
3
4 Max Error(Line Scan at Different Speed) Protection Band (in feet) Error(infeet) Scan Line Speed: 14.6feet/s Scan Line Speed: 8.6feet/s Scan Line Speed: 4.3feet/s (c) Max Localization Error Figure 22. Impact of Protection Band and Scanning Speed
0 2 4 6 8 10 12 14 16 18 20 Unlocalized Node Number(Scan Line Speed 8.57feet/s) Protection Band (in feet) Numberofunlocalizednodeoutof46 Unlocalized node of Basic MSP Unlocalized node of Sequence Based MSP Unlocalized node of Iterative MSP (a) Number of Unlocalized Nodes
1
Mean Error(Scan Line Speed 8.57feet/s) Protection Band (in feet) Error(infeet) Mean Error of Basic MSP Mean Error of Sequence Based MSP Mean Error of Iterative MSP (b) Mean Localization Error
2
3
4 Max Error(Scan Line Speed 8.57feet/s) Protection Band (in feet) Error(infeet) Max Error of Basic MSP Max Error of Sequence Based MSP Max Error of Iterative MSP (c) Max Localization Error Figure 23. Impact of Protection Band under Different MSP Methods
0
1
2
Unlocalized Node Number(Protection Band: 0.35 feet) Anchor Number UnlocalizedNodeNumber
(a) Number of Unlocalized Nodes
1 Mean Error(Protection Band: 0.35 feet) Anchor Number Error(infeet) Mean Error of 4 Scan Events at Speed 8.75feet/s Mean Error of 6 Scan Events at Speed 8.75feet/s Mean Error of 8 Scan Events at Speed 8.75feet/s (b) Mean Localization Error
1
2
Max Error(Protection Band: 0.35 feet) Anchor Number Error(infeet) Max Error of 4 Scan Events at Speed 8.75feet/s Max Error of 6 Scan Events at Speed 8.75feet/s Max Error of 8 Scan Events at Speed 8.75feet/s (c) Max Localization Error Figure 24. Impact of the Number of Anchors and Scans
In this experiment, we show a tradeoff between hardware cost (anchors) with soft cost (events). Figure 24(a) shows that with more cutting lines created by anchors, the chance of unlocalized nodes increases slightly. We note that with a 0.35 feet protection band, the percentage of unlocalized nodes is very small, e.g., in the worst-case with 11 anchors, only 2 out of 46 nodes are not localized due to flipping. Figures 24(b) and 24(c) show the tradeoff between number of anchors and the number of scans. Obviously, with the number of anchors increases, the error drops significantly. With 11 anchors we can achieve a localization accuracy as low as 0.25 ∼ 0.35 feet, which is nearly a 60% improvement. Similarly, with increasing number of scans, the accuracy drops significantly as well. We can observe about 30% across all anchor settings when we increase the number of scans from 4 to 8. For example, with only 3 anchors, we can achieve 0.6-foot accuracy with 8 scans.
The outdoor system evaluation contains two parts: (i) effective detection distance evaluation, which shows that the node sequence can be readily obtained, and (ii) sound propagation based localization, which shows the results of wavepropagation-based localization.
We firstly evaluate the sequence flip phenomenon in wave propagation. As shown in Figure 25, 20 motes were placed as five groups in front of the speaker, four nodes in each group at roughly the same distances to the speaker. The gap between each group is set to be 2, 3, 4 and 5 feet respectively in four experiments. Figure 26 shows the results. The x-axis in each subgraph indicates the group index. There are four nodes in each group (4 bars). The y-axis shows the detection rank (order) of each node in the node sequence. As distance between each group increases, number of flips in the resulting node sequence 27 Figure 25. Wave Detection
0 5 10 15 20
Rank Group Index
0 5 10 15 20
Rank Group Index
0 5 10 15 20
Rank Group Index
0 5 10 15 20
Rank Group Index Figure 26. Ranks vs. Distances 0 2 4 6 8 10 12 14 16 18 20 22 24
Y-Dimension(feet) X-Dimension (feet) Node 0 2 4 6 8 10 12 14 16 18 20 22 24
Y-Dimension(feet) X-Dimension (feet) Anchor Figure 27. Localization Error (Sound) decreases. For example, in the 2-foot distance subgraph, there are quite a few flips between nodes in adjacent and even nonadjacent groups, while in the 5-foot subgraph, flips between different groups disappeared in the test.
As shown in Figure 20, 20 motes are placed as a grid including 5 rows with 5 feet between each row and 4 columns with
propagation events are generated around the mote grid by a speaker.
Figure 27 shows the localization results using iterative MSP (3 times iterative processing) with a protection band of 3 feet.
The average error of the localization results is 3 feet and the maximum error is 5 feet with one un-localized node.
We found that sequence flip in wave propagation is more severe than that in the indoor, line-based test. This is expected due to the high propagation speed of sound. Currently we use MICAz mote, which is equipped with a low quality microphone. We believe that using a better speaker and more events, the system can yield better accuracy. Despite the hardware constrains, the MSP algorithm still successfully localized most of the nodes with good accuracy.
In this paper, we present the first work that exploits the concept of node sequence processing to localize sensor nodes. We demonstrated that we could significantly improve localization accuracy by making full use of the information embedded in multiple easy-to-get one-dimensional node sequences. We proposed four novel optimization methods, exploiting order and marginal distribution among non-anchor nodes as well as the feedback information from early localization results.
Importantly, these optimization methods can be used together, and improve accuracy additively. The practical issues of partial node sequence and sequence flip were identified and addressed in two physical system test-beds. We also evaluated performance at scale through analysis as well as extensive simulations. Results demonstrate that requiring neither costly hardware on sensor nodes nor precise event distribution, MSP can achieve a sub-foot accuracy with very few anchor nodes provided sufficient events.
[1] CC2420 Data Sheet. Avaiable at http://www.chipcon.com/. [2] P. Bahl and V. N. Padmanabhan. Radar: An In-Building RF-Based User Location and Tracking System. In IEEE Infocom "00. [3] M. Broxton, J. Lifton, and J. Paradiso. Localizing A Sensor Network via Collaborative Processing of Global Stimuli. In EWSN "05. [4] N. Bulusu, J. Heidemann, and D. Estrin. GPS-Less Low Cost Outdoor Localization for Very Small Devices. IEEE Personal Communications Magazine, 7(4), 2000. [5] D. Culler, D. Estrin, and M. Srivastava. Overview of Sensor Networks.
IEEE Computer Magazine, 2004. [6] J. Elson, L. Girod, and D. Estrin. Fine-Grained Network Time Synchronization Using Reference Broadcasts. In OSDI "02. [7] D. K. Goldenberg, P. Bihler, M. Gao, J. Fang, B. D. Anderson, A. Morse, and Y. Yang. Localization in Sparse Networks Using Sweeps. In MobiCom "06. [8] T. He, C. Huang, B. M. Blum, J. A. Stankovic, and T. Abdelzaher.
RangeFree Localization Schemes in Large-Scale Sensor Networks. In MobiCom "03. [9] B. Kusy, P. Dutta, P. Levis, M. Mar, A. Ledeczi, and D. Culler. Elapsed Time on Arrival: A Simple and Versatile Primitive for Canonical Time Synchronization Services. International Journal of ad-hoc and Ubiquitous Computing, 2(1), 2006. [10] L. Lazos and R. Poovendran. SeRLoc: Secure Range-Independent Localization for Wireless Sensor Networks. In WiSe "04. [11] M. Maroti, B. Kusy, G. Balogh, P. Volgyesi, A. Nadas, K. Molnar,
S. Dora, and A. Ledeczi. Radio Interferometric Geolocation. In SenSys "05. [12] M. Maroti, B. Kusy, G. Simon, and A. Ledeczi. The Flooding Time Synchronization Protocol. In SenSys "04. [13] D. Moore, J. Leonard, D. Rus, and S. Teller. Robust Distributed Network Localization with Noise Range Measurements. In SenSys "04. [14] R. Nagpal and D. Coore. An Algorithm for Group Formation in an Amorphous Computer. In PDCS "98. [15] D. Niculescu and B. Nath. ad-hoc Positioning System. In GlobeCom "01. [16] D. Niculescu and B. Nath. ad-hoc Positioning System (APS) Using AOA. In InfoCom "03. [17] N. B. Priyantha, A. Chakraborty, and H. Balakrishnan. The Cricket Location-Support System. In MobiCom "00. [18] K. R¨omer. The Lighthouse Location System for Smart Dust. In MobiSys "03. [19] A. Savvides, C. C. Han, and M. B. Srivastava. Dynamic Fine-Grained Localization in ad-hoc Networks of Sensors. In MobiCom "01. [20] R. Stoleru, T. He, J. A. Stankovic, and D. Luebke. A High-Accuracy,

Wireless Sensor Networks (WSN) have been envisioned to revolutionize the way humans perceive and interact with the surrounding environment. One vision is to embed tiny sensor devices in outdoor environments, by aerial deployments from unmanned air vehicles. The sensor nodes form a network and collaborate (to compensate for the extremely scarce resources available to each of them: computational power, memory size, communication capabilities) to accomplish the mission. Through collaboration, redundancy and fault tolerance, the WSN is then able to achieve unprecedented sensing capabilities.
A major step forward has been accomplished by developing systems for several domains: military surveillance [1] [2] [3], habitat monitoring [4] and structural monitoring [5].
Even after these successes, several research problems remain open. Among these open problems is sensor node localization, i.e., how to find the physical position of each sensor node. Despite the attention the localization problem in WSN has received, no universally acceptable solution has been developed. There are several reasons for this. On one hand, localization schemes that use ranging are typically high end solutions. GPS ranging hardware consumes energy, it is relatively expensive (if high accuracy is required) and poses form factor challenges that move us away from the vision of dust size sensor nodes. Ultrasound has a short range and is highly directional. Solutions that use the radio transceiver for ranging either have not produced encouraging results (if the received signal strength indicator is used) or are sensitive to environment (e.g., multipath). On the other hand, localization schemes that only use the connectivity information for inferring location information are characterized by low accuracies: ≈ 10 ft in controlled environments, 40−50 ft in realistic ones.
To address these challenges, we propose a framework for WSN localization, called StarDust, in which the complexity associated with the node localization is completely removed from the sensor node. The basic principle of the framework is localization through passivity: each sensor node is equipped with a corner-cube retro-reflector and possibly an optical filter (a coloring device). An aerial vehicle projects light onto the deployment area and records images containing retro-reflected light beams (they appear as luminous spots). Through image processing techniques, the locations of the retro-reflectors (i.e., sensor nodes) is deter57 mined. For inferring the identity of the sensor node present at a particular location, the StarDust framework develops a constraint-based node ID relaxation algorithm.
The main contributions of our work are the following. We propose a novel framework for node localization in WSNs that is very promising and allows for many future extensions and more accurate results. We propose a constraint-based label relaxation algorithm for mapping node IDs to the locations, and four constraints (node, connectivity, time and space), which are building blocks for very accurate and very fast localization systems. We develop a sensor node hardware prototype, called a SensorBall. We evaluate the performance of a localization system for which we obtain location accuracies of 2 − 5 ft with a localization duration ranging from 10 milliseconds to 2 minutes. We investigate the range of a system built on our framework by considering realities of physical phenomena that occurs during light propagation through the atmosphere.
The rest of the paper is structured as follows. Section 2 is an overview of the state of art. The design of the StarDust framework is presented in Section 3. One implementation and its performance evaluation are in Sections 4 and 5, followed by a suite of system optimization techniques, in Section 6. In Section 7 we present our conclusions.
We present the prior work in localization in two major categories: the range-based, and the range-free schemes.
The range-based localization techniques have been designed to use either more expensive hardware (and hence higher accuracy) or just the radio transceiver. Ranging techniques dependent on hardware are the time-of-flight (ToF) and the time-difference-of-arrival(TDoA). Solutions that use the radio are based on the received signal strength indicator (RSSI) and more recently on radio interferometry.
The ToF localization technique that is most widely used is the GPS. GPS is a costly solution for a high accuracy localization of a large scale sensor network. AHLoS [6] employs a TDoA ranging technique that requires extensive hardware and solves relatively large nonlinear systems of equations.
The Cricket location-support system (TDoA) [7] can achieve a location granularity of tens of inches with highly directional and short range ultrasound transceivers. In [2] the location of a sniper is determined in an urban terrain, by using the TDoA between an acoustic wave and a radio beacon.
The PushPin project [8] uses the TDoA between ultrasound pulses and light flashes for node localization. The RADAR system [9] uses the RSSI to build a map of signal strengths as emitted by a set of beacon nodes. A mobile node is located by the best match, in the signal strength space, with a previously acquired signature. In MAL [10], a mobile node assists in measuring the distances (acting as constraints) between nodes until a rigid graph is generated. The localization problem is formulated as an on-line state estimation in a nonlinear dynamic system [11]. A cooperative ranging that attempts to achieve a global positioning from distributed local optimizations is proposed in [12]. A very recent, remarkable, localization technique is based on radio interferometry, RIPS [13], which utilizes two transmitters to create an interfering signal. The frequencies of the emitters are very close to each other, thus the interfering signal will have a low frequency envelope that can be easily measured. The ranging technique performs very well. The long time required for localization and multi-path environments pose significant challenges.
Real environments create additional challenges for the range based localization schemes. These have been emphasized by several studies [14] [15] [16]. To address these challenges, and others (hardware cost, the energy expenditure, the form factor, the small range, localization time), several range-free localization schemes have been proposed. Sensor nodes use primarily connectivity information for inferring proximity to a set of anchors. In the Centroid localization scheme [17], a sensor node localizes to the centroid of its proximate beacon nodes. In APIT [18] each node decides its position based on the possibility of being inside or outside of a triangle formed by any three beacons within node"s communication range. The Gradient algorithm [19], leverages the knowledge about the network density to infer the average one hop length. This, in turn, can be transformed into distances to nodes with known locations. DV-Hop [20] uses the hop by hop propagation capability of the network to forward distances to landmarks. More recently, several localization schemes that exploit the sensing capabilities of sensor nodes, have been proposed. Spotlight [21] creates well controlled (in time and space) events in the network while the sensor nodes detect and timestamp this events. From the spatiotemporal knowledge for the created events and the temporal information provided by sensor nodes, nodes" spatial information can be obtained. In a similar manner, the Lighthouse system [22] uses a parallel light beam, that is emitted by an anchor which rotates with a certain period. A sensor node detects the light beam for a period of time, which is dependent on the distance between it and the light emitting device.
Many of the above localization solutions target specific sets of requirements and are useful for specific applications.
StarDust differs in that it addresses a particular demanding set of requirements that are not yet solved well. StarDust is meant for localizing air dropped nodes where node passiveness, high accuracy, low cost, small form factor and rapid localization are all required. Many military applications have such requirements.
The design of the StarDust system (and its name) was inspired by the similarity between a deployed sensor network, in which sensor nodes indicate their presence by emitting light, and the Universe consisting of luminous and illuminated objects: stars, galaxies, planets, etc.
The main difficulty when applying the above ideas to the real world is the complexity of the hardware that needs to be put on a sensor node so that the emitted light can be detected from thousands of feet. The energy expenditure for producing an intense enough light beam is also prohibitive.
Instead, what we propose to use for sensor node localization is a passive optical element called a retro-reflector.
The most common retro-reflective optical component is a Corner-Cube Retroreflector (CCR), shown in Figure 1(a). It consists of three mutually perpendicular mirrors. The inter58 (a) (b) Figure 1. Corner-Cube Retroreflector (a) and an array of CCRs molded in plastic (b) esting property of this optical component is that an incoming beam of light is reflected back, towards the source of the light, irrespective of the angle of incidence. This is in contrast with a mirror, which needs to be precisely positioned to be perpendicular to the incident light. A very common and inexpensive implementation of an array of CCRs is the retroreflective plastic material used on cars and bicycles for night time detection, shown in Figure 1(b).
In the StarDust system, each node is equipped with a small (e.g. 0.5in2) array of CCRs and the enclosure has self-righting capabilities that orient the array of CCRs predominantly upwards. It is critical to understand that the upward orientation does not need to be exact. Even when large angular variations from a perfectly upward orientation are present, a CCR will return the light in the exact same direction from which it came.
In the remaining part of the section, we present the architecture of the StarDust system and the design of its main components.
The envisioned sensor network localization scenario is as follows: • The sensor nodes are released, possibly in a controlled manner, from an aerial vehicle during the night. • The aerial vehicle hovers over the deployment area and uses a strobe light to illuminate it. The sensor nodes, equipped with CCRs and optical filters (acting as coloring devices) have self-righting capabilities and retroreflect the incoming strobe light. The retro-reflected light is either white, as the originating source light, or colored, due to optical filters. • The aerial vehicle records a sequence of two images very close in time (msec level). One image is taken when the strobe light is on, the other when the strobe light is off. The acquired images are used for obtaining the locations of sensor nodes (which appear as luminous spots in the image). • The aerial vehicle executes the mapping of node IDs to the identified locations in one of the following ways: a) by using the color of a retro-reflected light, if a sensor node has a unique color; b) by requiring sensor nodes to establish neighborhood information and report it to a base station; c) by controlling the time sequence of sensor nodes deployment and recording additional imLight Emitter Sensor Node i Transfer Function Φi(λ) Ψ(λ) Φ(Ψ(λ)) Image Processing Node ID Matching Radio Model R G(Λ,E) Central Device V" V" Figure 2. The StarDust system architecture ages; d) by controlling the location where a sensor node is deployed. • The computed locations are disseminated to the sensor network.
The architecture of the StarDust system is shown in Figure 2. The architecture consists of two main components: the first is centralized and it is located on a more powerful device. The second is distributed and it resides on all sensor nodes. The Central Device consists of the following: the Light Emitter, the Image Processing module, the Node ID Mapping module and the Radio Model. The distributed component of the architecture is the Transfer Function, which acts as a filter for the incoming light. The aforementioned modules are briefly described below: • Light Emitter - It is a strobe light, capable of producing very intense, collimated light pulses. The emitted light is non-monochromatic (unlike a laser) and it is characterized by a spectral density Ψ(λ), a function of the wavelength. The emitted light is incident on the CCRs present on sensor nodes. • Transfer Function Φ(Ψ(λ)) - This is a bandpass filter for the incident light on the CCR. The filter allows a portion of the original spectrum, to be retro-reflected.
From here on, we will refer to the transfer function as the color of a sensor node. • Image Processing - The Image Processing module acquires high resolution images. From these images the locations and the colors of sensor nodes are obtained.
If only one set of pictures can be taken (i.e., one location of the light emitter/image analysis device), then the map of the field is assumed to be known as well as the distance between the imaging device and the field. The aforementioned assumptions (field map and distance to it) are not necessary if the images can be simultaneously taken from different locations. It is important to remark here that the identity of a node can not be directly obtained through Image Processing alone, unless a specific characteristic of a sensor node can be identified in the image. • Node ID Matching - This module uses the detected locations and through additional techniques (e.g., sensor node coloring and connectivity information (G(Λ,E)) from the deployed network) to uniquely identify the sensor nodes observed in the image. The connectivity information is represented by neighbor tables sent from 59 Algorithm 1 Image Processing 1: Background filtering 2: Retro-reflected light recognition through intensity filtering 3: Edge detection to obtain the location of sensor nodes 4: Color identification for each detected sensor node each sensor node to the Central Device. • Radio Model - This component provides an estimate of the radio range to the Node ID Matching module. It is only used by node ID matching techniques that are based on the radio connectivity in the network. The estimate of the radio range R is based on the sensor node density (obtained through the Image Processing module) and the connectivity information (i.e., G(Λ,E)).
The two main components of the StarDust architecture are the Image Processing and the Node ID Mapping. Their design and analysis is presented in the sections that follow.
The goal of the Image Processing Algorithm (IPA) is to identify the location of the nodes and their color. Note that IPA does not identify which node fell where, but only what is the set of locations where the nodes fell.
IPA is executed after an aerial vehicle records two pictures: one in which the field of deployment is illuminated and one when no illuminations is present. Let Pdark be the picture of the deployment area, taken when no light was emitted and Plight be the picture of the same deployment area when a strong light beam was directed towards the sensor nodes.
The proposed IPA has several steps, as shown in Algorithm 1. The first step is to obtain a third picture Pfilter where only the differences between Pdark and Plight remain. Let us assume that Pdark has a resolution of n × m, where n is the number of pixels in a row of the picture, while m is the number of pixels in a column of the picture. Then Pdark is composed of n × m pixels noted Pdark(i, j), i ∈ 1 ≤ i ≤ n,1 ≤ j ≤ m. Similarly Plight is composed of n × m pixels noted Plight(i, j), 1 ≤ i ≤ n,1 ≤ j ≤ m.
Each pixel P is described by an RGB value where the R value is denoted by PR, the G value is denoted by PG, and the B value is denoted by PB. IPA then generates the third picture, Pfilter, through the following transformations: PR filter(i, j) = PR light(i, j)−PR dark(i, j) PG filter(i, j) = PG light(i, j)−PG dark(i, j) PB filter(i, j) = PB light(i, j)−PB dark(i, j) (1) After this transformation, all the features that appeared in both Pdark and Plight are removed from Pfilter. This simplifies the recognition of light retro-reflected by sensor nodes.
The second step consists of identifying the elements contained in Pfilter that retro-reflect light. For this, an intensity filter is applied to Pfilter. First IPA converts Pfilter into a grayscale picture. Then the brightest pixels are identified and used to create Preflect. This step is eased by the fact that the reflecting nodes should appear much brighter than any other illuminated object in the picture.
Support: Q(λk) ni P1 ...
P2 ...
PN λ1 ... λk ... λN Figure 3. Probabilistic label relaxation The third step runs an edge detection algorithm on Preflect to identify the boundary of the nodes present. A tool such as Matlab provides a number of edge detection techniques. We used the bwboundaries function. For the obtained edges, the location (x,y) (in the image) of each node is determined by computing the centroid of the points constituting its edges.
Standard computer graphics techniques [23] are then used to transform the 2D locations of sensor nodes detected in multiple images into 3D sensor node locations. The color of the node is obtained as the color of the pixel located at (x,y) in Plight.
The goal of the Node ID Matching module is to obtain the identity (node ID) of a luminous spot in the image, detected to be a sensor node. For this, we define V = {(x1,y1),(x2,y2),...,(xm,ym)} to be the set of locations of the sensor nodes, as detected by the Image Processing module and Λ = {λ1,λ2,...,λm} to be the set of unique node IDs assigned to the m sensor nodes, before deployment. From here on, we refer to node IDs as labels.
We model the problem of finding the label λj of a node ni as a probabilistic label relaxation problem, frequently used in image processing/understanding. In the image processing domain, scene labeling (i.e., identifying objects in an image) plays a major role. The goal of scene labeling is to assign a label to each object detected in an image, such that an appropriate image interpretation is achieved. It is prohibitively expensive to consider the interactions among all the objects in an image. Instead, constraints placed among nearby objects generate local consistencies and through iteration, global consistencies can be obtained.
The main idea of the sensor node localization through probabilistic label relaxation is to iteratively compute the probability of each label being the correct label for a sensor node, by taking into account, at each iteration, the support for a label. The support for a label can be understood as a hint or proof, that a particular label is more likely to be the correct one, when compared with the other potential labels for a sensor node. We pictorially depict this main idea in Figure 3. As shown, node ni has a set of candidate labels {λ1,...,λk}. Each of the labels has a different value for the Support function Q(λk). We defer the explanation of how the Support function is implemented until the subsections that follow, where we provide four concrete techniques. Formally, the algorithm is outlined in Algorithm 2, where the equations necessary for computing the new probability Pni(λk) for a label λk of a node ni, are expressed by the 60 Algorithm 2 Label Relaxation 1: for each sensor node ni do 2: assign equal prob. to all possible labels 3: end for 4: repeat 5: converged ← true 6: for each sensor node ni do 7: for each each label λj of ni do 8: compute the Support label λj: Equation 4 9: end for 10: compute K for the node ni: Equation 3 11: for each each label λj do 12: update probability of label λj: Equation 2 13: if |new prob.−old prob.| ≥ ε then 14: converged ← false 15: end if 16: end for 17: end for 18: until converged = true following equations: Ps+1 ni (λk) = 1 Kni Ps ni (λk)Qs ni (λk) (2) where Kni is a normalizing constant, given by: Kni = N ∑ k=1 Ps ni (λk)Qs ni (λk) (3) and Qs ni (λk) is: Qs ni (λk) = support for label λk of node ni (4) The label relaxation algorithm is iterative and it is polynomial in the size of the network(number of nodes). The pseudo-code is shown in Algorithm 2. It initializes the probabilities associated with each possible label, for a node ni, through a uniform distribution. At each iteration s, the algorithm updates the probability associated with each label, by considering the Support Qs ni (λk) for each candidate label of a sensor node.
In the sections that follow, we describe four different techniques for implementing the Support function: based on node coloring, radio connectivity, the time of deployment (time) and the location of deployment (space). While some of these techniques are simplistic, they are primitives which, when combined, can create powerful localization systems.
These design techniques have different trade-offs, which we will present in Section 3.3.6.
The unique mapping between a sensor node"s position (identified by the image processing) and a label can be obtained by assigning a unique color to each sensor node. For this we define C = {c1,c2,...,cn} to be the set of unique colors available and M : Λ → C to be a one-to-one mapping of labels to colors. This mapping is known prior to the sensor node deployment (from node manufacturing).
In the case of color constrained label relaxation, the support for label λk is expressed as follows: Qs ni (λk) = 1 (5) As a result, the label relaxation algorithm (Algorithm 2) consists of the following steps: one label is assigned to each sensor node (lines 1-3 of the algorithm), implicitly having a probability Pni(λk) = 1 ; the algorithm executes a single iteration, when the support function, simply, reiterates the confidence in the unique labeling.
However, it is often the case that unique colors for each node will not be available. It is interesting to discuss here the influence that the size of the coloring space (i.e., |C|) has on the accuracy of the localization algorithm. Several cases are discussed below: • If |C| = 0, no colors are used and the sensor nodes are equipped with simple CCRs that reflect back all the incoming light (i.e., no filtering, and no coloring of the incoming light). From the image processing system, the position of sensor nodes can still be obtained. Since all nodes appear white, no single sensor node can be uniquely identified. • If |C| = m − 1 then there are enough unique colors for all nodes (one node remains white, i.e. no coloring), the problem is trivially solved. Each node can be identified, based on its unique color. This is the scenario for the relaxation with color constraints. • If |C| ≥ 1, there are several options for how to partition the coloring space. If C = {c1} one possibility is to assign the color c1 to a single node, and leave the remaining m−1 sensor nodes white, or to assign the color c1 to more than one sensor node. One can observe that once a color is assigned uniquely to a sensor node, in effect, that sensor node is given the status of anchor, or node with known location.
It is interesting to observe that there is an entire spectrum of possibilities for how to partition the set of sensor nodes in equivalence classes (where an equivalence class is represented by one color), in order to maximize the success of the localization algorithm. One of the goals of this paper is to understand how the size of the coloring space and its partitioning affect localization accuracy.
Despite the simplicity of this method of constraining the set of labels that can be assigned to a node, we will show that this technique is very powerful, when combined with other relaxation techniques.
Connectivity information, obtained from the sensor network through beaconing, can provide additional information for locating sensor nodes. In order to gather connectivity information, the following need to occur: 1) after deployment, through beaconing of HELLO messages, sensor nodes build their neighborhood tables; 2) each node sends its neighbor table information to the Central device via a base station.
First, let us define G = (Λ,E) to be the weighted connectivity graph built by the Central device from the received neighbor table information. In G the edge (λi,λj) has a 61 λ1 λ2 ... λN ni nj gi2,j2 λ1 λ2 ... λN Pj,λ1 Pj,λ2 ...
Pj,λN Pi,λ1 Pi,λ1 ...
Pi,λN gi2,jm Figure 4. Label relaxation with connectivity constraints weight gij represented by the number of beacons sent by λj and received by λi. In addition, let R be the radio range of the sensor nodes.
The main idea of the connectivity constrained label relaxation is depicted in Figure 4 in which two nodes ni and nj have been assigned all possible labels. The confidence in each of the candidate labels for a sensor node, is represented by a probability, shown in a dotted rectangle.
It is important to remark that through beaconing and the reporting of neighbor tables to the Central Device, a global view of all constraints in the network can be obtained. It is critical to observe that these constraints are among labels.
As shown in Figure 4 two constraints exist between nodes ni and nj. The constraints are depicted by gi2,j2 and gi2,jM, the number of beacons sent the labels λj2 and λjM and received by the label λi2.
The support for the label λk of sensor node ni, resulting from the interaction (i.e., within radio range) with sensor node nj is given by: Qs ni (λk) = M ∑ m=1 gλkλm Ps nj (λm) (6) As a result, the localization algorithm (Algorithm 3 consists of the following steps: all labels are assigned to each sensor node (lines 1-3 of the algorithm), and implicitly each label has a probability initialized to Pni(λk) = 1/|Λ|; in each iteration, the probabilities for the labels of a sensor node are updated, when considering the interaction with the labels of sensor nodes within R. It is important to remark that the identity of the nodes within R is not known, only the candidate labels and their probabilities. The relaxation algorithm converges when, during an iteration, the probability of no label is updated by more than ε.
The label relaxation algorithm based on connectivity constraints, enforces such constraints between pairs of sensor nodes. For a large scale sensor network deployment, it is not feasible to consider all pairs of sensor nodes in the network.
Hence, the algorithm should only consider pairs of sensor nodes that are within a reasonable communication range (R).
We assume a circular radio range and a symmetric connectivity. In the remaining part of the section we propose a simple analytical model that estimates the radio range R for medium-connected networks (less than 20 neighbors per R).
We consider the following to be known: the size of the deployment field (L), the number of sensor nodes deployed (N) Algorithm 3 Localization 1: Estimate the radio range R 2: Execute the Label Relaxation Algorithm with Support Function given by Equation 6 for neighbors less than R apart 3: for each sensor node ni do 4: node identity is λk with max. prob. 5: end for and the total number of unidirectional (i.e., not symmetric) one-hop radio connections in the network (k). For our analysis, we uniformly distribute the sensor nodes in a square area of length L, by using a grid of unit length L/ √ N. We use the substitution u = L/ √ N to simplify the notation, in order to distinguish the following cases: if u ≤ R ≤ √ 2u each node has four neighbors (the expected k = 4N); if √ 2u ≤ R ≤ 2u each node has eight neighbors (the expected k = 8N); if 2u ≤ R ≤ √ 5u each node has twelve neighbors ( the expected k = 12N); if √ 5u ≤ R ≤ 3u each node has twenty neighbors (the expected k = 20N) For a given t = k/4N we take R to be the middle of the interval. As an example, if t = 5 then R = (3 + √ 5)u/2. A quadratic fitting for R over the possible values of t, produces the following closed-form solution for the communication range R, as a function of network connectivity k, assuming L and N constant: R(k) = L √ N −0.051 k 4N 2 +0.66 k 4N +0.6 (7) We investigate the accuracy of our model in Section 5.2.1.
Time constraints can be treated similarly with color constraints. The unique identification of a sensor node can be obtained by deploying sensor nodes individually, one by one, and recording a sequence of images. The sensor node that is identified as new in the last picture (it was not identified in the picture before last) must be the last sensor node dropped.
In a similar manner with color constrained label relaxation, the time constrained approach is very simple, but may take too long, especially for large scale systems. While it can be used in practice, it is unlikely that only a time constrained label relaxation is used. As we will see, by combining constrained-based primitives, realistic localization systems can be implemented.
The support function for the label relaxation with time constraints is defined identically with the color constrained relaxation: Qs ni (λk) = 1 (8) The localization algorithm (Algorithm 2 consists of the following steps: one label is assigned to each sensor node (lines 1-3 of the algorithm), and implicitly having a probability Pni(λk) = 1 ; the algorithm executes a single iteration, 62 D1 D2 D4 D 3Node Label-1 Label-2 Label-3 Label-4
Figure 5. Relaxation with space constraints 0
1
PDF Distance D σ = 0.5 σ = 1 σ = 2 Figure 6. Probability distribution of distances -4 -3 -2 -1 0 1 2 3 4 X -4 -3 -2 -1 0 1 2 3 4 Y 0
1 Node Density Figure 7. Distribution of nodes when the support function, simply, reiterates the confidence in the unique labeling.
Spatial information related to sensor deployment can also be employed as another input to the label relaxation algorithm. To do that, we use two types of locations: the node location pn and the label location pl. The former pn is defined as the position of nodes (xn,yn,zn) after deployment, which can be obtained through Image Processing as mentioned in Section 3.3. The latter pl is defined as the location (xl,yl,zl) where a node is dropped. We use Dni λm to denote the horizontal distance between the location of the label λm and the location of the node ni. Clearly, Dni λm = (xn −xl)2 +(yn −yl)2.
At the time of a sensor node release, the one-to-one mapping between the node and its label is known. In other words, the label location is the same as the node location at the release time. After release, the label location information is partially lost due to the random factors such as wind and surface impact. However, statistically, the node locations are correlated with label locations. Such correlation depends on the airdrop methods employed and environments. For the sake of simplicity, let"s assume nodes are dropped from the air through a helicopter hovering in the air. Wind can be decomposed into three components X,Y and Z. Only X and Y affect the horizontal distance a node can travel.
According to [24], we can assume that X and Y follow an independent normal distribution. Therefore, the absolute value of the wind speed follows a Rayleigh distribution. Obviously the higher the wind speed is, the further a node would land away horizontally from the label location. If we assume that the distance D is a function of the wind speed V [25] [26], we can obtain the probability distribution of D under a given wind speed distribution. Without loss of generality, we assume that D is proportional to the wind speed. Therefore,
D follows the Rayleigh distribution as well. As shown in Figure 5, the spatial-based relaxation is a recursive process to assign the probability that a nodes has a certain label by using the distances between the location of a node with multiple label locations.
We note that the distribution of distance D affects the probability with which a label is assigned. It is not necessarily true that the nearest label is always chosen. For example, if D follows the Rayleigh(σ2) distribution, we can obtain the Probability Density Function (PDF) of distances as shown in Figure 6. This figure indicates that the possibility of a node to fall vertically is very small under windy conditions (σ > 0), and that the distance D is affected by the σ. The spatial distribution of nodes for σ = 1 is shown in Figure 7.
Strong wind with a high σ value leads to a larger node dispersion. More formally, given a probability density function PDF(D), the support for label λk of sensor node ni can be formulated as: Qs ni (λk) = PDF(Dni λk ) (9) It is interesting to point out two special cases. First, if all nodes are released at once (i.e., only one label location for all released nodes), the distance D from a node to all labels is the same. In this case, Ps+1 ni (λk) = Ps ni (λk), which indicates that we can not use the spatial-based relaxation to recursively narrow down the potential labels for a node. Second, if nodes are released at different locations that are far away from each other, we have: (i) If node ni has label λk, Ps ni (λk) → 1 when s → ∞, (ii) If node ni does not have label λk, Ps ni (λk) → 0 when s → ∞. In this second scenario, there are multiple labels (one label per release), hence it is possible to correlate release times (labels) with positions on the ground. These results indicate that spatial-based relaxation can label the node with a very high probability if the physical separation among nodes is large.
Constraints One of the most interesting features of the StarDust architecture is that it allows for hybrid localization solutions to be built, depending on the system requirements. One example is a localization system that uses the color and connectivity constraints. In this scheme, the color constraints are used for reducing the number of candidate labels for sensor nodes, to a more manageable value. As a reminder, in the connectivity constrained relaxation, all labels are candidate labels for each sensor node. The color constraints are used in the initialization phase of Algorithm 3 (lines 1-3). After the initialization, the standard connectivity constrained relaxation algorithm is used.
For a better understanding of how the label relaxation algorithm works, we give a concrete example, exemplified in Figure 8. In part (a) of the figure we depict the data structures 63 11 8 4 1 12 9 7 5 3 ni nj 12 8 10 11 10
(a) 11 8 4 1 12 9 7 5 3 ni nj 12 8 10 11 10
0
0 (b) Figure 8. A step through the algorithm. After initialization (a) and after the 1st iteration for node ni (b) associated with nodes ni and nj after the initialization steps of the algorithm (lines 1-6), as well as the number of beacons between different labels (as reported by the network, through G(Λ,E)). As seen, the potential labels (shown inside the vertical rectangles) are assigned to each node. Node ni can be any of the following: 11,8,4,1. Also depicted in the figure are the probabilities associated with each of the labels. After initialization, all probabilities are equal.
Part (b) of Figure 8 shows the result of the first iteration of the localization algorithm for node ni, assuming that node nj is the first wi chosen in line 7 of Algorithm 3. By using Equation 6, the algorithm computes the support Q(λi) for each of the possible labels for node ni. Once the Q(λi)"s are computed, the normalizing constant, given by Equation
the probabilities associated with all potential labels of node ni, as given by Equation 2.
One interesting problem, which we explore in the performance evaluation section, is to assess the impact the partitioning of the color set C has on the accuracy of localization. When the size of the coloring set is smaller than the number of sensor nodes (as it is the case for our hybrid connectivity/color constrained relaxation), the system designer has the option of allowing one node to uniquely have a color (acting as an anchor), or multiple nodes. Intuitively, by assigning one color to more than one node, more constraints (distributed) can be enforced.
The proposed label relaxation techniques have different trade-offs. For our analysis of the trade-offs, we consider the following metrics of interest: the localization time (duration), the energy consumed (overhead), the network size (scale) that can be handled by the technique and the localization accuracy. The parameters of interest are the following: the number of sensor nodes (N), the energy spent for one aerial drop (εd), the energy spent in the network for collecting and reporting neighbor information εb and the time Td taken by a sensor node to reach the ground after being aerially deployed. The cost comparison of the different label relaxation techniques is shown in Table 1.
As shown, the relaxation techniques based on color and space constraints have the lowest localization duration, zero, for all practical purposes. The scalability of the color based relaxation technique is, however, limited to the number of (a) (b) Figure 9. SensorBall with self-righting capabilities (a) and colored CCRs (b) unique color filters that can be built. The narrower the Transfer Function Ψ(λ), the larger the number of unique colors that can be created. The manufacturing costs, however, are increasing as well. The scalability issue is addressed by all other label relaxation techniques. Most notably, the time constrained relaxation, which is very similar to the colorconstrained relaxation, addresses the scale issue, at a higher deployment cost.
Criteria Color Connectivity Time Space Duration 0 NTb NTd 0 Overhead εd εd +Nεb Nεd εd Scale |C| |N| |N| |N| Accuracy High Low High Medium Table 1. Comparison of label relaxation techniques
The StarDust localization framework, depicted in Figure 2, is flexible in that it enables the development of new localization systems, based on the four proposed label relaxation schemes, or the inclusion of other, yet to be invented, schemes. For our performance evaluation we implemented a version of the StarDust framework, namely the one proposed in Section 3.3.5, where the constraints are based on color and connectivity.
The Central device of the StarDust system consists of the following: the Light Emitter - we used a common-off-theshelf flash light (QBeam, 3 million candlepower); the image acquisition was done with a 3 megapixel digital camera (Sony DSC-S50) which provided the input to the Image Processing algorithm, implemented in Matlab.
For sensor nodes we built a custom sensor node, called SensorBall, with self-righting capabilities, shown in Figure 9(a). The self-righting capabilities are necessary in order to orient the CCR predominantly upwards. The CCRs that we used were inexpensive, plastic molded, night time warning signs commonly available on bicycles, as shown in Figure 9(b). We remark here the low quality of the CCRs we used.
The reflectivity of each CCR (there are tens molded in the plastic container) is extremely low, and each CCR is not built with mirrors. A reflective effect is achieved by employing finely polished plastic surfaces. We had 5 colors available, in addition to the standard CCR, which reflects all the incoming light (white CCR). For a slightly higher price (ours were 20cents/piece), better quality CCRs can be employed. 64 Figure 10. The field in the dark Figure 11. The illuminated field Figure 12. The difference: Figure 10Figure 11 Higher quality (better mirrors) would translate in more accurate image processing (better sensor node detection) and smaller form factor for the optical component (an array of CCRs with a smaller area can be used).
The sensor node platform we used was the micaZ mote.
The code that runs on each node is a simple application which broadcasts 100 beacons, and maintains a neighbor table containing the percentage of successfully received beacons, for each neighbor. On demand, the neighbor table is reported to a base station, where the node ID mapping is performed.
In this section we present the performance evaluation of a system implementation of the StarDust localization framework. The three major research questions that our evaluation tries to answer are: the feasibility of the proposed framework (can sensor nodes be optically detected at large distances), the localization accuracy of one actual implementation of the StarDust framework, and whether or not atmospheric conditions can affect the recognition of sensor nodes in an image. The first two questions are investigated by evaluating the two main components of the StarDust framework: the Image Processing and the Node ID Matching. These components have been evaluated separately mainly because of lack of adequate facilities. We wanted to evaluate the performance of the Image Processing Algorithm in a long range, realistic, experimental set-up, while the Node ID Matching required a relatively large area, available for long periods of time (for connectivity data gathering). The third research question is investigated through a computer modeling of atmospheric phenomena.
For the evaluation of the Image Processing module, we performed experiments in a football stadium where we deploy 6 sensor nodes in a 3×2 grid. The distance between the Central device and the sensor nodes is approximately 500 ft.
The metrics of interest are the number of false positives and false negatives in the Image Processing Algorithm.
For the evaluation of the Node ID Mapping component, we deploy 26 sensor nodes in an 120 × 60 ft2 flat area of a stadium. In order to investigate the influence the radio connectivity has on localization accuracy, we vary the height above ground of the deployed sensor nodes. Two set-ups are used: one in which the sensor nodes are on the ground, and the second one, in which the sensor nodes are raised 3 inches above ground. From here on, we will refer to these two experimental set-ups as the low connectivity and the high connectivity networks, respectively because when nodes are on the ground the communication range is low resulting in less neighbors than when the nodes are elevated and have a greater communication range. The metrics of interest are: the localization error (defined as the distance between the computed location and the true location - known from the manual placement), the percentage of nodes correctly localized, the convergence of the label relaxation algorithm, the time to localize and the robustness of the node ID mapping to errors in the Image Processing module.
The parameters that we vary experimentally are: the angle under which images are taken, the focus of the camera, and the degree of connectivity. The parameters that we vary in simulations (subsequent to image acquisition and connectivity collection) the number of colors, the number of anchors, the number of false positives or negatives as input to the Node ID Matching component, the distance between the imaging device and sensor network (i.e., range), atmospheric conditions (light attenuation coefficient) and CCR reflectance (indicative of its quality).
For the IPA evaluation, we deploy 6 sensor nodes in a
orientations of the camera and different zooming factors. All pictures were taken from the same location. Each set is composed of a picture taken in the dark and of a picture taken with a light beam pointed at the nodes. We process the pictures offline using a Matlab implementation of IPA. Since we are interested in the feasibility of identifying colored sensor nodes at large distance, the end result of our IPA is the 2D location of sensor nodes (position in the image). The transformation to 3D coordinates can be done through standard computer graphics techniques [23].
One set of pictures obtained as part of our experiment is shown in Figures 10 and 11. The execution of our IPA algorithm results in Figure 12 which filters out the background, and Figure 13 which shows the output of the edge detection step of IPA. The experimental results are depicted in Figure 14. For each set of pictures the graph shows the number of false positives (the IPA determines that there is a node 65 Figure 13. Retroreflectors detected in Figure 12 0 1 2 3
Experiment Number Count False Positives False Negatives Figure 14. False Positives and Negatives for the 6 nodes while there is none), and the number of false negatives (the IPA determines that there is no node while there is one). In about 45% of the cases, we obtained perfect results, i.e., no false positives and no false negatives. In the remaining cases, we obtained a number of false positives of at most one, and a number of false negatives of at most two.
We exclude two pairs of pictures from Figure 14. In the first excluded pair, we obtain 42 false positives and in the second pair 10 false positives and 7 false negatives. By carefully examining the pictures, we realized that the first pair was taken out of focus and that a car temporarily appeared in one of the pictures of the second pair. The anomaly in the second set was due to the fact that we waited too long to take the second picture. If the pictures had been taken a few milliseconds apart, the car would have been represented on either both or none of the pictures and the IPA would have filtered it out.
We evaluate the Node ID Matching component of our system by collecting empirical data (connectivity information) from the outdoor deployment of 26 nodes in the 120×60 ft2 area. We collect 20 sets of data for the high connectivity and low connectivity network deployments. Off-line we investigate the influence of coloring on the metrics of interest, by randomly assigning colors to the sensor nodes. For one experimental data set we generate 50 random assignments of colors to sensor nodes. It is important to observe that, for the evaluation of the Node ID Matching algorithm (color and connectivity constrained), we simulate the color assignment to sensor nodes. As mentioned in Section 4 the size of the coloring space available to us was 5 (5 colors). Through simulations of color assignment (not connectivity) we are able to investigate the influence that the size of the coloring space has on the accuracy of localization. The value of the param0 10 20 30 40 50 60
Distance [feet] Count Connected Not Connected Figure 15. The number of existing and missing radio connections in the sparse connectivity experiment 0 10 20 30 40 50 60 70
Distance [feet] Count Connected Not Connected Figure 16. The number of existing and missing radio connections in the high connectivity experiment eter ε used in Algorithm 2 was 0.001. The results presented here represent averages over the randomly generated colorings and over all experimental data sets.
We first investigate the accuracy of our proposed Radio Model, and subsequently use the derived values for the radio range in the evaluation of the Node ID matching component.
From experiments, we obtain the average number of observed beacons (k, defined in Section 3.3.2) for the low connectivity network of 180 beacons and for the high connectivity network of 420 beacons. From our Radio Model (Equation 7, we obtain a radio range R = 25 ft for the low connectivity network and R = 40 ft for the high connectivity network.
To estimate the accuracy of our simple model, we plot the number of radio links that exist in the networks, and the number of links that are missing, as functions of the distance between nodes. The results are shown in Figures 15 and 16.
We define the average radio range R to be the distance over which less than 20% of potential radio links, are missing.
As shown in Figure 15, the radio range is between 20 ft and
was between 30 ft and 40 ft.
We choose two conservative estimates of the radio range:
connectivity case, which are in good agreement with the values predicted by our Radio Model.
In this experiment we investigate the effect of the number of colors on the localization accuracy. For this, we randomly assign colors from a pool of a given size, to the sensor nodes. 66 0 5 10 15 20 25 30 35 40 45
Number of Colors LocalizationError[feet] R=15feet R=20feet R=25feet Figure 17. Localization error 0 10 20 30 40 50 60 70 80 90 100
Number of Colors %CorrectLocalized[x100] R=15feet R=20feet R=25feet Figure 18. Percentage of nodes correctly localized We then execute the localization algorithm, which uses the empirical data. The algorithm is run for three different radio ranges: 15, 20 and 25 ft, to investigate its influence on the localization error.
The results are depicted in Figure 17 (localization error) and Figure 18 (percentage of nodes correctly localized). As shown, for an estimate of 20 ft for the radio range (as predicted by our Radio Model) we obtain the smallest localization errors, as small as 2 ft, when enough colors are used.
Both Figures 17 and 18 confirm our intuition that a larger number of colors available significantly decrease the error in localization.
The well known fact that relaxation algorithms do not always converge, was observed during our experiments. The percentage of successful runs (when the algorithm converged) is depicted in Figure 19. As shown, in several situations, the algorithm failed to converge (the algorithm execution was stopped after 100 iterations per node). If the algorithm does not converge in a predetermined number of steps, it will terminate and the label with the highest probability will provide the identity of the node. It is very probable that the chosen label is incorrect, since the probabilities of some of labels are constantly changing (with each iteration).The convergence of relaxation based algorithms is a well known issue.
As mentioned in the Section 3.3.1, a unique color gives a sensor node the statute of an anchor. A sensor node that is an anchor can unequivocally be identified through the Image Processing module. In this section we investigate the effect unique colors have on the localization accuracy. Specifically, we want to experimentally verify our intuition that assigning more nodes to a color can benefit the localization accuracy, by enforcing more constraints, as opposed to uniquely assigning a color to a single node. 90 95 100 105
Number of Colors ConvergenceRate[x100] R=15feet R=20feet R=25feet Figure 19. Convergence error 0 2 4 6 8 10 12 14 16
Number of Colors LocalizationError[feet]
Figure 20. Localization error vs. number of colors For this, we fix the number of available colors to either 4,
colors, from 0, up to the maximum number of colors (4, 6 or 8). Naturally, if we have a maximum number of colors of 4, we can assign at most 4 anchors. The experimental results are depicted in Figure 20 (localization error) and Figure 21 (percentage of sensor node correctly localized). As expected, the localization accuracy increases with the increase in the number of colors available (larger coloring space). Also, for a given size of the coloring space (e.g., 6 colors available), if more colors are uniquely assigned to sensor nodes then the localization accuracy decreases. It is interesting to observe that by assigning colors uniquely to nodes, the benefit of having additional colors is diminished. Specifically, if 8 colors are available and all are assigned uniquely, the system would be less accurately localized (error ≈ 7 ft), when compared to the case of 6 colors and no unique assignments of colors (≈ 5 ft localization error).
The same trend, of a less accurate localization can be observed in Figure 21, which shows the percentage of nodes correctly localized (i.e., 0 ft localization error). As shown, if we increase the number of colors that are uniquely assigned, the percentage of nodes correctly localized decreases.
We collected empirical data for two network deployments with different degrees of connectivity (high and low) in order to assess the influence of connectivity on location accuracy. The results obtained from running our localization algorithm are depicted in Figure 22 and Figure 23. We varied the number of colors available and assigned no anchors (i.e., no unique assignments of colors).
In both scenarios, as expected, localization error decrease with an increase in the number of colors. It is interesting to observe, however, that the low connectivity scenario im67 0 20 40 60 80 100 120 140
Number of Colors %CorrectLocalized[x100]
Figure 21. Percentage of nodes correctly localized vs. number of colors 0 5 10 15 20 25 30 35 40 45
Number of Colors LocalizationError[feet] Low Connectivity High Connectivity Figure 22. Localization error vs. number of colors proves the localization accuracy quicker, from the additional number of colors available. When the number of colors becomes relatively large (twelve for our 26 sensor node network), both scenarios (low and high connectivity) have comparable localization errors, of less that 2 ft. The same trend of more accurate location information is evidenced by Figure 23 which shows that the percentage of nodes that are localized correctly grows quicker for the low connectivity deployment.
Errors So far we investigated the sources for error in localization that are intrinsic to the Node ID Matching component.
As previously presented, luminous objects can be mistakenly detected to be sensor nodes during the location detection phase of the Image Processing module. These false positives can be eliminated by the color recognition procedure of the Image Processing module. More problematic are false negatives (when a sensor node does not reflect back enough light to be detected). They need to be handled by the localization algorithm. In this case, the localization algorithm is presented with two sets of nodes of different sizes, that need to be matched: one coming from the Image Processing (which misses some nodes) and one coming from the network, with the connectivity information (here we assume a fully connected network, so that all sensor nodes report their connectivity information). In this experiment we investigate how Image Processing errors (false negatives) influence the localization accuracy.
For this evaluation, we ran our localization algorithm with empirical data, but dropped a percentage of nodes from the list of nodes detected by the Image Processing algorithm (we artificially introduced false negatives in the Image Process0 10 20 30 40 50 60 70 80 90 100
Number of Colors %CorrectLocalized[x100] Low Connectivity High Connectivity Figure 23. Percentage of nodes correctly localized 0 2 4 6 8 10 12 14
% False Negatives [x100] LocalizationError[feet]
Figure 24. Impact of false negatives on the localization error ing). The effect of false negatives on localization accuracy is depicted in Figure 24. As seen in the figure if the number of false negatives is 15%, the error in position estimation doubles when 4 colors are available. It is interesting to observe that the scenario when more colors are available (e.g., 12 colors) is being affected more drastically than the scenario with less colors (e.g., 4 colors). The benefit of having more colors available is still being maintained, at least for the range of colors we investigated (4 through 12 colors).
In this section we look more closely at the duration for each of the four proposed relaxation techniques and two combinations of them: color-connectivity and color-time.
We assume that 50 unique color filters can be manufactured, that the sensor network is deployed from 2,400 ft (necessary for the time-constrained relaxation) and that the time required for reporting connectivity grows linearly, with an initial reporting period of 160sec, as used in a real world tracking application [1]. The localization duration results, as presented in Table 1, are depicted in Figure 25.
As shown, for all practical purposes the time required by the space constrained relaxation techniques is 0sec. The same applies to the color constrained relaxation, for which the localization time is 0sec (if the number of colors is sufficient). Considering our assumptions, only for a network of size 50 the color constrained relaxation works. The localization duration for all other network sizes (100, 150 and 200) is infinite (i.e., unique color assignments to sensor nodes can not be made, since only 50 colors are unique), when only color constrained relaxation is used. Both the connectivity constrained and time constrained techniques increase linearly with the network size (for the time constrained, the Central device deploys sensor nodes one by one, recording an image after the time a sensor node is expected to reach the 68 0 500 1000 1500 2000 2500 3000 Color Connectivity Time Space ColorConenctivity Color-Time Localization technique Localizationtime[sec]
Figure 25. Localization time for different label relaxation schemes
0
1
2
3
4 r [feet] C r
Figure 26. Apparent contrast in a clear atmosphere
0
1
2
3
4 r [feet] C r
Figure 27. Apparent contrast in a hazing atmosphere ground).
It is interesting to notice in Figure 25 the improvement in the localization time obtained by simply combining the color and the connectivity constrained techniques. The localization duration in this case is identical with the connectivity constrained technique.
The combination of color and time constrained relaxations is even more interesting. For a reasonable localization duration of 52seconds a perfect (i.e., 0 ft localization error) localization system can be built. In this scenario, the set of sensor nodes is split in batches, with each batch having a set of unique colors. It would be very interesting to consider other scenarios, where the strength of the space constrained relaxation (0sec for any sensor network size) is used for improving the other proposed relaxation techniques.
We leave the investigation and rigorous classification of such technique combination for future work.
In this section we evaluate the feasibility of the StarDust localization framework when considering the realities of light propagation through the atmosphere.
The main factor that determines the range of our system is light scattering, which redirects the luminance of the source into the medium (in essence equally affecting the luminosity of the target and of the background). Scattering limits the visibility range by reducing the apparent contrast between the target and its background (approaches zero, as the distance increases). The apparent contrast Cr is quantitatively expressed by the formula: Cr = (Nt r −Nb r )/Nb r (10) where Nt r and Nb r are the apparent target radiance and apparent background radiance at distance r from the light source, respectively. The apparent radiance Nt r of a target at a distance r from the light source, is given by: Nt r = Na + Iρte−2σr πr2 (11) where I is the intensity of the light source, ρt is the target reflectance, σ is the spectral attenuation coefficient (≈
atmosphere, respectively) and Na is the radiance of the atmospheric backscatter, and it can be expressed as follows: Na = Gσ2I 2π 2σrZ
e−x x2 dx (12) where G = 0.24 is a backscatter gain. The apparent background radiance Nb r is given by formulas similar with Equations 11 and 12, where only the target reflectance ρt is substituted with the background reflectance ρb. It is important to remark that when Cr reaches its lower limit, no increase in the source luminance or receiver sensitivity will increase the range of the system. From Equations 11 and 12 it can be observed that the parameter which can be controlled and can influence the range of the system is ρt, the target reflectance.
Figures 26 and 27 depict the apparent contrast Cr as a function of the distance r for a clear and for a hazy atmosphere, respectively. The apparent contrast is investigated for reflectance coefficients ρt ranging from 0.3 to 1.0 (perfect reflector). For a contrast C of at least 0.5, as it can be seen in Figure 26 a range of approximately 4,500 ft can be achieved if the atmosphere is clear. The performance dramatically deteriorates, when the atmospheric conditions are problematic.
As shown in Figure 27 a range of up to 1,500 ft is achievable, when using highly reflective CCR components.
While our light source (3 million candlepower) was sufficient for a range of a few hundred feet, we remark that there exist commercially available light sources (20 million candlepower) or military (150 million candlepower [27]), powerful enough for ranges of a few thousand feet.
In this section we describe extensions of the proposed architecture that can constitute future research directions.
In this paper we proposed four primitives for constraintbased relaxation algorithms: color, connectivity, time and space. To demonstrate the power that can be obtained by combining them, we proposed and evaluated one combination of such primitives: color and connectivity. An interesting research direction to pursue could be to chain more than two of these primitives. An example of such chain is: color, temporal, spatial and connectivity. Other research directions could be to use voting scheme for deciding which primitive to use or assign different weights to different relaxation algorithms. 69
If after several iterations of the algorithm, none of the label probabilities for a node ni converges to a higher value, the confidence in our labeling of that node is relatively low. It would be interesting to associate with a node, more than one label (implicitly more than one location) and defer the label assignment decision until events are detected in the network (if the network was deployed for target tracking).
The initial driving force for the StarDust localization framework was to address the sensor node localization in extremely rugged environments. Canopies, dense vegetation, extremely obstructing environments pose significant challenges for sensor nodes localization. The hope, and our original idea, was to consider the time period between the aerial deployment and the time when the sensor node disappears under the canopy. By recording the last visible position of a sensor node (as seen from the aircraft) a reasonable estimate of the sensor node location can be obtained. This would require that sensor nodes posses self-righting capabilities, while in mid-air. Nevertheless, we remark on the suitability of our localization framework for rugged, non-line-of-sight environments.
StarDust solves the localization problem for aerial deployments where passiveness, low cost, small form factor and rapid localization are required. Results show that accuracy can be within 2 ft and localization time within milliseconds. StarDust also shows robustness with respect to errors.
We predict the influence the atmospheric conditions can have on the range of a system based on the StarDust framework, and show that hazy environments or daylight can pose significant challenges.
Most importantly, the properties of StarDust support the potential for even more accurate localization solutions as well as solutions for rugged, non-line-of-sight environments.
[1] T. He, S. Krishnamurthy, J. A. Stankovic, T. Abdelzaher, L. Luo,
R. Stoleru, T. Yan, L. Gu, J. Hui, and B. Krogh, An energy-efficient surveillance system using wireless sensor networks, in MobiSys, 2004. [2] G. Simon, M. Maroti, A. Ledeczi, G. Balogh, B. Kusy, A. Nadas,
G. Pap, J. Sallai, and K. Frampton, Sensor network-based countersniper system, in SenSys, 2004. [3] A. Arora, P. Dutta, and B. Bapat, A line in the sand: A wireless sensor network for trage detection, classification and tracking, in Computer Networks, 2004. [4] R. Szewczyk, A. Mainwaring, J. Polastre, J. Anderson, and D. Culler,
An analysis of a large scale habitat monitoring application, in ACM SenSys, 2004. [5] N. Xu, S. Rangwala, K. K. Chintalapudi, D. Ganesan, A. Broad,
R. Govindan, and D. Estrin, A wireless sensor network for structural monitoring, in ACM SenSys, 2004. [6] A. Savvides, C. Han, and M. Srivastava, Dynamic fine-grained localization in ad-hoc networks of sensors, in Mobicom, 2001. [7] N. Priyantha, A. Chakraborty, and H. Balakrishnan, The cricket location-support system, in Mobicom, 2000. [8] M. Broxton, J. Lifton, and J. Paradiso, Localizing a sensor network via collaborative processing of global stimuli, in EWSN, 2005. [9] P. Bahl and V. N. Padmanabhan, Radar: An in-building rf-based user location and tracking system, in IEEE Infocom, 2000. [10] N. Priyantha, H. Balakrishnan, E. Demaine, and S. Teller,
Mobileassisted topology generation for auto-localization in sensor networks, in IEEE Infocom, 2005. [11] P. N. Pathirana, A. Savkin, S. Jha, and N. Bulusu, Node localization using mobile robots in delay-tolerant sensor networks, IEEE Transactions on Mobile Computing, 2004. [12] C. Savarese, J. M. Rabaey, and J. Beutel, Locationing in distribued ad-hoc wireless sensor networks, in ICAASSP, 2001. [13] M. Maroti, B. Kusy, G. Balogh, P. Volgyesi, A. Nadas, K. Molnar,
S. Dora, and A. Ledeczi, Radio interferometric geolocation, in ACM SenSys, 2005. [14] K. Whitehouse, A. Woo, C. Karlof, F. Jiang, and D. Culler, The effects of ranging noise on multi-hop localization: An empirical study, in IPSN, 2005. [15] Y. Kwon, K. Mechitov, S. Sundresh, W. Kim, and G. Agha, Resilient localization for sensor networks in outdoor environment, UIUC, Tech.
Rep., 2004. [16] R. Stoleru and J. A. Stankovic, Probability grid: A location estimation scheme for wireless sensor networks, in SECON, 2004. [17] N. Bulusu, J. Heidemann, and D. Estrin, GPS-less low cost outdoor localization for very small devices, IEEE Personal Communications Magazine, 2000. [18] T. He, C. Huang, B. Blum, J. A. Stankovic, and T. Abdelzaher,
Range-Free localization schemes in large scale sensor networks, in ACM Mobicom, 2003. [19] R. Nagpal, H. Shrobe, and J. Bachrach, Organizing a global coordinate system from local information on an ad-hoc sensor network, in IPSN, 2003. [20] D. Niculescu and B. Nath, ad-hoc positioning system, in IEEE GLOBECOM, 2001. [21] R. Stoleru, T. He, J. A. Stankovic, and D. Luebke, A high-accuracy low-cost localization system for wireless sensor networks, in ACM SenSys, 2005. [22] K. R¨omer, The lighthouse location system for smart dust, in ACM/USENIX MobiSys, 2003. [23] R. Y. Tsai, A versatile camera calibration technique for highaccuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses, IEEE JRA, 1987. [24] C. L. Archer and M. Z. Jacobson, Spatial and temporal distributions of U.S. winds and wind power at 80m derived from measurements,
Geophysical Research Jrnl., 2003. [25] Team for advanced flow simulation and modeling. [Online].

Many different kinds of networked data-centric sensor applications have emerged in recent years. Sensors in these applications sense the environment and generate data that must be processed, filtered, interpreted, and archived in order to provide a useful infrastructure to its users. To achieve its goals, a typical sensor application needs access to both live and past sensor data. Whereas access to live data is necessary in monitoring and surveillance applications, access to past data is necessary for applications such as mining of sensor logs to detect unusual patterns, analysis of historical trends, and post-mortem analysis of particular events. Archival storage of past sensor data requires a storage system, the key attributes of which are: where the data is stored, whether it is indexed, and how the application can access this data in an energy-efficient manner with low latency.
There have been a spectrum of approaches for constructing sensor storage systems. In the simplest, sensors stream data or events to a server for long-term archival storage [3], where the server often indexes the data to permit efficient access at a later time. Since sensors may be several hops from the nearest base station, network costs are incurred; however, once data is indexed and archived, subsequent data accesses can be handled locally at the server without incurring network overhead. In this approach, the storage is centralized, reads are efficient and cheap, while writes are expensive.
Further, all data is propagated to the server, regardless of whether it is ever used by the application.
An alternate approach is to have each sensor store data or events locally (e.g., in flash memory), so that all writes are local and incur no communication overheads. A read request, such as whether an event was detected by a particular sensor, requires a message to be sent to the sensor for processing. More complex read requests are handled by flooding. For instance, determining if an intruder was detected over a particular time interval requires the request to be flooded to all sensors in the system. Thus, in this approach, the storage is distributed, writes are local and inexpensive, while reads incur significant network overheads. Requests that require flooding, due to the lack of an index, are expensive and may waste precious sensor resources, even if no matching data is stored at those sensors. Research efforts such as Directed Diffusion [17] have attempted to reduce these read costs, however, by intelligent message routing.
Between these two extremes lie a number of other sensor storage systems with different trade-offs, summarized in Table 1. The geographic hash table (GHT) approach [24, 26] advocates the use of an in-network index to augment the fully distributed nature of sensor storage. In this approach, each data item has a key associated with it, and a distributed or geographic hash table is used to map keys to nodes that store the corresponding data items. Thus, writes cause data items to be sent to the hashed nodes and also trigger updates to the in-network hash table. A read request requires a lookup in the in-network hash table to locate the node that stores the data 39 item; observe that the presence of an index eliminates the need for flooding in this approach.
Most of these approaches assume a flat, homogeneous architecture in which every sensor node is energy-constrained. In this paper, we propose a novel storage architecture called TSAR1 that reflects and exploits the multi-tier nature of emerging sensor networks, where the application is comprised of tens of tethered sensor proxies (or more), each controlling tens or hundreds of untethered sensors. TSAR is a component of our PRESTO [8] predictive storage architecture, which combines archival storage with caching and prediction. We believe that a fundamentally different storage architecture is necessary to address the multi-tier nature of future sensor networks. Specifically, the storage architecture needs to exploit the resource-rich nature of proxies, while respecting resource constraints at the remote sensors. No existing sensor storage architecture explicitly addresses this dichotomy in the resource capabilities of different tiers.
Any sensor storage system should also carefully exploit current technology trends, which indicate that the capacities of flash memories continue to rise as per Moore"s Law, while their costs continue to plummet. Thus it will soon be feasible to equip each sensor with
compelling argument is the energy cost of flash storage, which can be as much as two orders of magnitude lower than that for communication. Newer NAND flash memories offer very low write and erase energy costs - our comparison of a 1GB Samsung NAND flash storage [16] and the Chipcon CC2420 802.15.4 wireless radio [4] in Section 6.2 indicates a 1:100 ratio in per-byte energy cost between the two devices, even before accounting for network protocol overheads. These trends, together with the energy-constrained nature of untethered sensors, indicate that local storage offers a viable, energy-efficient alternative to communication in sensor networks.
TSAR exploits these trends by storing data or events locally on the energy-efficient flash storage at each sensor. Sensors send concise identifying information, which we term metadata, to a nearby proxy; depending on the representation used, this metadata may be an order of magnitude or more smaller than the data itself, imposing much lower communication costs. The resource-rich proxies interact with one another to construct a distributed index of the metadata reported from all sensors, and thus an index of the associated data stored at the sensors. This index provides a unified, logical view of the distributed data, and enables an application to query and read past data efficiently - the index is used to pinpoint all data that match a read request, followed by messages to retrieve that data from the corresponding sensors. In-network index lookups are eliminated, reducing network overheads for read requests. This separation of data, which is stored at the sensors, and the metadata, which is stored at the proxies, enables TSAR to reduce energy overheads at the sensors, by leveraging resources at tethered proxies.
This paper presents TSAR, a novel two-tier storage architecture for sensor networks. To the best of our knowledge, this is the first sensor storage system that is explicitly tailored for emerging multitier sensor networks. Our design and implementation of TSAR has resulted in four contributions.
At the core of the TSAR architecture is a novel distributed index structure based on interval skip graphs that we introduce in this paper. This index structure can store coarse summaries of sensor data and organize them in an ordered manner to be easily search1 TSAR: Tiered Storage ARchitecture for sensor networks. able. This data structure has O(log n) expected search and update complexity. Further, the index provides a logically unified view of all data in the system.
Second, at the sensor level, each sensor maintains a local archive that stores data on flash memory. Our storage architecture is fully stateless at each sensor from the perspective of the metadata index; all index structures are maintained at the resource-rich proxies, and only direct requests or simple queries on explicitly identified storage locations are sent to the sensors. Storage at the remote sensor is in effect treated as appendage of the proxy, resulting in low implementation complexity, which makes it ideal for small, resourceconstrained sensor platforms. Further, the local store is optimized for time-series access to archived data, as is typical in many applications. Each sensor periodically sends a summary of its data to a proxy. TSAR employs a novel adaptive summarization technique that adapts the granularity of the data reported in each summary to the ratio of false hits for application queries. More fine grain summaries are sent whenever more false positives are observed, thereby balancing the energy cost of metadata updates and false positives.
Third, we have implemented a prototype of TSAR on a multi-tier testbed comprising Stargate-based proxies and Mote-based sensors.
Our implementation supports spatio-temporal, value, and rangebased queries on sensor data.
Fourth, we conduct a detailed experimental evaluation of TSAR using a combination of EmStar/EmTOS [10] and our prototype.
While our EmStar/EmTOS experiments focus on the scalability of TSAR in larger settings, our prototype evaluation involves latency and energy measurements in a real setting. Our results demonstrate the logarithmic scaling property of the sparse skip graph and the low latency of end-to-end queries in a duty-cycled multi-hop network .
The remainder of this paper is structured as follows. Section 2 presents key design issues that guide our work. Section 3 and 4 present the proxy-level index and the local archive and summarization at a sensor, respectively. Section 5 discusses our prototype implementation, and Section 6 presents our experimental results. We present related work in Section 7 and our conclusions in Section 8.
In this section, we first describe the various components of a multi-tier sensor network assumed in our work. We then present a description of the expected usage models for this system, followed by several principles addressing these factors which guide the design of our storage system.
We envision a multi-tier sensor network comprising multiple tiers - a bottom tier of untethered remote sensor nodes, a middle tier of tethered sensor proxies, and an upper tier of applications and user terminals (see Figure 1).
The lowest tier is assumed to form a dense deployment of lowpower sensors. A canonical sensor node at this tier is equipped with low-power sensors, a micro-controller, and a radio as well as a significant amount of flash memory (e.g., 1GB). The common constraint for this tier is energy, and the need for a long lifetime in spite of a finite energy constraint. The use of radio, processor,
RAM, and the flash memory all consume energy, which needs to be limited. In general, we assume radio communication to be substantially more expensive than accesses to flash memory.
The middle tier consists of power-rich sensor proxies that have significant computation, memory and storage resources and can use 40 Table 1: Characteristics of sensor storage systems System Data Index Reads Writes Order preserving Centralized store Centralized Centralized index Handled at store Send to store Yes Local sensor store Fully distributed No index Flooding, diffusion Local No GHT/DCS [24] Fully distributed In-network index Hash to node Send to hashed node No TSAR/PRESTO Fully distributed Distributed index at proxies Proxy lookup + sensor query Local plus index update Yes User Unified Logical Store Queries (time, space, value) Query Response Cache Query forwarding Proxy Remote Sensors Local Data Archive on Flash Memory Interval Skip Graph Query forwarding summaries start index end index linear traversal Query Response Cache-miss triggered query forwarding summaries Figure 1: Architecture of a multi-tier sensor network. these resources continuously. In urban environments, the proxy tier would comprise a tethered base-station class nodes (e.g., Crossbow Stargate), each with with multiple radios-an 802.11 radio that connects it to a wireless mesh network and a low-power radio (e.g.
applications [10], this tier could comprise a similar Stargate node with a solar power cell. Each proxy is assumed to manage several tens to hundreds of lower-tier sensors in its vicinity. A typical sensor network deployment will contain multiple geographically distributed proxies. For instance, in a building monitoring application, one sensor proxy might be placed per floor or hallway to monitor temperature, heat and light sensors in their vicinity.
At the highest tier of our infrastructure are applications that query the sensor network through a query interface[20]. In this work, we focus on applications that require access to past sensor data. To support such queries, the system needs to archive data on a persistent store. Our goal is to design a storage system that exploits the relative abundance of resources at proxies to mask the scarcity of resources at the sensors.
The design of a storage system such as TSAR is affected by the queries that are likely to be posed to it. A large fraction of queries on sensor data can be expected to be spatio-temporal in nature.
Sensors provide information about the physical world; two key attributes of this information are when a particular event or activity occurred and where it occurred. Some instances of such queries include the time and location of target or intruder detections (e.g., security and monitoring applications), notifications of specific types of events such as pressure and humidity values exceeding a threshold (e.g., industrial applications), or simple data collection queries which request data from a particular time or location (e.g., weather or environment monitoring).
Expected queries of such data include those requesting ranges of one or more attributes; for instance, a query for all image data from cameras within a specified geographic area for a certain period of time. In addition, it is often desirable to support efficient access to data in a way that maintains spatial and temporal ordering. There are several ways of supporting range queries, such as locality-preserving hashes such as are used in DIMS [18].
However, the most straightforward mechanism, and one which naturally provides efficient ordered access, is via the use of order-preserving data structures. Order-preserving structures such as the well-known B-Tree maintain relationships between indexed values and thus allow natural access to ranges, as well as predecessor and successor operations on their key values.
Applications may also pose value-based queries that involve determining if a value v was observed at any sensor; the query returns a list of sensors and the times at which they observed this value. Variants of value queries involve restricting the query to a geographical region, or specifying a range (v1, v2) rather than a single value v. Value queries can be handled by indexing on the values reported in the summaries. Specifically, if a sensor reports a numerical value, then the index is constructed on these values. A search involves finding matching values that are either contained in the search range (v1, v2) or match the search value v exactly.
Hybrid value and spatio-temporal queries are also possible. Such queries specify a time interval, a value range and a spatial region and request all records that match these attributes - find all instances where the temperature exceeded 100o F at location R during the month of August. These queries require an index on both time and value.
In TSAR our focus is on range queries on value or time, with planned extensions to include spatial scoping.
Our design of a sensor storage system for multi-tier networks is based on the following set of principles, which address the issues arising from the system and usage models above. • Principle 1: Store locally, access globally: Current technology allows local storage to be significantly more energyefficient than network communication, while technology trends show no signs of erasing this gap in the near future.
For maximum network life a sensor storage system should leverage the flash memory on sensors to archive data locally, substituting cheap memory operations for expensive radio transmission. But without efficient mechanisms for retrieval, the energy gains of local storage may be outweighed by communication costs incurred by the application in searching for data. We believe that if the data storage system provides the abstraction of a single logical store to applications, as 41 does TSAR, then it will have additional flexibility to optimize communication and storage costs. • Principle 2: Distinguish data from metadata: Data must be identified so that it may be retrieved by the application without exhaustive search. To do this, we associate metadata with each data record - data fields of known syntax which serve as identifiers and may be queried by the storage system. Examples of this metadata are data attributes such as location and time, or selected or summarized data values. We leverage the presence of resource-rich proxies to index metadata for resource-constrained sensors. The proxies share this metadata index to provide a unified logical view of all data in the system, thereby enabling efficient, low-latency lookups.
Such a tier-specific separation of data storage from metadata indexing enables the system to exploit the idiosyncrasies of multi-tier networks, while improving performance and functionality. • Principle 3: Provide data-centric query support: In a sensor application the specific location (i.e. offset) of a record in a stream is unlikely to be of significance, except if it conveys information concerning the location and/or time at which the information was generated. We thus expect that applications will be best served by a query interface which allows them to locate data by value or attribute (e.g. location and time), rather than a read interface for unstructured data. This in turn implies the need to maintain metadata in the form of an index that provides low cost lookups.
TSAR embodies these design principles by employing local storage at sensors and a distributed index at the proxies. The key features of the system design are as follows: In TSAR, writes occur at sensor nodes, and are assumed to consist of both opaque data as well as application-specific metadata.
This metadata is a tuple of known types, which may be used by the application to locate and identify data records, and which may be searched on and compared by TSAR in the course of locating data for the application. In a camera-based sensing application, for instance, this metadata might include coordinates describing the field of view, average luminance, and motion values, in addition to basic information such as time and sensor location. Depending on the application, this metadata may be two or three orders of magnitude smaller than the data itself, for instance if the metadata consists of features extracted from image or acoustic data.
In addition to storing data locally, each sensor periodically sends a summary of reported metadata to a nearby proxy. The summary contains information such as the sensor ID, the interval (t1, t2) over which the summary was generated, a handle identifying the corresponding data record (e.g. its location in flash memory), and a coarse-grain representation of the metadata associated with the record. The precise data representation used in the summary is application-specific; for instance, a temperature sensor might choose to report the maximum and minimum temperature values observed in an interval as a coarse-grain representation of the actual time series.
The proxy uses the summary to construct an index; the index is global in that it stores information from all sensors in the system and it is distributed across the various proxies in the system.
Thus, applications see a unified view of distributed data, and can query the index at any proxy to get access to data stored at any sensor. Specifically, each query triggers lookups in this distributed index and the list of matches is then used to retrieve the corresponding data from the sensors. There are several distributed index and lookup methods which might be used in this system; however, the index structure described in Section 3 is highly suited for the task.
Since the index is constructed using a coarse-grain summary, instead of the actual data, index lookups will yield approximate matches. The TSAR summarization mechanism guarantees that index lookups will never yield false negatives - i.e. it will never miss summaries which include the value being searched for. However, index lookups may yield false positives, where a summary matches the query but when queried the remote sensor finds no matching value, wasting network resources. The more coarse-grained the summary, the lower the update overhead and the greater the fraction of false positives, while finer summaries incur update overhead while reducing query overhead due to false positives. Remote sensors may easily distinguish false positives from queries which result in search hits, and calculate the ratio between the two; based on this ratio, TSAR employs a novel adaptive technique that dynamically varies the granularity of sensor summaries to balance the metadata overhead and the overhead of false positives.
At the proxy tier, TSAR employs a novel index structure called the Interval Skip Graph, which is an ordered, distributed data structure for finding all intervals that contain a particular point or range of values. Interval skip graphs combine Interval Trees [5], an interval-based binary search tree, with Skip Graphs [1], a ordered, distributed data structure for peer-to-peer systems [13]. The resulting data structure has two properties that make it ideal for sensor networks. First, it has O(log n) search complexity for accessing the first interval that matches a particular value or range, and constant complexity for accessing each successive interval.
Second, indexing of intervals rather than individual values makes the data structure ideal for indexing summaries over time or value.
Such summary-based indexing is a more natural fit for energyconstrained sensor nodes, since transmitting summaries incurs less energy overhead than transmitting all sensor data.
Definitions: We assume that there are Np proxies and Ns sensors in a two-tier sensor network. Each proxy is responsible for multiple sensor nodes, and no assumption is made about the number of sensors per proxy. Each sensor transmits interval summaries of data or events regularly to one or more proxies that it is associated with, where interval i is represented as [lowi, highi]. These intervals can correspond to time or value ranges that are used for indexing sensor data. No assumption is made about the size of an interval or about the amount of overlap between intervals.
Range queries on the intervals are posed by users to the network of proxies and sensors; each query q needs to determine all index values that overlap the interval [lowq, highq]. The goal of the interval skip graph is to index all intervals such that the set that overlaps a query interval can be located efficiently. In the rest of this section, we describe the interval skip graph in greater detail.
In order to inform the description of the Interval Skip Graph, we first provide a brief overview of the Skip Graph data structure; for a more extensive description the reader is referred to [1]. Figure 2 shows a skip graph which indexes 8 keys; the keys may be seen along the bottom, and above each key are the pointers associated with that key. Each data element, consisting of a key and its associated pointers, may reside on a different node in the network, 42
level 0 level 1 level 2 key single skip graph element (each may be on different node) find(21) node-to-node messages Figure 2: Skip Graph of 8 Elements [6,14] [9,12] [14,16] [15,23] [18,19] [20,27] [21,30][2,5]
[low,high] max contains(13) match no match halt Figure 3: Interval Skip Graph [6,14] [9,12] [14,16] [15,23] [18,19] [20,27] [21,30] [2,5] Node 1 Node 2 Node 3 level 2 level 1 level 0 Figure 4: Distributed Interval Skip Graph and pointers therefore identify both a remote node as well as a data element on that node. In this figure we may see the following properties of a skip graph: • Ordered index: The keys are members of an ordered data type, for instance integers. Lookups make use of ordered comparisons between the search key and existing index entries. In addition, the pointers at the lowest level point directly to the successor of each item in the index. • In-place indexing: Data elements remain on the nodes where they were inserted, and messages are sent between nodes to establish links between those elements and others in the index. • Log n height: There are log2 n pointers associated with each element, where n is the number of data elements indexed.
Each pointer belongs to a level l in [0... log2 n − 1], and together with some other pointers at that level forms a chain of n/2l elements. • Probabilistic balance: Rather than relying on re-balancing operations which may be triggered at insert or delete, skip graphs implement a simple random balancing mechanism which maintains close to perfect balance on average, with an extremely low probability of significant imbalance. • Redundancy and resiliency: Each data element forms an independent search tree root, so searches may begin at any node in the network, eliminating hot spots at a single search root. In addition the index is resilient against node failure; data on the failed node will not be accessible, but remaining data elements will be accessible through search trees rooted on other nodes.
In Figure 2 we see the process of searching for a particular value in a skip graph. The pointers reachable from a single data element form a binary tree: a pointer traversal at the highest level skips over n/2 elements, n/4 at the next level, and so on. Search consists of descending the tree from the highest level to level 0, at each level comparing the target key with the next element at that level and deciding whether or not to traverse. In the perfectly balanced case shown here there are log2 n levels of pointers, and search will traverse 0 or 1 pointers at each level. We assume that each data element resides on a different node, and measure search cost by the number messages sent (i.e. the number of pointers traversed); this will clearly be O(log n).
Tree update proceeds from the bottom, as in a B-Tree, with the root(s) being promoted in level as the tree grows. In this way, for instance, the two chains at level 1 always contain n/2 entries each, and there is never a need to split chains as the structure grows. The update process then consists of choosing which of the 2l chains to insert an element into at each level l, and inserting it in the proper place in each chain.
Maintaining a perfectly balanced skip graph as shown in Figure 2 would be quite complex; instead, the probabilistic balancing method introduced in Skip Lists [23] is used, which trades off a small amount of overhead in the expected case in return for simple update and deletion. The basis for this method is the observation that any element which belongs to a particular chain at level l can only belong to one of two chains at level l+1. To insert an element we ascend levels starting at 0, randomly choosing one of the two possible chains at each level, an stopping when we reach an empty chain.
One means of implementation (e.g. as described in [1]) is to assign each element an arbitrarily long random bit string. Each chain at level l is then constructed from those elements whose bit strings match in the first l bits, thus creating 2l possible chains at each level and ensuring that each chain splits into exactly two chains at the next level. Although the resulting structure is not perfectly balanced, following the analysis in [23] we can show that the probability of it being significantly out of balance is extremely small; in addition, since the structure is determined by the random number stream, input data patterns cannot cause the tree to become imbalanced.
A skip graph is designed to store single-valued entries. In this section, we introduce a novel data structure that extends skip graphs to store intervals [lowi, highi] and allows efficient searches for all intervals covering a value v, i.e. {i : lowi ≤ v ≤ highi}. Our data structure can be extended to range searches in a straightforward manner.
The interval skip graph is constructed by applying the method of augmented search trees, as described by Cormen, Leiserson, and Rivest [5] and applied to binary search trees to create an Interval Tree. The method is based on the observation that a search structure based on comparison of ordered keys, such as a binary tree, may also be used to search on a secondary key which is non-decreasing in the first key.
Given a set of intervals sorted by lower bound - lowi ≤ lowi+1 - we define the secondary key as the cumulative maximum, maxi = maxk=0...i (highk). The set of intervals intersecting a value v may then be found by searching for the first interval (and thus the interval with least lowi) such that maxi ≥ v. We then 43 traverse intervals in increasing order lower bound, until we find the first interval with lowi > v, selecting those intervals which intersect v.
Using this approach we augment the skip graph data structure, as shown in Figure 3, so that each entry stores a range (lower bound and upper bound) and a secondary key (cumulative maximum of upper bound). To efficiently calculate the secondary key maxi for an entry i, we take the greatest of highi and the maximum values reported by each of i"s left-hand neighbors.
To search for those intervals containing the value v, we first search for v on the secondary index, maxi, and locate the first entry with maxi ≥ v. (by the definition of maxi, for this data element maxi = highi.) If lowi > v, then this interval does not contain v, and no other intervals will, either, so we are done. Otherwise we traverse the index in increasing order of mini, returning matching intervals, until we reach an entry with mini > v and we are done.
Searches for all intervals which overlap a query range, or which completely contain a query range, are straightforward extensions of this mechanism.
Lookup Complexity: Lookup for the first interval that matches a given value is performed in a manner very similar to an interval tree. The complexity of search is O(log n). The number of intervals that match a range query can vary depending on the amount of overlap in the intervals being indexed, as well as the range specified in the query.
Insert Complexity: In an interval tree or interval skip list, the maximum value for an entry need only be calculated over the subtree rooted at that entry, as this value will be examined only when searching within the subtree rooted at that entry. For a simple interval skip graph, however, this maximum value for an entry must be computed over all entries preceding it in the index, as searches may begin anywhere in the data structure, rather than at a distinguished root element. It may be easily seen that in the worse case the insertion of a single interval (one that covers all existing intervals in the index) will trigger the update of all entries in the index, for a worst-case insertion cost of O(n).
The final extensions we propose take advantage of the difference between the number of items indexed in a skip graph and the number of systems on which these items are distributed. The cost in network messages of an operation may be reduced by arranging the data structure so that most structure traversals occur locally on a single node, and thus incur zero network cost. In addition, since both congestion and failure occur on a per-node basis, we may eliminate links without adverse consequences if those links only contribute to load distribution and/or resiliency within a single node. These two modifications allow us to achieve reductions in asymptotic complexity of both update and search.
As may be in Section 3.2, insert and delete cost on an interval skip graph has a worst case complexity of O(n), compared to O(log n) for an interval tree. The main reason for the difference is that skip graphs have a full search structure rooted at each element, in order to distribute load and provide resilience to system failures in a distributed setting. However, in order to provide load distribution and failure resilience it is only necessary to provide a full search structure for each system. If as in TSAR the number of nodes (proxies) is much smaller than the number of data elements (data summaries indexed), then this will result in significant savings.
Implementation: To construct a sparse interval skip graph, we ensure that there is a single distinguished element on each system, the root element for that system; all searches will start at one of these root elements. When adding a new element, rather than splitting lists at increasing levels l until the element is in a list with no others, we stop when we find that the element would be in a list containing no root elements, thus ensuring that the element is reachable from all root elements. An example of applying this optimization may be seen in Figure 5. (In practice, rather than designating existing data elements as roots, as shown, it may be preferable to insert null values at startup.) When using the technique of membership vectors as in [1], this may be done by broadcasting the membership vectors of each root element to all other systems, and stopping insertion of an element at level l when it does not share an l-bit prefix with any of the Np root elements. The expected number of roots sharing a log2Np-bit prefix is 1, giving an expected expected height for each element of log2Np +O(1). An alternate implementation, which distributes information concerning root elements at pointer establishment time, is omitted due to space constraints; this method eliminates the need for additional messages.
Performance: In a (non-interval) sparse skip graph, since the expected height of an inserted element is now log2 Np + O(1), expected insertion complexity is O(log Np), rather than O(log n), where Np is the number of root elements and thus the number of separate systems in the network. (In the degenerate case of a single system we have a skip list; with splitting probability 0.5 the expected height of an individual element is 1.) Note that since searches are started at root elements of expected height log2 n, search complexity is not improved.
For an interval sparse skip graph, update performance is improved considerably compared to the O(n) worst case for the nonsparse case. In an augmented search structure such as this, an element only stores information for nodes which may be reached from that element-e.g. the subtree rooted at that element, in the case of a tree. Thus, when updating the maximum value in an interval tree, the update is only propagated towards the root. In a sparse interval skip graph, updates to a node only propagate towards the Np root elements, for a worst-case cost of Np log2 n.
Shortcut search: When beginning a search for a value v, rather than beginning at the root on that proxy, we can find the element that is closest to v (e.g. using a secondary local index), and then begin the search at that element. The expected distance between this element and the search terminus is log2 Np, and the search will now take on average log2 Np + O(1) steps. To illustrate this optimization, in Figure 4 depending on the choice of search root, a search for [21, 30] beginning at node 2 may take 3 network hops, traversing to node 1, then back to node 2, and finally to node 3 where the destination is located, for a cost of 3 messages. The shortcut search, however, locates the intermediate data element on node 2, and then proceeds directly to node 3 for a cost of 1 message.
Performance: This technique may be applied to the primary key search which is the first of two insertion steps in an interval skip graph. By combining the short-cut optimization with sparse interval skip graphs, the expected cost of insertion is now O(log Np), independent of the size of the index or the degree of overlap of the inserted intervals.
Thus far we have only compared the sparse interval skip graph with similar structures from which it is derived. A comparison with several other data structures which meet at least some of the requirements for the TSAR index is shown in Table 2. 44 Table 2: Comparison of Distributed Index Structures Range Query Support Interval Representation Re-balancing Resilience Small Networks Large Networks DHT, GHT no no no yes good good Local index, flood query yes yes no yes good bad P-tree, RP* (distributed B-Trees) yes possible yes no good good DIMS yes no yes yes yes yes Interval Skipgraph yes yes no yes good good [6,14] [9,12] [14,16] [15,23] [18,19] [20,27] [21,30][2,5] Roots Node 1 Node 2 Figure 5: Sparse Interval Skip Graph The hash-based systems, DHT [25] and GHT [26], lack the ability to perform range queries and are thus not well-suited to indexing spatio-temporal data. Indexing locally using an appropriate singlenode structure and then flooding queries to all proxies is a competitive alternative for small networks; for large networks the linear dependence on the number of proxies becomes an issue. Two distributed B-Trees were examined - P-Trees [6] and RP* [19]. Each of these supports range queries, and in theory could be modified to support indexing of intervals; however, they both require complex re-balancing, and do not provide the resilience characteristics of the other structures. DIMS [18] provides the ability to perform spatio-temporal range queries, and has the necessary resilience to failures; however, it cannot be used index intervals, which are used by TSAR"s data summarization algorithm.
Having described the proxy-level index structure, we turn to the mechanisms at the sensor tier. TSAR implements two key mechanisms at the sensor tier. The first is a local archival store at each sensor node that is optimized for resource-constrained devices. The second is an adaptive summarization technique that enables each sensor to adapt to changing data and query characteristics. The rest of this section describes these mechanisms in detail.
Interval skip graphs provide an efficient mechanism to lookup sensor nodes containing data relevant to a query. These queries are then routed to the sensors, which locate the relevant data records in the local archive and respond back to the proxy. To enable such lookups, each sensor node in TSAR maintains an archival store of sensor data. While the implementation of such an archival store is straightforward on resource-rich devices that can run a database, sensors are often power and resource-constrained. Consequently, the sensor archiving subsystem in TSAR is explicitly designed to exploit characteristics of sensor data in a resource-constrained setting.
Timestamp Calibration Parameters Opaque DataData/Event Attributes size Figure 6: Single storage record Sensor data has very distinct characteristics that inform our design of the TSAR archival store. Sensors produce time-series data streams, and therefore, temporal ordering of data is a natural and simple way of storing archived sensor data. In addition to simplicity, a temporally ordered store is often suitable for many sensor data processing tasks since they involve time-series data processing.
Examples include signal processing operations such as FFT, wavelet transforms, clustering, similarity matching, and target detection.
Consequently, the local archival store is a collection of records, designed as an append-only circular buffer, where new records are appended to the tail of the buffer. The format of each data record is shown in Figure 6. Each record has a metadata field which includes a timestamp, sensor settings, calibration parameters, etc. Raw sensor data is stored in the data field of the record. The data field is opaque and application-specific-the storage system does not know or care about interpreting this field. A camera-based sensor, for instance, may store binary images in this data field. In order to support a variety of applications, TSAR supports variable-length data fields; as a result, record sizes can vary from one record to another.
Our archival store supports three operations on records: create, read, and delete. Due to the append-only nature of the store, creation of records is simple and efficient. The create operation simply creates a new record and appends it to the tail of the store. Since records are always written at the tail, the store need not maintain a free space list. All fields of the record need to be specified at creation time; thus, the size of the record is known a priori and the store simply allocates the the corresponding number of bytes at the tail to store the record. Since writes are immutable, the size of a record does not change once it is created. proxy proxy proxy record
summary local archive in flash memory data summary start,end offset time interval sensor summary sent to proxy Insert summaries into interval skip graph Figure 7: Sensor Summarization 45 The read operation enables stored records to be retrieved in order to answer queries. In a traditional database system, efficient lookups are enabled by maintaining a structure such as a B-tree that indexes certain keys of the records. However, this can be quite complex for a small sensor node with limited resources. Consequently,
TSAR sensors do not maintain any index for the data stored in their archive. Instead, they rely on the proxies to maintain this metadata index-sensors periodically send the proxy information summarizing the data contained in a contiguous sequence of records, as well as a handle indicating the location of these records in flash memory.
The mechanism works as follows: In addition to the summary of sensor data, each node sends metadata to the proxy containing the time interval corresponding to the summary, as well as the start and end offsets of the flash memory location where the raw data corresponding is stored (as shown in Figure 7). Thus, random access is enabled at granularity of a summary-the start offset of each chunk of records represented by a summary is known to the proxy.
Within this collection, records are accessed sequentially. When a query matches a summary in the index, the sensor uses these offsets to access the relevant records on its local flash by sequentially reading data from the start address until the end address. Any queryspecific operation can then be performed on this data. Thus, no index needs to be maintained at the sensor, in line with our goal of simplifying sensor state management. The state of the archive is captured in the metadata associated with the summaries, and is stored and maintained at the proxy.
While we anticipate local storage capacity to be large, eventually there might be a need to overwrite older data, especially in high data rate applications. This may be done via techniques such as multi-resolution storage of data [9], or just simply by overwriting older data. When older data is overwritten, a delete operation is performed, where an index entry is deleted from the interval skip graph at the proxy and the corresponding storage space in flash memory at the sensor is freed.
The data summaries serve as glue between the storage at the remote sensor and the index at the proxy. Each update from a sensor to the proxy includes three pieces of information: the summary, a time period corresponding to the summary, and the start and end offsets for the flash archive. In general, the proxy can index the time interval representing a summary or the value range reported in the summary (or both). The former index enables quick lookups on all records seen during a certain interval, while the latter index enables quick lookups on all records matching a certain value.
As described in Section 2.4, there is a trade-off between the energy used in sending summaries (and thus the frequency and resolution of those summaries) and the cost of false hits during queries.
The coarser and less frequent the summary information, the less energy required, while false query hits in turn waste energy on requests for non-existent data.
TSAR employs an adaptive summarization technique that balances the cost of sending updates against the cost of false positives.
The key intuition is that each sensor can independently identify the fraction of false hits and true hits for queries that access its local archive. If most queries result in true hits, then the sensor determines that the summary can be coarsened further to reduce update costs without adversely impacting the hit ratio. If many queries result in false hits, then the sensor makes the granularity of each summary finer to reduce the number and overhead of false hits.
The resolution of the summary depends on two parametersthe interval over which summaries of the data are constructed and transmitted to the proxy, as well as the size of the applicationspecific summary. Our focus in this paper is on the interval over which the summary is constructed. Changing the size of the data summary can be performed in an application-specific manner (e.g. using wavelet compression techniques as in [9]) and is beyond the scope of this paper. Currently, TSAR employs a simple summarization scheme that computes the ratio of false and true hits and decreases (increases) the interval between summaries whenever this ratio increases (decreases) beyond a threshold.
We have implemented a prototype of TSAR on a multi-tier sensor network testbed. Our prototype employs Crossbow Stargate nodes to implement the proxy tier. Each Stargate node employs a 400MHz Intel XScale processor with 64MB RAM and runs the Linux 2.4.19 kernel and EmStar release 2.1. The proxy nodes are equipped with two wireless radios, a Cisco Aironet 340-based
using the EmStar transceiver. The 802.11b wireless network is used for inter-proxy communication within the proxy tier, while the wireless bridge enables sensor-proxy communication. The sensor tier consists of Crossbow Mica2s and Mica2dots, each consisting of a 915MHz CC1000 radio, a BMAC protocol stack, a 4 Mb on-board flash memory and an ATMega 128L processor. The sensor nodes run TinyOS 1.1.8. In addition to the on-board flash, the sensor nodes can be equipped with external MMC/SD flash cards using a custom connector. The proxy nodes can be equipped with external storage such as high-capacity compact flash (up to 4GB), 6GB micro-drives, or up to 60GB 1.8inch mobile disk drives.
Since sensor nodes may be several hops away from the nearest proxy, the sensor tier employs multi-hop routing to communicate with the proxy tier. In addition, to reduce the power consumption of the radio while still making the sensor node available for queries, low power listening is enabled, in which the radio receiver is periodically powered up for a short interval to sense the channel for transmissions, and the packet preamble is extended to account for the latency until the next interval when the receiving radio wakes up. Our prototype employs the MultiHopLEPSM routing protocol with the BMAC layer configured in the low-power mode with a 11% duty cycle (one of the default BMAC [22] parameters) Our TSAR implementation on the Mote involves a data gathering task that periodically obtains sensor readings and logs these reading to flash memory. The flash memory is assumed to be a circular append-only store and the format of the logged data is depicted in Figure 6. The Mote sends a report to the proxy every N readings, summarizing the observed data. The report contains: (i) the address of the Mote, (ii) a handle that contains an offset and the length of the region in flash memory containing data referred to by the summary, (iii) an interval (t1, t2) over which this report is generated, (iv) a tuple (low, high) representing the minimum and the maximum values observed at the sensor in the interval, and (v) a sequence number. The sensor updates are used to construct a sparse interval skip graph that is distributed across proxies, via network messages between proxies over the 802.11b wireless network.
Our current implementation supports queries that request records matching a time interval (t1, t2) or a value range (v1, v2). Spatial constraints are specified using sensor IDs. Given a list of matching intervals from the skip graph, TSAR supports two types of messages to query the sensor: lookup and fetch. A lookup message triggers a search within the corresponding region in flash memory and returns the number of matching records in that memory region (but does not retrieve data). In contrast, a fetch message not only 46 0 10 20 30 40 50 60 70 80
NumberofMessages Index size (entries) Insert (skipgraph) Insert (sparse skipgraph) Initial lookup (a) James Reserve Data 0 10 20 30 40 50 60 70 80
NumberofMessages Index size (entries) Insert (skipgraph) Insert (sparse skipgraph) Initial lookup (b) Synthetic Data Figure 8: Skip Graph Insert Performance triggers a search but also returns all matching data records to the proxy. Lookup messages are useful for polling a sensor, for instance, to determine if a query matches too many records.
In this section, we evaluate the efficacy of TSAR using our prototype and simulations. The testbed for our experiments consists of four Stargate proxies and twelve Mica2 and Mica2dot sensors; three sensors each are assigned to each proxy. Given the limited size of our testbed, we employ simulations to evaluate the behavior of TSAR in larger settings. Our simulation employs the EmTOS emulator [10], which enables us to run the same code in simulation and the hardware platform.
Rather than using live data from a real sensor, to ensure repeatable experiments, we seed each sensor node with a dataset (i.e., a trace) that dictates the values reported by that node to the proxy. One section of the flash memory on each sensor node is programmed with data points from the trace; these observations are then replayed during an experiment, logged to the local archive (located in flash memory, as well), and reported to the proxy. The first dataset used to evaluate TSAR is a temperature dataset from James Reserve [27] that includes data from eleven temperature sensor nodes over a period of 34 days. The second dataset is synthetically generated; the trace for each sensor is generated using a uniformly distributed random walk though the value space.
Our experimental evaluation has four parts. First, we run EmTOS simulations to evaluate the lookup, update and delete overhead for sparse interval skip graphs using the real and synthetic datasets.
Second, we provide summary results from micro-benchmarks of the storage component of TSAR, which include empirical characterization of the energy costs and latency of reads and writes for the flash memory chip as well as the whole mote platform, and comparisons to published numbers for other storage and communication technologies. These micro-benchmarks form the basis for our full-scale evaluation of TSAR on a testbed of four Stargate proxies and twelve Motes. We measure the end-to-end query latency in our multi-hop testbed as well as the query processing overhead at the mote tier. Finally, we demonstrate the adaptive summarization capability at each sensor node. The remainder of this section presents our experimental results.
This section evaluates the performance of sparse interval skip graphs by quantifying insert, lookup and delete overheads.
We assume a proxy tier with 32 proxies and construct sparse interval skip graphs of various sizes using our datasets. For each skip 0 5 10 15 20 25 30 35 409620481024512 NumberofMessages Index size (entries) Initial lookup Traversal (a) James Reserve Data 0 2 4 6 8 10 12 14 409620481024512 NumberofMessages Index size (entries) Initial lookup Traversal (b) Synthetic Data Figure 9: Skip Graph Lookup Performance 0 10 20 30 40 50 60 70
Numberofmessages Number of proxies Skipgraph insert Sparse skipgraph insert Initial lookup (a) Impact of Number of Proxies 0 20 40 60 80 100 120
NumberofMessages Index size (entries) Insert (redundant) Insert (non-redundant) Lookup (redundant) Lookup (non-redundant) (b) Impact of Redundant Summaries Figure 10: Skip Graph Overheads graph, we evaluate the cost of inserting a new value into the index.
Each entry was deleted after its insertion, enabling us to quantify the delete overhead as well. Figure 8(a) and (b) quantify the insert overhead for our two datasets: each insert entails an initial traversal that incurs log n messages, followed by neighbor pointer update at increasing levels, incurring a cost of 4 log n messages. Our results demonstrate this behavior, and show as well that performance of delete-which also involves an initial traversal followed by pointer updates at each level-incurs a similar cost.
Next, we evaluate the lookup performance of the index structure. Again, we construct skip graphs of various sizes using our datasets and evaluate the cost of a lookup on the index structure.
Figures 9(a) and (b) depict our results. There are two components for each lookup-the lookup of the first interval that matches the query and, in the case of overlapping intervals, the subsequent linear traversal to identify all matching intervals. The initial lookup can be seen to takes log n messages, as expected. The costs of the subsequent linear traversal, however, are highly data dependent.
For instance, temperature values for the James Reserve data exhibit significant spatial correlations, resulting in significant overlap between different intervals and variable, high traversal cost (see Figure 9(a)). The synthetic data, however, has less overlap and incurs lower traversal overhead as shown in Figure 9(b).
Since the previous experiments assumed 32 proxies, we evaluate the impact of the number of proxies on skip graph performance. We vary the number of proxies from 10 to 48 and distribute a skip graph with 4096 entries among these proxies. We construct regular interval skip graphs as well as sparse interval skip graphs using these entries and measure the overhead of inserts and lookups. Thus, the experiment also seeks to demonstrate the benefits of sparse skip graphs over regular skip graphs. Figure 10(a) depicts our results.
In regular skip graphs, the complexity of insert is O(log2n) in the 47 expected case (and O(n) in the worst case) where n is the number of elements. This complexity is unaffected by changing the number of proxies, as indicated by the flat line in the figure. Sparse skip graphs require fewer pointer updates; however, their overhead is dependent on the number of proxies, and is O(log2Np) in the expected case, independent of n. This can be seen to result in significant reduction in overhead when the number of proxies is small, which decreases as the number of proxies increases.
Failure handling is an important issue in a multi-tier sensor architecture since it relies on many components-proxies, sensor nodes and routing nodes can fail, and wireless links can fade. Handling of many of these failure modes is outside the scope of this paper; however, we consider the case of resilience of skip graphs to proxy failures. In this case, skip graph search (and subsequent repair operations) can follow any one of the other links from a root element. Since a sparse skip graph has search trees rooted at each node, searching can then resume once the lookup request has routed around the failure. Together, these two properties ensure that even if a proxy fails, the remaining entries in the skip graph will be reachable with high probability-only the entries on the failed proxy and the corresponding data at the sensors becomes inaccessible.
To ensure that all data on sensors remains accessible, even in the event of failure of a proxy holding index entries for that data, we incorporate redundant index entries. TSAR employs a simple redundancy scheme where additional coarse-grain summaries are used to protect regular summaries. Each sensor sends summary data periodically to its local proxy, but less frequently sends a lowerresolution summary to a backup proxy-the backup summary represents all of the data represented by the finer-grained summaries, but in a lossier fashion, thus resulting in higher read overhead (due to false hits) if the backup summary is used. The cost of implementing this in our system is low - Figure 10(b) shows the overhead of such a redundancy scheme, where a single coarse summary is send to a backup for every two summaries sent to the primary proxy.
Since a redundant summary is sent for every two summaries, the insert cost is 1.5 times the cost in the normal case. However, these redundant entries result in only a negligible increase in lookup overhead, due the logarithmic dependence of lookup cost on the index size, while providing full resilience to any single proxy failure.
Since sensors are resource-constrained, the energy consumption and the latency at this tier are important measures for evaluating the performance of a storage architecture. Before performing an endto-end evaluation of our system, we provide more detailed information on the energy consumption of the storage component used to implement the TSAR local archive, based on empirical measurements. In addition we compare these figures to those for other local storage technologies, as well as to the energy consumption of wireless communication, using information from the literature. For empirical measurements we measure energy usage for the storage component itself (i.e. current drawn by the flash chip), as well as for the entire Mica2 mote.
The power measurements in Table 3 were performed for the AT45DB041 [15] flash memory on a Mica2 mote, which is an older NOR flash device. The most promising technology for low-energy storage on sensing devices is NAND flash, such as the Samsung K9K4G08U0M device [16]; published power numbers for this device are provided in the table. Published energy requirements for wireless transmission using the Chipcon [4] CC2420 radio (used in MicaZ and Telos motes) are provided for comparison, assuming Energy Energy/byte Mote flash Read 256 byte page 58µJ* / 136µJ* total
Write 256 byte page 926µJ* / 1042µJ* total
NAND Flash Read 512 byte page 2.7µJ 1.8nJ Write 512 byte page 7.8µJ 15nJ Erase 16K byte sector 60µJ 3.7nJ CC2420 radio Transmit 8 bits (-25dBm)
Receive 8 bits 1.9µJ 1.9µJ Mote AVR processor In-memory search,
Table 3: Storage and Communication Energy Costs (*measured values) 0 200 400 600 800 1000
Latency(ms) Number of hops (a) Multi-hop query performance 0 100 200 300 400 500
Latency(ms) Index size (entries) Sensor communication Proxy communication Sensor lookup, processing (b) Query Performance Figure 11: Query Processing Latency zero network and protocol overhead. Comparing the total energy cost for writing flash (erase + write) to the total cost for communication (transmit + receive), we find that the NAND flash is almost
perfect network protocols.
This section reports results from an end-to-end evaluation of the TSAR prototype involving both tiers. In our setup, there are four proxies connected via 802.11 links and three sensors per proxy. The multi-hop topology was preconfigured such that sensor nodes were connected in a line to each proxy, forming a minimal tree of depth 0 400 800 1200 1600
Retrievallatency(ms) Archived data retrieved (bytes) (a) Data Query and Fetch Time 0 2 4 6 8 10
Latency(ms) Number of 34-byte records searched (b) Sensor query processing delay Figure 12: Query Latency Components 48
experiments with dozens of sensor nodes, however this topology ensured that the network diameter was as large as for a typical network of significantly larger size.
Our evaluation metric is the end-to-end latency of query processing. A query posed on TSAR first incurs the latency of a sparse skip graph lookup, followed by routing to the appropriate sensor node(s). The sensor node reads the required page(s) from its local archive, processes the query on the page that is read, and transmits the response to the proxy, which then forwards it to the user. We first measure query latency for different sensors in our multi-hop topology. Depending on which of the sensors is queried, the total latency increases almost linearly from about 400ms to 1 second, as the number of hops increases from 1 to 3 (see Figure 11(a)).
Figure 11(b) provides a breakdown of the various components of the end-to-end latency. The dominant component of the total latency is the communication over one or more hops. The typical time to communicate over one hop is approximately 300ms.
This large latency is primarily due to the use of a duty-cycled MAC layer; the latency will be larger if the duty cycle is reduced (e.g. the 2% setting as opposed to the 11.5% setting used in this experiment), and will conversely decrease if the duty cycle is increased.
The figure also shows the latency for varying index sizes; as expected, the latency of inter-proxy communication and skip graph lookups increases logarithmically with index size. Not surprisingly, the overhead seen at the sensor is independent of the index size.
The latency also depends on the number of packets transmitted in response to a query-the larger the amount of data retrieved by a query, the greater the latency. This result is shown in Figure 12(a).
The step function is due to packetization in TinyOS; TinyOS sends one packet so long as the payload is smaller than 30 bytes and splits the response into multiple packets for larger payloads. As the data retrieved by a query is increased, the latency increases in steps, where each step denotes the overhead of an additional packet.
Finally, Figure 12(b) shows the impact of searching and processing flash memory regions of increasing sizes on a sensor. Each summary represents a collection of records in flash memory, and all of these records need to be retrieved and processed if that summary matches a query. The coarser the summary, the larger the memory region that needs to be accessed. For the search sizes examined, amortization of overhead when searching multiple flash pages and archival records, as well as within the flash chip and its associated driver, results in the appearance of sub-linear increase in latency with search size. In addition, the operation can be seen to have very low latency, in part due to the simplicity of our query processing, requiring only a compare operation with each stored element. More complex operations, however, will of course incur greater latency.
When data is summarized by the sensor before being reported to the proxy, information is lost. With the interval summarization method we are using, this information loss will never cause the proxy to believe that a sensor node does not hold a value which it in fact does, as all archived values will be contained within the interval reported. However, it does cause the proxy to believe that the sensor may hold values which it does not, and forward query messages to the sensor for these values. These false positives constitute the cost of the summarization mechanism, and need to be balanced against the savings achieved by reducing the number of reports. The goal of adaptive summarization is to dynamically vary the summary size so that these two costs are balanced. 0
Fractionoftruehits Summary size (number of records) (a) Impact of summary size 0 5 10 15 20 25 30 35
Summarizationsize(num.records) Normalized time (units) query rate 0.2 query rate 0.03 query rate 0.1 (b) Adaptation to query rate Figure 13: Impact of Summarization Granularity Figure 13(a) demonstrates the impact of summary granularity on false hits. As the number of records included in a summary is increased, the fraction of queries forwarded to the sensor which match data held on that sensor (true positives) decreases. Next, in Figure 13(b) we run the a EmTOS simulation with our adaptive summarization algorithm enabled. The adaptive algorithm increases the summary granularity (defined as the number of records per summary) when Cost(updates) Cost(falsehits) > 1 + and reduces it if Cost(updates) Cost(falsehits) > 1 − , where is a small constant. To demonstrate the adaptive nature of our technique, we plot a time series of the summarization granularity. We begin with a query rate of 1 query per 5 samples, decrease it to 1 every 30 samples, and then increase it again to 1 query every 10 samples. As shown in Figure 13(b), the adaptive technique adjusts accordingly by sending more fine-grain summaries at higher query rates (in response to the higher false hit rate), and fewer, coarse-grain summaries at lower query rates.
In this section, we review prior work on storage and indexing techniques for sensor networks. While our work addresses both problems jointly, much prior work has considered them in isolation.
The problem of archival storage of sensor data has received limited attention in sensor network literature. ELF [7] is a logstructured file system for local storage on flash memory that provides load leveling and Matchbox is a simple file system that is packaged with the TinyOS distribution [14]. Both these systems focus on local storage, whereas our focus is both on storage at the remote sensors as well as providing a unified view of distributed data across all such local archives. Multi-resolution storage [9] is intended for in-network storage and search in systems where there is significant data in comparison to storage resources. In contrast,
TSAR addresses the problem of archival storage in two-tier systems where sufficient resources can be placed at the edge sensors. The RISE platform [21] being developed as part of the NODE project at UCR addresses the issues of hardware platform support for large amounts of storage in remote sensor nodes, but not the indexing and querying of this data.
In order to efficiently access a distributed sensor store, an index needs to be constructed of the data. Early work on sensor networks such as Directed Diffusion [17] assumes a system where all useful sensor data was stored locally at each sensor, and spatially scoped queries are routed using geographic co-ordinates to locations where the data is stored. Sources publish the events that they detect, and sinks with interest in specific events can subscribe to these events.
The Directed Diffusion substrate routes queries to specific locations 49 if the query has geographic information embedded in it (e.g.: find temperature in the south-west quadrant), and if not, the query is flooded throughout the network.
These schemes had the drawback that for queries that are not geographically scoped, search cost (O(n) for a network of n nodes) may be prohibitive in large networks with frequent queries.
Local storage with in-network indexing approaches address this issue by constructing indexes using frameworks such as Geographic Hash Tables [24] and Quad Trees [9]. Recent research has seen a growing body of work on data indexing schemes for sensor networks[26][11][18]. One such scheme is DCS [26], which provides a hash function for mapping from event name to location. DCS constructs a distributed structure that groups events together spatially by their named type. Distributed Index of Features in Sensornets (DIFS [11]) and Multi-dimensional Range Queries in Sensor Networks (DIM [18]) extend the data-centric storage approach to provide spatially distributed hierarchies of indexes to data.
While these approaches advocate in-network indexing for sensor networks, we believe that indexing is a task that is far too complicated to be performed at the remote sensor nodes since it involves maintaining significant state and large tables. TSAR provides a better match between resource requirements of storage and indexing and the availability of resources at different tiers. Thus complex operations such as indexing and managing metadata are performed at the proxies, while storage at the sensor remains simple.
In addition to storage and indexing techniques specific to sensor networks, many distributed, peer-to-peer and spatio-temporal index structures are relevant to our work. DHTs [25] can be used for indexing events based on their type, quad-tree variants such as Rtrees [12] can be used for optimizing spatial searches, and K-D trees [2] can be used for multi-attribute search. While this paper focuses on building an ordered index structure for range queries, we will explore the use of other index structures for alternate queries over sensor data.
In this paper, we argued that existing sensor storage systems are designed primarily for flat hierarchies of homogeneous sensor nodes and do not fully exploit the multi-tier nature of emerging sensor networks. We presented the design of TSAR, a fundamentally different storage architecture that envisions separation of data from metadata by employing local storage at the sensors and distributed indexing at the proxies. At the proxy tier, TSAR employs a novel multi-resolution ordered distributed index structure, the Sparse Interval Skip Graph, for efficiently supporting spatio-temporal and range queries. At the sensor tier, TSAR supports energy-aware adaptive summarization that can trade-off the energy cost of transmitting metadata to the proxies against the overhead of false hits resulting from querying a coarser resolution index structure. We implemented TSAR in a two-tier sensor testbed comprising Stargatebased proxies and Mote-based sensors. Our experimental evaluation of TSAR demonstrated the benefits and feasibility of employing our energy-efficient low-latency distributed storage architecture in multi-tier sensor networks.
[1] James Aspnes and Gauri Shah. Skip graphs. In Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 384-393, Baltimore, MD, USA, 12-14 January 2003. [2] Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Commun. ACM, 18(9):509-517, 1975. [3] Philippe Bonnet, J. E. Gehrke, and Praveen Seshadri. Towards sensor database systems. In Proceedings of the Second International Conference on Mobile Data Management., January 2001. [4] Chipcon. CC2420 2.4 GHz IEEE 802.15.4 / ZigBee-ready RF transceiver, 2004. [5] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms. The MIT Press and McGraw-Hill, second edition edition, 2001. [6] Adina Crainiceanu, Prakash Linga, Johannes Gehrke, and Jayavel Shanmugasundaram. Querying Peer-to-Peer Networks Using P-Trees.
Technical Report TR2004-1926, Cornell University, 2004. [7] Hui Dai, Michael Neufeld, and Richard Han. ELF: an efficient log-structured flash file system for micro sensor nodes. In SenSys "04: Proceedings of the 2nd international conference on Embedded networked sensor systems, pages 176-187, New York, NY, USA, 2004. ACM Press. [8] Peter Desnoyers, Deepak Ganesan, Huan Li, and Prashant Shenoy. PRESTO: A predictive storage architecture for sensor networks. In Tenth Workshop on Hot Topics in Operating Systems (HotOS X)., June 2005. [9] Deepak Ganesan, Ben Greenstein, Denis Perelyubskiy, Deborah Estrin, and John Heidemann. An evaluation of multi-resolution storage in sensor networks.
In Proceedings of the First ACM Conference on Embedded Networked Sensor Systems (SenSys)., 2003. [10] L. Girod, T. Stathopoulos, N. Ramanathan, J. Elson, D. Estrin, E. Osterweil, and T. Schoellhammer. A system for simulation, emulation, and deployment of heterogeneous sensor networks. In Proceedings of the Second ACM Conference on Embedded Networked Sensor Systems, Baltimore, MD, 2004. [11] B. Greenstein, D. Estrin, R. Govindan, S. Ratnasamy, and S. Shenker. DIFS: A distributed index for features in sensor networks. Elsevier Journal of ad-hoc Networks, 2003. [12] Antonin Guttman. R-trees: a dynamic index structure for spatial searching. In SIGMOD "84: Proceedings of the 1984 ACM SIGMOD international conference on Management of data, pages 47-57, New York, NY, USA, 1984.
ACM Press. [13] Nicholas Harvey, Michael B. Jones, Stefan Saroiu, Marvin Theimer, and Alec Wolman. Skipnet: A scalable overlay network with practical locality properties.
In In proceedings of the 4th USENIX Symposium on Internet Technologies and Systems (USITS "03), Seattle, WA, March 2003. [14] Jason Hill, Robert Szewczyk, Alec Woo, Seth Hollar, David Culler, and Kristofer Pister. System architecture directions for networked sensors. In Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IX), pages 93-104,
Cambridge, MA, USA, November 2000. ACM. [15] Atmel Inc. 4-megabit 2.5-volt or 2.7-volt DataFlash AT45DB041B, 2005. [16] Samsung Semiconductor Inc. K9W8G08U1M, K9K4G08U0M: 512M x 8 bit / 1G x 8 bit NAND flash memory, 2003. [17] Chalermek Intanagonwiwat, Ramesh Govindan, and Deborah Estrin. Directed diffusion: A scalable and robust communication paradigm for sensor networks.
In Proceedings of the Sixth Annual International Conference on Mobile Computing and Networking, pages 56-67, Boston, MA, August 2000. ACM Press. [18] Xin Li, Young-Jin Kim, Ramesh Govindan, and Wei Hong. Multi-dimensional range queries in sensor networks. In Proceedings of the First ACM Conference on Embedded Networked Sensor Systems (SenSys)., 2003. to appear. [19] Witold Litwin, Marie-Anne Neimat, and Donovan A. Schneider. RP*: A family of order preserving scalable distributed data structures. In VLDB "94: Proceedings of the 20th International Conference on Very Large Data Bases, pages 342-353, San Francisco, CA, USA, 1994. [20] Samuel Madden, Michael Franklin, Joseph Hellerstein, and Wei Hong. TAG: a tiny aggregation service for ad-hoc sensor networks. In OSDI, Boston, MA,
[21] A. Mitra, A. Banerjee, W. Najjar, D. Zeinalipour-Yazti, D.Gunopulos, and V. Kalogeraki. High performance, low power sensor platforms featuring gigabyte scale storage. In SenMetrics 2005: Third International Workshop on Measurement, Modeling, and Performance Analysis of Wireless Sensor Networks, July 2005. [22] J. Polastre, J. Hill, and D. Culler. Versatile low power media access for wireless sensor networks. In Proceedings of the Second ACM Conference on Embedded Networked Sensor Systems (SenSys), November 2004. [23] William Pugh. Skip lists: a probabilistic alternative to balanced trees. Commun.
ACM, 33(6):668-676, 1990. [24] S. Ratnasamy, D. Estrin, R. Govindan, B. Karp, L. Yin S. Shenker, and F. Yu.
Data-centric storage in sensornets. In ACM First Workshop on Hot Topics in Networks, 2001. [25] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A scalable content addressable network. In Proceedings of the 2001 ACM SIGCOMM Conference, 2001. [26] S. Ratnasamy, B. Karp, L. Yin, F. Yu, D. Estrin, R. Govindan, and S. Shenker.
GHT - a geographic hash-table for data-centric storage. In First ACM International Workshop on Wireless Sensor Networks and their Applications,

In wireless sensor networks, data or events will be named by attributes [15] or represented as virtual relations in a distributed database [18, 3]. Many of these attributes will have scalar values: e.g., temperature and light levels, soil moisture conditions, etc. In these systems, we argue, one natural way to query for events of interest will be to use multi-dimensional range queries on these attributes. For example, scientists analyzing the growth of marine microorganisms might be interested in events that occurred within certain temperature and light conditions: List all events that have temperatures between 50◦ F and 60◦ F, and light levels between 10 and 20.
Such range queries can be used in two distinct ways. They can help users efficiently drill-down their search for events of interest. The query described above illustrates this, where the scientist is presumably interested in discovering, and perhaps mapping the combined effect of temperature and light on the growth of marine micro-organisms. More importantly, they can be used by application software running within a sensor network for correlating events and triggering actions. For example, if in a habitat monitoring application, a bird alighting on its nest is indicated by a certain range of thermopile sensor readings, and a certain range of microphone readings, a multi-dimensional range query on those attributes enables higher confidence detection of the arrival of a flock of birds, and can trigger a system of cameras.
In traditional database systems, such range queries are supported using pre-computed indices. Indices trade-off some initial pre-computation cost to achieve a significantly more efficient querying capability. For sensor networks, we assert that a centralized index for multi-dimensional range queries may not be feasible for energy-efficiency reasons (as well as the fact that the access bandwidth to this central index will be limited, particularly for queries emanating from within the network). Rather, we believe, there will be situations when it is more appropriate to build an innetwork distributed data structure for efficiently answering multi-dimensional range queries.
In this paper, we present just such a data structure, that we call a DIM1 . DIMs are inspired by classical database indices, and are essentially embeddings of such indices within the sensor network. DIMs leverage two key ideas: in-network 1 Distributed Index for Multi-dimensional data. 63 data centric storage, and a novel locality-preserving geographic hash (Section 3). DIMs trace their lineage to datacentric storage systems [23]. The underlying mechanism in these systems allows nodes to consistently hash an event to some location within the network, which allows efficient retrieval of events. Building upon this, DIMs use a technique whereby events whose attribute values are close are likely to be stored at the same or nearby nodes. DIMs then use an underlying geographic routing algorithm (GPSR [16]) to route events and queries to their corresponding nodes in an entirely distributed fashion.
We discuss the design of a DIM, presenting algorithms for event insertion and querying, for maintaining a DIM in the event of node failure, and for making DIMs robust to data or packet loss (Section 3). We then extensively evaluate DIMs using analysis (Section 4), simulation (Section 5), and actual implementation (Section 6). Our analysis reveals that, under reasonable assumptions about query distributions, DIMs scale quite well with network size (both insertion and query costs scale as O( √ N)). In detailed simulations, we show that in practice, the event insertion and querying costs of other alternatives are sometimes an order of magnitude the costs of DIMs, even for moderately sized network.
Experiments on a small scale testbed validate the feasibility of DIMs (Section 6). Much work remains, including efficient support for skewed data distributions, existential queries, and node heterogeneity.
We believe that DIMs will be an essential, but perhaps not necessarily the only, distributed data structure supporting efficient queries in sensor networks. DIMs will be part of a suite of such systems that enable feature extraction [7], simple range querying [10], exact-match queries [23], or continuous queries [15, 18]. All such systems will likely be integrated to a sensor network database system such as TinyDB [17]. Application designers could then choose the appropriate method of information access. For instance, a fire tracking application would use DIM to detect the hotspots, and would then use mechanisms that enable continuous queries [15, 18] to track the spatio-temporal progress of the hotspots. Finally, we note that DIMs are applicable not just to sensor networks, but to other deeply distributed systems (embedded networks for home and factory automation) as well.
The basic problem that this paper addresses - multidimensional range queries - is typically solved in database systems using indexing techniques. The database community has focused mostly on centralized indices, but distributed indexing has received some attention in the literature.
Indexing techniques essentially trade-off some data insertion cost to enable efficient querying. Indexing has, for long, been a classical research problem in the database community [5, 2]. Our work draws its inspiration from the class of multi-key constant branching index structures, exemplified by k-d trees [2], where k represents the dimensionality of the data space. Our approach essentially represents a geographic embedding of such structures in a sensor field.
There is one important difference. The classical indexing structures are data-dependent (as are some indexing schemes that use locality preserving hashes, and developed in the theory literature [14, 8, 13]). The index structure is decided not only by the data, but also by the order in which data is inserted. Our current design is not data dependent.
Finally, tangentially related to our work is the class of spatial indexing systems [21, 6, 11].
While there has been some work on distributed indexing, the problem has not been extensively explored. There exist distributed indices of a restricted kind-those that allow exact match or partial prefix match queries. Examples of such systems, of course, are the Internet Domain Name System, and the class of distributed hash table (DHT) systems exemplified by Freenet[4], Chord[24], and CAN[19]. Our work is superficially similar to CAN in that both construct a zone-based overlay atop of the underlying physical network. The underlying details make the two systems very different: CAN"s overlay is purely logical while our overlay is consistent with the underlying physical topology. More recent work in the Internet context has addressed support for range queries in DHT systems [1, 12], but it is unclear if these directly translate to the sensor network context.
Several research efforts have expressed the vision of a database interface to sensor networks [9, 3, 18], and there are examples of systems that contribute to this vision [18, 3, 17]. Our work is similar in spirit to this body of literature. In fact, DIMs could become an important component of a sensor network database system such as TinyDB [17].
Our work departs from prior work in this area in two significant respects. Unlike these approaches, in our work the data generated at a node are hashed (in general) to different locations. This hashing is the key to scaling multi-dimensional range searches. In all the other systems described above, queries are flooded throughout the network, and can dominate the total cost of the system. Our work avoids query flooding by an appropriate choice of hashing. Madden et al. [17] also describe a distributed index, called Semantic Routing Trees (SRT). This index is used to direct queries to nodes that have detected relevant data. Our work differs from SRT in three key aspects. First, SRT is built on single attributes while DIM supports mulitple attributes.
Second, SRT constructs a routing tree based on historical sensor readings, and therefore works well only for slowlychanging sensor values. Finally, in SRT queries are issued from a fixed node while in DIM queries can be issued from any node.
A similar differentiation applies with respect to work on data-centric routing in sensor networks [15, 25], where data generated at a node is assumed to be stored at the node, and queries are either flooded throughout the network [15], or each source sets up a network-wide overlay announcing its presence so that mobile sinks can rendezvous with sources at the nearest node on the overlay [25]. These approaches work well for relatively long-lived queries.
Finally, our work is most close related to data-centric storage [23] systems, which include geographic hash-tables (GHTs) [20], DIMENSIONS [7], and DIFS [10].In a GHT, data is hashed by name to a location within the network, enabling highly efficient rendezvous. GHTs are built upon the GPSR [16] protocol and leverage some interesting properties of that protocol, such as the ability to route to a node nearest to a given location. We also leverage properties in GPSR (as we describe later), but we use a locality-preserving hash to store data, enabling efficient multi-dimensional range queries.
DIMENSIONS and DIFS can be thought of as using the same set of primitives as GHT (storage using consistent hashing), but for different ends: DIMENSIONS allows drill64 down search for features within a sensor network, while DIFS allows range queries on a single key in addition to other operations.
Most sensor networks are deployed to collect data from the environment. In these networks, nodes (either individually or collaboratively) will generate events. An event can generally be described as a tuple of attribute values,
A1, A2, · · · , Ak , where each attribute Ai represents a sensor reading, or some value corresponding to a detection (e.g., a confidence level). The focus of this paper is the design of systems to efficiently answer multi-dimensional range queries of the form: x1 − y1, x2 − y2, · · · , xk − yk . Such a query returns all events whose attribute values fall into the corresponding ranges. Notice that point queries, i.e., queries that ask for events with specified values for each attribute, are a special case of range queries.
As we have discussed in Section 1, range queries can enable efficient correlation and triggering within the network.
It is possible to implement range queries by flooding a query within the network. However, as we show in later sections, this alternative can be inefficient, particularly as the system scales, and if nodes within the network issue such queries relatively frequently. The other alternative, sending all events to an external storage node results in the access link being a bottleneck, especially if nodes within the network issue queries. Shenker et al. [23] also make similar arguments with respect to data-centric storage schemes in general; DIMs are an instance of such schemes.
The system we present in this paper, the DIM, relies upon two foundations: a locality-preserving geographic hash, and an underlying geographic routing scheme.
The key to resolving range queries efficiently is data locality: i.e., events with comparable attribute values are stored nearby. The basic insight underlying DIM is that data locality can be obtained by a locality-preserving geographic hash function. Our geographic hash function finds a localitypreserving mapping from the multi-dimensional space (described by the set of attributes) to a 2-d geographic space; this mapping is inspired by k-d trees [2] and is described later. Moreover, each node in the network self-organizes to claim part of the attribute space for itself (we say that each node owns a zone), so events falling into that space are routed to and stored at that node.
Having established the mapping, and the zone structure,
DIMs use a geographic routing algorithm previously developed in the literature to route events to their corresponding nodes, or to resolve queries. This algorithm, GPSR [16], essentially enables the delivery of a packet to a node at a specified location. The routing mechanism is simple: when a node receives a packet destined to a node at location X, it forwards the packet to the neighbor closest to X. In GPSR, this is called greedy-mode forwarding. When no such neighbor exists (as when there exists a void in the network), the node starts the packet on a perimeter mode traversal, using the well known right-hand rule to circumnavigate voids.
GPSR includes efficient techniques for perimeter traversal that are based on graph planarization algorithms amenable to distributed implementation.
For all of this to work, DIMs make two assumptions that are consistent with the literature [23]. First, all nodes know the approximate geographic boundaries of the network. These boundaries may either be configured in nodes at the time of deployment, or may be discovered using a simple protocol.
Second, each node knows its geographic location. Node locations can be automatically determined by a localization system or by other means.
Although the basic idea of DIMs may seem straightforward, it is challenging to design a completely distributed data structure that must be robust to packet losses and node failures, yet must support efficient query distribution and deal with communication voids and obstacles. We now describe the complete design of DIMs.
The key idea behind DIMs, as we have discussed, is a geographic locality-preserving hash that maps a multi-attribute event to a geographic zone. Intuitively, a zone is a subdivision of the geographic extent of a sensor field.
A zone is defined by the following constructive procedure.
Consider a rectangle R on the x-y plane. Intuitively, R is the bounding rectangle that contains all sensors withing the network. We call a sub-rectangle Z of R a zone, if Z is obtained by dividing R k times, k ≥ 0, using a procedure that satisfies the following property: After the i-th division, 0 ≤ i ≤ k, R is partitioned into 2i equal sized rectangles. If i is an odd (even) number, the i-th division is parallel to the y-axis (x-axis).
That is, the bounding rectangle R is first sub-divided into two zones at level 0 by a vertical line that splits R into two equal pieces, each of these sub-zones can be split into two zones at level 1 by a horizontal line, and so on. We call the non-negative integer k the level of zone Z, i.e. level(Z) = k.
A zone can be identified either by a zone code code(Z) or by an address addr(Z). The code code(Z) is a 0-1 bit string of length level(Z), and is defined as follows. If Z lies in the left half of R, the first (from the left) bit of code(Z) is 0, else 1. If Z lies in the bottom half of R, the second bit of code(Z) is 0, else 1. The remaining bits of code(Z) are then recursively defined on each of the four quadrants of R. This definition of the zone code matches the definition of zones given above, encoding divisions of the sensor field geography by bit strings. Thus, in Figure 2, the zone in the top-right corner of the rectangle R has a zone code of 1111.
Note that the zone codes collectively define a zone tree such that individual zones are at the leaves of this tree.
The address of a zone Z, addr(Z), is defined to be the centroid of the rectangle defined by Z. The two representations of a zone (its code and its address) can each be computed from the other, assuming the level of the zone is known.
Two zones are called sibling zones if their zone codes are the same except for the last bit. For example, if code(Z1) =
zones. The sibling subtree of a zone is the subtree rooted at the left or right sibling of the zone in the zone tree. We uniquely define a backup zone for each zone as follows: if the sibling subtree of the zone is on the left, the backup zone is the right-most zone in the sibling subtree; otherwise, the backup zone is the left-most zone in the sibling subtree. For a zone Z, let p be the first level(Z) − 1 digits of code(Z). Let backup(Z) be the backup zone of zone Z.
If code(Z) = p1, code(backup(Z)) = p01∗ with the most number of trailing 1"s (∗ means 0 or 1 occurrences). If 65 code(Z) = p0, code(backup(Z)) = p10∗ with the most number of trailing 0"s.
Our definition of a zone is independent of the actual distribution of nodes in the sensor field, and only depends upon the geographic extent (the bounding rectangle) of the sensor field. Now we describe how zones are mapped to nodes.
Conceptually, the sensor field is logically divided into zones and each zone is assigned to a single node. If the sensor network were deployed in a grid-like (i.e., very regular) fashion, then it is easy to see that there exists a k such that each node maps into a distinct level-k zone. In general, however, the node placements within a sensor field are likely to be less regular than the grid. For some k, some zones may be empty and other zones might have more than one node situated within them. One alternative would have been to choose a fixed k for the overall system, and then associate nodes with the zones they are in (and if a zone is empty, associate the nearest node with it, for some definition of nearest).
Because it makes our overall query routing system simpler, we allow nodes in a DIM to map to different-sized zones.
To precisely understand the associations between zones and nodes, we define the notion of zone ownership. For any given placement of network nodes, consider a node A. Let ZA to be the largest zone that includes only node A and no other node. Then, we say that A owns ZA. Notice that this definition of ownership may leave some sections of the sensor field un-associated with a node. For example, in Figure 2, the zone 110 does not contain any nodes and would not have an owner. To remedy this, for any empty zone Z, we define the owner to be the owner of backup(Z). In our example, that empty zone"s owner would also be the node that owns 1110, its backup zone.
Having defined the association between nodes and zones, the next problem we tackle is: given a node placement, does there exist a distributed algorithm that enables each node to determine which zones it owns, knowing only the overall boundary of the sensor network? In principle, this should be relatively straightforward, since each node can simply determine the location of its neighbors, and apply simple geometric methods to determine the largest zone around it such that no other node resides in that zone. In practice, however, communication voids and obstacles make the algorithm much more challenging. In particular, resolving the ownership of zones that do not contain any nodes is complicated. Equally complicated is the case where the zone of a node is larger than its communication radius and the node cannot determine the boundaries of its zone by local communication alone.
Our distributed zone building algorithm defers the resolution of such zones until when either a query is initiated, or when an event is inserted. The basic idea behind our algorithm is that each node tentatively builds up an idea of the zone it resides in just by communicating with its neighbors (remembering which boundaries of the zone are undecided because there is no radio neighbor that can help resolve that boundary). These undecided boundaries are later resolved by a GPSR perimeter traversal when data messages are actually routed.
We now describe the algorithm, and illustrate it using examples. In our algorithm, each node uses an array bound[0..3] to maintain the four boundaries of the zone it owns (rememFigure 1: A network, where circles represent sensor nodes and dashed lines mark the network boundary. 1111 011 00 110 100 101 1110 010 Figure 2: The zone code and boundaries.
10 10
10 00 Figure 3: The Corresponding Zone Tree ber that in this algorithm, the node only tries to determine the zone it resides in, not the other zones it might own because those zones are devoid of nodes). When a node starts up, each node initializes this array to be the network boundary, i.e., initially each node assumes its zone contains the whole network. The zone boundary algorithm now relies upon GPSR"s beacon messages to learn the locations of neighbors within radio range. Upon hearing of such a neighbor, the node calls the algorithm in Figure 4 to update its zone boundaries and its code accordingly. In this algorithm, we assume that A is the node at which the algorithm is executed, ZA is its zone, and a is a newly discovered neighbor of A. (Procedure Contain(ZA, a) is used to decide if node a is located within the current zone boundaries of node A).
Using this algorithm, then, each node can independently and asynchronously decide its own tentative zone based on the location of its neighbors. Figure 2 illustrates the results of applying this algorithm for the network in Figure 1.
Figure 3 describes the corresponding zone tree. Each zone resides at a leaf node and the code of a zone is the path from the root to the zone if we represent the branch to the left 66 Build-Zone(a)
Figure 4: Zone Boundary Determination, where A.x and A.y represent the geographic coordinate of node A.
Insert-Event(e)
Send-Message(c, m)
Figure 5: Inserting an event in a DIM. Procedure Closer(A, B, m) returns true if code(A) is closer to code(m) than code(B). source(m) is used to set the source address of message m. child by 0 and the branch to the right child by 1. This binary tree forms the index that we will use in the following event and query processing procedures.
We see that the zone sizes are different and depend on the local densities and so are the lengths of zone codes for different nodes. Notice that in Figure 2, there is an empty zone whose code should be 110. In this case, if the node in zone 1111 can only hear the node in zone 1110, it sets its boundary with the empty zone to undecided, because it did not hear from any neighboring nodes from that direction.
As we have mentioned before, the undecided boundaries are resolved using GPSR"s perimeter mode when an event is inserted, or a query sent. We describe event insertion in the next step.
Finally, this description does not describe how a node"s zone codes are adjusted when neighboring nodes fail, or new nodes come up. We return to this in Section 3.5.
In this section, we describe how events are inserted into a DIM. There are two algorithms of interest: a consistent hashing technique for mapping an event to a zone, and a routing algorithm for storing the event at the appropriate zone. As we shall see, these two algorithms are inter-related.
In Section 3.1, we described a recursive tessellation of the geographic extent of a sensor field. We now describe a consistent hashing scheme for a DIM that supports range queries on m distinct attributes2 Let us denote these attributes A1 . . . Am. For simplicity, assume for now that the depth of every zone in the network is k, k is a multiple of m, and that this value of k is known to every node. We will relax this assumption shortly.
Furthermore, for ease of discussion, we assume that all attribute values have been normalized to be between 0 and 1.
Our hashing scheme assigns a k bit zone code to an event as follows. For i between 1 and m, if Ai < 0.5, the i-th bit of the zone code is assigned 0, else 1. For i between m + 1 and 2m, if Ai−m < 0.25 or Ai−m ∈ [0.5, 0.75), the i-th bit of the zone is assigned 0, else 1, because the next level divisions are at 0.25 and 0.75 which divide the ranges to [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). We repeat this procedure until all k bits have been assigned. As an example, consider event E = 0.3, 0.8 . For this event, the 5-bit zone code is code(ZA) = 01110.
Essentially, our hashing scheme uses the values of the attributes in round-robin fashion on the zone tree (such as the one in Figure 3), in order to map an m-attribute event to a zone code. This is reminiscent of k-d trees [2], but is quite different from that data structure: zone trees are spatial embeddings and do not incorporate the re-balancing algorithms in k-d trees.
In our design of DIMs, we do not require nodes to have zone codes of the same length, nor do we expect a node to know the zone codes of other nodes. Rather, suppose the encoding node is A and its own zone code is of length kA.
Then, given an event E, node A only hashes E to a zone code of length kA. We denote the zone code assigned to an event E by code(E). As we describe below, as the event is routed, code(E) is refined by intermediate nodes. This lazy evaluation of zone codes allows different nodes to use different length zone codes without any explicit coordination.
The aim of hashing an event to a zone code is to store the event at the node within the network node that owns that zone. We call this node the owner of the event. Consider an event E that has just been generated at a node A. After encoding event E, node A compares code(E) with code(A).
If the two are identical, node A store event E locally; otherwise, node A will attempt to route the event to its owner.
To do this, note that code(E) corresponds to some zone Z , which is A"s current guess for the zone at which event E should be stored. A now invokes GPSR to send a message to addr(Z ) (the centroid of Z , Section 3.1). The message contains the event E, code(E), and the target geographic location for storing the event. In the message, A also marks itself as the owner of event E. As we will see later, the guessed zone Z , the address addr(Z ), and the owner of E, all of them contained in the message, will be refined by intermediate forwarding nodes.
GPSR now delivers this message to the next hop towards addr(Z ) from A. This next hop node (call it B) does not immediately forward the message. Rather, it attempts to com2 DIM does not assume that all nodes are homogeneous in terms of the sensors they have. Thus, in an m dimensional DIM, a node that does not possess all m sensors can use NULL values for the corresponding readings. DIM treats NULL as an extreme value for range comparisons. As an aside, a network may have many DIM instances running concurrently. 67 pute a new zone code for E to get a new code codenew(E).
B will update the code contained in the message (and also the geographic destination of the message) if codenew(E) is longer than the event code in the message. In this manner, as the event wends its way to its owner, its zone code gets refined. Now, B compares its own code code(B) against the owner code owner(E) contained in the incoming message.
If code(B) has a longer match with code(E) than the current owner owner(E), then B sets itself to be the current owner of E, meaning that if nobody is eligible to store E, then B will store the event (we shall see how this happens next). If B"s zone code does not exactly match code(E), B will invoke GPSR to deliver E to the next hop.
insertion Suppose that some node, say C, finds itself to be the destination (or eventual owner) of an event E. It does so by noticing that code code(C) equals code(E) after locally recomputing a code for E. In that case, C stores E locally, but only if all four of C"s zone boundaries are decided. When this condition holds, C knows for sure that no other nodes have overlapped zones with it. In this case, we call C an internal node.
Recall, though, that because the zone discovery algorithm Section 3.2 only uses information from immediate neighbors, one or more of C"s boundaries may be undecided. If so, C assumes that some other nodes have a zone that overlaps with its own, and sets out to resolve this overlap. To do this, C now sets itself to be the owner of E and continues forwarding the message. Here we rely on GPSR"s perimeter mode routing to probe around the void that causes the undecided boundary. Since the message starts from C and is destined for a geographic location near C, GPSR guarantees that the message will be delivered back to C if no other nodes will update the information in the message. If the message comes back to C with itself to be the owner, C infers that it must be the true owner of the zone and stores E locally.
If this does not happen, there are two possibilities. The first is that as the event traverses the perimeter, some intermediate node, say B whose zone overlaps with C"s marks itself to be the owner of the event, but otherwise does not change the event"s zone code. This node also recognizes that its own zone overlaps with C"s and initiates a message exchange which causes each of them to appropriately shrink their zone.
Figures 6 through 8 show an example of this data-driven zone shrinking. Initially, both node A and node B have claimed the same zone 0 because they are out of radio range of each other. Suppose that A inserts an event E = 0.4, 0.8, 0.9 .
A encodes E to 0 and claims itself to be the owner of E.
Since A is not an internal node, it sends out E, looking for other owner candidates of E. Once E gets to node B, B will see in the message"s owner field A"s code that is the same as its own. B then shrinks its zone from 0 to 01 according to A"s location which is also recorded in the message and send a shrink request to A. Upon receiving this request, A also shrinks its zone from 0 to 00.
A second possibility is if some intermediate node changes the destination code of E to a more specific value (i.e., longer zone code). Let us label this node D. D now tries to initiate delivery to the centroid of the new zone. This A B 0 0 110 100 1111 1110 101 Figure 6: Nodes A and B have claimed the same zone.
A B <0.4,0.8,0.9> Figure 7: An event/query message (filled arrows) triggers zone shrinking (hollow arrows).
A B 01 00 110 100 1111 1110 101 Figure 8: The zone layout after shrinking. Now node A and B have been mapped to different zones. might result in a new perimeter walk that returns to D (if, for example, D happens to be geographically closest to the centroid of the zone). However, D would not be the owner of the event, which would still be C. In routing to the centroid of this zone, the message may traverse the perimeter and return to D. Now D notices that C was the original owner, so it encapsulates the event and directs it to C. In case that there indeed is another node, say X, that owns an overlapped zone with C, X will notice this fact by finding in the message the same prefix of the code of one of its zones, but with a different geographic location from its own. X will shrink its zone to resolve the overlap. If X"s zone is smaller than or equal to C"s zone, X will also send a shrink request to C. Once C receives a shrink request, it will reduce its zone appropriately and fix its undecided boundary. In this manner, the zone formation process is resolved on demand in a data-driven way. 68 There are several interesting effects with respect to perimeter walking that arise in our algorithm. The first is that there are some cases where an event insertion might cause the entire outer perimeter of the network to be traversed3 .
Figure 6 also works as an example where the outer perimeter is traversed. Event E inserted by A will eventually be stored in node B. Before node B stores event E, if B"s nominal radio range does not intersect the network boundary, it needs to send out E again as A did, because B in this case is not an internal node. But if B"s nominal radio range intersects the network boundary, it then has two choices. It can assume that there will not be any nodes outside the network boundary and so B is an internal node. This is an aggressive approach. On the other hand, B can also make a conservative decision assuming that there might be some other nodes it have not heard of yet. B will then force the message walking another perimeter before storing it.
In some situations, especially for large zones where the node that owns a zone is far away from the centroid of the owned zone, there might exist a small perimeter around the destination that does not include the owner of the zone. The event will end up being stored at a different node than the real owner. In order to deal with this problem, we add an extra operation in event forwarding, called efficient neighbor discovery. Before invoking GPSR, a node needs to check if there exists a neighbor who is eligible to be the real owner of the event. To do this, a node C, say, needs to know the zone codes of its neighboring nodes. We deploy GPSR"s beaconing message to piggyback the zone codes for nodes. So by simply comparing the event"s code and neighbor"s code, a node can decide whether there exists a neighbor Y which is more likely to be the owner of event E. C delivers E to Y , which simply follows the decision making procedure discussed above.
In summary, our event insertion procedure is designed to nicely interact with the zone discovery mechanism, and the event hashing mechanism. The latter two mechanisms are kept simple, while the event insertion mechanism uses lazy evaluation at each hop to refine the event"s zone code, and it leverages GPSR"s perimeter walking mechanism to fix undecided zone boundaries. In Section 3.5, we address robustness of event insertion to packet loss or to node failures.
Figure 5 shows the pseudo-code for inserting and forwarding an event e. In this pseudo code, we have omitted a description of the zone shrinking procedure. In the pseudo code, procedure is Internal() is used to determine if the caller is an internal node and procedure is Owner() is used to determine if the caller is more eligible to be the owner of the event than is currently claimed owner as recorded in the message. Procedure Send-Message is used to send either an event message or a query message. If the message destination address has been changed, the packet source address needs also to be changed in order to avoid being dropped by GPSR, since GPSR does not allow a node to see the same packet in greedy mode twice. 3 This happens less frequently than for GHTs, where inserting an event to a location outside the actual (but inside the nominal) boundary of the network will always invoke an external perimeter walk.
DIMs support both point queries4 and range queries.
Routing a point query is identical to routing an event. Thus, the rest of this section details how range queries are routed.
The key challenge in routing zone queries is brought out by the following strawman design. If the entire network was divided evenly into zones of depth k (for some pre-defined constant k), then the querier (the node issuing the query) could subdivide a given range query into the relevant subzones and route individual requests to each of the zones.
This can be inefficient for large range queries and also hard to implement in our design where zone sizes are not predefined. Accordingly, we use a slightly different technique where a range query is initially routed to a zone corresponding to the entire range, and is then progressively split into smaller subqueries. We describe this algorithm here.
The first step of the algorithm is to map a range query to a zone code prefix. Conceptually, this is easy; in a zone tree (Figure 3), there exists some node which contains the entire range query in its sub-tree, and none of its children in the tree do. The initial zone code we choose for the query is the zone code corresponding to that tree node, and is a prefix of the zone codes of all zones (note that these zones may not be geographically contiguous) in the subtree. The querier computes the zone code of Q, denoted by code(Q) and then starts routing a query to addr(code(Q)).
Upon receiving a range query Q, a node A (where A is any node on the query propagation path) divides it into multiple smaller sized subqueries if there is an overlap between the zone of A, zone(A) and the zone code associated with Q, code(Q). Our approach to split a query Q into subqueries is as follows. If the range of Q"s first attribute contains the value 0.5, A divides Q into two sub-queries one of whose first attribute ranges from 0 to 0.5, and the other from 0.5 to
Let"s call it QA. If QA does not exist, then A stops splitting; otherwise, it continues splitting (using the second attribute range) and recomputing QA until QA is small enough so that it completely falls into zone(A) and hence A can now resolve it. For example, suppose that node A, whose code is 0110, is to split a range query Q = 0.3 − 0.8, 0.6 − 0.9 .
The splitting steps is shown in Figure 2. After splitting, we obtain three smaller queries q0 = 0.3 − 0.5, 0.6 − 0.75 , q1 = 0.3 − 0.5, 0.75 − 0.9 , and q2 = 0.5 − 0.8, 0.6 − 0.9 .
This splitting procedure is illustrated in Figure 9 which also shows the codes of each subquery after splitting.
A then replies to subquery q0 with data stored locally and sends subqueries q1 and q2 using the procedure outlined above. More generally, if node A finds itself to be inside the zone subtree that maximally covers Q, it will send the subqueries that resulted from the split. Otherwise, if there is no overlap between A and Q, then A forwards Q as is (in this case Q is either the original query, or a product of an earlier split).
Figure 10 describes the pseudo-code for the zone splitting algorithm. As shown in the above algorithm, once a subquery has been recognized as belonging to the caller"s zone, procedure Resolve is invoked to resolve the subquery and send a reply to the querier. Every query message contains 4 By point queries, we mean the equality condition on all indexed keys. DIM index attributes are not necessarily primary keys. 69 the geographic location of its initiator, so the corresponding reply message can be delivered directly back to the initiator. Finally, in the process of query resolution, zones might shrink similar to shrinkage during inserting. We omit this in the pseudo code.
Until now, we have not discussed the impact of node failures and packet losses, or node arrivals and departures on our algorithms. Packet losses can affect query and event insertion, and node failures can result in lost data, while node arrivals and departures can impact the zone structure. We now discuss how DIMs can be made robust to these kinds of dynamics.
In previous sections, we described how the zone discovery algorithm could leave zone boundaries undecided. These undecided boundaries are resolved during insertion or querying, using the zone shrinking procedure describe above.
When a new node joins the network, the zone discovery mechanism (Section 3.2) will cause neighboring zones to appropriately adjust their zone boundaries. At this time, those zones can also transfer to the new node those events they store but which should belong to the new node.
Before a node turns itself off (if this is indeed possible), it knows that its backup node (Section 3.1) will take over its zone, and will simply send all its events to its backup node.
Node deletion may also cause zone expansion. In order to keep the mapping between the binary zone tree"s leaf nodes and zones, we allow zone expansion to only occur among sibling zones (Section 3.1). The rule is: if zone(A)"s sibling zone becomes empty, then A can expand its own zone to include its sibling zone.
Now, we turn our attention to node failures. Node failures are just like node deletions except that a failed node does not have a chance to move its events to another node. But how does a node decide if its sibling has failed? If the sibling is within radio range, the absence of GPSR beaconing messages can detect this. Once it detects this, the node can expand its zone. A different approach is needed for detecting siblings who are not within radio range. These are the cases where two nodes own their zones after exchanging a shrink message; they do not periodically exchange messages thereafter to maintain this zone relationship. In this case, we detect the failure in a data-driven fashion, with obvious efficiency benefits compared to periodic keepalives. Once a node B has failed, an event or query message that previously should have been owned by the failed node will now be delivered to the node A that owns the empty zone left by node B. A can see this message because A stands right around the empty area left by B and is guaranteed to be visited in a GPSR perimeter traversal. A will set itself to be the owner of the message, and any node which would have dropped this message due to a perimeter loop will redirect the message to A instead. If A"s zone happens to be the sibling of B"s zone,
A can safely expand its own zone and notify its expanded zone to its neighbors via GPSR beaconing messages.
The algorithms described above are robust in terms of zone formation, but node failure can erase data. To avoid this, DIMs can employ two kinds of replication: local replication to be resilient to random node failures, and mirror replication for resilience to concurrent failure of geographically contiguous nodes.
Mirror replication is conceptually easy. Suppose an event E has a zone code code(E). Then, the node that inserts E would store two copies of E; one at the zone denoted by code(E), and the other at the zone corresponding to the one"s complement of code(E). This technique essentially creates a mirror DIM. A querier would need, in parallel, to query both the original DIM and its mirror since there is no way of knowing if a collection of nodes has failed. Clearly, the trade-off here is an approximate doubling of both insertion and query costs.
There exists a far cheaper technique to ensure resilience to random node failures. Our local replication technique rests on the observation that, for each node A, there exists a unique node which will take over its zone when A fails.
This node is defined as the node responsible for A"s zone"s backup zone (see Section 3.1). The basic idea is that A replicates each data item it has in this node. We call this node A"s local replica. Let A"s local replica be B. Often B will be a radio neighbor of A and can be detected from GPSR beacons. Sometimes, however, this is not the case, and B will have to be explicitly discovered.
We use an explicit message for discovering the local replica.
Discovering the local replica is data-driven, and uses a mechanism similar to that of event insertion. Node A sends a message whose geographic destination is a random nearby location chosen by A. The location is close enough to A such that GPSR will guarantee that the message will delivered back to A. In addition, the message has three fields, one for the zone code of A, code(A), one for the owner owner(A) of zone(A) which is set to be empty, and one for the geographic location of owner(A). Then the packet will be delivered in GPSR perimeter mode. Each node that receives this message will compare its zone code and code(A) in the message, and if it is more eligible to be the owner of zone(A) than the current owner(A) recorded in the message, it will update the field owner(A) and the corresponding geographic location. Once the packet comes back to A, it will know the location of its local replica and can start to send replicas.
In a dense sensor network, the local replica of a node is usually very near to the node, either its direct neighbor or 1-2 hops away, so the cost of sending replicas to local replication will not dominate the network traffic. However, a node"s local replica itself may fail. There are two ways to deal with this situation; periodic refreshes, or repeated datadriven discovery of local replicas. The former has higher overhead, but more quickly discovers failed replicas.
Finally, the mechanisms for querying and event insertion can be easily made resilient to packet loss. For event insertion, a simple ACK scheme suffices.
Of course, queries and responses can be lost as well. In this case, there exists an efficient approach for error recovery. This rests on the observation that the querier knows which zones fall within its query and should have responded (we assume that a node that has no data matching a query, but whose zone falls within the query, responds with a negative acknowledgment). After a conservative timeout, the querier can re-issue the queries selectively to these zones.
If DIM cannot get any answers (positive or negative) from 70 <0.3-0.8, 0.6-0.9> <0.5-0.8, 0.6-0.9><0.3-0.5, 0.6-0.9> <0.3-0.5, 0.6-0.9> <0.3-0.5, 0.6-0.9> <0.3-0.5, 0.6-0.75> <0.3-0.5, 0.75-0.9> 0 0 1 1 1 1 Figure 9: An example of range query splitting Resolve-Range-Query(Q)
Figure 10: Query resolving algorithm certain zones after repeated timeouts, it can at least return the partial query results to the application together with the information about the zones from which data is missing.
In this section, we present a simple analytic performance evaluation of DIMs, and compare their performance against other possible approaches for implementing multi-dimensional range queries in sensor networks. In the next section, we validate these analyses using detailed packet-level simulations.
Our primary metrics for the performance of a DIM are: Average Insertion Cost measures the average number of messages required to insert an event into the network.
Average Query Delivery Cost measures the average number of messages required to route a query message to all the relevant nodes in the network.
It does not measure the number of messages required to transmit responses to the querier; this latter number depends upon the precise data distribution and is the same for many of the schemes we compare DIMs against.
In DIMs, event insertion essentially uses geographic routing. In a dense N-node network where the likelihood of traversing perimeters is small, the average event insertion cost proportional to √ N [23].
On the other hand, the query delivery cost depends upon the size of ranges specified in the query. Recall that our query delivery mechanism is careful about splitting a query into sub-queries, doing so only when the query nears the zone that covers the query range. Thus, when the querier is far from the queried zone, there are two components to the query delivery cost. The first, which is proportional to √ N, is the cost to deliver the query near the covering zone. If within this covering zone, there are M nodes, the message delivery cost of splitting the query is proportional to M.
The average cost of query delivery depends upon the distribution of query range sizes. Now, suppose that query sizes follow some density function f(x), then the average cost of resolve a query can be approximated by Ê N 1 xf(x)dx. To give some intuition for the performance of DIMs, we consider four different forms for f(x): the uniform distribution where a query range encompassing the entire network is as likely as a point query; a bounded uniform distribution where all sizes up to a bound B are equally likely; an algebraic distribution in which most queries are small, but large queries are somewhat likely; and an exponential distribution where most queries are small and large queries are unlikely. In all our analyses, we make the simplifying assumption that the size of a query is proportional to the number of nodes that can answer that query.
For the uniform distribution P(x) ∝ c for some constant c.
If each query size from 1 . . . N is equally likely, the average query delivery cost of uniformly distributed queries is O(N).
Thus, for uniformly distributed queries, the performance of DIMs is comparable to that of flooding. However, for the applications we envision, where nodes within the network are trying to correlate events, the uniform distribution is highly unrealistic.
Somewhat more realistic is a situation where all query sizes are bounded by a constant B. In this case, the average cost for resolving such a query is approximately Ê B 1 xf(x)dx = O(B). Recall now that all queries have to pay an approximate cost of O( √ N) to deliver the query near the covering zone. Thus, if DIM limited queries to a size proportional to√ N, the average query cost would be O( √ N).
The algebraic distribution, where f(x) ∝ x−k , for some constant k between 1 and 2, has an average query resolution cost given by Ê N 1 xf(x)dx = O(N2−k ). In this case, if k >
cost to deliver the query to near the covering zone, given by O( √ N).
Finally, for the exponential distribution, f(x) = ce−cx for some constant c, and the average cost is just the mean of the corresponding distribution, i.e., O(1) for large N.
Asymptotically, then, the cost of the query for the exponential distribution is dominated by the cost to deliver the query near the covering zone (O( √ N)).
Thus, we see that if queries follow either the bounded uniform distribution, the algebraic distribution, or the exponential distribution, the query cost scales as the insertion cost (for appropriate choice of constants for the bounded uniform and the algebraic distributions).
How well does the performance of DIMs compare against alternative choices for implementing multi-dimensional queries?
A simple alternative is called external storage [23], where all events are stored centrally in a node outside the sensor network. This scheme incurs an insertion cost of O( √ N), and a zero query cost. However, as [23] points out, such systems may be impractical in sensor networks since the access link to the external node becomes a hotspot.
A second alternative implementation would store events at the node where they are generated. Queries are flooded 71 throughout the network, and nodes that have matching data respond. Examples of systems that can be used for this (although, to our knowledge, these systems do not implement multi-dimensional range queries) are Directed Diffusion [15] and TinyDB [17]. The flooding scheme incurs a zero insertion cost, but an O(N) query cost. It is easy to show that DIMs outperform flooding as long as the ratio of the number of insertions to the number of queries is less than √ N.
A final alternative would be to use a geographic hash table (GHT [20]). In this approach, attribute values are assumed to be integers (this is actually quite a reasonable assumption since attribute values are often quantized), and events are hashed on some (say, the first) attribute. A range query is sub-divided into several sub-queries, one for each integer in the range of the first attribute. Each sub-query is then hashed to the appropriate location. The nodes that receive a sub-query only return events that match all other attribute ranges. In this approach, which we call GHT-R (GHT"s for range queries) the insertion cost is O( √ N). Suppose that the range of the first attribute contains r discrete values.
Then the cost to deliver queries is O(r √ N). Thus, asymptotically, GHT-R"s perform similarly to DIMs. In practice, however, the proportionality constants are significantly different, and DIMs outperform GHT-Rs, as we shall show using detailed simulations.
Our analysis gives us some insight into the asymptotic behavior of various approaches for multi-dimensional range queries. In this section, we use simulation to compare DIMs against flooding and GHT-R; this comparison gives us a more detailed understanding of these approaches for moderate size networks, and gives us a nuanced view of the mechanistic differences between some of these approaches.
We use ns-2 for our simulations. Since DIMs are implemented on top of GPSR, we first ported an earlier GPSR implementation to the latest version of ns-2. We modified the GPSR module to call our DIM implementation when it receives any data message in transit or when it is about to drop a message because that message traversed the entire perimeter. This allows a DIM to modify message zone codes in flight (Section 3), and determine the actual owner of an event or query.
In addition, to this, we implemented in ns-2 most of the DIM mechanisms described in Section 3. Of those mechanisms, the only one we did not implement is mirror replication. We have implemented selective query retransmission for resiliency to packet loss, but have left the evaluation of this mechanism to future work. Our DIM implementation in ns-2 is 2800 lines of code.
Finally, we implemented GHT-R, our GHT-based multidimensional range query mechanism in ns-2. This implementation was relatively straightforward, given that we had ported GPSR, and modified GPSR to detect the completion of perimeter mode traversals.
Using this implementation, we conducted a fairly extensive evaluation of DIM and two alternatives (flooding, and our GHT-R). For all our experiments, we use uniformly placed sensor nodes with network sizes ranging from 50 nodes to 300 nodes. Each node has a radio range of 40m.
For the results presented here, each node has on average 20 nodes within its nominal radio range. We have conducted experiments at other node densities; they are in agreement with the results presented here.
In all our experiments, each node first generates 3 events5 on average (more precisely, for a topology of size N, we have 3N events, and each node is equally likely to generate an event). We have conducted experiments for three different event value distributions. Our uniform event distribution generates 2-dimensional events and, for each dimension, every attribute value is equally likely. Our normal event distribution generates 2-dimensional events and, for each dimension, the attribute value is normally distributed with a mean corresponding to the mid-point of the attribute value range. The normal event distribution represents a skewed data set. Finally, our trace event distribution is a collection of 4-dimensional events obtained from a habitat monitoring network. As we shall see, this represents a fairly skewed data set.
Having generated events, for each simulation we generate queries such that, on average, each node generates 2 queries. The query sizes are determined using the four size distributions we discussed in Section 4: uniform, boundeduniform, algebraic and exponential. Once a query size has been determined, the location of the query (i.e., the actual boundaries of the zone) are uniformly distributed. For our GHT-R experiments, the dynamic range of the attributes had 100 discrete values, but we restricted the query range for any one attribute to 50 discrete values to allow those simulations to complete in reasonable time.
Finally, using one set of simulations we evaluate the efficacy of local replication by turning off random fractions of nodes and measuring the fidelity of the returned results.
The primary metrics for our simulations are the average query and insertion costs, as defined in Section 4.
Although we have examined almost all the combinations of factors described above, we discuss only the most salient ones here, for lack of space.
Figure 11 plots the average insertion costs for DIM and GHT-R (for flooding, of course, the insertion costs are zero).
DIM incurs less per event overhead in inserting events (regardless of the actual event distribution; Figure 11 shows the cost for uniformly distributed events). The reason for this is interesting. In GHT-R, storing almost every event incurs a perimeter traversal, and storing some events require traversing the outer perimeter of the network [20]. By contrast, in DIM, storing an event incurs a perimeter traversal only when a node"s boundaries are undecided. Furthermore, an insertion or a query in a DIM can traverse the outer perimeter (Section 3.3), but less frequently than in GHTs.
Figure 13 plots the average query cost for a bounded uniform query size distribution. For this graph (and the next) we use a uniform event distribution, since the event distribution does not affect the query delivery cost. For this simulation, our bound was 1 4 th the size of the largest possible 5 Our metrics are chosen so that the exact number of events and queries is unimportant for our discussion. Of course, the overall performance of the system will depend on the relative frequency of events and queries, as we discuss in Section 4. Since we don"t have realistic ratios for these, we focus on the microscopic costs, rather than on the overall system costs. 72 0 2 4 6 8 10 12 14 16 18 20
AverageCostperInsertion Network Size DIM GHT-R Figure 11: Average insertion cost for DIM and GHT.
1
Fractionofrepliescomparedwithnon-failurecase Fraction of failed nodes (%) No Replication Local Replication Figure 12: Local replication performance. query (e.g., a query of the form 0 − 0.5, 0 − 0.5 . Even for this generous query size, DIMs perform quite well (almost a third the cost of flooding). Notice, however, that GHTRs incur high query cost since almost any query requires as many subqueries as the width of the first attribute"s range.
Figure 14 plots the average query cost for the exponential distribution (the average query size for this distribution was set to be 1 16 th the largest possible query). The superior scaling of DIMs is evident in these graphs. Clearly, this is the regime in which one might expect DIMs to perform best, when most of the queries are small and large queries are relatively rare. This is also the regime in which one would expect to use multi-dimensional range queries: to perform relatively tight correlations. As with the bounded uniform distribution, GHT query cost is dominated by the cost of sending sub-queries; for DIMs, the query splitting strategy works quite well in keep overall query delivery costs low.
Figure 12 describes the efficacy of local replication. To obtain this figure, we conducted the following experiment.
On a 100-node network, we inserted a number of events uniformly distributed throughout the network, then issued a query covering the entire network and recorded the answers. Knowing the expected answers for this query, we then successively removed a fraction f of nodes randomly, and re-issued the same query. The figure plots the fraction of expected responses actually received, with and without replication. As the graph shows, local replication performs well for random failures, returning almost 90% of the responses when up to 30% of the nodes have failed simultaneously 6 .In the absence of local replication, of course, when 6 In practice, the performance of local replication is likely to 0 100 200 300 400 500 600 700
AverageCostperQueryinBoundedUnifDistribution Network Size DIM flooding GHT-R Figure 13: Average query cost with a bounded uniform query distribution 0 50 100 150 200 250 300 350 400 450
AverageCostperQueryinExponentialDistribution Network Size DIM flooding GHT-R Figure 14: Average query cost with an exponential query distribution 30% of the nodes fail, the response rate is only 70% as one would expect.
We note that DIMs (as currently designed) are not perfect. When the data is highly skewed-as it was for our trace data set from the habitat monitoring application where most of the event values fell into within 10% of the attribute"s range-a few DIM nodes will clearly become the bottleneck.
This is depicted in Figure 15, which shows that for DIMs, and GHT-Rs, the maximum number of transmissions at any network node (the hotspots) is rather high. (For less skewed data distributions, and reasonable query size distributions, the hotspot curves for all three schemes are comparable.) This is a standard problem that the database indices have dealt with by tree re-balancing. In our case, simpler solutions might be possible (and we discuss this in Section 7).
However, our use of the trace data demonstrates that DIMs work for events which have more than two dimensions.
Increasing the number of dimensions does not noticeably degrade DIMs query cost (omitted for lack of space).
Also omitted are experiments examining the impact of several other factors, as they do not affect our conclusions in any way. As we expected, DIMs are comparable in performance to flooding when all sizes of queries are equally likely. For an algebraic distribution of query sizes, the relative performance is close to that for the exponential distribution. For normally distributed events, the insertion costs be much better than this. Assuming a node and its replica don"t simultaneously fail often, a node will almost always detect a replica failure and re-replicate, leading to near 100% response rates. 73 0 2000 4000 6000 8000 10000 12000
MaximumHotspotonTraceDataSet Network Size DIM flooding GHT-R Figure 15: Hotspot usage DIM Zone Manager Query Router Query Processor Event Manager Event Router GPSR interface(Event driven/Thread based) update useuse update GPSR Upper interface(Event driven/Thread based) Lower interface(Event driven/Thread based) Greedy Forwarding Perimeter Forwarding Beaconing Neighbor List Manager update use MoteNIC (MicaRadio) IP Socket (802.11b/Ethernet) Figure 16: Software architecture of DIM over GPSR are comparable to that for the uniform distribution.
Finally, we note that in all our evaluations we have only used list queries (those that request all events matching the specified range). We expect that for summary queries (those that expect an aggregate over matching events), the overall cost of DIMs could be lower because the matching data are likely to be found in one or a small number of zones. We leave an understanding of this to future work. Also left to future work is a detailed understanding of the impact of location error on DIM"s mechanisms. Recent work [22] has examined the impact of imprecise location information on other data-centric storage mechanisms such as GHTs, and found that there exist relatively simple fixes to GPSR that ameliorate the effects of location error.
We have implemented DIMs on a Linux platform suitable for experimentation on PDAs and PC-104 class machines.
To implement DIMs, we had to develop and test an independent implementation of GPSR. Our GPSR implementation is full-featured, while our DIM implementation has most of the algorithms discussed in Section 3; some of the robustness extensions have only simpler variants implemented.
The software architecture of DIM/GPSR system is shown in Figure 16. The entire system (about 5000 lines of code) is event-driven and multi-threaded. The DIM subsystem consists of six logical components: zone management, event maintenance, event routing, query routing, query processing, and GPSR interactions. The GPSR system is implemented as user-level daemon process. Applications are executed as clients. For the DIM subsystem, the GPSR module 0 2 4 6 8 10 12 14 16
Query size Average#ofreceivedresponses perquery Figure 17: Number of events received for different query sizes 0 2 4 6 8 10 12 14 16
Query sizeTotalnumberofmessages onlyforsendingthequery Figure 18: Query distribution cost provides several extensions: it exports information about neighbors, and provides callbacks during packet forwarding and perimeter-mode termination.
We tested our implementation on a testbed consisting of 8 PC-104 class machines. Each of these boxes runs Linux and uses a Mica mote (attached through a serial cable) for communication. These boxes are laid out in an office building with a total spatial separation of over a hundred feet. We manually measured the locations of these nodes relative to some coordinate system and configured the nodes with their location. The network topology is approximately a chain.
On this testbed, we inserted queries and events from a single designated node. Our events have two attributes which span all combinations of the four values [0, 0.25, 0.75, 1] (sixteen events in all). Our queries span four sizes, returning 1, 4, 9 and 16 events respectively.
Figure 17 plots the number of events received for different sized queries. It might appear that we received fewer events than expected, but this graph doesn"t count the events that were already stored at the querier. With that adjustment, the number of responses matches our expectation. Finally,
Figure 18 shows the total number of messages required for different query sizes on our testbed.
While these experiments do not reveal as much about the performance range of DIMs as our simulations do, they nevertheless serve as proof-of-concept for DIMs. Our next step in the implementation is to port DIMs to the Mica motes, and integrate them into the TinyDB [17] sensor database engine on motes. 74
In this paper, we have discussed the design and evaluation of a distributed data structure called DIM for efficiently resolving multi-dimensional range queries in sensor networks.
Our design of DIMs relies upon a novel locality-preserving hash inspired by early work in database indexing, and is built upon GPSR. We have a working prototype, both of GPSR and DIM, and plan to conduct larger scale experiments in the future.
There are several interesting future directions that we intend to pursue. One is adaptation to skewed data distributions, since these can cause storage and transmission hotspots. Unlike traditional database indices that re-balance trees upon data insertion, in sensor networks it might be feasible to re-structure the zones on a much larger timescale after obtaining a rough global estimate of the data distribution. Another direction is support for node heterogeneity in the zone construction process; nodes with larger storage space assert larger-sized zones for themselves. A third is support for efficient resolution of existential queries-whether there exists an event matching a multi-dimensional range.
Acknowledgments This work benefited greatly from discussions with Fang Bian,
Hui Zhang and other ENL lab members, as well as from comments provided by the reviewers and our shepherd Feng Zhao.
[1] J. Aspnes and G. Shah. Skip Graphs. In Proceedings of 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Baltimore, MD, January 2003. [2] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communicaions of the ACM, 18(9):475-484, 1975. [3] P. Bonnet, J. E. Gerhke, and P. Seshadri. Towards Sensor Database Systems. In Proceedings of the Second International Conference on Mobile Data Management,
Hong Kong, January 2001. [4] I. Clarke, O. Sandberg, B. Wiley, and T. W. Hong. Freenet: A Distributed Anonymous Information Storage and Retrieval System. In Designing Privacy Enhancing Technologies: International Workshop on Design Issues in Anonymity and Unobservability. Springer, New York, 2001. [5] D. Comer. The Ubiquitous B-tree. ACM Computing Surveys, 11(2):121-137, 1979. [6] R. A. Finkel and J. L. Bentley. Quad Trees: A Data Structure for Retrieval on Composite Keys. Acta Informatica, 4:1-9, 1974. [7] D. Ganesan, D. Estrin, and J. Heidemann. DIMENSIONS: Why do we need a new Data Handling architecture for Sensor Networks? In Proceedings of the First Workshop on Hot Topics In Networks (HotNets-I), Princeton, NJ,
October 2002. [8] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in High Dimensions via Hashing. In Proceedings of the 25th VLDB conference, Edinburgh, Scotland, September 1999. [9] R. Govindan, J. Hellerstein, W. Hong, S. Madden,
M. Franklin, and S. Shenker. The Sensor Network as a Database. Technical Report 02-771, Computer Science Department, University of Southern California, September
[10] B. Greenstein, D. Estrin, R. Govindan, S. Ratnasamy, and S. Shenker. DIFS: A Distributed Index for Features in Sensor Networks. In Proceedings of 1st IEEE International Workshop on Sensor Network Protocols and Applications,
Anchorage, AK, May 2003. [11] A. Guttman. R-trees: A Dynamic Index Structure for Spatial Searching. In Proceedings of the ACM SIGMOD,
Boston, MA, June 1984. [12] M. Harren, J. M. Hellerstein, R. Huebsch, B. T. Loo,
S. Shenker, and I. Stoica. Complex Queries in DHT-based Peer-to-Peer Networks. In P. Druschel, F. Kaashoek, and A. Rowstron, editors, Proceedings of 1st International Workshop on Peer-to-Peer Systems (IPTPS"02), volume
Springer-Verlag. [13] P. Indyk and R. Motwani. Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In Proceedings of the 30th Annual ACM Symposium on Theory of Computing, Dallas, Texas, May 1998. [14] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala.
Locality-preserving Hashing in Multidimensional Spaces. In Proceedings of the 29th Annual ACM symposium on Theory of Computing, pages 618 - 625, El Paso, Texas,
May 1997. ACM Press. [15] C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed Diffusion: A Scalable and Robust Communication Paradigm for Sensor Networks. In Proceedings of the Sixth Annual ACM/IEEE International Conference on Mobile Computing and Networking (Mobicom 2000), Boston, MA,
August 2000. [16] B. Karp and H. T. Kung. GPSR: Greedy Perimeter Stateless Routing for Wireless Networks. In Proceedings of the Sixth Annual ACM/IEEE International Conference on Mobile Computing and Networking (Mobicom 2000),
Boston, MA, August 2000. [17] S. Madden, M. Franklin, J. Hellerstein, and W. Hong. The Design of an Acquisitional Query Processor for Sensor Networks. In Proceedings of ACM SIGCMOD, San Diego,
CA, June 2003. [18] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong.
TAG: a Tiny AGregation Service for ad-hoc Sensor Networks. In Proceedings of 5th Annual Symposium on Operating Systems Design and Implementation (OSDI),
Boston, MA, December 2002. [19] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A Scalable Content-Addressable Network. In Proceedings of the ACM SIGCOMM, San Diego, CA,
August 2001. [20] S. Ratnasamy, B. Karp, L. Yin, F. Yu, D. Estrin,
R. Govindan, and S. Shenker. GHT: A Geographic Hash Table for Data-Centric Storage. In Proceedings of the First ACM International Workshop on Wireless Sensor Networks and Applications, Atlanta, GA, September 2002. [21] H. Samet. Spatial Data Structures. In W. Kim, editor,
Modern Database Systems: The Object Model,
Interoperability and Beyond, pages 361-385. Addison Wesley/ACM, 1995. [22] K. Sead, A. Helmy, and R. Govindan. On the Effect of Localization Errors on Geographic Face Routing in Sensor Networks. In Under submission, 2003. [23] S. Shenker, S. Ratnasamy, B. Karp, R. Govindan, and D. Estrin. Data-Centric Storage in Sensornets. In Proc.
ACM SIGCOMM Workshop on Hot Topics In Networks,

Mobile opportunistic networks are one kind of delay-tolerant network (DTN) [6]. Delay-tolerant networks provide service despite long link delays or frequent link breaks. Long link delays happen in networks with communication between nodes at a great distance, such as interplanetary networks [2]. Link breaks are caused by nodes moving out of range, environmental changes, interference from other moving objects, radio power-offs, or failed nodes. For us, mobile opportunistic networks are those DTNs with sparse node population and frequent link breaks caused by power-offs and the mobility of the nodes.
Mobile opportunistic networks have received increasing interest from researchers. In the literature, these networks include mobile sensor networks [25], wild-animal tracking networks [11], pocketswitched networks [8], and transportation networks [1, 14]. We expect to see more opportunistic networks when the one-laptopper-child (OLPC) project [18] starts rolling out inexpensive laptops with wireless networking capability for children in developing countries, where often no infrastructure exits. Opportunistic networking is one promising approach for those children to exchange information.
One fundamental problem in opportunistic networks is how to route messages from their source to their destination. Mobile opportunistic networks differ from the Internet in that disconnections are the norm instead of the exception. In mobile opportunistic networks, communication devices can be carried by people [4], vehicles [1] or animals [11]. Some devices can form a small mobile ad-hoc network when the nodes move close to each other. But a node may frequently be isolated from other nodes. Note that traditional Internet routing protocols and ad-hoc routing protocols, such as AODV [20] or DSDV [19], assume that a contemporaneous endto-end path exists, and thus fail in mobile opportunistic networks.
Indeed, there may never exist an end-to-end path between two given devices.
In this paper, we study protocols for routing messages between wireless networking devices carried by people. We assume that people send messages to other people occasionally, using their devices; when no direct link exists between the source and the destination of the message, other nodes may relay the message to the destination. Each device represents a unique person (it is out of the scope of this paper when a device maybe carried by multiple people). Each message is destined for a specific person and thus for a specific node carried by that person. Although one person may carry multiple devices, we assume that the sender knows which device is the best to receive the message. We do not consider multicast or geocast in this paper.
Many routing protocols have been proposed in the literature.
Few of them were evaluated in realistic network settings, or even in realistic simulations, due to the lack of any realistic people mobility model. Random walk or random way-point mobility models are often used to evaluate the performance of those routing protocols.
Although these synthetic mobility models have received extensive interest by mobile ad-hoc network researchers [3], they do not reflect people"s mobility patterns [9]. Realising the limitations of using random mobility models in simulations, a few researchers have studied routing protocols in mobile opportunistic networks with realistic mobility traces. Chaintreau et al. [5] theoretically analyzed the impact of routing algorithms over a model derived from a realistic mobility data set. Su et al. [22] simulated a set of routing 35 protocols in a small experimental network. Those studies help researchers better understand the theoretical limits of opportunistic networks, and the routing protocol performance in a small network (20-30 nodes).
Deploying and experimenting large-scale mobile opportunistic networks is difficult, we too resort to simulation. Instead of using a complex mobility model to mimic people"s mobility patterns, we used mobility traces collected in a production wireless network at Dartmouth College to drive our simulation. Our messagegeneration model, however, was synthetic.
To the best of our knowledge, we are the first to simulate the effect of routing protocols in a large-scale mobile opportunistic network, using realistic contact traces derived from real traces of a production network with more than 5, 000 users.
Using realistic contact traces, we evaluate the performance of three naive routing protocols (direct-delivery, epidemic, and random) and two prediction-based routing protocols, PRoPHET [16] and Link-State [22]. We also propose a new prediction-based routing protocol, and compare it to the above in our evaluation.
A routing protocol is designed for forwarding messages from one node (source) to another node (destination). Any node may generate messages for any other node, and may carry messages destined for other nodes. In this paper, we consider only messages that are unicast (single destination).
DTN routing protocols could be described in part by their transfer probability and replication probability; that is, when one node meets another node, what is the probability that a message should be transfered and if so, whether the sender should retain its copy.
Two extremes are the direct-delivery protocol and the epidemic protocol. The former transfers with probability 1 when the node meets the destination, 0 for others, and no replication. The latter uses transfer probability 1 for all nodes and unlimited replication.
Both these protocols have their advantages and disadvantages. All other protocols are between the two extremes.
First, we define the notion of contact between two nodes. Then we describe five existing protocols before presenting our own proposal.
A contact is defined as a period of time during which two nodes have the opportunity to communicate. Although we are aware that wireless technologies differ, we assume that a node can reliably detect the beginning and end time of a contact with nearby nodes.
A node may be in contact with several other nodes at the same time.
The contact history of a node is a sequence of contacts with other nodes. Node i has a contact history Hi(j), for each other node j, which denotes the historical contacts between node i and node j.
We record the start and end time for each contact; however, the last contacts in the node"s contact history may not have ended.
In this simple protocol, a message is transmitted only when the source node can directly communicate with the destination node of the message. In mobile opportunistic networks, however, the probability for the sender to meet the destination may be low, or even zero.
The epidemic routing protocol [23] floods messages into the network. The source node sends a copy of the message to every node that it meets. The nodes that receive a copy of the message also send a copy of the message to every node that they meet.
Eventually, a copy of the message arrives at the destination of the message.
This protocol is simple, but may use significant resources; excessive communication may drain each node"s battery quickly.
Moreover, since each node keeps a copy of each message, storage is not used efficiently, and the capacity of the network is limited.
At a minimum, each node must expire messages after some amount of time or stop forwarding them after a certain number of hops.
After a message expires, the message will not be transmitted and will be deleted from the storage of any node that holds the message.
An optimization to reduce the communication cost is to transfer index messages before transferring any data message. The index messages contain IDs of messages that a node currently holds.
Thus, by examining the index messages, a node only transfers messages that are not yet contained on the other nodes.
An obvious approach between the above two extremes is to select a transfer probability between 0 and 1 to forward messages at each contact. We use a simple replication strategy that allows only the source node to make replicas, and limits the replication to a specific number of copies. The message has some chance of being transferred to a highly mobile node, and thus may have a better chance to reach its destination before the message expires.
PRoPHET [16] is a Probabilistic Routing Protocol using History of past Encounters and Transitivity to estimate each node"s delivery probability for each other node. When node i meets node j, the delivery probability of node i for j is updated by pij = (1 − pij)p0 + pij, (1) where p0 is an initial probability, a design parameter for a given network. Lindgren et al. [16] chose 0.75, as did we in our evaluation. When node i does not meet j for some time, the delivery probability decreases by pij = αk pij, (2) where α is the aging factor (α < 1), and k is the number of time units since the last update.
The PRoPHET protocol exchanges index messages as well as delivery probabilities. When node i receives node j"s delivery probabilities, node i may compute the transitive delivery probability through j to z with piz = piz + (1 − piz)pijpjzβ, (3) where β is a design parameter for the impact of transitivity; we used β = 0.25 as did Lindgren [16].
Su et al. [22] use a link-state approach to estimate the weight of each path from the source of a message to the destination. They use the median inter-contact duration or exponentially aged intercontact duration as the weight on links. The exponentially aged inter-contact duration of node i and j is computed by wij = αwij + (1 − α)I, (4) where I is the new inter-contact duration and α is the aging factor.
Nodes share their link-state weights when they can communicate with each other, and messages are forwarded to the node that have the path with the lowest link-state weight. 36
We also use historical contact information to estimate the probability of meeting other nodes in the future. But our method differs in that we estimate the contact probability within a period of time.
For example, what is the contact probability in the next hour?
Neither PRoPHET nor Link-State considers time in this way.
One way to estimate the timely-contact probability is to use the ratio of the total contact duration to the total time. However, this approach does not capture the frequency of contacts. For example, one node may have a long contact with another node, followed by a long non-contact period. A third node may have a short contact with the first node, followed by a short non-contact period. Using the above estimation approach, both examples would have similar contact probability. In the second example, however, the two nodes have more frequent contacts.
We design a method to capture the contact frequency of mobile nodes. For this purpose, we assume that even short contacts are sufficient to exchange messages.1 The probability for node i to meet node j is computed by the following procedure. We divide the contact history Hi(j) into a sequence of n periods of ΔT starting from the start time (t0) of the first contact in history Hi(j) to the current time. We number each of the n periods from 0 to n − 1, then check each period. If node i had any contact with node j during a given period m, which is [t0 + mΔT, t0 + (m + 1)ΔT), we set the contact status Im to be 1; otherwise, the contact status Im is 0. The probability p (0) ij that node i meets node j in the next ΔT can be estimated as the average of the contact status in prior intervals: p (0) ij = 1 n n−1X m=0 Im. (5) To adapt to the change of contact patterns, and reduce the storage space for contact histories, a node may discard old history contacts; in this situation, the estimate would be based on only the retained history.
The above probability is the direct contact probability of two nodes. We are also interested in the probability that we may be able to pass a message through a sequence of k nodes. We define the k-order probability inductively, p (k) ij = p (0) ij + X α p (0) iα p (k−1) αj , (6) where α is any node other than i or j.
We first consider the case of a two-hop path, that is, with only one relay node. We consider two approaches: either the receiving neighbor decides whether to act as a relay, or the source decides which neighbors to use as relay.
Whenever a node meets other nodes, they exchange all their messages (or as above, index messages). If the destination of a message is the receiver itself, the message is delivered. Otherwise, if the probability of delivering the message to its destination through this receiver node within ΔT is greater than or equal to a certain threshold, the message is stored in the receiver"s storage to forward 1 In our simulation, however, we accurately model the communication costs and some short contacts will not succeed in transfer of all messages. to the destination. If the probability is less than the threshold, the receiver discards the message. Notice that our protocol replicates the message whenever a good-looking relay comes along.
To make decisions, a sender must have the information about its neighbors" contact probability with a message"s destination.
Therefore, meta-data exchange is necessary.
When two nodes meet, they exchange a meta-message, containing an unordered list of node IDs for which the sender of the metamessage has a contact probability greater than the threshold.
After receiving a meta-message, a node checks whether it has any message that destined to its neighbor, or to a node in the node list of the neighbor"s meta-message. If it has, it sends a copy of the message.
When a node receives a message, if the destination of the message is the receiver itself, the message is delivered. Otherwise, the message is stored in the receiver"s storage for forwarding to the destination.
When we use more than two hops to relay a message, each node needs to know the contact probabilities along all possible paths to the message destination.
Every node keeps a contact probability matrix, in which each cell pij is a contact probability between to nodes i and j. Each node i computes its own contact probabilities (row i) with other nodes using Equation (5) whenever the node ends a contact with other nodes. Each row of the contact probability matrix has a version number; the version number for row i is only increased when node i updates the matrix entries in row i. Other matrix entries are updated through exchange with other nodes when they meet.
When two nodes i and j meet, they first exchange their contact probability matrices. Node i compares its own contact matrix with node j"s matrix. If node j"s matrix has a row l with a higher version number, then node i replaces its own row l with node j"s row l.
Likewise node j updates its matrix. After the exchange, the two nodes will have identical contact probability matrices.
Next, if a node has a message to forward, the node estimates its neighboring node"s order-k contact probability to contact the destination of the message using Equation (6). If p (k) ij is above a threshold, or if j is the destination of the message, node i will send a copy of the message to node j.
All the above effort serves to determine the transfer probability when two nodes meet. The replication decision is orthogonal to the transfer decision. In our implementation, we always replicate.
Although PRoPHET [16] and Link-State [22] do no replication, as described, we added replication to those protocols for better comparison to our protocol.
We evaluate and compare the results of direct delivery, epidemic, random, PRoPHET, Link-State, and timely-contact routing protocols.
We use real mobility data collected at Dartmouth College.
Dartmouth College has collected association and disassociation messages from devices on its wireless network wireless users since spring 2001 [13]. Each message records the wireless card MAC address, the time of association/disassociation, and the name of the access point. We treat each unique MAC address as a node. For 37 more information about Dartmouth"s network and the data collection, see previous studies [7, 12].
Our data are not contacts in a mobile ad-hoc network. We can approximate contact traces by assuming that two users can communicate with each other whenever they are associated with the same access point. Chaintreau et al. [5] used Dartmouth data traces and made the same assumption to theoretically analyze the impact of human mobility on opportunistic forwarding algorithms. This assumption may not be accurate,2 but it is a good first approximation.
In our simulation, we imagine the same clients and same mobility in a network with no access points. Since our campus has full WiFi coverage, we assume that the location of access points had little impact on users" mobility.
We simulated one full month of trace data (November 2003) taken from CRAWDAD [13], with 5, 142 users. Although predictionbased protocols require prior contact history to estimate each node"s delivery probability, our preliminary results show that the performance improvement of warming-up over one month of trace was marginal. Therefore, for simplicity, we show the results of all protocols without warming-up.
We developed a custom simulator.3 Since we used contact traces derived from real mobility data, we did not need a mobility model and omitted physical and link-layer details for node discovery. We were aware that the time for neighbor discovery in different wireless technologies vary from less than one seconds to several seconds. Furthermore, connection establishment also takes time, such as DHCP. In our simulation, we assumed the nodes could discover and connect each other instantly when they were associated with a same AP. To accurately model communication costs, however, we simulated some MAC-layer behaviors, such as collision.
The default settings of the network of our simulator are listed in Table 1, using the values recommended by other papers [22, 16].
The message probability was the probability of generating messages, as described in Section 4.3. The default transmission bandwidth was 11 Mb/s. When one node tried to transmit a message, it first checked whether any nearby node was transmitting. If it was, the node backed off a random number of slots. Each slot was 1 millisecond, and the maximum number of backoff slots was 30. The size of messages was uniformly distributed between 80 bytes and
of hops before a message should stop forwarding. The time to live (TTL) was the maximum duration that a message may exist before expiring. The storage capacity was the maximum space that a node can use for storing messages. For our routing method, we used a default prediction window ΔT of 10 hours and a probability threshold of 0.01. The replication factor r was not limited by default, so the source of a message transferred the messages to any other node that had a contact probability with the message destination higher than the probability threshold.
After each contact event in the contact trace, we generated a message with a given probability; we choose a source node and a des2 Two nodes may not have been able to directly communicate while they were at two far sides of an access point, or two nodes may have been able to directly communicate if they were between two adjacent access points. 3 We tried to use a general network simulator (ns2), which was extremely slow when simulating a large number of mobile nodes (in our case, more than 5000 nodes), and provided unnecessary detail in modeling lower-level network protocols.
Table 1: Default Settings of the Simulation Parameter Default value message probability 0.001 bandwidth 11 Mb/s transmission slot 1 millisecond max backoff slots 30 message size 80-1024 bytes hop count limit (HCL) unlimited time to live (TTL) unlimited storage capacity unlimited prediction window ΔT 10 hours probability threshold 0.01 contact history length 20 replication always aging factor α 0.9 (0.98 PRoPHET) initial probability p0 0.75 (PRoPHET) transitivity impact β 0.25 (PRoPHET) 0 20000 40000 60000 80000 100000 120000
Numberofoccurrence hour movements contacts Figure 1: Movements and contacts duration each hour tination node randomly using a uniform distribution across nodes seen in the contact trace up to the current time. When there were more contacts during a certain period, there was a higher likelihood that a new message was generated in that period. This correlation is not unreasonable, since there were more movements during the day than during the night, and so the number of contacts. Figure 1 shows the statistics of the numbers of movements and the numbers of contacts during each hour of the day, summed across all users and all days. The plot shows a clear diurnal activity pattern. The activities reached lowest around 5am and peaked between 4pm and 5pm. We assume that in some applications, network traffic exhibits similar patterns, that is, people send more messages during the day, too.
Messages expire after a TTL. We did not use proactive methods to notify nodes the delivery of messages, so that the messages can be removed from storage.
We define a set of metrics that we use in evaluating routing protocols in opportunistic networks: • delivery ratio, the ratio of the number of messages delivered to the number of total messages generated. • message transmissions, the total number of messages transmitted during the simulation across all nodes. 38 • meta-data transmissions, the total number of meta-data units transmitted during the simulation across all nodes. • message duplications, the number of times a message copy occurred, due to replication. • delay, the duration between a message"s generation time and the message"s delivery time. • storage usage, the max and mean of maximum storage (bytes) used across all nodes.
Here we compare simulation results of the six routing protocols.
1 unlimited 100 24 10 1 Deliveryratio Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Figure 2: Delivery ratio (log scale). The direct and random protocols for one-hour TTL had delivery ratios that were too low to be visible in the plot.
Figure 2 shows the delivery ratio of all the protocols, with different TTLs. (In all the plots in the paper, prediction stands for our method, state stands for the Link-State protocol, and prophet represents PRoPHET.) Although we had 5,142 users in the network, the direct-delivery and random protocols had low delivery ratios (note the log scale). Even for messages with an unlimited lifetime, only 59 out of 2077 messages were delivered during this one-month simulation. The delivery ratio of epidemic routing was the best. The three prediction-based approaches had low delivery ratio, compared to epidemic routing. Although our method was slightly better than the other two, the advantage was marginal.
The high delivery ratio of epidemic routing came with a price: excessive transmissions. Figure 3 shows the number of message data transmissions. The number of message transmissions of epidemic routing was more than 10 times higher than for the predictionbased routing protocols. Obviously, the direct delivery protocol had the lowest number of message transmissions - the number of message delivered. Among the three prediction-based methods, the PRoPHET transmitted fewer messages, but had comparable delivery-ratio as seen in Figure 2.
Figure 4 shows that epidemic and all prediction-based methods had substantial meta-data transmissions, though epidemic routing had relatively more, with shorter TTLs. Because epidemic protocol transmitted messages at every contact, in turn, more nodes had messages that required meta-data transmission during contact. The direct-delivery and random protocols had no meta-data transmissions.
In addition to its message transmissions and meta-data transmissions, the epidemic routing protocol also had excessive message 1 10 100 1000 10000 100000 1e+06 1e+07 1e+08 unlimited 100 24 10 1 Numberofmessagetransmitted Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Figure 3: Message transmissions (log scale) 1 10 100 1000 10000 100000 1e+06 1e+07 1e+08 unlimited 100 24 10 1 Numberofmeta-datatransmissions Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Figure 4: Meta-data transmissions (log scale). Direct and random protocols had no meta-data transmissions. duplications, spreading replicas of messages over the network.
Figure 5 shows that epidemic routing had one or two orders more duplication than the prediction-based protocols. Recall that the directdelivery and random protocols did not replicate, thus had no data duplications.
Figure 6 shows both the median and mean delivery delays. All protocols show similar delivery delays in both mean and median measures for medium TTLs, but differ for long and short TTLs.
With a 100-hour TTL, or unlimited TTL, epidemic routing had the shortest delays. The direct-delivery had the longest delay for unlimited TTL, but it had the shortest delay for the one-hour TTL.
The results seem contrary to our intuition: the epidemic routing protocol should be the fastest routing protocol since it spreads messages all over the network. Indeed, the figures show only the delay time for delivered messages. For direct delivery, random, and the probability-based routing protocols, relatively few messages were delivered for short TTLs, so many messages expired before they could reach their destination; those messages had infinite delivery delay and were not included in the median or mean measurements.
For longer TTLs, more messages were delivered even for the directdelivery protocol. The statistics of longer TTLs for comparison are more meaningful than those of short TTLs.
Since our message generation rate was low, the storage usage was also low in our simulation. Figure 7 shows the maximum and average of maximum volume (in KBytes) of messages stored 39 1 10 100 1000 10000 100000 1e+06 1e+07 1e+08 unlimited 100 24 10 1 Numberofmessageduplications Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Figure 5: Message duplications (log scale). Direct and random protocols had no message duplications. 1 10 100 1000 10000 unlimited100 24 10 1 unlimited100 24 10 1 Delay(minute) Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Mean delayMedian delay Figure 6: Median and mean delays (log scale). in each node. The epidemic routing had the most storage usage.
The message time-to-live parameter was the big factor affecting the storage usage for epidemic and prediction-based routing protocols.
We studied the impact of different parameters of our predictionbased routing protocol. Our prediction-based protocol was sensitive to several parameters, such as the probability threshold and the prediction window ΔT. Figure 8 shows the delivery ratios when we used different probability thresholds. (The leftmost value 0.01 is the value used for the other plots.) A higher probability threshold limited the transfer probability, so fewer messages were delivered.
It also required fewer transmissions as shown in Figure 9. With a larger prediction window, we got a higher contact probability.
Thus, for the same probability threshold, we had a slightly higher delivery ratio as shown in Figure 10, and a few more transmissions as shown in Figure 11.
In addition to the protocols that we evaluated in our simulation, several other opportunistic network routing protocols have been proposed in the literature. We did not implement and evaluate these routing protocols, because either they require domain-specific information (location information) [14, 15], assume certain mobility patterns [17], present orthogonal approaches [10, 24] to other routing protocols.
1 10 100 1000 10000 unlimited100 24 10 1 unlimited100 24 10 1 Storageusage(KB) Message time-to-live (TTL) (hour) direct random prediction state prophet epidemic Mean of maximumMax of maximum Figure 7: Max and mean of maximum storage usage across all nodes (log scale). 0
1
Deliveryratio Probability threshold Figure 8: Probability threshold impact on delivery ratio of timely-contact routing.
LeBrun et al. [14] propose a location-based delay-tolerant network routing protocol. Their algorithm assumes that every node knows its own position, and the destination is stationary at a known location. A node forwards data to a neighbor only if the neighbor is closer to the destination than its own position. Our protocol does not require knowledge of the nodes" locations, and learns their contact patterns.
Leguay et al. [15] use a high-dimensional space to represent a mobility pattern, then routes messages to nodes that are closer to the destination node in the mobility pattern space. Location information of nodes is required to construct mobility patterns.
Musolesi et al. [17] propose an adaptive routing protocol for intermittently connected mobile ad-hoc networks. They use a Kalman filter to compute the probability that a node delivers messages. This protocol assumes group mobility and cloud connectivity, that is, nodes move as a group, and among this group of nodes a contemporaneous end-to-end connection exists for every pair of nodes. When two nodes are in the same connected cloud, DSDV [19] routing is used.
Network coding also draws much interest from DTN research.
Erasure-coding [10, 24] explores coding algorithms to reduce message replicas. The source node replicates a message m times, then uses a coding scheme to encode them in one big message.
After replicas are encoded, the source divides the big message into k 40 0
1
2
3
Numberofmessagetransmitted(million) Probability threshold Figure 9: Probability threshold impact on message transmission of timely-contact routing. 0
1
Deliveryratio Prediction window (hour) Figure 10: Prediction window impact on delivery ratio of timely-contact routing (semi-log scale). blocks of the same size, and transmits a block to each of the first k encountered nodes. If m of the blocks are received at the destination, the message can be restored, where m < k. In a uniformly distributed mobility scenario, the delivery probability increases because the probability that the destination node meets m relays is greater than it meets k relays, given m < k.
We propose a prediction-based routing protocol for opportunistic networks. We evaluate the performance of our protocol using realistic contact traces, and compare to five existing routing protocols.
Our simulation results show that direct delivery had the lowest delivery ratio, the fewest data transmissions, and no meta-data transmission or data duplication. Direct delivery is suitable for devices that require an extremely low power consumption. The random protocol increased the chance of delivery for messages otherwise stuck at some low mobility nodes. Epidemic routing delivered the most messages. The excessive transmissions, and data duplication, however, consume more resources than portable devices may be able to provide.
None of these protocols (direct-delivery, random and epidemic routing) are practical for real deployment of opportunistic networks, 0
1
2
3
Numberofmessagetransmitted(million) Prediction window (hour) Figure 11: Prediction window impact on message transmission of timely-contact routing (semi-log scale). because they either had an extremely low delivery ratio, or had an extremely high resource consumption. The prediction-based routing protocols had a delivery ratio more than 10 times better than that for direct-delivery and random routing, and fewer transmissions and less storage usage than epidemic routing. They also had fewer data duplications than epidemic routing.
All the prediction-based routing protocols that we have evaluated had similar performance. Our method had a slightly higher delivery ratio, but more transmissions and higher storage usage.
There are many parameters for prediction-based routing protocols, however, and different parameters may produce different results.
Indeed, there is an opportunity for some adaptation; for example, high priority messages may be given higher transfer and replication probabilities to increase the chance of delivery and reduce the delay, or a node with infrequent contact may choose to raise its transfer probability.
We only studied the impact of predicting peer-to-peer contact probability for routing in unicast messages. In some applications, context information (such as location) may be available for the peers. One may also consider other messaging models, for example, where messages are sent to a location, such that every node at that location will receive a copy of the message. Location prediction [21] may be used to predict nodes" mobility, and to choose as relays those nodes moving toward the destined location.
Research on routing in opportunistic networks is still in its early stage. Many other issues of opportunistic networks, such as security and privacy, are mainly left open. We anticipate studying these issues in future work.
This research is a project of the Center for Mobile Computing and the Institute for Security Technology Studies at Dartmouth College. It was supported by DoCoMo Labs USA, the CRAWDAD archive at Dartmouth College (funded by NSF CRI Award 0454062), NSF Infrastructure Award EIA-9802068, and by Grant number 2005-DD-BX-1091 awarded by the Bureau of Justice Assistance. Points of view or opinions in this document are those of the authors and do not represent the official position or policies of any sponsor.
[1] John Burgess, Brian Gallagher, David Jensen, and Brian Neil Levine. MaxProp: routing for vehicle-based 41 disruption-tolerant networks. In Proceedings of the 25th IEEE International Conference on Computer Communications (INFOCOM), April 2006. [2] Scott Burleigh, Adrian Hooke, Leigh Torgerson, Kevin Fall,
Vint Cerf, Bob Durst, Keith Scott, and Howard Weiss.
Delay-tolerant networking: An approach to interplanetary Internet. IEEE Communications Magazine, 41(6):128-136,
June 2003. [3] Tracy Camp, Jeff Boleng, and Vanessa Davies. A survey of mobility models for ad-hoc network research. Wireless Communication & Mobile Computing (WCMC): Special issue on Mobile ad-hoc Networking: Research, Trends and Applications, 2(5):483-502, 2002. [4] Andrew Campbell, Shane Eisenman, Nicholas Lane,
Emiliano Miluzzo, and Ronald Peterson. People-centric urban sensing. In IEEE Wireless Internet Conference, August
[5] Augustin Chaintreau, Pan Hui, Jon Crowcroft, Christophe Diot, Richard Gass, and James Scott. Impact of human mobility on the design of opportunistic forwarding algorithms. In Proceedings of the 25th IEEE International Conference on Computer Communications (INFOCOM),
April 2006. [6] Kevin Fall. A delay-tolerant network architecture for challenged internets. In Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM), August 2003. [7] Tristan Henderson, David Kotz, and Ilya Abyzov. The changing usage of a mature campus-wide wireless network.
In Proceedings of the 10th Annual International Conference on Mobile Computing and Networking (MobiCom), pages 187-201, September 2004. [8] Pan Hui, Augustin Chaintreau, James Scott, Richard Gass,
Jon Crowcroft, and Christophe Diot. Pocket switched networks and human mobility in conference environments.
In ACM SIGCOMM Workshop on Delay Tolerant Networking, pages 244-251, August 2005. [9] Ravi Jain, Dan Lelescu, and Mahadevan Balakrishnan.
Model T: an empirical model for user registration patterns in a campus wireless LAN. In Proceedings of the 11th Annual International Conference on Mobile Computing and Networking (MobiCom), pages 170-184, 2005. [10] Sushant Jain, Mike Demmer, Rabin Patra, and Kevin Fall.
Using redundancy to cope with failures in a delay tolerant network. In Proceedings of the 2005 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM), pages 109-120,
August 2005. [11] Philo Juang, Hidekazu Oki, Yong Wang, Margaret Martonosi, Li-Shiuan Peh, and Daniel Rubenstein.
Energy-efficient computing for wildlife tracking: Design tradeoffs and early experiences with ZebraNet. In the Tenth International Conference on Architectural Support for Programming Languages and Operating Systems, October
[12] David Kotz and Kobby Essien. Analysis of a campus-wide wireless network. Wireless Networks, 11:115-133, 2005. [13] David Kotz, Tristan Henderson, and Ilya Abyzov.
CRAWDAD data set dartmouth/campus. http://crawdad.cs.dartmouth.edu/dartmouth/campus,
December 2004. [14] Jason LeBrun, Chen-Nee Chuah, Dipak Ghosal, and Michael Zhang. Knowledge-based opportunistic forwarding in vehicular wireless ad-hoc networks. In IEEE Vehicular Technology Conference, pages 2289-2293, May 2005. [15] Jeremie Leguay, Timur Friedman, and Vania Conan.
Evaluating mobility pattern space routing for DTNs. In Proceedings of the 25th IEEE International Conference on Computer Communications (INFOCOM), April 2006. [16] Anders Lindgren, Avri Doria, and Olov Schelen.
Probabilistic routing in intermittently connected networks. In Workshop on Service Assurance with Partial and Intermittent Resources (SAPIR), pages 239-254, 2004. [17] Mirco Musolesi, Stephen Hailes, and Cecilia Mascolo.
Adaptive routing for intermittently connected mobile ad-hoc networks. In IEEE International Symposium on a World of Wireless Mobile and Multimedia Networks, pages 183-189,
June 2005. extended version. [18] OLPC. One laptop per child project. http://laptop.org. [19] C. E. Perkins and P. Bhagwat. Highly dynamic destination-sequenced distance-vector routing (DSDV) for mobile computers. Computer Communication Review, pages 234-244, October 1994. [20] C. E. Perkins and E. M. Royer. ad-hoc on-demand distance vector routing. In IEEE Workshop on Mobile Computing Systems and Applications, pages 90-100, February 1999. [21] Libo Song, David Kotz, Ravi Jain, and Xiaoning He.
Evaluating next-cell predictors with extensive Wi-Fi mobility data. IEEE Transactions on Mobile Computing, 5(12):1633-1649, December 2006. [22] Jing Su, Ashvin Goel, and Eyal de Lara. An empirical evaluation of the student-net delay tolerant network. In International Conference on Mobile and Ubiquitous Systems (MobiQuitous), July 2006. [23] Amin Vahdat and David Becker. Epidemic routing for partially-connected ad-hoc networks. Technical Report CS-2000-06, Duke University, July 2000. [24] Yong Wang, Sushant Jain, Margaret Martonosia, and Kevin Fall. Erasure-coding based routing for opportunistic networks. In ACM SIGCOMM Workshop on Delay Tolerant Networking, pages 229-236, August 2005. [25] Yu Wang and Hongyi Wu. DFT-MSN: the delay fault tolerant mobile sensor network for pervasive information gathering.

Search and rescue of people in emergency situation in a timely manner is an extremely important service. It has been difficult to provide such a service due to lack of timely information needed to determine the current location of a person who may be in an emergency situation. With the emergence of pervasive computing, several systems [12, 19, 1, 5, 6, 4, 11] have been developed over the last few years that make use of small devices such as cell phones, sensors, etc. All these systems require a connected network via satellites, GSM base stations, or mobile devices. This requirement severely limits their applicability, particularly in remote wilderness areas where maintaining a connected network is very difficult.
For example, a GSM transmitter has to be in the range of a base station to transmit. As a result, it cannot operate in most wilderness areas. While a satellite transmitter is the only viable solution in wilderness areas, it is typically expensive and cumbersome. Furthermore, a line of sight is required to transmit to satellite, and that makes it infeasible to stay connected in narrow canyons, large cities with skyscrapers, rain forests, or even when there is a roof or some other obstruction above the transmitter, e.g. in a car. An RF transmitter has a relatively smaller range of transmission. So, while an in-situ sensor is cheap as a single unit, it is expensive to build a large network that can provide connectivity over a large wilderness area. In a mobile environment where sensors are carried by moving people, power-efficient routing is difficult to implement and maintain over a large wilderness area. In fact, building an adhoc sensor network using only the sensors worn by hikers is nearly impossible due to a relatively small number of sensors spread over a large wilderness area.
In this paper, we describe the design, implementation and evaluation of a search and rescue system called CenWits (Connection-less Sensor-Based Tracking System Using Witnesses). CenWits is comprised of mobile, in-situ sensors that are worn by subjects (people, wild animals, or in-animate objects), access points (AP) that collect information from these sensors, and GPS receivers and location points (LP) that provide location information to the sensors. A subject uses GPS receivers (when it can connect to a satellite) and LPs to determine its current location. The key idea of CenWits is that it uses a concept of witnesses to convey a subject"s movement and location information to the outside world. This averts a need for maintaining a connected network to transmit location information to the outside world. In particular, there is no need for expensive GSM or satellite transmitters, or maintaining an adhoc network of in-situ sensors in CenWits. 180 CenWits employs several important mechanisms to address the key problem of resource constraints (low signal strength, low power and limited memory) in sensors. In particular, it makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center.
The problem of low signal strengths (short range RF communication) is addressed by avoiding a need for maintaining a connected network. Instead, CenWits propagates the location information of sensors using the concept of witnesses through an intermittently connected network. As a result, this system can be deployed in remote wilderness areas, as well as in large urban areas with skyscrapers and other tall structures. Also, this makes CenWits cost-effective. A subject only needs to wear light-weight and low-cost sensors that have GPS receivers but no expensive GSM or satellite transmitters. Furthermore, since there is no need for a connected sensor network, there is no need to deploy sensors in very large numbers.
The problem of limited battery life and limited memory of a sensor is addressed by incorporating the concepts of groups and partitions. Groups and partitions allow sensors to stay in sleep or receive modes most of the time. Using groups and partitions, the location information collected by a sensor can be distributed among several sensors, thereby reducing the amount of memory needed in one sensor to store that information. In fact, CenWits provides an adaptive tradeoff between memory and power consumption of sensors. Each sensor can dynamically adjust its power and memory consumption based on its remaining power or available memory.
It has amply been noted that the strength of sensor networks comes from the fact that several sensor nodes can be distributed over a relatively large area to construct a multihop network. This paper demonstrates that important large-scale applications can be built using sensors by judiciously integrating the storage, communication and computation capabilities of sensors. The paper describes important techniques to combine memory, transmission and battery power of many sensors to address resource constraints in the context of a search and rescue application. However, these techniques are quite general. We discuss several other sensor-based applications that can employ these techniques.
While CenWits addresses the general location tracking and reporting problem in a wide-area network, there are two important differences from the earlier work done in this area. First, unlike earlier location tracking solutions,
CenWits does not require a connected network. Second, unlike earlier location tracking solutions, CenWits does not aim for a very high accuracy of localization. Instead, the main goal is to provide an approximate, small area where search and rescue efforts can be concentrated.
The rest of this paper is organized as follows. In Section 2, we overview some of the recent projects and technologies related to movement and location tracking, and search and rescue systems. In Section 3, we describe the overall architecture of CenWits, and provide a high-level description of its functionality. In the next section, Section 4, we discuss power and memory management in CenWits. To simplify our presentation, we will focus on a specific application of tracking lost/injured hikers in all these sections. In Section 6, we describe a prototype implementation of CenWits and present performance measured from this implementation. We discuss how the ideas of CenWits can be used to build several other applications in Section 7. Finally, in Section 8, we discuss some related issues and conclude the paper.
A survey of location systems for ubiquitous computing is provided in [11]. A location tracking system for adhoc sensor networks using anchor sensors as reference to gain location information and spread it out to outer node is proposed in [17]. Most location tracking systems in adhoc sensor networks are for benefiting geographic-aware routing.
They don"t fit well for our purposes. The well-known active badge system [19] lets a user carry a badge around.
An infrared sensor in the room can detect the presence of a badge and determine the location and identification of the person. This is a useful system for indoor environment, where GPS doesn"t work. Locationing using 802.11 devices is probably the cheapest solution for indoor position tracking [8]. Because of the popularity and low cost of 802.11 devices, several business solutions based on this technology have been developed[1].
A system that combines two mature technologies and is viable in suburban area where a user can see clear sky and has GSM cellular reception at the same time is currently available[5]. This system receives GPS signal from a satellite and locates itself, draws location on a map, and sends location information through GSM network to the others who are interested in the user"s location.
A very simple system to monitor children consists an RF transmitter and a receiver. The system alarms the holder of the receiver when the transmitter is about to run out of range [6].
Personal Locater Beacons (PLB) has been used for avalanche rescuing for years. A skier carries an RF transmitter that emits beacons periodically, so that a rescue team can find his/her location based on the strength of the RF signal.
Luxury version of PLB combines a GPS receiver and a COSPASSARSAT satellite transmitter that can transmit user"s location in latitude and longitude to the rescue team whenever an accident happens [4]. However, the device either is turned on all the time resulting in fast battery drain, or must be turned on after the accident to function.
Another related technology in widespread use today is the ONSTAR system [3], typically used in several luxury cars.
In this system, a GPS unit provides position information, and a powerful transmitter relays that information via satellite to a customer service center. Designed for emergencies, the system can be triggered either by the user with the push of a button, or by a catastrophic accident. Once the system has been triggered, a human representative attempts to gain communication with the user via a cell phone built as an incar device. If contact cannot be made, emergency services are dispatched to the location provided by GPS. Like PLBs, this system has several limitations. First, it is heavy and expensive. It requires a satellite transmitter and a connected network. If connectivity with either the GPS network or a communication satellite cannot be maintained, the system fails. Unfortunately, these are common obstacles encountered in deep canyons, narrow streets in large cities, parking garages, and a number of other places. 181 The Lifetch system uses GPS receiver board combined with a GSM/GPRS transmitter and an RF transmitter in one wireless sensor node called Intelligent Communication Unit (ICU). An ICU first attempts to transmit its location to a control center through GSM/GPRS network. If that fails, it connects with other ICUs (adhoc network) to forward its location information until the information reaches an ICU that has GSM/GPRS reception. This ICU then transmits the location information of the original ICU via the GSM/GPRS network.
ZebraNet is a system designed to study the moving patterns of zebras [13]. It utilizes two protocols: History-based protocol and flooding protocol. History-based protocol is used when the zebras are grazing and not moving around too much. While this might be useful for tracking zebras, it"s not suitable for tracking hikers because two hikers are most likely to meet each other only once on a trail. In the flooding protocol, a node dumps its data to a neighbor whenever it finds one and doesn"t delete its own copy until it finds a base station. Without considering routing loops, packet filtering and grouping, the size of data on a node will grow exponentially and drain the power and memory of a sensor node with in a short time. Instead, Cenwits uses a four-phase hand-shake protocol to ensure that a node transmits only as much information as the other node is willing to receive. While ZebraNet is designed for a big group of sensors moving together in the same direction with same speed, Cenwits is designed to be used in the scenario where sensors move in different directions at different speeds.
Delay tolerant network architecture addresses some important problems in challenged (resource-constrained) networks [9]. While this work is mainly concerned with interoperability of challenged networks, some problems related to occasionally-connected networks are similar to the ones we have addressed in CenWits.
Among all these systems, luxury PLB and Lifetch are designed for location tracking in wilderness areas. However, both of these systems require a connected network. Luxury PLB requires the user to transmit a signal to a satellite, while Lifetch requires connection to GSM/GPRS network.
Luxury PLB transmits location information, only when an accident happens. However, if the user is buried in the snow or falls into a deep canyon, there is almost no chance for the signal to go through and be relayed to the rescue team. This is because satellite transmission needs line of sight.
Furthermore, since there is no known history of user"s location, it is not possible for the rescue team to infer the current location of the user. Another disadvantage of luxury PLB is that a satellite transmitter is very expensive, costing in the range of $750. Lifetch attempts to transmit the location information by GSM/GPRS and adhoc sensor network that uses AODV as the routing protocol. However, having a cellular reception in remote areas in wilderness areas, e.g.
American national parks is unlikely. Furthermore, it is extremely unlikely that ICUs worn by hikers will be able to form an adhoc network in a large wilderness area. This is because the hikers are mobile and it is very unlikely to have several ICUs placed dense enough to forward packets even on a very popular hike route.
CenWits is designed to address the limitations of systems such as luxury PLB and Lifetch. It is designed to provide hikers, skiers, and climbers who have their activities mainly in wilderness areas a much higher chance to convey their location information to a control center. It is not reliant upon constant connectivity with any communication medium. Rather, it communicates information along from user to user, finally arriving at a control center. Unlike several of the systems discussed so far, it does not require that a user"s unit is constantly turned on. In fact, it can discover a victim"s location, even if the victim"s sensor was off at the time of accident and has remained off since then. CenWits solves one of the greatest problems plaguing modern search and rescue systems: it has an inherent on-site storage capability. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well.
Figure 1: Hiker A and Hiker B are are not in the range of each other
We describe CenWits in the context of locating lost/injured hikers in wilderness areas. Each hiker wears a sensor (MICA2 motes in our prototype) equipped with a GPS receiver and an RF transmitter. Each sensor is assigned a unique ID and maintains its current location based on the signal received by its GPS receiver. It also emits beacons periodically. When any two sensors are in the range of one another, they record the presence of each other (witness information), and also exchange the witness information they recorded earlier. The key idea here is that if two sensors come with in range of each other at any time, they become each other"s witnesses.
Later on, if the hiker wearing one of these sensors is lost, the other sensor can convey the last known (witnessed) location of the lost hiker. Furthermore, by exchanging the witness information that each sensor recorded earlier, the witness information is propagated beyond a direct contact between two sensors.
To convey witness information to a processing center or to a rescue team, access points are established at well-known locations that the hikers are expected to pass through, e.g. at the trail heads, trail ends, intersection of different trails, scenic view points, resting areas, and so on. Whenever a sensor node is in the vicinity of an access point, all witness information stored in that sensor is automatically dumped to the access point. Access points are connected to a processing center via satellite or some other network1 . The witness information is downloaded to the processing center from various access points at regular intervals. In case, connection to an access point is lost, the information from that 1 A connection is needed only between access points and a processing center. There is no need for any connection between different access points. 182 access point can be downloaded manually, e.g. by UAVs.
To estimate the speed, location and direction of a hiker at any point in time, all witness information of that hiker that has been collected from various access points is processed.
Figure 2: Hiker A and Hiker B are in the range of each other. A records the presence of B and B records the presence of A. A and B become each other"s witnesses.
Figure 3: Hiker A is in the range of an access point. It uploads its recorded witness information and clears its memory.
An example of how CenWits operates is illustrated in Figures 1, 2 and 3. First, hikers A and B are on two close trails, but out of range of each other (Figure 1). This is a very common scenario during a hike. For example, on a popular four-hour hike, a hiker might run into as many as
minutes on average. A slow hiker can go 1 mile (5,280 feet) per hour. Thus in 12 minutes a slow hiker can go as far as
4-hour, one-way hike evenly, the range of each sensor node should be at least 1056 feet for them to communicate with one another continuously. The signal strength starts dropping rapidly for two Mica2 nodes to communicate with each other when they are 180 feet away, and is completely lost when they are 230 feet away from each other[7]. So, for the sensors to form a sensor network on a 4-hour hiking trail, there should be at least 120 hikers scattered evenly. Clearly, this is extremely unlikely. In fact, in a 4-hour, less-popular hiking trail, one might only run into say five other hikers.
CenWits takes advantage of the fact that sensors can communicate with one another and record their presence. Given a walking speed of one mile per hour (88 feet per minute) and Mica2 range of about 150 feet for non-line-of-sight radio transmission, two hikers have about 150/88 = 1.7 minutes to discover the presence of each other and exchange their witness information. We therefore design our system to have each sensor emit a beacon every one-and-a-half minute. In Figure 2, hiker B"s sensor emits a beacon when A is in range, this triggers A to exchange data with B. A communicates the following information to B: My ID is A; I saw C at 1:23 PM at (39◦
PM at (40◦
with My ID is B; I saw K at 11:20 AM at (39◦
105◦
PM at (41◦
saw A at 4:17 PM at (41◦
B goes on his way to overnight camping while A heads back to trail head where there is an AP, which emits beacon every 5 seconds to avoid missing any hiker. A dumps all witness information it has collected to the access point. This is shown in Figure 3.
A critical concern is that there is limited amount of memory available on motes (4 KB SDRAM memory, 128 KB flash memory, and 4-512 KB EEPROM). So, it is important to organize witness information efficiently. CenWits stores witness information at each node as a set of witness records (Format is shown in Figure 4.
Node ID Record Time X, Y Location Time Hop Count
Figure 4: Format of a witness record.
When two nodes i and j encounter each other, each node generates a new witness record. In the witness record generated by i, Node ID is j, Record Time is the current time in i"s clock, (X,Y) are the coordinates of the location of i that i recorded most recently (either from satellite or an LP), Location Time is the time when the this location was recorded, and Hop Count is 0.
Each node is assigned a unique Node Id when it enters a trail. In our current prototype, we have allocated one byte for Node Id, although this can be increased to two or more bytes if a large number of hikers are expected to be present at the same time. We can represent time in 17 bits to a second precision. So, we have allocated 3 bytes each for Record Time and Location Time. The circumference of the Earth is approximately 40,075 KM. If we use a 32-bit number to represent both longitude and latitude, the precision we get is 40,075,000/232 = 0.0093 meter = 0.37 inches, which is quite precise for our needs. So, we have allocated 4 bytes each for X and Y coordinates of the location of a node. In fact, a foot precision can be achieved by using only 27 bits.
Although a GPS receiver provides an accurate location information, it has it"s limitation. In canyons and rainy forests, a GPS receiver does not work. When there is a heavy cloud cover, GPS users have experienced inaccuracy in the reported location as well. Unfortunately, a lot of hiking trails are in dense forests and canyons, and it"s not that uncommon to rain after hikers start hiking. To address this,
CenWits incorporates the idea of location points (LP). A location point can update a sensor node with its current location whenever the node is near that LP. LPs are placed at different locations in a wilderness area where GPS receivers don"t work. An LP is a very simple device that emits prerecorded location information at some regular time interval.
It can be placed in difficult-to-reach places such as deep canyons and dense rain forests by simply dropping them from an airplane. LPs allow a sensor node to determine its current location more accurately. However, they are not 183 an essential requirement of CenWits. If an LP runs out of power, the CenWits will continue to work correctly.
Figure 5: GPS receiver not working correctly.
Sensors then have to rely on LP to provide coordination In Figure 5, B cannot get GPS reception due to bad weather. It then runs into A on the trail who doesn"t have GPS reception either. Their sensors record the presence of each other. After 10 minutes, A is in range of an LP that provides an accurate location information to A. When A returns to trail head and uploads its data (Figure 6), the system can draw a circle centered at the LP from which A fetched location information for the range of encounter location of A and B. By Overlapping this circle with the trail map, two or three possible location of encounter can be inferred. Thus when a rescue is required, the possible location of B can be better inferred (See Figures 7 and 8).
Figure 6: A is back to trail head, It reports the time of encounter with B to AP, but no location information to AP Figure 7: B is still missing after sunset. CenWits infers the last contact point and draws the circle of possible current locations based on average hiking speed CenWits requires that the clocks of different sensor nodes be loosely synchronized with one another. Such a synchronization is trivial when GPS coverage is available. In addition, sensor nodes in CenWits synchronize their clocks whenever they are in the range of an AP or an LP. The Figure 8: Based on overlapping landscape, B might have hiked to wrong branch and fallen off a cliff. Hot rescue areas can thus be determined synchronization accuracy Cenwits needs is of the order of a second or so. As long as the clocks are synchronized with in one second range, whether A met B at 12:37"45 or 12:37"46 doesn"t matter in the ordering of witness events and inferring the path.
CenWits employs several important mechanisms to conserve power and memory. It is important to note while current sensor nodes have limited amount of memory, future sensor nodes are expected to have much more memory.
With this in mind, the main focus in our design is to provide a tradeoff between the amount of memory available and amount of power consumption.
The size of witness information stored at a node can get very large. This is because the node may come across several other nodes during a hike, and may end up accumulating a large amount of witness information over time. To address this problem, CenWits allows a node to pro-actively free up some parts of its memory periodically. This raises an interesting question of when and which witness record should be deleted from the memory of a node? CenWits uses three criteria to determine this: record count, hop count, and record gap.
Record count refers to the number of witness records with same node id that a node has stored in its memory. A node maintains an integer parameter MAX RECORD COUNT.
It stores at most MAX RECORD COUNT witness records of any node.
Every witness record has a hop count field that stores the number times (hops) this record has been transferred since being created. Initially this field is set to 0. Whenever a node receives a witness record from another node, it increments the hop count of that record by 1. A node maintains an integer parameter called MAX HOP COUNT. It keeps only those witness records in its memory, whose hop count is less than MAX HOP COUNT. The MAX HOP COUNT parameter provides a balance between two conflicting goals: (1) To ensure that a witness record has been propagated to and thus stored at as many nodes as possible, so that it has a high probability of being dumped at some AP as quickly as possible; and (2) To ensure that a witness record is stored only at a few nodes, so that it does not clog up too much of the combined memory of all sensor nodes. We chose to use hop count instead of time-to-live to decide when to drop a packet. The main reason for this is that the probability of a packet reaching an AP goes up as the hop count adds up.
For example, when the hop count if 5 for a specific record, 184 the record is in at least 5 sensor nodes. On the other hand, if we discard old records, without considering hop count, there is no guarantee that the record is present in any other sensor node.
Record gap refers to the time difference between the record times of two witness records with the same node id. To save memory, a node n ensures the the record gap between any two witness records with the same node id is at least MIN RECORD GAP. For each node id i, n stores the witness record with the most recent record time rti, the witness with most recent record time that is at least MIN RECORD GAP time units before rti, and so on until the record count limit (MAX RECORD COUNT) is reached.
When a node is tight in memory, it adjusts the three parameters, MAX RECORD COUNT, MAX HOP COUNT and MIN RECORD GAP to free up some memory. It decrements MAX RECORD COUNT and MAX HOP COUNT, and increments MIN RECORD GAP. It then first erases all witness records whose hop count exceeds the reduced MAX HOP COUNT value, and then erases witness records to satisfy the record gap criteria. Also, when a node has extra memory space available, e.g. after dumping its witness information at an access point, it resets MAX RECORD COUNT,
MAX HOP COUNT and MIN RECORD GAP to some predefined values.
An important advantage of using sensors for tracking purposes is that we can regulate the behavior of a sensor node based on current conditions. For example, we mentioned earlier that a sensor should emit a beacon every 1.7 minute, given a hiking speed of 1 mile/hour. However, if a user is moving 10 feet/sec, a beacon should be emitted every 10 seconds. If a user is not moving at all, a beacon can be emitted every 10 minutes. In the night, a sensor can be put into sleep mode to save energy, when a user is not likely to move at all for a relatively longer period of time. If a user is active for only eight hours in a day, we can put the sensor into sleep mode for the other 16 hours and thus save 2/3rd of the energy.
In addition, a sensor node can choose to not send any beacons during some time intervals. For example, suppose hiker A has communicated its witness information to three other hikers in the last five minutes. If it is running low on power, it can go to receive mode or sleep mode for the next ten minutes. It goes to receive mode if it is still willing to receive additional witness information from hikers that it encounters in the next ten minutes. It goes to sleep mode if it is extremely low on power.
The bandwidth and energy limitations of sensor nodes require that the amount of data transferred among the nodes be reduced to minimum. It has been observed that in some scenarios 3000 instructions could be executed for the same energy cost of sending a bit 100m by radio [15]. To reduce the amount of data transfer, CenWits employs a handshake protocol that two nodes execute when they encounter one another. The goal of this protocol is to ensure that a node transmits only as much witness information as the other node is willing to receive. This protocol is initiated when a node i receives a beacon containing the node ID of the sender node j and i has not exchanged witness information with j in the last δ time units. Assume that i < j. The protocol consists of four phases (See Figure 9):
number of witness records it has in its memory.
receive constraints and the number of witness records it has in its memory.
sends its witness information (filtered based on receive constraints received in phase II).
i, j sends its witness information (filtered based on receive constraints received in phase I). j <Constaints, Witness info size> <Constaints, Witness info size> <Filtered Witness info> <Filtered Witness info> i j j j i i i Figure 9: Four-Phase Hand Shake Protocol (i < j) Receive constraints are a function of memory and power.
In the most general case, they are comprised of the three parameters (record count, hop count and record gap) used for memory management. If i is low on memory, it specifies the maximum number of records it is willing to accept from j. Similarly, i can ask j to send only those records that have hop count value less than MAX HOP COUNT − 1.
Finally, i can include its MIN RECORD GAP value in its receive constraints. Note that the handshake protocol is beneficial to both i and j. They save memory by receiving only as much information as they are willing to accept and conserve energy by sending only as many witness records as needed.
It turns out that filtering witness records based on MIN RECORD GAP is complex. It requires that the witness records of any given node be arranged in an order sorted by their record time values. Maintaining this sorted order is complex in memory, because new witness records with the same node id can arrive later that may have to be inserted in between to preserve the sorted order. For this reason, the receive constraints in the current CenWits prototype do not include record gap.
Suppose i specifies a hop count value of 3. In this case, j checks the hop count field of every witness record before sending them. If the hop count value is greater than 3, the record is not transmitted.
To further reduce communication and increase the lifetime of our system, we introduce the notion of groups. The idea is based on the concept of abstract regions presented in [20]. A group is a set of n nodes that can be defined in terms of radio connectivity, geographic location, or other properties of nodes. All nodes within a group can communicate directly with one another and they share information to maintain their view of the external world. At any point in time, a group has exactly one leader that communicates 185 with external nodes on behalf of the entire group. A group can be static, meaning that the group membership does not change over the period of time, or it could be dynamic in which case nodes can leave or join the group. To make our analysis simple and to explain the advantages of group, we first discuss static groups.
A static group is formed at the start of a hiking trail or ski slope. Suppose there are five family members who want to go for a hike in the Rocky Mountain National Park. Before these members start their hike, each one of them is given a sensor node and the information is entered in the system that the five nodes form a group. Each group member is given a unique id and every group member knows about other members of the group. The group, as a whole, is also assigned an id to distinguish it from other groups in the system.
Figure 10: A group of five people. Node 2 is the group leader and it is communicating on behalf of the group with an external node 17. All other (shown in a lighter shade) are in sleep mode.
As the group moves through the trail, it exchanges information with other nodes or groups that it comes across. At any point in time, only one group member, called the leader, sends and receives information on behalf of the group and all other n − 1 group members are put in the sleep mode (See Figure 10). It is this property of groups that saves us energy. The group leadership is time-multiplexed among the group members. This is done to make sure that a single node does not run out of battery due to continuous exchange of information. Thus after every t seconds, the leadership is passed on to another node, called the successor, and the leader (now an ordinary member) is put to sleep. Since energy is dear, we do not implement an extensive election algorithm for choosing the successor. Instead, we choose the successor on the basis of node id. The node with the next highest id in the group is chosen as the successor. The last node, of course, chooses the node with the lowest id as its successor.
We now discuss the data storage schemes for groups.
Memory is a scarce resource in sensor nodes and it is therefore important that witness information be stored efficiently among group members. Efficient data storage is not a trivial task when it comes to groups. The tradeoff is between simplicity of the scheme and memory savings. A simpler scheme incurs lesser energy cost as compared to a more sophisticated scheme, but offers lesser memory savings as well. This is because in a more complicated scheme, the group members have to coordinate to update and store information. After considering a number of different schemes, we have come to a conclusion that there is no optimal storage scheme for groups. The system should be able to adapt according to its requirements. If group members are low on battery, then the group can adapt a scheme that is more energy efficient.
Similarly, if the group members are running out of memory, they can adapt a scheme that is more memory efficient. We first present a simple scheme that is very energy efficient but does not offer significant memory savings. We then present an alternate scheme that is much more memory efficient.
As already mentioned a group can receive information only through the group leader. Whenever the leader comes across an external node e, it receives information from that node and saves it. In our first scheme, when the timeslot for the leader expires, the leader passes this new information it received from e to its successor. This is important because during the next time slot, if the new leader comes across another external node, it should be able to pass information about all the external nodes this group has witnessed so far. Thus the information is fully replicated on all nodes to maintain the correct view of the world.
Our first scheme does not offer any memory savings but is highly energy efficient and may be a good choice when the group members are running low on battery. Except for the time when the leadership is switched, all n − 1 members are asleep at any given time. This means that a single member is up for t seconds once every n∗t seconds and therefore has to spend approximately only 1/nth of its energy. Thus, if there are 5 members in a group, we save 80% energy, which is huge. More energy can be saved by increasing the group size.
We now present an alternate data storage scheme that aims at saving memory at the cost of energy. In this scheme we divide the group into what we call partitions. Partitions can be thought of as subgroups within a group. Each partition must have at least two nodes in it. The nodes within a partition are called peers. Each partition has one peer designated as partition leader. The partition leader stays in receive mode at all times, while all others peers a partition stay in the sleep mode. Partition leadership is time-multiplexed among the peers to make sure that a single node does not run out of battery. Like before, a group has exactly one leader and the leadership is time-multiplexed among partitions. The group leader also serves as the partition leader for the partition it belongs to (See Figure 11).
In this scheme, all partition leaders participate in information exchange. Whenever a group comes across an external node e, every partition leader receives all witness information, but it only stores a subset of that information after filtering. Information is filtered in such a way that each partition leader has to store only B/K bytes of data, where K is the number of partitions and B is the total number of bytes received from e. Similarly when a group wants to send witness information to e, each partition leader sends only B/K bytes that are stored in the partition it belongs to. However, before a partition leader can send information, it must switch from receive mode to send mode. Also, partition leaders must coordinate with one another to ensure that they do not send their witness information at the same time, i.e. their message do not collide. All this is achieved by having the group leader send a signal to every partition leader in turn. 186 Figure 11: The figure shows a group of eight nodes divided into four partitions of 2 nodes each. Node
partition leaders. All other nodes are in the sleep mode.
Since the partition leadership is time-multiplexed, it is important that any information received by the partition leader, p1, be passed on to the next leader, p2. This has to be done to make sure that p2 has all the information that it might need to send when it comes across another external node during its timeslot. One way of achieving this is to wake p2 up just before p1"s timeslot expires and then have p1 transfer information only to p2. An alternate is to wake all the peers up at the time of leadership change, and then have p1 broadcast the information to all peers. Each peer saves the information sent by p1 and then goes back to sleep. In both cases, the peers send acknowledgement to the partition leader after receiving the information. In the former method, only one node needs to wake up at the time of leadership change, but the amount of information that has to be transmitted between the nodes increases as time passes. In the latter case, all nodes have to be woken up at the time of leadership change, but small piece of information has to be transmitted each time among the peers. Since communication is much more expensive than bringing the nodes up, we prefer the second method over the first one.
A group can be divided into partitions in more than one way. For example, suppose we have a group of six members.
We can divide this group into three partitions of two peers each, or two partitions with three peers each. The choice once again depends on the requirements of the system. A few big partitions will make the system more energy efficient.
This is because in this configuration, a greater number of nodes will stay in sleep mode at any given point in time.
On the other hand, several small partitions will make the system memory efficient, since each node will have to store lesser information (See Figure 12).
A group that is divided into partitions must be able to readjust itself when a node leaves or runs out of battery.
This is crucial because a partition must have at least two nodes at any point in time to tolerate failure of one node.
For example, in figure 3 (a), if node 2 or node 5 dies, the partition is left with only one node. Later on, if that single node in the partition dies, all witness information stored in that partition will be lost. We have devised a very simple protocol to solve this problem. We first explain how partiFigure 12: The figure shows two different ways of partitioning a group of six nodes. In (a), a group is divided into three partitions of two nodes. Node
leaders, and nodes 2, 3, and 6 are in sleep mode. In (b) the group is divided into two partitions of three nodes. Node 1 is the group leader, node 9 is the partition leader and nodes 2, 3, 5, and 6 are in sleep mode. tions are adjusted when a peer dies, and then explain what happens if a partition leader dies.
Suppose node 2 in figure 3 (a) dies. When node 5, the partition leader, sends information to node 2, it does not receive an acknowledgement from it and concludes that node
. At this point, node 5 contacts other partition leaders (nodes 1 ... 9) using a broadcast message and informs them that one of its peers has died. Upon hearing this, each partition leader informs node 5 (i) the number of nodes in its partition, (ii) a candidate node that node 5 can take if the number of nodes in its partition is greater than 2, and (iii) the amount of witness information stored in its partition. Upon hearing from every leader, node 5 chooses the candidate node from the partition with maximum number (must be greater than 2) of peers, and sends a message back to all leaders. Node 5 then sends data to its new peer to make sure that the information is replicated within the partition.
However, if all partitions have exactly two nodes, then node 5 must join another partition. It chooses the partition that has the least amount of witness information to join. It sends its witness information to the new partition leader.
Witness information and membership update is propagated to all peers during the next partition leadership change.
We now consider the case where the partition leader dies.
If this happens, then we wait for the partition leadership to change and for the new partition leader to eventually find out that a peer has died. Once the new partition leader finds out that it needs more peers, it proceeds with the protocol explained above. However, in this case, we do lose information that the previous partition leader might have received just before it died. This problem can be solved by implementing a more rigorous protocol, but we have decided to give up on accuracy to save energy.
Our current design uses time-division multiplexing to schedule wakeup and sleep modes in the sensor nodes. However, recent work on radio wakeup sensors [10] can be used to do this scheduling more efficiently. we plan to incorporate radio wakeup sensors in CenWits when the hardware is mature. 2 The algorithm to conclude that a node has died can be made more rigorous by having the partition leader query the suspected node a few times. 187
A sensor is constrained in the amount of memory and power. In general, the amount of memory needed and power consumption depends on a variety of factors such as node density, number of hiker encounters, and the number of access points. In this Section, we provide an estimate of how long the power of a MICA2 mote will last under certain assumtions.
First, we assume that each sensor node carries about 100 witness records. On encountering another hiker, a sensor node transmits 50 witness records and receives 50 new witness records. Since, each record is 16 bytes long, it will take
seconds to receive 50 records over a 19200 bps link. The power consumption of MICA2 due to CPU processing, transmission and reception are approximately 8.0 mA, 7.0 mA and
alkaline battery is 2500mAh.
Since the radio module of Mica2 is half-duplex and assuming that the CPU is always active when a node is awake, power consumption due to transmission is 8 + 8.5 = 16.5 mA per hour and due to reception is 8 + 7 = 15mA per hour. So, average power consumtion due to transmission and reception is (16.5 + 15)/2 = 15.75 mA per hour.
Given that the capacity of an alkaline battery is 2500 mAh, a battery should last for 2500/15.75 = 159 hours of transmission and reception. An encounter between two hikers results in exchange of about 50 witness records that takes about 0.68 seconds as calculated above. Thus, a single alkaline battery can last for (159 ∗ 60 ∗ 60)/0.68 = 841764 hiker encounters.
Assuming that a node emits a beacon every 90 seconds and a hiker encounter occurs everytime a beacon is emitted (worst-case scenario), a single alkaline battery will last for (841764 ∗ 90)/(30 ∗ 24 ∗ 60 ∗ 60) = 29 days. Since, a Mica2 is equipped with two batteries, a Mica2 sensor can remain operation for about two months. Notice that this calculation is preliminary, because it assumes that hikers are active
seconds. In a more realistic scenario, power is expected to last for a much longer time period. Also, this time period will significantly increase when groups of hikers are moving together.
Finally, the lifetime of a sensor running on two batteries can definitely be increased significantly by using energy scavenging techniques and energy harvesting techniques [16, 14].
We have implemented a prototype of CenWits on MICA2 sensor 900MHz running Mantis OS 0.9.1b. One of the sensor is equipped with MTS420CA GPS module, which is capable of barometric pressure and two-axis acceleration sensing in addition to GPS location tracking. We use SiRF, the serial communication protocol, to control GPS module. SiRF has a rich command set, but we record only X and Y coordinates.
A witness record is 16 bytes long. When a node starts up, it stores its current location and emits a beacon periodicallyin the prototype, a node emits a beacon every minute.
We have conducted a number of experiments with this prototype. A detailed report on these experiments with the raw data collected and photographs of hikers, access points etc. is available at http://csel.cs.colorado.edu/∼huangjh/ Cenwits.index.htm. Here we report results from three of them. In all these experiments, there are three access points (A, B and C) where nodes dump their witness information.
These access points also provide location information to the nodes that come with in their range. We first show how CenWits can be used to determine the hiking trail a hiker is most likely on and the speed at which he is hiking, and identify hot search areas in case he is reported missing. Next, we show the results of power and memory management techniques of CenWits in conserving power and memory of a sensor node in one of our experiments.
The first experiment is called Direct Contact. It is a very simple experiment in which a single hiker starts from A, goes to B and then C, and finally returns to A (See Figure 13). The goal of this experiment is to illustrate that CenWits can deduce the trail a hiker takes by processing witness information.
Figure 13: Direct Contact Experiment Node Id Record (X,Y) Location Hop Time Time Count
Table 1: Witness information collected in the direct contact experiment.
The witness information dumped at the three access points was then collected and processed at a control center. Part of the witness information collected at the control center is shown in Table 1. The X,Y locations in this table correspond to the location information provided by access points A, B, and C. A is located at (12,7), B is located at (31,17) and C is located at (12,23). Three encounter points (between hiker 1 and the three access points) extracted from 188 this witness information are shown in Figure 13 (shown in rectangular boxes). For example, A,1 at 16 means 1 came in contact with A at time 16. Using this information, we can infer the direction in which hiker 1 was moving and speed at which he was moving. Furthermore, given a map of hiking trails in this area, it is clearly possible to identify the hiking trail that hiker 1 took.
The second experiment is called Indirect Inference. This experiment is designed to illustrate that the location, direction and speed of a hiker can be inferred by CenWits, even if the hiker never comes in the range of any access point. It illustrates the importance of witness information in search and rescue applications. In this experiment, there are three hikers, 1, 2 and 3. Hiker 1 takes a trail that goes along access points A and B, while hiker 3 takes trail that goes along access points C and B. Hiker 2 takes a trail that does not come in the range of any access points. However, this hiker meets hiker 1 and 3 during his hike. This is illustrated in Figure 14.
Figure 14: Indirect Inference Experiment Node Id Record (X,Y) Location Hop Time Time Count
Table 2: Witness information collected from hiker 1 in indirect inference experiment.
Part of the witness information collected at the control center from access points A, B and C is shown in Tables
For example, the location time in some witness records is not the same as the record time. This means that the node that generated that record did not have its most up-to-date location at the encounter time. For example, when hikers
Node Id Record (X,Y) Location Hop Time Time Count
Table 3: Witness information collected from hiker 3 in indirect inference experiment. hiker 1 is (12,7) recorded at time 6. So, node 1 generates a witness record with record time 16, location (12,7) and location time 6. In fact, the last two records in Table 3 have (?,?) as their location. This has happened because these witness records were generate by hiker 2 during his encounter with 1 at time 15 and 16. Until this time, hiker
Interestingly, a more accurate location information of 1 and 2 encounter or 2 and 3 encounter can be computed by process the witness information at the control center. It took 25 units of time for hiker 1 to go from A (12,7) to B (31,17). Assuming a constant hiking speed and a relatively straight-line hike, it can be computed that at time 16, hiker
accurate location of encounter between 1 and 2.
Finally, our third experiment called Identifying Hot Search Areas is designed to determine the trail a hiker has taken and identify hot search areas for rescue after he is reported missing. There are six hikers (1, 2, 3, 4, 5 and 6) in this experiment. Figure 15 shows the trails that hikers 1, 2, 3, 4 and 5 took, along with the encounter points obtained from witness records collected at the control center. For brevity, we have not shown the entire witness information collected at the control center. This information is available at http://csel.cs.colorado.edu/∼huangjh/Cenwits/index.htm.
Figure 15: Identifying Hot Search Area Experiment (without hiker 6) 189 Now suppose hiker 6 is reported missing at time 260. To determine the hot search areas, the witness records of hiker
the speed at which he had been moving, direction in which he had been moving, and his last known location. Based on this information and the hiking trail map, hot search areas are identified. The hiking trail taken by hiker 6 as inferred by CenWits is shown by a dotted line and the hot search areas identified by CenWits are shown by dark lines inside the dotted circle in Figure 16.
Figure 16: Identifying Hot Search Area Experiment (with hiker 6)
Management The witness information shown in Tables 1, 2 and 3 has not been filtered using the three criteria described in Section 4.1. For example, the witness records generated by 3 at record times 76, 78 and 79 (see Table 3) have all been generated due a single contact between access point C and node
records will be erased. Similarly, the witness records generated by 1 at record times 10, 15 and 16 (see Table 1) have all been generated due a single contact between access point A and node 1. Again, by applying the record gap criteria, two of these three records will be erased. Our experiments did not generate enough data to test the impact of record count or hop count criteria.
To evaluate the impact of these criteria, we simulated CenWits to generate a significantly large number of records for a given number of hikers and access points. We generated witness records by having the hikers walk randomly. We applied the three criteria to measure the amount of memory savings in a sensor node. The results are shown in Table 4. The number of hikers in this simulation was 10 and the number of access points was 5. The number of witness records reported in this table is an average number of witness records a sensor node stored at the time of dump to an access point.
These results show that the three memory management criteria significantly reduces the memory consumption of sensor nodes in CenWits. For example, they can reduce MAX MIN MAX # of RECORD RECORD HOP Witness COUNT GAP COUNT Records
Table 4: Impact of memory management techniques. the memory consumption by up to 75%. However, these results are premature at present for two reasons: (1) They are generated via simulation of hikers walking at random; and (2) It is not clear what impact the erasing of witness records has on the accuracy of inferred location/hot search areas of lost hikers. In our future work, we plan to undertake a major study to address these two concerns.
In addition to the hiking in wilderness areas, CenWits can be used in several other applications, e.g. skiing, climbing, wild life monitoring, and person tracking. Since CenWits relies only on intermittent connectivity, it can take advantage of the existing cheap and mature technologies, and thereby make tracking cheaper and fairly accurate. Since CenWits doesn"t rely on keeping track of a sensor holder all time, but relies on maintaining witnesses, the system is relatively cheaper and widely applicable. For example, there are some dangerous cliffs in most ski resorts. But it is too expensive for a ski resort to deploy a connected wireless sensor network through out the mountain. Using CenWits, we can deploy some sensors at the cliff boundaries. These boundary sensors emit beacons quite frequently, e.g. every second, and so can record presence of skiers who cross the boundary and fall off the cliff. Ski patrols can cruise the mountains every hour, and automatically query the boundary sensor when in range using PDAs. If a PDA shows that a skier has been close to the boundary sensor, the ski patrol can use a long range walkie-talkie to query control center at the resort base to check the witness record of the skier. If there is no witness record after the recorded time in the boundary sensor, there is a high chance that a rescue is needed.
In wildlife monitoring, a very popular method is to attach a GPS receiver on the animals. To collect data, either a satellite transmitter is used, or the data collector has to wait until the GPS receiver brace falls off (after a year or so) and then search for the GPS receiver. GPS transmitters are very expensive, e.g. the one used in geese tracking is $3,000 each [2]. Also, it is not yet known if continuous radio signal is harmful to the birds. Furthermore, a GPS transmitter is quite bulky and uncomfortable, and as a result, birds always try to get rid of it. Using CenWits, not only can we record the presence of wildlife, we can also record the behavior of wild animals, e.g. lions might follow the migration of deers. CenWits does nor require any bulky and expensive satellite transmitters, nor is there a need to wait for a year and search for the braces. CenWits provides a very simple and cost-effective solution in this case. Also, access points 190 can be strategically located, e.g. near a water source, to increase chances of collecting up-to-date data. In fact, the access points need not be statically located. They can be placed in a low-altitude plane (e.g a UAV) and be flown over a wilderness area to collect data from wildlife.
In large cities, CenWits can be used to complement GPS, since GPS doesn"t work indoor and near skyscrapers. If a person A is reported missing, and from the witness records we find that his last contacts were C and D, we can trace an approximate location quickly and quite efficiently.
This paper presents a new search and rescue system called CenWits that has several advantages over the current search and rescue systems. These advantages include a looselycoupled system that relies only on intermittent network connectivity, power and storage efficiency, and low cost. It solves one of the greatest problems plaguing modern search and rescue systems: it has an inherent on-site storage capability. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well. It utilizes the concept of witnesses to propagate information, infer current possible location and speed of a subject, and identify hot search and rescue areas in case of emergencies.
A large part of CenWits design focuses on addressing the power and memory limitations of current sensor nodes. In fact, power and memory constraints depend on how much weight (of sensor node) a hiker is willing to carry and the cost of these sensors. An important goal of CenWits is build small chips that can be implanted in hiking boots or ski jackets. This goal is similar to the avalanche beacons that are currently implanted in ski jackets. We anticipate that power and memory will continue to be constrained in such an environment.
While the paper focuses on the development of a search and rescue system, it also provides some innovative, systemlevel ideas for information processing in a sensor network system.
We have developed and experimented with a basic prototype of CenWits at present. Future work includes developing a more mature prototype addressing important issues such as security, privacy, and high availability. There are several pressing concerns regarding security, privacy, and high availability in CenWits. For example, an adversary can sniff the witness information to locate endangered animals, females, children, etc. He may inject false information in the system. An individual may not be comfortable with providing his/her location and movement information, even though he/she is definitely interested in being located in a timely manner at the time of emergency. In general, people in hiking community are friendly and usually trustworthy.
So, a bullet-proof security is not really required. However, when CenWits is used in the context of other applications, security requirements may change. Since the sensor nodes used in CenWits are fragile, they can fail. In fact, the nature and level of security, privacy and high availability support needed in CenWits strongly depends on the application for which it is being used and the individual subjects involved.
Accordingly, we plan to design a multi-level support for security, privacy and high availability in CenWits.
So far, we have experimented with CenWits in a very restricted environment with a small number of sensors. Our next goal is to deploy this system in a much larger and more realistic environment. In particular, discussions are currenly underway to deploy CenWits in the Rocky Mountain and Yosemite National Parks.
[1] 802.11-based tracking system. http://www.pangonetworks.com/locator.htm. [2] Brent geese 2002. http://www.wwt.org.uk/brent/. [3] The onstar system. http://www.onstar.com. [4] Personal locator beacons with GPS receiver and satellite transmitter. http://www.aeromedix.com/. [5] Personal tracking using GPS and GSM system. http://www.ulocate.com/trimtrac.html. [6] Rf based kid tracking system. http://www.ion-kids.com/. [7] F. Alessio. Performance measurements with motes technology. MSWiM"04, 2004. [8] P. Bahl and V. N. Padmanabhan. RADAR: An in-building RF-based user location and tracking system. IEEE Infocom, 2000. [9] K. Fall. A delay-tolerant network architecture for challenged internets. In SIGCOMM, 2003. [10] L. Gu and J. Stankovic. Radio triggered wake-up capability for sensor networks. In Real-Time Applications Symposium, 2004. [11] J. Hightower and G. Borriello. Location systems for ubiquitous computing. IEEE Computer, 2001. [12] W. Jaskowski, K. Jedrzejek, B. Nyczkowski, and S. Skowronek. Lifetch life saving system. CSIDC, 2004. [13] P. Juang, H. Oki, Y. Wang, M. Martonosi, L. Peh, and D. Rubenstein. Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet. In ASPLOS, 2002. [14] K. Kansal and M. Srivastava. Energy harvesting aware power management. In Wireless Sensor Networks: A Systems Perspective, 2005. [15] G. J. Pottie and W. J. Kaiser. Embedding the internet: wireless integrated network sensors.
Communications of the ACM, 43(5), May 2000. [16] S. Roundy, P. K. Wright, and J. Rabaey. A study of low-level vibrations as a power source for wireless sensor networks. Computer Communications, 26(11),
[17] C. Savarese, J. M. Rabaey, and J. Beutel. Locationing in distributed ad-hoc wireless sensor networks.
ICASSP, 2001. [18] V. Shnayder, M. Hempstead, B. Chen, G. Allen, and M. Welsh. Simulating the power consumption of large-scale sensor network applications. In Sensys,

In a distributed multi-player game, players are normally distributed across the Internet and have varying delays to each other or to a central game server. Usually, in such games, the players are part of the game and in addition they may control entities that make up the game. During the course of the game, the players and the entities move within the game space. A player sends information about her movement as well as the movement of the entities she controls to the other players using a Dead-Reckoning (DR) vector.
A DR vector contains information about the current position of the player/entity in terms of x, y and z coordinates (at the time the DR vector was sent) as well as the trajectory of the entity in terms of the velocity component in each of the dimensions. Each of the participating players receives such DR vectors from one another and renders the other players/entities on the local consoles until a new DR vector is received for that player/entity. In a peer-to-peer game, players send DR vectors directly to each other; in a client-server game, these DR vectors may be forwarded through a game server.
The idea of DR is used because it is almost impossible for players/entities to exchange their current positions at every time unit.
DR vectors are quantization of the real trajectory (which we refer to as real path) at a player. Normally, a new DR vector is computed and sent whenever the real path deviates from the path extrapolated using the previous DR vector (say, in terms of distance in the x, y, z plane) by some amount specified by a threshold. We refer to the trajectory that can be computed using the sequence of DR vectors as the exported path. Therefore, at the sending player, there is a deviation between the real path and the exported path. The error due to this deviation can be removed if each movement of player/entity is communicated to the other players at every time unit; that is a DR vector is generated at every time unit thereby making the real and exported paths the same. Given that it is not feasible to satisfy this due to bandwidth limitations, this error is not of practical interest. Therefore, the receiving players can, at best, follow the exported path. Because of the network delay between the sending and receiving players, when a DR vector is received and rendered at a player, the original trajectory of the player/entity may have already changed. Thus, in physical time, there is a deviation at the receiving player between the exported path and the rendered trajectory (which we refer to as placed path). We refer to this error as the export error. Note that the export error, in turn, results in a deviation between the real and the placed paths.
The export error manifests itself due to the deviation between the exported path at the sender and the placed path at the receiver (i) 1 before the DR vector is received at the receiver (referred to as the before export error, and (ii) after the DR vector is received at the receiver (referred to as the after export error). In an earlier paper [1], we showed that by synchronizing the clocks at all the players and by using a technique based on time-stamping messages that carry the DR vectors, we can guarantee that the after export error is made zero. That is, the placed and the exported paths match after the DR vector is received. We also showed that the before export error can never be eliminated since there is always a non-zero network delay, but can be significantly reduced using our technique [1].
Henceforth we assume that the players use such a technique which results in unavoidable but small overall export error.
In this paper we consider the problem of different and varying network delays between each sender-receiver pair of a DR vector, and consequently, the different and varying export errors at the receivers. Due to the difference in the export errors among the receivers, the same entity is rendered at different physical time at different receivers. This brings in unfairness in game playing. For instance a player with a large delay would always see an entity late in physical time compared to the other players and, therefore, her action on the entity would be delayed (in physical time) even if she reacted instantaneously after the entity was rendered.
Our goal in this paper is to improve the fairness of these games in spite of the varying network delays by equalizing the export error at the players. We explore whether the time-average of the export errors (which is the cumulative export error over a period of time averaged over the time period) at all the players can be made the same by scheduling the sending of the DR vectors appropriately at the sender. We propose two algorithms to achieve this.
Both the algorithms are based on delaying (or dropping) the sending of DR vectors to some players on a continuous basis to try and make the export error the same at all the players. At an abstract level, the algorithm delays sending DR vectors to players whose accumulated error so far in the game is smaller than others; this would mean that the export error due to this DR vector at these players will be larger than that of the other players, thereby making them the same. The goal is to make this error at least approximately equal at every DR vector with the deviation in the error becoming smaller as time progresses.
The first algorithm (which we refer to as the scheduling algorithm) is based on estimating the delay between players and refining the sending of DR vectors by scheduling them to be sent to different players at different times at every DR generation point.
Through an implementation of this algorithm using the open source game BZflag, we show that this algorithm makes the game very fair (we measure fairness in terms of the standard deviation of the error). The drawback of this algorithm is that it tends to push the error of all the players towards that of the player with the worst error (which is the error at the farthest player, in terms of delay, from the sender of the DR). To alleviate this effect, we propose a budget based algorithm which budgets how the DRs are sent to different players. At a high level, the algorithm is based on the idea of sending more DRs to players who are farther away from the sender compared to those who are closer. Experimental results from BZflag illustrates that the budget based algorithm follows a more balanced approach. It improves the fairness of the game but at the same time does so without pushing up the mean error of the players thereby maintaining the accuracy of the game. In addition, the budget based algorithm is shown to achieve the same level of accuracy of game playing as the current implementation of BZflag using much less number of DR vectors.
Earlier work on network games to deal with network latency has mostly focussed on compensation techniques for packet delay and loss [2, 3, 4]. These methods are aimed at making large delays and message loss tolerable for players but does not consider the problems that may be introduced by varying delays from the server to different players or from the players to one another. For example, the concept of local lag has been used in [3] where each player delays every local operation for a certain amount of time so that remote players can receive information about the local operation and execute the same operation at the about same time, thus reducing state inconsistencies. The online multi-player game MiMaze [2, 5, 6], for example, takes a static bucket synchronization approach to compensate for variable network delays. In MiMaze, each player delays all events by 100 ms regardless of whether they are generated locally or remotely. Players with a network delay larger than 100 ms simply cannot participate in the game. In general, techniques based on bucket synchronization depend on imposing a worst case delay on all the players.
There have been a few papers which have studied the problem of fairness in a distributed game by more sophisticated message delivery mechanisms. But these works [7, 8] assume the existence of a global view of the game where a game server maintains a view (or state) of the game. Players can introduce objects into the game or delete objects that are already part of the game (for example, in a first-person shooter game, by shooting down the object). These additions and deletions are communicated to the game server using action messages. Based on these action messages, the state of the game is changed at the game server and these changes are communicated to the players using update messages. Fairness is achieved by ordering the delivery of action and update messages at the game server and players respectively based on the notion of a fair-order which takes into account the delays between the game server and the different players. Objects that are part of the game may move but how this information is communicated to the players seems to be beyond the scope of these works. In this sense, these works are very limited in scope and may be applicable only to firstperson shooter games and that too to only games where players are not part of the game.
DR vectors can be exchanged directly among the players (peerto-peer model) or using a central server as a relay (client-server model). It has been shown in [9] that multi-player games that use DR vectors together with bucket synchronization are not cheatproof unless additional mechanisms are put in place. Both the scheduling algorithm and the budget-based algorithm described in our paper use DR vectors and hence are not cheat-proof. For example, a receiver could skew the delay estimate at the sender to make the sender believe that the delay between the sender and the receiver is high thereby gaining undue advantage. We emphasize that the focus of this paper is on fairness without addressing the issue of cheating.
In the next section, we describe the game model that we use and illustrate how senders and receivers exchange DR vectors and how entities are rendered at the receivers based on the time-stamp augmented DR vector exchange as described in [1]. In Section 4, we describe the DR vector scheduling algorithm that aims to make the export error equal across the players with varying delays from the sender of a DR vector, followed by experimental results obtained from instrumentation of the scheduling algorithm on the open source game BZFlag. Section 5, describes the budget based algorithm that achieves improved fairness but without reducing the level accuracy of game playing. Conclusions are presented in Section 6. 2
The game architecture is based on players distributed across the Internet and exchanging DR vectors to each other. The DR vectors could either be sent directly from one player to another (peerto-peer model) or could be sent through a game server which receives the DR vector from a player and forwards it to other players (client-server model). As mentioned before, we assume synchronized clocks among the participating players.
Each DR vector sent from one player to another specifies the trajectory of exactly one player/entity. We assume a linear DR vector in that the information contained in the DR vector is only enough at the receiving player to compute the trajectory and render the entity in a straight line path. Such a DR vector contains information about the starting position and velocity of the player/entity where the velocity is constant1 . Thus, the DR vectors sent by a player specifies the current time at the player when the DR vector is computed (not the time at which this DR vector is sent to the other players as we will explain later), the current position of the player/entity in terms of the x, y, z coordinates and the velocity vector in the direction of x, y and z coordinates. Specifically, the ith DR vector sent by player j about the kth entity is denoted by DRj ik and is represented by the following tuple (Tj ik, xj ik, yj ik, zj ik, vxj ik, vyj ik, vzj ik).
Without loss of generality, in the rest of the discussion, we consider a sequence of DR vectors sent by only one player and for only one entity. For simplicity, we consider a two dimensional game space rather than a three dimensional one. Hence we use DRi to denote the ith such DR vector represented as the tuple (Ti, xi, yi, vxi, vyi). The receiving player computes the starting position for the entity based on xi, yi and the time difference between when the DR vector is received and the time Ti at which it was computed. Note that the computation of time difference is feasible since all the clocks are synchronized. The receiving player then uses the velocity components to project and render the trajectory of the entity. This trajectory is followed until a new DR vector is received which changes the position and/or velocity of the entity. timeT1 Real Exported Placed dt1 A B C D DR1 = (T1, x1, y1, vx1, vy1) computed at time T1 and sent to the receiver DR0 = (T0, x0, y0, vx0, vy0) computed at time T0 and sent to the receiver T0 dt0 Placed E Figure 1: Trajectories and deviations.
Based on this model, Figure 1 illustrates the sending and receiv1 Other type of DR vectors include quadratic DR vectors which specify the acceleration of the entity and cubic spline DR vectors that consider the starting position and velocity and the ending position and velocity of the entity. ing of DR vectors and the different errors that are encountered. The figure shows the reception of DR vectors at a player (henceforth called the receiver). The horizontal axis shows the time which is synchronized among all the players. The vertical axis tries to conceptually capture the two-dimensional position of an entity.
Assume that at time T0 a DR vector DR0 is computed by the sender and immediately sent to the receiver. Assume that DR0 is received at the receiver after a delay of dt0 time units. The receiver computes the initial position of the entity as (x0 + vx0 × dt0, y0 + vy0 × dt0) (shown as point E). The thick line EBD represents the projected and rendered trajectory at the receiver based on the velocity components vx0 and vy0 (placed path). At time T1 a DR vector DR1 is computed for the same entity and immediately sent to the receiver2 . Assume that DR1 is received at the receiver after a delay of dt1 time units. When this DR vector is received, assume that the entity is at point D. A new position for the entity is computed as (x1 + vx1 × dt1, y1 + vy0 × dt1) and the entity is moved to this position (point C). The velocity components vx1 and vy1 are used to project and render this entity further.
Let us now consider the error due to network delay. Although DR1 was computed at time T1 and sent to the receiver, it did not reach the receiver until time T1 + dt1. This means, although the exported path based on DR1 at the sender at time T1 is the trajectory AC, until time T1 + dt1, at the receiver, this entity was being rendered at trajectory BD based on DR0. Only at time T1 + dt1 did the entity get moved to point C from which point onwards the exported and the placed paths are the same. The deviation between the exported and placed paths creates an error component which we refer to as the export error. A way to represent the export error is to compute the integral of the distance between the two trajectories over the time when they are out of sync. We represent the integral of the distances between the placed and exported paths due to some DR DRi over a time interval [t1, t2] as Err(DRi, t1, t2). In the figure, the export error due to DR1 is computed as the integral of the distance between the trajectories AC and BD over the time interval [T1, T1 + dt1]. Note that there could be other ways of representing this error as well, but in this paper, we use the integral of the distance between the two trajectories as a measure of the export error. Note that there would have been an export error created due to the reception of DR0 at which time the placed path would have been based on a previous DR vector. This is not shown in the figure but it serves to remind the reader that the export error is cumulative when a sequence of DR vectors are received. Starting from time T1 onwards, there is a deviation between the real and the exported paths. As we discussed earlier, this export error is unavoidable.
The above figure and example illustrates one receiver only. But in reality, DR vectors DR0 and DR1 are sent by the sender to all the participating players. Each of these players receives DR0 and DR1 after varying delays thereby creating different export error values at different players. The goal of the DR vector scheduling algorithm to be described in the next section is to make this (cumulative) export error equal at every player independently for each of the entities that make up the game.
FORSENDING DR VECTORS In Section 3 we showed how delay from the sender of a new DR 2 Normally, DR vectors are not computed on a periodic basis but on an on-demand basis where the decision to compute a new DR vector is based on some threshold being exceeded on the deviation between the real path and the path exported by the previous DR vector. 3 vector to the receiver of the DR vector could lead to export error because of the deviation of the placed path from the exported path at the receiver until this new DR vector is received. We also mentioned that the goal of the DR vector scheduling algorithm is to make the export error equal at all receivers over a period of time.
Since the game is played in a distributed environment, it makes sense for the sender of an entity to keep track of all the errors at the receivers and try to make them equal. However, the sender cannot know the actual error at a receiver till it gets some information regarding the error back from the receiver. Our algorithm estimates the error to compute a schedule to send DR vectors to the receivers and corrects the error when it gets feedbacks from the receivers. In this section we provide motivations for the algorithm and describe the steps it goes through. Throughout this section, we will use the following example to illustrate the algorithm. timeT1 Exported path Placed path at receiver 2 dt1 A B C D E F T0 G2 G1 dt2 DR1 sent to receiver 1 DR1 sent to receiver 2 T1
2 da1 da2 G H I J K L N M DR1 estimated to be received by receiver 2 DR1 estimated to be received by receiver 1 DR1 actually received by receiver 1 DR1 actually received by receiver 2 DR0 sent to both receivers DR1 computed by sender Placed path at receiver 1 Figure 2: DR vector flow between a sender and two receivers and the evolution of estimated and actual placed paths at the receivers. DR0 = (T0, T0, x0, y0, vx0, vy0), sent at time T0 to both receivers. DR1 = (T1, T1
T1
sent at time T2
Consider the example in Figure 2. The figure shows a single sender sending DR vectors for an entity to two different receivers
sometime between T0 and T1 at which time they move the location of the entity to match the exported path. Thus, the path of the entity is shown only from the point where the placed path matches the exported path for DR0. Now consider DR1. At time T1, DR1 is computed by the sender but assume that it is not immediately sent to the receivers and is only sent after time δ1 to receiver 1 (at time T1
T2
timestamp with the DR vector as shown in the figure. Assume that the sender estimates (it will be clear shortly why the sender has to estimate the delay) that after a delay of dt1, receiver 1 will receive it, will use the coordinate and velocity parameters to compute the entity"s current location and move it there (point C) and from this time onwards, the exported and the placed paths will become the same. However, in reality, receiver 1 receives DR1 after a delay of da1 (which is less than sender"s estimates of dt1), and moves the corresponding entity to point H. Similarly, the sender estimates that after a delay of dt2, receiver 2 will receive DR1, will compute the current location of the entity and move it to that point (point E), while in reality it receives DR1 after a delay of da2 > dt2 and moves the entity to point N. The other points shown on the placed and exported paths will be used later in the discussion to describe different error components.
Referring back to the discussion from Section 3, from the sender"s perspective, the export error at receiver 1 due to DR1 is given by Err(DR1, T1, T1 + δ1 + dt1) (the integral of the distance between the trajectories AC and DB over the time interval [T1, T1 + δ1 + dt1]) of Figure 2. This is due to the fact that the sender uses the estimated delay dt1 to compute this error. Similarly, the export error from the sender"s perspective at received 2 due to DR1 is given by Err(DR1, T1, T1 + δ2 + dt2) (the integral of the distance between the trajectories AE and DF over the time interval [T1, T1 + δ2 + dt2]). Note that the above errors from the sender"s perspective are only estimates. In reality, the export error will be either smaller or larger than the estimated value, based on whether the delay estimate was larger or smaller than the actual delay that DR1 experienced. This difference between the estimated and the actual export error is the relative export error (which could either be positive or negative) which occurs for every DR vector that is sent and is accumulated at the sender.
The concept of relative export error is illustrated in Figure 2.
Since the actual delay to receiver 1 is da1, the export error induced by DR1 at receiver 1 is Err(DR1, T1, T1 + δ1 + da1).
This means, there is an error in the estimated export error and the sender can compute this error only after it gets a feedback from the receiver about the actual delay for the delivery of DR1, i.e., the value of da1. We propose that once receiver 1 receives DR1, it sends the value of da1 back to the sender. The receiver can compute this information as it knows the time at which DR1 was sent (T1
Figure 2) and the local receiving time (which is synchronized with the sender"s clock). Therefore, the sender computes the relative export error for receiver 1, represented using R1 as R1 = Err(DR1, T1, T1 + δ1 + dt1) − Err(DR1, T1, T1 + δ1 + da1) = Err(DR1, T1 + δ1 + dt1, T1 + δ1 + da1) Similarly the relative export error for receiver 2 is computed as R2 = Err(DR1, T1, T1 + δ2 + dt2) − Err(DR1, T1, T1 + δ2 + da2) = Err(DR1, T1 + δ2 + dt2, T1 + δ2 + da2) Note that R1 > 0 as da1 < dt1, and R2 < 0 as da2 > dt2.
Relative export errors are computed by the sender as and when it receives the feedback from the receivers. This example shows the 4 relative export error values after DR1 is sent and the corresponding feedbacks are received.
We now explain what we mean by making the errors equal at all the receivers and how this can be achieved. As stated before the sender keeps estimates of the delays to the receivers, dt1 and dt2 in the example of Figure 2. This says that at time T1 when DR1 is computed, the sender already knows how long it may take messages carrying this DR vector to reach the receivers. The sender uses this information to compute the export errors, which are Err(DR1, T1, T1 + δ1 + dt1) and Err(DR1, T1, T1 + δ2 + dt2) for receivers 1 and 2, respectively. Note that the areas of these error components are a function of δ1 and δ2 as well as the network delays dt1 and dt2. If we are to make the exports errors due to DR1 the same at both receivers, the sender needs to choose δ1 and δ2 such that Err(DR1, T1, T1 + δ1 + dt1) = Err(DR1, T1, T1 + δ2 + dt2).
But when T1 was computed there could already have been accumulated relative export errors due to previous DR vectors (DR0 and the ones before). Let us represent the accumulated relative error up to DRi for receiver j as Ri j. To accommodate these accumulated relative errors, the sender should now choose δ1 and δ2 such that R0
R0
The δi determines the scheduling instant of the DR vector at the sender for receiver i. This method of computation of δ"s ensures that the accumulated export error (i.e., total actual error) for each receiver equalizes at the transmission of each DR vector.
In order to establish this, assume that the feedback for DR vector Di from a receiver comes to the sender before schedule for Di+1 is computed. Let Si m and Ai m denote the estimated error for receiver m used for computing schedule for Di and accumulated error for receiver m computed after receiving feedback for Di, respectively.
Then Ri m = Ai m −Si m. In order to compute the schedule instances (i.e., δ"s) for Di, for any pair of receivers m and n, we do Ri−1 m + Si m = Ri−1 n + Si n. The following theorem establishes the fact that the accumulated export error is equalized at every scheduling instant.
THEOREM 4.1. When the schedule instances for sending Di are computed for any pair of receivers m and n, the following condition is satisfied: i−1 k=1 Ak m + Si m = i−1 k=1 Ak n + Si n.
Proof: By induction. Assume that the premise holds for some i.
We show that it holds for i+1. The base case for i = 1 holds since initially R0 m = R0 n = 0, and the S1 m = S1 n is used to compute the scheduling instances.
In order to compute the schedule for Di+1, the we first compute the relative errors as Ri m = Ai m − Si m, and Ri n = Ai n − Si n.
Then to compute δ"s we execute Ri m + Si+1 m = Ri n + Si+1 n Ai m − Si m + Si+1 m = Ai n − Si n + Si+1 n .
Adding the condition of the premise on both sides we get, i k=1 Ak m + Si+1 m = i k=1 Ak n + Si+1 n .
Let us now consider how the export errors can be computed.
From the previous section, to find δ1 and δ2 we need to find Err(DR1, T1, T1 +δ1 +dt1) and Err(DR1, T1, T1 +δ2 +dt2).
Note that the values of R0
Consider the computation of Err(DR1, T1, T1 +δ1 +dt1). This is the integral of the distance between the trajectories AC due to DR1 and BD due to DR0. From DR0 and DR1, point A is (X1, Y1) = (x1, y1) and point B is (X0, Y0) = (x0 + (T1 − T0) × vx0, y0 + (T1 − T0) × vy0). The trajectory AC can be represented as a function of time as (X1(t), Y1(t) = (X1 + vx1 × t, Y1 + vy1 × t) and the trajectory of BD can be represented as (X0(t), Y0(t) = (X0 + vx0 × t, Y0 + vy0 × t).
The distance between the two trajectories as a function of time then becomes, dist(t) = (X1(t) − X0(t))2 + (Y1(t) − Y0(t))2 = ((X1 − X0) + (vx1 − vx0)t)2 +((Y1 − Y0) + (vy1 − vy0)t)2 = ((vx1 − vx0)2 + (vy1 − vy0)2)t2 +2((X1 − X0)(vx1 − vx0) +(Y1 − Y0)(vy1 − vy0))t +(X1 − X0)2 + (Y1 − Y0)2 Let a = (vx1 − vx0)2 + (vy1 − vy0)2 b = 2((X1 − X0)(vx1 − vx0) +(Y1 − Y0)(vy1 − vy0)) c = (X1 − X0)2 + (Y1 − Y0)2 Then dist(t) can be written as dist(t) = a × t2 + b × t + c.
Then Err(DR1, t1, t2) for some time interval [t1, t2] becomes t2 t1 dist(t) dt = t2 t1 a × t2 + b × t + c dt.
A closed form solution for the indefinite integral a × t2 + b × t + c dt = (2at + b) √ at2 + bt + c 4a + 1 2 ln 1 2b + at √ a + at2 + bt + c c 1 √ a − 1 8 ln 1 2b + at √ a + at2 + bt + c b2 a− 3 2 Err(DR1, T1, T1 +δ1 +dt1) and Err(DR1, T1, T1 +δ2 +dt2) can then be calculated by applying the appropriate limits to the above solution. In the next section, we consider the computation of the δ"s for N receivers. 5
We again look at the computation of δ"s by referring to Figure 2.
The sender chooses δ1 and δ2 such that R0
δ1 +dt1) = R0
are zero, then δ1 and δ2 should be chosen such that Err(DR1, T1, T1+ δ1 +dt1) = Err(DR1, T1, T1 +δ2 +dt2). This equality will hold if δ1 + dt1 = δ2 + dt2. Thus, if there is no accumulated relative export error, all that the sender needs to do is to choose the δ"s in such a way that they counteract the difference in the delay to the two receivers, so that they receive the DR vector at the same time.
As discussed earlier, because the sender is not able to a priori learn the delay, there will always be an accumulated relative export error from a previous DR vector that does have to be taken into account.
To delve deeper into this, consider the computation of the export error as illustrated in the previous section. To compute the δ"s we require that R0
Err(DR1, T1, T1 + δ2 + dt2). That is,
R0
T1+δ1+dt1 T1 dist(t) dt = R0
T1+δ2+dt2 T1 dist(t) dt.
That is R0
T1+dt1 T1 dist(t) dt + T1+dt1+δ1 T1+dt1 dist(t) dt = R0
T1+dt2 T1 dist(t) dt + T1+dt2+δ2 T1+dt2 dist(t) dt.
The components R0 1, R0 2, are already known to (or estimated by) the sender. Further, the error components T1+dt1 T1 dist(t) dt and T1+dt2 T1 dist(t) dt can be a priori computed by the sender using estimated values of dt1 and dt2. Let us use E1 to denote R0
T1+dt1 T1 dist(t) dt and E2 to denote R0
T1+dt2 T1 dist(t) dt.
Then, we require that E1 + T1+dt1+δ1 T1+dt1 dist(t) dt = E2 + T1+dt2+δ2 T1+dt2 dist(t) dt.
Assume that E1 > E2. Then, for the above equation to hold, we require that T1+dt1+δ1 T1+dt1 dist(t) dt < T1+dt2+δ2 T1+dt2 dist(t) dt.
To make the game as fast as possible within this framework, the δ values should be made as small as possible so that DR vectors are sent to the receivers as soon as possible subject to the fairness requirement. Given this, we would choose δ1 to be zero and compute δ2 from the equation E1 = E2 + T1+dt2+δ2 T1+dt2 dist(t) dt.
In general, if there are N receivers 1, . . . , N, when a sender generates a DR vector and decides to schedule them to be sent, it first computes the Ei values for all of them from the accumulated relative export errors and estimates of delays. Then, it finds the smallest of these values. Let Ek be the smallest value. The sender makes δk to be zero and computes the rest of the δ"s from the equality Ei + T1+dti+δi T1+dti dist(t) dt = Ek, ∀i 1 ≤ i ≤ N, i = k. (1) The δ"s thus obtained gives the scheduling instants of the DR vector for the receivers.
For the purpose of the discussion below, as before let us denote the accumulated relative export error at a sender for receiver k up until DRi to be Ri k. Let us denote the scheduled delay at the sender before DRi is sent to receiver k as δi k. Given the above discussion, the algorithm steps are as follows:
computes δi k, and Ri−1 k , ∀k, 1 ≤ k ≤ N based on the estimation of delays dtk, ∀k, 1 ≤ k ≤ N as per Equation (1). It schedules, DRi to be sent to receiver k at time Ti + δi k.
times which are received after a delay of dak, ∀k, 1 ≤ k ≤ N where dak ≤ or > dtk. The receivers send the value of dak back to the sender (the receiver can compute this value based on the time stamps on the DR vector as described earlier).
k as described earlier and illustrated in Figure 2. The sender also recomputes (using exponential averaging method similar to round-trip time estimation by TCP [10]) the estimate of delay dtk from the new value of dak for receiver k.
and follow the steps of the algorithm to schedule and send this DR vector to the receivers.
So far we implicity assumed that DRi is sent out to all receivers before a decision is made to compute the next DR vector DRi+1, and the receivers send the value of dak corresponding to DRi and this information reaches the sender before it computes DRi+1 so that it can compute Ri+1 k and then use it in the computation of δi+1 k .
Two issues need consideration with respect to the above algorithm when it is used in practice. • It may so happen that a new DR vector is computed even before the previous DR vector is sent out to all receivers.
How will this situation be handled? • What happens if the feedback does not arrive before DRi+1 is computed and scheduled to be sent?
Let us consider the first scenario. We assume that DRi has been scheduled to be sent and the scheduling instants are such that δi
δi
N . Assume that DRi+1 is to be computed (because the real path has deviated exceeding a threshold from the path exported by DRi) at time Ti+1 where Ti + δi k < Ti+1 < Ti + δi k+1.
This means, DRi has been sent only to receivers up to k in the scheduled order. In our algorithm, in this case, the scheduled delay ordering queue is flushed which means DRi is not sent to receivers still queued to receive it, but a new scheduling order is computed for all the receivers to send DRi+1.
For those receivers who have been sent DRi, assume for now that daj, 1 ≤ j ≤ k has been received from all receivers (the scenario where daj has not been received will be considered as a part of the second scenario later). For these receivers, Ei j, 1 ≤ j ≤ k can be computed. For those receivers j, k + 1 ≤ j ≤ N to whom DRi was not sent Ei j does not apply. Consider a receiver j, k + 1 ≤ j ≤ N to whom DRi was not sent. Refer to Figure 3. For such a receiver j, when DRi+1 is to be scheduled and 6 timeTi Exported path dtj A B C D Ti-1 Gi j DRi+1 computed by sender and DRi for receiver k+1 to N is removed from queue DRi+1 scheduled for receiver k+1 Ti+1 G H E F DRi scheduled for receiver j DRi computed by sender Placed path at receiver k+1 Gi+1 j Figure 3: Schedule computation when DRi is not sent to receiver j, k + 1 ≤ j ≤ N. δi+1 j needs to be computed, the total export error is the accumulated relative export error at time Ti when schedule for DRi was computed, plus the integral of the distance between the two trajectories AC and BD of Figure 3 over the time interval [Ti, Ti+1 + δi+1 j + dtj]. Note that this integral is given by Err(DRi, Ti, Ti+1) + Err(DRi+1, Ti+1, Ti+1 + δi+1 j + dtj). Therefore, instead of Ei j of Equation (1), we use the value Ri−1 j + Err(DRi, Ti, Ti+1) + Err(DRi+1, Ti+1, Ti+1 + δi+1 j + dtj) where Ri−1 j is relative export error used when the schedule for DRi was computed.
Now consider the second scenario. Here the feedback dak corresponding to DRi has not arrived before DRi+1 is computed and scheduled. In this case, Ri k cannot be computed. Thus, in the computation of δk for DRi+1, this will be assumed to be zero. We do assume that a reliable mechanism is used to send dak back to the sender. When this information arrives at a later time, Ri k will be computed and accumulated to future relative export errors (for example Ri+1 k if dak is received before DRi+2 is computed) and used in the computation of δk when a future DR vector is to be scheduled (for example DRi+2).
In order to evaluate the effectiveness and quantify benefits obtained through the use of the scheduling algorithm, we implemented the proposed algorithm in BZFlag (Battle Zone Flag) [11] game.
It is a first-person shooter game where the players in teams drive tanks and move within a battle field. The aim of the players is to navigate and capture flags belonging to the other team and bring them back to their own area. The players shoot each other"s tanks using shooting bullets. The movement of the tanks as well as that of the shots are exchanged among the players using DR vectors.
We have modified the implementation of BZFlag to incorporate synchronized clocks among the players and the server and exchange time-stamps with the DR vector. We set up a testbed with four players running the instrumented version of BZFlag, with one as a sender and the rest as receivers. The scheduling approach and the base case where each DR vector was sent to all the receivers concurrently at every trigger point were implemented in the same run by tagging the DR vectors according to the type of approach used to send the DR vector. NISTNet [12] was used to introduce delays across the sender and the three receivers. Mean delays of 800ms, 500ms and 200ms were introduced between the sender and first, second and the third receiver, respectively. We introduce a variance of 100 msec (to the mean delay of each receiver) to model variability in delay. The sender logged the errors of each receiver every 100 milliseconds for both the scheduling approach and the base case. The sender also calculated the standard deviation and the mean of the accumulated export error of all the receivers every
of the accumulated export error of all the receivers in the scheduling case against the base case. Note that the x-axis of these graphs (and the other graphs that follow) represents the system time when the snapshot of the game was taken.
Observe that the standard deviation of the error with scheduling is much lower as compared to the base case. This implies that the accumulated errors of the receivers in the scheduling case are closer to one another. This shows that the scheduling approach achieves fairness among the receivers even if they are at different distances (i.e, latencies) from the sender.
Observe that the mean of the accumulated error increased multifold with scheduling in comparison to the base case. Further exploration for the reason for the rise in the mean led to the conclusion that every time the DR vectors are scheduled in a way to equalize the total error, it pushes each receivers total error higher. Also, as the accumulated error has an estimated component, the schedule is not accurate to equalize the errors for the receivers, leading to the DR vector reaching earlier or later than the actual schedule. In either case, the error is not equalized and if the DR vector reaches late, it actually increases the error for a receiver beyond the highest accumulated error. This means that at the next trigger, this receiver will be the one with highest error and every other receiver"s error will be pushed to this error value. This flip-flop effect leads to the increase in the accumulated error for all the receivers.
The scheduling for fairness leads to the decrease in standard deviation (i.e., increases the fairness among different players), but it comes at the cost of higher mean error, which may not be a desirable feature. This led us to explore different ways of equalizing the accumulated errors. The approach discussed in the following section is a heuristic approach based on the following idea. Using the same amount of DR vectors over time as in the base case, instead of sending the DR vectors to all the receivers at the same frequency as in the base case, if we can increase the frequency of sending the DR vectors to the receiver with higher accumulated error and decrease the frequency of sending DR vectors to the receiver with lower accumulated error, we can equalize the export error of all receivers over time. At the same time we wish to decrease the error of the receiver with the highest accumulated error in the base case (of course, this receiver would be sent more DR vectors than in the base case). We refer to such an algorithm as a budget based algorithm.
In a game, the sender of an entity sends DR vectors to all the receivers every time a threshold is crossed by the entity. Lower the threshold, more DR vectors are generated during a given time period. Since the DR vectors are sent to all the receivers and the network delay between the sender-receiver pairs cannot be avoided, the before export error 3 with the most distant player will always 3 Note that after export error is eliminated by using synchronized clock among the players. 7 0 1000 2000 3000 4000 5000
MeanAccumulatedError Time in Seconds Base Case Scheduling Algorithm #1 0 50 100 150 200 250 300 350 400 450 500
StandardDeviationofAccumulatedError Time in Seconds Base Case Scheduling Algorithm #1 Figure 4: Mean and standard deviation of error with scheduling and without (i.e., base case). be higher than the rest. In order to mitigate the imbalance in the error, we propose to send DR vectors selectively to different players based on the accumulated errors of these players. The budget based algorithm is based on this idea and there are two variations of it. One is a probabilistic budget based scheme and the other, a deterministic budget base scheme.
The probabilistic budget based scheme has three main steps: a) lower the dead reckoning threshold but at the same time keep the total number of DRs sent the same as the base case, b) at every trigger, probabilistically pick a player to send the DR vector to, and c) send the DR vector to the chosen player. These steps are described below.
The lowering of DR threshold is implemented as follows.
Lowering the threshold is equivalent to increasing the number of trigger points where DR vectors are generated. Suppose the threshold is such that the number of triggers caused by it in the base case is t and at each trigger n DR vectors sent by the sender, which results in a total of nt DR vectors. Our goal is to keep the total number of DR vectors sent by the sender fixed at nt, but lower the number of DR vectors sent at each trigger (i.e., do not send the DR vector to all the receivers). Let n and t be the number of DR vectors sent at each trigger and number of triggers respectively in the modified case. We want to ensure n t = nt. Since we want to increase the number of trigger points, i.e, t > t, this would mean that n < n.
That is, not all receivers will be sent the DR vector at every trigger.
In the probabilistic budget based scheme, at each trigger, a probability is calculated for each receiver to be sent a DR vector and only one receiver is sent the DR (n = 1). This probability is based on the relative weights of the receivers" accumulated errors. That is, a receiver with a higher accumulated error will have a higher probability of being sent the DR vector. Consider that the accumulated error for three players are a1, a2 and a3 respectively.
Then the probability of player 1 receiving the DR vector would be a1 a1+a2+a3 . Similarly for the other players. Once the player is picked, the DR vector is sent to that player.
To compare the probabilistic budget based algorithm with the base case, we needed to lower the threshold for the base case (for fair comparison). As the dead reckoning threshold in the base case was already very fine, it was decided that instead of lowering the threshold, the probabilistic budget based approach would be compared against a modified base case that would use the normal threshold as the budget based algorithm but the base case was modified such that every third trigger would be actually used to send out a DR vector to all the three receivers used in our experiments. This was called as the 1/3 base case as it resulted in 1/3 number of DR vectors being sent as compared to the base case.
The budget per trigger for the probability based approach was calculated as one DR vector at each trigger as compared to three DR vectors at every third trigger in the 1/3 base case; thus the two cases lead to the same number of DR vectors being sent out over time.
In order to evaluate the effectiveness of the probabilistic budget based algorithm, we instrumented the BZFlag game to use this approach. We used the same testbed consisting of one sender and three receivers with delays of 800ms, 500ms and 200ms from the sender and with low delay variance (100ms) and moderate delay variance (180ms). The results are shown in Figures 5 and 6. As mentioned earlier, the x-axis of these graphs represents the system time when the snapshot of the game was taken. Observe from the figures that the standard deviation of the accumulated error among the receivers with the probabilistic budget based algorithm is less than the 1/3 base case and the mean is a little higher than the 1/3 base case. This implies that the game is fairer as compared to the 1/3 base case at the cost of increasing the mean error by a small amount as compared to the 1/3 base case.
The increase in mean error in the probabilistic case compared to the 1/3 base case can be attributed to the fact that the even though the probabilistic approach on average sends the same number of DR vectors as the 1/3 base case, it sometimes sends DR vectors to a receiver less frequently and sometimes more frequently than the 1/3 base case due to its probabilistic nature. When a receiver does not receive a DR vector for a long time, the receiver"s trajectory is more and more off of the sender"s trajectory and hence the rate of buildup of the error at the receiver is higher. At times when a receiver receives DR vectors more frequently, it builds up error at a lower rate but there is no way of reversing the error that was built up when it did not receive a DR vector for a long time. This leads the receivers to build up more error in the probabilistic case as compared to the 1/3 base case where the receivers receive a DR vector almost periodically. 8 0 200 400 600 800 1000
MeanAccumulatedError Time in Seconds 1/3 Base Case Deterministic Algorithm Probabilistic Algorithm 0 50 100 150 200 250 300 350 400 450 500
StandardDeviationofAccumulatedError Time in Seconds 1/3 Base Case Deterministic Algorithm Probabilistic Algorithm Figure 5: Mean and standard deviation of error for different algorithms (including budget based algorithms) for low delay variance. 0 200 400 600 800 1000
MeanAccumulatedError Time in Seconds 1/3 Base Case Deterministic Algorithm Probabilistic Algorithm 0 50 100 150 200 250 300
StandardDeviationofAccumulatedError Time in Seconds 1/3 Base Case Deterministic Algorithm Probabilistic Algorithm Figure 6: Mean and standard deviation of error for different algorithms (including budget based algorithms) for moderate delay variance.
To bound the increase in mean error we decided to modify the budget based algorithm to be deterministic. The first two steps of the algorithm are the same as in the probabilistic algorithm; the trigger points are increased to lower the threshold and accumulated errors are used to compute the probability that a receiver will receiver a DR vector. Once these steps are completed, a deterministic schedule for the receiver is computed as follows:
the current trigger, the sender sends out the DR vector to the respective receiver(s). If at least one receiver was sent a DR vector, the sender calculates the probabilities of each receiver receiving a DR vector as explained before and follows steps
budget available at each trigger (which is set to 1 as explained below) to give the frequency of sending the DR vector to each receiver.
budget goes over 1, the receiver"s frequency is set as 1 and the surplus amount is equally distributed to all the receivers by adding the amount to their existing frequencies. This process is repeated until all the receivers have a frequency of less than or equal to 1. This is due to the fact that at a trigger we cannot send more than one DR vector to the respective receiver. That will be wastage of DR vectors by sending redundant information.
send DR vectors to the respective receiver. Credit obtained previously (explained in step 5) if any is subtracted from the schedule. Observe that the resulting value of the schedule might not be an integer; hence, the value is rounded off by taking the ceiling of the schedule. For example, if the frequency is 1/3.5, this implies that we would like to have a DR vector sent every 3.5 triggers. However, we are constrained to send it at the 4th trigger giving us a credit of 0.5. When we do send the DR vector next time, we would be able to send it 9 on the 3rd trigger because of the 0.5 credit.
schedule is the credit that the receiver has obtained which is remembered for the future and used at the next time as explained in step 4.
the current trigger, the receivers are tagged to receive the next DR vector at the trigger that happens exactly schedule (the ceiling of the schedule) number of times away from the current trigger. Observe that no other receiver"s schedule is modified at this point as they all are running a schedule calculated at some previous point of time. Those schedules will be automatically modified at the trigger when they are scheduled to receive the next DR vector. At the first trigger, the sender sends the DR vector to all the receivers and uses a relative probability of 1/n for each receiver and follows the steps 2 to 6 to calculate the next schedule for each receiver in the same way as mentioned for other triggers. This algorithm ensures that every receiver has a guaranteed schedule of receiving DR vectors and hence there is no irregularity in sending the DR vector to any receiver as was observed in the budget based probabilistic algorithm.
We used the testbed described earlier (three receivers with varying delays) to evaluate the deterministic algorithm using the budget of 1 DR vector per trigger so as to use the same number of DR vectors as in the 1/3 base case. Results from our experiments are shown in Figures 5 and 6. It can be observed that the standard deviation of error in the deterministic budget based algorithm is less than the 1/3 base case and also has the same mean error as the 1/3 base case. This indicates that the deterministic algorithm is more fair than the 1/3 base case and at the same time does not increase the mean error thereby leading to a better game quality compared to the probabilistic algorithm.
In general, when comparing the deterministic approach to the probabilistic approach, we found that the mean accumulated error was always less in the deterministic approach. With respect to standard deviation of the accumulated error, we found that in the fixed or low variance cases, the deterministic approach was generally lower, but in higher variance cases, it was harder to draw conclusions as the probabilistic approach was sometimes better than the deterministic approach.
In distributed multi-player games played across the Internet, object and player trajectory within the game space are exchanged in terms of DR vectors. Due to the variable delay between players, these DR vectors reach different players at different times. There is unfair advantage gained by receivers who are closer to the sender of the DR as they are able to render the sender"s position more accurately in real time. In this paper, we first developed a model for estimating the error in rendering player trajectories at the receivers. We then presented an algorithm based on scheduling the DR vectors to be sent to different players at different times thereby equalizing the error at different players. This algorithm is aimed at making the game fair to all players, but tends to increase the mean error of the players. To counter this effect, we presented budget based algorithms where the DR vectors are still scheduled to be sent at different players at different times but the algorithm balances the need for fairness with the requirement that the error of the worst case players (who are furthest from the sender) are not increased compared to the base case (where all DR vectors are sent to all players every time a DR vector is generated). We presented two variations of the budget based algorithms and through experimentation showed that the algorithms reduce the standard deviation of the error thereby making the game more fair and at the same time has comparable mean error to the base case.

Nowadays, many distributed multiplayer games adopt replicated architectures. In such games, the states of entities are changed not only by the operations of players, but also by the passing of time [1, 2]. These games are referred to as Continuous Distributed Multiplayer Games (CDMG). Like other distributed applications,
CDMG also suffer from the consistency problem caused by network transmission delay. Although new network techniques (e.g. QoS) can reduce or at least bound the delay, they can not completely eliminate it, as there exists the physical speed limitation of light, for instance, 100 ms is needed for light to propagate from Europe to Australia [3]. There are many studies about the effects of network transmission delay in different applications [4, 5, 6, 7]. In replication based games, network transmission delay makes the states of local and remote sites to be inconsistent, which can cause serious problems, such as reducing the fairness of a game and leading to paradoxical situations etc. In order to maintain consistency for distributed systems, many different approaches have been proposed, among which local lag and Dead-Reckoning (DR) are two representative approaches.
Mauve et al [1] proposed local lag to maintain high consistency for replicated continuous applications. It synchronizes the physical clocks of all sites in a system. After an operation is issued at local site, it delays the execution of the operation for a short time. During this short time period the operation is transmitted to remote sites, and all sites try to execute the operation at a same physical time. In order to tackle the inconsistency caused by exceptional network transmission delay, a time warp based mechanism is proposed to repair the state.
Local lag can achieve significant high consistency, but it is based on operation transmission, which forwards every operation on a shared entity to remote sites. Since operation transmission mechanism requests that all operations should be transmitted in a reliable way, message filtering is difficult to be deployed and the scalability of a system is limited.
DR is based on state transmission mechanism. In addition to the high fidelity model that maintains the accurate states of its own entities, each site also has a DR model that estimates the states of all entities (including its own entities). After each update of its own entities, a site compares the accurate state with the estimated one. If the difference exceeds a pre-defined threshold, a state update would be transmitted to all sites and all DR models would be corrected. Through state estimation, DR can not only maintain consistency but also decrease the number of transmitted state updates. Compared with aforementioned local lag, DR cannot maintain high consistency. Due to network transmission delay, when a remote site receives a state update of an entity the state of the entity might have changed at the site sending the state update.
In order to make DR maintain high consistency, Aggarwal et al [8] proposed Globally Synchronized DR (GS-DR), which synchronizes the physical clocks of all sites in a system and adds time stamps to transmitted state updates. Detailed description of GS-DR can be found in Section 3.
When a state update is available, GS-DR immediately updates the state of local site and then transmits the state update to remote sites, which causes the states of local site and remote sites to be inconsistent in the transmission procedure. Thus with the synchronization of physical clocks, GS-DR can eliminate after inconsistency, but it cannot tackle before inconsistency [8]. In this paper, we propose a new method named globally synchronized DR with Local Lag (GS-DR-LL), which combines local lag and GS-DR. By delaying the update to local site, GS-DR-LL can achieve higher consistency than GS-DR. The rest of this paper is organized as follows: Section 2 gives the definition of consistency and corresponding metrics; the cause of the inconsistency of DR is analyzed in Section 3; Section 4 describes how GS-DR-LL works; performance evaluation is presented in Section 5; Section
METRICS The consistency of replicated applications has already been well defined in discrete domain [9, 10, 11, 12], but few related work has been done in continuous domain. Mauve et al [1] have given a definition of consistency for replicated applications in continuous domain, but the definition is based on operation transmission and it is difficult for the definition to describe state transmission based methods (e.g. DR). Here, we present an alternative definition of consistency in continuous domain, which suits state transmission based methods well.
Given two distinct sites i and j, which have replicated a shared entity e, at a given time t, the states of e at sites i and j are Si(t) and Sj(t).
DEFINITION 1: the states of e at sites i and j are consistent at time t, iff: De(i, j, t) = |Si(t) - Sj(t)| = 0 (1) DEFINITION 2: the states of e at sites i and j are consistent between time t1 and t2 (t1 < t2), iff: De(i, j, t1, t2) = dt|)t(S)t(S| t2 t1 ji = 0 (2) In this paper, formulas (1) and (2) are used to determine whether the states of shared entities are consistent between local and remote sites. Due to network transmission delay, it is difficult to maintain the states of shared entities absolutely consistent.
Corresponding metrics are needed to measure the consistency of shared entities between local and remote sites.
De(i, j, t) can be used as a metric to measure the degree of consistency at a certain time point. If De(i, j, t1) > De(i, j, t2), it can be stated that between sites i and j, the consistency of the states of entity e at time point t1 is lower than that at time point t2.
If De(i, j, t) > De(l, k, t), it can be stated that, at time point t, the consistency of the states of entity e between sites i and j is lower than that between sites l and k.
Similarly, De(i, j, t1, t2) can been used as a metric to measure the degree of consistency in a certain time period. If De(i, j, t1, t2) > De(i, j, t3, t4) and |t1 - t2| = |t3 - t4|, it can be stated that between sites i and j, the consistency of the states of entity e between time points t1 and t2 is lower than that between time points t3 and t4. If De(i, j, t1, t2) > De(l, k, t1, t2), it can be stated that between time points t1 and t2, the consistency of the states of entity e between sites i and j is lower than that between sites l and k.
In DR, the states of entities are composed of the positions and orientations of entities and some prediction related parameters (e.g. the velocities of entities). Given two distinct sites i and j, which have replicated a shared entity e, at a given time point t, the positions of e at sites i and j are (xit, yit, zit) and (xjt, yjt, zjt), De(i, j, t) and D (i, j, t1, t2) could be calculated as: De(i, j, t) = )zz()yy()xx( jtit 2 jtit 2 jtit 2 (3) De(i, j, t1, t2) = dt)zz()yy()xx( 2t 1t jtit 2 jtit 2 jtit 2 (4) In this paper, formulas (3) and (4) are used as metrics to measure the consistency of shared entities between local and remote sites.
The inconsistency in DR can be divided into two sections by the time point when a remote site receives a state update. The inconsistency before a remote site receives a state update is referred to as before inconsistency, and the inconsistency after a remote site receives a state update is referred to as after inconsistency. Before inconsistency and after inconsistency are similar with the terms before export error and after export error [8].
After inconsistency is caused by the lack of synchronization between the physical clocks of all sites in a system. By employing physical clock synchronization, GS-DR can accurately calculate the states of shared entities after receiving state updates, and it can eliminate after inconsistency. Before inconsistency is caused by two reasons. The first reason is the delay of sending state updates, as local site does not send a state update unless the difference between accurate state and the estimated one is larger than a predefined threshold. The second reason is network transmission delay, as a shared entity can be synchronized only after remote sites receiving corresponding state update.
Figure 1. The paths of a shared entity by using GS-DR.
For example, it is assumed that the velocity of a shared entity is the only parameter to predict the entity"s position, and current position of the entity can be calculated by its last position and current velocity. To simplify the description, it is also assumed that there are only two sites i and j in a game session, site i acts as
local site and site j acts as remote site, and t1 is the time point the local site updates the state of the shared entity. Figure 1 illustrates the paths of the shared entity at local site and remote site in x axis by using GS-DR. At the beginning, the positions of the shared entity are the same at sites i and j and the velocity of the shared entity is 0. Before time point t0, the paths of the shared entity at sites i and j in x coordinate are exactly the same. At time point t0, the player at site i issues an operation, which changes the velocity in x axis to v0. Site i first periodically checks whether the difference between the accurate position of the shared entity and the estimated one, 0 in this case, is larger than a predefined threshold. At time point t1, site i finds that the difference is larger than the threshold and it sends a state update to site j. The state update contains the position and velocity of the shared entity at time point t1 and time point t1 is also attached as a timestamp. At time point t2, the state update reaches site j, and the received state and the time deviation between time points t1 and t2 are used to calculate the current position of the shared entity. Then site j updates its replicated entity"s position and velocity, and the paths of the shared entity at sites i and j overlap again.
From Figure 1, it can be seen that the after inconsistency is 0, and the before consistency is composed of two parts, D1 and D2. D1 is De(i, j, t0, t1) and it is caused by the state filtering mechanism of DR. D2 is De(i, j, t1, t2) and it is caused by network transmission delay.
WITH LOCAL LAG From the analysis in Section 3, It can be seen that GS-DR can eliminate after inconsistency, but it cannot effectively tackle before inconsistency. In order to decrease before inconsistency, we propose GS-DR-LL, which combines GS-DR with local lag and can effectively decrease before inconsistency.
In GS-DR-LL, the state of a shared entity at a certain time point t is notated as S = (t, pos, par 1, par 2, ……, par n), in which pos means the position of the entity and par 1 to par n means the parameters to calculate the position of the entity. In order to simplify the description of GS-DR-LL, it is assumed that there are only one shared entity and one remote site.
At the beginning of a game session, the states of the shared entity are the same at local and remote sites, with the same position p0 and parameters pars0 (pars represents all the parameters). Local site keeps three states: the real state of the entity Sreal, the predicted state at remote site Sp-remote, and the latest state updated to remote site Slate. Remote site keep only one state Sremote, which is the real state of the entity at remote site. Therefore, at the beginning of a game session Sreal = Sp-remote = Slate = Sremote = (t0, p0, pars0). In GS-DR-LL, it is assumed that the physical clocks of all sites are synchronized with a deviation of less than 50 ms (using NTP or GPS clock). Furthermore, it is necessary to make corrections to a physical clock in a way that does not result in decreasing the value of the clock, for example by slowing down or halting the clock for a period of time. Additionally it is assumed that the game scene is updated at a fixed frequency and T stands for the time interval between two consecutive updates, for example, if the scene update frequency is 50 Hz, T would be
current physical time.
After updating the scene, local site waits for a constant amount of time T. During this time period, local site receives the operations of the player and stores them in a list L. All operations in L are sorted by their issue time. At the end of time period T, local site executes all stored operations, whose issue time is between t - T and t, on Slate to get the new Slate, and it also executes all stored operations, whose issue time is between t - (n + T) and t - n, on Sreal to get the new Sreal. Additionally, local site uses Sp-remote and corresponding prediction methods to estimate the new Sp-remote.
After new Slate, Sreal, and Sp-remote are calculated, local site compares whether the difference between the new Slate and Spremote exceeds the predefined threshold. If YES, local site sends new Slate to remote site and Sp-remote is updated with new Slate. Note that the timestamp of the sent state update is t. After that, local site uses Sreal to update local scene and deletes the operations, whose issue time is less than t - n, from L.
After updating the scene, remote site waits for a constant amount of time T. During this time period, remote site stores received state update(s) in a list R. All state updates in R are sorted by their timestamps. At the end of time period T, remote site checks whether R contains state updates whose timestamps are less than t - n. Note that t is current physical time and it increases during the transmission of state updates. If YES, it uses these state updates and corresponding prediction methods to calculate the new Sremote, else they use Sremote and corresponding prediction methods to estimate the new Sremote. After that, local site uses Sremote to update local scene and deletes the sate updates, whose timestamps are less than t - n, from R.
From the above description, it can been see that the main difference between GS-DR and GS-DR-LL is that GS-DR-LL uses the operations, whose issue time is less than t - n, to calculate Sreal. That means that the scene seen by local player is the results of the operations issued a period of time (i.e. n) ago.
Meanwhile, if the results of issued operations make the difference between Slate and Sp-remote exceed a predefined threshold, corresponding state updates are sent to remote sites immediately.
The aforementioned is the basic mechanism of GS-DR-LL. In the case with multiple shared entities and remote sites, local site calculates Slate, Sreal, and Sp-remote for different shared entities respectively, if there are multiple Slate need to be transmitted, local site packets them in one state update and then send it to all remote sites.
Figure 2 illustrates the paths of a shared entity at local site and remote site while using GS-DR and GS-DR-LL. All conditions are the same with the conditions used in the aforementioned example describing GS-DR. Compared with t1, t2, and n, T (i.e. the time interval between two consecutive updates) is quite small and it is ignored in the following description.
At time point t0, the player at site i issues an operation, which changes the velocity of the shared entity form 0 to v0. By using GS-DR-LL, the results of the operation are updated to local scene at time point t0 + n. However the operation is immediately used to calculate Slate, thus in spite of GS-DR or GS-DR-LL, at time point t1 site i finds that the difference between accurate position and the estimated one is larger than the threshold and it sends a state update to site j. At time point t2, the state update is received by remote site j. Assuming that the timestamp of the state update is less than t - n, site j uses it to update local scene immediately.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 3 With GS-DR, the time period of before inconsistency is (t2 - t1) + (t1 - t0), whereas it decreases to (t2 - t1 - n) + (t1 - t0) with the help of GS-DR-LL. Note that t2 - t1 is caused by network transmission delay and t1 - t0 is caused by the state filtering mechanism of DR. If n is larger than t2 - t1, GS-DR-LL can eliminate the before inconsistency caused by network transmission delay, but it cannot eliminate the before inconsistency caused by the state filtering mechanism of DR (unless the threshold is set to 0). In highly interactive games, which request high consistency and GS-DR-LL might be employed, the results of operations are quite difficult to be estimated and a small threshold must be used. Thus, in practice, most before inconsistency is caused by network transmission delay and GS-DR-LL has the capability to eliminate such before inconsistency.
Figure 2. The paths of a shared entity by using GS-DR and GS-DR-LL.
To GS-DR-LL, the selection of lag value n is very important, and both network transmission delay and the effects of local lag on interaction should be considered. According to the results of HCI related researches, humans cannot perceive the delay imposed on a system when it is smaller than a specific value, and the specific value depends on both the system and the task. For example, in a graphical user interface a delay of approximately 150 ms cannot be noticed for keyboard interaction and the threshold increases to
uncritical for a car-racing game [5]. Thus if network transmission delay is less than the specific value of a game system, n can be set to the specific value. Else n can be set in terms of the effects of local lag on the interaction of a system [14]. In the case that a large n must be used, some HCI methods (e.g. echo [15]) can be used to relieve the negative effects of the large lag. In the case that n is larger than the network transmission delay, GS-DR-LL can eliminate most before inconsistency. Traditional local lag requests that the lag value must be larger than typical network transmission delay, otherwise state repairs would flood the system.
However GS-DR-LL allows n to be smaller than typical network transmission delay. In this case, the before inconsistency caused by network transmission delay still exists, but it can be decreased.
In order to evaluate GS-DR-LL and compare it with GS-DR in a real application, we had implemented both two methods in a networked game named spaceship [1]. Spaceship is a very simple networked computer game, in which players can control their spaceships to accelerate, decelerate, turn, and shoot spaceships controlled by remote players with laser beams. If a spaceship is hit by a laser beam, its life points decrease one. If the life points of a spaceship decrease to 0, the spaceship is removed from the game and the player controlling the spaceship loses the game.
In our practical implementation, GS-DR-LL and GS-DR coexisted in the game system, and the test bed was composed of two computers connected by 100 M switched Ethernet, with one computer acted as local site and the other acted as remote site. In order to simulate network transmission delay, a specific module was developed to delay all packets transmitted between the two computers in terms of a predefined delay value.
The main purpose of performance evaluation is to study the effects of GS-DR-LL on decreasing before inconsistency in a particular game system under different thresholds, lags, and network transmission delays. Two different thresholds were used in the evaluation, one is 10 pixels deviation in position or 15 degrees deviation in orientation, and the other is 4 pixels or 5 degrees. Six different combinations of lag and network transmission delay were used in the evaluation and they could be divided into two categories. In one category, the lag was fixed at
network transmission delay was fixed at 800 ms and three different lags (100 ms, 300 ms, and 500 ms) were used. Therefore the total number of settings used in the evaluation was 12 (2 × 6).
The procedure of performance evaluation was composed of three steps. In the first step, two participants were employed to play the game, and the operation sequences were recorded. Based on the records, a sub operation sequence, which lasted about one minute and included different operations (e.g. accelerate, decelerate, and turn), was selected. In the second step, the physical clocks of the two computers were synchronized first. Under different settings and consistency maintenance approaches, the selected sub operation sequence was played back on one computer, and it drove the two spaceships, one was local and the other was remote, to move. Meanwhile, the tracks of the spaceships on the two computers were recorded separately and they were called as a track couple. Since there are 12 settings and 2 consistency maintenance approaches, the total number of recorded track couples was 24. In the last step, to each track couple, the inconsistency between them was calculated, and the unit of inconsistency was pixel. Since the physical clocks of the two computers were synchronized, the calculation of inconsistency was quite simple. The inconsistency at a particular time point was the distance between the positions of the two spaceships at that time point (i.e. formula (3)).
In order to show the results of inconsistency in a clear way, only parts of the results, which last about 7 seconds, are used in the following figures, and the figures show almost the same parts of the results. Figures 3, 4, and 5 show the results of inconsistency when the lag is fixed at 300 ms and the network transmission delays are 100, 300, and 500 ms. It can been seen that inconsistency does exist, but in most of the time it is 0.
Additionally, inconsistency increases with the network transmission delay, but decreases with the threshold. Compared with GS-DR, GS-DR-LL can decrease more inconsistency, and it eliminates most inconsistency when the network transmission delay is 100 ms and the threshold is 4 pixels or 5 degrees.
According to the prediction and state filtering mechanisms of DR, inconsistency cannot be completely eliminated if the threshold is not 0. With the definitions of before inconsistency and after inconsistency, it can be indicated that GS-DR and GS-DR-LL both can eliminate after inconsistency, and GS-DR-LL can effectively decrease before inconsistency. It can be foreseen that with proper lag and threshold (e.g. the lag is larger than the network transmission delay and the threshold is 0), GS-DR-LL even can eliminate before inconsistency. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 3. Inconsistency when the network transmission delay is 100 ms and the lag is 300 ms. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 4. Inconsistency when the network transmission delay is 300 ms and the lag is 300 ms. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 5. Inconsistency when the network transmission delay is 500 ms and the lag is 300 ms.
Figures 6, 7, and 8 show the results of inconsistency when the network transmission delay is fixed at 800 ms and the lag are 100, 300, and 500 ms. It can be seen that with GS-DR-LL before inconsistency decreases with the lag. In traditional local lag, the lag must be set to a value larger than typical network transmission delay, otherwise the state repairs would flood the system. From the above results it can be seen that there does not exist any constraint on the selection of the lag, with GS-DR-LL a system would work fine even if the lag is much smaller than the network transmission delay.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 5 From all above results, it can be indicated that GS-DR and GSDR-LL both can eliminate after inconsistency, and GS-DR-LL can effectively decrease before inconsistency, and the effects increase with the lag. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 6. Inconsistency when the network transmission delay is 800 ms and the lag is 100 ms. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 7. Inconsistency when the network transmission delay is 800 ms and the lag is 300 ms. 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 10 pixels or 15degrees 0 10 20 30 40
Time (seconds) Inconsistency(pixels) GS-DR-LL GS-DR The threshold is 4 pixels or 5degrees Figure 8. Inconsistency when the network transmission delay is 800 ms and the lag is 500 ms.
Compared with traditional DR, GS-DR can eliminate after inconsistency through the synchronization of physical clocks, but it cannot tackle before inconsistency, which would significantly influence the usability and fairness of a game. In this paper, we proposed a method named GS-DR-LL, which combines local lag and GS-DR, to decrease before inconsistency through delaying updating the execution results of local operations to local scene.
Performance evaluation indicates that GS-DR-LL can effectively decrease before inconsistency, and the effects increase with the lag.
GS-DR-LL has significant implications to consistency maintenance approaches. First, GS-DR-LL shows that improved DR can not only eliminate after inconsistency but also decrease
before inconsistency, with proper lag and threshold, it would even eliminate before inconsistency. As a result, the application of DR can be greatly broadened and it could be used in the systems which request high consistency (e.g. highly interactive games).
Second, GS-DR-LL shows that by combining local lag and GSDR, the constraint on selecting lag value is removed and a lag, which is smaller than typical network transmission delay, could be used. As a result, the application of local lag can be greatly broadened and it could be used in the systems which have large typical network transmission delay (e.g. Internet based games).

In recent years, enterprises in the public and private sectors have provided access to large volumes of spatial data over the Internet. Interactive work with such large volumes of online spatial data is a challenging task. We have been developing an interactive browser for accessing spatial online databases: the SAND (Spatial and Non-spatial Data) Internet Browser. Users of this browser can interactively and visually manipulate spatial data remotely. Unfortunately, interactive remote access to spatial data slows to a crawl without proper data access mechanisms. We developed two separate methods for improving the system performance, together, form a dynamic network infrastructure that is highly scalable and provides a satisfactory user experience for interactions with large volumes of online spatial data.
The core functionality responsible for the actual database operations is performed by the server-based SAND system.
SAND is a spatial database system developed at the University of Maryland [12]. The client-side SAND Internet Browser provides a graphical user interface to the facilities of SAND over the Internet. Users specify queries by choosing the desired selection conditions from a variety of menus and dialog boxes.
SAND Internet Browser is Java-based, which makes it deployable across many platforms. In addition, since Java has often been installed on target computers beforehand, our clients can be deployed on these systems with little or no need for any additional software installation or customization. The system can start being utilized immediately without any prior setup which can be extremely beneficial in time-sensitive usage scenarios such as emergencies.
There are two ways to deploy SAND. First, any standard Web browser can be used to retrieve and run the client piece (SAND Internet Browser) as a Java application or an applet.
This way, users across various platforms can continuously access large spatial data on a remote location with little or 15 no need for any preceding software installation. The second option is to use a stand-alone SAND Internet Browser along with a locally-installed Internet-enabled database management system (server piece). In this case, the SAND Internet Browser can still be utilized to view data from remote locations. However, frequently accessed data can be downloaded to the local database on demand, and subsequently accessed locally. Power users can also upload large volumes of spatial data back to the remote server using this enhanced client.
We focused our efforts in two directions. We first aimed at developing a client-server architecture with efficient caching methods to balance local resources on one side and the significant latency of the network connection on the other. The low bandwidth of this connection is the primary concern in both cases. The outcome of this research primarily addresses the issues of our first type of usage (i.e., as a remote browser application or an applet) for our browser and other similar applications. The second direction aims at helping users that wish to manipulate large volumes of online data for prolonged periods. We have developed a centralized peerto-peer approach to provide the users with the ability to transfer large volumes of data (i.e., whole data sets to the local database) more efficiently by better utilizing the distributed network resources among active clients of a clientserver architecture. We call this architecture APPOINTApproach for Peer-to-Peer Oﬄoading the INTernet. The results of this research addresses primarily the issues of the second type of usage for our SAND Internet Browser (i.e., as a stand-alone application).
The rest of this paper is organized as follows. Section 2 describes our client-server approach in more detail. Section 3 focuses on APPOINT, our peer-to-peer approach. Section 4 discusses our work in relation to existing work. Section 5 outlines a sample SAND Internet Browser scenario for both of our remote access approaches. Section 6 contains concluding remarks as well as future research directions.
Traditionally, Geographic Information Systems (GIS) such as ArcInfo from ESRI [2] and many spatial databases are designed to be stand-alone products. The spatial database is kept on the same computer or local area network from where it is visualized and queried. This architecture allows for instantaneous transfer of large amounts of data between the spatial database and the visualization module so that it is perfectly reasonable to use large-bandwidth protocols for communication between them. There are however many applications where a more distributed approach is desirable. In these cases, the database is maintained in one location while users need to work with it from possibly distant sites over the network (e.g., the Internet). These connections can be far slower and less reliable than local area networks and thus it is desirable to limit the data flow between the database (server) and the visualization unit (client) in order to get a timely response from the system.
Our client-server approach (Figure 1) allows the actual database engine to be run in a central location maintained by spatial database experts, while end users acquire a Javabased client component that provides them with a gateway into the SAND spatial database engine.
Our client is more than a simple image viewer. Instead, it operates on vector data allowing the client to execute many operations such as zooming or locational queries locally. In Figure 1: SAND Internet Browser - Client-Server architecture. essence, a simple spatial database engine is run on the client.
This database keeps a copy of a subset of the whole database whose full version is maintained on the server. This is a concept similar to ‘caching". In our case, the client acts as a lightweight server in that given data, it evaluates queries and provides the visualization module with objects to be displayed. It initiates communication with the server only in cases where it does not have enough data stored locally.
Since the locally run database is only updated when additional or newer data is needed, our architecture allows the system to minimize the network traffic between the client and the server when executing the most common user-side operations such as zooming and panning. In fact, as long as the user explores one region at a time (i.e., he or she is not panning all over the database), no additional data needs to be retrieved after the initial population of the client-side database. This makes the system much more responsive than the Web mapping services. Due to the complexity of evaluating arbitrary queries (i.e., more complex queries than window queries that are needed for database visualization), we do not perform user-specified queries on the client. All user queries are still evaluated on the server side and the results are downloaded onto the client for display. However, assuming that the queries are selective enough (i.e., there are far fewer elements returned from the query than the number of elements in the database), the response delay is usually within reasonable limits.
As mentioned above, the SAND Internet Browser is a client piece of the remotely accessible spatial database server built around the SAND kernel. In order to communicate with the server, whose application programming interface (API) is a Tcl-based scripting language, a servlet specifically designed to interface the SAND Internet Browser with the SAND kernel is required on the server side. This servlet listens on a given port of the server for incoming requests from the client. It translates these requests into the SAND-Tcl language. Next, it transmits these SAND-Tcl commands or scripts to the SAND kernel. After results are provided by the kernel, the servlet fetches and processes them, and then sends those results back to the originating client.
Once the Java servlet is launched, it waits for a client to initiate a connection. It handles both requests for the actual client Java code (needed when the client is run as an applet) and the SAND traffic. When the client piece is launched, it connects back to the SAND servlet, the communication is driven by the client piece; the server only responds to the client"s queries. The client initiates a transaction by 6 sending a query. The Java servlet parses the query and creates a corresponding SAND-Tcl expression or script in the SAND kernel"s native format. It is then sent to the kernel for evaluation or execution. The kernel"s response naturally depends on the query and can be a boolean value, a number or a string representing a value (e.g., a default color) or, a whole tuple (e.g., in response to a nearest tuple query). If a script was sent to the kernel (e.g., requesting all the tuples matching some criteria), then an arbitrary amount of data can be returned by the SAND server. In this case, the data is first compressed before it is sent over the network to the client. The data stream gets decompressed at the client before the results are parsed.
Notice, that if another spatial database was to be used instead of the SAND kernel, then only a simple modification to the servlet would need to be made in order for the SAND Internet Browser to function properly. In particular, the queries sent by the client would need to be recoded into another query language which is native to this different spatial database. The format of the protocol used for communication between the servlet and the client is unaffected.
Many users may want to work on a complete spatial data set for a prolonged period of time. In this case, making an initial investment of downloading the whole data set may be needed to guarantee a satisfactory session. Unfortunately, spatial data tends to be large. A few download requests to a large data set from a set of idle clients waiting to be served can slow the server to a crawl. This is due to the fact that the common client-server approach to transferring data between the two ends of a connection assumes a designated role for each one of the ends (i.e, some clients and a server).
We built APPOINT as a centralized peer-to-peer system to demonstrate our approach for improving the common client-server systems. A server still exists. There is a central source for the data and a decision mechanism for the service. The environment still functions as a client-server environment under many circumstances. Yet, unlike many common client-server environments, APPOINT maintains more information about the clients. This includes, inventories of what each client downloads, their availabilities, etc.
When the client-server service starts to perform poorly or a request for a data item comes from a client with a poor connection to the server, APPOINT can start appointing appropriate active clients of the system to serve on behalf of the server, i.e., clients who have already volunteered their services and can take on the role of peers (hence, moving from a client-server scheme to a peer-to-peer scheme). The directory service for the active clients is still performed by the server but the server no longer serves all of the requests.
In this scheme, clients are used mainly for the purpose of sharing their networking resources rather than introducing new content and hence they help oﬄoad the server and scale up the service. The existence of a server is simpler in terms of management of dynamic peers in comparison to pure peerto-peer approaches where a flood of messages to discover who is still active in the system should be used by each peer that needs to make a decision. The server is also the main source of data and under regular circumstances it may not forward the service.
Data is assumed to be formed of files. A single file forms the atomic means of communication. APPOINT optimizes requests with respect to these atomic requests. Frequently accessed data sets are replicated as a byproduct of having been requested by a large number of users. This opens up the potential for bypassing the server in future downloads for the data by other users as there are now many new points of access to it. Bypassing the server is useful when the server"s bandwidth is limited. Existence of a server assures that unpopular data is also available at all times. The service depends on the availability of the server. The server is now more resilient to congestion as the service is more scalable.
Backups and other maintenance activities are already being performed on the server and hence no extra administrative effort is needed for the dynamic peers. If a peer goes down, no extra precautions are taken. In fact, APPOINT does not require any additional resources from an already existing client-server environment but, instead, expands its capability. The peers simply get on to or get off from a table on the server.
Uploading data is achieved in a similar manner as downloading data. For uploads, the active clients can again be utilized. Users can upload their data to a set of peers other than the server if the server is busy or resides in a distant location. Eventually the data is propagated to the server.
All of the operations are performed in a transparent fashion to the clients. Upon initial connection to the server, they can be queried as to whether or not they want to share their idle networking time and disk space. The rest of the operations follow transparently after the initial contact.
APPOINT works on the application layer but not on lower layers. This achieves platform independence and easy deployment of the system. APPOINT is not a replacement but an addition to the current client-server architectures. We developed a library of function calls that when placed in a client-server architecture starts the service. We are developing advanced peer selection schemes that incorporate the location of active clients, bandwidth among active clients, data-size to be transferred, load on active clients, and availability of active clients to form a complete means of selecting the best clients that can become efficient alternatives to the server.
With APPOINT we are defining a very simple API that could be used within an existing client-server system easily.
Instead of denial of service or a slow connection, this API can be utilized to forward the service appropriately. The API for the server side is: start(serverPortNo) makeFileAvailable(file,location,boolean) callback receivedFile(file,location) callback errorReceivingFile(file,location,error) stop() Similarly the API for the client side is: start(clientPortNo,serverPortNo,serverAddress) makeFileAvailable(file,location,boolean) receiveFile(file,location) sendFile(file,location) stop() The server, after starting the APPOINT service, can make all of the data files available to the clients by using the makeFileAvailable method. This will enable APPOINT to treat the server as one of the peers.
The two callback methods of the server are invoked when a file is received from a client, or when an error is encountered while receiving a file from a client. APPOINT guar7 Figure 2: The localization operation in APPOINT. antees that at least one of the callbacks will be called so that the user (who may not be online anymore) can always be notified (i.e., via email). Clients localizing large data files can make these files available to the public by using the makeFileAvailable method on the client side.
For example, in our SAND Internet Browser, we have the localization of spatial data as a function that can be chosen from our menus. This functionality enables users to download data sets completely to their local disks before starting their queries or analysis. In our implementation, we have calls to the APPOINT service both on the client and the server sides as mentioned above. Hence, when a localization request comes to the SAND Internet Browser, the browser leaves the decisions to optimally find and localize a data set to the APPOINT service. Our server also makes its data files available over APPOINT. The mechanism for the localization operation is shown with more details from the APPOINT protocols in Figure 2. The upload operation is performed in a similar fashion.
There has been a substantial amount of research on remote access to spatial data. One specific approach has been adopted by numerous Web-based mapping services (MapQuest [5], MapsOnUs [6], etc.). The goal in this approach is to enable remote users, typically only equipped with standard Web browsers, to access the company"s spatial database server and retrieve information in the form of pictorial maps from them. The solution presented by most of these vendors is based on performing all the calculations on the server side and transferring only bitmaps that represent results of user queries and commands. Although the advantage of this solution is the minimization of both hardware and software resources on the client site, the resulting product has severe limitations in terms of available functionality and response time (each user action results in a new bitmap being transferred to the client).
Work described in [9] examines a client-server architecture for viewing large images that operates over a lowbandwidth network connection. It presents a technique based on wavelet transformations that allows the minimization of the amount of data needed to be transferred over the network between the server and the client. In this case, while the server holds the full representation of the large image, only a limited amount of data needs to be transferred to the client to enable it to display a currently requested view into the image. On the client side, the image is reconstructed into a pyramid representation to speed up zooming and panning operations. Both the client and the server keep a common mask that indicates what parts of the image are available on the client and what needs to be requested. This also allows dropping unnecessary parts of the image from the main memory on the server.
Other related work has been reported in [16] where a client-server architecture is described that is designed to provide end users with access to a server. It is assumed that this data server manages vast databases that are impractical to be stored on individual clients. This work blends raster data management (stored in pyramids [22]) with vector data stored in quadtrees [19, 20].
For our peer-to-peer transfer approach (APPOINT),
Napster is the forefather where a directory service is centralized on a server and users exchange music files that they have stored on their local disks. Our application domain, where the data is already freely available to the public, forms a prime candidate for such a peer-to-peer approach. Gnutella is a pure (decentralized) peer-to-peer file exchange system.
Unfortunately, it suffers from scalability issues, i.e., floods of messages between peers in order to map connectivity in the system are required. Other systems followed these popular systems, each addressing a different flavor of sharing over the Internet. Many peer-to-peer storage systems have also recently emerged. PAST [18], Eternity Service [7], CFS [10], and OceanStore [15] are some peer-to-peer storage systems.
Some of these systems have focused on anonymity while others have focused on persistence of storage. Also, other approaches, like SETI@Home [21], made other resources, such as idle CPUs, work together over the Internet to solve large scale computational problems. Our goal is different than these approaches. With APPOINT, we want to improve existing client-server systems in terms of performance by using idle networking resources among active clients. Hence, other issues like anonymity, decentralization, and persistence of storage were less important in our decisions. Confirming the authenticity of the indirectly delivered data sets is not yet addressed with APPOINT. We want to expand our research, in the future, to address this issue.
From our perspective, although APPOINT employs some of the techniques used in peer-to-peer systems, it is also closely related to current Web caching architectures.
Squirrel [13] forms the middle ground. It creates a pure peer-topeer collaborative Web cache among the Web browser caches of the machines in a local-area network. Except for this recent peer-to-peer approach, Web caching is mostly a wellstudied topic in the realm of server/proxy level caching [8, 11, 14, 17]. Collaborative Web caching systems, the most relevant of these for our research, focus on creating either a hierarchical, hash-based, central directory-based, or multicast-based caching schemes. We do not compete with these approaches. In fact, APPOINT can work in tandem with collaborative Web caching if they are deployed together. We try to address the situation where a request arrives at a server, meaning all the caches report a miss.
Hence, the point where the server is reached can be used to take a central decision but then the actual service request can be forwarded to a set of active clients, i.e., the down8 load and upload operations. Cache misses are especially common in the type of large data-based services on which we are working. Most of the Web caching schemes that are in use today employ a replacement policy that gives a priority to replacing the largest sized items over smaller-sized ones. Hence, these policies would lead to the immediate replacement of our relatively large data files even though they may be used frequently. In addition, in our case, the user community that accesses a certain data file may also be very dispersed from a network point of view and thus cannot take advantage of any of the caching schemes. Finally, none of the Web caching methods address the symmetric issue of large data uploads.
FedStats [1] is an online source that enables ordinary citizens access to official statistics of numerous federal agencies without knowing in advance which agency produced them.
We are using a FedStats data set as a testbed for our work.
Our goal is to provide more power to the users of FedStats by utilizing the SAND Internet Browser. As an example, we looked at two data files corresponding to Environmental Protection Agency (EPA)-regulated facilities that have chlorine and arsenic, respectively. For each file, we had the following information available: EPA-ID, name, street, city, state, zip code, latitude, longitude, followed by flags to indicate if that facility is in the following EPA programs: Hazardous Waste, Wastewater Discharge, Air Emissions,
Abandoned Toxic Waste Dump, and Active Toxic Release.
We put this data into a SAND relation where the spatial attribute ‘location" corresponds to the latitude and longitude. Some queries that can be handled with our system on this data include:
participate in the Air Emissions program, and: (a) Lie in Georgia to Illinois, alphabetically. (b) Lie within Arkansas or 30 miles within its border. (c) Lie within 30 miles of the border of Arkansas (i.e., both sides of the border).
all EPA-regulated facilities that have chlorine and: (a) That are closer to it than to any other EPAregulated facility that has arsenic. (b) That participate in the Air Emissions program and are closer to it than to any other EPAregulated facility which has arsenic. In order to avoid reporting a particular facility more than once, we use our ‘group by EPA-ID" mechanism.
Figure 3 illustrates the output of an example query that finds all arsenic sites within a given distance of the border of Arkansas. The sites are obtained in an incremental manner with respect to a given point. This ordering is shown by using different color shades.
With this example data, it is possible to work with the SAND Internet Browser online as an applet (connecting to a remote server) or after localizing the data and then opening it locally. In the first case, for each action taken, the client-server architecture will decide what to ask for from the server. In the latter case, the browser will use the peerto-peer APPOINT architecture for first localizing the data.
An overview of our efforts in providing remote access to large spatial data has been given. We have outlined our approaches and introduced their individual elements. Our client-server approach improves the system performance by using efficient caching methods when a remote server is accessed from thin-clients. APPOINT forms an alternative approach that improves performance under an existing clientserver system by using idle client resources when individual users want work on a data set for longer periods of time using their client computers.
For the future, we envision development of new efficient algorithms that will support large online data transfers within our peer-to-peer approach using multiple peers simultaneously. We assume that a peer (client) can become unavailable at any anytime and hence provisions need to be in place to handle such a situation. To address this, we will augment our methods to include efficient dynamic updates. Upon completion of this step of our work, we also plan to run comprehensive performance studies on our methods.
Another issue is how to access data from different sources in different formats. In order to access multiple data sources in real time, it is desirable to look for a mechanism that would support data exchange by design. The XML protocol [3] has emerged to become virtually a standard for describing and communicating arbitrary data. GML [4] is an XML variant that is becoming increasingly popular for exchange of geographical data. We are currently working on making SAND XML-compatible so that the user can instantly retrieve spatial data provided by various agencies in the GML format via their Web services and then explore, query, or process this data further within the SAND framework. This will turn the SAND system into a universal tool for accessing any spatial data set as it will be deployable on most platforms, work efficiently given large amounts of data, be able to tap any GML-enabled data source, and provide an easy to use graphical user interface. This will also convert the SAND system from a research-oriented prototype into a product that could be used by end users for accessing, viewing, and analyzing their data efficiently and with minimum effort.

Today"s computing environments are characterized by an increasing number of powerful, wirelessly connected mobile devices. Users can move throughout an environment while carrying their computers with them and having remote access to information and services, anytime and anywhere. New situations appear, where the user"s context - for example his current location or nearby people - is more dynamic; computation does not occur at a single location and in a single context any longer, but comprises a multitude of situations and locations. This development leads to a new class of applications, which are aware of the context in which they run in and thus bringing virtual and real worlds together.
Motivated by this and the fact, that only a few studies have been done for supporting group communication in such computing environments [12], we have developed a system, which we refer to as Group Interaction Support System (GISS). It supports group interaction in mobile distributed computing environments in a way that group members need not to at the same place any longer in order to interact with each other or just to be aware of the others situation.
In the following subchapters, we will give a short overview on context aware computing and motivate its benefits for supporting group interaction. A software framework for developing contextsensitive applications is presented, which serves as middleware for GISS. Chapter 2 presents the architecture of GISS, and chapter
concepts of GISS in more detail. Chapter 5 gives a final summary of our work.
According to Merriam-Webster"s Online Dictionary1 , context is defined as the interrelated conditions in which something exists or occurs. Because this definition is very general, many approaches have been made to define the notion of context with respect to computing environments.
Most definitions of context are done by enumerating examples or by choosing synonyms for context. The term context-aware has been introduced first in [10] where context is referred to as location, identities of nearby people and objects, and changes to those objects. In [2], context is also defined by an enumeration of examples, namely location, identities of the people around the user, the time of the day, season, temperature etc. [9] defines context as the user"s location, environment, identity and time.
Here we conform to a widely accepted and more formal definition, which defines context as any information than can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves. [4] [4] identifies four primary types of context information (sometimes referred to as context dimensions), that are - with respect to characterizing the situation of an entity - more important than others. These are location, identity, time and activity, which can also be used to derive other sources of contextual information (secondary context types). For example, if we know a person"s identity, we can easily derive related information about this person from several data sources (e.g. day of birth or e-mail address).
According to this definition, [4] defines a system to be contextaware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user"s task. [4] also gives a classification of features for context-aware applications, which comprises presentation of information and services to a user, automatic execution of a service and tagging of context to information for later retrieval.
Figure 1. Layers of a context-aware system Context computing is based on two major issues, namely identifying relevant context (identity, location, time, activity) and using obtained context (automatic execution, presentation, tagging). In order to do this, there are a few layers between (see Figure 1). First, the obtained low-level context information has to be transformed, aggregated and interpreted (context transformation) and represented in an abstract context world model (context representation), either centralized or decentralized. Finally, the stored context information is used to trigger certain context events (context triggering). [7]
After these abstract and formal definitions about what context and context computing is, we will now focus on the main goal of this work, namely how the interaction of mobile group members can be supported by using context information.
In [6] we have identified organizational systems to be crucial for supporting mobile groups (see Figure 2). First, there has to be an Information and Knowledge Management System, which is capable of supporting a team with its information processing- and knowledge gathering needs. The next part is the Awareness System, which is dedicated to the perceptualisation of the effects of team activity. It does this by communicating work context, agenda and workspace information to the users. The Interaction Systems provide support for the communication among team members, either synchronous or asynchronous, and for the shared access to artefacts, such as documents. Mobility Systems deploy mechanisms to enable any-place access to team memory as well as the capturing and delivery of awareness information from and to any places. Finally yet importantly, the organisational innovation system integrates aspects of the team itself, like roles, leadership and shared facilities.
With respect to these five aspects of team support, we focus on interaction and partly cover mobility- and awareness-support.
Group interaction includes all means that enable group members to communicate freely with all the other members. At this point, the question how context information can be used for supporting group interaction comes up. We believe that information about the current situation of a person provides a surplus value to existing group interaction systems. Context information facilitates group interaction by allowing each member to be aware of the availability status or the current location of each other group member, which again makes it possible to form groups dynamically, to place virtual post-its in the real world or to determine which people are around.
Figure 2. Support for Mobile Groups [6] Most of today"s context-aware applications use location and time only, and location is referred to as a crucial type of context information [3]. We also see the importance of location information in mobile and ubiquitous environments, wherefore a main focus of our work is on the utilization of location information and information about users in spatial proximity.
Nevertheless, we believe that location, as the only used type of context information, is not sufficient to support group interaction, wherefore we also take advantage of the other three primary types, namely identity, time and activity. This provides a comprehensive description of a user"s current situation and thus enabling numerous means for supporting group interaction, which are described in detail in chapter 4.4.
When we look at the types of context information stated above, we can see that all of them are single user-centred, taking into account only the context of the user itself. We believe, that for the support of group interaction, the status of the group itself has also be taken into account. Therefore, we have added a fifth contextdimension group-context, which comprises more than the sum of the individual member"s contexts. Group context includes any information about the situation of a whole group, for example how many members a group currently has or if a certain group meets right now.
The Group Interaction Support System (GISS) uses the softwareframework introduced in [1], which serves as a middleware for developing context-sensitive applications. This so-called Context Framework is based on a distributed communication architecture and it supports different kinds of transport protocols and message coding mechanisms. 89 A main feature of the framework is the abstraction of context information retrieval via various sensors and its delivery to a level where no difference appears, for the application designer, between these different kinds of context retrieval mechanisms; the information retrieval is hidden from the application developer.
This is achieved by so-called entities, which describe objectse.g. a human user - that are important for a certain context scenario.
Entities express their functionality by the use of so-called attributes, which can be loaded into the entity. These attributes are complex pieces of software, which are implemented as Java classes. Typical attributes are encapsulations of sensors, but they can also be used to implement context services, for example to notify other entities about location changes of users.
Each entity can contain a collection of such attributes, where an entity itself is an attribute. The initial set of attributes an entity contains can change dynamically at runtime, if an entity loads or unloads attributes from the local storage or over the network. In order to load and deploy new attributes, an entity has to reference a class loader and a transport and lookup layer, which manages the lookup mechanism for discovering other entities and the transport. XML configuration files specify which initial set of entities should be loaded and which attributes these entities own.
The communication between entities and attributes is based on context events. Each attribute is able to trigger events, which are addressed to other attributes and entities respectively, independently on which physical computer they are running.
Among other things, and event contains the name of the event and a list of parameters delivering information about the event itself.
Related with this event-based architecture is the use of ECA (Event-Condition-Action)-rules for defining the behaviour of the context system. Therefore, every entity has a rule-interpreter, which catches triggered events, checks conditions associated with them and causes certain actions. These rules are referenced by the entity"s XML configuration. A rule itself is even able to trigger the insertion of new rules or the unloading of existing rules at runtime in order to change the behaviour of the context system dynamically.
To sum up, the context framework provides a flexible, distributed architecture for hiding low-level sensor data from high-level applications and it hides external communication details from the application developer. Furthermore, it is able to adapt its behaviour dynamically by loading attributes, entities or ECArules at runtime.
As GISS uses the Context Framework described in chapter 1.3 as middleware, every user is represented by an entity, as well as the central server, which is responsible for context transformation, context representation and context triggering (cf. Figure 1).
A main part of our work is about the automated acquisition of position information and its sensor-independent provision at application level. We do not only sense the current location of users, but also determine spatial proximities between them.
Developing the architecture, we focused on keeping the client as simple as possible and reducing the communication between client and server to a minimum.
Each client may have various location and/or proximity sensors attached, which are encapsulated by respective Context Framework-attributes (Sensor Encapsulation). These attributes are responsible for integrating native sensor-implementations into the Context Framework and sending sensor-dependent position information to the server. We consider it very important to support different types of sensors even at the same time, in order to improve location accuracy on the one hand, while providing a pervasive location-sensing environment with seamless transition between different location sensing techniques on the other hand.
All location- and proximity-sensors supported are represented by server-side context-attributes, which correspond to the client-side sensor encapsulation-attributes and abstract the sensor-dependent position information received from all users via the wireless network (sensor abstraction). This requires a context repository, where the mapping of diverse physical positions to standardized locations is stored.
The standardized location- and proximity-information of each user is then passed to the so-called Sensor Fusion-attributes, one for symbolic locations and a second one for spatial proximities. Their job is to merge location- and proximityinformation of clients, respectively, which is described in detail in Chapter 3.3. Every time the symbolic location of a user or the spatial proximity between two users changes, the Sensor Fusion-attributes notify the GISS Core-attribute, which controls the application.
Because of the abstraction of sensor-dependent position information, the system can easily be extended by additional sensors, just by implementing the (typically two) attributes for encapsulating sensors (some sensors may not need a client-side part), abstracting physical positions and observing the interface to GISS Core.
Figure 3. Architecture of the Group Interaction Support System (GISS) The GISS Core-attribute is the central coordinator of the application as it shows to the user. It not only serves as an interface to the location-sensing subsystem, but also collects further context information in other dimensions (time, identity or activity). 90 Every time a change in the context of one or more users is detected, GISS Core evaluates the effect of these changes on the user, on the groups he belongs to and on the other members of these groups. Whenever necessary, events are thrown to the affected clients to trigger context-aware activities, like changing the presentation of awareness information or the execution of services.
The client-side part of the application is kept as simple as possible. Furthermore, modular design was not only an issue on the sensor side but also when designing the user interface architecture. Thus, the complete user interface can be easily exchanged, if all of the defined events are taken into account and understood by the new interface-attribute.
The currently implemented user interface is split up in two parts, which are also represented by two attributes. The central attribute on client-side is the so-called Instant Messenger Encapsulation, which on the one hand interacts with the server through events and on the other hand serves as a proxy for the external application the user interface is built on.
As external application, we use an existing open source instant messenger - the ICQ2 -compliant Simple Instant Messenger (SIM)3 . We have chosen and instant messenger as front-end because it provides a well-known interface for most users and facilitates a seamless integration of group interaction support, thus increasing acceptance and ease of use. As the basic functionality of the instant messenger - to serve as a client in an instant messenger network - remains fully functional, our application is able to use the features already provided by the messenger. For example, the contexts activity and identity are derived from the messenger network as it is described later.
The Instant Messenger Encapsulation is also responsible for supporting group communication. Through the interface of the messenger, it provides means of synchronous and asynchronous communication as well as a context-aware reminder system and tools for managing groups and the own availability status.
The second part of the user interface is a visualisation of the user"s locations, which is implemented in the attribute Viewer.
The current implementation provides a two-dimensional map of the campus, but it can easily be replaced by other visualisations, a three-dimensional VRML-model for example. Furthermore, this visualisation is used to show the artefacts for asynchronous communication. Based on a floor plan-view of the geographical area the user currently resides in, it gives a quick overview of which people are nearby, their state and provides means to interact with them.
In the following chapters 3 and 4, we describe the location sensing-backend and the application front-end for supporting group interaction in more detail.
In the following chapter, we will introduce a location model, which is used for representing locations; afterwards, we will describe the integration of location- and proximity-sensors in 2 http://www.icq.com/ 3 http://sim-icq.sourceforge.net more detail. Finally, we will have a closer look on the fusion of location- and proximity-information, acquired by various sensors.
A location model (i.e. a context representation for the contextinformation location) is needed to represent the locations of users, in order to be able to facilitate location-related queries like given a location, return a list of all the objects there or given an object, return its current location. In general, there are two approaches [3,5]: symbolic models, which represent location as abstract symbols, and a geometric model, which represent location as coordinates.
We have chosen a symbolic location model, which refers to locations as abstract symbols like Room P111 or Physics Building, because we do not require geometric location data.
Instead, abstract symbols are more convenient for human interaction at application level. Furthermore, we use a symbolic location containment hierarchy similar to the one introduced in [11], which consists of top-level regions, which contain buildings, which contain floors, and the floors again contain rooms. We also distinguish four types, namely region (e.g. a whole campus), section (e.g. a building or an outdoor section), level (e.g. a certain floor in a building) and area (e.g. a certain room). We introduce a fifth type of location, which we refer to as semantic. These socalled semantic locations can appear at any level in the hierarchy and they can be nested, but they do not necessarily have a geographic representation. Examples for such semantic locations are tagged objects within a room (e.g. a desk and a printer on this desk) or the name of a department, which contains certain rooms.
Figure 4. Symbolic Location Containment Hierarchy The hierarchy of symbolic locations as well as the type of each position is stored in the context repository.
Our architecture supports two different kinds of sensors: location sensors, which acquire location information, and proximity sensors, which detect spatial proximities between users.
As described above, each sensor has a server- and in most cases a corresponding client-side-implementation, too. While the clientattributes (Sensor Abstraction) are responsible for acquiring low-level sensor-data and transmitting it to the server, the corresponding Sensor Encapsulation-attributes transform them into a uniform and sensor-independent format, namely symbolic locations and IDs of users in spatial proximity, respectively. 91 Afterwards, the respective attribute Sensor Fusion is being triggered with this sensor-independent information of a certain user, detected by a particular sensor. Such notifications are performed every time the sensor acquired new information.
Accordingly, Sensor Abstraction-attributes are responsible to detect when a certain sensor is no longer available on the client side (e.g. if it has been unplugged by the user) or when position respectively proximity could not be determined any longer (e.g.
RFID reader cannot detect tags) and notify the corresponding sensor fusion about this.
In order to sense physical positions, the Sensor Encapsulationattributes asynchronously transmit sensor-dependent position information to the server. The corresponding location Sensor Abstraction-attributes collect these physical positions delivered by the sensors of all users, and perform a repository-lookup in order to get the associated symbolic location. This requires certain tables for each sensor, which map physical positions to symbolic locations. One physical position may have multiple symbolic locations at different accuracy-levels in the location hierarchy assigned to, for example if a sensor covers several rooms. If such a mapping could be found, an event is thrown in order to notify the attribute Location Sensor Fusion about the symbolic locations a certain sensor of a particular user determined.
We have prototypically implemented three kinds of location sensors, which are based on WLAN (IEEE 802.11), Bluetooth and RFID (Radio Frequency Identification). We have chosen these three completely different sensors because of their differences concerning accuracy, coverage and administrative effort, in order to evaluate the flexibility of our system (see Table 1).
The most accurate one is an RFID sensor, which is based on an active RFID-reader. As soon as the reader is plugged into the client, it scans for active RFID tags in range and transmits their serial numbers to the server, where they are mapped to symbolic locations. We also take into account RSSI (Radio Signal Strength Information), which provides position accuracy of few centimetres and thus enables us to determine which RFID-tag is nearest. Due to this high accuracy, RFID is used for locating users within rooms. The administration is quite simple; once a new RFID tag is placed, its serial number simply has to be assigned to a single symbolic location. A drawback is the poor availability, which can be traced back to the fact that RFID readers are still very expensive.
The second one is an 802.11 WLAN sensor. Therefore, we integrated a purely software-based, commercial WLAN positioning system for tracking clients on the university campuswide WLAN infrastructure. The reached position accuracy is in the range of few meters and thus is suitable for location sensing at the granularity of rooms. A big disadvantage is that a map of the whole area has to be calibrated with measuring points at a distance of 5 meters each. Because most mobile computers are equipped with WLAN technology and the positioning-system is a software-only solution, nearly everyone is able to use this kind of sensor.
Finally, we have implemented a Bluetooth sensor, which detects Bluetooth tags (i.e. Bluetooth-modules with known position) in range and transmits them to the server that maps to symbolic locations. Because of the fact that we do not use signal strengthinformation in the current implementation, the accuracy is above
associated with several symbolic locations, according to the physical locations such a Bluetooth module covers. This leads to the disadvantage that the range of each Bluetooth-tag has to be determined and mapped to symbolic locations within this range.
Table 1. Comparison of implemented sensors Sensor Accuracy Coverage Administration RFID < 10 cm poor easy WLAN 1-4 m very well very timeconsuming Bluetooth ~ 10 m well time-consuming
Any sensor that is able to detect whether two users are in spatial proximity is referred to as proximity sensor. Similar to the location sensors, the Proximity Sensor Abstraction-attributes collect physical proximity information of all users and transform them to mappings of user-IDs.
We have implemented two types of proximity-sensors, which are based on Bluetooth on the one hand and on fused symbolic locations (see chapter 3.3.1) on the other hand.
The Bluetooth-implementation goes along with the implementation of the Bluetooth-based location sensor. The already determined Bluetooth MAC addresses in range of a certain client are being compared with those of all other clients, and each time the attribute Bluetooth Sensor Abstraction detects congruence, it notifies the proximity sensor fusion about this.
The second sensor is based on symbolic locations processed by Location Sensor Fusion, wherefore it does not need a client-side implementation. Each time the fused symbolic location of a certain user changes, it checks whether he is at the same symbolic location like another user and again notifies the proximity sensor fusion about the proximity between these two users. The range can be restricted to any level of the location containment hierarchy, for example to room granularity.
A currently unresolved issue is the incomparable granularity of different proximity sensors. For example, the symbolic locations at same level in the location hierarchy mostly do not cover the same geographic area.
Core of the location sensing subsystem is the sensor fusion. It merges data of various sensors, while coping with differences concerning accuracy, coverage and sample-rate. According to the two kinds of sensors described in chapter 3.2, we distinguish between fusion of location sensors on the one hand, and fusion of proximity sensors on the other hand.
The fusion of symbolic locations as well as the fusion of spatial proximities operates on standardized information (cf. Figure 3).
This has the advantage, that additional position- and proximitysensors can be added easily or the fusion algorithms can be replaced by ones that are more sophisticated. 92 Fusion is performed for each user separately and takes into account the measurements at a single point in time only (i.e. no history information is used for determining the current location of a certain user). The algorithm collects all events thrown by the Sensor Abstraction-attributes, performs fusion and triggers the GISS Core-attribute if the symbolic location of a certain user or the spatial proximity between users changed.
An important feature is the persistent storage of location- and proximity-history in a database in order to allow future retrieval.
This enables applications to visualize the movement of users for example.
Goal of the fusion of location information is to improve precision and accuracy by merging the set of symbolic locations supplied by various location sensors, in order to reduce the number of these locations to a minimum, ideally to a single symbolic location per user. This is quite difficult, because different sensors may differ in accuracy and sample rate as well.
The Location Sensor Fusion-attribute is triggered by events, which are thrown by the Location Sensor Abstractionattributes. These events contain information about the identity of the user concerned, his current location and the sensor by which the location has been determined.
If the attribute Location Sensor Fusion receives such an event, it checks if the amount of symbolic locations of the user concerned has changed (compared with the last event). If this is the case, it notifies the GISS Core-attribute about all symbolic locations this user is currently associated with.
However, this information is not very useful on its own if a certain user is associated with several locations. As described in chapter 3.2.1, a single location sensor may deliver multiple symbolic locations. Moreover, a certain user may have several location sensors, which supply symbolic locations differing in accuracy (i.e. different levels in the location containment hierarchy). To cope with this challenge, we implemented a fusion algorithm in order to reduce the number of symbolic locations to a minimum (ideally to a single location).
In a first step, each symbolic location is associated with its number of occurrences. A symbolic location may occur several times if it is referred to by more than one sensor or if a single sensor detects multiple tags, which again refer to several locations. Furthermore, this number is added to the previously calculated number of occurrences of each symbolic location, which is a child-location of the considered one in the location containment hierarchy. For example, if - in Figure 4 - room2 occurs two times and desk occurs a single time, the value 2 of room2 is added to the value 1 of desk, whereby desk finally gets the value 3. In a final step, only those symbolic locations are left which are assigned with the highest number of occurrences.
A further reduction can be achieved by assigning priorities to sensors (based on accuracy and confidence) and cumulating these priorities for each symbolic location instead of just counting the number of occurrences.
If the remaining fused locations have changed (i.e. if they differ from the fused locations the considered user is currently associated with), they are provided with the current timestamp, written to the database and the GISS-attribute is notified about where the user is probably located.
Finally, the most accurate, common location in the location hierarchy is calculated (i.e. the least upper bound of these symbolic locations) in order to get a single symbolic location. If it changes, the GISS Core-attribute is triggered again.
Proximity sensor fusion is much simpler than the fusion of symbolic locations. The corresponding proximity sensor fusionattribute is triggered by events, which are thrown by the Proximity Sensor Abstraction-attributes. These special events contain information about the identity of the two users concerned, if they are currently in spatial proximity or if proximity no longer persists, and by which proximity-sensor this has been detected.
If the sensor fusion-attribute is notified by a certain Proximity Sensor Abstraction-attribute about an existing spatial proximity, it first checks if these two users are already known to be in proximity (detected either by another user or by another proximity-sensor of the user, which caused the event). If not, this change in proximity is written to the context repository with current timestamp. Similarly, if the attribute Proximity Fusion is notified about an ended proximity, it checks if the users are still known to be in proximity, and writes this change to the repository if not.
Finally, if spatial proximity between the two users actually changed, an event is thrown to notify the GISS Core-attribute about this.
In most of today"s systems supporting interaction in groups, the provided means lack any awareness of the user"s current context, thus being unable to adapt to his needs.
In our approach, we use context information to enhance interaction and provide further services, which offer new possibilities to the user. Furthermore, we believe that interaction in groups also has to take into account the current context of the group itself and not only the context of individual group members. For this reason, we also retrieve information about the group"s current context, derived from the contexts of the group members together with some sort of meta-information (see chapter 4.3).
The sources of context used for our application correspond with the four primary context types given in chapter 1.1 - identity (I), location (L), time (T) and activity (A). As stated before, we also take into account the context of the group the user is interaction with, so that we could add a fifth type of context informationgroup awareness (G) - to the classification. Using this context information, we can trigger context-aware activities in all of the three categories described in chapter 1.1 - presentation of information (P), automatic execution of services (A) and tagging of context to information for later retrieval (T).
Table 2 gives an overview of activities we have already implemented; they are described comprehensively in chapter 4.4.
The table also shows which types of context information are used for each activity and the category the activity could be classified in. 93 Table 2. Classification of implemented context-aware activities Service L T I A G P A T Location Visualisation X X X Group Building Support X X X X Support for Synchronous Communication X X X X Support for Asynchronous Communication X X X X X X X Availability Management X X X Task Management Support X X X X Meeting Support X X X X X X Reasons for implementing these very features are to take advantage of all four types of context information in order to support group interaction by utilizing a comprehensive knowledge about the situation a single user or a whole group is in.
A critical issue for the user acceptance of such a system is the usability of its interface. We have evaluated several ways of presenting context-aware means of interaction to the user, until we came to the solution we use right now. Although we think that the user interface that has been implemented now offers the best trade-off between seamless integration of features and ease of use, it would be no problem to extend the architecture with other user interfaces, even on different platforms.
The chosen solution is based on an existing instant messenger, which offers several possibilities to integrate our system (see chapter 4.2). The biggest advantage of this approach is that the user is confronted with a graphical user interface he is already used to in most cases. Furthermore, our system uses an instant messenger account as an identifier, so that the user does not have to register a further account anywhere else (for example, the user can use his already existing ICQ2 -account).
Our system is based upon an existing instant messenger, the socalled Simple Instant Messenger (SIM)3 . The implementation of this messenger is carried out as a project at Sourceforge4 .
SIM supports multiple messenger protocols such as AIM5 , ICQ2 and MSN6 . It also supports connections to multiple accounts at the same time. Furthermore, full support for SMS-notification (where provided from the used protocol) is given.
SIM is based on a plug-in concept. All protocols as well as parts of the user-interface are implemented as plug-ins. Its architecture is also used to extend the application"s abilities to communicate with external applications. For this purpose, a remote control plug-in is provided, by which SIM can be controlled from external applications via socket connection. This remote control interface is extensively used by GISS for retrieving the contact list, setting the user"s availability-state or sending messages. The 4 http://sourceforge.net/ 5 http://www.aim.com/ 6 http://messenger.msn.com/ functionality of the plug-in was extended in several ways, for example to accept messages for an account (as if they would have been sent via the messenger network).
The messenger, more exactly the contact list (i.e. a list of profiles of all people registered with the instant messenger, which is visualized by listing their names as it can be seen in Figure 5), is also used to display locations of other members of the groups a user belongs to. This provides location awareness without taking too much space or requesting the user"s full attention. A more comprehensive description of these features is given in chapter
While the location-context of a user is obtained from our location sensing subsystem described in chapter 3, we consider further types of context than location relevant for the support of group interaction, too.
Local time as a very important context dimension can be easily retrieved from the real time clock of the user"s system. Besides location and time, we also use context information of user"s activity and identity, where we exploit the functionality provided by the underlying instant messenger system. Identity (or more exactly, the mapping of IDs to names as well as additional information from the user"s profile) can be distilled out of the contents of the user"s contact list.
Information about the activity or a certain user is only available in a very restricted area, namely the activity at the computer itself.
Other activities like making a phone call or something similar, cannot be recognized with the current implementation of the activity sensor. The only context-information used is the instant messenger"s availability state, thus only providing a very coarse classification of the user"s activity (online, offline, away, busy etc.). Although this may not seem to be very much information, it is surely relevant and can be used to improve or even enable several services.
Having collected the context information from all available users, it is now possible to distil some information about the context of a certain group. Information about the context of a group includes how many members the group currently has, if the group meets right now, which members are participating at a meeting, how many members have read which of the available posts from other team members and so on.
Therefore, some additional information like a list of members for each group is needed. These lists can be assembled manually (by users joining and leaving groups) or retrieved automatically. The context of a group is secondary context and is aggregated from the available contexts of the group members. Every time the context of a single group member changes, the context of the whole group is changing and has to be recalculated.
With knowledge about a user"s context and the context of the groups he belongs to, we can provide several context-aware services to the user, which enhance his interaction abilities. A brief description of these services is given in chapter 4.4. 94
An important feature is the visualisation of location information, thus allowing users to be aware of the location of other users and members of groups he joined, respectively.
As already described in chapter 2, we use two different forms of visualisation. The maybe more important one is to display location information in the contact list of the instant messenger, right beside the name, thus being always visible while not drawing the user"s attention on it (compared with a twodimensional view for example, which requires a own window for displaying a map of the environment).
Due to the restricted space in the contact list, it has been necessary to implement some sort of level-of-detail concept. As we use a hierarchical location model, we are able to determine the most accurate common location of two users. In the contact list, the current symbolic location one level below the previously calculated common location is then displayed. If, for example, user A currently resides in room P121 at the first floor of a building and user B, which has to be displayed in the contact list of user A, is in room P304 at the third floor, the most accurate common location of these two users is the building they are in.
For that reason, the floor (i.e. one level beyond the common location, namely the building) of user B is displayed in the contact list of user A. If both people reside on the same floor or even in the same room, the room would be taken.
Figure 5 shows a screenshot of the Simple Instant Messenger3 , where the current location of those people, whose location is known by GISS, is displayed in brackets right beside their name.
On top of the image, the heightened, integrated GISS-toolbar is shown, which currently contains the following, implemented functionality (from left to right): Asynchronous communication for groups (see chapter 4.4.4), context-aware reminders (see chapter 4.4.6), two-dimensional visualisation of locationinformation, forming and managing groups (see chapter 4.4.2), context-aware availability-management (see chapter 4.4.5) and finally a button for terminating GISS.
Figure 5. GISS integration in Simple Instant Messenger3 As displaying just this short form of location may not be enough for the user, because he may want to see the most accurate position available, a fully qualified position is shown if a name in the contact-list is clicked (e.g. in the form of desk@room2@department1@1stfloor@building 1@campus).
The second possible form of visualisation is a graphical one. We have evaluated a three-dimensional view, which was based on a VRML model of the respective area (cf. Figure 6). Due to lacks in navigational and usability issues, we decided to use a twodimensional view of the floor (it is referred to as level in the location hierarchy, cf. Figure 4). Other levels of granularity like section (e.g. building) and region (e.g. campus) are also provided.
In this floor-plan-based view, the current locations are shown in the manner of ICQ2 contacts, which are placed at the currently sensed location of the respective person. The availability-status of a user, for example away if he is not on the computer right now, or busy if he does not want to be disturbed, is visualized by colour-coding the ICQ2 -flower left beside the name. Furthermore, the floor-plan-view shows so-called the virtual post-its, which are virtual counterparts of real-life post-its and serve as our means of asynchronous communication (more about virtual post-its can be found in chapter 4.4.4).
Figure 6. 3D-view of the floor (VRML) Figure 7 shows the two-dimensional map of a certain floor, where several users are currently located (visualized by their name and the flower left beside). The location of the client, on which the map is displayed, is visualized by a green circle. Down to the right, two virtual post-its can be seen.
Figure 7. 2D view of the floor Another feature of the 2D-view is the visualisation of locationhistory of users. As we store the complete history of a user"s locations together with a timestamp, we are able to provide information about the locations he has been back in time. When the mouse is moved over the name of a certain user in the 2Dview, footprints of a user, placed at the locations he has been, are faded out the stronger, the older the location information is. 95
To support interaction in groups, it is first necessary to form groups. As groups can have different purposes, we distinguish two types of groups.
So-called static groups are groups, which are built up manually by people joining and leaving them. Static groups can be further divided into two subtypes. In open static groups, everybody can join and leave anytime, useful for example to form a group of lecture attendees of some sort of interest group. Closed static groups have an owner, who decides, which persons are allowed to join, although everybody could leave again at any time. Closed groups enable users for example to create a group of their friends, thus being able to communicate with them easily.
In contrast to that, we also support the creation of dynamic groups. They are formed among persons, who are at the same location at the same time. The creation of dynamic groups is only performed at locations, where it makes sense to form groups, for example in lecture halls or meeting rooms, but not on corridors or outdoor. It would also be not very meaningful to form a group only of the people residing in the left front sector of a hall; instead, the complete hall should be considered. For these reasons, all the defined locations in the hierarchy are tagged, whether they allow the formation of groups or not. Dynamic groups are also not only formed granularity of rooms, but also on higher levels in the hierarchy, for example with the people currently residing in the area of a department.
As the members of dynamic groups constantly change, it is possible to create an open static group out of them.
The most important form of synchronous communication on computers today is instant messaging; some people even see instant messaging to be the real killer application on the Internet.
This has also motivated the decision to build GISS upon an instant messaging system.
In today"s messenger systems, peer-to-peer-communication is extensively supported. However, when it comes to communication in groups, the support is rather poor most of the time. Often, only sending a message to multiple recipients is supported, lacking means to take into account the current state of the recipients. Furthermore, groups can only be formed of members in one"s contact list, thus being not able to send messages to a group, where not all of its members are known (which may be the case in settings, where the participants of a lecture form a group).
Our approach does not have the mentioned restrictions. We introduce group-entries in the user"s contact list; enable him or his to send messages to this group easily, without knowing who exactly is currently a member of this group. Furthermore, group messages are only delivered to persons, who are currently not busy, thus preventing a disturbance by a message, which is possibly unimportant for the user.
These features cannot be carried out in the messenger network itself, so whenever a message to a group account is sent, we intercept it and route it through our system to all the recipients, which are available at a certain time. Communication via a group account is also stored centrally, enabling people to query missed messages or simply viewing the message history.
Asynchronous communication in groups is not a new idea. The goal of this approach is not to reinvent the wheel, as email is maybe the most widely used form of asynchronous communication on computers and is broadly accepted and standardized. In out work, we aim at the combination of asynchronous communication with location awareness.
For this reason, we introduce the concept of so-called virtual postits (cp. [13]), which are messages that are bound to physical locations. These virtual post-its could be either visible for all users that are passing by or they can be restricted to be visible for certain groups of people only. Moreover, a virtual post-it can also have an expiry date after which it is dropped and not displayed anymore. Virtual post-its can also be commented by others, thus providing some from of forum-like interaction, where each post-it forms a thread.
Virtual post-its are displayed automatically, whenever a user (available) passes by the first time. Afterwards, post-its can be accessed via the 2D-viewer, where all visible post-its are shown.
All readers of a post-it are logged and displayed when viewing it, providing some sort of awareness about the group members" activities in the past.
Instant messengers in general provide some kind of availability information about a user. Although this information can be only defined in a very coarse granularity, we have decided to use these means of gathering activity context, because the introduction of an additional one would strongly decrease the usability of the system.
To support the user managing his availability, we provide an interface that lets the user define rules to adapt his availability to the current context. These rules follow the form on event (E) if condition (C) then action (A), which is directly supported by the ECA-rules of the Context Framework described in chapter 1.3.
The testing of conditions is periodically triggered by throwing events (whenever the context of a user changes). The condition itself is defined by the user, who can demand the change of his availability status as the action in the rule. As a condition, the user can define his location, a certain time (also triggering daily, every week or every month) or any logical combination of these criteria.
Reminders [14] are used to give the user the opportunity of defining tasks and being reminded of those, when certain criteria are fulfilled. Thus, a reminder can be seen as a post-it to oneself, which is only visible in certain cases. Reminders can be bound to a certain place or time, but also to spatial proximity of users or groups. These criteria can be combined with Boolean operators, thus providing a powerful means to remind the user of tasks that he wants to carry out when a certain context occurs.
A reminder will only pop up the first time the actual context meets the defined criterion. On showing up the reminder, the user has the chance to resubmit it to be reminded again, for example five minutes later or the next time a certain user is in spatial proximity. 96
Group Meetings With the available context information, we try to recognize meetings of a group. The determination of the criteria, when the system recognizes a group having a meeting, is part of the ongoing work. In a first approach, we use the location- and activity-context of the group members to determine a meeting.
Whenever more than 50 % of the members of a group are available at a location, where a meeting is considered to make sense (e.g. not on a corridor), a meeting minutes post-it is created at this location and all absent group members are notified of the meeting and the location it takes place.
During the meeting, the comment-feature of virtual post-its provides a means to take notes for all of the participants. When members are joining or leaving the meeting, this is automatically added as a note to the list of comments.
Like the recognition of the beginning of a meeting, the recognition of its end is still part of ongoing work. If the end of the meeting is recognized, all group members get the complete list of comments as a meeting protocol at the end of the meeting.
This paper discussed the potentials of support for group interaction by using context information. First, we introduced the notions of context and context computing and motivated their value for supporting group interaction.
An architecture is presented to support context-aware group interaction in mobile, distributed environments. It is built upon a flexible and extensible framework, thus enabling an easy adoption to available context sources (e.g. by adding additional sensors) as well as the required form of representation.
We have prototypically developed a set of services, which enhance group interaction by taking into account the current context of the users as well as the context of groups itself.
Important features are dynamic formation of groups, visualization of location on a two-dimensional map as well as unobtrusively integrated in an instant-messenger, asynchronous communication by virtual post-its, which are bound to certain locations, and a context-aware availability-management, which adapts the availability-status of a user to his current situation.
To provide location information, we have implemented a subsystem for automated acquisition of location- and proximityinformation provided by various sensors, which provides a technology-independent presentation of locations and spatial proximities between users and merges this information using sensor-independent fusion algorithms. A history of locations as well as of spatial proximities is stored in a database, thus enabling context history-based services.
[1] Beer, W., Christian, V., Ferscha, A., Mehrmann, L.
Modeling Context-aware Behavior by Interpreted ECA Rules. In Proceedings of the International Conference on Parallel and Distributed Computing (EUROPAR"03). (Klagenfurt, Austria, August 26-29, 2003). Springer Verlag,
LNCS 2790, 1064-1073. [2] Brown, P.J., Bovey, J.D., Chen X. Context-Aware Applications: From the Laboratory to the Marketplace.
IEEE Personal Communications, 4(5) (1997), 58-64. [3] Chen, H., Kotz, D. A Survey of Context-Aware Mobile Computing Research. Technical Report TR2000-381,
Computer Science Department, Dartmouth College,
Hanover, New Hampshire, November 2000. [4] Dey, A. Providing Architectural Support for Building Context-Aware Applications. Ph.D. Thesis, Department of Computer Science, Georgia Institute of Technology,
Atlanta, November 2000. [5] Svetlana Domnitcheva. Location Modeling: State of the Art and Challenges. In Proceedings of the Workshop on Location Modeling for Ubiquitous Computing. (Atlanta,
Georgia, United States, September 30, 2001). 13-19. [6] Ferscha, A. Workspace Awareness in Mobile Virtual Teams.
In Proceedings of the IEEE 9th International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE"00). (Gaithersburg, Maryland, March 14-16, 2000). IEEE Computer Society Press, 272-277. [7] Ferscha, A. Coordination in Pervasive Computing Environments. In Proceedings of the Twelfth International IEEE Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE-2003). (June 9-11, 2003). IEEE Computer Society Press, 3-9. [8] Leonhard, U. Supporting Location Awareness in Open Distributed Systems. Ph.D. Thesis, Department of Computing, Imperial College, London, May 1998. [9] Ryan, N., Pascoe, J., Morse, D. Enhanced Reality Fieldwork: the Context-Aware Archaeological Assistant.
Gaffney, V., Van Leusen, M., Exxon, S. (eds.) Computer Applications in Archaeology (1997) [10] Schilit, B.N., Theimer, M. Disseminating Active Map Information to Mobile Hosts. IEEE Network, 8(5) (1994), 22-32. [11] Schilit, B.N. A System Architecture for Context-Aware Mobile Computing. Ph.D. Thesis, Columbia University,

Grid computing is a model for wide-area distributed and parallel computing across heterogeneous networks in multiple administrative domains. This research field aims to promote sharing of resources and provides breakthrough computing power over this wide network of virtual organizations in a seamless manner [8]. Traditionally, as in Globus [6], Condor-G [9] and Legion [10], there is a minimal infrastructure that provides data resource sharing, computational resource utilization management, and distributed execution.
Specifically, considering distributed execution, most of the existing grid infrastructures supports execution of isolated tasks, but they do not consider their task interdependencies as in processes (workflows) [12]. This deficiency restricts better scheduling algorithms, distributed execution coordination and automatic execution recovery.
There are few proposed middleware infrastructures that support process execution over the grid. In general, they model processes by interconnecting their activities through control and data dependencies. Among them, WebFlow [1] emphasizes an architecture to construct distributed processes; Opera-G [3] provides execution recovering and steering, GridFlow [5] focuses on improved scheduling algorithms that take advantage of activity dependencies, and SwinDew [13] supports totally distributed execution on peer-to-peer networks. However, such infrastructures contain scheduling algorithms that are centralized by process [1, 3, 5], or completely distributed, but difficult to monitor and control [13].
In order to address such constraints, this paper proposes a structured programming model for process description and a hierarchical process execution infrastructure. The programming model employs structured control flow to promote controlled and contextualized activity execution.
Complementary, the support infrastructure, which executes a process specification, takes advantage of the hierarchical structure of a specified process in order to distribute and schedule strong dependent activities as a unit, allowing a better execution performance and fault-tolerance and providing localized communication.
The programming model and the support infrastructure, named X avantes, are under implementation in order to show the feasibility of the proposed model and to demonstrate its two major advantages: to promote widely distributed process execution and scheduling, but in a controlled, structured and localized way.
Next Section describes the programming model, and Section 3, the support infrastructure for the proposed grid computing model. Section 4 demonstrates how the support infrastructure executes processes and distributes activities.
Related works are presented and compared to the proposed model in Section 5. The last Section concludes this paper encompassing the advantages of the proposed hierarchical process execution support for the grid computing area and lists some future works.
ProcessElement Process Activity Controller 1 * 1 * Figure 1: High-level framework of the programming model
The programming model designed for the grid computing architecture is very similar to the specified to the Business Process Execution Language (BPEL) [2]. Both describe processes in XML [4] documents, but the former specifies processes strictly synchronous and structured, and has more constructs for structured parallel control. The rationale behind of its design is the possibility of hierarchically distribute the process control and coordination based on structured constructs, differently from BPEL, which does not allow hierarchical composition of processes.
In the proposed programming model, a process is a set of interdependent activities arranged to solve a certain problem. In detail, a process is composed of activities, subprocesses, and controllers (see Figure 1). Activities represent simple tasks that are executed on behalf of a process; subprocesses are processes executed in the context of a parent process; and controllers are control elements used to specify the execution order of these activities and subprocesses. Like structured languages, controllers can be nested and then determine the execution order of other controllers.
Data are exchanged among process elements through parameters. They are passed by value, in case of simple objects, or by reference, if they are remote objects shared among elements of the same controller or process. External data can be accessed through data sources, such as relational databases or distributed objects.
Controllers are structured control constructs used to define the control flow of processes. There are sequential and parallel controllers.
The sequential controller types are: block, switch, for and while. The block controller is a simple sequential construct, and the others mimic equivalent structured programming language constructs. Similarly, the parallel types are: par, parswitch, parfor and parwhile. They extend the respective sequential counterparts to allow parallel execution of process elements.
All parallel controller types fork the execution of one or more process elements, and then, wait for each execution to finish. Indeed, they contain a fork and a join of execution.
Aiming to implement a conditional join, all parallel controller types contain an exit condition, evaluated all time that an element execution finishes, in order to determine when the controller must end.
The parfor and parwhile are the iterative versions of the parallel controller types. Both fork executions while the iteration condition is true. This provides flexibility to determine, at run-time, the number of process elements to execute simultaneously.
When compared to workflow languages, the parallel controller types represent structured versions of the workflow control constructors, because they can nest other controllers and also can express fixed and conditional forks and joins, present in such languages.
This section presents an example of a prime number search application that receives a certain range of integers and returns a set of primes contained in this range. The whole computation is made by a process, which uses a parallel controller to start and dispatch several concurrent activities of the same type, in order to find prime numbers. The portion of the XML document that describes the process and activity types is shown below. <PROCESS_TYPE NAME="FindPrimes"> <IN_PARAMETER TYPE="int" NAME="min"/> <IN_PARAMETER TYPE="int" NAME="max"/> <IN_PARAMETER TYPE="int" NAME="numPrimes"/> <IN_PARAMETER TYPE="int" NAME="numActs"/> <BODY> <PRE_CODE> setPrimes(new RemoteHashSet()); parfor.setMin(getMin()); parfor.setMax(getMax()); parfor.setNumPrimes(getNumPrimes()); parfor.setNumActs(getNumActs()); parfor.setPrimes(getPrimes()); parfor.setCounterBegin(0); parfor.setCounterEnd(getNumActs()-1); </PRE_CODE> <PARFOR NAME="parfor"> <IN_PARAMETER TYPE="int" NAME="min"/> <IN_PARAMETER TYPE="int" NAME="max"/> <IN_PARAMETER TYPE="int" NAME="numPrimes"/> <IN_PARAMETER TYPE="int" NAME="numActs"/> <IN_PARAMETER TYPE="RemoteCollection" NAME="primes"/> <ITERATE> <PRE_CODE> int range= (getMax()-getMin()+1)/getNumActs(); int minNum = range*getCounter()+getMin(); int maxNum = minNum+range-1; if (getCounter() == getNumActs()-1) maxNum = getMax(); findPrimes.setMin(minNum); findPrimes.setMax(maxNum); findPrimes.setNumPrimes(getNumPrimes()); findPrimes.setPrimes(getPrimes()); </PRE_CODE> <ACTIVITY TYPE="FindPrimes" NAME="findPrimes"/> </ITERATE> </PARFOR> </BODY> <OUT_PARAMETER TYPE="RemoteCollection" NAME="primes"/> </PROCESS_TYPE> Middleware for Grid Computing 88 <ACTIVITY_TYPE NAME="FindPrimes"> <IN_PARAMETER TYPE="int" NAME="min"/> <IN_PARAMETER TYPE="int" NAME="max"/> <IN_PARAMETER TYPE="int" NAME="numPrimes"/> <IN_PARAMETER TYPE="RemoteCollection" NAME="primes"/> <CODE> for (int num=getMin(); num<=getMax(); num++) { // stop, required number of primes was found if (primes.size() >= getNumPrimes()) break; boolean prime = true; for (int i=2; i<num; i++) { if (num % i == 0) { prime = false; break; } } if (prime) { primes.add(new Integer(num)); } } </CODE> </ACTIVITY_TYPE> Firstly, a process type that finds prime numbers, named FindPrimes, is defined. It receives, through its input parameters, a range of integers in which prime numbers have to be found, the number of primes to be returned, and the number of activities to be executed in order to perform this work. At the end, the found prime numbers are returned as a collection through its output parameter.
This process contains a PARFOR controller aiming to execute a determined number of parallel activities. It iterates from 0 to getNumActs() - 1, which determines the number of activities, starting a parallel activity in each iteration. In such case, the controller divides the whole range of numbers in subranges of the same size, and, in each iteration, starts a parallel activity that finds prime numbers in a specific subrange. These activities receive a shared object by reference in order to store the prime numbers just found and control if the required number of primes has been reached.
Finally, it is defined the activity type, FindPrimes, used to find prime numbers in each subrange. It receives, through its input parameters, the range of numbers in which it has to find prime numbers, the total number of prime numbers to be found by the whole process, and, passed by reference, a collection object to store the found prime numbers.
Between its CODE markers, there is a simple code to find prime numbers, which iterates over the specified range and verifies if the current integer is a prime. Additionally, in each iteration, the code verifies if the required number of primes, inserted in the primes collection by all concurrent activities, has been reached, and exits if true.
The advantage of using controllers is the possibility of the support infrastructure determines the point of execution the process is in, allowing automatic recovery and monitoring, and also the capability of instantiating and dispatching process elements only when there are enough computing resources available, reducing unnecessary overhead. Besides, due to its structured nature, they can be easily composed and the support infrastructure can take advantage of this in order to distribute hierarchically the nested controllers to Group Server Group Java Virtual Machine RMI JDBC Group Manager Process Server Java Virtual Machine RMI JDBC Process Coordinator Worker Java Virtual Machine RMI Activity Manager Repository Figure 2: Infrastructure architecture different machines over the grid, allowing enhanced scalability and fault-tolerance.
The support infrastructure comprises tools for specification, and services for execution and monitoring of structured processes in highly distributed, heterogeneous and autonomous grid environments. It has services to monitor availability of resources in the grid, to interpret processes and schedule activities and controllers, and to execute activities.
The support infrastructure architecture is composed of groups of machines and data repositories, which preserves its administrative autonomy. Generally, localized machines and repositories, such as in local networks or clusters, form a group. Each machine in a group must have a Java Virtual Machine (JVM) [11], and a Java Runtime Library, besides a combination of the following grid support services: group manager (GM), process coordinator (PC) and activity manager (AM). This combination determines what kind of group node it represents: a group server, a process server, or simply a worker (see Figure 2).
In a group there are one or more group managers, but only one acts as primary and the others, as replicas. They are responsible to maintain availability information of group machines. Moreover, group managers maintain references to data resources of the group. They use group repositories to persist and recover the location of nodes and their availability.
To control process execution, there are one or more process coordinators per group. They are responsible to instantiate and execute processes and controllers, select resources, and schedule and dispatch activities to workers. In order to persist and recover process execution and data, and also load process specification, they use group repositories.
Finally, in several group nodes there is an activity manager. It is responsible to execute activities in the hosted machine on behalf of the group process coordinators, and to inform the current availability of the associated machine to group managers. They also have pendent activity queues, containing activities to be executed.
In order to model real grid architecture, the infrastructure must comprise several, potentially all, local networks, like Internet does. Aiming to satisfy this intent, local groups are
GM GM GM GM Figure 3: Inter-group relationships connected to others, directly or indirectly, through its group managers (see Figure 3).
Each group manager deals with requests of its group (represented by dashed ellipses), in order to register local machines and maintain correspondent availability.
Additionally, group managers communicate to group managers of other groups. Each group manager exports coarse availability information to group managers of adjacent groups and also receives requests from other external services to furnish detailed availability information. In this way, if there are resources available in external groups, it is possible to send processes, controllers and activities to these groups in order to execute them in external process coordinators and activity managers, respectively.
In the proposed grid architecture, a process is specified in XML, using controllers to determine control flow; referencing other processes and activities; and passing objects to their parameters in order to define data flow. After specified, the process is compiled in a set of classes, which represent specific process, activity and controller types. At this time, it can be instantiated and executed by a process coordinator.
To execute a specified process, it must be instantiated by referencing its type on a process coordinator service of a specific group. Also, the initial parameters must be passed to it, and then it can be started.
The process coordinator carries out the process by executing the process elements included in its body sequentially.
If the element is a process or a controller, the process coordinator can choose to execute it in the same machine or to pass it to another process coordinator in a remote machine, if available. Else, if the element is an activity, it passes to an activity manager of an available machine.
Process coordinators request the local group manager to find available machines that contain the required service, process coordinator or activity manager, in order to execute a process element. Then, it can return a local machine, a machine in another group or none, depending on the availability of such resource in the grid. It returns an external worker (activity manager machine) if there are no available workers in the local group; and, it returns an external process server (process coordinator machine), if there are no available process servers or workers in the local group.
Obeying this rule, group managers try to find process servers in the same group of the available workers.
Such procedure is followed recursively by all process coGM FindPrimes Activity AM FindPrimes Activity AM FindPrimes Activity AM FindPrimes Process PC Figure 4: FindPrimes process execution ordinators that execute subprocesses or controllers of a process. Therefore, because processes are structured by nesting process elements, the process execution is automatically distributed hierarchically through one or more grid groups according to the availability and locality of computing resources.
The advantage of this distribution model is wide area execution, which takes advantage of potentially all grid resources; and localized communication of process elements, because strong dependent elements, which are under the same controller, are placed in the same or near groups.
Besides, it supports easy monitoring and steering, due to its structured controllers, which maintain state and control over its inner elements.
Revisiting the example shown in Section 2.2, a process type is specified to find prime numbers in a certain range of numbers. In order to solve this problem, it creates a number of activities using the parfor controller. Each activity, then, finds primes in a determined part of the range of numbers.
Figure 4 shows an instance of this process type executing over the proposed infrastructure. A FindPrimes process instance is created in an available process coordinator (PC), which begins executing the parfor controller. In each iteration of this controller, the process coordinator requests to the group manager (GM) an available activity manager (AM) in order to execute a new instance of the FindPrimes activity. If there is any AM available in this group or in an external one, the process coordinator sends the activity class and initial parameters to this activity manager and requests its execution. Else, if no activity manager is available, then the controller enters in a wait state until an activity manager is made available, or is created.
In parallel, whenever an activity finishes, its result is sent back to the process coordinator, which records it in the parfor controller. Then, the controller waits until all activities that have been started are finished, and it ends. At this point, the process coordinator verifies that there is no other process element to execute and finishes the process.
There are several academic and commercial products that promise to support grid computing, aiming to provide interfaces, protocols and services to leverage the use of widely Middleware for Grid Computing 90 distributed resources in heterogeneous and autonomous networks. Among them, Globus [6], Condor-G [9] and Legion [10] are widely known. Aiming to standardize interfaces and services to grid, the Open Grid Services Architecture (OGSA) [7] has been defined.
The grid architectures generally have services that manage computing resources and distribute the execution of independent tasks on available ones. However, emerging architectures maintain task dependencies and automatically execute tasks in a correct order. They take advantage of these dependencies to provide automatic recovery, and better distribution and scheduling algorithms.
Following such model, WebFlow [1] is a process specification tool and execution environment constructed over CORBA that allows graphical composition of activities and their distributed execution in a grid environment. Opera-G [3], like WebFlow, uses a process specification language similar to the data flow diagram and workflow languages, but furnishes automatic execution recovery and limited steering of process execution.
The previously referred architectures and others that enact processes over the grid have a centralized coordination.
In order to surpass this limitation, systems like SwinDew [13] proposed a widely distributed process execution, in which each node knows where to execute the next activity or join activities in a peer-to-peer environment.
In the specific area of activity distribution and scheduling, emphasized in this work, GridFlow [5] is remarkable. It uses a two-level scheduling: global and local. In the local level, it has services that predict computing resource utilization and activity duration. Based on this information, GridFlow employs a PERT-like technique that tries to forecast the activity execution start time and duration in order to better schedule them to the available resources.
The architecture proposed in this paper, which encompasses a programming model and an execution support infrastructure, is widely decentralized, differently from WebFlow and Opera-G, being more scalable and fault-tolerant. But, like the latter, it is designed to support execution recovery.
Comparing to SwinDew, the proposed architecture contains widely distributed process coordinators, which coordinate processes or parts of them, differently from SwinDew where each node has a limited view of the process: only the activity that starts next. This makes easier to monitor and control processes.
Finally, the support infrastructure breaks the process and its subprocesses for grid execution, allowing a group to require another group for the coordination and execution of process elements on behalf of the first one. This is different from GridFlow, which can execute a process in at most two levels, having the global level as the only responsible to schedule subprocesses in other groups. This can limit the overall performance of processes, and make the system less scalable.
Grid computing is an emerging research field that intends to promote distributed and parallel computing over the wide area network of heterogeneous and autonomous administrative domains in a seamless way, similar to what Internet does to the data sharing. There are several products that support execution of independent tasks over grid, but only a few supports the execution of processes with interdependent tasks.
In order to address such subject, this paper proposes a programming model and a support infrastructure that allow the execution of structured processes in a widely distributed and hierarchical manner. This support infrastructure provides automatic, structured and recursive distribution of process elements over groups of available machines; better resource use, due to its on demand creation of process elements; easy process monitoring and steering, due to its structured nature; and localized communication among strong dependent process elements, which are placed under the same controller. These features contribute to better scalability, fault-tolerance and control for processes execution over the grid. Moreover, it opens doors for better scheduling algorithms, recovery mechanisms, and also, dynamic modification schemes.
The next work will be the implementation of a recovery mechanism that uses the execution and data state of processes and controllers to recover process execution. After that, it is desirable to advance the scheduling algorithm to forecast machine use in the same or other groups and to foresee start time of process elements, in order to use this information to pre-allocate resources and, then, obtain a better process execution performance. Finally, it is interesting to investigate schemes of dynamic modification of processes over the grid, in order to evolve and adapt long-term processes to the continuously changing grid environment.
We would like to thank Paulo C. Oliveira, from the State Treasury Department of Sao Paulo, for its deeply revision and insightful comments.
[1] E. Akarsu, G. C. Fox, W. Furmanski, and T. Haupt.
WebFlow: High-Level Programming Environment and Visual Authoring Toolkit for High Performance Distributed Computing. In Proceedings of Supercom puting (SC98), 1998. [2] T. Andrews and F. Curbera. Specification: Business Process Execution Language for W eb Services V ersion
http://www-106.ibm.com/developerworks/library/wsbpel. [3] W. Bausch. O PERA -G :A M icrokernelfor Com putationalG rids. PhD thesis, Swiss Federal Institute of Technology, Zurich, 2004. [4] T. Bray and J. Paoli. Extensible M arkup Language (X M L) 1.0. XML Core WG, W3C, 2004. Available at http://www.w3.org/TR/2004/REC-xml-20040204. [5] J. Cao, S. A. Jarvis, S. Saini, and G. R. Nudd.
GridFlow: Workflow Management for Grid Computing. In Proceedings ofthe International Sym posium on Cluster Com puting and the G rid (CCG rid 2003), 2003. [6] I. Foster and C. Kesselman. Globus: A Metacomputing Infrastructure Toolkit. Intl.J.
Supercom puter A pplications, 11(2):115-128, 1997. [7] I. Foster, C. Kesselman, J. M. Nick, and S. Tuecke.
The Physiology ofthe G rid: A n O pen G rid Services A rchitecture for D istributed System s Integration.
Open Grid Service Infrastructure WG, Global Grid Forum, 2002. [8] I. Foster, C. Kesselman, and S. Tuecke. The Anatomy of the Grid: Enabling Scalable Virtual Organization.
The Intl.JournalofH igh Perform ance Com puting A pplications, 15(3):200-222, 2001. [9] J. Frey, T. Tannenbaum, M. Livny, I. Foster, and S. Tuecke. Condor-G: A Computational Management Agent for Multi-institutional Grids. In Proceedings of the Tenth Intl.Sym posium on H igh Perform ance D istributed Com puting (H PD C-10). IEEE, 2001. [10] A. S. Grimshaw and W. A. Wulf. Legion - A View from 50,000 Feet. In Proceedings ofthe Fifth Intl.
Sym posium on H igh Perform ance D istributed Com puting. IEEE, 1996. [11] T. Lindholm and F. Yellin. The Java V irtualM achine Specification. Sun Microsystems, Second Edition edition, 1999. [12] B. R. Schulze and E. R. M. Madeira. Grid Computing with Active Services. Concurrency and Com putation: Practice and Experience Journal, 5(16):535-542, 2004. [13] J. Yan, Y. Yang, and G. K. Raikundalia. Enacting Business Processes in a Decentralised Environment with P2P-Based Workflow Support. In Proceedings of the Fourth Intl.Conference on W eb-Age Inform ation M anagem ent(W A IM 2003), 2003.

We study the effects of resource failures in congestion settings. This study is motivated by a variety of situations in multi-agent systems with unreliable components, such as machines, computers etc. We define a model for congestion games with load-dependent failures (CGLFs) which provides simple and natural description of such situations. In this model, we are given a finite set of identical resources (service providers) where each element possesses a failure probability describing the probability of unsuccessful completion of its assigned tasks as a (nondecreasing) function of its congestion. There is a fixed number of agents, each having a task which can be carried out by any of the resources.
For reliability reasons, each agent may decide to assign his task, simultaneously, to a number of resources. Thus, the congestion on the resources is not known in advance, but is strategy-dependent. Each resource is associated with a cost, which is a (nonnegative) function of the congestion experienced by this resource. The objective of each agent is to maximize his own utility, which is the difference between his benefit from successful task completion and the sum of the costs over the set of resources he uses. The benefits of the agents from successful completion of their tasks are allowed to vary across the agents.
The resource cost function describes the cost suffered by an agent for selecting that resource, as a function of the number of agents who have selected it. Thus, it is natural to assume that these functions are nonnegative. In addition, in many real-life applications of our model the resource cost functions have a special structure. In particular, they can monotonically increase or decrease with the number of the users, depending on the context. The former case is motivated by situations where high congestion on a resource causes longer delay in its assigned tasks execution and as a result, the cost of utilizing this resource might be higher.
A typical example of such situation is as follows. Assume we need to deliver an important package. Since there is no guarantee that a courier will reach the destination in time, we might send several couriers to deliver the same package.
The time required by each courier to deliver the package increases with the congestion on his way. In addition, the payment to a courier is proportional to the time he spends in delivering the package. Thus, the payment to the courier increases when the congestion increases. The latter case (decreasing cost functions) describes situations where a group of agents using a particular resource have an opportunity to share its cost among the group"s members, or, the cost of 210 using a resource decreases with the number of users, according to some marketing policy.
Our results We show that CGLFs and, in particular, CGLFs with nondecreasing cost functions, do not admit a potential function. Therefore, the CGLF model can not be reduced to congestion games. Nevertheless, if the failure probabilities are constant (do not depend on the congestion) then a potential function is guaranteed to exist.
We show that CGLFs and, in particular, CGLFs with decreasing cost functions, do not possess pure strategy Nash equilibria. However, as we show in our main result, there exists a pure strategy Nash equilibrium in any CGLF with nondecreasing cost functions.
Related work Our model extends the well-known class of congestion games [11]. In a congestion game, every agent has to choose from a finite set of resources, where the utility (or cost) of an agent from using a particular resource depends on the number of agents using it, and his total utility (cost) is the sum of the utilities (costs) obtained from the resources he uses. An important property of these games is the existence of pure strategy Nash equilibria. Monderer and Shapley [9] introduced the notions of potential function and potential game and proved that the existence of a potential function implies the existence of a pure strategy Nash equilibrium. They observed that Rosenthal [11] proved his theorem on congestion games by constructing a potential function (hence, every congestion game is a potential game). Moreover, they showed that every finite potential game is isomorphic to a congestion game; hence, the classes of finite potential games and congestion games coincide.
Congestion games have been extensively studied and generalized. In particular, Leyton-Brown and Tennenholtz [5] extended the class of congestion games to the class of localeffect games. In a local-effect game, each agent"s payoff is effected not only by the number of agents who have chosen the same resources as he has chosen, but also by the number of agents who have chosen neighboring resources (in a given graph structure). Monderer [8] dealt with another type of generalization of congestion games, in which the resource cost functions are player-specific (PS-congestion games). He defined PS-congestion games of type q (q-congestion games), where q is a positive number, and showed that every game in strategic form is a q-congestion game for some q.
Playerspecific resource cost functions were discussed for the first time by Milchtaich [6]. He showed that simple and strategysymmetric PS-congestion games are not potential games, but always possess a pure strategy Nash equilibrium.
PScongestion games were generalized to weighted congestion games [6] (or, ID-congestion games [7]), in which the resource cost functions are not only player-specific, but also depend on the identity of the users of the resource.
Ackermann et al. [1] showed that weighted congestion games admit pure strategy Nash equilibria if the strategy space of each player consists of the bases of a matroid on the set of resources.
Much of the work on congestion games has been inspired by the fact that every such game has a pure strategy Nash equilibrium. In particular, Fabrikant et al. [3] studied the computational complexity of finding pure strategy Nash equilibria in congestion games. Intensive study has also been devoted to quantify the inefficiency of equilibria in congestion games. Koutsoupias and Papadimitriou [4] proposed the worst-case ratio of the social welfare achieved by a Nash equilibrium and by a socially optimal strategy profile (dubbed the price of anarchy) as a measure of the performance degradation caused by lack of coordination.
Christodoulou and Koutsoupias [2] considered the price of anarchy of pure equilibria in congestion games with linear cost functions. Roughgarden and Tardos [12] used this approach to study the cost of selfish routing in networks with a continuum of users.
However, the above settings do not take into consideration the possibility that resources may fail to execute their assigned tasks. In the computer science context of congestion games, where the alternatives of concern are machines, computers, communication lines etc., which are obviously prone to failures, this issue should not be ignored.
Penn, Polukarov and Tennenholtz were the first to incorporate the issue of failures into congestion settings [10].
They introduced a class of congestion games with failures (CGFs) and proved that these games, while not being isomorphic to congestion games, always possess Nash equilibria in pure strategies. The CGF-model significantly differs from ours. In a CGF, the authors considered the delay associated with successful task completion, where the delay for an agent is the minimum of the delays of his successful attempts and the aim of each agent is to minimize his expected delay. In contrast with the CGF-model, in our model we consider the total cost of the utilized resources, where each agent wishes to maximize the difference between his benefit from a successful task completion and the sum of his costs over the resources he uses.
The above differences imply that CGFs and CGLFs possess different properties. In particular, if in our model the resource failure probabilities were constant and known in advance, then a potential function would exist. This, however, does not hold for CGFs; in CGFs, the failure probabilities are constant but there is no potential function.
Furthermore, the procedures proposed by the authors in [10] for the construction of a pure strategy Nash equilibrium are not valid in our model, even in the simple, agent-symmetric case, where all agents have the same benefit from successful completion of their tasks.
Our work provides the first model of congestion settings with resource failures, which considers the sum of congestiondependent costs over utilized resources, and therefore, does not extend the CGF-model, but rather generalizes the classic model of congestion games. Moreover, it is the first model to consider load-dependent failures in the above context. 211 Organization The rest of the paper is organized as follows. In Section 2 we define our model. In Section 3 we present our results.
In 3.1 we show that CGLFs, in general, do not have pure strategy Nash equilibria. In 3.2 we focus on CGLFs with nondecreasing cost functions (nondecreasing CGLFs). We show that these games do not admit a potential function.
However, in our main result we show the existence of pure strategy Nash equilibria in nondecreasing CGLFs. Section
omitted from this conference version of the paper, and will appear in the full version.
The scenarios considered in this work consist of a finite set of agents where each agent has a task that can be carried out by any element of a set of identical resources (service providers). The agents simultaneously choose a subset of the resources in order to perform their tasks, and their aim is to maximize their own expected payoff, as described in the sequel.
Let N be a set of n agents (n ∈ N), and let M be a set of m resources (m ∈ N). Agent i ∈ N chooses a strategy σi ∈ Σi which is a (potentially empty) subset of the resources. That is, Σi is the power set of the set of resources: Σi = P(M). Given a subset S ⊆ N of the agents, the set of strategy combinations of the members of S is denoted by ΣS = ×i∈SΣi, and the set of strategy combinations of the complement subset of agents is denoted by Σ−S (Σ−S = ΣN S = ×i∈N SΣi). The set of pure strategy profiles of all the agents is denoted by Σ (Σ = ΣN ).
Each resource is associated with a cost, c(·), and a failure probability, f(·), each of which depends on the number of agents who use this resource. We assume that the failure probabilities of the resources are independent. Let σ = (σ1, . . . , σn) ∈ Σ be a pure strategy profile. The (m-dimensional) congestion vector that corresponds to σ is hσ = (hσ e )e∈M , where hσ e = ˛ ˛{i ∈ N : e ∈ σi} ˛ ˛. The failure probability of a resource e is a monotone nondecreasing function f : {1, . . . , n} → [0, 1) of the congestion experienced by e. The cost of utilizing resource e is a function c : {1, . . . , n} → R+ of the congestion experienced by e.
The outcome for agent i ∈ N is denoted by xi ∈ {S, F}, where S and F, respectively, indicate whether the task execution succeeded or failed. We say that the execution of agent"s i task succeeds if the task of agent i is successfully completed by at least one of the resources chosen by him.
The benefit of agent i from his outcome xi is denoted by Vi(xi), where Vi(S) = vi, a given (nonnegative) value, and Vi(F) = 0.
The utility of agent i from strategy profile σ and his outcome xi, ui(σ, xi), is the difference between his benefit from the outcome (Vi(xi)) and the sum of the costs of the resources he has used: ui(σ, xi) = Vi(xi) − X e∈σi c(hσ e ) .
The expected utility of agent i from strategy profile σ, Ui(σ), is, therefore: Ui(σ) = 1 − Y e∈σi f(hσ e ) ! vi − X e∈σi c(hσ e ) , where 1 − Q e∈σi f(hσ e ) denotes the probability of successful completion of agent i"s task. We use the convention thatQ e∈∅ f(hσ e ) = 1. Hence, if agent i chooses an empty set σi = ∅ (does not assign his task to any resource), then his expected utility, Ui(∅, σ−i), equals zero.
IN CGLFS In this section we present our results on CGLFs. We investigate the property of the (non-)existence of pure strategy Nash equilibria in these games. We show that this class of games does not, in general, possess pure strategy equilibria.
Nevertheless, if the resource cost functions are nondecreasing then such equilibria are guaranteed to exist, despite the non-existence of a potential function.
We start by showing that the class of CGLFs and, in particular, the subclass of CGLFs with decreasing cost functions, does not, in general, possess Nash equilibria in pure strategies.
Consider a CGLF with two agents (N = {1, 2}) and two resources (M = {e1, e2}). The cost function of each resource is given by c(x) = 1 xx , where x ∈ {1, 2}, and the failure probabilities are f(1) = 0.01 and f(2) = 0.26. The benefits of the agents from successful task completion are v1 = 1.1 and v2 = 4. Below we present the payoff matrix of the game. ∅ {e1} {e2} {e1, e2} ∅ U1 = 0 U1 = 0 U1 = 0 U1 = 0 U2 = 0 U2 = 2.96 U2 = 2.96 U2 = 1.9996 {e1} U1 = 0.089 U1 = 0.564 U1 = 0.089 U1 = 0.564 U2 = 0 U2 = 2.71 U2 = 2.96 U2 = 2.7396 {e2} U1 = 0.089 U1 = 0.089 U1 = 0.564 U1 = 0.564 U2 = 0 U2 = 2.96 U2 = 2.71 U2 = 2.7396 {e1, e2} U1 = −0.90011 U1 = −0.15286 U1 = −0.15286 U1 = 0.52564 U2 = 0 U2 = 2.71 U2 = 2.71 U2 = 3.2296 Table 1: Example for non-existence of pure strategy Nash equilibria in CGLFs.
It can be easily seen that for every pure strategy profile σ in this game there exist an agent i and a strategy σi ∈ Σi such that Ui(σ−i, σi) > Ui(σ). That is, every pure strategy profile in this game is not in equilibrium.
However, if the cost functions in a given CGLF do not decrease in the number of users, then, as we show in the main result of this paper, a pure strategy Nash equilibrium is guaranteed to exist. 212
This section focuses on the subclass of CGLFs with nondecreasing cost functions (henceforth, nondecreasing CGLFs).
We show that nondecreasing CGLFs do not, in general, admit a potential function. Therefore, these games are not congestion games. Nevertheless, we prove that all such games possess pure strategy Nash equilibria.
Recall that Monderer and Shapley [9] introduced the notions of potential function and potential game, where potential game is defined to be a game that possesses a potential function. A potential function is a real-valued function over the set of pure strategy profiles, with the property that the gain (or loss) of an agent shifting to another strategy while the other agents" strategies are kept unchanged, equals to the corresponding increment of the potential function. The authors [9] showed that the classes of finite potential games and congestion games coincide.
Here we show that the class of CGLFs and, in particular, the subclass of nondecreasing CGLFs, does not admit a potential function, and therefore is not included in the class of congestion games. However, for the special case of constant failure probabilities, a potential function is guaranteed to exist. To prove these statements we use the following characterization of potential games [9].
A path in Σ is a sequence τ = (σ0 → σ1 → · · · ) such that for every k ≥ 1 there exists a unique agent, say agent i, such that σk = (σk−1 −i , σi) for some σi = σk−1 i in Σi. A finite path τ = (σ0 → σ1 → · · · → σK ) is closed if σ0 = σK .
It is a simple closed path if in addition σl = σk for every
defined to be the number of distinct points in it; that is, the length of τ = (σ0 → σ1 → · · · → σK ) is K.
Theorem 1. [9] Let G be a game in strategic form with a vector U = (U1, . . . , Un) of utility functions. For a finite path τ = (σ0 → σ1 → · · · → σK ), let U(τ) = PK k=1[Uik (σk )− Uik (σk−1 )], where ik is the unique deviator at step k. Then,
G is a potential game if and only if U(τ) = 0 for every simple closed path τ of length 4.
Load-Dependent Failures Based on Theorem 1, we present the following counterexample that demonstrates the non-existence of a potential function in CGLFs.
We consider the following agent-symmetric game G in which two agents (N = {1, 2}) wish to assign a task to two resources (M = {e1, e2}). The benefit from a successful task completion of each agent equals v, and the failure probability function strictly increases with the congestion. Consider the simple closed path of length 4 which is formed by α = (∅, {e2}) , β = ({e1}, {e2}) , γ = ({e1}, {e1, e2}) , δ = (∅, {e1, e2}) : {e2} {e1, e2} ∅ U1 = 0 U1 = 0 U2 = (1 − f(1)) v − c(1) U2 = `
´ v − 2c(1) {e1} U1 = (1 − f(1)) v − c(1) U1 = (1 − f(2)) v − c(2) U2 = (1 − f(1)) v − c(1) U2 = (1 − f(1)f(2)) v − c(1) − c(2) Table 2: Example for non-existence of potentials in CGLFs.
Therefore,
U1(α) − U1(β) + U2(β) − U2(γ) + U1(γ) − U1(δ) +U2(δ) − U2(α) = v (1 − f(1)) (f(1) − f(2)) = 0.
Thus, by Theorem 1, nondecreasing CGLFs do not admit potentials. As a result, they are not congestion games.
However, as presented in the next section, the special case in which the failure probabilities are constant, always possesses a potential function.
Constant Failure Probabilities We show below that CGLFs with constant failure probabilities always possess a potential function. This follows from the fact that the expected benefit (revenue) of each agent in this case does not depend on the choices of the other agents.
In addition, for each agent, the sum of the costs over his chosen subset of resources, equals the payoff of an agent choosing the same strategy in the corresponding congestion game.
Assume we are given a game G with constant failure probabilities. Let τ = (α → β → γ → δ → α) be an arbitrary simple closed path of length 4. Let i and j denote the active agents (deviators) in τ and z ∈ Σ−{i,j} be a fixed strategy profile of the other agents. Let α = (xi, xj, z), β = (yi, xj, z), γ = (yi, yj, z), δ = (xi, yj, z), where xi, yi ∈ Σi and xj, yj ∈ Σj. Then,
U(τ) = Ui(xi, xj, z) − Ui(yi, xj, z) +Uj(yi, xj, z) − Uj(yi, yj, z) +Ui(yi, yj, z) − Ui(xi, yj, z) +Uj(xi, yj, z) − Uj(xi, xj, z) =
vi − X e∈xi c(h (xi,xj ,z) e ) − . . . −
vj + X e∈xj c(h (xi,xj ,z) e ) = »
vi − . . . −
vj  − » X e∈xi c(h (xi,xj ,z) e ) − . . . − X e∈xj c(h (xi,xj ,z) e )  .
Notice that »
vi − . . . −
vj  = 0, as a sum of a telescope series. The remaining sum equals 0, by applying Theorem 1 to congestion games, which are known to possess a potential function. Thus, by Theorem 1, G is a potential game. 213 We note that the above result holds also for the more general settings with non-identical resources (having different failure probabilities and cost functions) and general cost functions (not necessarily monotone and/or nonnegative).
Equilibrium In the previous section, we have shown that CGLFs and, in particular, nondecreasing CGLFs, do not admit a potential function, but this fact, in general, does not contradict the existence of an equilibrium in pure strategies. In this section, we present and prove the main result of this paper (Theorem 2) which shows the existence of pure strategy Nash equilibria in nondecreasing CGLFs.
Theorem 2. Every nondecreasing CGLF possesses a Nash equilibrium in pure strategies.
The proof of Theorem 2 is based on Lemmas 4, 7 and 8, which are presented in the sequel. We start with some definitions and observations that are needed for their proofs.
In particular, we present the notions of A-, D- and S-stability and show that a strategy profile is in equilibrium if and only if it is A-, D- and S- stable. Furthermore, we prove the existence of such a profile in any given nondecreasing CGLF.
Definition 3. For any strategy profile σ ∈ Σ and for any agent i ∈ N, the operation of adding precisely one resource to his strategy, σi, is called an A-move of i from σ.
Similarly, the operation of dropping a single resource is called a D-move, and the operation of switching one resource with another is called an S-move.
Clearly, if agent i deviates from strategy σi to strategy σi by applying a single A-, D- or S-move, then max {|σi σi|, |σi σi|} = 1, and vice versa, if max {|σi σi|, |σi σi|} =
move. For simplicity of exposition, for any pair of sets A and B, let µ(A, B) = max {|A B|, |B A|}.
The following lemma implies that any strategy profile, in which no agent wishes unilaterally to apply a single A-,
Dor S-move, is a Nash equilibrium. More precisely, we show that if there exists an agent who benefits from a unilateral deviation from a given strategy profile, then there exists a single A-, D- or S-move which is profitable for him as well.
Lemma 4. Given a nondecreasing CGLF, let σ ∈ Σ be a strategy profile which is not in equilibrium, and let i ∈ N such that ∃xi ∈ Σi for which Ui(σ−i, xi) > Ui(σ). Then, there exists yi ∈ Σi such that Ui(σ−i, yi) > Ui(σ) and µ(yi, σi) = 1.
Therefore, to prove the existence of a pure strategy Nash equilibrium, it suffices to look for a strategy profile for which no agent wishes to unilaterally apply an A-, D- or S-move.
Based on the above observation, we define A-, D- and Sstability as follows.
Definition 5. A strategy profile σ is said to be A-stable (resp., D-stable, S-stable) if there are no agents with a profitable A- (resp., D-, S-) move from σ. Similarly, we define a strategy profile σ to be DS-stable if there are no agents with a profitable D- or S-move from σ.
The set of all DS-stable strategy profiles is denoted by Σ0 . Obviously, the profile (∅, . . . , ∅) is DS-stable, so Σ0 is not empty. Our goal is to find a DS-stable profile for which no profitable A-move exists, implying this profile is in equilibrium. To describe how we achieve this, we define the notions of light (heavy) resources and (nearly-) even strategy profiles, which play a central role in the proof of our main result.
Definition 6. Given a strategy profile σ, resource e is called σ-light if hσ e ∈ arg mine∈M hσ e and σ-heavy otherwise.
A strategy profile σ with no heavy resources will be termed even. A strategy profile σ satisfying |hσ e − hσ e | ≤ 1 for all e, e ∈ M will be termed nearly-even.
Obviously, every even strategy profile is nearly-even. In addition, in a nearly-even strategy profile, all heavy resources (if exist) have the same congestion. We also observe that the profile (∅, . . . , ∅) is even (and DS-stable), so the subset of even, DS-stable strategy profiles is not empty.
Based on the above observations, we define two types of an A-move that are used in the sequel. Suppose σ ∈ Σ0 is a nearly-even DS-stable strategy profile. For each agent i ∈ N, let ei ∈ arg mine∈M σi hσ e . That is, ei is a lightest resource not chosen previously by i. Then, if there exists any profitable A-move for agent i, then the A-move with ei is profitable for i as well. This is since if agent i wishes to unilaterally add a resource, say a ∈ M σi, then Ui (σ−i, (σi ∪ {a})) > Ui(σ). Hence,
Y e∈σi f(hσ e )f(hσ a + 1) ! vi − X e∈σi c(hσ e ) − c(hσ a + 1) > 1 − Y e∈σi f(hσ e ) ! vi − X e∈σi c(hσ e ) ⇒ vi Y e∈σi f(hσ e ) > c(hσ a + 1)
a + 1) ≥ c(hσ ei + 1)
ei + 1) ⇒ Ui (σ−i, (σi ∪ {ei})) > Ui(σ) .
If no agent wishes to change his strategy in this manner, i.e. Ui(σ) ≥ Ui(σ−i, σi ∪{ei}) for all i ∈ N, then by the above Ui(σ) ≥ Ui(σ−i, σi ∪{a}) for all i ∈ N and a ∈ M σi.
Hence, σ is A-stable and by Lemma 4, σ is a Nash equilibrium strategy profile. Otherwise, let N(σ) denote the subset of all agents for which there exists ei such that a unilateral addition of ei is profitable. Let a ∈ arg minei : i∈N(σ) hσ ei . Let also i ∈ N(σ) be the agent for which ei = a. If a is σ-light, then let σ = (σ−i, σi ∪ {a}). In this case we say that σ is obtained from σ by a one-step addition of resource a, and a is called an added resource. If a is σ-heavy then there exists a σ-light resource b and an agent j such that a ∈ σj and b /∈ σj. Then let σ = ` σ−{i,j}, σi ∪ {a}, (σj {a}) ∪ {b} ´ .
In this case we say that σ is obtained from σ by a two-step addition of resource b, and b is called an added resource.
We notice that, in both cases, the congestion of each resource in σ is the same as in σ, except for the added resource, for which its congestion in σ increased by 1. Thus, since the added resource is σ-light and σ is nearly-even, σ is nearly-even. Then, the following lemma implies the Sstability of σ . 214 Lemma 7. In a nondecreasing CGLF, every nearly-even strategy profile is S-stable.
Coupled with Lemma 7, the following lemma shows that if σ is a nearly-even and DS-stable strategy profile, and σ is obtained from σ by a one- or two-step addition of resource a, then the only potential cause for a non-DS-stability of σ is the existence of an agent k ∈ N with σk = σk, who wishes to drop the added resource a.
Lemma 8. Let σ be a nearly-even DS-stable strategy profile of a given nondecreasing CGLF, and let σ be obtained from σ by a one- or two-step addition of resource a. Then, there are no profitable D-moves for any agent i ∈ N with σi = σi. For an agent i ∈ N with σi = σi, the only possible profitable D-move (if exists) is to drop the added resource a.
We are now ready to prove our main result - Theorem
Lemma 4, it suffices to prove the existence of a strategy profile which is A-, D- and S-stable. We start with the set of even and DS-stable strategy profiles which is obviously not empty. In this set, we consider the subset of strategy profiles with maximum congestion and maximum sum of the agents" utilities. Assuming on the contrary that every DSstable profile admits a profitable A-move, we show the existence of a strategy profile x in the above subset, such that a (one-step) addition of some resource a to x results in a DSstable strategy. Then by a finite series of one- or two-step addition operations we obtain an even, DS-stable strategy profile with strictly higher congestion on the resources, contradicting the choice of x. The full proof is presented below.
Proof of Theorem 2: Let Σ1 ⊆ Σ0 be the subset of all even, DS-stable strategy profiles. Observe that since (∅, . . . , ∅) is an even, DS-stable strategy profile, then Σ1 is not empty, and minσ∈Σ0 ˛ ˛{e ∈ M : e is σ−heavy} ˛ ˛ = 0.
Then, Σ1 could also be defined as Σ1 = arg min σ∈Σ0 ˛ ˛{e ∈ M : e is σ−heavy} ˛ ˛ , with hσ being the common congestion.
Now, let Σ2 ⊆ Σ1 be the subset of Σ1 consisting of all those profiles with maximum congestion on the resources.
That is,
Σ2 = arg max σ∈Σ1 hσ .
Let UN (σ) = P i∈N Ui(σ) denotes the group utility of the agents, and let Σ3 ⊆ Σ2 be the subset of all profiles in Σ2 with maximum group utility. That is,
Σ3 = arg max σ∈Σ2 X i∈N Ui(σ) = arg max σ∈Σ2 UN (σ) .
Consider first the simple case in which maxσ∈Σ1 hσ = 0.
Obviously, in this case, Σ1 = Σ2 = Σ3 = {x = (∅, . . . , ∅)}.
We show below that by performing a finite series of (onestep) addition operations on x, we obtain an even,
DSstable strategy profile y with higher congestion, that is with hy > hx = 0, in contradiction to x ∈ Σ2 . Let z ∈ Σ0 be a nearly-even (not necessarily even) DS-stable profile such that mine∈M hz e = 0, and note that the profile x satisfies the above conditions. Let N(z) be the subset of agents for which a profitable A-move exists, and let i ∈ N(z).
Obviously, there exists a z-light resource a such that Ui(z−i, zi ∪ {a}) > Ui(z) (otherwise, arg mine∈M hz e ⊆ zi, in contradiction to mine∈M hz e = 0). Consider the strategy profile z = (z−i, zi ∪ {a}) which is obtained from z by a (one-step) addition of resource a by agent i. Since z is nearly-even and a is z-light, we can easily see that z is nearly-even. Then,
Lemma 7 implies that z is S-stable. Since i is the only agent using resource a in z , by Lemma 8, no profitable D-moves are available. Thus, z is a DS-stable strategy profile.
Therefore, since the number of resources is finite, there is a finite series of one-step addition operations on x = (∅, . . . , ∅) that leads to strategy profile y ∈ Σ1 with hy = 1 > 0 = hx , in contradiction to x ∈ Σ2 .
We turn now to consider the other case where maxσ∈Σ1 hσ ≥ 1. In this case we select from Σ3 a strategy profile x, as described below, and use it to contradict our contrary assumption. Specifically, we show that there exists x ∈ Σ3 such that for all j ∈ N, vjf(hx )|xj |−1 ≥ c(hx + 1)
. (1) Let x be a strategy profile which is obtained from x by a (one-step) addition of some resource a ∈ M by some agent i ∈ N(x) (note that x is nearly-even). Then, (1) is derived from and essentially equivalent to the inequality Uj(x ) ≥ Uj(x−j, xj {a}), for all a ∈ xj. That is, after performing an A-move with a by i, there is no profitable D-move with a. Then, by Lemmas 7 and 8, x is DS-stable.
Following the same lines as above, we construct a procedure that initializes at x and achieves a strategy profile y ∈ Σ1 with hy > hx , in contradiction to x ∈ Σ2 .
Now, let us confirm the existence of x ∈ Σ3 that satisfies (1). Let x ∈ Σ3 and let M(x) be the subset of all resources for which there exists a profitable (one-step) addition. First, we show that (1) holds for all j ∈ N such that xj ∩M(x) = ∅, that is, for all those agents with one of their resources being desired by another agent.
Let a ∈ M(x), and let x be the strategy profile that is obtained from x by the (one-step) addition of a by agent i.
Assume on the contrary that there is an agent j with a ∈ xj such that vjf(hx )|xj |−1 < c(hx + 1)
.
Let x = (x−j, xj {a}). Below we demonstrate that x is a DS-stable strategy profile and, since x and x correspond to the same congestion vector, we conclude that x lies in Σ2 . In addition, we show that UN (x ) > UN (x), contradicting the fact that x ∈ Σ3 .
To show that x ∈ Σ0 we note that x is an even strategy profile, and thus no S-moves may be performed for x . In addition, since hx = hx and x ∈ Σ0 , there are no profitable D-moves for any agent k = i, j. It remains to show that there are no profitable D-moves for agents i and j as well. 215 Since Ui(x ) > Ui(x), we get vif(hx )|xi| > c(hx + 1)
⇒ vif(hx )|xi |−1 = vif(hx )|xi| > c(hx + 1)
> c(hx )
= c(hx )
, which implies Ui(x ) > Ui(x−i, xi {b}), for all b ∈ xi .
Thus, there are no profitable D-moves for agent i. By the DS-stability of x, for agent j and for all b ∈ xj, we have Uj(x) ≥ Uj(x−j, xj {b}) ⇒ vjf(hx )|xj |−1 ≥ c(hx )
.
Then, vjf(hx )|xj |−1 > vjf(hx )|xj | = vjf(hx )|xj |−1 ≥ c(hx )
= c(hx )
⇒ Uj(x ) > Uj(x−j, xj {b}), for all b ∈ xi. Therefore, x is DS-stable and lies in Σ2 .
To show that UN (x ), the group utility of x , satisfies UN (x ) > UN (x), we note that hx = hx , and thus Uk(x ) = Uk(x), for all k ∈ N {i, j}. Therefore, we have to show that Ui(x ) + Uj(x ) > Ui(x) + Uj(x), or Ui(x ) − Ui(x) > Uj(x) − Uj(x ). Observe that Ui(x ) > Ui(x) ⇒ vif(hx )|xi| > c(hx + 1)
and Uj(x ) < Uj(x ) ⇒ vjf(hx )|xj |−1 < c(hx + 1)
, which yields vif(hx )|xi| > vjf(hx )|xj |−1 .
Thus, Ui(x ) − Ui(x) =
)|xi|+1  vi − (|xi| + 1) c(hx ) − h
)|xi|  vi − |xi|c(hx ) i = vif(hx )|xi| (1 − f(hx )) − c(hx ) > vjf(hx )|xj |−1 (1 − f(hx )) − c(hx ) =
)|xj |  vj − |xj|c(hx ) − h
)|xj |−1  vj − (|xi| − 1) c(hx ) i = Uj(x) − Uj(x ) .
Therefore, x lies in Σ2 and satisfies UN (x ) > UN (x), in contradiction to x ∈ Σ3 .
Hence, if x ∈ Σ3 then (1) holds for all j ∈ N such that xj ∩M(x) = ∅. Now let us see that there exists x ∈ Σ3 such that (1) holds for all the agents. For that, choose an agent i ∈ arg mink∈N vif(hx )|xk| . If there exists a ∈ xi ∩ M(x) then i satisfies (1), implying by the choice of agent i, that the above obviously yields the correctness of (1) for any agent k ∈ N. Otherwise, if no resource in xi lies in M(x), then let a ∈ xi and a ∈ M(x). Since a ∈ xi, a /∈ xi, and hx a = hx a , then there exists agent j such that a ∈ xj and a /∈ xj. One can easily check that the strategy profile x = ` x−{i,j}, (xi {a}) ∪ {a }, (xj {a }) ∪ {a} ´ lies in Σ3 . Thus, x satisfies (1) for agent i, and therefore, for any agent k ∈ N.
Now, let x ∈ Σ3 satisfy (1). We show below that by performing a finite series of one- and two-step addition operations on x, we can achieve a strategy profile y that lies in Σ1 , such that hy > hx , in contradiction to x ∈ Σ2 . Let z ∈ Σ0 be a nearly-even (not necessarily even), DS-stable strategy profile, such that vi Y e∈zi {b} f(hz e) ≥ c(hz b + 1)
b + 1) , (2) for all i ∈ N and for all z-light resource b ∈ zi. We note that for profile x ∈ Σ3 ⊆ Σ1 , with all resources being x-light, conditions (2) and (1) are equivalent. Let z be obtained from z by a one- or two-step addition of a z-light resource a. Obviously, z is nearly-even. In addition, hz e ≥ hz e for all e ∈ M, and mine∈M hz e ≥ mine∈M hz e. To complete the proof we need to show that z is DS-stable, and, in addition, that if mine∈M hz e = mine∈M hz e then z has property (2).
The DS-stability of z follows directly from Lemmas 7 and 8, and from (2) with respect to z. It remains to prove property (2) for z with mine∈M hz e = mine∈M hz e. Using (2) with respect to z, for any agent k with zk = zk and for any zlight resource b ∈ zk, we get vk Y e∈zk {b} f(hz e ) ≥ vk Y e∈zk {b} f(hz e) ≥ c(hz b + 1)
b + 1) = c(hz b + 1)
b + 1) , as required. Now let us consider the rest of the agents.
Assume z is obtained by the one-step addition of a by agent i. In this case, i is the only agent with zi = zi. The required property for agent i follows directly from Ui(z ) > Ui(z). In the case of a two-step addition, let z = ` z−{i,j}, zi ∪ {b}, (zj {b}) ∪ {a}), where b is a z-heavy resource. For agent i, from Ui(z−i, zi ∪ {b}) > Ui(z) we get
Y e∈zi f(hz e)f(hz b + 1) ! vi − X e∈zi c(hz e) − c(hz b + 1) > 1 − Y e∈zi f(hz e) ! vi − X e∈zi c(hz e) ⇒ vi Y e∈zi f(hz e) > c(hz b + 1)
b + 1) , (3) and note that since hz b ≥ hz e for all e ∈ M and, in particular, for all z -light resources, then c(hz b + 1)
b + 1) ≥ c(hz e + 1)
e + 1) , (4) for any z -light resource e . 216 Now, since hz e ≥ hz e for all e ∈ M and b is z-heavy, then vi Y e∈zi {e } f(hz e ) ≥ vi Y e∈zi {e } f(hz e) = vi Y e∈(zi∪{b}) {e } f(hz e) ≥ vi Y e∈zi f(hz e) , for any z -light resource e . The above, coupled with (3) and (4), yields the required. For agent j we just use (2) with respect to z and the equality hz b = hz a . For any z -light resource e , vj Y e∈zj {e } f(hz e ) ≥ vi Y e∈zi {e } f(hz e) ≥ c(hz e + 1)
e + 1) = c(hz e + 1)
e + 1) .
Thus, since the number of resources is finite, there is a finite series of one- and two-step addition operations on x that leads to strategy profile y ∈ Σ1 with hy > hx , in contradiction to x ∈ Σ2 . This completes the proof.
In this paper, we introduce and investigate congestion settings with unreliable resources, in which the probability of a resource"s failure depends on the congestion experienced by this resource. We defined a class of congestion games with load-dependent failures (CGLFs), which generalizes the wellknown class of congestion games. We study the existence of pure strategy Nash equilibria and potential functions in the presented class of games. We show that these games do not, in general, possess pure strategy equilibria. Nevertheless, if the resource cost functions are nondecreasing then such equilibria are guaranteed to exist, despite the non-existence of a potential function.
The CGLF-model can be modified to the case where the agents pay only for non-faulty resources they selected. Both the model discussed in this paper and the modified one are reasonable. In the full version we will show that the modified model leads to similar results. In particular, we can show the existence of a pure strategy equilibrium for nondecreasing CGLFs also in the modified model.
In future research we plan to consider various extensions of CGLFs. In particular, we plan to consider CGLFs where the resources may have different costs and failure probabilities, as well as CGLFs in which the resource failure probabilities are mutually dependent. In addition, it is of interest to develop an efficient algorithm for the computation of pure strategy Nash equilibrium, as well as discuss the social (in)efficiency of the equilibria.
[1] H. Ackermann, H. R¨oglin, and B. V¨ocking. Pure nash equilibria in player-specific and weighted congestion games. In WINE-06, 2006. [2] G. Christodoulou and E. Koutsoupias. The price of anarchy of finite congestion games. In Proceedings of the 37th Annual ACM Symposium on Theory and Computing (STOC-05), 2005. [3] A. Fabrikant, C. Papadimitriou, and K. Talwar. The complexity of pure nash equilibria. In STOC-04, pages 604-612, 2004. [4] E. Koutsoupias and C. Papadimitriou. Worst-case equilibria. In Proceedings of the 16th Annual Symposium on Theoretical Aspects of Computer Science, pages 404-413, 1999. [5] K. Leyton-Brown and M. Tennenholtz. Local-effect games. In IJCAI-03, 2003. [6] I. Milchtaich. Congestion games with player-specific payoff functions. Games and Economic Behavior, 13:111-124, 1996. [7] D. Monderer. Solution-based congestion games.
Advances in Mathematical Economics, 8:397-407,
[8] D. Monderer. Multipotential games. In IJCAI-07,
[9] D. Monderer and L. Shapley. Potential games. Games and Economic Behavior, 14:124-143, 1996. [10] M. Penn, M. Polukarov, and M. Tennenholtz.

The goal of this research is to design and build a Scalable Distributed Information Management System (SDIMS) that aggregates information about large-scale networked systems and that can serve as a basic building block for a broad range of large-scale distributed applications. Monitoring, querying, and reacting to changes in the state of a distributed system are core components of applications such as system management [15, 31, 37, 42], service placement [14, 43], data sharing and caching [18, 29, 32, 35, 46], sensor monitoring and control [20, 21], multicast tree formation [8, 9, 33, 36, 38], and naming and request routing [10, 11]. We therefore speculate that a SDIMS in a networked system would provide a distributed operating systems backbone and facilitate the development and deployment of new distributed services.
For a large scale information system, hierarchical aggregation is a fundamental abstraction for scalability. Rather than expose all information to all nodes, hierarchical aggregation allows a node to access detailed views of nearby information and summary views of global information. In a SDIMS based on hierarchical aggregation, different nodes can therefore receive different answers to the query find a [nearby] node with at least 1 GB of free memory or find a [nearby] copy of file foo. A hierarchical system that aggregates information through reduction trees [21, 38] allows nodes to access information they care about while maintaining system scalability.
To be used as a basic building block, a SDIMS should have four properties. First, the system should be scalable: it should accommodate large numbers of participating nodes, and it should allow applications to install and monitor large numbers of data attributes. Enterprise and global scale systems today might have tens of thousands to millions of nodes and these numbers will increase over time. Similarly, we hope to support many applications, and each application may track several attributes (e.g., the load and free memory of a system"s machines) or millions of attributes (e.g., which files are stored on which machines).
Second, the system should have flexibility to accommodate a broad range of applications and attributes. For example, readdominated attributes like numCPUs rarely change in value, while write-dominated attributes like numProcesses change quite often.
An approach tuned for read-dominated attributes will consume high bandwidth when applied to write-dominated attributes. Conversely, an approach tuned for write-dominated attributes will suffer from unnecessary query latency or imprecision for read-dominated attributes. Therefore, a SDIMS should provide mechanisms to handle different types of attributes and leave the policy decision of tuning replication to the applications.
Third, a SDIMS should provide administrative isolation. In a large system, it is natural to arrange nodes in an organizational or an administrative hierarchy. A SDIMS should support administraSession 10: Distributed Information Systems 379 tive isolation in which queries about an administrative domain"s information can be satisfied within the domain so that the system can operate during disconnections from other domains, so that an external observer cannot monitor or affect intra-domain queries, and to support domain-scoped queries efficiently.
Fourth, the system must be robust to node failures and disconnections. A SDIMS should adapt to reconfigurations in a timely fashion and should also provide mechanisms so that applications can tradeoff the cost of adaptation with the consistency level in the aggregated results when reconfigurations occur.
We draw inspiration from two previous works: Astrolabe [38] and Distributed Hash Tables (DHTs).
Astrolabe [38] is a robust information management system.
Astrolabe provides the abstraction of a single logical aggregation tree that mirrors a system"s administrative hierarchy. It provides a general interface for installing new aggregation functions and provides eventual consistency on its data. Astrolabe is robust due to its use of an unstructured gossip protocol for disseminating information and its strategy of replicating all aggregated attribute values for a subtree to all nodes in the subtree. This combination allows any communication pattern to yield eventual consistency and allows any node to answer any query using local information. This high degree of replication, however, may limit the system"s ability to accommodate large numbers of attributes. Also, although the approach works well for read-dominated attributes, an update at one node can eventually affect the state at all nodes, which may limit the system"s flexibility to support write-dominated attributes.
Recent research in peer-to-peer structured networks resulted in Distributed Hash Tables (DHTs) [18, 28, 29, 32, 35, 46]-a data structure that scales with the number of nodes and that distributes the read-write load for different queries among the participating nodes. It is interesting to note that although these systems export a global hash table abstraction, many of them internally make use of what can be viewed as a scalable system of aggregation trees to, for example, route a request for a given key to the right DHT node. Indeed, rather than export a general DHT interface, Plaxton et al."s [28] original application makes use of hierarchical aggregation to allow nodes to locate nearby copies of objects. It seems appealing to develop a SDIMS abstraction that exposes this internal functionality in a general way so that scalable trees for aggregation can be a basic system building block alongside the DHTs.
At a first glance, it might appear to be obvious that simply fusing DHTs with Astrolabe"s aggregation abstraction will result in a SDIMS. However, meeting the SDIMS requirements forces a design to address four questions: (1) How to scalably map different attributes to different aggregation trees in a DHT mesh? (2) How to provide flexibility in the aggregation to accommodate different application requirements? (3) How to adapt a global, flat DHT mesh to attain administrative isolation property? and (4) How to provide robustness without unstructured gossip and total replication?
The key contributions of this paper that form the foundation of our SDIMS design are as follows.
attribute type and attribute name and that associates an aggregation function with a particular attribute type. This abstraction paves the way for utilizing the DHT system"s internal trees for aggregation and for achieving scalability with both nodes and attributes.
propagation of reads and writes and thus trade off update cost, read latency, replication, and staleness.
convergence and path locality properties in order to achieve administrative isolation.
by (a) providing temporal replication through lazy reaggregation that guarantees eventual consistency and (b) ensuring that our flexible API allows demanding applications gain additional robustness by using tunable spatial replication of data aggregates or by performing fast on-demand reaggregation to augment the underlying lazy reaggregation or by doing both.
We have built a prototype of SDIMS. Through simulations and micro-benchmark experiments on a number of department machines and PlanetLab [27] nodes, we observe that the prototype achieves scalability with respect to both nodes and attributes through use of its flexible API, inflicts an order of magnitude lower maximum node stress than unstructured gossiping schemes, achieves isolation properties at a cost of modestly increased read latency compared to flat DHTs, and gracefully handles node failures.
This initial study discusses key aspects of an ongoing system building effort, but it does not address all issues in building a SDIMS.
For example, we believe that our strategies for providing robustness will mesh well with techniques such as supernodes [22] and other ongoing efforts to improve DHTs [30] for further improving robustness. Also, although splitting aggregation among many trees improves scalability for simple queries, this approach may make complex and multi-attribute queries more expensive compared to a single tree. Additional work is needed to understand the significance of this limitation for real workloads and, if necessary, to adapt query planning techniques from DHT abstractions [16, 19] to scalable aggregation tree abstractions.
In Section 2, we explain the hierarchical aggregation abstraction that SDIMS provides to applications. In Sections 3 and 4, we describe the design of our system for achieving the flexibility, scalability, and administrative isolation requirements of a SDIMS. In Section 5, we detail the implementation of our prototype system.
Section 6 addresses the issue of adaptation to the topological reconfigurations. In Section 7, we present the evaluation of our system through large-scale simulations and microbenchmarks on real networks. Section 8 details the related work, and Section 9 summarizes our contribution.
Aggregation is a natural abstraction for a large-scale distributed information system because aggregation provides scalability by allowing a node to view detailed information about the state near it and progressively coarser-grained summaries about progressively larger subsets of a system"s data [38].
Our aggregation abstraction is defined across a tree spanning all nodes in the system. Each physical node in the system is a leaf and each subtree represents a logical group of nodes. Note that logical groups can correspond to administrative domains (e.g., department or university) or groups of nodes within a domain (e.g., 10 workstations on a LAN in CS department). An internal non-leaf node, which we call virtual node, is simulated by one or more physical nodes at the leaves of the subtree for which the virtual node is the root. We describe how to form such trees in a later section.
Each physical node has local data stored as a set of (attributeType, attributeName, value) tuples such as (configuration, numCPUs, 16), (mcast membership, session foo, yes), or (file stored, foo, myIPaddress). The system associates an aggregation function ftype with each attribute type, and for each level-i subtree Ti in the system, the system defines an aggregate value Vi,type,name for each (at380 tributeType, attributeName) pair as follows. For a (physical) leaf node T0 at level 0, V0,type,name is the locally stored value for the attribute type and name or NULL if no matching tuple exists. Then the aggregate value for a level-i subtree Ti is the aggregation function for the type, ftype computed across the aggregate values of each of Ti"s k children: Vi,type,name = ftype(V0 i−1,type,name,V1 i−1,type,name,...,Vk−1 i−1,type,name).
Although SDIMS allows arbitrary aggregation functions, it is often desirable that these functions satisfy the hierarchical computation property [21]: f(v1,...,vn)= f(f(v1,...,vs1 ), f(vs1+1,...,vs2 ), ..., f(vsk+1,...,vn)), where vi is the value of an attribute at node i. For example, the average operation, defined as avg(v1,...,vn) = 1/n.∑n i=0 vi, does not satisfy the property. Instead, if an attribute stores values as tuples (sum,count), the attribute satisfies the hierarchical computation property while still allowing the applications to compute the average from the aggregate sum and count values.
Finally, note that for a large-scale system, it is difficult or impossible to insist that the aggregation value returned by a probe corresponds to the function computed over the current values at the leaves at the instant of the probe. Therefore our system provides only weak consistency guarantees - specifically eventual consistency as defined in [38].
A major innovation of our work is enabling flexible aggregate computation and propagation. The definition of the aggregation abstraction allows considerable flexibility in how, when, and where aggregate values are computed and propagated. While previous systems [15, 29, 38, 32, 35, 46] implement a single static strategy, we argue that a SDIMS should provide flexible computation and propagation to efficiently support wide variety of applications with diverse requirements. In order to provide this flexibility, we develop a simple interface that decomposes the aggregation abstraction into three pieces of functionality: install, update, and probe.
This definition of the aggregation abstraction allows our system to provide a continuous spectrum of strategies ranging from lazy aggregate computation and propagation on reads to aggressive immediate computation and propagation on writes. In Figure 1, we illustrate both extreme strategies and an intermediate strategy.
Under the lazy Update-Local computation and propagation strategy, an update (or write) only affects local state. Then, a probe (or read) that reads a level-i aggregate value is sent up the tree to the issuing node"s level-i ancestor and then down the tree to the leaves. The system then computes the desired aggregate value at each layer up the tree until the level-i ancestor that holds the desired value.
Finally, the level-i ancestor sends the result down the tree to the issuing node. In the other extreme case of the aggressive Update-All immediate computation and propagation on writes [38], when an update occurs, changes are aggregated up the tree, and each new aggregate value is flooded to all of a node"s descendants. In this case, each level-i node not only maintains the aggregate values for the level-i subtree but also receives and locally stores copies of all of its ancestors" level- j ( j > i) aggregation values. Also, a leaf satisfies a probe for a level-i aggregate using purely local data. In an intermediate Update-Up strategy, the root of each subtree maintains the subtree"s current aggregate value, and when an update occurs, the leaf node updates its local state and passes the update to its parent, and then each successive enclosing subtree updates its aggregate value and passes the new value to its parent. This strategy satisfies a leaf"s probe for a level-i aggregate value by sending the probe up to the level-i ancestor of the leaf and then sending the aggregate value down to the leaf. Finally, notice that other strategies exist. In general, an Update-Upk-Downj strategy aggregates up to parameter description optional attrType Attribute Type aggrfunc Aggregation Function up How far upward each update is sent (default: all) X down How far downward each aggregate is sent (default: none) X domain Domain restriction (default: none) X expTime Expiry Time Table 1: Arguments for the install operation the kth level and propagates the aggregate values of a node at level l (s.t. l ≤ k) downward for j levels.
A SDIMS must provide a wide range of flexible computation and propagation strategies to applications for it to be a general abstraction. An application should be able to choose a particular mechanism based on its read-to-write ratio that reduces the bandwidth consumption while attaining the required responsiveness and precision. Note that the read-to-write ratio of the attributes that applications install vary extensively. For example, a read-dominated attribute like numCPUs rarely changes in value, while a writedominated attribute like numProcesses changes quite often. An aggregation strategy like Update-All works well for read-dominated attributes but suffers high bandwidth consumption when applied for write-dominated attributes. Conversely, an approach like UpdateLocal works well for write-dominated attributes but suffers from unnecessary query latency or imprecision for read-dominated attributes.
SDIMS also allows non-uniform computation and propagation across the aggregation tree with different up and down parameters in different subtrees so that applications can adapt with the spatial and temporal heterogeneity of read and write operations. With respect to spatial heterogeneity, access patterns may differ for different parts of the tree, requiring different propagation strategies for different parts of the tree. Similarly with respect to temporal heterogeneity, access patterns may change over time requiring different strategies over time.
We provide the flexibility described above by splitting the aggregation API into three functions: Install() installs an aggregation function that defines an operation on an attribute type and specifies the update strategy that the function will use, Update() inserts or modifies a node"s local value for an attribute, and Probe() obtains an aggregate value for a specified subtree. The install interface allows applications to specify the k and j parameters of the Update-Upk-Downj strategy along with the aggregation function.
The update interface invokes the aggregation of an attribute on the tree according to corresponding aggregation function"s aggregation strategy. The probe interface not only allows applications to obtain the aggregated value for a specified tree but also allows a probing node to continuously fetch the values for a specified time, thus enabling an application to adapt to spatial and temporal heterogeneity.
The rest of the section describes these three interfaces in detail.
The Install operation installs an aggregation function in the system. The arguments for this operation are listed in Table 1. The attrType argument denotes the type of attributes on which this aggregation function is invoked. Installed functions are soft state that must be periodically renewed or they will be garbage collected at expTime.
The arguments up and down specify the aggregate computation 381 Update Strategy On Update On Probe for Global Aggregate Value On Probe for Level-1 Aggregate Value Update-Local Update-Up Update-All Figure 1: Flexible API parameter description optional attrType Attribute Type attrName Attribute Name mode Continuous or One-shot (default: one-shot) X level Level at which aggregate is sought (default: at all levels) X up How far up to go and re-fetch the value (default: none) X down How far down to go and reaggregate (default: none) X expTime Expiry Time Table 2: Arguments for the probe operation and propagation strategy Update-Upk-Downj. The domain argument, if present, indicates that the aggregation function should be installed on all nodes in the specified domain; otherwise the function is installed on all nodes in the system.
The Update operation takes three arguments attrType, attrName, and value and creates a new (attrType, attrName, value) tuple or updates the value of an old tuple with matching attrType and attrName at a leaf node.
The update interface meshes with installed aggregate computation and propagation strategy to provide flexibility. In particular, as outlined above and described in detail in Section 5, after a leaf applies an update locally, the update may trigger re-computation of aggregate values up the tree and may also trigger propagation of changed aggregate values down the tree. Notice that our abstraction associates an aggregation function with only an attrType but lets updates specify an attrName along with the attrType. This technique helps achieve scalability with respect to nodes and attributes as described in Section 4.
The Probe operation returns the value of an attribute to an application. The complete argument set for the probe operation is shown in Table 2. Along with the attrName and the attrType arguments, a level argument specifies the level at which the answers are required for an attribute. In our implementation we choose to return results at all levels k < l for a level-l probe because (i) it is inexpensive as the nodes traversed for level-l probe also contain level k aggregates for k < l and as we expect the network cost of transmitting the additional information to be small for the small aggregates which we focus and (ii) it is useful as applications can efficiently get several aggregates with a single probe (e.g., for domain-scoped queries as explained in Section 4.2).
Probes with mode set to continuous and with finite expTime enable applications to handle spatial and temporal heterogeneity. When node A issues a continuous probe at level l for an attribute, then regardless of the up and down parameters, updates for the attribute at any node in A"s level-l ancestor"s subtree are aggregated up to level l and the aggregated value is propagated down along the path from the ancestor to A. Note that continuous mode enables SDIMS to support a distributed sensor-actuator mechanism where a sensor monitors a level-i aggregate with a continuous mode probe and triggers an actuator upon receiving new values for the probe.
The up and down arguments enable applications to perform ondemand fast re-aggregation during reconfigurations, where a forced re-aggregation is done for the corresponding levels even if the aggregated value is available, as we discuss in Section 6. When present, the up and down arguments are interpreted as described in the install operation.
At the API level, the up and down arguments in install API can be regarded as hints, since they suggest a computation strategy but do not affect the semantics of an aggregation function. A SDIMS implementation can dynamically adjust its up/down strategies for an attribute based on its measured read/write frequency. But a virtual intermediate node needs to know the current up and down propagation values to decide if the local aggregate is fresh in order to answer a probe. This is the key reason why up and down need to be statically defined at the install time and can not be specified in the update operation. In dynamic adaptation, we implement a leasebased mechanism where a node issues a lease to a parent or a child denoting that it will keep propagating the updates to that parent or child. We are currently evaluating different policies to decide when to issue a lease and when to revoke a lease.
Our design achieves scalability with respect to both nodes and attributes through two key ideas. First, it carefully defines the aggregation abstraction to mesh well with its underlying scalable DHT system. Second, it refines the basic DHT abstraction to form an Autonomous DHT (ADHT) to achieve the administrative isolation properties that are crucial to scaling for large real-world systems.
In this section, we describe these two ideas in detail.
In contrast to previous systems [4, 15, 38, 39, 45], SDIMS"s aggregation abstraction specifies both an attribute type and attribute name and associates an aggregation function with a type rather than just specifying and associating a function with a name. Installing a single function that can operate on many different named attributes matching a type improves scalability for sparse attribute types with large, sparsely-filled name spaces. For example, to construct a file location service, our interface allows us to install a single function that computes an aggregate value for any named file. A subtree"s aggregate value for (FILELOC, name) would be the ID of a node in the subtree that stores the named file. Conversely,
Astrolabe copes with sparse attributes by having aggregation functions compute sets or lists and suggests that scalability can be improved by representing such sets with Bloom filters [6]. Supporting sparse names within a type provides at least two advantages. First, when the value associated with a name is updated, only the state associ382
000
111 110
L0 L1 L2 L3 Figure 2: The DHT tree corresponding to key 111 (DHTtree111) and the corresponding aggregation tree. ated with that name needs to be updated and propagated to other nodes. Second, splitting values associated with different names into different aggregation values allows our system to leverage Distributed Hash Tables (DHTs) to map different names to different trees and thereby spread the function"s logical root node"s load and state across multiple physical nodes.
Given this abstraction, scalably mapping attributes to DHTs is straightforward. DHT systems assign a long, random ID to each node and define an algorithm to route a request for key k to a node rootk such that the union of paths from all nodes forms a tree DHTtreek rooted at the node rootk. Now, as illustrated in Figure 2, by aggregating an attribute along the aggregation tree corresponding to DHTtreek for k =hash(attribute type, attribute name), different attributes will be aggregated along different trees.
In comparison to a scheme where all attributes are aggregated along a single tree, aggregating along multiple trees incurs lower maximum node stress: whereas in a single aggregation tree approach, the root and the intermediate nodes pass around more messages than leaf nodes, in a DHT-based multi-tree, each node acts as an intermediate aggregation point for some attributes and as a leaf node for other attributes. Hence, this approach distributes the onus of aggregation across all nodes.
Aggregation trees should provide administrative isolation by ensuring that for each domain, the virtual node at the root of the smallest aggregation subtree containing all nodes of that domain is hosted by a node in that domain. Administrative isolation is important for three reasons: (i) for security - so that updates and probes flowing in a domain are not accessible outside the domain, (ii) for availability - so that queries for values in a domain are not affected by failures of nodes in other domains, and (iii) for efficiency - so that domain-scoped queries can be simple and efficient.
To provide administrative isolation to aggregation trees, a DHT should satisfy two properties:
the smallest possible domain.
nodes in a domain should converge at a node in that domain.
Existing DHTs support path locality [18] or can easily support it by using the domain nearness as the distance metric [7, 17], but they do not guarantee path convergence as those systems try to optimize the search path to the root to reduce response latency. For example,
Pastry [32] uses prefix routing in which each node"s routing table contains one row per hexadecimal digit in the nodeId space where the ith row contains a list of nodes whose nodeIds differ from the current node"s nodeId in the ith digit with one entry for each possible digit value. Given a routing topology, to route a packet to an arbitrary destination key, a node in Pastry forwards a packet to the node with a nodeId prefix matching the key in at least one more digit than the current node. If such a node is not known, the current node uses an additional data structure, the leaf set containing 110XX 010XX 011XX 100XX 101XX univ dep1 dep2 key = 111XX 011XX 100XX 101XX 110XX 010XX L1 L0 L2 Figure 3: Example shows how isolation property is violated with original Pastry. We also show the corresponding aggregation tree. 110XX 010XX 011XX 100XX 101XX univ dep1 dep2 key = 111XX X 011XX 100XX 101XX 110XX 010XX L0 L1 L2 Figure 4: Autonomous DHT satisfying the isolation property.
Also the corresponding aggregation tree is shown.
L immediate higher and lower neighbors in the nodeId space, and forwards the packet to a node with an identical prefix but that is numerically closer to the destination key in the nodeId space. This process continues until the destination node appears in the leaf set, after which the message is routed directly. Pastry"s expected number of routing steps is logn, where n is the number of nodes, but as Figure 3 illustrates, this algorithm does not guarantee path convergence: if two nodes in a domain have nodeIds that match a key in the same number of bits, both of them can route to a third node outside the domain when routing for that key.
Simple modifications to Pastry"s route table construction and key-routing protocols yield an Autonomous DHT (ADHT) that satisfies the path locality and path convergence properties. As Figure 4 illustrates, whenever two nodes in a domain share the same prefix with respect to a key and no other node in the domain has a longer prefix, our algorithm introduces a virtual node at the boundary of the domain corresponding to that prefix plus the next digit of the key; such a virtual node is simulated by the existing node whose id is numerically closest to the virtual node"s id. Our ADHT"s routing table differs from Pastry"s in two ways. First, each node maintains a separate leaf set for each domain of which it is a part. Second, nodes use two proximity metrics when populating the routing tables - hierarchical domain proximity is the primary metric and network distance is secondary. Then, to route a packet to a global root for a key, ADHT routing algorithm uses the routing table and the leaf set entries to route to each successive enclosing domain"s root (the virtual or real node in the domain matching the key in the maximum number of digits). Additional details about the ADHT algorithm are available in an extended technical report [44].
Properties. Maintaining a different leaf set for each administrative hierarchy level increases the number of neighbors that each node tracks to (2b)∗lgb n+c.l from (2b)∗lgb n+c in unmodified Pastry, where b is the number of bits in a digit, n is the number of nodes, c is the leaf set size, and l is the number of domain levels.
Routing requires O(lgbn + l) steps compared to O(lgbn) steps in Pastry; also, each routing hop may be longer than in Pastry because the modified algorithm"s routing table prefers same-domain nodes over nearby nodes. We experimentally quantify the additional routing costs in Section 7.
In a large system, the ADHT topology allows domains to im383 A1 A2 B1 ((B1.B.,1), (B.,1),(.,1)) ((B1.B.,1), (B.,1),(.,1)) L2 L1 L0 ((B1.B.,1), (B.,1),(.,3)) ((A1.A.,1), (A.,2),(.,2)) ((A1.A.,1), (A.,1),(.,1)) ((A2.A.,1), (A.,1),(.,1)) Figure 5: Example for domain-scoped queries prove security for sensitive attribute types by installing them only within a specified domain. Then, aggregation occurs entirely within the domain and a node external to the domain can neither observe nor affect the updates and aggregation computations of the attribute type. Furthermore, though we have not implemented this feature in the prototype, the ADHT topology would also support domainrestricted probes that could ensure that no one outside of a domain can observe a probe for data stored within the domain.
The ADHT topology also enhances availability by allowing the common case of probes for data within a domain to depend only on a domain"s nodes. This, for example, allows a domain that becomes disconnected from the rest of the Internet to continue to answer queries for local data.
Aggregation trees that provide administrative isolation also enable the definition of simple and efficient domain-scoped aggregation functions to support queries like what is the average load on machines in domain X? For example, consider an aggregation function to count the number of machines in an example system with three machines illustrated in Figure 5. Each leaf node l updates attribute NumMachines with a value vl containing a set of tuples of form (Domain, Count) for each domain of which the node is a part. In the example, the node A1 with name A1.A. performs an update with the value ((A1.A.,1),(A.,1),(.,1)). An aggregation function at an internal virtual node hosted on node N with child set C computes the aggregate as a set of tuples: for each domain D that N is part of, form a tuple (D,∑c∈C(count|(D,count) ∈ vc)). This computation is illustrated in the Figure 5. Now a query for NumMachines with level set to MAX will return the aggregate values at each intermediate virtual node on the path to the root as a set of tuples (tree level, aggregated value) from which it is easy to extract the count of machines at each enclosing domain. For example, A1 would receive ((2, ((B1.B.,1),(B.,1),(.,3))), (1, ((A1.A.,1),(A.,2),(.,2))), (0, ((A1.A.,1),(A.,1),(.,1)))). Note that supporting domain-scoped queries would be less convenient and less efficient if aggregation trees did not conform to the system"s administrative structure. It would be less efficient because each intermediate virtual node will have to maintain a list of all values at the leaves in its subtree along with their names and it would be less convenient as applications that need an aggregate for a domain will have to pick values of nodes in that domain from the list returned by a probe and perform computation.
The internal design of our SDIMS prototype comprises of two layers: the Autonomous DHT (ADHT) layer manages the overlay topology of the system and the Aggregation Management Layer (AML) maintains attribute tuples, performs aggregations, stores and propagates aggregate values. Given the ADHT construction described in Section 4.2, each node implements an Aggregation Management Layer (AML) to support the flexible API described in Section 3. In this section, we describe the internal state and operation of the AML layer of a node in the system. local MIB MIBs ancestor reduction MIB (level 1)MIBs ancestor MIB from child 0X...
MIB from child 0X...
Level 2 Level 1 Level 3 Level 0 1XXX... 10XX... 100X...
From parents0X..
To parent 0X... −− aggregation functions From parents To parent 10XX... 1X.. 1X.. 1X..
To parent 11XX...
Node Id: (1001XXX) 1001X.. 100X.. 10X.. 1X..
Virtual Node Figure 6: Example illustrating the data structures and the organization of them at a node.
We refer to a store of (attribute type, attribute name, value) tuples as a Management Information Base or MIB, following the terminology from Astrolabe [38] and SNMP [34]. We refer an (attribute type, attribute name) tuple as an attribute key.
As Figure 6 illustrates, each physical node in the system acts as several virtual nodes in the AML: a node acts as leaf for all attribute keys, as a level-1 subtree root for keys whose hash matches the node"s ID in b prefix bits (where b is the number of bits corrected in each step of the ADHT"s routing scheme), as a level-i subtree root for attribute keys whose hash matches the node"s ID in the initial i ∗ b bits, and as the system"s global root for attribute keys whose hash matches the node"s ID in more prefix bits than any other node (in case of a tie, the first non-matching bit is ignored and the comparison is continued [46]).
To support hierarchical aggregation, each virtual node at the root of a level-i subtree maintains several MIBs that store (1) child MIBs containing raw aggregate values gathered from children, (2) a reduction MIB containing locally aggregated values across this raw information, and (3) an ancestor MIB containing aggregate values scattered down from ancestors. This basic strategy of maintaining child, reduction, and ancestor MIBs is based on Astrolabe [38], but our structured propagation strategy channels information that flows up according to its attribute key and our flexible propagation strategy only sends child updates up and ancestor aggregate results down as far as specified by the attribute key"s aggregation function. Note that in the discussion below, for ease of explanation, we assume that the routing protocol is correcting single bit at a time (b = 1). Our system, built upon Pastry, handles multi-bit correction (b = 4) and is a simple extension to the scheme described here.
For a given virtual node ni at level i, each child MIB contains the subset of a child"s reduction MIB that contains tuples that match ni"s node ID in i bits and whose up aggregation function attribute is at least i. These local copies make it easy for a node to recompute a level-i aggregate value when one child"s input changes. Nodes maintain their child MIBs in stable storage and use a simplified version of the Bayou log exchange protocol (sans conflict detection and resolution) for synchronization after disconnections [26].
Virtual node ni at level i maintains a reduction MIB of tuples with a tuple for each key present in any child MIB containing the attribute type, attribute name, and output of the attribute type"s aggregate functions applied to the children"s tuples.
A virtual node ni at level i also maintains an ancestor MIB to store the tuples containing attribute key and a list of aggregate values at different levels scattered down from ancestors. Note that the 384 list for a key might contain multiple aggregate values for a same level but aggregated at different nodes (see Figure 4). So, the aggregate values are tagged not only with level information, but are also tagged with ID of the node that performed the aggregation.
Level-0 differs slightly from other levels. Each level-0 leaf node maintains a local MIB rather than maintaining child MIBs and a reduction MIB. This local MIB stores information about the local node"s state inserted by local applications via update() calls. We envision various sensor programs and applications insert data into local MIB. For example, one program might monitor local configuration and perform updates with information such as total memory, free memory, etc., A distributed file system might perform update for each file stored on the local node.
Along with these MIBs, a virtual node maintains two other tables: an aggregation function table and an outstanding probes table. An aggregation function table contains the aggregation function and installation arguments (see Table 1) associated with an attribute type or an attribute type and name. Each aggregate function is installed on all nodes in a domain"s subtree, so the aggregate function table can be thought of as a special case of the ancestor MIB with domain functions always installed up to a root within a specified domain and down to all nodes within the domain. The outstanding probes table maintains temporary information regarding in-progress probes.
Given these data structures, it is simple to support the three API functions described in Section 3.1.
Install The Install operation (see Table 1) installs on a domain an aggregation function that acts on a specified attribute type.
Execution of an install operation for function aggrFunc on attribute type attrType proceeds in two phases: first the install request is passed up the ADHT tree with the attribute key (attrType, null) until it reaches the root for that key within the specified domain. Then, the request is flooded down the tree and installed on all intermediate and leaf nodes.
Update When a level i virtual node receives an update for an attribute from a child below: it first recomputes the level-i aggregate value for the specified key, stores that value in its reduction MIB and then, subject to the function"s up and domain parameters, passes the updated value to the appropriate parent based on the attribute key. Also, the level-i (i ≥ 1) virtual node sends the updated level-i aggregate to all its children if the function"s down parameter exceeds zero. Upon receipt of a level-i aggregate from a parent, a level k virtual node stores the value in its ancestor MIB and, if k ≥ i−down, forwards this aggregate to its children.
Probe A Probe collects and returns the aggregate value for a specified attribute key for a specified level of the tree. As Figure 1 illustrates, the system satisfies a probe for a level-i aggregate value using a four-phase protocol that may be short-circuited when updates have previously propagated either results or partial results up or down the tree. In phase 1, the route probe phase, the system routes the probe up the attribute key"s tree to either the root of the level-i subtree or to a node that stores the requested value in its ancestor MIB. In the former case, the system proceeds to phase 2 and in the latter it skips to phase 4. In phase 2, the probe scatter phase, each node that receives a probe request sends it to all of its children unless the node"s reduction MIB already has a value that matches the probe"s attribute key, in which case the node initiates phase 3 on behalf of its subtree. In phase 3, the probe aggregation phase, when a node receives values for the specified key from each of its children, it executes the aggregate function on these values and either (a) forwards the result to its parent (if its level is less than i) or (b) initiates phase 4 (if it is at level i). Finally, in phase 4, the aggregate routing phase the aggregate value is routed down to the node that requested it. Note that in the extreme case of a function installed with up = down = 0, a level-i probe can touch all nodes in a level-i subtree while in the opposite extreme case of a function installed with up = down = ALL, probe is a completely local operation at a leaf.
For probes that include phases 2 (probe scatter) and 3 (probe aggregation), an issue is how to decide when a node should stop waiting for its children to respond and send up its current aggregate value. A node stops waiting for its children when one of three conditions occurs: (1) all children have responded, (2) the ADHT layer signals one or more reconfiguration events that mark all children that have not yet responded as unreachable, or (3) a watchdog timer for the request fires. The last case accounts for nodes that participate in the ADHT protocol but that fail at the AML level.
At a virtual node, continuous probes are handled similarly as one-shot probes except that such probes are stored in the outstanding probe table for a time period of expTime specified in the probe.
Thus each update for an attribute triggers re-evaluation of continuous probes for that attribute.
We implement a lease-based mechanism for dynamic adaptation.
A level-l virtual node for an attribute can issue the lease for levell aggregate to a parent or a child only if up is greater than l or it has leases from all its children. A virtual node at level l can issue the lease for level-k aggregate for k > l to a child only if down≥ k −l or if it has the lease for that aggregate from its parent. Now a probe for level-k aggregate can be answered by level-l virtual node if it has a valid lease, irrespective of the up and down values. We are currently designing different policies to decide when to issue a lease and when to revoke a lease and are also evaluating them with the above mechanism.
Our current prototype does not implement access control on install, update, and probe operations but we plan to implement Astrolabe"s [38] certificate-based restrictions. Also our current prototype does not restrict the resource consumption in executing the aggregation functions; but, ‘techniques from research on resource management in server systems and operating systems [2, 3] can be applied here.
In large scale systems, reconfigurations are common. Our two main principles for robustness are to guarantee (i) read availability - probes complete in finite time, and (ii) eventual consistency - updates by a live node will be visible to probes by connected nodes in finite time. During reconfigurations, a probe might return a stale value for two reasons. First, reconfigurations lead to incorrectness in the previous aggregate values. Second, the nodes needed for aggregation to answer the probe become unreachable. Our system also provides two hooks that applications can use for improved end-to-end robustness in the presence of reconfigurations: (1) Ondemand re-aggregation and (2) application controlled replication.
Our system handles reconfigurations at two levels - adaptation at the ADHT layer to ensure connectivity and adaptation at the AML layer to ensure access to the data in SDIMS.
Our ADHT layer adaptation algorithm is same as Pastry"s adaptation algorithm [32] - the leaf sets are repaired as soon as a reconfiguration is detected and the routing table is repaired lazily. Note that maintaining extra leaf sets does not degrade the fault-tolerance property of the original Pastry; indeed, it enhances the resilience of ADHTs to failures by providing additional routing links. Due to redundancy in the leaf sets and the routing table, updates can be routed towards their root nodes successfully even during failures. 385 Reconfig reconfig notices DHT partial DHT complete DHT ends Lazy Time Data
Lazy Data starts Lazy Data starts Lazy Data repairrepair reaggr reaggr reaggr reaggr happens Figure 7: Default lazy data re-aggregation time line Also note that the administrative isolation property satisfied by our ADHT algorithm ensures that the reconfigurations in a level i domain do not affect the probes for level i in a sibling domain.
Broadly, we use two types of strategies for AML adaptation in the face of reconfigurations: (1) Replication in time as a fundamental baseline strategy, and (2) Replication in space as an additional performance optimization that falls back on replication in time when the system runs out of replicas. We provide two mechanisms for replication in time. First, lazy re-aggregation propagates already received updates to new children or new parents in a lazy fashion over time. Second, applications can reduce the probability of probe response staleness during such repairs through our flexible API with appropriate setting of the down parameter.
Lazy Re-aggregation: The DHT layer informs the AML layer about reconfigurations in the network using the following three function calls - newParent, failedChild, and newChild. On newParent(parent, prefix), all probes in the outstanding-probes table corresponding to prefix are re-evaluated. If parent is not null, then aggregation functions and already existing data are lazily transferred in the background. Any new updates, installs, and probes for this prefix are sent to the parent immediately. On failedChild(child, prefix), the AML layer marks the child as inactive and any outstanding probes that are waiting for data from this child are re-evaluated.
On newChild(child, prefix), the AML layer creates space in its data structures for this child.
Figure 7 shows the time line for the default lazy re-aggregation upon reconfiguration. Probes initiated between points 1 and 2 and that are affected by reconfigurations are reevaluated by AML upon detecting the reconfiguration. Probes that complete or start between points 2 and 8 may return stale answers.
On-demand Re-aggregation: The default lazy aggregation scheme lazily propagates the old updates in the system.
Additionally, using up and down knobs in the Probe API, applications can force on-demand fast re-aggregation of updates to avoid staleness in the face of reconfigurations. In particular, if an application detects or suspects an answer as stale, then it can re-issue the probe increasing the up and down parameters to force the refreshing of the cached data. Note that this strategy will be useful only after the DHT adaptation is completed (Point 6 on the time line in Figure 7).
Replication in Space: Replication in space is more challenging in our system than in a DHT file location application because replication in space can be achieved easily in the latter by just replicating the root node"s contents. In our system, however, all internal nodes have to be replicated along with the root.
In our system, applications control replication in space using up and down knobs in the Install API; with large up and down values, aggregates at the intermediate virtual nodes are propagated to more nodes in the system. By reducing the number of nodes that have to be accessed to answer a probe, applications can reduce the probability of incorrect results occurring due to the failure of nodes that do not contribute to the aggregate. For example, in a file location application, using a non-zero positive down parameter ensures that a file"s global aggregate is replicated on nodes other than the root.
1 10 100 1000 10000
Avg.numberofmessagesperoperation Read to Write ratio Update-All Up=ALL, Down=9 Up=ALL, Down=6 Update-Up Update-Local Up=2, Down=0 Up=5, Down=0 Figure 8: Flexibility of our approach. With different UP and DOWN values in a network of 4096 nodes for different readwrite ratios.
Probes for the file location can then be answered without accessing the root; hence they are not affected by the failure of the root.
However, note that this technique is not appropriate in some cases. An aggregated value in file location system is valid as long as the node hosting the file is active, irrespective of the status of other nodes in the system; whereas an application that counts the number of machines in a system may receive incorrect results irrespective of the replication. If reconfigurations are only transient (like a node temporarily not responding due to a burst of load), the replicated aggregate closely or correctly resembles the current state.
We have implemented a prototype of SDIMS in Java using the FreePastry framework [32] and performed large-scale simulation experiments and micro-benchmark experiments on two real networks: 187 machines in the department and 69 machines on the PlanetLab [27] testbed. In all experiments, we use static up and down values and turn off dynamic adaptation. Our evaluation supports four main conclusions. First, flexible API provides different propagation strategies that minimize communication resources at different read-to-write ratios. For example, in our simulation we observe Update-Local to be efficient for read-to-write ratios below 0.0001, Update-Up around 1, and Update-All above 50000.
Second, our system is scalable with respect to both nodes and attributes. In particular, we find that the maximum node stress in our system is an order lower than observed with an Update-All, gossiping approach. Third, in contrast to unmodified Pastry which violates path convergence property in upto 14% cases, our system conforms to the property. Fourth, the system is robust to reconfigurations and adapts to failures with in a few seconds.
Flexibility and Scalability: A major innovation of our system is its ability to provide flexible computation and propagation of aggregates. In Figure 8, we demonstrate the flexibility exposed by the aggregation API explained in Section 3. We simulate a system with
(bf) of 16 and install several attributes with different up and down parameters. We plot the average number of messages per operation incurred for a wide range of read-to-write ratios of the operations for different attributes. Simulations with other sizes of networks with different branching factors reveal similar results. This graph clearly demonstrates the benefit of supporting a wide range of computation and propagation strategies. Although having a small UP 386 1 10 100 1000 10000 100000 1e+06 1e+07
MaximumNodeStress Number of attributes installed Gossip 256 Gossip 4096 Gossip 65536 DHT 256 DHT 4096 DHT 65536 Figure 9: Max node stress for a gossiping approach vs. ADHT based approach for different number of nodes with increasing number of sparse attributes. value is efficient for attributes with low read-to-write ratios (write dominated applications), the probe latency, when reads do occur, may be high since the probe needs to aggregate the data from all the nodes that did not send their aggregate up. Conversely, applications that wish to improve probe overheads or latencies can increase their UP and DOWN propagation at a potential cost of increase in write overheads.
Compared to an existing Update-all single aggregation tree approach [38], scalability in SDIMS comes from (1) leveraging DHTs to form multiple aggregation trees that split the load across nodes and (2) flexible propagation that avoids propagation of all updates to all nodes. Figure 9 demonstrates the SDIMS"s scalability with nodes and attributes. For this experiment, we build a simulator to simulate both Astrolabe [38] (a gossiping, Update-All approach) and our system for an increasing number of sparse attributes. Each attribute corresponds to the membership in a multicast session with a small number of participants. For this experiment, the session size is set to 8, the branching factor is set to 16, the propagation mode for SDIMS is Update-Up, and the participant nodes perform continuous probes for the global aggregate value. We plot the maximum node stress (in terms of messages) observed in both schemes for different sized networks with increasing number of sessions when the participant of each session performs an update operation.
Clearly, the DHT based scheme is more scalable with respect to attributes than an Update-all gossiping scheme. Observe that at some constant number of attributes, as the number of nodes increase in the system, the maximum node stress increases in the gossiping approach, while it decreases in our approach as the load of aggregation is spread across more nodes. Simulations with other session sizes (4 and 16) yield similar results.
Administrative Hierarchy and Robustness: Although the routing protocol of ADHT might lead to an increased number of hops to reach the root for a key as compared to original Pastry, the algorithm conforms to the path convergence and locality properties and thus provides administrative isolation property. In Figure 10, we quantify the increased path length by comparisons with unmodified Pastry for different sized networks with different branching factors of the domain hierarchy tree. To quantify the path convergence property, we perform simulations with a large number of probe pairs - each pair probing for a random key starting from two randomly chosen nodes. In Figure 11, we plot the percentage of probe pairs for unmodified pastry that do not conform to the path convergence property. When the branching factor is low, the domain hierarchy tree is deeper resulting in a large difference between 0 1 2 3 4 5 6 7
PathLength Number of Nodes ADHT bf=4 ADHT bf=16 ADHT bf=64 PASTRY bf=4,16,64 Figure 10: Average path length to root in Pastry versus ADHT for different branching factors. Note that all lines corresponding to Pastry overlap. 0 2 4 6 8 10 12 14 16
Percentageofviolations Number of Nodes bf=4 bf=16 bf=64 Figure 11: Percentage of probe pairs whose paths to the root did not conform to the path convergence property with Pastry.
U pdate-All U pdate-U p U pdate-Local 0 200 400 600 800 Latency(inms) Average Latency U pdate-All U pdate-U p U pdate-Local 0 1000 2000 3000 Latency(inms) Average Latency (a) (b) Figure 12: Latency of probes for aggregate at global root level with three different modes of aggregate propagation on (a) department machines, and (b) PlanetLab machines Pastry and ADHT in the average path length; but it is at these small domain sizes, that the path convergence fails more often with the original Pastry.
We run our prototype on 180 department machines (some machines ran multiple node instances, so this configuration has a total of 283 SDIMS nodes) and also on 69 machines of the PlanetLab [27] testbed. We measure the performance of our system with two micro-benchmarks. In the first micro-benchmark, we install three aggregation functions of types Update-Local, Update-Up, and Update-All, perform update operation on all nodes for all three aggregation functions, and measure the latencies incurred by probes for the global aggregate from all nodes in the system. Figure 12 387 0 20 40 60 80 100 120 140
2700 2720 2740 2760 2780 2800 2820 2840 Latency(inms) ValuesObserved Time(in sec) Values latency Node Killed Figure 13: Micro-benchmark on department network showing the behavior of the probes from a single node when failures are happening at some other nodes. All 283 nodes assign a value of
10 100 1000 10000 100000
500 550 600 650 700 Latency(inms) ValuesObserved Time(in sec) Values latency Node Killed Figure 14: Probe performance during failures on 69 machines of PlanetLab testbed shows the observed latencies for both testbeds. Notice that the latency in Update-Local is high compared to the Update-UP policy.
This is because latency in Update-Local is affected by the presence of even a single slow machine or a single machine with a high latency network connection.
In the second benchmark, we examine robustness. We install one aggregation function of type Update-Up that performs sum operation on an integer valued attribute. Each node updates the attribute with the value 10. Then we monitor the latencies and results returned on the probe operation for global aggregate on one chosen node, while we kill some nodes after every few probes. Figure 13 shows the results on the departmental testbed. Due to the nature of the testbed (machines in a department), there is little change in the latencies even in the face of reconfigurations. In Figure 14, we present the results of the experiment on PlanetLab testbed. The root node of the aggregation tree is terminated after about 275 seconds. There is a 5X increase in the latencies after the death of the initial root node as a more distant node becomes the root node after repairs. In both experiments, the values returned on probes start reflecting the correct situation within a short time after the failures.
From both the testbed benchmark experiments and the simulation experiments on flexibility and scalability, we conclude that (1) the flexibility provided by SDIMS allows applications to tradeoff read-write overheads (Figure 8), read latency, and sensitivity to slow machines (Figure 12), (2) a good default aggregation strategy is Update-Up which has moderate overheads on both reads and writes (Figure 8), has moderate read latencies (Figure 12), and is scalable with respect to both nodes and attributes (Figure 9), and (3) small domain sizes are the cases where DHT algorithms fail to provide path convergence more often and SDIMS ensures path convergence with only a moderate increase in path lengths (Figure 11).
SDIMS is designed as a general distributed monitoring and control infrastructure for a broad range of applications. Above, we discuss some simple microbenchmarks including a multicast membership service and a calculate-sum function. Van Renesse et al. [38] provide detailed examples of how such a service can be used for a peer-to-peer caching directory, a data-diffusion service, a publishsubscribe system, barrier synchronization, and voting.
Additionally, we have initial experience using SDIMS to construct two significant applications: the control plane for a large-scale distributed file system [12] and a network monitor for identifying heavy hitters that consume excess resources.
Distributed file system control: The PRACTI (Partial Replication, Arbitrary Consistency, Topology Independence) replication system provides a set of mechanisms for data replication over which arbitrary control policies can be layered. We use SDIMS to provide several key functions in order to create a file system over the lowlevel PRACTI mechanisms.
First, nodes use SDIMS as a directory to handle read misses.
When a node n receives an object o, it updates the (ReadDir, o) attribute with the value n; when n discards o from its local store, it resets (ReadDir, o) to NULL. At each virtual node, the ReadDir aggregation function simply selects a random non-null child value (if any) and we use the Update-Up policy for propagating updates.
Finally, to locate a nearby copy of an object o, a node n1 issues a series of probe requests for the (ReadDir, o) attribute, starting with level = 1 and increasing the level value with each repeated probe request until a non-null node ID n2 is returned. n1 then sends a demand read request to n2, and n2 sends the data if it has it.
Conversely, if n2 does not have a copy of o, it sends a nack to n1, and n1 issues a retry probe with the down parameter set to a value larger than used in the previous probe in order to force on-demand re-aggregation, which will yield a fresher value for the retry.
Second, nodes subscribe to invalidations and updates to interest sets of files, and nodes use SDIMS to set up and maintain perinterest-set network-topology-sensitive spanning trees for propagating this information. To subscribe to invalidations for interest set i, a node n1 first updates the (Inval, i) attribute with its identity n1, and the aggregation function at each virtual node selects one non-null child value. Finally, n1 probes increasing levels of the the (Inval, i) attribute until it finds the first node n2 = n1; n1 then uses n2 as its parent in the spanning tree. n1 also issues a continuous probe for this attribute at this level so that it is notified of any change to its spanning tree parent. Spanning trees for streams of pushed updates are maintained in a similar manner.
In the future, we plan to use SDIMS for at least two additional services within this replication system. First, we plan to use SDIMS to track the read and write rates to different objects; prefetch algorithms will use this information to prioritize replication [40, 41].
Second, we plan to track the ranges of invalidation sequence numbers seen by each node for each interest set in order to augment the spanning trees described above with additional hole filling to allow nodes to locate specific invalidations they have missed.
Overall, our initial experience with using SDIMS for the PRACTII replication system suggests that (1) the general aggregation interface provided by SDIMS simplifies the construction of distributed applications-given the low-level PRACTI mechanisms, 388 we were able to construct a basic file system that uses SDIMS for several distinct control tasks in under two weeks and (2) the weak consistency guarantees provided by SDIMS meet the requirements of this application-each node"s controller effectively treats information from SDIMS as hints, and if a contacted node does not have the needed data, the controller retries, using SDIMS on-demand reaggregation to obtain a fresher hint.
Distributed heavy hitter problem: The goal of the heavy hitter problem is to identify network sources, destinations, or protocols that account for significant or unusual amounts of traffic. As noted by Estan et al. [13], this information is useful for a variety of applications such as intrusion detection (e.g., port scanning), denial of service detection, worm detection and tracking, fair network allocation, and network maintenance. Significant work has been done on developing high-performance stream-processing algorithms for identifying heavy hitters at one router, but this is just a first step; ideally these applications would like not just one router"s views of the heavy hitters but an aggregate view.
We use SDIMS to allow local information about heavy hitters to be pooled into a view of global heavy hitters. For each destination IP address IPx, a node updates the attribute (DestBW,IPx) with the number of bytes sent to IPx in the last time window. The aggregation function for attribute type DestBW is installed with the Update-UP strategy and simply adds the values from child nodes.
Nodes perform continuous probe for global aggregate of the attribute and raise an alarm when the global aggregate value goes above a specified limit. Note that only nodes sending data to a particular IP address perform probes for the corresponding attribute.
Also note that techniques from [25] can be extended to hierarchical case to tradeoff precision for communication bandwidth.
The aggregation abstraction we use in our work is heavily influenced by the Astrolabe [38] project. Astrolabe adopts a PropagateAll and unstructured gossiping techniques to attain robustness [5].
However, any gossiping scheme requires aggressive replication of the aggregates. While such aggressive replication is efficient for read-dominated attributes, it incurs high message cost for attributes with a small read-to-write ratio. Our approach provides a flexible API for applications to set propagation rules according to their read-to-write ratios. Other closely related projects include Willow [39], Cone [4], DASIS [1], and SOMO [45]. Willow, DASIS and SOMO build a single tree for aggregation. Cone builds a tree per attribute and requires a total order on the attribute values.
Several academic [15, 21, 42] and commercial [37] distributed monitoring systems have been designed to monitor the status of large networked systems. Some of them are centralized where all the monitoring data is collected and analyzed at a central host.
Ganglia [15, 23] uses a hierarchical system where the attributes are replicated within clusters using multicast and then cluster aggregates are further aggregated along a single tree. Sophia [42] is a distributed monitoring system designed with a declarative logic programming model where the location of query execution is both explicit in the language and can be calculated during evaluation.
This research is complementary to our work. TAG [21] collects information from a large number of sensors along a single tree.
The observation that DHTs internally provide a scalable forest of reduction trees is not new. Plaxton et al."s [28] original paper describes not a DHT, but a system for hierarchically aggregating and querying object location data in order to route requests to nearby copies of objects. Many systems-building upon both Plaxton"s bit-correcting strategy [32, 46] and upon other strategies [24, 29, 35]-have chosen to hide this power and export a simple and general distributed hash table abstraction as a useful building block for a broad range of distributed applications. Some of these systems internally make use of the reduction forest not only for routing but also for caching [32], but for simplicity, these systems do not generally export this powerful functionality in their external interface.
Our goal is to develop and expose the internal reduction forest of DHTs as a similarly general and useful abstraction.
Although object location is a predominant target application for DHTs, several other applications like multicast [8, 9, 33, 36] and DNS [11] are also built using DHTs. All these systems implicitly perform aggregation on some attribute, and each one of them must be designed to handle any reconfigurations in the underlying DHT.
With the aggregation abstraction provided by our system, designing and building of such applications becomes easier.
Internal DHT trees typically do not satisfy domain locality properties required in our system. Castro et al. [7] and Gummadi et al. [17] point out the importance of path convergence from the perspective of achieving efficiency and investigate the performance of Pastry and other DHT algorithms, respectively. SkipNet [18] provides domain restricted routing where a key search is limited to the specified domain. This interface can be used to ensure path convergence by searching in the lowest domain and moving up to the next domain when the search reaches the root in the current domain.
Although this strategy guarantees path convergence, it loses the aggregation tree abstraction property of DHTs as the domain constrained routing might touch a node more than once (as it searches forward and then backward to stay within a domain).
This paper presents a Scalable Distributed Information Management System (SDIMS) that aggregates information in large-scale networked systems and that can serve as a basic building block for a broad range of applications. For large scale systems, hierarchical aggregation is a fundamental abstraction for scalability.
We build our system by extending ideas from Astrolabe and DHTs to achieve (i) scalability with respect to both nodes and attributes through a new aggregation abstraction that helps leverage DHT"s internal trees for aggregation, (ii) flexibility through a simple API that lets applications control propagation of reads and writes, (iii) administrative isolation through simple augmentations of current DHT algorithms, and (iv) robustness to node and network reconfigurations through lazy reaggregation, on-demand reaggregation, and tunable spatial replication.
Acknowlegements We are grateful to J.C. Browne, Robert van Renessee, Amin Vahdat, Jay Lepreau, and the anonymous reviewers for their helpful comments on this work.
[1] K. Albrecht, R. Arnold, M. Gahwiler, and R. Wattenhofer.
Join and Leave in Peer-to-Peer Systems: The DASIS approach. Technical report, CS, ETH Zurich, 2003. [2] G. Back, W. H. Hsieh, and J. Lepreau. Processes in KaffeOS: Isolation, Resource Management, and Sharing in Java. In Proc. OSDI, Oct 2000. [3] G. Banga, P. Druschel, and J. Mogul. Resource Containers: A New Facility for Resource Management in Server Systems. In OSDI99, Feb. 1999. [4] R. Bhagwan, P. Mahadevan, G. Varghese, and G. M. Voelker.
Cone: A Distributed Heap-Based Approach to Resource Selection. Technical Report CS2004-0784, UCSD, 2004. 389 [5] K. P. Birman. The Surprising Power of Epidemic Communication. In Proceedings of FuDiCo, 2003. [6] B. Bloom. Space/time tradeoffs in hash coding with allowable errors. Comm. of the ACM, 13(7):422-425, 1970. [7] M. Castro, P. Druschel, Y. C. Hu, and A. Rowstron.
Exploiting Network Proximity in Peer-to-Peer Overlay Networks. Technical Report MSR-TR-2002-82, MSR. [8] M. Castro, P. Druschel, A.-M. Kermarrec, A. Nandi,
A. Rowstron, and A. Singh. SplitStream: High-bandwidth Multicast in a Cooperative Environment. In SOSP, 2003. [9] M. Castro, P. Druschel, A.-M. Kermarrec, and A. Rowstron.
SCRIBE: A Large-scale and Decentralised Application-level Multicast Infrastructure. IEEE JSAC (Special issue on Network Support for Multicast Communications), 2002. [10] J. Challenger, P. Dantzig, and A. Iyengar. A scalable and highly available system for serving dynamic data at frequently accessed web sites. In In Proceedings of ACM/IEEE, Supercomputing "98 (SC98), Nov. 1998. [11] R. Cox, A. Muthitacharoen, and R. T. Morris. Serving DNS using a Peer-to-Peer Lookup Service. In IPTPS, 2002. [12] M. Dahlin, L. Gao, A. Nayate, A. Venkataramani,
P. Yalagandula, and J. Zheng. PRACTI replication for large-scale systems. Technical Report TR-04-28, The University of Texas at Austin, 2004. [13] C. Estan, G. Varghese, and M. Fisk. Bitmap algorithms for counting active flows on high speed links. In Internet Measurement Conference 2003, 2003. [14] Y. Fu, J. Chase, B. Chun, S. Schwab, and A. Vahdat.
SHARP: An architecture for secure resource peering. In Proc. SOSP, Oct. 2003. [15] Ganglia: Distributed Monitoring and Execution System. http://ganglia.sourceforge.net. [16] S. Gribble, A. Halevy, Z. Ives, M. Rodrig, and D. Suciu.
What Can Peer-to-Peer Do for Databases, and Vice Versa? In Proceedings of the WebDB, 2001. [17] K. Gummadi, R. Gummadi, S. D. Gribble, S. Ratnasamy,
S. Shenker, and I. Stoica. The Impact of DHT Routing Geometry on Resilience and Proximity. In SIGCOMM, 2003. [18] N. J. A. Harvey, M. B. Jones, S. Saroiu, M. Theimer, and A. Wolman. SkipNet: A Scalable Overlay Network with Practical Locality Properties. In USITS, March 2003. [19] R. Huebsch, J. M. Hellerstein, N. Lanham, B. T. Loo,
S. Shenker, and I. Stoica. Querying the Internet with PIER.
In Proceedings of the VLDB Conference, May 2003. [20] C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed diffusion: a scalable and robust communication paradigm for sensor networks. In MobiCom, 2000. [21] S. R. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TAG: a Tiny AGgregation Service for ad-hoc Sensor Networks. In OSDI, 2002. [22] D. Malkhi. Dynamic Lookup Networks. In FuDiCo, 2002. [23] M. L. Massie, B. N. Chun, and D. E. Culler. The ganglia distributed monitoring system: Design, implementation, and experience. In submission. [24] P. Maymounkov and D. Mazieres. Kademlia: A Peer-to-peer Information System Based on the XOR Metric. In Proceesings of the IPTPS, March 2002. [25] C. Olston and J. Widom. Offering a precision-performance tradeoff for aggregation queries over replicated data. In VLDB, pages 144-155, Sept. 2000. [26] K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and A. Demers. Flexible Update Propagation for Weakly Consistent Replication. In Proc. SOSP, Oct. 1997. [27] Planetlab. http://www.planet-lab.org. [28] C. G. Plaxton, R. Rajaraman, and A. W. Richa. Accessing Nearby Copies of Replicated Objects in a Distributed Environment. In ACM SPAA, 1997. [29] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A Scalable Content Addressable Network. In Proceedings of ACM SIGCOMM, 2001. [30] S. Ratnasamy, S. Shenker, and I. Stoica. Routing Algorithms for DHTs: Some Open Questions. In IPTPS, March 2002. [31] T. Roscoe, R. Mortier, P. Jardetzky, and S. Hand. InfoSpect: Using a Logic Language for System Health Monitoring in Distributed Systems. In Proceedings of the SIGOPS European Workshop, 2002. [32] A. Rowstron and P. Druschel. Pastry: Scalable, Distributed Object Location and Routing for Large-scale Peer-to-peer Systems. In Middleware, 2001. [33] S.Ratnasamy, M.Handley, R.Karp, and S.Shenker.
Application-level Multicast using Content-addressable Networks. In Proceedings of the NGC, November 2001. [34] W. Stallings. SNMP, SNMPv2, and CMIP. Addison-Wesley,
[35] I. Stoica, R. Morris, D. Karger, F. Kaashoek, and H. Balakrishnan. Chord: A scalable Peer-To-Peer lookup service for internet applications. In ACM SIGCOMM, 2001. [36] S.Zhuang, B.Zhao, A.Joseph, R.Katz, and J.Kubiatowicz.
Bayeux: An Architecture for Scalable and Fault-tolerant Wide-Area Data Dissemination. In NOSSDAV, 2001. [37] IBM Tivoli Monitoring. www.ibm.com/software/tivoli/products/monitor. [38] R. VanRenesse, K. P. Birman, and W. Vogels. Astrolabe: A Robust and Scalable Technology for Distributed System Monitoring, Management, and Data Mining. TOCS, 2003. [39] R. VanRenesse and A. Bozdog. Willow: DHT, Aggregation, and Publish/Subscribe in One Protocol. In IPTPS, 2004. [40] A. Venkataramani, P. Weidmann, and M. Dahlin. Bandwidth constrained placement in a wan. In PODC, Aug. 2001. [41] A. Venkataramani, P. Yalagandula, R. Kokku, S. Sharif, and M. Dahlin. Potential costs and benefits of long-term prefetching for content-distribution. Elsevier Computer Communications, 25(4):367-375, Mar. 2002. [42] M. Wawrzoniak, L. Peterson, and T. Roscoe. Sophia: An Information Plane for Networked Systems. In HotNets-II,
[43] R. Wolski, N. Spring, and J. Hayes. The network weather service: A distributed resource performance forecasting service for metacomputing. Journal of Future Generation Computing Systems, 15(5-6):757-768, Oct 1999. [44] P. Yalagandula and M. Dahlin. SDIMS: A scalable distributed information management system. Technical Report TR-03-47, Dept. of Computer Sciences, UT Austin,

Manipulating and managing content is and has always been one of the primary functions of a computer. Initial computing applications include text formatters and program compilers. Content was initially managed by explicit user interaction through the use of files and filesystems. As technology has advanced, both the types of content and the way people wish to use it have greatly changed.
New content types such as continuous multimedia streams have become commonplace due to the convergence of advances in storage, encoding, and networking technologies. For example, by combining improvements in storage and encoding, it is now possible to store many hours of TV-quality encoded video on a single disk drive. This has led to the introduction of stand alone digital video recording or personal video recording (PVR) systems such as TiVO [8] and ReplayTV [7]. Another example is the combination of encoding and broadband networking technology. This combination has allowed users to access and share multimedia content in both local and remote area networks with the network itself acting as a huge data repository.
The proliferation of high quality content enabled by these advances in storage, encoding, and networking technology creates the need for new ways to manipulate and manage the data. The focus of our work is on the storage of media rich content and in particular the storage of continuous media content in either pre-packaged or live forms. The need for content management in this area is apparent when one consider the following: • Increases in the capacity and decreases in the cost of storage means that even modest desktop systems today have the ability to store massive amounts of content. Managing such content manually (or more correctly manual non-management of such content) lead to great inefficiencies where unwanted and forgotten content waste storage and where wanted content cannot be found. • While true for all types of content the storage of continuous media content is especially problematic. First continuous media content is still very demanding in terms of storage resources which means that a policy-less approach to storing it will not work for all but the smallest systems.
Second, the storing of live content such as TV or radio is inherently problematic as these signals are continuous streams with no endpoints. This means that before one can even think about managing such content there is a need to abstract it into something that could be manipulated and managed. 4 • When dealing with stored continuous media there is a need to manage such content at both a fine-grained as well as an aggregate level. For example, an individual PVR user wanting to keep only the highlights of a particular sporting event should not be required to have to store the content pertaining to the complete event. At the same time the user might want to think of content in the aggregate, e.g. remove all of the content that I have not watched for the last month except that content which was explicitly marked for archival. • As indicated above, trying to keep track of content on a standalone system without a content management system is very difficult. However, when the actual storage devices are distributed across a network the task of keeping track of content is almost impossible. This scenario is increasingly common in network based content distribution systems and is likely to also become important in home-networking scenarios.
It would seem clear then that a content management system that can efficiently handle media rich content while also exploiting the networked capability of storage devices is needed. This system should allow efficient storage of and access to content across heterogeneous network storage devices according to user preferences.
The content management system should translate user preferences into appropriate low-level storage policies and should allow those preferences to be expressed at a fine level of granularity (while not requiring it in general). The content management system should allow the user to manipulate and reason about (i.e. change the storage policy associated with) the storage of (parts of) continuous media content.
Addressing this distributed content management problem is difficult due to the number of requirements placed on the system. For example: • The content management system must operate on a large number of heterogeneous systems. In some cases the system may be managing content stored on a local filesystem, while in others the content may be stored on a separate network storage appliance. The content manager may be responsible for implementing the policies it uses to reference content or that role may be delegated to a separate computer. A application program interface (API) and associated network protocols are needed in order for the content management system to provide a uniform interface. • The content management system should be flexible and be able to handle differing requirements for content management policies. These policies reflect what content should be obtained, when it should be fetched, how long it should be retained, and under what circumstances it should be discarded.
This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. • The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. • The content management system must handle the interaction between implicit and explicit population of content at the network edge. • The content system must be able to efficiently manage large sets of content, including continuous streams. It needs to be able to package this content in such a way that it is convenient for users to access.
To address these issues we have designed and implemented the Spectrum content management system architecture. Our layered architecture is flexible - its API allows the layers to reside either on a single computer or on multiple networked heterogeneous computers. It allows multiple applications to reference content using differing policies. Note that the Spectrum architecture assumes the existence of a content distribution network (CDN) that can facilitate the efficient distribution of content (for example, the PRISM CDN architecture [2]).
The rest of this paper is organized as follows. Section 2 describes the architecture of our content management system. In Section 3 we describe both our implementation of the Spectrum architecture and examples of its use. Related work is described in Section 4, and Section 5 contains our conclusion and suggestions for future work.
CONTENT MANAGEMENT SYSTEM ARCHITECTURE The Spectrum architecture consists of three distinct management layers that may or may not be distributed across multiple machines, as shown in Figure 1. The three layers are: content manager: contains application specific information that is used to manage all of an application"s content according to user preferences. For example, in a personal video recorder (PVR) application the content manager receives requests for content from a user interface and interacts with the lower layers of the Spectrum architecture to store and manage content on the device. policy manager: implements and enforces various storage polices that the content manager uses to refer to content. The policy manager exports an interface to the content manager that allows the content manager to request that a piece content be treated according to a specific policy. Spectrum allows for arbitrary policies to be realized by providing a fixed set of base-policy templates that can easily be parameterized. It is our belief that for most implementations this will be adequate (if not, Spectrum can easily be extended to dynamically load new base-policy template code at run time). A key aspect of the policy manager is that it allows different policies to be simultaneously applied to the same content (or parts of the same content). Furthermore content can only exist in the system so long as it is referenced by at least one existing policy. Policy conflicts are eliminated by having the policy manager deal exclusively with retention policies rather than with a mix of retention and eviction policies. This means that content with no policy associated with it is immediately and automatically removed from the system. This approach allows us to naturally support sharing of content across different policies which is critical to the efficient storage of large objects.
Note that a key difference between the content manager and the policy manager is that the content manager manages references to multiple pieces of content, i.e. it has an applicationview of content. On the other hand, the policy manager is only concerned with the policy used to manage standalone pieces of content. For example, in a PVR application, the content manager layer would know about the different groups of managed content such as keep-indefinitely, keep for one day, and keep if available diskspace.
However, at the policy manager level, each piece of content has 5 Content Manager Policy Manager Storage Manager Content Manager Content Manager Content Manager Policy Manager Policy Manager Policy Manager Storage Manager Storage Manager Storage Manager Remote Invocation Figure 1: The components of the Spectrum architecture and the four ways they can be configured its own policy (or policies) applied to it and is independent from other content. storage manager: stores content in an efficient manner while facilitating the objectives of the higher layers. Specifically the storage manager stores content in sub-object chunks. This approach has advantages for the efficient retrieval of content but more importantly allows policies to be applied at a subobject level which is critically important when dealing with very large objects such as parts of continuous media, e.g. selected pieces of TV content being stored on a PVR. Note that the storage manager has no knowledge of the policies being used by the content and policy managers.
Another unique part of our approach is that the interfaces between the layers can either be local or distributed. Figure 1 shows the four possible cases. The case on the far left of the Figure shows the simplest (non-distributed) case where all the layers are implemented on a single box. This configuration would be used in selfcontained applications such as PVRs.
The next case over corresponds to the case where there is a centralized content manager that controls distributed storage devices each of which is responsible for implementing policy based storage. In this case although the remote devices are controlled by the central manager they operate much more independently. For example, once they receive instructions from the central manager they typically operate in autonomous fashion. An example of this type of configuration is a content distribution network (CDN) that distributes and stores content based on a schedule determined by some centralized controller. For example, the CDN could pre-populate edge devices with content that is expected to be very popular or distribute large files to branch offices during off-peak hours in a bandwidth constrained enterprise environment.
Allowing a single policy manager to control several storage managers leads to the next combination of functions and the most distributed case. The need for this sort of separation might occur for scalability reasons or when different specialized storage devices or appliances are required to be controlled by a single policy manager.
The final case shows a content manager combined with a policy manager controlling a remote storage manager. This separation would be possible if the storage manager is somewhat autonomous and does not require continuous fine grained control by the policy manager.
We now examine the function of the three layers in detail.
The content manager layer is the primary interface through which specific applications use the Spectrum architecture. As such the content manager layer provides an API for the application to manipulate all aspects of the Spectrum architecture at different levels of granularity. The content manager API has functions that handle: Physical devices: This set of functions allows physical storage devices to be added to Spectrum thereby putting them under control of the content manager and making the storage available to the system. Physical devices can be local or remote - this is the only place in the architecture where the application is required to be aware of this distinction. Once a device is mapped into the application through this interface, the system tracks its type and location. Users simply refer to the content through an application-provided label.
Stores: Stores are subsets of physical storage devices. Through these functions an application can create a store on a physical device and assign resources (e.g. disk space) to it. Stores can only be created in physical devices that are mapped into the system.
Policy Groups: Policy groups are the means whereby an application specifies, instantiates, and modifies the policies that are applied to Spectrum content. Typical usage of this set of functions is to select one of a small set of base policies and to parameterize this specific instance of the policy. Policy groups are created within existing stores in the system. The Spectrum architecture has policies that are normally associated with storage that aim to optimize disk usage. In addition a set of policies that take a sophisticated time specification enable storage that is cognizant of time. For example, a simple time-based policy could evict content from the system at a certain absolute or relative time. A slightly more involved time-based policy enabled by the Spectrum architecture could allow content to be stored in rolling window of a number of hours (for example, the most recent N-number of hours is kept in the system). Time-based polices are of particular use when dealing with continuous content like a live broadcast. 6 Content: At the finest level of granularity content can be added to or removed from the system. Content is specified to the system by means of a uniform resource locator (URL) which concisely indicates the location of the content as well as the protocol to be used to retrieve it. Optionally a time specification can be associated with content. This allows content to be fetched into the system at some future time, or at future time intervals. Again, this is particularly useful for dealing with the storage and management of live content.
The policy manager layer of the Spectrum architecture has two main types of API functions. First, there are functions that operate on managed storage areas and policy-based references (prefs) to content stored there. Second, there are sets of functions used to implement each management policy. The first class of functions is used by the content manager layer to access storage. Operations include: create, open, and close: These operations are used by the content manager to control its access to storage. The policy manager"s create operation is used to establish contact with a store for the first time. Once this is done, the store can be open and closed using the appropriate routines. Note that the parameters used to create a store contain information on how to reach it. For example, local stores have a path associated with them, while remote stores have a remote host and remote path associated with them. The information only needs to be passed to the policy manager once at create time. For open operations, the policy manager will use cached information to contact the store. lookup: The lookup operation provides a way for the content manager to query the policy manager about what content is currently present for a given URL. For continuous media time ranges of present media will be returned. resource: The resource routines are used to query the policy manager about its current resource usage. There are two resource routines: one that applies to the store as a whole and another that applies to a particular policy reference. The resource API is extensible, we currently support queries on disk usage and I/O load. pref establish/update: The pref establish operation is used by the content manager to reference content on the store. If the content is not present, this call will result in the content being fetched (or being scheduled to be fetched if the content is not currently available). Parameters of this function include the URL to store it under, the URL to fetch data from if it is not present, the policy to store the content under, and the arguments used to parameterize the policy. The result of a successful pref establish operation is a policy reference ID string. This ID can be used with the update operation to either change the storage policy parameters or delete the reference entirely.
The second group of policy manager functions are used to implement all the polices supported by Spectrum. We envision a small set of base-level policy functions that can be parameterized to produce a wide range of storage polices. For example, a policy that implements recording a repeating time window can be parameterized to function daily, weekly, or monthly. Note that the policy manager is only concerned with executing a specific policy. The higher-level reasons for choosing a given policy are handled by the content and application manager.
A base policy is implemented using six functions: establish: called when a pref is established with the required URLs and base policy"s parameters. The establish routine references any content already present in the store and then determines the next time it needs to take action (e.g. start a download) and schedules a callback for that time. It can also register to receive callbacks if new content is received for a given URL. update: called to change the parameters of a pref, or to discard the policy reference. newclip: called when a chunk of new content is received for a URL of interest. The base policy typically arranges for newclip to be called for a given URL when the pref is established.
When newclip is called, the base policy checks its parameters to determine if it wishes to add a reference to the clip just received. callback: called when the pref schedules a timer-based callback.
This is a useful wakeup mechanism for prefs that need to be idle for a long period of time (e.g. between programs). boot/shutdown: called when the content management system is booting or shutting down. The boot operation is typically used to schedule initial callbacks or start I/O operations. The shutdown operation is used to gracefully shutdown I/O streams and save state.
The role of Spectrum"s storage manager is to control all I/O operations associated with a given store. Spectrum"s storage manager supports storing content both on a local filesystem and on a remote fileserver (e.g. a storage appliance). For continuous media, at the storage manager level content is stored as a collection of time-based chunks. Depending on the underlying filesystem, a chunk could correspond to a single file or a data node in a storage database.
The two main storage manager operations are input and output.
The input routine is used to store content in a store under a given name. The output routine is used to send data from the store to a client. For streaming media both the input and output routines take time ranges that schedule when the I/O operation should happen, and both routines return an I/O handle that can be used to modify or cancel the I/O request in the future.
Much like the policy manager, the storage manager also provides API functions to create, open, and close stores. It also supports operations to query the resource usages and options supported by the store. Finally, the storage manager also has a discard routine that may be used by the policy manager to inform the store to remove content from the store.
In this section we describe our implementation of Spectrum and describe how it can be used.
We have implemented Spectrum"s three layers in C as part of a library that can be linked with Spectrum-based applications. Each layer keeps track of its state through a set of local data files that persist across reboots, thus allowing Spectrum to smoothly handle power cycles. For layers that reside on remote systems (e.g. a remote store) only the meta-information needed to contact the remote 7 Content Manager Policy Manager Storage Manager Storage Fetcher Program Listings Graphical User Interface Network Enabled DVR Program Information Content DVR Application Figure 2: Spectrum in a Network Enabled DVR node is stored locally. Our test application uses a local policy and storage manager to fetch content and store it in a normal Unixbased filesystem.
To efficiently handle communications with layers running on remote systems, all Spectrum"s API calls support both synchronous and asynchronous modes through a uniform interface defined by the reqinfo structure. Each API call takes a pointer to a reqinfo structure as one of its arguments. This structure is used to hold the call state and return status. For async calls, the reqinfo also contains a pointer to a callback function. To use a Spectrum API function, the caller first chooses either the sync or async mode and allocates a reqinfo structure. For sync calls, the reqinfo can be allocated on the stack, otherwise it is allocated with malloc. For async calls, a callback function must be provided when the reqinfo is allocated. Next the caller invokes the desired Spectrum API function passing the reqinfo structure as an argument. For sync calls, the result of the calls is returned immediately in the reqinfo structure. For successful async calls, a call in progress value is returned. Later, when the async call completes or a timeout occurs, the async callback function is called with the appropriate information needed to complete processing.
The modular/layered design of the Spectrum architecture simplifies the objective of distribution of functionality. Furthermore, communication between functions is typically of a master-slave(s) nature. This means that several approaches to distributed operation are possible that would satisfy the architectural requirements. In our implementation we have opted to realize this functionality with a simple modular design. We provide a set of asynchronous remote access stub routines that allow users to select the transport protocol to use and to select the encoding method that should be used with the data to be transferred. Transport protocols can range simple protocols such as UDP up to more complex protocols such as HTTP. We currently are using plain TCP for most of our transport.
Function calls across the different Spectrum APIs can be encoded using a variety of formats include plain text, XDR, and XML.
We are currently using the eXpat XML library [4] to encode our calls. While we are current transferring our XML encoded messages using a simple TCP connection, in a real world setting this can easily be replaced with an implementation based on secure sockets layer (SSL) to improve security by adding SSL as a transport protocol.
An important aspect of Spectrum is that it can manage content based on a given policy across heterogenous platforms. As we explained previously in Section 2.2, envision a small set of base-level policy functions that can be parameterized to produce a wide range of storage polices. In order for this to work properly, all Spectrumbased applications must understand the base-level policies and how they can be parameterized. To address this issue, we treat each base-level policy as if it was a separate program. Each base-level policy should have a well known name and command line options for parameterization. In fact, in our implementation we pass parameters to base-level policies as a string that can be parsed using a getopt-like function. This format is easily understood and provides portability since byte order is not an issue in a string. Since this part of Spectrum is not on the critical data path, this type of formatting is not a performance issue.
System In this section we show two examples of the use of the Spectrum Content Management System in our environment. The focus of our previous work has been content distribution for streaming media content [2] and network enabled digital video recording [3]. The Spectrum system is applicable to both scenarios as follows.
Figure 2 shows the Network Enabled DVR (NED) architecture.
In this case all layers of the Spectrum architecture reside on the same physical device in a local configuration. The DVR application obtains program listings from some network source, deals with user presentation through a graphical user interface (GUI), and interface with the Spectrum system through the content management layer APIs. This combination of higher level functions allows the user to select both content to be stored and what storage policies to 8 Content Manager Centralized Content Management station Content InformationUser Interface Policy Manager Storage Manager Storage Fetcher Edge Portal Server Policy Manager Storage Manager Storage Fetcher Edge Portal Server Distributed Content To Media Endpoints To Media Endpoints Figure 3: Spectrum in a Content Distribution Architecture apply to such content. Obtaining the content (through the network or locally) and the subsequent storage on the local system is then handled by the policy and storage managers.
The use of Spectrum in a streaming content distribution architecture (e.g. PRISM [2]) is depicted in Figure 3. In this environment streaming media content (both live, canned-live and on-demand) is being distributed to edge portals from where streaming endpoints are being served. In our environment content distribution and storage is done from a centralized content management station which controls several of the edge portals. The centralized station allows administrators to manage the distribution and storage of content without requiring continuous communication between the content manager and the edge devices, i.e. once instructions have been given to edge devices they can operate independently until changes are to be made.
To illustrate how Spectrum handles references to content, consider a Spectrum-based PVR application programmed to store one days worth of streaming content in a rolling window. To set up the rolling window, the application would use the content manager API to create a policy group and policy reference to the desired content.
The establishment of the one-day rolling window policy reference would cause the policy manger to ask the storage manager to start receiving the stream. As each chunk of streaming data arrives, the policy manager executes the policy reference"s newclip function.
The newclip function adds a reference to each arriving chunk, and schedules a callback a day later. At that time, the policy will drop its now day-old reference to the content and the content will be discarded unless it is referenced by some other policy.
Now, consider the case where the user decides to save part of the content (e.g. a specific program) in the rolling window for an extra week. To do this, the application requests that the content manager add an additional new policy reference to the part of the content to preserved. Thus, the preserved content has two references to it: one from the rolling window and one from the request to preserve the content for an additional week. After one day the reference from the rolling window will be discarded, but the content will be 9 ref2, etc. base data url1 url2 (media files...) (media files...) meta store (general info...) url1 chunks prefs ranges media chunks, etc.url2 poly host ref1 ref1.files ref1.state Figure 4: Data layout of Spectrum policy store preserved by the second reference. After the additional week has past, the callback function for the second reference will be called.
This function will discard the remaining reference to the content and as there are no remaining references the content will be freed.
In order to function in scenarios like the ones described above,
Spectrum"s policy manager must manage and maintain all the references to various chunks of media. These references are persistent and thus must be able to survive even if the machine maintaining them is rebooted. Our Spectrum policy manager implementation accomplishes this using the file and directory structure shown in Figure 4. There are three classes of data stored, and each class has its own top level directory. The directories are: data: this directory is used by the storage manager to store each active URL"s chunks of media. The media files can be encoded in any format, for example MPEG, Windows Media, or QuickTime. Note that this directory is used only if the storage manager is local. If the policy manager is using an external storage manager (e.g. a storage appliance), then the media files are stored remotely and are only remotely referenced by the policy manager. meta: this directory contains general meta information about the storage manager being used and the data it is storing.
General information is stored in the store subdirectory and includes the location of the store (local or remote) and information about the types of chunks of data the store can handle.
The meta directory also contains a subdirectory per-URL that contains information about the chunks of data stored.
The chunks file contains a list of chunks currently stored and their reference counts. The prefs file contains a list of active policy references that point to this URL. The ranges file contains a list of time ranges of data currently stored.
Finally, the media file describes the format of the media being stored under the current URL. poly: this directory contains a set of host subdirectories. Each host subdirectory contains the set of policy references created by that host. Information on each policy reference is broken up into three files. For example, a policy reference named ref1 would be stored in ref1, ref1.files, and ref1.state. The ref1 file contains information about the policy reference that does not change frequently. This information includes the base-policy and the parameters used to create the reference. The ref1.files file contains the list of references to chunks that pref ref1 owns. Finally, the ref1.state file contains optional policy-specific state information that can change over time.
Together, these files and directories are used to track references in our implementation of Spectrum. Note that other implementations are possible. For example, a carrier-grade Spectrum manager might store all its policy and reference information in a high-performance database system. 10
Several authors have addressed the problem of the management of content in distributed networks. Much of the work focuses on the policy management aspect. For example in [5], the problem of serving multimedia content via distributed servers is considered. Content is distributed among server resources in proportion to user demand using a Demand Dissemination Protocol. The performance of the scheme is benchmarked via simulation. In [1] content is distributed among sub-caches. The authors construct a system employing various components, such as a Central Router,
Cache Knowledge base, Subcaches, and a Subcache eviction judge.
The Cache Knowledge base allows sophisticated policies to be employed. Simulation is used to compare the proposed scheme with well-known replacement algorithms. Our work differs in that we are considering more than the policy management aspects of the problem. After carefully considering the required functionality to implement content management in the networked environment, we have partitioned the system into three simple functions, namely Content manager, Policy manager and Storage manager. This has allowed us to easily implement and experiment with a prototype system.
Other related work involves so called TV recommendation systems which are used in PVRs to automatically select content for users, e.g. [6]. In the case where Spectrum is used in a PVR configuration this type of system would perform a higher level function and could clearly benefit from the functionalities of the Spectrum architecture.
Finally, in the commercial CDN environment vendors (e.g. Cisco and Netapp) have developed and implemented content management products and tools. Unlike the Spectrum architecture which allows edge devices to operate in a largely autonomous fashion, the vendor solutions typically are more tightly coupled to a centralized controller and do not have the sophisticated time-based operations offered by Spectrum.
In this paper we presented the design and implementation of the Spectrum content management architecture. Spectrum allows storage policies to be applied to large volumes of content to facilitate efficient storage. Specifically, the system allows different policies to be applied to the same content without replication. Spectrum can also apply policies that are time-aware which effectively deals with the storage of continuous media content. Finally, the modular design of the Spectrum architecture allows both stand-alone and distributed realizations so that the system can be deployed in a variety of applications.
There are a number of open issues that will require future work.
Some of these issues include: • We envision Spectrum being able to manage content on systems ranging from large CDNs down to smaller appliances such as TiVO [8]. In order for these smaller systems to support Spectrum they will require networking and an external API. When that API becomes available, we will have to work out how it can be fit into the Spectrum architecture. • Spectrum names content by URL, but we have intentionally not defined the format of Spectrum URLs, how they map back to the content"s actual name, or how the names and URLs should be presented to the user. While we previously touched on these issues elsewhere [2], we believe there is more work to be done and that consensus-based standards on naming need to be written. • In this paper we"ve focused on content management for continuous media objects. We also believe the Spectrum architecture can be applied to any type of document including plain files, but we have yet to work out the details necessary to support this in our prototype environment. • Any project that helps allow multimedia content to be easily shared over the Internet will have legal hurdles to overcome before it can achieve widespread acceptance. Adapting Spectrum to meet legal requirements will likely require more technical work.
[1] K. . Cheng and Y. Kambayashi. Multicache-based Content Management for Web Caching. Proceedings of the First International Conference on Web Information Systems Engineering, Jume 2000. [2] C. Cranor, M. Green, C. Kalmanek, D. Shur, S. Sibal,
C. Sreenan, and J. van der Merwe. PRISM Architecture: Supporting Enhanced Streaming Services in a Content Distribution Network. IEEE Internet Computing, July/August
[3] C. Cranor, C. Kalmanek, D. Shur, S. Sibal, C. Sreenan, and J. van der Merwe. NED: a Network-Enabled Digital Video Recorder. 11th IEEE Workshop on Local and Metropolitan Area Networks, March 2001. [4] eXpat. expat.sourceforge.net. [5] Z. Ge, P. Ji, and P. Shenoy. A Demand Adaptive and Locality Aware (DALA) Streaming Media Server Cluster Architecture.
NOSSDAV, May 2002. [6] K. Kurapati and S. Gutta and D. Schaffer and J. Martino and J.

In Massively Multi-player On-line Games (MMOG), game clients who are positioned across the Internet connect to a game server to interact with other clients in order to be part of the game. In current architectures, these interactions are direct in that the game clients and the servers exchange game messages with each other. In addition, current MMOGs delegate all authority to the game server to make decisions about the results pertaining to the actions that game clients take and also to decide upon the result of other game related events. Such centralized authority has been implemented with the claim that this improves the security and consistency required in a gaming environment.
A number of works have shown the effect of network latency on distributed multi-player games [1, 2, 3, 4]. It has been shown that network latency has real impact on practical game playing experience [3, 5]. Some types of games can function quite well even in the presence of large delays. For example, [4] shows that in a modern RPG called Everquest 2, the breakpoint of the game when adding artificial latency was 1250ms. This is accounted to the fact that the combat system used in Everquest 2 is queueing based and has very low interaction. For example, a player queues up
seconds to actually perform, giving the server plenty of time to validate these actions. But there are other games such as FPS games that break even in the presence of moderate network latencies [3, 5]. Latency compensation techniques have been proposed to alleviate the effect of latency [1, 6, 7] but it is obvious that if MMOGs are to increase in interactivity and speed, more architectures will have to be developed that address responsiveness, accuracy and consistency of the gamestate.
In this paper, we propose two important features that would make game playing within MMOGs more responsive for movement and scalable. First, we propose that centralized server-based architectures be made hierarchical through the introduction of communication proxies so that game updates made by clients that are time sensitive, such as movement, can be more efficiently distributed to other players within their game-space. Second, we propose that assignment of authority in terms of who makes the decision on client actions such as object pickups and hits, and collisions between players, be distributed between the clients and the servers in order to distribute the computing load away from the central server. In order to move towards more complex real-time networked games, we believe that definitions of authority must be refined.
Most currently implemented MMOGs have game servers that have almost absolute authority. We argue that there is no single consistent view of the virtual game space that can be maintained on any one component within a network that has significant latency, such as the one that many MMOG players would experience. We believe that in most cases, the client with the most accurate view of an entity is the best suited to make decisions for that entity when the causality of that action will not immediately affect any other players. In this paper we define what it means to have authority within the context of events and objects in a virtual game space. We then show the benefits of delegating authority for different actions and game events between the clients and server.
In our model, the game space consists of game clients (representing the players) and objects that they control. We divide the client actions and game events (we will collectively refer to these as events) such as collisions, hits etc. into three different categories, a) events for which the game client has absolute authority, b) events for which the game server has absolute authority, and c) events for which the authority changes dynamically from client to the server and vice-versa. Depending on who has the authority, that entity will make decisions on the events that happen within a game space. We propose that authority for all decisions that pertain to a single player or object in the game that neither affects the other players or objects, nor are affected by the actions of other players be delegated to that player"s game client. These type of decisions would include collision detection with static objects within the virtual game space and hit detection with linear path bullets (whose trajectory is fixed and does not change with time) fired by other players.
Authority for decisions that could be affected by two or more players should be delegated to the impartial central server, in some cases, to ensure that no conflicts occur and in other cases can be delegated to the clients responsible for those players. For example, collision detection of two players that collide with each other and hit detection of non-linear bullets (that changes trajectory with time) should be delegated to the server. Decision on events such as item pickup (for example, picking up items in a game to accumulate points) should be delegated to a server if there are multiple players within close proximity of an item and any one of the players could succeed in picking the item; for item pick-up contention where the client realizes that no other player, except its own player, is within a certain range of the item, the client could be delegated the responsibility to claim the item. The client"s decision can always be accurately verified by the server.
In summary, we argue that while current authority models that only delegate responsibility to the server to make authoritative decisions on events is more secure than allowing the clients to make the decisions, these types of models add undesirable delays to events that could very well be decided by the clients without any inconsistency being introduced into the game. As networked games become more complex, our architecture will become more applicable. This architecture is applicable for massively multiplayer games where the speed and accuracy of game-play are a major concern while consistency between player game-states is still desired.
We propose that a mixed authority assignment mechanism such as the one outlined above be implemented in high interaction MMOGs.
Our paper has the following contributions. First we propose an architecture that uses communication proxies to enable clients to connect to the game server. A communication proxy in the proposed architecture maintains information only about portions of the game space that are relevant to clients connected to it and is able to process the movement information of objects and players within these portions.
In addition, it is capable of multicasting this information only to a relevant subset of other communication proxies.
These functionalities of a communication proxy leads to a decrease in latency of event update and subsequently, better game playing experience. Second, we propose a mixed authority assignment mechanism as described above that improves game playing experience. Third, we implement the proposed mixed authority assignment mechanism within a MMOG called RPGQuest [8] to validate its viability within MMOGs.
In Section 2, we describe the proxy-based game architecture in more detail and illustrate its advantages. In Section 3, we provide a generic description of the mixed authority assignment mechanism and discuss how it improves game playing experience. In Section 4, we show the feasibility of implementing the proposed mixed authority assignment mechanism within existing MMOGs by describing a proof-of-concept implementation within an existing MMOG called RPGQuest. Section 5 discusses related work. In Section 6, we present our conclusions and discuss future work.
Massively Multi-player Online Games (MMOGs) usually consist of a large game space in which the players and different game objects reside and move around and interact with each-other. State information about the whole game space could be kept in a single central server which we would refer to as a Central-Server Architecture. But to alleviate the heavy demand on the processing for handling the large player population and the objects in the game in real-time, a MMOG is normally implemented using a distributed server architecture where the game space is further sub-divided into regions so that each region has relatively smaller number of players and objects that can be handled by a single server. In other words, the different game regions are hosted by different servers in a distributed fashion. When a player moves out of one game region to another adjacent one, the player must communicate with a different server (than it was currently communicating with) hosting the new region. The servers communicate with one another to hand off a player or an object from one region to another. In this model, the player on the client machine has to establish multiple gaming sessions with different servers so that it can roam in the entire game space.
We propose a communication proxy based architecture where a player connects to a (geographically) nearby proxy instead of connecting to a central server in the case of a centralserver architecture or to one of the servers in case of dis2 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 tributed server architecture. In the proposed architecture, players who are close by geographically join a particular proxy. The proxy then connects to one or more game servers, as needed by the set of players that connect to it and maintains persistent transport sessions with these server. This alleviates the problem of each player having to connect directly to multiple game servers, which can add extra connection setup delay. Introduction of communication proxies also mitigates the overhead of a large number of transport sessions that must be managed and reduces required network bandwidth [9] and processing at the game servers both with central server and distributed server architectures. With central server architectures, communication proxies reduce the overhead at the server by not requiring the server to terminate persistent transport sessions from every one of the clients. With distributed-server architectures, additionally, communication proxies eliminate the need for the clients to maintain persistent transport sessions to every one of the servers. Figure 1 shows the proposed architecture.
Figure 1: Architecture of the gaming environment.
Note that the communication proxies need not be cognizant of the game. They host a number of players and inform the servers which players are hosted by the proxy in question.
Also note that the players hosted by a proxy may not be in the same game space. That is, a proxy hosts players that are geographically close to it, but the players themselves can reside in different parts of the game space. The proxy communicates with the servers responsible for maintaining the game spaces subscribed by the different players. The proxies communicate with one another in a peer-to-peer to fashion. The responsiveness of the game can be improved for updates that do not need to wait on processing at a central authority. In this way, information about players can be disseminated faster before even the game server gets to know about it. This definitely improves the responsiveness of the game. However, it ignores consistency that is critical in MMORPGs. The notion that an architecture such as this one can still maintain temporal consistency will be discussed in detail in Section 3.
Figure 2 shows and example of the working principle of the proposed architecture. Assume that the game space is divided into 9 regions and there are three servers responsible for managing the regions. Server S1 owns regions 1 and 2,
S2 manages 4, 5, 7, and 8, and S3 is responsible for 3, 6 and
Figure 2: An example.
There are four communication proxies placed in geographically distant locations. Players a, b, c join proxy P1, proxy P2 hosts players d, e, f, players g, h are with proxy P3, whereas players i, j, k, l are with proxy P4. Underneath each player, the figure shows which game region the player is located currently. For example, players a, b, c are in regions 1, 2, 6, respectively. Therefore, proxy P1 must communicate with servers S1 and S3. The reader can verify the rest of the links between the proxies and the servers.
Players can move within the region and between regions.
Player movement within a region will be tracked by the proxy hosting the player and this movement information (for example, the player"s new coordinates) will be multicast to a subset of other relevant communication proxies directly. At the same time, this information will be sent to the server responsible for that region with the indication that this movement has already been communicated to all the other relevant communication proxies (so that the server does not have to relay this information to all the proxies).
For example, if player a moves within region 1, this information will be communicated by proxy P1 to server S1 and multicast to proxies P3 and P4. Note that proxies that do not keep state information about this region at this point in time (because they do not have any clients within that region) such as P2 do not have to receive this movement information.
If a player is at the boundary of a region and moves into a new region, there are two possibilities. The first possibility is that the proxy hosting the player can identify the region into which the player is moving (based on the trajectory information) because it is also maintaining state information about the new region at that point in time. In this case, the proxy can update movement information directly at the other relevant communication proxies and also send information to the appropriate server informing of the movement (this may require handoff between servers as we will describe). Consider the scenario where player a is at the boundary of region 1 and proxy P1 can identify that the player is moving into region 2. Because proxy P1 is currently keeping state information about region 2, it can inform all The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 3 the other relevant communication proxies (in this example, no other proxy maintains information about region 2 at this point and so no update needs to be sent to any of the other proxies) about this movement and then inform the server independently. In this particular case, server S1 is responsible for region 2 as well and so no handoff between servers would be needed. Now consider another scenario where player j moves from region 9 to region 8 and that proxy P4 is able to identify this movement. Again, because proxy P4 maintains state information about region 8, it can inform any other relevant communication proxies (again, none in this example) about this movement. But now, regions 9 and 8 are managed by different servers (servers S3 and S2 respectively) and thus a hand-off between these servers is needed.
We propose that in this particular scenario, the handoff be managed by the proxy P4 itself. When the proxy sends movement update to server S3 (informing the server that the player is moving out of its region), it would also send a message to server S2 informing the server of the presence and location of the player in one of its region.
In the intra-region and inter-region scenarios described above, the proxy is able to manage movement related information, update only the relevant communication proxies about the movement, update the servers with the movement and enable handoff of a player between the servers if needed. In this way, the proxy performs movement updates without involving the servers in any way in this time-critical function thereby speeding up the game and improving game playing experience for the players. We consider this the fast path for movement update. We envision the proxies to be just communication proxies in that they do not know about the workings of specific games. They merely process movement information of players and objects and communicate this information to the other proxies and the servers. If the proxies are made more intelligent in that they understand more of the game logic, it is possible for them to quickly check on claims made by the clients and mitigate cheating.
The servers could perform the same functionality but with more delay. Even without being aware of game logic, the proxies can provide additional functionalities such as timestamping messages to make the game playing experience more accurate [10] and fair [11].
The second possibility that should be considered is when players move between regions. It is possible that a player moves from one region to another but the proxy that is hosting the player is not able to determine the region into which the player is moving, a) the proxy does not maintain state information about all the regions into which the player could potentially move, or b) the proxy is not able to determine which region the player may move into (even if maintains state information about all these regions). In this case, we propose that the proxy be not responsible for making the movement decision, but instead communicate the movement indication to the server responsible for the region within which the player is currently located. The server will then make the movement decision and then a) inform all the proxies including the proxy hosting the player, and b) initiate handoff with another server if the player moves into a region managed by another server. We consider this the slow path for movement update in that the servers need to be involved in determining the new position of the player.
In the example, assume that player a moves from region 1 to region 4. Proxy P1 does not maintain state information about region 4 and thus would pass the movement information to server S1. The server will identify that the player has moved into region 4 and would inform proxy P1 as well as proxy P2 (which is the only other proxy that maintains information about region 4 at this point in time). Server S1 will also initiate a handoff of player a with server S2. Proxy P1 will now start maintaining state information about region 4 because one of its hosted players, player a has moved into this region. It will do so by requesting and receiving the current state information about region 4 from server S2 which is responsible for this region.
Thus, a proxy architecture allows us to make use of faster movement updates through the fast path through a proxy if and when possible as opposed to conventional server-based architectures that always have to use the slow path through the server for movement updates. By selectively maintaining relevant regional game state information at the proxies, we are able to achieve this capability in our architecture without the need for maintaining the complete game state at every proxy.
As a MMOG is played, the players and the game objects that are part of the game, continually change their state. For example, consider a player who owns a tank in a battlefield game. Based on action of the player, the tank changes its position in the game space, the amount of ammunition the tank contains changes as it fires at other tanks, the tank collects bonus firing power based on successful hits, etc.
Similarly objects in the battlefield, such as flags, buildings etc. change their state when a flag is picked up by a player (i.e. tank) or a building is destroyed by firing at it. That is, some decision has to be made on the state of each player and object as the game progresses. Note that the state of a player and/or object can contain several parameters (e.g., position, amount of ammunition, fuel storage, points collected, etc), and if any of the parameters changes, the state of the player/object changes.
In a client-server based game, the server controls all the players and the objects. When a player at a client machine makes a move, the move is transmitted to the server over the network. The server then analyzes the move, and if the move is a valid one, changes the state of the player at the server and informs the client of the change. The client subsequently updates the state of the player and renders the player at the new location. In this case the authority to change the state of the player resides with the server entirely and the client simply follows what the server instructs it to do.
Most of the current first person shooter (FPS) games and role playing games (RPG) fall under this category. In current FPS games, much like in RPG games, the client is not trusted. All moves and actions that it makes are validated.
If a client detects that it has hit another player with a bullet, it proceeds assuming that it is a hit. Meanwhile, an update is sent to the server and the server will send back a message either affirming or denying that the player was hit. If the remote player was not hit, then the client will know that it
did not actually make the shot. If it did make the hit, an update will also be sent from the server to the other clients informing them that the other player was hit. A difference that occurs in some RPGs is that they use very dumb client programs. Some RPGs do not maintain state information at the client and therefore, cannot predict anything such as hits at the client. State information is not maintained because the client is not trusted with it. In RPGs, a cheating player with a hacked game client can use state information stored at the client to gain an advantage and find things such as hidden treasure or monsters lurking around the corner. This is a reason why most MMORPGs do not send a lot of state information to the client and causes the game to be less responsive and have lower interaction game-play than FPS games.
In a peer-to-peer game, each peer controls the player and object that it owns. When a player makes a move, the peer machine analyzes the move and if it is a valid one, changes the state of the player and places the player in new position. Afterwards, the owner peer informs all other peers about the new state of the player and the rest of the peers update the state of the player. In this scenario, the authority to change the state of the player is given to the owning peer and all other peers simply follow the owner.
For example, Battle Zone Flag (BzFlag) [12] is a multiplayer client-server game where the client has all authority for making decisions. It was built primarily with LAN play in mind and cheating as an afterthought. Clients in BzFlag are completely authoritative and when they detect that they were hit by a bullet, they send an update to the server which simply forwards the message along to all other players. The server does no sort of validation.
Each of the above two traditional approaches has its own set of advantages and disadvantages. The first approach, which we will refer to as server authoritative henceforth, uses a centralized method to assign authority. While a centralized approach can keep the state of the game (i.e., state of all the players and objects) consistent across any number of client machines, it suffers from delayed response in game-play as any move that a player at the client machine makes must go through one round-trip delay to the server before it can take effect on the client"s screen. In addition to the round-trip delay, there is also queuing delay in processing the state change request at the server. This can result in additional processing delay, and can also bring in severe scalability problems if there are large number of clients playing the game. One definite advantage of the server authoritative approach is that it can easily detect if a client is cheating and can take appropriate action to prevent cheating.
The peer-to-peer approach, henceforth referred to as client authoritative, can make games very responsive. However, it can make the game state inconsistent for a few players and tie break (or roll back) has to be performed to bring the game back to a consistent state. Neither tie break nor roll back is a desirable feature of online gaming. For example, assume that for a game, the goal of each player is to collect as many flags as possible from the game space (e.g. BzFlag).
When two players in proximity try to collect the same flag at the same time, depending on the algorithm used at the client-side, both clients may determine that it is the winner, although in reality only one player can pick the flag up. Both players will see on their screen that it is the winner. This makes the state of the game inconsistent. Ways to recover from this inconsistency are to give the flag to only one player (using some tie break rule) or roll the game back so that the players can try again. Neither of these two approaches is a pleasing experience for online gaming. Another problem with client authoritative approach is that of cheating by clients as there is no cross checking of the validation of the state changes authorized by the owner client.
We propose to use a hybrid approach to assign the authority dynamically between the client and the server. That is, we assign the authority to the client to make the game responsive, and use the server"s authority only when the client"s individual authoritative decisions can make the game state inconsistent. By moving the authority of time critical updates to the client, we avoid the added delay caused by requiring the server to validate these updates. For example, in the flag pickup game, the clients will be given the authority to pickup flags only when other players are not within a range that they could imminently pickup a flag. Only when two or more players are close by so that more than one player may claim to have picked up a flag, the authority for movement and flag pickup would go to the central server so that the game state does not become inconsistent. We believe that in a large game-space where a player is often in a very wide open and sparsely populated area such as those often seen in the game Second Life [13], this hybrid architecture would be very beneficial because of the long periods that the client would have authority to send movement updates for itself. This has two advantages over the centralauthority approach, it distributes the processing load down to the clients for the majority of events and it allows for a more responsive game that does not need to wait on a server for validation.
We believe that our notion of authority can be used to develop a globally consistent state model of the evolution of a game. Fundamentally, the consistent state of the system is the one that is defined by the server. However, if local authority is delegated to the client, in this case, the client"s state is superimposed on the server"s state to determine the correct global state. For example, if the client is authoritative with respect to movement of a player, then the trajectory of the player is the true trajectory and must replace the server"s view of the player"s trajectory. Note that this could be problematic and lead to temporal inconsistency only if, for example, two or more entities are moving in the same region and can interact with each other. In this situation, the client authority must revert to the server and the sever would then make decisions. Thus, the client is only authoritative in situations where there is no potential to imminently interact with other players. We believe that in complex MMOGs, when allowing more rapid movement, it will still be the case that local authority is possible for significant spans of game time. Note that it might also be possible to minimize the occurrences of the Dead Man Shooting problem described in [14]. This could be done by allowing the client to be authoritative for more actions such as its player"s own death and disallowing other players from making preemptive decisions based on a remote player.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 5 One reason why the client-server based architecture has gained popularity is due to belief that the fastest route to the other clients is through the server. While this may be true, we aim to create a new architecture where decisions do not always have to be made at the game server and the fastest route to a client is actually through a communication proxy located close to the client. That is, the shortest distance in our architecture is not through the game server but through the communication proxy. After a client makes an action such as movement, it will simultaneously distribute it directly to the clients and the game server by way of the communications proxy. We note that our architecture however is not practical for a game where game players setup their own servers in an ad-hoc fashion and do not have access to proxies at the various ISPs. This proxy and distributed authority architecture can be used to its full potential only when the proxies can be placed at strategic places within the main ISPs and evenly distributed geographically.
Our game architecture does not assume that the client is not to be trusted. We are designing our architecture on the fact that there will be sufficient cheat deterring and detection mechanisms present so that it will be both undesirable and very difficult to cheat [15]. In our proposed approach, we can make the games cheat resilient by using the proxybased architecture when client authoritative decisions take place. In order to achieve this, the proxies have to be game cognizant so that decisions made by a client can be cross checked by a proxy that the client connects to. For example, assume that in a game a plane controlled by a client moves in the game space. It is not possible for the plane to go through a building unharmed. In a client authoritative mode, it is possible for the client to cheat by maneuvering the plane through a building and claiming the plane to be unharmed. However, when such move is published by the client, the proxy, being aware of the game space that the plane is in, can quickly check that the client has misused the authority and then can block such move. This allows us to distribute authority to make decisions about the clients.
In the following section we use a multiplayer game called RPGQuest to implement different authoritative schemes and discuss our experience with the implementation. Our implementation shows the viability of our proposed solution.
We have experimented with the authority assignment mechanism described in the last section by implementing the mechanisms in a game called RPGQuest. A screen shot from this game is shown in Figure 3. The purpose of the implementation is to test its feasibility in a real game. RPGQuest is a basic first person game where the player can move around a three dimensional environment. Objects are placed within the game world and players gain points for each object that is collected. The game clients connect to a game server which allows many players to coexist in the same game world. The basic functionality of this game is representative of current online first person shooter and role playing games. The game uses the DirectX 8 graphics API and DirectPlay networking API. In this section we will discuss the three different versions of the game that we experimented with.
Figure 3: The RPGQuest Game.
The first version of the game, which is the original implementation of RPGQuest, was created with a completely authoritative server and a non-authoritative client. Authority given to the server includes decisions of when a player collides with static objects and other players and when a player picks up an object. This version of the game performs well up to 100ms round-trip latency between the client and the server. There is little lag between the time player hits a wall and the time the server corrects the player"s position.
However, as more latency is induced between the client and server, the game becomes increasingly difficult to play. With the increased latency, the messages coming from the server correcting the player when it runs into a wall are not received fast enough. This causes the player to pass through the wall for the period that it is waiting for the server to resolve the collision.
When studying the source code of the original version of the RPGQuest game, there is a substantial delay that is unavoidable each time an action must be validated by the server. Whenever a movement update is sent to the server, the client must then wait whatever the round trip delay is, plus some processing time at the server in order to receive its validated or corrected position. This is obviously unacceptable in any game where movement or any other rapidly changing state information must be validated and disseminated to the other clients rapidly.
In order to get around this problem, we developed a second version of the game, which gives all authority to the client.
The client was delegated the authority to validate its own movement and the authority to pick up objects without validation from the server. In this version of the game when a player moves around the game space, the client validates that the player"s new position does not intersect with any walls or static objects. A position update is then sent to the server which then immediately forwards the update to the other clients within the region. The update does not have to go through any extra processing or validation.
This game model of complete authority given to the client is beneficial with respect to movement. When latencies of
100ms and up are induced into the link between the client and server, the game is still playable since time critical aspects of the game like movement do not have to wait on a reply from the server. When a player hits a wall, the collision is processed locally and does not have to wait on the server to resolve the collision.
Although game playing experience with respect to responsiveness is improved when the authority for movement is given to the client, there are still aspects of games that do not benefit from this approach. The most important of these is consistency. Although actions such as movement are time critical, other actions are not as time critical, but instead require consistency among the player states. An example of a game aspect that requires consistency is picking up objects that should only be possessed by a single player.
In our client authoritative version of RPGQuest clients send their own updates to all other players whenever they pick up an object. From our tests we have realized this is a problem because when there is a realistic amount of latency between the client and server, it is possible for two players to pick up the same object at the same time. When two players attempt to pick up an object at physical times which are close to each other, the update sent by the player who picked up the object first will not reach the second player in time for it to see that the object has already been claimed. The two players will now both think that they own the object.
This is why a server is still needed to be authoritative in this situation and maintain consistency throughout the players.
These two versions of the RPGQuest game has showed us why it is necessary to mix the two absolute models of authority. It is better to place authority on the client for quickly changing actions such as movement. It is not desirable to have to wait for server validation on a movement that could change before the reply is even received. It is also sometimes necessary to place consistency over efficiency in aspects of the game that cannot tolerate any inconsistencies such as object ownership. We believe that as the interactivity of games increases, our architecture of mixed authority that does not rely on server validation will be necessary.
To test the benefits and show the feasibility of our architecture of mixed authority, we developed a third version of the RPGQuest game that distributed authority for different actions between the client and server. In this version, in the interest of consistency, the server remained authoritative for deciding who picked up an object. The client was given full authority to send positional updates to other clients and verify its own position without the need to verify its updates with the server. When the player tries to move their avatar, the client verifies that the move will not cause it to move through a wall. A positional update is then sent to the server which then simply forwards it to the other clients within the region. This eliminates any extra processing delay that would occur at the server and is also a more accurate means of verification since the client has a more accurate view of its own state than the server.
This version of the RPGQuest game where authority is distributed between the client and server is an improvement from the server authoritative version. The client has no delay in waiting for an update for its own position and other clients do not have to wait on the server to verify the update.
The inconsistencies where two clients can pick up the same object in the client authoritative architecture are not present in this version of the client. However, the benefits of mixed authority will not truly be seen until an implementation of our communication proxy is integrated into the game. With the addition of the communication proxy, after the client verifies its own positional updates it will be able to send the update to all clients within its region through a low latency link instead of having to first go through the game server which could possibly be in a very remote location.
The coding of the different versions of the game was very simple. The complexity of the client increased very slightly in the client authoritative and hybrid models. The original dumb clients of RPGQuest know the position of other players; it is not just sent a screen snapshot from the server.
The server updates each client with the position of all nearby clients. The dumb clients use client side prediction to fill in the gaps between the updates they receive. The only extra processing the client has to do in the hybrid architecture is to compare its current position to the positions of all objects (walls, boxes, etc.) in its area. This obviously means that each client will have to already have downloaded the locations of all static objects within its current region.
It has been noted that in addition to latency, bandwidth requirements also dictate the type of gaming architecture to be used. In [16], different types of architectures are studied with respect to bandwidth efficiencies and latency. It is pointed out that Central Server architectures are not scalable because of bandwidth requirements at the server but the overhead for consistency checks are limited as they are performed at the server. A Peer-to-Peer architecture, on the other hand, is scalable but there is a significant overhead for consistency checks as this is required at every player.
The paper proposes a hybrid architecture which is Peer-toPeer in terms of message exchange (and thereby is scalable) where a Central Server is used for off-line consistency checks (thereby mitigating consistency check overhead). The paper provides an implementation example of BZFlag which is a peer-to-peer game which is modified to transfer all authority to a central server. In essence, this paper advocates an authority architecture which is server based even for peerto-peer games, but does not consider division of authority between a client and a server to minimize latency which could affect game playing experience even with the type of latency found in server based games (where all authority is with the server).
There is also previous work that has suggested that proxy based architectures be used to alleviate the latency problem and in addition use proxies to provide congestion control and cheat-proof mechanisms in distributed multi-player games [17]. In [18], a proxy server-network architecture is presented that is aimed at improving scalability of multiplayer games and lowering latency in server-client data transmission. The main goal of this work is to improve scalability of First-Person Shooter (FPS) and RPG games. The further objective is to improve the responsiveness MMOGs by providing low latency communications between the client and The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 7 server. The architecture uses interconnected proxy servers that each have a full view of the global game state. Proxy servers are located at various different ISPs. It is mentioned in this work that dividing the game space among multiple games servers such as the federated model presented in [19] is inefficient for a relatively fast game flow and that the proposed architecture alleviates this problem because users do not have to connect to a different server whenever they cross the server boundary. This architecture still requires all proxies to be aware of the overall game state over the whole game space unlike our work where we require the proxies to maintain only partial state information about the game space.
Fidelity based agent architectures have been proposed in [20, 21]. These works propose a distributed client-server architecture for distributed interactive simulations where different servers are responsible for different portions of the game space. When an object moves from one portion to another, there is a handoff from one server to another. Although these works propose an architecture where different portions of the simulation space are managed by different servers, they do not address the issue of decreasing the bandwidth required through the use of communication proxies.
Our work differs from the above discussed previous works by proposing a) a distributed proxy-based architecture to decrease bandwidth requirements at the clients and the servers without requiring the proxies to keep state information about the whole game space, b) a dynamic authority assignment technique to reduce latency (by performing consistency checks locally at the client whenever possible) by splitting the authority between the clients and servers on a per object basis, and c) proposing that cheat detection can be built into the proxies if they are provided more information about the specific game instead of using them purely as communication proxies (although this idea has not been implemented yet and is part of our future work).
In this paper, we first proposed a proxy-based architecture for MMOGs that enables MMOGs to scale to a large number of users by mitigating the need for a large number of transport sessions to be maintained and decreasing both bandwidth overhead and latency of event update.
Second, we proposed a mixed authority assignment mechanism that divides authority for making decisions on actions and events within the game between the clients and server and argued how such an authority assignment leads to better game playing experience without sacrificing the consistency of the game. Third, to validate the viability of the mixed authority assignment mechanism, we implemented it within a MMOG called RPGQuest and described our implementation experience.
In future work, we propose to implement the communications proxy architecture described in this paper and integrate the mixed authority mechanism within this architecture. We propose to evaluate the benefits of the proxy-based architecture in terms of scalability, accuracy and responsiveness. We also plan to implement a version of the RPGQuest game with dynamic assignment of authority to allow players the authority to pickup objects when no other players are near. As discussed earlier, this will allow for a more efficient and responsive game in certain situations and alleviate some of the processing load from the server.
Also, since so much trust is put into the clients of our architecture, it will be necessary to integrate into the architecture many of the cheat detection schemes that have been proposed in the literature. Software such as Punkbuster [22] and a reputation system like those proposed by [23] and [15] would be integral to the operation of an architecture such as ours which has a lot of trust placed on the client. We further propose to make the proxies in our architecture more game cognizant so that cheat detection mechanisms can be built into the proxies themselves.

Many studies before us have noted the Internet"s resistance to new services and evolution. In recent decades, numerous ideas have been developed in universities, implemented in code, and even written into the routers and end systems of the network, only to languish as network operators fail to turn them on on a large scale.
The list includes Multicast, IPv6, IntServ, and DiffServ. Lacking the incentives just to activate services, there seems to be little hope of ISPs devoting adequate resources to developing new ideas. In the long term, this pathology stands out as a critical obstacle to the network"s continued success (Ratnasamy, Shenker, and McCanne provide extensive discussion in [11]).
On a smaller time scale, ISPs shun new services in favor of cost cutting measures. Thus, the network has characteristics of a commodity market. Although in theory, ISPs have a plethora of routing policies at their disposal, the prevailing strategy is to route in the cheapest way possible [2]. On one hand, this leads directly to suboptimal routing. More importantly, commoditization in the short term is surely related to the lack of innovation in the long term.
When the routing decisions of others ignore quality characteristics,
ISPs are motivated only to lower costs. There is simply no reward for introducing new services or investing in quality improvements.
In response to these pathologies and others, researchers have put forth various proposals for improving the situation. These can be divided according to three high-level strategies: The first attempts to improve the status quo by empowering end-users. Clark, et al., suggest that giving end-users control over routing would lead to greater service diversity, recognizing that some payment mechanism must also be provided [5]. Ratnasamy, Shenker, and McCanne postulate a link between network evolution and user-directed routing [11]. They propose a system of Anycast to give end-users the ability to tunnel their packets to an ISP that introduces a desirable protocol. The extra traffic to the ISP, the authors suggest, will motivate the initial investment.
The second strategy suggests a revision of the contracting system.
This is exemplified by MacKie-Mason and Varian, who propose a smart market to control access to network resources [10]. Prices are set to the market-clearing level based on bids that users associate to their traffic. In another direction, Afergan and Wroclawski suggest that prices should be explicitly encoded in the routing protocols [2]. They argue that such a move would improve stability and align incentives.
The third high-level strategy calls for greater network accountability. In this vein, Argyraki, et al., propose a system of packet obituaries to provide feedback as to which ISPs drop packets [3]. They argue that such feedback would help reveal which ISPs were adequately meeting their contractual obligations. Unlike the first two strategies, we are not aware of any previous studies that have connected accountability with the pathologies of commoditization or lack of innovation.
It is clear that these three strategies are closely linked to each other (for example, [2], [5], and [9] each argue that giving end-users routing control within the current contracting system is problematic). Until today, however, the relationship between them has been poorly understood. There is currently little theoretical foundation to compare the relative merits of each proposal, and a particular lack of evidence linking accountability with innovation and service differentiation. This paper will address both issues.
We will begin by introducing an economic network model that relates accountability, contracts, competition, and innovation. Our model is highly stylized and may be considered preliminary: it is based on a single source sending data to a single destination.
Nevertheless, the structure is rich enough to expose previously unseen features of network behavior. We will use our model for two main purposes: First, we will use our model to argue that the lack of accountability in today"s network is a fundamental obstacle to overcoming the pathologies of commoditization and lack of innovation. In other words, unless new monitoring capabilities are introduced, and integrated with the system of contracts, the network cannot achieve optimal routing and innovation characteristics. This result provides motivation for the remainder of the paper, in which we explore how accountability can be leveraged to overcome these pathologies and create a sustainable industry. We will approach this problem from a clean-slate perspective, deriving the level of accountability needed to sustain an ideal competitive structure.
When we say that today"s Internet has poor accountability, we mean that it reveals little information about the behavior - or misbehavior - of ISPs. This well-known trait is largely rooted in the network"s history. In describing the design philosophy behind the Internet protocols, Clark lists accountability as the least important among seven second level goals. [4] Accordingly, accountability received little attention during the network"s formative years. Clark relates this to the network"s military context, and finds that had the network been designed for commercial development, accountability would have been a top priority.
Argyraki, et al., conjecture that applying the principles of layering and transparency may have led to the network"s lack of accountability [3]. According to these principles, end hosts should be informed of network problems only to the extent that they are required to adapt. They notice when packet drops occur so that they can perform congestion control and retransmit packets. Details of where and why drops occur are deliberately concealed.
The network"s lack of accountability is highly relevant to a discussion of innovation because it constrains the system of contracts. This is because contracts depend upon external institutions to function - the judge in the language of incomplete contract theory, or simply the legal system. Ultimately, if a judge cannot verify that some condition holds, she cannot enforce a contract based on that condition. Of course, the vast majority of contracts never end up in court. Especially when a judge"s ruling is easily predicted, the parties will typically comply with the contract terms on their own volition. This would not be possible, however, without the judge acting as a last resort.
An institution to support contracts is typically complex, but we abstract it as follows: We imagine that a contract is an algorithm that outputs a payment transfer among a set of ISPs (the parties) at every time. This payment is a function of the past and present behaviors of the participants, but only those that are verifiable.
Hence, we imagine that a contract only accepts proofs as inputs.
We will call any process that generates these proofs a contractible monitor. Such a monitor includes metering or sensing devices on the physical network, but it is a more general concept. Constructing a proof of a particular behavior may require readings from various devices distributed among many ISPs. The contractible monitor includes whatever distributed algorithmic mechanism is used to motivate ISPs to share this private information.
Figure 1 demonstrates how our model of contracts fits together. We make the assumption that all payments are mediated by contracts.
This means that without contractible monitors that attest to, say, latency, payments cannot be conditioned on latency.
Figure 1: Relationship between monitors and contracts With this model, we may conclude that the level of accountability in today"s Internet only permits best effort contracts. Nodes cannot condition payments on either quality or path characteristics.
Is there anything wrong with best-effort contracts? The reader might wonder why the Internet needs contracts at all. After all, in non-network industries, traditional firms invest in research and differentiate their products, all in the hopes of keeping their customers and securing new ones. One might believe that such market forces apply to ISPs as well. We may adopt this as our null hypothesis: Null hypothesis: Market forces are sufficient to maintain service diversity and innovation on a network, at least to the same extent as they do in traditional markets.
There is a popular intuitive argument that supports this hypothesis, and it may be summarized as follows: Intuitive argument supporting null hypothesis:
consumers.
ISPs, and the second hops will therefore try to provide highquality service in order to secure traffic from access providers. Access providers try to select high quality transit because that increases their quality.
ISP a competitive reason to increase quality.
We are careful to model our network in continuous time, in order to capture the essence of this argument. We can, for example, specify equilibria in which nodes switch to a new next hop in the event of a quality drop.
Moreover, our model allows us to explore any theoretically possible punishments against cheaters, including those that are costly for end-users to administer. By contrast, customers in the real world rarely respond collectively, and often simply seek the best deal currently offered. These constraints limit their ability to punish cheaters.
Even with these liberal assumptions, however, we find that we must reject our null hypothesis. Our model will demonstrate that identifying a cheating ISP is difficult under low accountability, limiting the threat of market driven punishment. We will define an index of commoditization and show that it increases without bound as data paths grow long. Furthermore, we will demonstrate a framework in which an ISP"s maximum research investment decreases hyperbolically with its distance from the end-user.
Network Behavior Monitor Contract Proof Payments 184 To summarize, we argue that the Internet"s lack of accountability must be addressed before the pathologies of commoditization and lack of innovation can be resolved. This leads us to our next topic: How can we leverage accountability to overcome these pathologies?
We approach this question from a clean-slate perspective. Instead of focusing on incremental improvements, we try to imagine how an ideal industry would behave, then derive the level of accountability needed to meet that objective. According to this approach, we first craft a new equilibrium concept appropriate for network competition. Our concept includes the following requirements: First, we require that punishing ISPs that cheat is done without rerouting the path. Rerouting is likely to prompt end-users to switch providers, punishing access providers who administer punishments correctly. Next, we require that the equilibrium cannot be threatened by a coalition of ISPs that exchanges illicit side payments. Finally, we require that the punishment mechanism that enforces contracts does not punish innocent nodes that are not in the coalition.
The last requirement is somewhat unconventional from an economic perspective, but we maintain that it is crucial for any reasonable solution. Although ISPs provide complementary services when they form a data path together, they are likely to be horizontal competitors as well. If innocent nodes may be punished, an ISP may decide to deliberately cheat and draw punishment onto itself and its neighbors. By cheating, the ISP may save resources, thereby ensuring that the punishment is more damaging to the other ISPs, which probably compete with the cheater directly for some customers. In the extreme case, the cheater may force the other ISPs out of business, thereby gaining a monopoly on some routes.
Applying this equilibrium concept, we derive the monitors needed to maintain innovation and optimize routes. The solution is surprisingly simple: contractible monitors must report the quality of the rest of the path, from each ISP to the destination. It turns out that this is the correct minimum accountability requirement, as opposed to either end-to-end monitors or hop-by-hop monitors, as one might initially suspect.
Rest of path monitors can be implemented in various ways. They may be purely local algorithms that listen for packet echoes.
Alternately, they can be distributed in nature. We describe a way to construct a rest of path monitor out of monitors for individual ISP quality and for the data path. This requires a mechanism to motivate ISPs to share their monitor outputs with each other. The rest of path monitor then includes the component monitors and the distributed algorithmic mechanism that ensures that information is shared as required. This example shows that other types of monitors may be useful as building blocks, but must be combined to form rest of path monitors in order to achieve ideal innovation characteristics.
Our study has several practical implications for future protocol design. We show that new monitors must be implemented and integrated with the contracting system before the pathologies of commoditization and lack of innovation can be overcome.
Moreover, we derive exactly what monitors are needed to optimize routes and support innovation. In addition, our results provide useful input for clean-slate architectural design, and we use several novel techniques that we expect will be applicable to a variety of future research.
The rest of this paper is organized as follows: In section 2, we lay out our basic network model. In section 3, we present a lowaccountability network, modeled after today"s Internet. We demonstrate how poor monitoring causes commoditization and a lack of innovation. In section 4, we present verifiable monitors, and show that proofs, even without contracts, can improve the status quo. In section 5, we turn our attention to contractible monitors.
We show that rest of path monitors can support competition games with optimal routing and innovation. We further show that rest of path monitors are required to support such competition games. We continue by discussing how such monitors may be constructed using other monitors as building blocks. In section 6, we conclude and present several directions for future research.
A source, S, wants to send data to destination, D. S and D are nodes on a directed, acyclic graph, with a finite set of intermediate nodes, { }NV ,...2,1= , representing ISPs. All paths lead to D, and every node not connected to D has at least two choices for next hop.
We will represent quality by a finite dimensional vector space, Q, called the quality space. Each dimension represents a distinct network characteristic that end-users care about. For example, latency, loss probability, jitter, and IP version can each be assigned to a dimension.
To each node, i, we associate a vector in the quality space, Qqi ∈ .
This corresponds to the quality a user would experience if i were the only ISP on the data path. Let N Q∈q be the vector of all node qualities.
Of course, when data passes through multiple nodes, their qualities combine in some way to yield a path quality. We represent this by an associative binary operation, *: QQQ →× . For path ( )nvvv ,...,, 21 , the quality is given by nvvv qqq ∗∗∗ ...21 . The * operation reflects the characteristics of each dimension of quality.
For example, * can act as an addition in the case of latency, multiplication in the case of loss probability, or a minimumargument function in the case of security.
When data flows along a complete path from S to D, the source and destination, generally regarded as a single player, enjoy utility given by a function of the path quality, →Qu : . Each node along the path, i, experiences some cost of transmission, ci.
Ultimately, we are most interested in policies that promote innovation on the network. In this study, we will use innovation in a fairly general sense. Innovation describes any investment by an ISP that alters its quality vector so that at least one potential data path offers higher utility. This includes researching a new routing algorithm that decreases the amount of jitter users experience. It also includes deploying a new protocol that supports quality of service. Even more broadly, buying new equipment to decrease S D 185 latency may also be regarded as innovation. Innovation may be thought of as the micro-level process by which the network evolves.
Our analysis is limited in one crucial respect: We focus on inventions that a single ISP can implement to improve the end-user experience. This excludes technologies that require adoption by all ISPs on the network to function.
Because such technologies do not create a competitive advantage, rewarding them is difficult and may require intellectual property or some other market distortion. We defer this interesting topic to future work.
At first, it may seem unclear how a large-scale distributed process such as innovation can be influenced by mechanical details like networks monitors. Our model must draw this connection in a realistic fashion.
The rate of innovation depends on the profits that potential innovators expect in the future. The reward generated by an invention must exceed the total cost to develop it, or the inventor will not rationally invest. This reward, in turn, is governed by the competitive environment in which the firm operates, including the process by which firms select prices, and agree upon contracts with each other. Of course, these decisions depend on how routes are established, and how contracts determine actual monetary exchanges.
Any model of network innovation must therefore relate at least three distinct processes: innovation, competition, and routing. We select a game dynamics that makes the relation between these processes as explicit as possible. This is represented schematically in Figure 2.
The innovation stage occurs first, at time 2−=t . In this stage, each agent decides whether or not to make research investments. If she chooses not to, her quality remains fixed. If she makes an investment, her quality may change in some way. It is not necessary for us to specify how such changes take place. The agents" choices in this stage determine the vector of qualities, q, common knowledge for the rest of the game.
Next, at time 1−=t , agents participate in the competition stage, in which contracts are agreed upon. In today"s industry, these contracts include prices for transit access, and peering agreements.
Since access is provided on a best-effort basis, a transit agreement can simply be represented by its price. Other contracting systems we will explore will require more detail.
Finally, beginning at 0=t , firms participate in the routing stage.
Other research has already employed repeated games to study routing, for example [1], [12]. Repetition reveals interesting effects not visible in a single stage game, such as informal collusion to elevate prices in [12]. We use a game in continuous time in order to study such properties. For example, we will later ask whether a player will maintain higher quality than her contracts require, in the hope of keeping her customer base or attracting future customers.
Our dynamics reflect the fact that ISPs make innovation decisions infrequently. Although real firms have multiple opportunities to innovate, each opportunity is followed by a substantial length of time in which qualities are fixed. The decision to invest focuses on how the firm"s new quality will improve the contracts it can enter into. Hence, our model places innovation at the earliest stage, attempting to capture a single investment decision. Contracting decisions are made on an intermediate time scale, thus appearing next in the dynamics. Routing decisions are made very frequently, mainly to maximize immediate profit flows, so they appear in the last stage.
Because of this ordering, our model does not allow firms to route strategically to affect future innovation or contracting decisions. In opposition, Afergan and Wroclawski argue that contracts are formed in response to current traffic patterns, in a feedback loop [2].
Although we are sympathetic to their observation, such an addition would make our analysis intractable. Our model is most realistic when contracting decisions are infrequent.
Throughout this paper, our solution concept will be a subgame perfect equilibrium (SPE). An SPE is a strategy point that is a Nash equilibrium when restricted to each subgame. Three important subgames have been labeled in Figure 2. The innovation game includes all three stages. The competition game includes only the competition stage and the routing stage. The routing game includes only the routing stage.
An SPE guarantees that players are forward-looking. This means, for example, that in the competition stage, firms must act rationally, maximizing their expected profits in the routing stage. They cannot carry out threats they made in the innovation stage if it lowers their expected payoff.
Our schematic already suggests that the routing game is crucial for promoting innovation. To support innovation, the competition game must somehow reward ISPs with high quality. But that means that the routing game must tend to route to nodes with high quality. If the routing game always selects the lowest-cost routes, for example, innovation will not be supported. We will support this observation with analysis later.
The routing game proceeds in continuous time, with all players discounting by a common factor, r. The outputs from previous stages, q and the set of contracts, are treated as exogenous parameters for this game. For each time 0≥t , each node must select a next hop to route data to. Data flows across the resultant path, causing utility flow to S and D, and a flow cost to the nodes on the path, as described above. Payment flows are also created, based on the contracts in place.
Relating our game to the familiar repeated prisoners" dilemma, imagine that we are trying to impose a high quality, but costly path.
As we argued loosely above, such paths must be sustainable in order to support innovation. Each ISP on the path tries to maximize her own payment, net of costs, so she may not want to cooperate with our plan. Rather, if she can find a way to save on costs, at the expense of the high quality we desire, she will be tempted to do so.
Innovation Game Competition Game Routing Game Innovation stage Competition stage Routing stageQualities (q) Contracts (prices) Profits t = -2 t = -1 t ∈ [ 0 , ) Figure 2: Game Dynamics 186 Analogously to the prisoners" dilemma, we will call such a decision cheating. A little more formally,
Cheating refers to any action that an ISP can take, contrary to some target strategy point that we are trying to impose, that enhances her immediate payoff, but compromises the quality of the data path.
One type of cheating relates to the data path. Each node on the path has to pay the next node to deliver its traffic. If the next node offers high quality transit, we may expect that a lower quality node will offer a lower price. Each node on the path will be tempted to route to a cheaper next hop, increasing her immediate profits, but lowering the path quality. We will call this type of action cheating in route.
Another possibility we can model, is that a node finds a way to save on its internal forwarding costs, at the expense of its own quality.
We will call this cheating internally to distinguish it from cheating in route. For example, a node might drop packets beyond the rate required for congestion control, in order to throttle back TCP flows and thus save on forwarding costs [3]. Alternately, a node employing quality of service could give high priority packets a lower class of service, thus saving on resources and perhaps allowing itself to sell more high priority service.
If either cheating in route or cheating internally is profitable, the specified path will not be an equilibrium. We assume that cheating can never be caught instantaneously. Rather, a cheater can always enjoy the payoff from cheating for some positive time, which we label 0t . This includes the time for other players to detect and react to the cheating. If the cheater has a contract which includes a customer lock-in period, 0t also includes the time until customers are allowed to switch to a new ISP. As we will see later, it is socially beneficial to decrease 0t , so such lock-in is detrimental to welfare.
LOWACCOUNTABILITY NETWORK In order to motivate an exploration of monitoring systems, we begin in this section by considering a network with a poor degree of accountability, modeled after today"s Internet. We will show how the lack of monitoring necessarily leads to poor routing and diminishes the rate of innovation. Thus, the network"s lack of accountability is a fundamental obstacle to resolving these pathologies.
First, we reflect on what accountability characteristics the present Internet has. Argyraki, et al., point out that end hosts are given minimal information about packet drops [3]. Users know when drops occur, but not where they occur, nor why. Dropped packets may represent the innocent signaling of congestion, or, as we mentioned above, they may be a form of cheating internally. The problem is similar for other dimensions of quality, or in fact more acute. Finding an ISP that gives high priority packets a lower class of service, for example, is further complicated by the lack of even basic diagnostic tools.
In fact, it is similarly difficult to identify an ISP that cheats in route.
Huston notes that Internet traffic flows do not always correspond to routing information [8]. An ISP may hand a packet off to a neighbor regardless of what routes that neighbor has advertised.
Furthermore, blocks of addresses are summarized together for distant hosts, so a destination may not even be resolvable until packets are forwarded closer.
One might argue that diagnostic tools like ping and traceroute can identify cheaters. Unfortunately, Argyraki, et al., explain that these tools only reveal whether probe packets are echoed, not the fate of past packets [3]. Thus, for example, they are ineffective in detecting low-frequency packet drops. Even more fundamentally, a sophisticated cheater can always spot diagnostic packets and give them special treatment.
As a further complication, a cheater may assume different aliases for diagnostic packets arriving over different routes. As we will see below, this gives the cheater a significant advantage in escaping punishment for bad behavior, even if the data path is otherwise observable.
As the above evidence suggests, the current industry allows for very little insight into the behavior of the network. In this section, we attempt to capture this lack of accountability in our model. We begin by defining a monitor, our model of the way that players receive external information about network behavior,
A monitor is any distributed algorithmic mechanism that runs on the network graph, and outputs, to specific nodes, informational statements about current or past network behavior.
We assume that all external information about network behavior is mediated in this way. The accountability properties of the Internet can be represented by the following monitors: E2E (End to End): A monitor that informs S/D about what the total path quality is at any time (this is the quality they experience).
ROP (Rest of Path): A monitor that informs each node along the data path what the quality is for the rest of the path to the destination.
PRc (Packets Received): A monitor that tells nodes how much data they accept from each other, so that they can charge by volume. It is important to note, however, that this information is aggregated over many source-destination pairs. Hence, for the sake of realism, it cannot be used to monitor what the data path is.
Players cannot measure the qualities of other, single nodes, just the rest of the path. Nodes cannot see the path past the next hop. This last assumption is stricter than needed for our results. The critical ingredient is that nodes cannot verify that the path avoids a specific hop. This holds, for example, if the path is generally visible, except nodes can use different aliases for different parents. Similar results also hold if alternate paths always converge after some integer number, m, of hops.
It is important to stress that E2E and ROP are not the contractible monitors we described in the introduction - they do not generate proofs. Thus, even though a player observes certain information, she generally cannot credibly share it with another player. For example, if a node after the first hop starts cheating, the first hop will detect the sudden drop in quality for the rest of the path, but the first hop cannot make the source believe this observation - the 187 source will suspect that the first hop was the cheater, and fabricated the claim against the rest of the path.
Typically, E2E and ROP are envisioned as algorithms that run on a single node, and listen for packet echoes. This is not the only way that they could be implemented, however; an alternate strategy is to aggregate quality measurements from multiple points in the network. These measurements can originate in other monitors, located at various ISPs. The monitor then includes the component monitors as well as whatever mechanisms are in place to motivate nodes to share information honestly as needed. For example, if the source has monitors that reveal the qualities of individual nodes, they could be combined with path information to create an ROP monitor.
Since we know that contracts only accept proofs as input, we can infer that payments in this environment can only depend on the number of packets exchanged between players. In other words, contracts are best-effort. For the remainder of this section, we will assume that contracts are also linear - there is a constant payment flow so long as a node accepts data, and all conditions of the contract are met. Other, more complicated tariffs are also possible, and are typically used to generate lock-in. We believe that our parameter t0 is sufficient to describe lock-in effects, and we believe that the insights in this section apply equally to any tariffs that are bounded so that the routing game remains continuous at infinity.
Restricting attention to linear contracts allows us to represent some node i"s contract by its price, pi.
Because we further know that nodes cannot observe the path after the next hop, we can infer that contracts exist only between neighboring nodes on the graph. We will call this arrangement of contracts bilateral. When a competition game exclusively uses bilateral contracts, we will call it a bilateral contract competition game.
We first focus on the routing game and ask whether a high quality route can be maintained, even when a low quality route is cheaper.
Recall that this is a requirement in order for nodes to have any incentive to innovate. If nodes tend to route to low price next hops, regardless of quality, we say that the network is commoditized. To measure this tendency, we define an index of commoditization as follows: For a node on the data path, i, define its quality premium, minppd ji −= , where pj is the flow payment to the next hop in equilibrium, and pmin is the price of the lowest cost next hop.
Definition: The index of commoditization, CI , is the average, over each node on the data path, i, of i"s flow profit as a fraction of i"s quality premium, ( ) ijii dpcp /−− .
CI ranges from 0, when each node spends all of its potential profit on its quality premium, to infinite, when a node absorbs positive profit, but uses the lowest price next hop. A high value for CI implies that nodes are spending little of their money inflow on purchasing high quality for the rest of the path. As the next claim shows, this is exactly what happens as the path grows long: Claim 1. If the only monitors are E2E, ROP, and PRc, ∞→CI as ∞→n , where n is the number of nodes on the data path.
To show that this is true, we first need the following lemma, which will establish the difficulty of punishing nodes in the network.
First a bit of notation: Recall that a cheater can benefit from its actions for 00 >t before other players can react. When a node cheats, it can expect a higher profit flow, at least until it is caught and other players react, perhaps by diverting traffic. Let node i"s normal profit flow be iπ , and her profit flow during cheating be some greater value, yi. We will call the ratio, iiy π/ , the temptation to cheat.
Lemma 1. If the only monitors are E2E, ROP, and PRc, the discounted time, −nt rt e 0 , needed to punish a cheater increases at least as fast as the product of the temptations to cheat along the data path, ∏ −− ≥ 0 0 pathdataon 0 t rt i i i t rt e y e n π (1) Corollary. If nodes share a minimum temptation to cheat, π/y , the discounted time needed to punish cheating increases at least exponentially in the length of the data path, n, −− ≥ 0 00 t rt nt rt e y e n π (2) Since it is the discounted time that increases exponentially, the actual time increases faster than exponentially. If n is so large that tn is undefined, the given path cannot be maintained in equilibrium.
Proof. The proof proceeds by induction on the number of nodes on the equilibrium data path, n. For 1=n , there is a single node, say i.
By cheating, the node earns extra profit ( ) − − 0 0 t rt ii ey π . If node i is then punished until time 1t , the extra profit must be cancelled out by the lost profit between time 0t and 1t , −1 0 t t rt i eπ . A little manipulation gives −− = 01 00 t rt i i t rt e y e π , as required.
For 1>n , assume for induction that the claim holds for 1−n . The source does not know whether the cheater is the first hop, or after the first hop. Because the source does not know the data path after the first hop, it is unable to punish nodes beyond it. If it chooses a new first hop, it might not affect the rest of the data path. Because of this, the source must rely on the first hop to punish cheating nodes farther along the path. The first hop needs discounted time, ∏ −0 0 hopfirstafter t rt i i i e y π , to accomplish this by assumption. So the source must give the first hop this much discounted time in order to punish defectors further down the line (and the source will expect poor quality during this period).
Next, the source must be protected against a first hop that cheats, and pretends that the problem is later in the path. The first hop can 188 do this for the full discounted time, ∏ −0 0 hopfirstafter t rt i i i e y π , so the source must punish the first hop long enough to remove the extra profit it can make. Following the same argument as for 1=n , we can show that the full discounted time is ∏ −0 0 pathdataon t rt i i i e y π , which completes the proof.
The above lemma and its corollary show that punishing cheaters becomes more and more difficult as the data path grows long, until doing so is impossible. To capture some intuition behind this result, imagine that you are an end user, and you notice a sudden drop in service quality. If your data only travels through your access provider, you know it is that provider"s fault. You can therefore take your business elsewhere, at least for some time. This threat should motivate your provider to maintain high quality.
Suppose, on the other hand, that your data traverses two providers.
When you complain to your ISP, he responds, yes, we know your quality went down, but it"s not our fault, it"s the next ISP. Give us some time to punish them and then normal quality will resume. If your access provider is telling the truth, you will want to listen, since switching access providers may not even route around the actual offender. Thus, you will have to accept lower quality service for some longer time. On the other hand, you may want to punish your access provider as well, in case he is lying. This means you have to wait longer to resume normal service. As more ISPs are added to the path, the time increases in a recursive fashion.
With this lemma in hand, we can return to prove Claim 1.
Proof of Claim 1. Fix an equilibrium data path of length n. Label the path nodes 1,2,…,n. For each node i, let i"s quality premium be '11 ++ −= iii ppd . Then we have, [ ] = − = − + + = − + ++ = + −=− −− −− = −− − = −− = n i i n i iii iii n i iii ii n i i iii C g npcp pcp n pcp pp nd pcp n I 1 1 1 1 1 1 1 1 1 11 1 1 1 1 1 '1 '11 , (3) where gi is node i"s temptation to cheat by routing to the lowest price next hop. Lemma 1 tells us that Tg n i i <∏ =1 , where ( )01 rt eT − −= . It requires a bit of calculus to show that IC is minimized by setting each gi equal to n T /1 . However, as ∞→n , we have 1/1 →n T , which shows that ∞→CI .
According to the claim, as the data path grows long, it increasingly resembles a lowest-price path. Since lowest-price routing does not support innovation, we may speculate that innovation degrades with the length of the data path. Though we suspect stronger claims are possible, we can demonstrate one such result by including an extra assumption: Available Bargain Path: A competitive market exists for lowcost transit, such that every node can route to the destination for no more than flow payment, lp .
Claim 2. Under the available bargain path assumption, if node i , a distance n from S, can invest to alter its quality, and the source will spend no more than sP for a route including node i"s new quality, then the payment to node i, p, decreases hyperbolically with n, ( ) ( ) s n l P n T pp 1 1/1 − +≤ − , (4) where ( )01 rt eT − −= is the bound on the product of temptations from the previous claim. Thus, i will spend no more than ( ) ( )− + − s n l P n T p r 1
on this quality improvement, which approaches the bargain path"s payment, r pl , as ∞→n .
The proof is given in the appendix. As a node gets farther from the source, its maximum payment approaches the bargain price, pl.
Hence, the reward for innovation is bounded by the same amount.
Large innovations, meaning substantially more expensive than rpl / , will not be pursued deep into the network.
Claim 2 can alternately be viewed as a lower bound on how much it costs to elicit innovation in a network. If the source S wants node i to innovate, it needs to get a motivating payment, p, to i during the routing stage. However, it must also pay the nodes on the way to i a premium in order to motivate them to route properly. The claim shows that this premium increases with the distance to i, until it dwarfs the original payment, p.
Our claims stand in sharp contrast to our null hypothesis from the introduction. Comparing the intuitive argument that supported our hypothesis with these claims, we can see that we implicitly used an oversimplified model of market pressure (as either present or not).
As is now clear, market pressure relies on the decisions of customers, but these are limited by the lack of information. Hence, competitive forces degrade as the network deepens.
In this section, we begin to introduce more accountability into the network. Recall that in the previous section, we assumed that players couldn"t convince each other of their private information.
What would happen if they could? If a monitor"s informational signal can be credibly conveyed to others, we will call it a verifiable monitor. The monitor"s output in this case can be thought of as a statement accompanied by a proof, a string that can be processed by any player to determine that the statement is true.
A verifiable monitor is a distributed algorithmic mechanism that runs on the network graph, and outputs, to specific nodes, proofs about current or past network behavior.
Along these lines, we can imagine verifiable counterparts to E2E and ROP. We will label these E2Ev and ROPv. With these monitors, each node observes the quality of the rest of the path and can also convince other players of these observations by giving them a proof. 189 By adding verifiability to our monitors, identifying a single cheater is straightforward. The cheater is the node that cannot produce proof that the rest of path quality decreased. This means that the negative results of the previous section no longer hold. For example, the following lemma stands in contrast to Lemma 1.
Lemma 2. With monitors E2Ev, ROPv, and PRc, and provided that the node before each potential cheater has an alternate next hop that isn"t more expensive, it is possible to enforce any data path in SPE so long as the maximum temptation is less than what can be deterred in finite time, − ≤ 0 0 max 1 t rt er y π (5) Proof. This lemma follows because nodes can share proofs to identify who the cheater is. Only that node must be punished in equilibrium, and the preceding node does not lose any payoff in administering the punishment.
With this lemma in mind, it is easy to construct counterexamples to Claim 1 and Claim 2 in this new environment.
Unfortunately, there are at least four reasons not to be satisfied with this improved monitoring system. The first, and weakest reason is that the maximum temptation remains finite, causing some distortion in routes or payments. Each node along a route must extract some positive profit unless the next hop is also the cheapest.
Of course, if t0 is small, this effect is minimal.
The second, and more serious reason is that we have always given our source the ability to commit to any punishment. Real world users are less likely to act collectively, and may simply search for the best service currently offered. Since punishment phases are generally characterized by a drop in quality, real world end-users may take this opportunity to shop for a new access provider. This will make nodes less motivated to administer punishments.
The third reason is that Lemma 2 does not apply to cheating by coalitions. A coalition node may pretend to punish its successor, but instead enjoy a secret payment from the cheating node.
Alternately, a node may bribe its successor to cheat, if the punishment phase is profitable, and so forth. The required discounted time for punishment may increase exponentially in the number of coalition members, just as in the previous section! The final reason not to accept this monitoring system is that when a cheater is punished, the path will often be routed around not just the offender, but around other nodes as well. Effectively, innocent nodes will be punished along with the guilty. In our abstract model, this doesn"t cause trouble since the punishment falls off the equilibrium path. The effects are not so benign in the real world.
When ISPs lie in sequence along a data path, they contribute complementary services, and their relationship is vertical. From the perspective of other source-destination pairs, however, these same firms are likely to be horizontal competitors. Because of this, a node might deliberately cheat, in order to trigger punishment for itself and its neighbors. By cheating, the node will save money to some extent, so the cheater is likely to emerge from the punishment phase better off than the innocent nodes. This may give the cheater a strategic advantage against its competitors. In the extreme, the cheater may use such a strategy to drive neighbors out of business, and thereby gain a monopoly on some routes.
At the end of the last section, we identified several drawbacks that persist in an environment with E2Ev, ROPv, and PRc. In this section, we will show how all of these drawbacks can be overcome.
To do this, we will require our third and final category of monitor: A contractible monitor is simply a verifiable monitor that generates proofs that can serve as input to a contract. Thus, contractible is jointly a property of the monitor and the institutions that must verify its proofs. Contractibility requires that a court,
the extent required to police illegal activity.
Understanding the agreements between companies has traditionally been a matter of reading contracts on paper. This may prove to be a harder task in a future network setting. Contracts may plausibly be negotiated by machine, be numerous, even per-flow, and be further complicated by the many dimensions of quality.
When a monitor (together with institutional infrastructure) meets these criteria, we will label it with a subscript c, for contractible.
The reader may recall that this is how we labeled the packets received monitor, PRc, which allows ISPs to form contracts with per-packet payments. Similarly, E2Ec and ROPc are contractible versions of the monitors we are now familiar with.
At the end of the previous section, we argued for some desirable properties that we"d like our solution to have. Briefly, we would like to enforce optimal data paths with an equilibrium concept that doesn"t rely on re-routing for punishment, is coalition proof, and doesn"t punish innocent nodes when a coalition cheats. We will call such an equilibrium a fixed-route coalition-proof protect-theinnocent equilibrium.
As the next claim shows, ROPc allows us to create a system of linear (price, quality) contracts under just such an equilibrium.
Claim 3. With ROPc, for any feasible and consistent assignment of rest of path qualities to nodes, and any corresponding payment schedule that yields non-negative payoffs, these qualities can be maintained with bilateral contracts in a fixed-route coalition-proof protect-the-innocent equilibrium.
Proof: Fix any data path consistent with the given rest of path qualities. Select some monetary punishment, P, large enough to prevent any cheating for time t0 (the discounted total payment from the source will work). Let each node on the path enter into a contract with its parent, which fixes an arbitrary payment schedule so long as the rest of path quality is as prescribed. When the parent node, which has ROPc, submits a proof that the rest of path quality is less than expected, the contract awards her an instantaneous transfer, P, from the downstream node. Such proofs can be submitted every 0t for the previous interval.
Suppose now that a coalition, C, decides to cheat. The source measures a decrease in quality, and according to her contract, is awarded P from the first hop. This means that there is a net outflow of P from the ISPs as a whole. Suppose that node i is not in C. In order for the parent node to claim P from i, it must submit proof that the quality of the path starting at i is not as prescribed. This means 190 that there is a cheater after i. Hence, i would also have detected a change in quality, so i can claim P from the next node on the path.
Thus, innocent nodes are not punished. The sequence of payments must end by the destination, so the net outflow of P must come from the nodes in C. This establishes all necessary conditions of the equilibrium.
Essentially, ROPc allows for an implementation of (price, quality) contracts. Building upon this result, we can construct competition games in which nodes offer various qualities to each other at specified prices, and can credibly commit to meet these performance targets, even allowing for coalitions and a desire to damage other ISPs.
Example 1. Define a Stackelberg price-quality competition game as follows: Extend the partial order of nodes induced by the graph to any complete ordering, such that downstream nodes appear before their parents. In this order, each node selects a contract to offer to its parents, consisting of a rest of path quality, and a linear price. In the routing game, each node selects a next hop at every time, consistent with its advertised rest of path quality. The Stackelberg price-quality competition game can be implemented in our model with ROPc monitors, by using the strategy in the proof, above. It has the following useful property: Claim 4. The Stackelberg price-quality competition game yields optimal routes in SPE.
The proof is given in the appendix. This property is favorable from an innovation perspective, since firms that invest in high quality will tend to fall on the optimal path, gaining positive payoff. In general, however, investments may be over or under rewarded. Extra conditions may be given under which innovation decisions approach perfect efficiency for large innovations. We omit the full analysis here.
Example 2. Alternately, we can imagine that players report their private information to a central authority, which then assigns all contracts. For example, contracts could be computed to implement the cost-minimizing VCG mechanism proposed by Feigenbaum, et al. in [7]. With ROPc monitors, we can adapt this mechanism to maximize welfare. For node, i, on the optimal path, L, the net payment must equal, essentially, its contribution to the welfare of S,
D, and the other nodes. If L" is an optimal path in the graph with i removed, the profit flow to i is, ( ) ( ) ∈≠∈ +−− ', ' Lj j ijLj jLL ccququ , (6) where Lq and 'Lq are the qualities of the two paths. Here, (price, quality) contracts ensure that nodes report their qualities honestly.
The incentive structure of the VCG mechanism is what motivates nodes to report their costs accurately.
A nice feature of this game is that individual innovation decisions are efficient, meaning that a node will invest in an innovation whenever the investment cost is less than the increased welfare of the optimal data path. Unfortunately, the source may end up paying more than the utility of the path.
Notice that with just E2Ec, a weaker version of Claim 3 holds.
Bilateral (price, quality) contracts can be maintained in an equilibrium that is fixed-route and coalition-proof, but not protectthe-innocent. This is done by writing contracts to punish everyone on the path when the end to end quality drops. If the path length is n, the first hop pays nP to the source, the second hop pays ( )Pn 1− to the first, and so forth. This ensures that every node is punished sufficiently to make cheating unprofitable. For the reasons we gave previously, we believe that this solution concept is less than ideal, since it allows for malicious nodes to deliberately trigger punishments for potential competitors.
Up to this point, we have adopted fixed-route coalition-proof protect-the-innocent equilibrium as our desired solution concept, and shown that ROPc monitors are sufficient to create some competition games that are desirable in terms of service diversity and innovation. As the next claim will show, rest of path monitoring is also necessary to construct such games under our solution concept.
Before we proceed, what does it mean for a game to be desirable from the perspective of service diversity and innovation? We will use a very weak assumption, essentially, that the game is not fully commoditized for any node. The claim will hold for this entire class of games.
Definition: A competition game is nowhere-commoditized if for each node, i, not adjacent to D, there is some assignment of qualities and marginal costs to nodes, such that the optimal data path includes i, and i has a positive temptation to cheat.
In the case of linear contracts, it is sufficient to require that ∞<CI , and that every node make positive profit under some assignment of qualities and marginal costs.
Strictly speaking, ROPc monitors are not the only way to construct these desirable games. To prove the next claim, we must broaden our notion of rest of path monitoring to include the similar ROPc" monitor, which attests to the quality starting at its own node, through the end of the path. Compare the two monitors below: ROPc: gives a node proof that the path quality from the next node to the destination is not correct.
ROPc": gives a node proof that the path quality from that node to the destination is correct.
We present a simplified version of this claim, by including an assumption that only one node on the path can cheat at a time (though conspirators can still exchange side payments). We will discuss the full version after the proof.
Claim 5. Assume a set of monitors, and a nowhere-commoditized bilateral contract competition game that always maintains the optimal quality in fixed-route coalition-proof protect-the-innocent equilibrium, with only one node allowed to cheat at a time. Then for each node, i, not adjacent to D, either i has an ROPc monitor, or i"s children each have an ROPc" monitor.
Proof: First, because of the fixed-route assumption, punishments must be purely monetary.
Next, when cheating occurs, if the payment does not go to the source or destination, it may go to another coalition member, rendering it ineffective. Thus, the source must accept some monetary compensation, net of its normal flow payment, when cheating occurs. Since the source only contracts with the first hop, it must accept this money from the first hop. The source"s contract must therefore distinguish when the path quality is normal from when it is lowered by cheating. To do so, it can either accept proofs 191 from the source, that the quality is lower than required, or it can accept proofs from the first hop, that the quality is correct. These nodes will not rationally offer the opposing type of proof.
By definition, any monitor that gives the source proof that the path quality is wrong is an ROPc monitor. Any monitor that gives the first hop proof that the quality is correct is a ROPc" monitor. Thus, at least one of these monitors must exist.
By the protect-the-innocent assumption, if cheating occurs, but the first hop is not a cheater, she must be able to claim the same size reward from the next ISP on the path, and thus pass on the punishment. The first hop"s contract with the second must then distinguish when cheating occurs after the first hop. By argument similar to that for the source, either the first hop has a ROPc monitor, or the second has a ROPc" monitor. This argument can be iterated along the entire path to the penultimate node before D.
Since the marginal costs and qualities can be arranged to make any path the optimal path, these statements must hold for all nodes and their children, which completes the proof.
The two possibilities for monitor correspond to which node has the burden of proof. In one case, the prior node must prove the suboptimal quality to claim its reward. In the other, the subsequent node must prove that the quality was correct to avoid penalty.
Because the two monitors are similar, it seems likely that they require comparable costs to implement. If submitting the proofs is costly, it seems natural that nodes would prefer to use the ROPc monitor, placing the burden of proof on the upstream node.
Finally, we note that it is straightforward to derive the full version of the claim, which allows for multiple cheaters. The only complication is that cheaters can exchange side payments, which makes any money transfers between them redundant. Because of this, we have to further generalize our rest of path monitors, so they are less constrained in the case that there are cheaters on either side.
Claim 5 should not be interpreted as a statement that each node must compute the rest of path quality locally, without input from other nodes. Other monitors, besides ROPc and ROPc" can still be used, loosely speaking, as building blocks. For instance, network tomography is concerned with measuring properties of the network interior with tools located at the edge. Using such techniques, our source might learn both individual node qualities and the data path.
This is represented by the following two monitors: SHOPc i : (source-based hop quality) A monitor that gives the source proof of what the quality of node i is.
SPATHc: (source-based path) A monitor that gives the source proof of what the data path is at any time, at least as far as it matches the equilibrium path.
With these monitors, a punishment mechanism can be designed to fulfill the conditions of Claim 5. It involves the source sharing the proofs it generates with nodes further down the path, which use them to determine bilateral payments. Ultimately however, the proof of Claim 5 shows us that each node i"s bilateral contracts require proof of the rest of path quality. This means that node i (or possibly its children) will have to combine the proofs that they receive to generate a proof of the rest of path quality. Thus, the combined process is itself a rest of path monitor.
What we have done, all in all, is constructed a rest of path monitor using SPATHc and SHOPc i as building blocks. Our new monitor includes both the component monitors and whatever distributed algorithmic mechanism exists to make sure nodes share their proofs correctly.
This mechanism can potentially involve external institutions. For a concrete example, suppose that when node i suspects it is getting poor rest of path quality from its successor, it takes the downstream node to court. During the discovery process, the court subpoenas proofs of the path and of node qualities from the source (ultimately, there must be some threat to ensure the source complies). Finally, for the court to issue a judgment, one party or the other must compile a proof of what the rest of path quality was. Hence, the entire discovery process acts as a rest of path monitor, albeit a rather costly monitor in this case.
Of course, mechanisms can be designed to combine these monitors at much lower cost. Typically, such mechanisms would call for automatic sharing of proofs, with court intervention only as a last resort. We defer these interesting mechanisms to future work.
As an aside, intuition might dictate that SHOPc i generates more information than ROPc; after all, inferring individual node qualities seems a much harder problem. Yet, without path information,
SHOPc i is not sufficient for our first-best innovation result. The proof of this demonstrates a useful technique: Claim 6. With monitors E2E, ROP, SHOPc i and PRc, and a nowhere-commoditized bilateral contract competition game, the optimal quality cannot be maintained for all assignments of quality and marginal cost, in fixed-route coalition-proof protect-theinnocent equilibrium.
Proof: Because nodes cannot verify the data path, they cannot form a proof of what the rest of path quality is. Hence, ROPc monitors do not exist, and therefore the requirements of Claim 5 cannot hold.
It is our hope that this study will have a positive impact in at least three different ways. The first is practical: we believe our analysis has implications for the design of future monitoring protocols and for public policy.
For protocol designers, we first provide fresh motivation to create monitoring systems. We have argued that the poor accountability of the Internet is a fundamental obstacle to alleviating the pathologies of commoditization and lack of innovation. Unless accountability improves, these pathologies are guaranteed to remain.
Secondly, we suggest directions for future advances in monitoring.
We have shown that adding verifiability to monitors allows for some improvements in the characteristics of competition. At the same time, this does not present a fully satisfying solution. This paper has suggested a novel standard for monitors to aspire to - one of supporting optimal routes in innovative competition games under fixed-route coalition-proof protect-the-innocent equilibrium. We have shown that under bilateral contracts, this specifically requires contractible rest of path monitors.
This is not to say that other types of monitors are unimportant. We included an example in which individual hop quality monitors and a path monitor can also meet our standard for sustaining competition.
However, in order for this to happen, a mechanism must be included 192 to combine proofs from these monitors to form a proof of rest of path quality. In other words, the monitors must ultimately be combined to form contractible rest of path monitors. To support service differentiation and innovation, it may be easier to design rest of path monitors directly, thereby avoiding the task of designing mechanisms for combining component monitors.
As far as policy implications, our analysis points to the need for legal institutions to enforce contracts based on quality. These institutions must be equipped to verify proofs of quality, and police illegal contracting behavior. As quality-based contracts become numerous and complicated, and possibly negotiated by machine, this may become a challenging task, and new standards and regulations may have to emerge in response. This remains an interesting and unexplored area for research.
The second area we hope our study will benefit is that of clean-slate architectural design. Traditionally, clean-slate design tends to focus on creating effective and elegant networks for a static set of requirements. Thus, the approach is often one of engineering, which tends to neglect competitive effects. We agree with Ratnasamy, Shenker, and McCanne, that designing for evolution should be a top priority [11]. We have demonstrated that the network"s monitoring ability is critical to supporting innovation, as are the institutions that support contracting. These elements should feature prominently in new designs. Our analysis specifically suggests that architectures based on bilateral contracts should include contractible rest of path monitoring. From a clean-slate perspective, these monitors can be transparently and fully integrated with the routing and contracting systems.
Finally, the last contribution our study makes is methodological.
We believe that the mathematical formalization we present is applicable to a variety of future research questions. While a significant literature addresses innovation in the presence of network effects, to the best of our knowledge, ours is the first model of innovation in a network industry that successfully incorporates the actual topological structure as input. This allows the discovery of new properties, such as the weakening of market forces with the number of ISPs on a data path that we observe with lowaccountability.
Our method also stands in contrast to the typical approach of distributed algorithmic mechanism design. Because this field is based on a principle-agent framework, contracts are usually proposed by the source, who is allowed to make a take it or leave it offer to network nodes. Our technique allows contracts to emerge from a competitive framework, so the source is limited to selecting the most desirable contract. We believe this is a closer reflection of the industry.
Based on the insights in this study, the possible directions for future research are numerous and exciting. To some degree, contracting based on quality opens a Pandora"s Box of pressing questions: Do quality-based contracts stand counter to the principle of network neutrality? Should ISPs be allowed to offer a choice of contracts at different quality levels? What anti-competitive behaviors are enabled by quality-based contracts? Can a contracting system support optimal multicast trees?
In this study, we have focused on bilateral contracts. This system has seemed natural, especially since it is the prevalent system on the current network. Perhaps its most important benefit is that each contract is local in nature, so both parties share a common, familiar legal jurisdiction. There is no need to worry about who will enforce a punishment against another ISP on the opposite side of the planet, nor is there a dispute over whose legal rules to apply in interpreting a contract.
Although this benefit is compelling, it is worth considering other systems. The clearest alternative is to form a contract between the source and every node on the path. We may call these source contracts. Source contracting may present surprising advantages.
For instance, since ISPs do not exchange money with each other, an ISP cannot save money by selecting a cheaper next hop.
Additionally, if the source only has contracts with nodes on the intended path, other nodes won"t even be willing to accept packets from this source since they won"t receive compensation for carrying them. This combination seems to eliminate all temptation for a single cheater to cheat in route. Because of this and other encouraging features, we believe source contracts are a fertile topic for further study.
Another important research task is to relax our assumption that quality can be measured fully and precisely. One possibility is to assume that monitoring is only probabilistic or suffers from noise.
Even more relevant is the possibility that quality monitors are fundamentally incomplete. A quality monitor can never anticipate every dimension of quality that future applications will care about, nor can it anticipate a new and valuable protocol that an ISP introduces. We may define a monitor space as a subspace of the quality space that a monitor can measure, QM ⊂ , and a corresponding monitoring function that simply projects the full range of qualities onto the monitor space, MQm →: .
Clearly, innovations that leave quality invariant under m are not easy to support - they are invisible to the monitoring system. In this environment, we expect that path monitoring becomes more important, since it is the only way to ensure data reaches certain innovator ISPs. Further research is needed to understand this process.
We would like to thank the anonymous reviewers, Jens Grossklags,
Moshe Babaioff, Scott Shenker, Sylvia Ratnasamy, and Hal Varian for their comments. This work is supported in part by the National Science Foundation under ITR award ANI-0331659.
[1] Afergan, M. Using Repeated Games to Design IncentiveBased Routing Systems. In Proceedings of IEEE INFOCOM (April 2006). [2] Afergan, M. and Wroclawski, J. On the Benefits and Feasibility of Incentive Based Routing Infrastructure. In ACM SIGCOMM'04 Workshop on Practice and Theory of Incentives in Networked Systems (PINS) (August 2004). [3] Argyraki, K., Maniatis, P., Cheriton, D., and Shenker, S.
Providing Packet Obituaries. In Third Workshop on Hot Topics in Networks (HotNets) (November 2004). [4] Clark, D. D. The Design Philosophy of the DARPA Internet Protocols. In Proceedings of ACM SIGCOMM (1988). 193 [5] Clark, D. D., Wroclawski, J., Sollins, K. R., and Braden, R.
Tussle in cyberspace: Defining tomorrow's internet. In Proceedings of ACM SIGCOMM (August 2002). [6] Dang-Nguyen, G. and Pénard, T. Interconnection Agreements: Strategic Behaviour and Property Rights. In Brousseau, E. and Glachant, J.M. Eds. The Economics of Contracts: Theories and Applications, Cambridge University Press, 2002. [7] Feigenbaum, J., Papadimitriou, C., Sami, R., and Shenker, S.
A BGP-based Mechanism for Lowest-Cost Routing.
Distributed Computing 18 (2005), pp. 61-72. [8] Huston, G. Interconnection, Peering, and Settlements. Telstra,
Australia. [9] Liu, Y., Zhang, H., Gong, W., and Towsley, D. On the Interaction Between Overlay Routing and Traffic Engineering.
In Proceedings of IEEE INFOCOM (2005). [10] MacKie-Mason, J. and Varian, H. Pricing the Internet. In Kahin, B. and Keller, J. Eds. Public access to the Internet.
Englewood Cliffs, NJ; Prentice-Hall, 1995. [11] Ratnasamy, S., Shenker, S., and McCanne, S. Towards an Evolvable Internet Architecture. In Proceeding of ACM SIGCOMM (2005). [12] Shakkottai, S., and Srikant, R. Economics of Network Pricing with Multiple ISPs. In Proceedings of IEEE INFOCOM (2005).
Proof of Claim 2. Node i must fall on the equilibrium data path to receive any payment. Let the prices along the data path be ppppP nS == ,..., 21 , with marginal costs, ncc ,...,1 . We may assume the prices on the path are greater than lp or the claim follows trivially. Each node along the data path can cheat in route by giving data to the bargain path at price no more than lp . So node j"s temptation to cheat is at least
− ≥ −− −− jj l jjj ljj pp pp pcp pcp . Then Lemma 1 gives, 1 13221
− − − −> − − ⋅⋅ − − − − ≥ n S l n lll P pp n pp pp pp pp pp pp T (7) This can be rearranged to give ( ) ( ) s n l P n T pp 1 1/1 − +≤ − , as required.
The rest of the claim simply recognizes that rp / is the greatest reward node i can receive for its investment, so it will not invest sums greater than this.
Proof of Claim 4. Label the nodes 1,2,.. N in the order in which they select contracts. Let subgame n be the game that begins with n choosing its contract. Let Ln be the set of possible paths restricted to nodes n,…,N. That is, Ln is the set of possible routes from S to reach some node that has already moved.
For subgame n, define the local welfare over paths nLl ∈ , and their possible next hops, nj < as follows, ( ) ( ) j li ipathjl pcqqujlV −−= ∈ *, , (8) where ql is the quality of path l in the set {n,…,N}, and pathjq and pj are the quality and price of the contract j has offered.
For induction, assume that subgame n + 1 maximizes local welfare.
We show that subgame n does as well. If node n selects next hop k, we can write the following relation, ( ) ( )( ) ( )( ) nknn knlVpcpknlVnlV π−=++−= ,,,,, , (9) where n is node n"s profit if the path to n is chosen. This path is chosen whenever ( )nlV , is maximal over Ln+1 and possible next hops. If ( )( )knlV ,, is maximal over Ln, it is also maximal over the paths in Ln+1 that don"t lead to n. This means that node n can choose some n small enough so that ( )nlV , is maximal over Ln+1, so the route will lead to k.
Conversely, if ( )( )knlV ,, is not maximal over Ln, either V is greater for another of n"s next hops, in which case n will select that one in order to increase n, or V is greater for some path in Ln+1 that don"t lead to n, in which case ( )nlV , cannot be maximal for any nonnegative n.
Thus, we conclude that subgame n maximizes local welfare. For the initial case, observe that this assumption holds for the source.
Finally, we deduce that subgame 1, which is the entire game, maximizes local welfare, which is equivalent to actual welfare.

The importance of countersniper systems is underscored by the constant stream of news reports coming from the Middle East. In October 2006 CNN reported on a new tactic employed by insurgents. A mobile sniper team moves around busy city streets in a car, positions itself at a good standoff distance from dismounted US military personnel, takes a single well-aimed shot and immediately melts in the city traffic. By the time the soldiers can react, they are gone. A countersniper system that provides almost immediate shooter location to every soldier in the vicinity would provide clear benefits to the warfigthers.
Our team introduced PinPtr, the first sensor networkbased countersniper system [17, 8] in 2003. The system is based on potentially hundreds of inexpensive sensor nodes deployed in the area of interest forming an ad-hoc multihop network. The acoustic sensors measure the Time of Arrival (ToA) of muzzle blasts and ballistic shockwaves, pressure waves induced by the supersonic projectile, send the data to a base station where a sensor fusion algorithm determines the origin of the shot. PinPtr is characterized by high precision: 1m average 3D accuracy for shots originating within or near the sensor network and 1 degree bearing precision for both azimuth and elevation and 10% accuracy in range estimation for longer range shots. The truly unique characteristic of the system is that it works in such reverberant environments as cluttered urban terrain and that it can resolve multiple simultaneous shots at the same time. This capability is due to the widely distributed sensing and the unique sensor fusion approach [8]. The system has been tested several times in US Army MOUT (Military Operations in Urban Terrain) facilities.
The obvious disadvantage of such a system is its static nature. Once the sensors are distributed, they cover a certain area. Depending on the operation, the deployment may be needed for an hour or a month, but eventually the area looses its importance. It is not practical to gather and reuse the sensors, especially under combat conditions. Even if the sensors are cheap, it is still a waste and a logistical problem to provide a continuous stream of sensors as the operations move from place to place. As it is primarily the soldiers that the system protects, a natural extension is to mount the sensors on the soldiers themselves. While there are vehiclemounted countersniper systems [1] available commercially, we are not aware of a deployed system that protects dismounted soldiers. A helmet-mounted system was developed in the mid 90s by BBN [3], but it was not continued beyond the Darpa program that funded it. 113 To move from a static sensor network-based solution to a highly mobile one presents significant challenges. The sensor positions and orientation need to be constantly monitored.
As soldiers may work in groups of as little as four people, the number of sensors measuring the acoustic phenomena may be an order of magnitude smaller than before.
Moreover, the system should be useful to even a single soldier.
Finally, additional requirements called for caliber estimation and weapon classification in addition to source localization.
The paper presents the design and evaluation of our soldierwearable mobile countersniper system. It describes the hardware and software architecture including the custom sensor board equipped with a small microphone array and connected to a COTS MICAz mote [12]. Special emphasis is paid to the sensor fusion technique that estimates the trajectory, range, caliber and weapon type simultaneously. The results and analysis of an independent evaluation of the system at the US Army Aberdeen Test Center are also presented.
The firing of a typical military rifle, such as the AK47 or M16, produces two distinct acoustic phenomena. The muzzle blast is generated at the muzzle of the gun and travels at the speed sound. The supersonic projectile generates an acoustic shockwave, a kind of sonic boom. The wavefront has a conical shape, the angle of which depends on the Mach number, the speed of the bullet relative to the speed of sound.
The shockwave has a characteristic shape resembling a capital N. The rise time at both the start and end of the signal is very fast, under 1 μsec. The length is determined by the caliber and the miss distance, the distance between the trajectory and the sensor. It is typically a few hundred μsec.
Once a trajectory estimate is available, the shockwave length can be used for caliber estimation.
Our system is based on four microphones connected to a sensorboard. The board detects shockwaves and muzzle blasts and measures their ToA. If at least three acoustic channels detect the same event, its AoA is also computed.
If both the shockwave and muzzle blast AoA are available, a simple analytical solution gives the shooter location as shown in Section 6. As the microphones are close to each other, typically 2-4, we cannot expect very high precision.
Also, this method does not estimate a trajectory. In fact, an infinite number of trajectory-bullet speed pairs satisfy the observations. However, the sensorboards are also connected to COTS MICAz motes and they share their AoA and ToA measurements, as well as their own location and orientation, with each other using a multihop routing service [9]. A hybrid sensor fusion algorithm then estimates the trajectory, the range, the caliber and the weapon type based on all available observations.
The sensorboard is also Bluetooth capable for communication with the soldier"s PDA or laptop computer. A wired USB connection is also available. The sensorfusion algorithm and the user interface get their data through one of these channels.
The orientation of the microphone array at the time of detection is provided by a 3-axis digital compass. Currently the system assumes that the soldier"s PDA is GPS-capable and it does not provide self localization service itself.
However, the accuracy of GPS is a few meters degrading the Figure 1: Acoustic sensorboard/mote assembly . overall accuracy of the system. Refer to Section 7 for an analysis. The latest generation sensorboard features a Texas Instruments CC-1000 radio enabling the high-precision radio interferometric self localization approach we have developed separately [7]. However, we leave the integration of the two technologies for future work.
Since the first static version of our system in 2003, the sensor nodes have been built upon the UC Berkeley/Crossbow MICA product line [11]. Although rudimentary acoustic signal processing can be done on these microcontroller-based boards, they do not provide the required computational performance for shockwave detection and angle of arrival measurements, where multiple signals from different microphones need to be processed in parallel at a high sampling rate. Our 3rd generation sensorboard is designed to be used with MICAz motes-in fact it has almost the same size as the mote itself (see Figure 1).
The board utilizes a powerful Xilinx XC3S1000 FPGA chip with various standard peripheral IP cores, multiple soft processor cores and custom logic for the acoustic detectors (Figure 2). The onboard Flash (4MB) and PSRAM (8MB) modules allow storing raw samples of several acoustic events, which can be used to build libraries of various acoustic signatures and for refining the detection cores off-line. Also, the external memory blocks can store program code and data used by the soft processor cores on the FPGA.
The board supports four independent analog channels sampled at up to 1 MS/s (million samples per seconds). These channels, featuring an electret microphone (Panasonic WM64PNT), amplifiers with controllable gain (30-60 dB) and a 12-bit serial ADC (Analog Devices AD7476), reside on separate tiny boards which are connected to the main sensorboard with ribbon cables. This partitioning enables the use of truly different audio channels (eg.: slower sampling frequency, different gain or dynamic range) and also results in less noisy measurements by avoiding long analog signal paths.
The sensor platform offers a rich set of interfaces and can be integrated with existing systems in diverse ways. An RS232 port and a Bluetooth (BlueGiga WT12) wireless link with virtual UART emulation are directly available on the board and provide simple means to connect the sensor to PCs and PDAs. The mote interface consists of an I2 C bus along with an interrupt and GPIO line (the latter one is used 114 Figure 2: Block diagram of the sensorboard. for precise time synchronization between the board and the mote). The motes are equipped with IEEE 802.15.4 compliant radio transceivers and support ad-hoc wireless networking among the nodes and to/from the base station. The sensorboard also supports full-speed USB transfers (with custom USB dongles) for uploading recorded audio samples to the PC. The on-board JTAG chain-directly accessible through a dedicated connector-contains the FPGA part and configuration memory and provides in-system programming and debugging facilities.
The integrated Honeywell HMR3300 digital compass module provides heading, pitch and roll information with 1◦ accuracy, which is essential for calculating and combining directional estimates of the detected events.
Due to the complex voltage requirements of the FPGA, the power supply circuitry is implemented on the sensorboard and provides power both locally and to the mote. We used a quad pack of rechargeable AA batteries as the power source (although any other configuration is viable that meets the voltage requirements). The FPGA core (1.2 V) and I/O (3.3 V) voltages are generated by a highly efficient buck switching regulator. The FPGA configuration (2.5 V) and a separate 3.3 V power net are fed by low current LDOs, the latter one is used to provide independent power to the mote and to the Bluetooth radio. The regulators-except the last one-can be turned on/off from the mote or through the Bluetooth radio (via GPIO lines) to save power.
The first prototype of our system employed 10 sensor nodes. Some of these nodes were mounted on military kevlar helmets with the microphones directly attached to the surface at about 20 cm separation as shown in Figure 3(a). The rest of the nodes were mounted in plastic enclosures (Figure 3(b)) with the microphones placed near the corners of the boxes to form approximately 5 cm×10 cm rectangles.
The sensor application relies on three subsystems exploiting three different computing paradigms as they are shown in Figure 4. Although each of these execution models suit their domain specific tasks extremely well, this diversity (a) (b) Figure 3: Sensor prototypes mounted on a kevlar helmet (a) and in a plastic box on a tripod (b). presents a challenge for software development and system integration. The sensor fusion and user interface subsystem is running on PDAs and were implemented in Java.
The sensing and signal processing tasks are executed by an FPGA, which also acts as a bridge between various wired and wireless communication channels. The ad-hoc internode communication, time synchronization and data sharing are the responsibilities of a microcontroller based radio module.
Similarly, the application employs a wide variety of communication protocols such as Bluetooth and IEEE 802.14.5 wireless links, as well as optional UARTs, I2 C and/or USB buses.
Soldier Operated Device (PDA/Laptop) FPGA Sensor Board Mica Radio Module
Radio Control Message Routing Acoustic Event Encoder Sensor Time Synch.
Network Time Synch.Remote Control Time stamping Interrupts Virtual Register Interface C O O R D I N A T O R A n a l o g c h a n n e l s Compass PicoBlaze Comm.
Interface PicoBlaze WT12 Bluetooth Radio MOTE IF:I2C,Interrupts USB PSRAM U A R T U A R T MB det SW det REC Bluetooth Link User Interface Sensor Fusion Location Engine GPS Message (Dis-)AssemblerSensor Control Figure 4: Software architecture diagram.
The sensor fusion module receives and unpacks raw measurements (time stamps and feature vectors) from the sensorboard through the Bluetooth link. Also, it fine tunes the execution of the signal processing cores by setting parameters through the same link. Note that measurements from other nodes along with their location and orientation information also arrive from the sensorboard which acts as a gateway between the PDA and the sensor network. The handheld device obtains its own GPS location data and di115 rectly receives orientation information through the sensorboard. The results of the sensor fusion are displayed on the PDA screen with low latency. Since, the application is implemented in pure Java, it is portable across different PDA platforms.
The border between software and hardware is considerably blurred on the sensor board. The IP cores-implemented in hardware description languages (HDL) on the reconfigurable FPGA fabric-closely resemble hardware building blocks. However, some of them-most notably the soft processor cores-execute true software programs. The primary tasks of the sensor board software are 1) acquiring data samples from the analog channels, 2) processing acoustic data (detection), and 3) providing access to the results and run-time parameters through different interfaces.
As it is shown in Figure 4, a centralized virtual register file contains the address decoding logic, the registers for storing parameter values and results and the point to point data buses to and from the peripherals. Thus, it effectively integrates the building blocks within the sensorboard and decouples the various communication interfaces. This architecture enabled us to deploy the same set of sensors in a centralized scenario, where the ad-hoc mote network (using the I2 C interface) collected and forwarded the results to a base station or to build a decentralized system where the local PDAs execute the sensor fusion on the data obtained through the Bluetooth interface (and optionally from other sensors through the mote interface). The same set of registers are also accessible through a UART link with a terminal emulation program. Also, because the low-level interfaces are hidden by the register file, one can easily add/replace these with new ones (eg.: the first generation of motes supported a standard μP interface bus on the sensor connector, which was dropped in later designs).
The most important results are the time stamps of the detected events. These time stamps and all other timing information (parameters, acoustic event features) are based on a 1 MHz clock and an internal timer on the FPGA. The time conversion and synchronization between the sensor network and the board is done by the mote by periodically requesting the capture of the current timer value through a dedicated GPIO line and reading the captured value from the register file through the I2 C interface. Based on the the current and previous readings and the corresponding mote local time stamps, the mote can calculate and maintain the scaling factor and offset between the two time domains.
The mote interface is implemented by the I2 C slave IP core and a thin adaptation layer which provides a data and address bus abstraction on top of it. The maximum effective bandwidth is 100 Kbps through this interface. The FPGA contains several UART cores as well: for communicating with the on-board Bluetooth module, for controlling the digital compass and for providing a wired RS232 link through a dedicated connector. The control, status and data registers of the UART modules are available through the register file. The higher level protocols on these lines are implemented by Xilinx PicoBlaze microcontroller cores [13] and corresponding software programs. One of them provides a command line interface for test and debug purposes, while the other is responsible for parsing compass readings. By default, they are connected to the RS232 port and to the on-board digital compass line respectively, however, they can be rewired to any communication interface by changing the register file base address in the programs (e.g. the command line interface can be provided through the Bluetooth channel).
Two of the external interfaces are not accessible through the register file: a high speed USB link and the SRAM interface are tied to the recorder block. The USB module implements a simple FIFO with parallel data lines connected to an external FT245R USB device controller. The RAM driver implements data read/write cycles with correct timing and is connected to the on-board pseudo SRAM. These interfaces provide 1 MB/s effective bandwidth for downloading recorded audio samples, for example.
The data acquisition and signal processing paths exhibit clear symmetry: the same set of IP cores are instantiated four times (i.e. the number of acoustic channels) and run independently. The signal paths meet only just before the register file. Each of the analog channels is driven by a serial A/D core for providing a 20 MHz serial clock and shifting in 8-bit data samples at 1 MS/s and a digital potentiometer driver for setting the required gain. Each channel has its own shockwave and muzzle blast detector, which are described in Section 5. The detectors fetch run-time parameter values from the register file and store their results there as well. The coordinator core constantly monitors the detection results and generates a mote interrupt promptly upon full detection or after a reasonable timeout after partial detection.
The recorder component is not used in the final deployment, however, it is essential for development purposes for refining parameter values for new types of weapons or for other acoustic sources. This component receives the samples from all channels and stores them in circular buffers in the PSRAM device. If the signal amplitude on one of the channels crosses a predefined threshold, the recorder component suspends the sample collection with a predefined delay and dumps the contents of the buffers through the USB link.
The length of these buffers and delays, the sampling rate, the threshold level and the set of recorded channels can be (re)configured run-time through the register file. Note that the core operates independently from the other signal processing modules, therefore, it can be used to validate the detection results off-line.
The FPGA cores are implemented in VHDL, the PicoBlaze programs are written in assembly. The complete configuration occupies 40% of the resources (slices) of the FPGA and the maximum clock speed is 30 MHz, which is safely higher than the speed used with the actual device (20MHz).
The MICAz motes are responsible for distributing measurement data across the network, which drastically improves the localization and classification results at each node.
Besides a robust radio (MAC) layer, the motes require two essential middleware services to achieve this goal. The messages need to be propagated in the ad-hoc multihop network using a routing service. We successfully integrated the Directed Flood-Routing Framework (DFRF) [9] in our application. Apart from automatic message aggregation and efficient buffer management, the most unique feature of DFRF is its plug-in architecture, which accepts custom routing policies. Routing policies are state machines that govern how received messages are stored, resent or discarded.
Example policies include spanning tree routing, broadcast, geographic routing, etc. Different policies can be used for different messages concurrently, and the application is able to 116 change the underlying policies at run-time (eg.: because of the changing RF environment or power budget). In fact, we switched several times between a simple but lavish broadcast policy and a more efficient gradient routing on the field.
Correlating ToA measurements requires a common time base and precise time synchronization in the sensor network.
The Routing Integrated Time Synchronization (RITS) [15] protocol relies on very accurate MAC-layer time-stamping to embed the cumulative delay that a data message accrued since the time of the detection in the message itself. That is, at every node it measures the time the message spent there and adds this to the number in the time delay slot of the message, right before it leaves the current node. Every receiving node can subtract the delay from its current time to obtain the detection time in its local time reference. The service provides very accurate time conversion (few μs per hop error), which is more than adequate for this application.
Note, that the motes also need to convert the sensorboard time stamps to mote time as it is described earlier.
The mote application is implemented in nesC [5] and is running on top of TinyOS [6]. With its 3 KB RAM and
the MICAz motes.
There are several characteristics of acoustic shockwaves and muzzle blasts which distinguish their detection and signal processing algorithms from regular audio applications.
Both events are transient by their nature and present very intense stimuli to the microphones. This is increasingly problematic with low cost electret microphones-designed for picking up regular speech or music. Although mechanical damping of the microphone membranes can mitigate the problem, this approach is not without side effects. The detection algorithms have to be robust enough to handle severe nonlinear distortion and transitory oscillations. Since the muzzle blast signature closely follows the shockwave signal and because of potential automatic weapon bursts, it is extremely important to settle the audio channels and the detection logic as soon as possible after an event. Also, precise angle of arrival estimation necessitates high sampling frequency (in the MHz range) and accurate event detection.
Moreover, the detection logic needs to process multiple channels in parallel (4 channels on our existing hardware).
These requirements dictated simple and robust algorithms both for muzzle blast and shockwave detections. Instead of using mundane energy detectors-which might not be able to distinguish the two different events-the applied detectors strive to find the most important characteristics of the two signals in the time-domain using simple state machine logic. The detectors are implemented as independent IP cores within the FPGA-one pair for each channel. The cores are run-time configurable and provide detection event signals with high precision time stamps and event specific feature vectors. Although the cores are running independently and in parallel, a crude local fusion module integrates them by shutting down those cores which missed their events after a reasonable timeout and by generating a single detection message towards the mote. At this point, the mote can read and forward the detection times and features and is responsible to restart the cores afterwards.
The most conspicuous characteristics of an acoustic shockwave (see Figure 5(a)) are the steep rising edges at the be0 200 400 600 800 1000 1200 1400 1600 -1 -0.8 -0.6 -0.4 -0.2 0
1 Shockwave (M16) Time (µs) Amplitude 1 3 5 2 4 len (a) s[t] - s[t-D] > E tstart := t s[t] - s[t-D] < E s[t] - s[t-D] > E & t - t_start > Lmin s[t] - s[t-D] < E len := t - tstart IDLE 1 FIRST EDGE DONE 3 SECOND EDGE 4 FIRST EDGE 2 FOUND 5 t - tstart ≥ Lmax t - tstart ≥ Lmax (b) Figure 5: Shockwave signal generated by a 5.56 ×
of the detection algorithm (b). ginning and end of the signal. Also, the length of the N-wave is fairly predictable-as it is described in Section 6.5-and is relatively short (200-300 μs). The shockwave detection core is continuously looking for two rising edges within a given interval. The state machine of the algorithm is shown in Figure 5(b). The input parameters are the minimum steepness of the edges (D, E), and the bounds on the length of the wave (Lmin, Lmax). The only feature calculated by the core is the length of the observed shockwave signal.
In contrast to shockwaves, the muzzle blast signatures are characterized by a long initial period (1-5 ms) where the first half period is significantly shorter than the second half [4].
Due to the physical limitations of the analog circuitry described at the beginning of this section, irregular oscillations and glitches might show up within this longer time window as they can be clearly seen in Figure 6(a). Therefore, the real challenge for the matching detection core is to identify the first and second half periods properly. The state machine (Figure 6(b)) does not work on the raw samples directly but is fed by a zero crossing (ZC) encoder. After the initial triggering, the detector attempts to collect those ZC segments which belong to the first period (positive amplitude) while discarding too short (in our terminology: garbage) segments-effectively implementing a rudimentary low-pass filter in the ZC domain. After it encounters a sufficiently long negative segment, it runs the same collection logic for the second half period. If too much garbage is discarded in the collection phases, the core resets itself to prevent the (false) detection of the halves from completely different periods separated by rapid oscillation or noise. Finally, if the constraints on the total length and on the length ratio hold, the core generates a detection event along with the actual length, amplitude and energy of the period calculated concurrently. The initial triggering mechanism is based on two amplitude thresholds: one static (but configurable) amplitude level and a dynamically computed one. The latter one is essential to adapt the sensor to different ambient noise environments and to temporarily suspend the muzzle blast detector after a shock wave event (oscillations in the analog section or reverberations in the sensor enclosure might otherwise trigger false muzzle blast detections). The dynamic noise level is estimated by a single pole recursive low-pass filter (cutoff @ 0.5 kHz ) on the FPGA. 117
-1 -0.8 -0.6 -0.4 -0.2 0
1 Time (µs) Amplitude Muzzle blast (M16) 1 2 3
len2 + len1 (a) IDLE 1 SECOND ZC 3 PENDING ZC 4 FIRST ZC 2 FOUND 5 amplitude threshold long positive ZC long negative ZC valid full period max garbage wrong sign garbage collect first period garbage collect first period garbage (b) Figure 6: Muzzle blast signature (a) produced by an M16 assault rifle and the corresponding detection logic (b).
The detection cores were originally implemented in Java and evaluated on pre-recorded signals because of much faster test runs and more convenient debugging facilities. Later on, they were ported to VHDL and synthesized using the Xilinx ISE tool suite. The functional equivalence between the two implementations were tested by VHDL test benches and Python scripts which provided an automated way to exercise the detection cores on the same set of pre-recorded signals and to compare the results.
The sensor fusion algorithm receives detection messages from the sensor network and estimates the bullet trajectory, the shooter position, the caliber of the projectile and the type of the weapon. The algorithm consists of well separated computational tasks outlined below:
arrivals for each individual sensor (see 6.1).
analytically fuse a pair of shockwave and muzzle blast AoA estimates. (see 6.2).
measurements (see 6.3).
else compute shooter position first and then trajectory based on it. (see 6.4)
We describe each step in the following sections in detail.
The first step of the sensor fusion is to calculate the muzzle blast and shockwave AoA-s for each sensorboard. Each sensorboard has four microphones that measure the ToA-s.
Since the microphone spacing is orders of magnitude smaller than the distance to the sound source, we can approximate the approaching sound wave front with a plane (far field assumption).
Let us formalize the problem for 3 microphones first. Let P1, P2 and P3 be the position of the microphones ordered by time of arrival t1 < t2 < t3. First we apply a simple geometry validation step. The measured time difference between two microphones cannot be larger than the sound propagation time between the two microphones: |ti − tj| <= |Pi − Pj |/c + ε Where c is the speed of sound and ε is the maximum measurement error. If this condition does not hold, the corresponding detections are discarded. Let v(x, y, z) be the normal vector of the unknown direction of arrival. We also use r1(x1, y1, z1), the vector from P1 to P2 and r2(x2, y2, z2), the vector from P1 to P3. Let"s consider the projection of the direction of the motion of the wave front (v) to r1 divided by the speed of sound (c). This gives us how long it takes the wave front to propagate form P1 to P2: vr1 = c(t2 − t1) The same relationship holds for r2 and v: vr2 = c(t3 − t1) We also know that v is a normal vector: vv = 1 Moving from vectors to coordinates using the dot product definition leads to a quadratic system: xx1 + yy1 + zz1 = c(t2 − t1) xx2 + yy2 + zz2 = c(t3 − t1) x2 + y2 + z2 = 1 We omit the solution steps here, as they are straightforward, but long. There are two solutions (if the source is on the P1P2P3 plane the two solutions coincide). We use the fourth microphone"s measurement-if there is one-to eliminate one of them. Otherwise, both solutions are considered for further processing.
u v 11,tP 22,tP tP, 2P′ Bullet trajectory Figure 7: Section plane of a shot (at P) and two sensors (at P1 and at P2). One sensor detects the muzzle blast"s, the other the shockwave"s time and direction of arrivals.
Consider the situation in Figure 7. A shot was fired from P at time t. Both P and t are unknown. We have one muzzle blast and one shockwave detections by two different sensors 118 with AoA and hence, ToA information available. The muzzle blast detection is at position P1 with time t1 and AoA u. The shockwave detection is at P2 with time t2 and AoA v. u and v are normal vectors. It is shown below that these measurements are sufficient to compute the position of the shooter (P).
Let P2 be the point on the extended shockwave cone surface where PP2 is perpendicular to the surface. Note that PP2 is parallel with v. Since P2 is on the cone surface which hits P2, a sensor at P2 would detect the same shockwave time of arrival (t2). The cone surface travels at the speed of sound (c), so we can express P using P2: P = P2 + cv(t2 − t).
P can also be expressed from P1: P = P1 + cu(t1 − t) yielding P1 + cu(t1 − t) = P2 + cv(t2 − t).
P2P2 is perpendicular to v: (P2 − P2)v = 0 yielding (P1 + cu(t1 − t) − cv(t2 − t) − P2)v = 0 containing only one unknown t. One obtains: t = (P1−P2)v c +uvt1−t2 uv−1 .
From here we can calculate the shoter position P.
Let"s consider the special single sensor case where P1 = P2 (one sensor detects both shockwave and muzzle blast AoA).
In this case: t = uvt1−t2 uv−1 .
Since u and v are not used separately only uv, the absolute orientation of the sensor can be arbitrary, we still get t which gives us the range.
Here we assumed that the shockwave is a cone which is only true for constant projectile speeds. In reality, the angle of the cone slowly grows; the surface resembles one half of an American football. The decelerating bullet results in a smaller time difference between the shockwave and the muzzle blast detections because the shockwave generation slows down with the bullet. A smaller time difference results in a smaller range, so the above formula underestimates the true range. However, it can still be used with a proper deceleration correction function. We leave this for future work.
Danicki showed that the bullet trajectory and speed can be computed analytically from two independent shockwave measurements where both ToA and AoA are measured [2].
The method gets more sensitive to measurement errors as the two shockwave directions get closer to each other. In the special case when both directions are the same, the trajectory cannot be computed. In a real world application, the sensors are typically deployed on a plane approximately.
In this case, all sensors located on one side of the trajectory measure almost the same shockwave AoA. To avoid this error sensitivity problem, we consider shockwave measurement pairs only if the direction of arrival difference is larger than a certain threshold.
We have multiple sensors and one sensor can report two different directions (when only three microphones detect the shockwave). Hence, we typically have several trajectory candidates, i.e. one for each AoA pair over the threshold. We applied an outlier filtering and averaging method to fuse together the shockwave direction and time information and come up with a single trajectory. Assume that we have N individual shockwave AoA measurements. Let"s take all possible unordered pairs where the direction difference is above the mentioned threshold and compute the trajectory for each. This gives us at most N(N−1) 2 trajectories. A trajectory is represented by one point pi and the normal vector vi (where i is the trajectory index). We define the distance of two trajectories as the dot product of their normal vectors: D(i, j) = vivj For each trajectory a neighbor set is defined: N(i) := {j|D(i, j) < R} where R is a radius parameter. The largest neighbor set is considered to be the core set C, all other trajectories are outliers. The core set can be found in O(N2 ) time. The trajectories in the core set are then averaged to get the final trajectory.
It can happen that we cannot form any sensor pairs because of the direction difference threshold. It means all sensors are on the same side of the trajectory. In this case, we first compute the shooter position (described in the next section) that fixes p making v the only unknown. To find v in this case, we use a simple high resolution grid search and minimize an error function based on the shockwave directions.
We have made experiments to utilize the measured shockwave length in the trajectory estimation. There are some promising results, but it needs further research.
The shooter position estimation algorithm aggregates the following heterogenous information generated by earlier computational steps:
bearing estimate to the shooter, and
muzzle blast AoA are available).
Some sensors report only ToA, some has bearing estimate(s) also and some has range estimate(s) as well, depending on the number of successful muzzle blast and shockwave detections by the sensor. For an example, refer to Figure 8.
Note that a sensor may have two different bearing and range estimates. 3 detections gives two possible AoA-s for muzzle blast (i.e. bearing) and/or shockwave. Furthermore, the combination of two different muzzle blast and shockwave AoA-s may result in two different ranges. 119
4t 5t 6t bullet trajectory shooter position Figure 8: Example of heterogenous input data for the shooter position estimation algorithm. All sensors have ToA measurements (t1, t2, t3, t4, t5), one sensor has a single bearing estimate (v2), one sensor has two possible bearings (v3, v3) and one sensor has two bearing and two range estimates (v1, v1,r1, r1) In a multipath environment, these detections will not only contain gaussian noise, but also possibly large errors due to echoes. It has been showed in our earlier work that a similar problem can be solved efficiently with an interval arithmetic based bisection search algorithm [8]. The basic idea is to define a discrete consistency function over the area of interest and subdivide the space into 3D boxes. For any given 3D box, this function gives the number of measurements supporting the hypothesis that the shooter was within that box. The search starts with a box large enough to contain the whole area of interest, then zooms in by dividing and evaluating boxes. The box with the maximum consistency is divided until the desired precision is reached.
Backtracking is possible to avoid getting stuck in a local maximum.
This approach has been shown to be fast enough for online processing. Note, however, that when the trajectory has already been calculated in previous steps, the search needs to be done only on the trajectory making it orders of magnitude faster.
Next let us describe how the consistency function is calculated in detail. Consider B, a three dimensional box, we would like to compute the consistency value of. First we consider only the ToA information. If one sensor has multiple ToA detections, we use the average of those times, so one sensor supplies at most one ToA estimate. For each ToA, we can calculate the corresponding time of the shot, since the origin is assumed to be in box B. Since it is a box and not a single point, this gives us an interval for the shot time. The maximum number of overlapping time intervals gives us the value of the consistency function for B. For a detailed description of the consistency function and search algorithm, refer to [8].
Here we extend the approach the following way. We modify the consistency function based on the bearing and range data from individual sensors. A bearing estimate supports B if the line segment starting from the sensor with the measured direction intersects the B box. A range supports B, if the sphere with the radius of the range and origin of the sensor intersects B. Instead of simply checking whether the position specified by the corresponding bearing-range pairs falls within B, this eliminates the sensor"s possible orientation error. The value of the consistency function is incremented by one for each bearing and range estimate that is consistent with B.
The shockwave signal characteristics has been studied before by Whitham [20]. He showed that the shockwave period T is related to the projectile diameter d, the length l, the perpendicular miss distance b from the bullet trajectory to the sensor, the Mach number M and the speed of sound c.
T = 1.82Mb1/4 c(M2−1)3/8 d l1/4 ≈ 1.82d c (Mb l )1/4 0 100 200 300 400 500 600
miss distance (m)shockwavelength(microseconds) .50 cal
Figure 9: Shockwave length and miss distance relationship. Each data point represents one sensorboard after an aggregation of the individual measurements of the four acoustic channels. Three different caliber projectiles have been tested (196 shots, 10 sensors).
To illustrate the relationship between miss distance and shockwave length, here we use all 196 shots with three different caliber projectiles fired during the evaluation. (During the evaluation we used data obtained previously using a few practice shots per weapon.) 10 sensors (4 microphones by sensor) measured the shockwave length. For each sensor, we considered the shockwave length estimation valid if at least three out of four microphones agreed on a value with at most 5 microsecond variance. This filtering leads to a 86% report rate per sensor and gets rid of large measurement errors. The experimental data is shown in Figure 9.
Whitham"s formula suggests that the shockwave length for a given caliber can be approximated with a power function of the miss distance (with a 1/4 exponent). Best fit functions on our data are: .50 cal: T = 237.75b0.2059
To evaluate a shot, we take the caliber whose approximation function results in the smallest RMS error of the filtered sensor readings. This method has less than 1% caliber estimation error when an accurate trajectory estimate is available. In other words, caliber estimation only works if enough shockwave detections are made by the system to compute a trajectory. 120
We analyzed all measured signal characteristics to find weapon specific information. Unfortunately, we concluded that the observed muzzle blast signature is not characteristic enough of the weapon for classification purposes. The reflections of the high energy muzzle blast from the environment have much higher impact on the muzzle blast signal shape than the weapon itself. Shooting the same weapon from different places caused larger differences on the recorded signal than shooting different weapons from the same place. 0 100 200 300 400 500 600 700 800 900
range (m) speed(m/s) AK-47 M240 Figure 10: AK47 and M240 bullet deceleration measurements. Both weapons have the same caliber.
Data is approximated using simple linear regression. 0 100 200 300 400 500 600 700 800 900 1000
range (m) speed(m/s) M16 M249 M4 Figure 11: M16, M249 and M4 bullet deceleration measurements. All weapons have the same caliber.
Data is approximated using simple linear regression.
However, the measured speed of the projectile and its caliber showed good correlation with the weapon type. This is because for a given weapon type and ammunition pair, the muzzle velocity is nearly constant. In Figures 10 and
measured bullet speed for different calibers and weapons.
In the supersonic speed range, the bullet deceleration can be approximated with a linear function. In case of the
be clearly separated (Figure 10). Unfortunately, this is not necessarily true for the 5.56 mm caliber. The M16 with its higher muzzle speed can still be well classified, but the M4 and M249 weapons seem practically undistinguishable (Figure 11). However, this may be partially due to the limited number of practice shots we were able to take before the actual testing began. More training data may reveal better separation between the two weapons since their published muzzle velocities do differ somewhat.
The system carries out weapon classification in the following manner. Once the trajectory is known, the speed can be calculated for each sensor based on the shockwave geometry.
To evaluate a shot, we choose the weapon type whose deceleration function results in the smallest RMS error of the estimated range-speed pairs for the estimated caliber class.
An independent evaluation of the system was carried out by a team from NIST at the US Army Aberdeen Test Center in April 2006 [19]. The experiment was setup on a shooting range with mock-up wooden buildings and walls for supporting elevated shooter positions and generating multipath effects. Figure 12 shows the user interface with an aerial photograph of the site. 10 sensor nodes were deployed on surveyed points in an approximately 30×30 m area. There were five fixed targets behind the sensor network. Several firing positions were located at each of the firing lines at 50, 100, 200 and 300 meters. These positions were known to the evaluators, but not to the operators of the system.
Six different weapons were utilized: AK47 and M240 firing 7.62 mm projectiles, M16, M4 and M249 with 5.56mm ammunition and the .50 caliber M107.
Note that the sensors remained static during the test. The primary reason for this is that nobody is allowed downrange during live fire tests. Utilizing some kind of remote control platform would have been too involved for the limited time the range was available for the test. The experiment, therefore, did not test the mobility aspect of the system.
During the one day test, there were 196 shots fired. The results are summarized in Table 1. The system detected all shots successfully. Since a ballistic shockwave is a unique acoustic phenomenon, it makes the detection very robust.
There were no false positives for shockwaves, but there were a handful of false muzzle blast detections due to parallel tests of artillery at a nearby range.
Shooter Local- Caliber Trajectory Trajectory Distance No.
Range ization Accu- Azimuth Distance Error of (m) Rate racy Error (deg) Error (m) (m) Shots
All 96% 99.5% 0.88 2.47 23.0 196 Table 1: Summary of results fusing all available sensor observations. All shots were successfully detected, so the detection rate is omitted. Localization rate means the percentage of shots that the sensor fusion was able to estimate the trajectory of. The caliber accuracy rate is relative to the shots localized and not all the shots because caliber estimation requires the trajectory. The trajectory error is broken down to azimuth in degrees and the actual distance of the shooter from the trajectory. The distance error shows the distance between the real shooter position and the estimated shooter position. As such, it includes the error caused by both the trajectory and that of the range estimation. Note that the traditional bearing and range measures are not good ones for a distributed system such as ours because of the lack of a single reference point. 121 Figure 12: The user interface of the system showing the experimental setup. The 10 sensor nodes are labeled by their ID and marked by dark circles.
The targets are black squares marked T-1 through T-5. The long white arrows point to the shooter position estimated by each sensor. Where it is missing, the corresponding sensor did not have enough detections to measure the AoA of either the muzzle blast, the shockwave or both. The thick black line and large circle indicate the estimated trajectory and the shooter position as estimated by fusing all available detections from the network. This shot from the 100-meter line at target T-3 was localized almost perfectly by the sensor network. The caliber and weapon were also identified correctly. 6 out of
Their bearing accuracy is within a degree, while the range is off by less than 10% in the worst case.
The localization rate characterizes the system"s ability to successfully estimate the trajectory of shots. Since caliber estimation and weapon classification relies on the trajectory, non-localized shots are not classified either. There were 7 shots out of 196 that were not localized. The reason for missed shots is the trajectory ambiguity problem that occurs when the projectile passes on one side of all the sensors. In this case, two significantly different trajectories can generate the same set of observations (see [8] and also Section 6.3).
Instead of estimating which one is more likely or displaying both possibilities, we decided not to provide a trajectory at all. It is better not to give an answer other than a shot alarm than misleading the soldier.
Localization accuracy is broken down to trajectory accuracy and range estimation precision. The angle of the estimated trajectory was better than 1 degree except for the
estimation as long as the projectile passes over the network, we suspect that the slightly worse angle precision for 300 m is due to the hurried shots we witnessed the soldiers took near the end of the day. This is also indicated by another datapoint: the estimated trajectory distance from the actual targets has an average error of 1.3 m for 300 m shots,
As the distance between the targets and the sensor network was fixed, this number should not show a 2× improvement just because the shooter is closer.
Since the angle of the trajectory itself does not characterize the overall error-there can be a translation alsoTable 1 also gives the distance of the shooter from the estimated trajectory. These indicate an error which is about 1-2% of the range. To put this into perspective, a trajectory estimate for a 100 m shot will very likely go through or very near the window the shooter is located at. Again, we believe that the disproportionally larger errors at 300 m are due to human errors in aiming. As the ground truth was obtained by knowing the precise location of the shooter and the target, any inaccuracy in the actual trajectory directly adds to the perceived error of the system.
We call the estimation of the shooter"s position on the calculated trajectory range estimation due to the lack of a better term. The range estimates are better than 5% accurate from 50 m and 10% for 100 m. However, this goes to 20% or worse for longer distances. We did not have a facility to test system before the evaluation for ranges beyond 100 m. During the evaluation, we ran into the problem of mistaking shockwave echoes for muzzle blasts. These echoes reached the sensors before the real muzzle blast for long range shots only, since the projectile travels 2-3× faster than the speed of sound, so the time between the shockwave (and its possible echo from nearby objects) and the muzzle blast increases with increasing ranges. This resulted in underestimating the range, since the system measured shorter times than the real ones. Since the evaluation we finetuned the muzzle blast detection algorithm to avoid this problem.
Distance M16 AK47 M240 M107 M4 M249 M4-M249 50m 100% 100% 100% 100% 11% 25% 94% 100m 100% 100% 100% 100% 22% 33% 100% 200m 100% 100% 100% 100% 50% 22% 100% 300m 67% 100% 83% 100% 33% 0% 57% All 96% 100% 97% 100% 23% 23% 93% Table 2: Weapon classification results. The percentages are relative to the number of shots localized and not all shots, as the classification algorithm needs to know the trajectory and the range. Note that the difference is small; there were 189 shots localized out of the total 196.
The caliber and weapon estimation accuracy rates are based on the 189 shots that were successfully localized. Note that there was a single shot that was falsely classified by the caliber estimator. The 73% overall weapon classification accuracy does not seem impressive. But if we break it down to the six different weapons tested, the picture changes dramatically as shown in Table 2. For four of the weapons (AK14, M16, M240 and M107), the classification rate is almost 100%. There were only two shots out of approximately
similar and they were mistaken for each other most of the time. One possible explanation is that we had only a limited number of test shots taken with these weapons right before the evaluation and used the wrong deceleration approximation function. Either this or a similar mistake was made 122 since if we simply used the opposite of the system"s answer where one of these weapons were indicated, the accuracy would have improved 3x. If we consider these two weapons a single weapon class, then the classification accuracy for it becomes 93%.
Note that the AK47 and M240 have the same caliber (7.62 mm), just as the M16, M4 and M249 do (5.56 mm).
That is, the system is able to differentiate between weapons of the same caliber. We are not aware of any system that classifies weapons this accurately.
As was shown previously, a single sensor alone is able to localize the shooter if it can determine both the muzzle blast and the shockwave AoA, that is, it needs to measure the ToA of both on at least three acoustic channels. While shockwave detection is independent of the range-unless the projectile becomes subsonic-, the likelihood of muzzle blast detection beyond 150 meters is not enough for consistently getting at least three per sensor node for AoA estimation.
Hence, we only evaluate the single sensor performance for the 104 shots that were taken from 50 and 100 m. Note that we use the same test data as in the previous section, but we evaluate individually for each sensor.
Table 3 summarizes the results broken down by the ten sensors utilized. Since this is now not a distributed system, the results are given relative to the position of the given sensor, that is, a bearing and range estimate is provided. Note that many of the common error sources of the networked system do not play a role here. Time synchronization is not applicable. The sensor"s absolute location is irrelevant (just as the relative location of multiple sensors). The sensor"s orientation is still important though. There are several disadvantages of the single sensor case compared to the networked system: there is no redundancy to compensate for other errors and to perform outlier rejection, the localization rate is markedly lower, and a single sensor alone is not able to estimate the caliber or classify the weapon.
Sensor id 1 2 3 5 7 8 9 10 11 12 Loc. rate 44% 37% 53% 52% 19% 63% 51% 31% 23% 44% Bearing (deg) 0.80 1.25 0.60 0.85 1.02 0.92 0.73 0.71 1.28 1.44 Range (m) 3.2 6.1 4.4 4.7 4.6 4.6 4.1 5.2 4.8 8.2 Table 3: Single sensor accuracy for 108 shots fired from 50 and 100 meters. Localization rate refers to the percentage of shots the given sensor alone was able to localize. The bearing and range values are average errors. They characterize the accuracy of localization from the given sensor"s perspective.
The data indicates that the performance of the sensors varied significantly especially considering the localization rate. One factor has to be the location of the given sensor including how far it was from the firing lines and how obstructed its view was. Also, the sensors were hand-built prototypes utilizing nowhere near production quality packaging/mounting. In light of these factors, the overall average bearing error of 0.9 degrees and range error of 5 m with a microphone spacing of less than 10 cm are excellent.
We believe that professional manufacturing and better microphones could easily achieve better performance than the best sensor in our experiment (>60% localization rate and
Interestingly, the largest error in range was a huge 90 m clearly due to some erroneous detection, yet the largest bearing error was less than 12 degrees which is still a good indication for the soldier where to look.
The overall localization rate over all single sensors was 42%, while for 50 m shots only, this jumped to 61%. Note that the firing range was prepared to simulate an urban area to some extent: there were a few single- and two-storey wooden structures built both in and around the sensor deployment area and the firing lines. Hence, not all sensors had line-of-sight to all shooting positions. We estimate that 10% of the sensors had obstructed view to the shooter on average. Hence, we can claim that a given sensor had about 50% chance of localizing a shot within 130 m. (Since the sensor deployment area was 30 m deep, 100 m shots correspond to actual distances between 100 and 130 m.) Again, we emphasize that localization needs at least three muzzle blast and three shockwave detections out of a possible four for each per sensor. The detection rate for single sensors-corresponding to at least one shockwave detection per sensor-was practically 100%. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
number of sensors percentageofshots Figure 13: Histogram showing what fraction of the
localized by at most how many individual sensors alone. 13% of the shots were missed by every single sensor, i.e., none of them had both muzzle blast and shockwave AoA detections. Note that almost all of these shots were still accurately localized by the networked system, i.e. the sensor fusion using all available observations in the sensor network.
It would be misleading to interpret these results as the system missing half the shots. As soldiers never work alone and the sensor node is relatively cheap to afford having every soldier equipped with one, we also need to look at the overall detection rates for every shot. Figure 13 shows the histogram of the percentage of shots vs. the number of individual sensors that localized it. 13% of shots were not localized by any sensor alone, but 87% was localized by at least one sensor out of ten.
In this section, we analyze the most significant sources of error that affect the performance of the networked shooter localization and weapon classification system. In order to correlate the distributed observations of the acoustic events, the nodes need to have a common time and space reference.
Hence, errors in the time synchronization, node localization and node orientation all degrade the overall accuracy of the system. 123 Our time synchronization approach yields errors significantly less than 100 microseconds. As the sound travels about 3 cm in that time, time synchronization errors have a negligible effect on the system.
On the other hand, node location and orientation can have a direct effect on the overall system performance. Notice that to analyze this, we do not have to resort to simulation, instead we can utilize the real test data gathered at Aberdeen. But instead of using the real sensor locations known very accurately and the measured and calibrated almost perfect node orientations, we can add error terms to them and run the sensor fusion. This exactly replicates how the system would have performed during the test using the imprecisely known locations and orientations.
Another aspect of the system performance that can be evaluated this way is the effect of the number of available sensors. Instead of using all ten sensors in the data fusion, we can pick any subset of the nodes to see how the accuracy degrades as we decrease the number of nodes.
The following experiment was carried out. The number of sensors were varied from 2 to 10 in increments of 2. Each run picked the sensors randomly using a uniform distribution. At each run each node was randomly moved to a new location within a circle around its true position with a radius determined by a zero-mean Gaussian distribution.
Finally, the node orientations were perturbed using a zeromean Gaussian distribution. Each combination of parameters were generated 100 times and utilized for all 196 shots.
The results are summarized in Figure 14. There is one 3D barchart for each of the experiment sets with the given fixed number of sensors. The x-axis shows the node location error, that is, the standard deviation of the corresponding Gaussian distribution that was varied between 0 and 6 meters.
The y-axis shows the standard deviation of the node orientation error that was varied between 0 and 6 degrees. The z-axis is the resulting trajectory azimuth error. Note that the elevation angles showed somewhat larger errors than the azimuth. Since all the sensors were in approximately a horizontal plane and only a few shooter positions were out of the same plane and only by 2 m or so, the test was not sufficient to evaluate this aspect of the system.
There are many interesting observation one can make by analyzing these charts. Node location errors in this range have a small effect on accuracy. Node orientation errors, on the other hand, noticeably degrade the performance. Still the largest errors in this experiment of 3.5 degrees for 6 sensors and 5 degrees for 2 sensors are still very good.
Note that as the location and orientation errors increase and the number of sensors decrease, the most significantly affected performance metric is the localization rate. See Table 4 for a summary. Successful localization goes down from almost 100% to 50% when we go from 10 sensors to
by geometry: for a successful localization, the bullet needs to pass over the sensor network, that is, at least one sensor should be on the side of the trajectory other than the rest of the nodes. (This is a simplification for illustrative purposes. If all the sensors and the trajectory are not coplanar, localization may be successful even if the projectile passes on one side of the network. See Section 6.3.) As the numbers of sensors decreased in the experiment by randomly selecting a subset, the probability of trajectories abiding by this rule decreased. This also means that even if there are 0 2 4 6 0 2 4 6 0 1 2 3 4 5 6 azimutherror(degree) position error (m) orientation error (degree)
0 2 4 6 0 2 4 6 0 1 2 3 4 5 6 azimutherror(degree) position error (m) orientation error (degree)
0 2 4 6 0 2 4 6 0 1 2 3 4 5 6 azimutherror(degree) position error (m) orientation error (degree)
0 2 4 6 0 2 4 6 0 1 2 3 4 5 6 azimutherror(degree) position error (m) orientation error (degree)
Figure 14: The effect of node localization and orientation errors on azimuth accuracy with 2, 4, 6 and
identical for the 8-node case, hence, it is omitted. 124 many sensors (i.e. soldiers), but all of them are right next to each other, the localization rate will suffer. However, when the sensor fusion does provide a result, it is still accurate even with few available sensors and relatively large individual errors. A very few consistent observation lead to good accuracy as the inconsistent ones are discarded by the algorithm. This is also supported by the observation that for the cases with the higher number of sensors (8 or 10), the localization rate is hardly affected by even large errors.
Errors/Sensors 2 4 6 8 10
Table 4: Localization rate as a function of the number of sensors used, the sensor node location and orientation errors.
One of the most significant observations on Figure 14 and Table 4 is that there is hardly any difference in the data for 6, 8 and 10 sensors. This means that there is little advantage of adding more nodes beyond 6 sensors as far as the accuracy is concerned.
The speed of sound depends on the ambient temperature.
The current prototype considers it constant that is typically set before a test. It would be straightforward to employ a temperature sensor to update the value of the speed of sound periodically during operation. Note also that wind may adversely affect the accuracy of the system. The sensor fusion, however, could incorporate wind speed into its calculations. It would be more complicated than temperature compensation, but could be done.
Other practical issues also need to be looked at before a real world deployment. Silencers reduce the muzzle blast energy and hence, the effective range the system can detect it at. However, silencers do not effect the shockwave and the system would still detect the trajectory and caliber accurately. The range and weapon type could not be estimated without muzzle blast detections. Subsonic weapons do not produce a shockwave. However, this is not of great significance, since they have shorter range, lower accuracy and much less lethality. Hence, their use is not widespread and they pose less danger in any case.
Another issue is the type of ammunition used. Irregular armies may use substandard, even hand manufactured bullets. This effects the muzzle velocity of the weapon. For weapon classification to work accurately, the system would need to be calibrated with the typical ammunition used by the given adversary.
Acoustic detection and recognition has been under research since the early fifties. The area has a close relevance to the topic of supersonic flow mechanics [20]. Fansler analyzed the complex near-field pressure waves that occur within a foot of the muzzle blast. Fansler"s work gives a good idea of the ideal muzzle blast pressure wave without contamination from echoes or propagation effects [4].
Experiments with greater distances from the muzzle were conducted by Stoughton [18]. The measurements of the ballistic shockwaves using calibrated pressure transducers at known locations, measured bullet speeds, and miss distances of
Results indicate that ground interaction becomes a problem for miss distances of 30 meters or larger.
Another area of research is the signal processing of gunfire acoustics. The focus is on the robust detection and length estimation of small caliber acoustic shockwaves and muzzle blasts. Possible techniques for classifying signals as either shockwaves or muzzle blasts includes short-time Fourier Transform (STFT), the Smoothed Pseudo Wigner-Ville distribution (SPWVD), and a discrete wavelet transformation (DWT). Joint time-frequency (JTF) spectrograms are used to analyze the typical separation of the shockwave and muzzle blast transients in both time and frequency. Mays concludes that the DWT is the best method for classifying signals as either shockwaves or muzzle blasts because it works well and is less expensive to compute than the SPWVD [10].
The edges of the shockwave are typically well defined and the shockwave length is directly related to the bullet characteristics. A paper by Sadler [14] compares two shockwave edge detection methods: a simple gradient-based detector, and a multi-scale wavelet detector. It also demonstrates how the length of the shockwave, as determined by the edge detectors, can be used along with Whithams equations [20] to estimate the caliber of a projectile. Note that the available computational performance on the sensor nodes, the limited wireless bandwidth and real-time requirements render these approaches infeasible on our platform.
A related topic is the research and development of experimental and prototype shooter location systems. Researchers at BBN have developed the Bullet Ears system [3] which has the capability to be installed in a fixed position or worn by soldiers. The fixed system has tetrahedron shaped microphone arrays with 1.5 meter spacing. The overall system consists of two to three of these arrays spaced 20 to 100 meters from each other. The soldier-worn system has 12 microphones as well as a GPS antenna and orientation sensors mounted on a helmet. There is a low speed RF connection from the helmet to the processing body. An extensive test has been conducted to measure the performance of both type of systems. The fixed systems performance was one order of magnitude better in the angle calculations while their range performance where matched. The angle accuracy of the fixed system was dominantly less than one degree while it was around five degrees for the helmet mounted one. The range accuracy was around 5 percent for both of the systems. The problem with this and similar centralized systems is the need of the one or handful of microphone arrays to be in line-of-sight of the shooter. A sensor networked based solution has the advantage of widely distributed sensing for better coverage, multipath effect compensation and multiple simultaneous shot resolution [8]. This is especially important for operation in acoustically reverberant urban areas. Note that BBN"s current vehicle-mounted system called BOOMERANG, a modified version of Bullet Ears, is currently used in Iraq [1].
The company ShotSpotter specializes in law enforcement systems that report the location of gunfire to police within seconds. The goal of the system is significantly different than that of military systems. Shotspotter reports 25 m typical accuracy which is more than enough for police to 125 respond. They are also manufacturing experimental soldier wearable and UAV mounted systems for military use [16], but no specifications or evaluation results are publicly available.
The main contribution of this work is twofold. First, the performance of the overall distributed networked system is excellent. Most noteworthy are the trajectory accuracy of one degree, the correct caliber estimation rate of well over 90% and the close to 100% weapon classification rate for 4 of the 6 weapons tested. The system proved to be very robust when increasing the node location and orientation errors and decreasing the number of available sensors all the way down to a couple. The key factor behind this is the sensor fusion algorithm"s ability to reject erroneous measurements. It is also worth mentioning that the results presented here correspond to the first and only test of the system beyond 100 m and with six different weapons. We believe that with the lessons learned in the test, a consecutive field experiment could have showed significantly improved results especially in range estimation beyond 100 m and weapon classification for the remaining two weapons that were mistaken for each other the majority of the times during the test.
Second, the performance of the system when used in standalone mode, that is, when single sensors alone provided localization, was also very good. While the overall localization rate of 42% per sensor for shots up to 130 m could be improved, the bearing accuracy of less than a degree and the average 5% range error are remarkable using the handmade prototypes of the low-cost nodes. Note that 87% of the shots were successfully localized by at least one of the ten sensors utilized in standalone mode.
We believe that the technology is mature enough that a next revision of the system could be a commercial one.
However, important aspects of the system would still need to be worked on. We have not addresses power management yet. A current node runs on 4 AA batteries for about
the sensor node would need to be asleep during normal operation and only wake up when an interesting event occurs.
An analog trigger circuit could solve this problem, however, the system would miss the first shot. Instead, the acoustic channels would need to be sampled and stored in a circular buffer. The rest of the board could be turned off. When a trigger wakes up the board, the acoustic data would be immediately available. Experiments with a previous generation sensor board indicated that this could provide a 10x increase in battery life. Other outstanding issues include weatherproof packaging and ruggedization, as well as integration with current military infrastructure.
[1] BBN technologies website. http://www.bbn.com. [2] E. Danicki. Acoustic sniper localization. Archives of Acoustics, 30(2):233-245, 2005. [3] G. L. Duckworth et al. Fixed and wearable acoustic counter-sniper systems for law enforcement. In E. M.
Carapezza and D. B. Law, editors, Proc. SPIE Vol. 3577, p. 210-230, pages 210-230, Jan. 1999. [4] K. Fansler. Description of muzzle blast by modified scaling models. Shock and Vibration, 5(1):1-12, 1998. [5] D. Gay, P. Levis, R. von Behren, M. Welsh,
E. Brewer, and D. Culler. The nesC language: a holistic approach to networked embedded systems.
Proceedings of Programming Language Design and Implementation (PLDI), June 2003. [6] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. Culler, and K. Pister. System architecture directions for networked sensors. in Proc. of ASPLOS 2000, Nov. 2000. [7] B. Kus´y, G. Balogh, P. V¨olgyesi, J. Sallai, A. N´adas,
A. L´edeczi, M. Mar´oti, and L. Meertens. Node-density independent localization. Information Processing in Sensor Networks (IPSN 06) SPOTS Track, Apr. 2006. [8] A. L´edeczi, A. N´adas, P. V¨olgyesi, G. Balogh,
B. Kus´y, J. Sallai, G. Pap, S. D´ora, K. Moln´ar,
M. Mar´oti, and G. Simon. Countersniper system for urban warfare. ACM Transactions on Sensor Networks, 1(1):153-177, Nov. 2005. [9] M. Mar´oti. Directed flood-routing framework for wireless sensor networks. In Proceedings of the 5th ACM/IFIP/USENIX International Conference on Middleware, pages 99-114, New York, NY, USA, 2004.
Springer-Verlag New York, Inc. [10] B. Mays. Shockwave and muzzle blast classification via joint time frequency and wavelet analysis.
Technical report, Army Research Lab Adelphi MD 20783-1197, Sept. 2001. [11] TinyOS Hardware Platforms. http://tinyos.net/scoop/special/hardware. [12] Crossbow MICAz (MPR2400) Radio Module. http://www.xbow.com/Products/productsdetails. aspx?sid=101. [13] PicoBlaze User Resources. http://www.xilinx.com/ipcenter/processor_ central/picoblaze/picoblaze_user_resources.htm. [14] B. M. Sadler, T. Pham, and L. C. Sadler. Optimal and wavelet-based shock wave detection and estimation. Acoustical Society of America Journal, 104:955-963, Aug. 1998. [15] J. Sallai, B. Kus´y, A. L´edeczi, and P. Dutta. On the scalability of routing-integrated time synchronization. 3rd European Workshop on Wireless Sensor Networks (EWSN 2006), Feb. 2006. [16] ShotSpotter website. http: //www.shotspotter.com/products/military.html. [17] G. Simon, M. Mar´oti, A. L´edeczi, G. Balogh, B. Kus´y,
A. N´adas, G. Pap, J. Sallai, and K. Frampton. Sensor network-based countersniper system. In SenSys "04: Proceedings of the 2nd international conference on Embedded networked sensor systems, pages 1-12, New York, NY, USA, 2004. ACM Press. [18] R. Stoughton. Measurements of small-caliber ballistic shock waves in air. Acoustical Society of America Journal, 102:781-787, Aug. 1997. [19] B. A. Weiss, C. Schlenoff, M. Shneier, and A. Virts.
Technology evaluations and performance metrics for soldier-worn sensors for assist. In Performance Metrics for Intelligent Systems Workshop, Aug. 2006. [20] G. Whitham. Flow pattern of a supersonic projectile.

Web services can be composed into workflows to provide streamlined end-to-end functionality for human users or other systems.
Although previous research efforts have looked at ways to intelligently automate the composition of web services into workflows (e.g. [1, 9]), an important remaining problem is the assignment of web service requests to the underlying web service providers in a multi-tiered runtime scenario within constraints. In this paper we address this scheduling problem and examine means to manage a large number of business process workflows in a scalable manner.
The problem of scheduling web service requests to providers is relevant to modern business domains that depend on multi-tiered service provisioning. Consider the example shown in Figure 1 that illustrates our problem space. Workflows comprise multiple related business processes that are web service consumers; here we assume that the workflows represent requested service from customers or automated systems and that the workflow has already been composed with an existing choreography toolkit. These workflows are then submitted to a portal (not shown) that acts as a scheduling agent between the web service consumers and the web service providers.
In this example, a workflow could represent the actions needed to instantiate a vacation itinerary, where one business process requests booking an airline ticket, another business process requests a hotel room, and so forth. Each of these requests target a particular service type (e.g. airline reservations, hotel reservations, car reservations, etc.), and for each service type, there are multiple instances of service providers that publish a web service interface. An important challenge is that the workflows must meet some quality-of-service (QoS) metric, such as end-to-end completion time of all its business processes, and that meeting or failing this goal results in the assignment of a quantitative business value metric for the workflow; intuitively, it is desired that all workflows meet their respective QoS goals. We further leverage the notion that QoS service agreements are generally agreed-upon between the web service providers and the scheduling agent such that the providers advertise some level of guaranteed QoS to the scheduler based upon runtime conditions such as turnaround time and maximum available concurrency. The resulting problem is then to schedule and assign the business processes" requests for service types to one of the service providers for that type. The scheduling must be done such that the aggregate business value across all the workflows is maximised.
In Section 3 we state the scenario as a combinatorial problem and utilise a genetic search algorithm [5] to find the best assignment of web service requests to providers. This approach converges towards an assignment that maximises the overall business value for all the workflows.
In Section 4 we show through experimentation that this search heuristic finds better assignments than other algorithms (greedy, round-robin, and proportional). Further, this approach allows us to scale the number of simultaneous workflows (up to one thousand workflows in our experiments) and yet still find effective schedules.
In the context of service assignment and scheduling, [11] maps web service calls to potential servers using linear programming, but their work is concerned with mapping only single workflows; our principal focus is on scalably scheduling multiple workflows (up 30 Service Type SuperHotels.com Business Process Business Process Workflow ...
Business Process Business Process ...
HostileHostels.com IncredibleInns.com Business Process Business Process Business Process ...
Business Process Service Provider SkyHighAirlines.com SuperCrazyFlights.com Business Process . . . . . .
Advertised QoS Service Agreement CarRentalService.com Figure 1: An example scenario demonstrating the interaction between business processes in workflows and web service providers.
Each business process accesses a service type and is then mapped to a service provider for that type. to one thousand as we show later) using different business metrics and a search heuristic. [10] presents a dynamic provisioning approach that uses both predictive and reactive techniques for multi-tiered Internet application delivery. However, the provisioning techniques do not consider the challenges faced when there are alternative query execution plans and replicated data sources. [8] presents a feedback-based scheduling mechanism for multi-tiered systems with back-end databases, but unlike our work, it assumes a tighter coupling between the various components of the system.
Our work also builds upon prior scheduling research. The classic job-shop scheduling problem, shown to be NP-complete [4] [3], is similar to ours in that tasks within a job must be scheduled onto machinery (c.f. our scenario is that business processes within a workflow must be scheduled onto web service providers). The salient differences are that the machines can process only one job at a time (we assume servers can multi-task but with degraded performance and a maximum concurrency level), tasks within a job cannot simultaneously run on different machines (we assume business processes can be assigned to any available server), and the principal metric of performance is the makespan, which is the time for the last task among all the jobs to complete (and as we show later, optimising on the makespan is insufficient for scheduling the business processes, necessitating different metrics).
In this section we describe our model and discuss how we can find scheduling assignments using a genetic search algorithm.
We base our model on the simplified scenario shown in Figure
the execution of a workflow. The workflows comprise business processes, each of which makes one web service invocation to a service type. Further, business processes have an ordering in the workflow. The arrangement and execution of the business processes and the data flow between them are all managed by a composition or choreography tool (e.g. [1, 9]). Although composition languages can use sophisticated flow-control mechanisms such as conditional branches, for simplicity we assume the processes execute sequentially in a given order.
This scenario can be naturally extended to more complex relationships that can be expressed in BPEL [7], which defines how business processes interact, messages are exchanged, activities are ordered, and exceptions are handled. Due to space constraints, we focus on the problem space presented here and will extend our model to more advanced deployment scenarios in the future.
Each workflow has a QoS requirement to complete within a specified number of time units (e.g. on the order of seconds, as detailed in the Experiments section). Upon completion (or failure), the workflow is assigned a business value. We extended this approach further and considered different types of workflow completion in order to model differentiated QoS levels that can be applied by businesses (for example, to provide tiered customer service).
We say that a workflow is successful if it completes within its QoS requirement, acceptable if it completes within a constant factor κ 31 of its QoS bound (in our experiments we chose κ=3), or failing if it finishes beyond κ times its QoS bound. For each category, a business value score is assigned to the workflow, with the successful category assigned the highest positive score, followed by acceptable and then failing. The business value point distribution is non-uniform across workflows, further modelling cases where some workflows are of higher priority than others.
Each service type is implemented by a number of different service providers. We assume that the providers make service level agreements (SLAs) to guarantee a level of performance defined by the completion time for completing a web service invocation.
Although SLAs can be complex, in this paper we assume for simplicity that the guarantees can take the form of a linear performance degradation under load. This guarantee is defined by several parameters: α is the expected completion time (for example, on the order of seconds) if the assigned workload of web service requests is less than or equal to β, the maximum concurrency, and if the workload is higher than β, the expected completion for a workload of size ω is α+ γ(ω − β) where γ is a fractional coefficient. In our experiments we vary α, β, and γ with different distributions.
Ideally, all workflows would be able to finish within their QoS limits and thus maximise the aggregate business value across all workflows. However, because we model service providers with degrading performance under load, not all workflows will achieve their QoS limit: it may easily be the case that business processes are assigned to providers who are overloaded and cannot complete within the respective workflow"s QoS limit. The key research problem, then, is to assign the business processes to the web service providers with the goal of optimising on the aggregate business value of all workflows.
Given that the scope of the optimisation is the entire set of workflows, it may be that the best scheduling assignments may result in some workflows having to fail in order for more workflows to succeed. This intuitive observation suggests that traditional scheduling approaches such as round-robin or proportional assignments will not fare well, which is what we observe and discuss in Section 4.
On the other hand, an exhaustive search of all the possible assignments will find the best schedule, but the computational complexity is prohibitively high. Suppose there are W workflows with an average of B business processes per workflow. Further, in the worst case each business process requests one service type, for which there are P providers. There are thus W · PB combinations to explore to find the optimal assignments of business processes to providers. Even for small configurations (e.g. W =10, B=5, P=10), the computational time for exhaustive search is significant, and in our work we look to scale these parameters. In the next subsection, discuss how a genetic search algorithm can be used to converge toward the optimum scheduling assignments.
Given an exponential search space of business process assignments to web service providers, the problem is to find the optimal assignment that produces the overall highest aggregate business value across all workflows. To explore the solution space, we use a genetic algorithm (GA) search heuristic that simulates Darwinian natural selection by having members of a population compete to survive in order to pass their genetic chromosomes onto the next generation; after successive generations, there is a tendency for the chromosomes to converge toward the best combination [5] [6].
Although other search heuristics exist that can solve optimization problems (e.g. simulated annealing or steepest-ascent hillclimbing), the business process scheduling problem fits well with a GA because potential solutions can be represented in a matrix form and allows us to use prior research in effective GA chromosome recombination to form new members of the population (e.g. [2]).
Figure 2: An example chromosome representing a scheduling assignment of (workflow,service type) → service provider. Each row represents a workflow, and each column represents a service type. For example, here there are 3 workflows (0 to 2) and
type 3 goes to provider 2. Note that the service provider identifier is within a range limited to its service type (i.e. its column), so the 2 listed for service type 3 is a different server from server 2 in other columns.
Chromosome representation of a solution. In Figure 2 we show an example chromosome that encodes one scheduling assignment. The representation is a 2-dimensional matrix that maps {workflow, service type} to a service provider. For a business process in workflow i and utilising service type j, the (i, j)th entry in the table is the identifier for the service provider to which the business process is assigned. Note that the service provider identifier is within a range limited to its service type.
GA execution. A GA proceeds as follows. Initially a random set of chromosomes is created for the population. The chromosomes are evaluated (hashed) to some metric, and the best ones are chosen to be parents. In our problem, the evaluation produces the net business value across all workflows after executing all business processes once they are assigned to their respective service providers according to the mapping in the chromosome. The parents recombine to produce children, simulating sexual crossover, and occasionally a mutation may arise which produces new characteristics that were not available in either parent. The principal idea is that we would like the children to be different from the parents (in order to explore more of the solution space) yet not too different (in order to contain the portions of the chromosome that result in good scheduling assignments). Note that finding the global optimum is not guaranteed because the recombination and mutation are stochastic.
GA recombination and mutation. As mentioned, the chromosomes are 2-dimensional matrices that represent scheduling assignments. To simulate sexual recombination of two chromosomes to produce a new child chromosome, we applied a one-point crossover scheme twice (once along each dimension). The crossover is best explained by analogy to Cartesian space as follows. A random point is chosen in the matrix to be coordinate (0, 0). Matrix elements from quadrants II and IV from the first parent and elements from quadrants I and III from the second parent are used to create the new child. This approach follows GA best practices by keeping contiguous chromosome segments together as they are transmitted from parent to child.
The uni-chromosome mutation scheme randomly changes one of the service provider assignments to another provider within the available range. Other recombination and mutation schemes are an area of research in the GA community, and we look to explore new operators in future work.
GA evaluation function. An important GA component is the evaluation function. Given a particular chromosome representing one scheduling mapping, the function deterministically calculates the net business value across all workloads. The business processes in each workload are assigned to service providers, and each provider"s completion time is calculated based on the service agreement guarantee using the parameters mentioned in Section 3.1, namely the unloaded completion time α, the maximum concur32 rency β, and a coefficient γ that controls the linear performance degradation under heavy load. Note that the evaluation function can be easily replaced if desired; for example, other evaluation functions can model different service provider guarantees or parallel workflows.
In this section we show the benefit of using our GA-based scheduler. Because we wanted to scale the scenarios up to a large number of workflows (up to 1000 in our experiments), we implemented a simulation program that allowed us to vary parameters and to measure the results with different metrics. The simulator was written in standard C++ and was run on a Linux (Fedora Core) desktop computer running at 2.8 GHz with 1GB of RAM.
We compared our algorithm against alternative candidates: • A well-known round-robin algorithm that assigns each business process in circular fashion to the service providers for a particular service type. This approach provides the simplest scheme for load-balancing. • A random-proportional algorithm that proportionally assigns business processes to the service providers; that is, for a given service type, the service providers are ranked by their guaranteed completion time, and business processes are assigned proportionally to the providers based on their completion time. (We also tried a proportionality scheme based on both the completion times and maximum concurrency but attained the same results, so only the former scheme"s results are shown here.) • A strawman greedy algorithm that always assigns business processes to the service provider that has the fastest guaranteed completion time. This algorithm represents a naive approach based on greedy, local observations of each workflow without taking into consideration all workflows.
In the experiments that follow, all results were averaged across
during the GA, each trial started by reading in pre-initialised data from disk. In Table 1 we list our experimental parameters.
In Figure 3 we show the results of running our GA against the three candidate alternatives. The x-axis shows the number for workflows scaled up to 1000, and the y-axis shows the aggregate business value for all workflows. As can be seen, the GA consistently produces the highest business value even as the number of workflows grows; at 1000 workflows, the GA produces a 115% improvement over the next-best alternative. (Note that although we are optimising against the business value metric we defined earlier, genetic algorithms are able to converge towards the optimal value of any metric, as long as the evaluation function can consistently measure a chromosome"s value with that metric.) As expected, the greedy algorithm performs very poorly because it does the worst job at balancing load: all business processes for a given service type are assigned to only one server (the one advertised to have the fastest completion time), and as more business processes arrive, the provider"s performance degrades linearly.
The round-robin scheme is initially outperformed by the randomproportional scheme up to around 120 workflows (as shown in the magnified graph of Figure 4), but as the number of workflows increases, the round-robin scheme consistently wins over randomproportional. The reason is that although the random-proportional scheme assigns business processes to providers proportionally according to the advertised completion times (which is a measure of the power of the service provider), even the best providers will eventually reach a real-world maximum concurrency for the large -2000 -1000 0 1000 2000 3000 4000 5000 6000 7000
Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 3: Net business value scores of different scheduling algorithms. -500 0 500 1000 1500 2000 2500 3000 3500 4000
Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 4: Magnification of the left-most region in Figure 3. number of workflows that we are considering. For a very large number of workflows, the round-robin scheme is able to better balance the load across all service providers.
To better understand the behaviour resulting from the scheduling assignments, we show the workflow completion results in Figures 5, 6, and 7 for 100, 500, and 900 workflows, respectively. These figures show the percentage of workflows that are successful (can complete with their QoS limit), acceptable (can complete within κ=3 times their QoS limit), and failed (cannot complete within κ=3 times their QoS limit). The GA consistently produces the highest percentage of successful workflows (resulting in higher business values for the aggregate set of workflows). Further, the round-robin scheme produces better results than the random-proportional for a large number of workflows but does not perform as well as the GA.
In Figure 8 we graph the makespan resulting from the same experiments above. Makespan is a traditional metric from the job scheduling community measuring elapsed time for the last job to complete. While useful, it does not capture the high-level business value metric that we are optimising against. Indeed, the makespan is oblivious to the fact that we provide multiple levels of completion (successful, acceptable, and failed) and assign business value scores accordingly. For completeness, we note that the GA provides the fastest makespan, but it is matched by the round robin algorithm. The GA produces better business values (as shown in Figure 3) because it is able to search the solution space to find better mappings that produce more successful workflows (as shown in Figures 5 to 7).
We also looked at the effect of the scheduling algorithms on balancing the load. Figure 9 shows the percentage of services providers that were accessed while the workflows ran. As expected, the greedy algorithm always hits one service provider; on the other hand, the round-robin algorithm is the fastest to spread the business 33 Experimental parameter Comment Workflows 5 to 1000 Business processes per workflow uniform random: 1 - 10 Service types 10 Service providers per service type uniform random: 1 - 10 Workflow QoS goal uniform random: 10-30 seconds Service provider completion time (α) uniform random: 1 - 12 seconds Service provider maximum concurrency (β) uniform random: 1 - 12 Service provider degradation coefficient (γ) uniform random: 0.1 - 0.9 Business value for successful workflows uniform random: 10 - 50 points Business value for acceptable workflows uniform random: 0 - 10 points Business value for failed workflows uniform random: -10 - 0 points GA: number of parents 20 GA: number of children 80 GA: number of generations 1000 Table 1: Experimental parameters Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 100 workflows Figure 5: Workflow behaviour for 100 workflows.
Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 6: Workflow behaviour for 500 workflows.
Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 7: Workflow behaviour for 900 workflows. 0 50 100 150 200 250 300
Makespan[seconds] Number of workflows Maximum completion time for all workflows Genetic algorithm Round robin Random proportional Greedy Figure 8: Maximum completion time for all workflows. This value is the makespan metric used in traditional scheduling research.
Although useful, the makespan does not take into consideration the business value scoring in our problem domain. processes. Figure 10 is the percentage of accessed service providers (that is, the percentage of service providers represented in Figure 9) that had more assigned business processes than their advertised maximum concurrency. For example, in the greedy algorithm only one service provider is utilised, and this one provider quickly becomes saturated. On the other hand, the random-proportional algorithm uses many service providers, but because business processes are proportionally assigned with more assignments going to the better providers, there is a tendency for a smaller percentage of providers to become saturated.
For completeness, we show the performance of the genetic algorithm itself in Figure 11. The algorithm scales linearly with an increasing number of workflows. We note that the round-robin, random-proportional, and greedy algorithms all finished within 1 second even for the largest workflow configuration. However, we feel that the benefit of finding much higher business value scores justifies the running time of the GA; further we would expect that the running time will improve with both software tuning as well as with a computer faster than our off-the-shelf PC.
Business processes within workflows can be orchestrated to access web services. In this paper we looked at multi-tiered service provisioning where web service requests to service types can be mapped to different service providers. The resulting problem is that in order to support a very large number of workflows, the assignment of business process to web service provider must be intelligent. We used a business value metric to measure the be34 0
1
Percentageofallserviceproviders Number of workflows Service providers utilised Genetic algorithm Round robin Random proportional Greedy Figure 9: The percentage of service providers utilized during workload executions. The Greedy algorithm always hits the one service provider, while the Round Robin algorithm spreads requests evenly across the providers. 0
1
Percentageofallserviceproviders Number of workflows Service providers saturated Genetic algorithm Round robin Random proportional Greedy Figure 10: The percentage of service providers that are saturated among those providers who were utilized (that is, percentage of the service providers represented in Figure 9). A saturated service provider is one whose workload is greater that its advertised maximum concurrency. 0 5 10 15 20 25
Runningtimeinseconds Total number of workflows Running time of genetic algorithm GA running time Figure 11: Running time of the genetic algorithm. haviour of workflows meeting or failing QoS values, and we optimised our scheduling to maximise the aggregate business value across all workflows. Since the solution space of scheduler mappings is exponential, we used a genetic search algorithm to search the space and converge toward the best schedule. With a default configuration for all parameters and using our business value scoring, the GA produced up to 115% business value improvement over the next best algorithm. Finally, because a genetic algorithm will converge towards the optimal value using any metric (even other than the business value metric we used), we believe our approach has strong potential for continuing work.
In future work, we look to acquire real-world traces of web service instances in order to get better estimates of service agreement guarantees, although we expect that such guarantees between the providers and their consumers are not generally available to the public. We will also look at other QoS metrics such as CPU and I/O usage. For example, we can analyse transfer costs with varying bandwidth, latency, data size, and data distribution. Further, we hope to improve our genetic algorithm and compare it to more scheduler alternatives. Finally, since our work is complementary to existing work in web services choreography (because we rely on pre-configured workflows), we look to integrate our approach with available web service workflow systems expressed in BPEL.
[1] A. Ankolekar, et al. DAML-S: Semantic Markup For Web Services, In Proc. of the Int"l Semantic Web Working Symposium, 2001. [2] L. Davis. Job Shop Scheduling with Genetic Algorithms,
In Proc. of the Int"l Conference on Genetic Algorithms, 1985. [3] H.-L. Fang, P. Ross, and D. Corne. A Promising Genetic Algorithm Approach to Job-Shop Scheduling, Rescheduling, and Open-Shop Scheduling Problems , In Proc. on the 5th Int"l Conference on Genetic Algorithms, 1993. [4] M. Gary and D. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness, Freeman, 1979. [5] J. Holland. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence, MIT Press, 1992. [6] D. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning, Kluwer Academic Publishers, 1989. [7] Business Processes in a Web Services World, www-128.ibm.com/developerworks/ webservices/library/ws-bpelwp/. [8] G. Soundararajan, K. Manassiev, J. Chen, A. Goel, and C.
Amza. Back-end Databases in Shared Dynamic Content Server Clusters, In Proc. of the IEEE Int"l Conference on Autonomic Computing, 2005. [9] B. Srivastava and J. Koehler. Web Service Composition Current Solutions and Open Problems, ICAP, 2003. [10] B. Urgaonkar, P. Shenoy, A. Chandra, and P. Goyal.
Dynamic Provisioning of Multi-Tier Internet Applications,
In Proc. of the IEEE Int"l Conference on Autonomic Computing, 2005. [11] L. Zeng, B. Benatallah, M. Dumas, J. Kalagnanam, and Q.

Grid computing does not have a single, universally accepted definition. The technology behind grid computing model is not new. Its roots lie in early distributed computing models that date back to early 1980s, where scientists harnessed the computing power of idle workstations to let compute intensive applications to run on multiple workstations, which dramatically shortening processing times. Although numerous distributed computing models were available for discipline-specific scientific applications, only recently have the tools became available to use general-purpose applications on a grid. Consequently, the grid computing model is gaining popularity and has become a show piece of "utility computing". Since in the IT industry, various computing models are used interchangeably with grid computing, we first sort out the similarities and difference between these computing models so that grid computing can be placed in perspective.
A cluster is a group of machines in a fixed configuration united to operate and be managed as a single entity to increase robustness and performance. The cluster appears as a single high-speed system or a single highly available system. In this model, resources can not enter and leave the group as necessary. There are at least two types of clusters: parallel clusters and highavailability clusters. Clustered machines are generally in spatial proximity, such as in the same server room, and dedicated solely to their task.
In a high-availability cluster, each machine provides the same service. If one machine fails, another seamlessly takes over its workload. For example, each computer could be a web server for a web site. Should one web server "die," another provides the service, so that the web site rarely, if ever, goes down.
A parallel cluster is a type of supercomputer. Problems are split into many parts, and individual cluster members are given part of the problem to solve. An example of a parallel cluster is composed of Apple Power Mac G5 computers at Virginia Tech University [1].
Distributed computing spatially expands network services so that the components providing the services are separated. The major objective of this computing model is to consolidate processing power over a network. A simple example is spreading services such as file and print serving, web serving, and data storage across multiple machines rather than a single machine handling all the tasks. Distributed computing can also be more fine-grained, where even a single application is broken into parts and each part located on different machines: a word processor on one server, a spell checker on a second server, etc.
Literally, utility computing resembles common utilities such as telephone or electric service. A service provider makes computing resources and infrastructure management available to a customer as needed, and charges for usage rather than a flat rate. The important thing to note is that resources are only used as needed, and not dedicated to a single customer.
Grid computing contains aspects of clusters, distributed computing, and utility computing. In the most basic sense, grid turns a group of heterogeneous systems into a centrally managed but flexible computing environment that can work on tasks too time intensive for the individual systems. The grid members are not necessarily in proximity, but must merely be accessible over a network; the grid can access computers on a LAN, WAN, or anywhere in the world via the Internet. In addition, the computers comprising the grid need not be dedicated to the grid; rather, they can function as normal workstations, and then advertise their availability to the grid when not in use.
The last characteristic is the most fundamental to the grid described in this paper. A well-known example of such an ad hoc grid is the SETI@home project [2] of the University of California at Berkeley, which allows any person in the world with a computer and an Internet connection to donate unused processor time for analyzing radio telescope data.
A computer grid expands the capabilities of the cluster by loosing its spatial bounds, so that any computer accessible through the network gains the potential to augment the grid. A fundamental grid feature is that it scales well. The processing power of any machine added to the grid is immediately availably for solving problems. In addition, the machines on the grid can be generalpurpose workstations, which keep down the cost of expanding the grid.
COMPUTING Effective use of a grid requires a computation that can be divided into independent (i.e., parallel) tasks. The results of each task cannot depend on the results of any other task, and so the members of the grid can solve the tasks in parallel. Once the tasks have been completed, the results can be assembled into the solution. Examples of parallelizable computations are the Mandelbrot set of fractals, the Monte Carlo calculations used in disciplines such as Solid State Physics, and the individual frames of a rendered animation. This paper is concerned with the last example.
Computing The applications used in grid computing must either be specifically designed for grid use, or scriptable in such a way that they can receive data from the grid, process the data, and then return results. In other words, the best candidates for grid computing are applications that run the same or very similar computations on a large number of pieces of data without any dependencies on the previous calculated results. Applications heavily dependent on data handling rather than processing power are generally more suitable to run on a traditional environment than on a grid platform. Of course, the applications must also run on the computing platform that hosts the grid. Our interest is in using the Alias Maya application [3] with Apple"s Xgrid [4] on Mac OS X.
Commercial applications usually have strict license requirements.
This is an important concern if we install a commercial application such as Maya on all members of our grid. By its nature, the size of the grid may change as the number of idle computers changes. How many licenses will be required? Our resolution of this issue will be discussed in a later section.
Infrastructure The grid requires a controller that recognizes when grid members are available, and parses out job to available members. The controller must be able to see members on the network. This does not require that members be on the same subnet as the controller, but if they are not, any intervening firewalls and routers must be configured to allow grid traffic.
Xgrid is Apple"s grid implementation. It was inspired by Zilla, a desktop clustering application developed by NeXT and acquired by Apple. In this report we describe the Xgrid Technology Preview 2, a free download that requires Mac OS X 10.2.8 or later and a minimum 128 MB RAM [5].
Xgrid, leverages Apple"s traditional ease of use and configuration.
If the grid members are on the same subnet, by default Xgrid automatically discovers available resources through Rendezvous [6]. Tasks are submitted to the grid through a GUI interface or by the command line. A System Preference Pane controls when each computer is available to the grid.
It may be best to view Xgrid as a facilitator. The Xgrid architecture handles software and data distribution, job execution, and result aggregation. However, Xgrid does not perform the actual calculations.
Xgrid has three major components: the client, controller, and the agent. Each component is included in the default installation, and any computer can easily be configured to assume any role. In 120 fact, for testing purposes, a computer can simultaneously assume all roles in local mode. The more typical production use is called cluster mode.
The client submits jobs to the controller through the Xgrid GUI or command line. The client defines how the job will be broken into tasks for the grid. If any files or executables must be sent as part of a job, they must reside on the client or at a location accessible to the client. When a job is complete, the client can retrieve the results from the controller. A client can only connect to a single controller at a time.
The controller runs the GridServer process. It queues tasks received from clients, distributes those tasks to the agents, and handles failover if an agent cannot complete a task. In Xgrid Technology Preview 2, a controller can handle a maximum of 10,000 agent connections. Only one controller can exist per logical grid.
The agents run the GridAgent process. When the GridAgent process starts, it registers with a controller; an agent can only be connected to one controller at a time. Agents receive tasks from their controller, perform the specified computations, and then send the results back to the controller. An agent can be configured to always accept tasks, or to just accept them when the computer is not otherwise busy.
By default, Xgrid requires two passwords. First, a client needs a password to access a controller. Second, the controller needs a password to access an agent. Either password requirement can be disabled. Xgrid uses two-way-random mutual authentication protocol with MD5 hashes. At this time, data encryption is only used for passwords.
As mentioned earlier, an agent registers with a controller when the GridAgent process starts. There is no native method for the controller to reject agents, and so it must accept any agent that registers. This means that any agent could submit a job that consumes excessive processor and disk space on the agents. Of course, since Mac OS X is a BSD-based operating system, the controller could employ Unix methods of restricting network connections from agents.
The Xgrid daemons run as the user nobody, which means the daemons can read, write, or execute any file according to world permissions. Thus, Xgrid jobs can execute many commands and write to /tmp and /Volumes. In general, this is not a major security risk, but is does require a level of trust between all members of the grid.
Basic Xgrid installation and configuration is described both in Apple documentation [5] and online at the University of Utah web site [8]. The installation is straightforward and offers no options for customization. This means that every computer on which Xgrid is installed has the potential to be a client, controller, or agent.
The agents and controllers can be configured through the Xgrid Preference Pane in the System Preferences or XML files in /Library/Preferences. Here the GridServer and GridAgent processes are started, passwords set, and the controller discovery method used by agents is selected. By default, agents use Rendezvous to find a controller, although the agents can also be configured to look for a specific host.
The Xgrid Preference Pane also sets whether the Agents will always accept jobs, or only accept jobs when idle. In Xgrid terms, idle either means that the Xgrid screen saver has activated, or the mouse and keyboard have not been used for more than 15 minutes. Even if the agent is configured to always accept tasks, if the computer is being used these tasks will run in the background at a low priority.
However, if an agent only accepts jobs when idle, any unfinished task being performed when the computer ceases being idle are immediately stopped and any intermediary results lost. Then the controller assigns the task to another available member of the grid.
Advertising the controller via Rendezvous can be disabled by editing /Library/Preferences/com.apple.xgrid.controller.plist. This, however, will not prevent an agent from connecting to the controller by hostname.
The client sends jobs to the controller either through the Xgrid GUI or the command line. The Xgrid GUI submits jobs via small applications called plug-ins. Sample plug-ins are provided by Apple, but they are only useful as simple testing or as examples of how to create a custom plug-in. If we are to employ Xgrid for useful work, we will require a custom plug-in.
James Reynolds details the creation of custom plug-ins on the University of Utah Mac OS web site [8]. Xgrid stores plug-ins in /Library/Xgrid/Plug-ins or ~/Library/Xgrid/Plug-ins, depending on whether the plug-in was installed with Xgrid or created by a user.
The core plug-in parameter is the command, which includes the executable the agents will run. Another important parameter is the working directory. This directory contains necessary files that are not installed on the agents or available to them over a network.
The working directory will always be copied to each agent, so it is best to keep this directory small. If the files are installed on the agents or available over a network, the working directory parameter is not needed.
The command line allows the options available with the GUI plug-in, but it can be slightly more cumbersome. However, the command line probably will be the method of choice for serious work. The command arguments must be included in a script unless they are very basic. This can be a shell, perl, or python script, as long as the agent can interpret it.
When the Xgrid job is started, the command tells the controller how to break the job into tasks for the agents. Then the command is tarred and gzipped and sent to each agent; if there is a working directory, this is also tarred and gzipped and sent to the agents. 121 The agents extract these files into /tmp and run the task. Recall that since the GridAgent process runs as the user nobody, everything associated with the command must be available to nobody.
Executables called by the command should be installed on the agents unless they are very simple. If the executable depends on libraries or other files, it may not function properly if transferred, even if the dependent files are referenced in the working directory.
When the task is complete, the results are available to the client.
In principle, the results are sent to the client, but whether this actually happens depends on the command. If the results are not sent to the client, they will be in /tmp on each agent. When available, a better solution is to direct the results to a network volume accessible to the client.
Since Xgrid is only in its second preview release, there are some rough edges and limitations. Apple acknowledges some limitations [7]. For example, the controller cannot determine whether an agent is trustworthy and the controller always copies the command and working directory to the agent without checking to see if these exist on the agent.
Other limitations are likely just a by-product of an unfinished work. Neither the client nor controller can specify which agents will receive the tasks, which is particularly important if the agents contain a variety of processor types and speeds and the user wants to optimize the calculations. At this time, the best solution to this problem may be to divide the computers into multiple logical grids. There is also no standard way to monitor the progress of a running job on each agent. The Xgrid GUI and command line indicate which agents are working on tasks, but gives no indication of progress.
Finally, at this time only Mac OS X clients can submit jobs to the grid. The framework exists to allow third parties to write plug-ins for other Unix flavors, but Apple has not created them.
Our goal is an Xgrid render farm for Alias Maya. The Ringling School has about 400 Apple Power Mac G4"s and G5"s in 13 computer labs. The computers range from 733 MHz singleprocessor G4"s and 500 MHz and 1 GHz dual-processor G4"s to
used in the evening and on weekends and represent an enormous processing resource for our student rendering projects.
During our Xgrid testing, we loaded software on each computer multiple times, including the operating systems. We saved time by facilitating our installations with the remote administration daemon (radmind) software developed at the University of Michigan [9], [10].
Everything we installed for testing was first created as a radmind base load or overload. Thus, Mac OS X, Mac OS X Developer Tools, Xgrid, POV-Ray [11], and Alias Maya were stored on a radmind server and then installed on our test computers when needed.
We used six 1.8 GHz dual-processor Apple Power Mac G5"s for our Xgrid tests. Each computer ran Mac OS X 10.3.3 and contained 1 GB RAM. As shown in Figure 1, one computer served as both client and controller, while the other five acted as agents.
Before attempting Maya rendering with Xgrid, we performed basic calculations to cement our understanding of Xgrid. Apple"s Xgrid documentation is sparse, so finding helpful web sites facilitated our learning.
We first ran the Mandelbrot set plug-in provided by Apple, which allowed us to test the basic functionality of our grid. Then we performed benchmark rendering with the Open Source Application POV-Ray, as described by Daniel Côté [12] and James Reynolds [8]. Our results showed that one dual-processor G5 rendering the benchmark POV-Ray image took 104 minutes.
Breaking the image into three equal parts and using Xgrid to send the parts to three agents required 47 minutes. However, two agents finished their rendering in 30 minutes, while the third agent used 47 minutes; the entire render was only as fast as the slowest agent.
These results gave us two important pieces of information. First, the much longer rendering time for one of the tasks indicated that we should be careful how we split jobs into tasks for the agents.
All portions of the rendering will not take equal amounts of time, even if the pixel size is the same. Second, since POV-Ray cannot take advantage of both processors in a G5, neither can an Xgrid task running POV-Ray. Alias Maya does not have this limitation.
We first installed Alias Maya 6 for Mac OS X on the client/controller and each agent. Maya 6 requires licenses for use as a workstation application. However, if it is just used for rendering from the command line or a script, no license is needed.
We thus created a minimal installation of Maya as a radmind overload. The application was installed in a hidden directory inside /Applications. This was done so that normal users of the workstations would not find and attempt to run Maya, which would fail because these installations are not licensed for such use.
In addition, Maya requires the existence of a directory ending in the path /maya. The directory must be readable and writable by the Maya user. For a user running Maya on a Mac OS X workstation, the path would usually be ~/Documents/maya.
Unless otherwise specified, this directory will be the default location for Maya data and output files. If the directory does not Figure 1. Xgrid test grid.
Client/ Controller Agent 1 Agent 2 Agent 3 Agent 4 Agent 5 Network Volume Jobs Data Data 122 exist, Maya will try to create it, even if the user specifies that the data and output files exist in other locations.
However, Xgrid runs as the user nobody, which does not have a home directory. Maya is unable to create the needed directory, and looks instead for /Alias/maya. This directory also does not exist, and the user nobody has insufficient rights to create it. Our solution was to manually create /Alias/maya and give the user nobody read and write permissions.
We also created a network volume for storage of both the rendering data and the resulting rendered frames. This avoided sending the Maya files and associated textures to each agent as part of a working directory. Such a solution worked well for us because our computers are geographically close on a LAN; if greater distance had separated the agents from the client/controller, specifying a working directory may have been a better solution.
Finally, we created a custom GUI plug-in for Xgrid. The plug-in command calls a Perl script with three arguments. Two arguments specify the beginning and end frames of the render and the third argument the number of frames in each job (which we call the cluster size). The script then calculates the total number of jobs and parses them out to the agents. For example, if we begin at frame 201 and end at frame 225, with 5 frames for each job, the plug-in will create 5 jobs and send them out to the agents.
Once the jobs are sent to the agents, the script executes the /usr/sbin/Render command on each agent with the parameters appropriate for the particular job. The results are sent to the network volume.
With the setup described, we were able to render with Alias Maya
our first goal was to implement the grid, and in that we succeeded.
Plug-in In this section we summarize in simplified pseudo code format the Perl script used in our Xgrig plug-in. agent_jobs{ • Read beginning frame, end frame, and cluster size of render. • Check whether the render can be divided into an integer number of jobs based on the cluster size. • If there are not an integer number of jobs, reduce the cluster size of the last job and set its last frame to the end frame of the render. • Determine the start frame and end frame for each job. • Execute the Render command. }
Rendering with Maya from the Xgrid GUI was not trivial. The lack of Xgrid documentation and the requirements of Maya combined into a confusing picture, where it was difficult to decide the true cause of the problems we encountered. Trial and error was required to determine the best way to set up our grid.
The first hurdle was creating the directory /Alias/maya with read and write permissions for the user nobody. The second hurdle was learning that we got the best performance by storing the rendering data on a network volume.
The last major hurdle was retrieving our results from the agents.
Unlike the POV-Ray rendering tests, our initial Maya results were never returned to the client; instead, Maya stored the results in /tmp on each agent. Specifying in the plug-in where to send the results would not change this behavior. We decided this was likely a Maya issue rather than an Xgrid issue, and the solution was to send the results to the network volume via the Perl script.
Maya on Xgrid is not yet ready to be used by the students of Ringling School. In order to do this, we must address at least the following concerns. • Continue our rendering tests through the command line rather than the GUI plug-in. This will be essential for the following step. • Develop an appropriate interface for users to send jobs to the Xgrid controller. This will probably be an extension to the web interface of our existing render farm, where the student specifies parameters that are placed in a script that issues the Render command. • Perform timed Maya rendering tests with Xgrid. Part of this should compare the rendering times for Power Mac G4"s and G5"s.
Grid computing continues to advance. Recently, the IT industry has witnessed the emergence of numerous types of contemporary grid applications in addition to the traditional grid framework for compute intensive applications. For instance, peer-to-peer applications such as Kazaa, are based on storage grids that do not share processing power but instead an elegant protocol to swap files between systems. Although in our campuses we discourage students from utilizing peer-to-peer applications from music sharing, the same protocol can be utilized on applications such as decision support and data mining. The National Virtual Collaboratory grid project [13] will link earthquake researchers across the U.S. with computing resources, allowing them to share extremely large data sets, research equipment, and work together as virtual teams over the Internet.
There is an assortment of new grid players in the IT world expanding the grid computing model and advancing the grid technology to the next level. SAP [14] is piloting a project to grid-enable SAP ERP applications, Dell [15] has partnered with Platform Computing to consolidate computing resources and provide grid-enabled systems for compute intensive applications,
Oracle has integrated support for grid computing in their 10g release [16], United Devices [17] offers hosting service for gridon-demand, and Sun Microsystems continues their research and development of Sun"s N1 Grid engine [18] which combines grid and clustering platforms.
Simply, the grid computing is up and coming. The potential benefits of grid computing are colossal in higher education learning while the implementation costs are low. Today, it would be difficult to identify an application with as high a return on investment as grid computing in information technology divisions in higher education institutions. It is a mistake to overlook this technology with such a high payback. 123
The authors would like to thank Scott Hanselman of the IT team at the Ringling School of Art and Design for providing valuable input in the planning of our Xgrid testing. We would also like to thank the posters of the Xgrid Mailing List [13] for providing insight into many areas of Xgrid.

Technological advances in areas of storage and wireless communications have now made it feasible to envision on-demand delivery of data items, for e.g., video and audio clips, in vehicular peer-topeer networks. In prior work, Ghandeharizadeh et al. [10] introduce the concept of vehicles equipped with a Car-to-Car-Peer-toPeer device, termed AutoMata, for in-vehicle entertainment. The notable features of an AutoMata include a mass storage device offering hundreds of gigabytes (GB) of storage, a fast processor, and several types of networking cards. Even with today"s 500 GB disk drives, a repository of diverse entertainment content may exceed the storage capacity of a single AutoMata. Such repositories constitute the focus of this study. To exchange data, we assume each AutoMata is configured with two types of networking cards: 1) a low-bandwidth networking card with a long radio-range in the order of miles that enables an AutoMata device to communicate with a nearby cellular or WiMax station, 2) a high-bandwidth networking card with a limited radio-range in the order of hundreds of feet.
The high bandwidth connection supports data rates in the order of tens to hundreds of Megabits per second and represents the ad-hoc peer to peer network between the vehicles. This is labelled as the data plane and is employed to exchange data items between devices. The low-bandwidth connection serves as the control plane, enabling AutoMata devices to exchange meta-data with one or more centralized servers. This connection offers bandwidths in the order of tens to hundreds of Kilobits per second. The centralized servers, termed dispatchers, compute schedules of data delivery along the data plane using the provided meta-data. These schedules are transmitted to the participating vehicles using the control plane. The technical feasibility of such a two-tier architecture is presented in [7], with preliminary results to demonstrate the bandwidth of the control plane is sufficient for exchange of control information needed for realizing such an application.
In a typical scenario, an AutoMata device presents a passenger with a list of data items1 , showing both the name of each data item and its availability latency. The latter, denoted as δ, is defined as the earliest time at which the client encounters a copy of its requested data item. A data item is available immediately when it resides in the local storage of the AutoMata device serving the request. Due to storage constraints, an AutoMata may not store the entire repository. In this case, availability latency is the time from when the user issues a request until when the AutoMata encounters another car containing the referenced data item. (The terms car and AutoMata are used interchangeably in this study.) The availability latency for an item is a function of the current location of the client, its destination and travel path, the mobility model of the AutoMata equipped vehicles, the number of replicas constructed for the different data items, and the placement of data item replicas across the vehicles. A method to improve the availability latency is to employ data carriers which transport a replica of the requested data item from a server car containing it to a client that requested it. These data carriers are termed ‘zebroids".
Selection of zebroids is facilitated by the two-tiered architecture.
The control plane enables centralized information gathering at a dispatcher present at a base-station.2 Some examples of control in1 Without loss of generality, the term data item might be either traditional media such as text or continuous media such as an audio or video clip. 2 There may be dispatchers deployed at a subset of the base-stations for fault-tolerance and robustness. Dispatchers between basestations may communicate via the wired infrastructure. 75 formation are currently active requests, travel path of the clients and their destinations, and paths of the other cars. For each client request, the dispatcher may choose a set of z carriers that collaborate to transfer a data item from a server to a client (z-relay zebroids).
Here, z is the number of zebroids such that 0 ≤ z < N, where N is the total number of cars. When z = 0 there are no carriers, requiring a server to deliver the data item directly to the client.
Otherwise, the chosen relay team of z zebroids hand over the data item transitively to one another to arrive at the client, thereby reducing availability latency (see Section 3.1 for details). To increase robustness, the dispatcher may employ multiple relay teams of z-carriers for every request. This may be useful in scenarios where the dispatcher has lower prediction accuracy in the information about the routes of the cars. Finally, storage constraints may require a zebroid to evict existing data items from its local storage to accommodate the client requested item.
In this study, we quantify the following main factors that affect availability latency in the presence of zebroids: (i) data item repository size, (ii) car density, (iii) storage capacity per car, (iv) client trip duration, (v) replacement scheme employed by the zebroids, and (vi) accuracy of the car route predictions. For a significant subset of these factors, we address some key questions pertaining to use of zebroids both via analysis and extensive simulations.
Our main findings are as follows. A naive random replacement policy employed by the zebroids shows competitive performance in terms of availability latency. With such a policy, substantial improvements in latency can be obtained with zebroids at a minimal replacement overhead. In more practical scenarios, where the dispatcher has inaccurate information about the car routes, zebroids continue to provide latency improvements. A surprising result is that changes in popularity of the data items do not impact the latency gains obtained with a simple instantiation of z-relay zebroids called one-instantaneous zebroids (see Section 3.1). This study suggests a number of interesting directions to be pursued to gain better understanding of design of carrier-based systems that improve availability latency.
Related Work: Replication in mobile ad-hoc networks has been a widely studied topic [11, 12, 15]. However, none of these studies employ zebroids as data carriers to reduce the latency of the client"s requests. Several novel and important studies such as ZebraNet [13], DakNet [14], Data Mules [16], Message Ferries [20], and Seek and Focus [17] have analyzed factors impacting intermittently connected networks consisting of data carriers similar in spirit to zebroids. Factors considered by each study are dictated by their assumed environment and target application. A novel characteristic of our study is the impact on availability latency for a given database repository of items. A detailed description of related works can be obtained in [9].
The rest of this paper is organized as follows. Section 2 provides an overview of the terminology along with the factors that impact availability latency in the presence of zebroids. Section 3 describes how the zebroids may be employed. Section 4 provides details of the analysis methodology employed to capture the performance with zebroids. Section 5 describes the details of the simulation environment used for evaluation. Section 6 enlists the key questions examined in this study and answers them via analysis and simulations. Finally, Section 7 presents brief conclusions and future research directions.
Table 1 summarizes the notation of the parameters used in the paper. Below we introduce some terminology used in the paper.
Assume a network of N AutoMata-equipped cars, each with storage capacity of α bytes. The total storage capacity of the system is ST =N ·α. There are T data items in the database, each with Database Parameters T Number of data items.
Si Size of data item i fi Frequency of access to data item i.
Replication Parameters Ri Normalized frequency of access to data item i ri Number of replicas for data item i n Characterizes a particular replication scheme. δi Average availability latency of data item i δagg Aggregate availability latency, δagg = T j=1 δj · fj AutoMata System Parameters G Number of cells in the map (2D-torus).
N Number of AutoMata devices in the system. α Storage capacity per AutoMata. γ Trip duration of the client AutoMata.
ST Total storage capacity of the AutoMata system, ST = N · α.
Table 1: Terms and their definitions size Si. The frequency of access to data item i is denoted as fi, with T j=1 fj = 1. Let the trip duration of the client AutoMata under consideration be γ.
We now define the normalized frequency of access to the data item i, denoted by Ri, as: Ri = (fi)n T j=1(fj)n ; 0 ≤ n ≤ ∞ (1) The exponent n characterizes a particular replication technique.
A square-root replication scheme is realized when n = 0.5 [5].
This serves as the base-line for comparison with the case when zebroids are deployed. Ri is normalized to a value between 0 and
min (N, max (1, Ri·N·α Si )). This captures the case when at least one copy of every data item must be present in the ad-hoc network at all times. In cases where a data item may be lost from the ad-hoc network, this equation becomes ri = min (N, max (0, Ri·N·α Si )).
In this case, a request for the lost data item may need to be satisfied by fetching the item from a remote server.
The availability latency for a data item i, denoted as δi, is defined as the earliest time at which a client AutoMata will find the first replica of the item accessible to it. If this condition is not satisfied, then we set δi to γ. This indicates that data item i was not available to the client during its journey. Note that since there is at least one replica in the system for every data item i, by setting γ to a large value we ensure that the client"s request for any data item i will be satisfied. However, in most practical circumstances γ may not be so large as to find every data item.
We are interested in the availability latency observed across all data items. Hence, we augment the average availability latency for every data item i with its fi to obtain the following weighted availability latency (δagg) metric: δagg = T i=1 δi · fi Next, we present our solution approach describing how zebroids are selected.
When a client references a data item missing from its local storage, the dispatcher identifies all cars with a copy of the data item as servers. Next, the dispatcher obtains the future routes of all cars for a finite time duration equivalent to the maximum time the client is willing to wait for its request to be serviced. Using this information, the dispatcher schedules the quickest delivery path from any of the servers to the client using any other cars as intermediate carriers. Hence, it determines the optimal set of forwarding decisions 76 that will enable the data item to be delivered to the client in the minimum amount of time. Note that the latency along the quickest delivery path that employs a relay team of z zebroids is similar to that obtained with epidemic routing [19] under the assumptions of infinite storage and no interference.
A simple instantiation of z-relay zebroids occurs when z = 1 and the client"s request triggers a transfer of a copy of the requested data item from a server to a zebroid in its vicinity. Such a zebroid is termed one-instantaneous zebroid. In some cases, the dispatcher might have inaccurate information about the routes of the cars. Hence, a zebroid scheduled on the basis of this inaccurate information may not rendezvous with its target client. To minimize the likelihood of such scenarios, the dispatcher may schedule multiple zebroids. This may incur additional overhead due to redundant resource utilization to obtain the same latency improvements.
The time required to transfer a data item from a server to a zebroid depends on its size and the available link bandwidth. With small data items, it is reasonable to assume that this transfer time is small, especially in the presence of the high bandwidth data plane.
Large data items may be divided into smaller chunks enabling the dispatcher to schedule one or more zebroids to deliver each chunk to a client in a timely manner. This remains a future research direction.
Initially, number of replicas for each data item replicas might be computed using Equation 1. This scheme computes the number of data item replicas as a function of their popularity. It is static because number of replicas in the system do not change and no replacements are performed. Hence, this is referred to as the ‘nozebroids" environment. We quantify the performance of the various replacement policies with reference to this base-line that does not employ zebroids.
One may assume a cold start phase, where initially only one or few copies of every data item exist in the system. Many storage slots of the cars may be unoccupied. When the cars encounter one another they construct new replicas of some selected data items to occupy the empty slots. The selection procedure may be to choose the data items uniformly at random. New replicas are created as long as a car has a certain threshold of its storage unoccupied.
Eventually, majority of the storage capacity of a car will be exhausted.
The replacement policies considered in this paper are reactive since a replacement occurs only in response to a request issued for a certain data item. When the local storage of a zebroid is completely occupied, it needs to replace one of its existing items to carry the client requested data item. For this purpose, the zebroid must select an appropriate candidate for eviction. This decision process is analogous to that encountered in operating system paging where the goal is to maximize the cache hit ratio to prevent disk access delay [18]. The carrier-based replacement policies employed in our study are Least Recently Used (LRU), Least Frequently Used (LFU) and Random (where a eviction candidate is chosen uniformly at random). We have considered local and global variants of the LRU/LFU policies which determine whether local or global knowledge of contents of the cars known at the dispatcher is used for the eviction decision at a zebroid (see [9] for more details).
The replacement policies incur the following overheads. First, the complexity associated with the implementation of a policy.
Second, the bandwidth used to transfer a copy of a data item from a server to the zebroid. Third, the average number of replacements incurred by the zebroids. Note that in the no-zebroids case neither overhead is incurred.
The metrics considered in this study are aggregate availability latency, δagg, percentage improvement in δagg with zebroids as compared to the no-zebroids case and average number of replacements incurred per client request which is an indicator of the overhead incurred by zebroids.
Note that the dispatchers with the help of the control plane may ensure that no data item is lost from the system. In other words, at least one replica of every data item is maintained in the ad-hoc network at all times. In such cases, even though a car may meet a requesting client earlier than other servers, if its local storage contains data items with only a single copy in the system, then such a car is not chosen as a zebroid.
Here, we present the analytical evaluation methodology and some approximations as closed-form equations that capture the improvements in availability latency that can be obtained with both oneinstantaneous and z-relay zebroids. First, we present some preliminaries of our analysis methodology. • Let N be the number of cars in the network performing a 2D random walk on a √ G× √ G torus. An additional car serves as a client yielding a total of N + 1 cars. Such a mobility model has been used widely in the literature [17, 16] chiefly because it is amenable to analysis and provides a baseline against which performance of other mobility models can be compared. Moreover, this class of Markovian mobility models has been used to model the movements of vehicles [3, 21]. • We assume that all cars start from the stationary distribution and perform independent random walks. Although for sparse density scenarios, the independence assumption does hold, it is no longer valid when N approaches G. • Let the size of data item repository of interest be T. Also, data item i has ri replicas. This implies ri cars, identified as servers, have a copy of this data item when the client requests item i.
All analysis results presented in this section are obtained assuming that the client is willing to wait as long as it takes for its request to be satisfied (unbounded trip duration γ = ∞). With the random walk mobility model on a 2D-torus, there is a guarantee that as long as there is at least one replica of the requested data item in the network, the client will eventually encounter this replica [2].
Extensions to the analysis that also consider finite trip durations can be obtained in [9].
Consider a scenario where no zebroids are employed. In this case, the expected availability latency for the data item is the expected meeting time of the random walk undertaken by the client with any of the random walks performed by the servers. Aldous et al. [2] show that the the meeting time of two random walks in such a setting can be modelled as an exponential distribution with the mean C = c · G · log G, where the constant c 0.17 for G ≥ 25.
The meeting time, or equivalently the availability latency δi, for the client requesting data item i is the time till it encounters any of these ri replicas for the first time. This is also an exponential distribution with the following expected value (note that this formulation is valid only for sparse cases when G >> ri): δi = cGlogG ri The aggregate availability latency without employing zebroids is then this expression averaged over all data items, weighted by their frequency of access: δagg(no − zeb) = T i=1 fi · c · G · log G ri = T i=1 fi · C ri (2) 77
Recall that with one-instantaneous zebroids, for a given request, a new replica is created on a car in the vicinity of the server, provided this car meets the client earlier than any of the ri servers.
Moreover, this replica is spawned at the time step when the client issues the request. Let Nc i be the expected total number of nodes that are in the same cell as any of the ri servers. Then, we have Nc i = (N − ri) · (1 − (1 − 1 G )ri ) (3) In the analytical model, we assume that Nc i new replicas are created, so that the total number of replicas is increased to ri +Nc i .
The availability latency is reduced since the client is more likely to meet a replica earlier. The aggregated expected availability latency in the case of one-instantaneous zebroids is then given by, δagg(zeb) = T i=1 fi · c · G · log G ri + Nc i = T i=1 fi · C ri + Nc i (4) Note that in obtaining this expression, for ease of analysis, we have assumed that the new replicas start from random locations in the torus (not necessarily from the same cell as the original ri servers). It thus treats all the Nc i carriers independently, just like the ri original servers. As we shall show below by comparison with simulations, this approximation provides an upper-bound on the improvements that can be obtained because it results in a lower expected latency at the client.
It should be noted that the procedure listed above will yield a similar latency to that employed by a dispatcher employing oneinstantaneous zebroids (see Section 3.1). Since the dispatcher is aware of all future car movements it would only transfer the requested data item on a single zebroid, if it determines that the zebroid will meet the client earlier than any other server. This selected zebroid is included in the Nc i new replicas.
To calculate the expected availability latency with z-relay zebroids, we use a coloring problem analog similar to an approach used by Spyropoulos et al. [17]. Details of the procedure to obtain a closed-form expression are given in [9]. The aggregate availability latency (δagg) with z-relay zebroids is given by, δagg(zeb) = T i=1 [fi · C N + 1 · 1 N + 1 − ri · (N · log N ri − log (N + 1 − ri))] (5)
The simulation environment considered in this study comprises of vehicles such as cars that carry a fraction of the data item repository. A prediction accuracy parameter inherently provides a certain probabilistic guarantee on the confidence of the car route predictions known at the dispatcher. A value of 100% implies that the exact routes of all cars are known at all times. A 70% value for this parameter indicates that the routes predicted for the cars will match the actual ones with probability 0.7. Note that this probability is spread across the car routes for the entire trip duration. We now provide the preliminaries of the simulation study and then describe the parameter settings used in our experiments. • Similar to the analysis methodology, the map used is a 2D torus. A Markov mobility model representing a unbiased 2D random walk on the surface of the torus describes the movement of the cars across this torus. • Each grid/cell is a unique state of this Markov chain. In each time slot, every car makes a transition from a cell to any of its neighboring 8 cells. The transition is a function of the current location of the car and a probability transition matrix Q = [qij] where qij is the probability of transition from state i to state j. Only AutoMata equipped cars within the same cell may communicate with each other. • The parameters γ, δ have been discretized and expressed in terms of the number of time slots. • An AutoMata device does not maintain more than one replica of a data item. This is because additional replicas occupy storage without providing benefits. • Either one-instantaneous or z-relay zebroids may be employed per client request for latency improvement. • Unless otherwise mentioned, the prediction accuracy parameter is assumed to be 100%. This is because this study aims to quantify the effect of a large number of parameters individually on availability latency.
Here, we set the size of every data item, Si, to be 1. α represents the number of storage slots per AutoMata. Each storage slot stores one data item. γ represents the duration of the client"s journey in terms of the number of time slots. Hence the possible values of availability latency are between 0 and γ. δ is defined as the number of time slots after which a client AutoMata device will encounter a replica of the data item for the first time. If a replica for the data item requested was encountered by the client in the first cell then we set δ = 0. If δ > γ then we set δ = γ indicating that no copy of the requested data item was encountered by the client during its entire journey. In all our simulations, for illustration we consider a
trends in the results scale to maps of larger size.
We simulated a skewed distribution of access to the T data items that obeys Zipf"s law with a mean of 0.27. This distribution is shown to correspond to sale of movie theater tickets in the United States [6]. We employ a replication scheme that allocates replicas for a data item as a function of the square-root of the frequency of access of that item. The square-root replication scheme is shown to have competitive latency performance over a large parameter space [8]. The data item replicas are distributed uniformly across the AutoMata devices. This serves as the base-line no-zebroids case. The square-root scheme also provides the initial replica distribution when zebroids are employed. Note that the replacements performed by the zebroids will cause changes to the data item replica distribution. Requests generated as per the Zipf distribution are issued one at a time. The client car that issues the request is chosen in a round-robin manner. After a maximum period of γ, the latency encountered by this request is recorded.
In all the simulation results, each point is an average of 200,000 requests. Moreover, the 95% confidence intervals determined for the results are quite tight for the metrics of latency and replacement overhead. Hence, we only present them for the metric that captures the percentage improvement in latency with respect to the no-zebroids case.
In this section, we describe our evaluation results where the following key questions are addressed. With a wide choice of replacement schemes available for a zebroid, what is their effect on availability latency? A more central question is: Do zebroids provide 78
2
3
Number of cars Aggregate availability latency (δ agg ) lru_global lfu_global lru_local lfu_local random Figure 1: Figure 1 shows the availability latency when employing one-instantaneous zebroids as a function of (N,α) values, when the total storage in the system is kept fixed, ST = 200. significant improvements in availability latency? What is the associated overhead incurred in employing these zebroids? What happens to these improvements in scenarios where a dispatcher may have imperfect information about the car routes? What inherent trade-offs exist between car density and storage per car with regards to their combined as well as individual effect on availability latency in the presence of zebroids? We present both simple analysis and detailed simulations to provide answers to these questions as well as gain insights into design of carrier-based systems.
by a zebroid impact availability latency?
For illustration, we present ‘scale-up" experiments where oneinstantaneous zebroids are employed (see Figure 1). By scale-up, we mean that α and N are changed proportionally to keep the total system storage, ST , constant. Here, T = 50 and ST = 200. We choose the following values of (N,α) = {(20,10), (25,8), (50,4), (100,2)}. The figure indicates that a random replacement scheme shows competitive performance. This is because of several reasons.
Recall that the initial replica distribution is set as per the squareroot replication scheme. The random replacement scheme does not alter this distribution since it makes replacements blind to the popularity of a data item. However, the replacements cause dynamic data re-organization so as to better serve the currently active request. Moreover, the mobility pattern of the cars is random, hence, the locations from which the requests are issued by clients are also random and not known a priori at the dispatcher. These findings are significant because a random replacement policy can be implemented in a simple decentralized manner.
The lru-global and lfu-global schemes provide a latency performance that is worse than random. This is because these policies rapidly develop a preference for the more popular data items thereby creating a larger number of replicas for them. During eviction, the more popular data items are almost never selected as a replacement candidate. Consequently, there are fewer replicas for the less popular items. Hence, the initial distribution of the data item replicas changes from square-root to that resembling linear replication. The higher number of replicas for the popular data items provide marginal additional benefits, while the lower number of replicas for the other data items hurts the latency performance of these global policies. The lfu-local and lru-local schemes have similar performance to random since they do not have enough history of local data item requests. We speculate that the performance of these local policies will approach that of their global variants for a large enough history of data item requests. On account of the competitive performance shown by a random policy, for the remainder of the paper, we present the performance of zebroids that employ a random replacement policy.
improvements in availability latency?
We find that in many scenarios employing zebroids provides substantial improvements in availability latency.
We first consider the case of one-instantaneous zebroids.
Figure 2.a shows the variation in δagg as a function of N for T = 10 and α = 1 with a 10 × 10 torus using Equation 4. Both the x and y axes are drawn to a log-scale. Figure 2.b show the % improvement in δagg obtained with one-instantaneous zebroids. In this case, only the x-axis is drawn to a log-scale. For illustration, we assume that the T data items are requested uniformly.
Initially, when the network is sparse the analytical approximation for improvements in latency with zebroids, obtained from Equations 2 and 4, closely matches the simulation results. However, as N increases, the sparseness assumption for which the analysis is valid, namely N << G, is no longer true. Hence, the two curves rapidly diverge. The point at which the two curves move away from each other corresponds to a value of δagg ≤ 1. Moreover, as mentioned earlier, the analysis provides an upper bound on the latency improvements, as it treats the newly created replicas given by Nc i independently. However, these Nc i replicas start from the same cell as one of the server replicas ri. Finally, the analysis captures a oneshot scenario where given an initial data item replica distribution, the availability latency is computed. The new replicas created do not affect future requests from the client.
On account of space limitations, here, we summarize the observations in the case when z-relay zebroids are employed. The interested reader can obtain further details in [9]. Similar observations, like the one-instantaneous zebroid case, apply since the simulation and analysis curves again start diverging when the analysis assumptions are no longer valid. However, the key observation here is that the latency improvement with z-relay zebroids is significantly better than the one-instantaneous zebroids case, especially for lower storage scenarios. This is because in sparse scenarios, the transitive hand-offs between the zebroids creates higher number of replicas for the requested data item, yielding lower availability latency.
Moreover, it is also seen that the simulation validation curve for the improvements in δagg with z-relay zebroids approaches that of the one-instantaneous zebroid case for higher storage (higher N values). This is because one-instantaneous zebroids are a special case of z-relay zebroids.
We conduct simulations to examine the entire storage spectrum obtained by changing car density N or storage per car α to also capture scenarios where the sparseness assumptions for which the analysis is valid do not hold. We separate the effect of N and α by capturing the variation of N while keeping α constant (case 1) and vice-versa (case 2) both with z-relay and one-instantaneous zebroids. Here, we set the repository size as T = 25. Figure 3 captures case 1 mentioned above. Similar trends are observed with case 2, a complete description of those results are available in [9].
With Figure 3.b, keeping α constant, initially increasing car density has higher latency benefits because increasing N introduces more zebroids in the system. As N is further increased, ω reduces because the total storage in the system goes up. Consequently, the number of replicas per data item goes up thereby increasing the 79 number of servers. Hence, the replacement policy cannot find a zebroid as often to transport the requested data item to the client earlier than any of the servers. On the other hand, the increased number of servers benefits the no-zebroids case in bringing δagg down. The net effect results in reduction in ω for larger values of N. 10 1 10 2 10 3 10 −1 10 0 10 1 10 2 Number of cars no−zebroidsanal no−zebroids sim one−instantaneous anal one−instantaneoussim Aggregate Availability latency (δagg )
10 1 10 2 10 3 0 10 20 30 40 50 60 70 80 90 100 Number of cars % Improvement in δagg wrt no−zebroids (ω) analytical upper−bound simulation
Figure 2: Figure 2 shows the latency performance with oneinstantaneous zebroids via simulations along with the analytical approximation for a 10 × 10 torus with T = 10.
The trends mentioned above are similar to that obtained from the analysis. However, somewhat counter-intuitively with relatively higher system storage, z-relay zebroids provide slightly lower improvements in latency as compared to one-instantaneous zebroids.
We speculate that this is due to the different data item replica distributions enforced by them. Note that replacements performed by the zebroids cause fluctuations in these replica distributions which may effect future client requests. We are currently exploring suitable choices of parameters that can capture these changing replica distributions.
improvements in latency with zebroids?
We find that the improvements in latency with zebroids are obtained at a minimal replacement overhead (< 1 per client request).
With one-instantaneous zebroids, for each client request a maximum of one zebroid is employed for latency improvement. Hence, the replacement overhead per client request can amount to a maximum of one. Recall that to calculate the latency with one-instantaneous
0 1 2 3 4 5 6 Number of cars Aggregate availability latency (δagg ) no−zebroids one−instantaneous z−relays
0 10 20 30 40 50 60 Number of cars % Improvement in δagg wrt no−zebroids (ω) one−instantaneous z−relays
Figure 3: Figure 3 depicts the latency performance with both one-instantaneous and z-relay zebroids as a function of the car density when α = 2 and T = 25. zebroids, Nc i new replicas are created in the same cell as the servers.
Now a replacement is only incurred if one of these Nc i newly created replicas meets the client earlier than any of the ri servers.
Let Xri and XNc i respectively be random variables that capture the minimum time till any of the ri and Nc i replicas meet the client.
Since Xri and XNc i are assumed to be independent, by the property of exponentially distributed random variables we have,
Overhead/request = 1 · P(XNc i < Xri ) + 0 · P(Xri ≤ XNc i ) (6) Overhead/request = ri C ri C + Nc i C = ri ri + Nc i (7) Recall that the number of replicas for data item i, ri, is a function of the total storage in the system i.e., ri = k·N ·α where k satisfies the constraint 1 ≤ ri ≤ N. Using this along with Equation 2, we get Overhead/request = 1 − G G + N · (1 − k · α) (8) Now if we keep the total system storage N · α constant since G and T are also constant, increasing N increases the replacement overhead. However, if N ·α is constant then increasing N causes α 80
0
Number of cars one−instantaneous zebroids Average number of replacements per request (N=20,α=10) (N=25,α=8) (N=50,α=4) (N=100,α=2) Figure 4: Figure 4 captures replacement overhead when employing one-instantaneous zebroids as a function of (N,α) values, when the total storage in the system is kept fixed, ST =
to go down. This implies that a higher replacement overhead is incurred for higher N and lower α values. Moreover, when ri = N, this means that every car has a replica of data item i. Hence, no zebroids are employed when this item is requested, yielding an overhead/request for this item as zero. Next, we present simulation results that validate our analysis hypothesis for the overhead associated with deployment of one-instantaneous zebroids.
Figure 4 shows the replacement overhead with one-instantaneous zebroids when (N,α) are varied while keeping the total system storage constant. The trends shown by the simulation are in agreement with those predicted by the analysis above. However, the total system storage can be changed either by varying car density (N) or storage per car (α). On account of similar trends, here we present the case when α is kept constant and N is varied (Figure 5). We refer the reader to [9] for the case when α is varied and N is held constant.
We present an intuitive argument for the behavior of the perrequest replacement overhead curves. When the storage is extremely scarce so that only one replica per data item exists in the AutoMata network, the number of replacements performed by the zebroids is zero since any replacement will cause a data item to be lost from the system. The dispatcher ensures that no data item is lost from the system. At the other end of the spectrum, if storage becomes so abundant that α = T then the entire data item repository can be replicated on every car. The number of replacements is again zero since each request can be satisfied locally. A similar scenario occurs if N is increased to such a large value that another car with the requested data item is always available in the vicinity of the client. However, there is a storage spectrum in the middle where replacements by the scheduled zebroids result in improvements in δagg (see Figure 3).
Moreover, we observe that for sparse storage scenarios, the higher improvements with z-relay zebroids are obtained at the cost of a higher replacement overhead when compared to the one-instantaneous zebroids case. This is because in the former case, each of these z zebroids selected along the lowest latency path to the client needs to perform a replacement. However, the replacement overhead is still less than 1 indicating that on an average less than one replacement per client request is needed even when z-relay zebroids are employed.
0
1 Number of cars z−relays one−instantaneous Average number of replacements per request Figure 5: Figure 5 shows the replacement overhead with zebroids for the cases when N is varied keeping α = 2.
0
1
2
3
4 Prediction percentage no−zebroids (N=50) one−instantaneous (N=50) z−relays (N=50) no−zebroids (N=200) one−instantaneous (N=200) z−relays (N=200) Aggregate Availability Latency (δagg ) Figure 6: Figure 6 shows δagg for different car densities as a function of the prediction accuracy metric with α = 2 and T =
with zebroids in scenarios with inaccuracies in the car route predictions?
We find that zebroids continue to provide improvements in availability latency even with lower accuracy in the car route predictions. We use a single parameter p to quantify the accuracy of the car route predictions.
Since p represents the probability that a car route predicted by the dispatcher matches the actual one, hence, the latency with zebroids can be approximated by, δerr agg = p · δagg(zeb) + (1 − p) · δagg(no − zeb) (9) δerr agg = p · δagg(zeb) + (1 − p) · C ri (10) Expressions for δagg(zeb) can be obtained from Equations 4 (one-instantaneous) or 5 (z-relay zebroids).
Figure 6 shows the variation in δagg as a function of this route prediction accuracy metric. We observe a smooth reduction in the 81 improvement in δagg as the prediction accuracy metric reduces. For zebroids that are scheduled but fail to rendezvous with the client due to the prediction error, we tag any such replacements made by the zebroids as failed. It is seen that failed replacements gradually increase as the prediction accuracy reduces.
improvements in availability latency with zebroids maximized?
Surprisingly, we find that the improvements in latency obtained with one-instantaneous zebroids are independent of the input distribution of the popularity of the data items.
The fractional difference (labelled ω) in the latency between the no-zebroids and one-instantaneous zebroids is obtained from equations 2, 3, and 4 as ω = T i=1 fi·C ri − T i=1 fi·C ri+(N−ri)·(1−(1− 1 G )ri ) T i=1 fi·C ri (11) Here C = c·G·log G. This captures the fractional improvement in the availability latency obtained by employing one-instantaneous zebroids. Let α = 1, making the total storage in the system ST = N. Assuming the initial replica distribution is as per the squareroot replication scheme, we have, ri = √ fi·N T j=1 √ fj . Hence, we get fi = K2 ·r2 i N2 , where K = T j=1 fj. Using this, along with the approximation (1 − x)n
above equation to get, ω = 1 − T i=1 ri 1+ N−ri G T i=1 ri In order to determine when the gains with one-instantaneous zebroids are maximized, we can frame an optimization problem as follows: Maximize ω, subject to T i=1 ri = ST THEOREM 1. With a square-root replication scheme, improvements obtained with one-instantaneous zebroids are independent of the input popularity distribution of the data items. (See [9] for proof)
We perform simulations with two different frequency distribution of data items: Uniform and Zipfian (with mean= 0.27).
Similar latency improvements with one-instantaneous zebroids are obtained in both cases. This result has important implications. In cases with biased popularity toward certain data items, the aggregate improvements in latency across all data item requests still remain the same. Even in scenarios where the frequency of access to the data items changes dynamically, zebroids will continue to provide similar latency improvements.
FUTURE RESEARCH DIRECTIONS In this study, we examined the improvements in latency that can be obtained in the presence of carriers that deliver a data item from a server to a client. We quantified the variation in availability latency as a function of a rich set of parameters such as car density, storage per car, title database size, and replacement policies employed by zebroids.
Below we summarize some key future research directions we intend to pursue. To better reflect reality we would like to validate the observations obtained from this study with some real world simulation traces of vehicular movements (for example using CORSIM [1]). This will also serve as a validation for the utility of the Markov mobility model used in this study. We are currently analyzing the performance of zebroids on a real world data set comprising of an ad-hoc network of buses moving around a small neighborhood in Amherst [4]. Zebroids may also be used for delivery of data items that carry delay sensitive information with a certain expiry. Extensions to zebroids that satisfy such application requirements presents an interesting future research direction.
This research was supported in part by an Annenberg fellowship and NSF grants numbered CNS-0435505 (NeTS NOSS), CNS-0347621 (CAREER), and IIS-0307908.
[1] Federal Highway Administration. Corridor simulation. Version 5.1, http://www.ops.fhwa.dot.gov/trafficanalysistools/cors im.htm. [2] D. Aldous and J. Fill. Reversible markov chains and random walks on graphs. Under preparation. [3] A. Bar-Noy, I. Kessler, and M. Sidi. Mobile Users: To Update or Not to Update. In IEEE Infocom, 1994. [4] J. Burgess, B. Gallagher, D. Jensen, and B. Levine. MaxProp: Routing for Vehicle-Based Disruption-Tolerant Networking. In IEEE Infocom, April 2006. [5] E. Cohen and S. Shenker. Replication Strategies in Unstructured Peer-to-Peer Networks. In SIGCOMM, 2002. [6] A. Dan, D. Dias, R. Mukherjee, D. Sitaram, and R. Tewari. Buffering and Caching in Large-Scale Video Servers. In COMPCON, 1995. [7] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari. PAVAN: A Policy Framework for Content Availabilty in Vehicular ad-hoc Networks. In VANET, New York, NY, USA, 2004. ACM Press. [8] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari.
Comparison of Replication Strategies for Content Availability in C2P2 networks. In MDM, May 2005. [9] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari. An Evaluation of Availability Latency in Carrier-based Vehicular ad-hoc Networks. Technical report, Department of Computer Science,
University of Southern California,CENG-2006-1, 2006. [10] S. Ghandeharizadeh and B. Krishnamachari. C2p2: A peer-to-peer network for on-demand automobile information services. In Globe.
IEEE, 2004. [11] T. Hara. Effective Replica Allocation in ad-hoc Networks for Improving Data Accessibility. In IEEE Infocom, 2001. [12] H. Hayashi, T. Hara, and S. Nishio. A Replica Allocation Method Adapting to Topology Changes in ad-hoc Networks. In DEXA, 2005. [13] P. Juang, H. Oki, Y. Wang, M. Martonosi, L. Peh, and D. Rubenstein.
Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet. SIGARCH Comput. Archit.
News, 2002. [14] A. Pentland, R. Fletcher, and A. Hasson. DakNet: Rethinking Connectivity in Developing Nations. Computer, 37(1):78-83, 2004. [15] F. Sailhan and V. Issarny. Cooperative Caching in ad-hoc Networks.
In MDM, 2003. [16] R. Shah, S. Roy, S. Jain, and W. Brunette. Data mules: Modeling and analysis of a three-tier architecture for sparse sensor networks.
Elsevier ad-hoc Networks Journal, 1, September 2003. [17] T. Spyropoulos, K. Psounis, and C. Raghavendra. Single-Copy Routing in Intermittently Connected Mobile Networks. In SECON,
April 2004. [18] A. Tanenbaum. Modern Operating Systems, 2nd Edition, Chapter 4,
Section 4.4 . Prentice Hall, 2001. [19] A. Vahdat and D. Becker. Epidemic routing for partially-connected ad-hoc networks. Technical report, Department of Computer Science,

The increasing ubiquity of wireless networks and decreasing cost of hardware is fueling a proliferation of mobile wireless handheld devices, both as standalone wireless Personal Digital Assistants (PDA) and popular integrated PDA/cell phone devices. These devices are enabling new forms of mobile computing and communication. Service providers are leveraging these devices to deliver pervasive web access, and mobile web users already often use these devices to access web-enabled information such as news, email, and localized travel guides and maps. It is likely that within a few years, most of the devices accessing the web will be mobile.
Users typically access web content by running a web browser and associated helper applications locally on the PDA.
Although native web browsers exist for PDAs, they deliver subpar performance and have a much smaller feature set and more limited functionality than their desktop computing counterparts [10]. As a result, PDA web browsers are often not able to display web content from web sites that leverage more advanced web technologies to deliver a richer web experience. This fundamental problem arises for two reasons. First, because PDAs have a completely different hardware/software environment from traditional desktop computers, web applications need to be rewritten and customized for PDAs if at all possible, duplicating development costs.
Because the desktop application market is larger and more mature, most development effort generally ends up being spent on desktop applications, resulting in greater functionality and performance than their PDA counterparts.
Second, PDAs have a more resource constrained environment than traditional desktop computers to provide a smaller form factor and longer battery life. Desktop web browsers are large, complex applications that are unable to run on a PDA. Instead, developers are forced to significantly strip down these web browsers to provide a usable PDA web browser, thereby crippling PDA browser functionality.
Thin-client computing provides an alternative approach for enabling pervasive web access from handheld devices.
A thin-client computing system consists of a server and a client that communicate over a network using a remote display protocol. The protocol enables graphical displays to be virtualized and served across a network to a client device, while application logic is executed on the server. Using the remote display protocol, the client transmits user input to the server, and the server returns screen updates of the applications from the server to the client. Using a thin-client model for mobile handheld devices, PDAs can become simple stateless clients that leverage the remote server capabilities to execute web browsers and other helper applications.
The thin-client model provides several important benefits for mobile wireless web. First, standard desktop web applications can be used to deliver web content to PDAs without rewriting or adapting applications to execute on a PDA, reducing development costs and leveraging existing software investments. Second, complex web applications can be executed on powerful servers instead of running stripped down versions on more resource constrained PDAs, providing greater functionality and better performance [10]. Third, web applications can take advantage of servers with faster networks and better connectivity, further boosting application performance. Fourth, PDAs can be even simpler devices since they do not need to perform complex application logic, potentially reducing energy consumption and extend143 ing battery life. Finally, PDA thin clients can be essentially stateless appliances that do not need to be backed up or restored, require almost no maintenance or upgrades, and do not store any sensitive data that can be lost or stolen. This model provides a viable avenue for medical organizations to comply with HIPAA regulations [6] while embracing mobile handhelds in their day to day operations.
Despite these potential advantages, thin clients have been unable to provide the full range of these benefits in delivering web applications to mobile handheld devices. Existing thin clients were not designed for PDAs and do not account for important usability issues in the context of small form factor devices, resulting in difficulty in navigating displayed web content. Furthermore, existing thin clients are ineffective at providing seamless mobility across the heterogeneous mix of device display sizes and resolutions. While existing thin clients can already provide faster performance than native PDA web browsers in delivering HTML web content, they do not effectively support more display-intensive web helper applications such as multimedia video, which is increasingly an integral part of available web content.
To harness the full potential of thin-client computing in providing mobile wireless web on PDAs, we have developed pTHINC (PDA THin-client InterNet Computing). pTHINC builds on our previous work on THINC [1] to provide a thinclient architecture for mobile handheld devices. pTHINC virtualizes and resizes the display on the server to efficiently deliver high-fidelity screen updates to a broad range of different clients, screen sizes, and screen orientations, including both portrait and landscape viewing modes. This enables pTHINC to provide the same persistent web session across different client devices. For example, pTHINC can provide the same web browsing session appropriately scaled for display on a desktop computer and a PDA so that the same cookies, bookmarks, and other meta-data are continuously available on both machines simultaneously. pTHINC"s virtual display approach leverages semantic information available in display commands, and client-side video hardware to provide more efficient remote display mechanisms that are crucial for supporting more display-intensive web applications. Given limited display resolution on PDAs, pTHINC maximizes the use of screen real estate for remote display by moving control functionality from the screen to readily available PDA control buttons, improving system usability.
We have implemented pTHINC on Windows Mobile and demonstrated that it works transparently with existing applications, window systems, and operating systems, and does not require modifying, recompiling, or relinking existing software. We have quantitatively evaluated pTHINC against local PDA web browsers and other thin-client approaches on Pocket PC devices. Our experimental results demonstrate that pTHINC provides superior web browsing performance and is the only PDA thin client that effectively supports crucial browser helper applications such as video playback.
This paper presents the design and implementation of pTHINC. Section 2 describes the overall usage model and usability characteristics of pTHINC. Section 3 presents the design and system architecture of pTHINC. Section 4 presents experimental results measuring the performance of pTHINC on web applications and comparing it against native PDA browsers and other popular PDA thin-client systems.
Section 5 discusses related work. Finally, we present some concluding remarks.
pTHINC is a thin-client system that consists of a simple client viewer application that runs on the PDA and a server that runs on a commodity PC. The server leverages more powerful PCs to to run web browsers and other application logic. The client takes user input from the PDA stylus and virtual keyboard and sends them to the server to pass to the applications. Screen updates are then sent back from the server to the client for display to the user.
When the pTHINC PDA client is started, the user is presented with a simple graphical interface where information such as server address and port, user authentication information, and session settings can be provided. pTHINC first attempts to connect to the server and perform the necessary handshaking. Once this process has been completed, pTHINC presents the user with the most recent display of his session. If the session does not exist, a new session is created. Existing sessions can be seamlessly continued without changes in the session setting or server configuration.
Unlike other thin-client systems, pTHINC provides a user with a persistent web session model in which a user can launch a session running a web browser and associated applications at the server, then disconnect from that session and reconnect to it again anytime. When a user reconnects to the session, all of the applications continue running where the user left off, so that the user can continue working as though he or she never disconnected. The ability to disconnect and reconnect to a session at anytime is an important benefit for mobile wireless PDA users which may have intermittent network connectivity. pTHINC"s persistent web session model enables a user to reconnect to a web session from devices other than the one on which the web session was originally initiated. This provides users with seamless mobility across different devices.
If a user loses his PDA, he can easily use another PDA to access his web session. Furthermore, pTHINC allows users to use non-PDA devices to access web sessions as well. A user can access the same persistent web session on a desktop PC as on a PDA, enabling a user to use the same web session from any computer. pTHINC"s persistent web session model addresses a key problem encountered by mobile web users, the lack of a common web environment across computers. Web browsers often store important information such as bookmarks, cookies, and history, which enable them to function in a much more useful manner. The problem that occurs when a user moves between computers is that this data, which is specific to a web browser installation, cannot move with the user.
Furthermore, web browsers often need helper applications to process different media content, and those applications may not be consistently available across all computers. pTHINC addresses this problem by enabling a user to remotely use the exact same web browser environment and helper applications from any computer. As a result, pTHINC can provide a common, consistent web browsing environment for mobile users across different devices without requiring them to attempt to repeatedly synchronize different web browsing environments across multiple machines.
To enable a user to access the same web session on different devices, pTHINC must provide mechanisms to support different display sizes and resolutions. Toward this end, pTHINC provides a zoom feature that enables a user to zoom in and out of a display and allows the display of a web 144 Figure 1: pTHINC shortcut keys session to be resized to fit the screen of the device being used. For example, if the server is running a web session at 1024×768 but the client is a PDA with a display resolution of 640×480, pTHINC will resize the desktop display to fit the full display in the smaller screen of the PDA. pTHINC provides the PDA user with the option to increase the size of the display by zooming in to different parts of the display.
Users are often familiar with the general layout of commonly visited websites, and are able to leverage this resizing feature to better navigate through web pages. For example, a user can zoom out of the display to view the entire page content and navigate hyperlinks, then zoom in to a region of interest for a better view.
To enable a user to access the same web session on different devices, pTHINC must also provide mechanisms to support different display orientations. In a desktop environment, users are typically accustomed to having displays presented in landscape mode where the screen width is larger than its height. However, in a PDA environment, the choice is not always obvious. Some users may prefer having the display in portrait mode, as it is easier to hold the device in their hands, while others may prefer landscape mode in order to minimize the amount of side-scrolling necessary to view a web page. To accommodate PDA user preferences, pTHINC provides an orientation feature that enables it to seamless rotate the display between landscape and portrait mode. The landscape mode is particularly useful for pTHINC users who frequently access their web sessions on both desktop and PDA devices, providing those users with the same familiar landscape setting across different devices.
Because screen space is a relatively scarce resource on PDAs, pTHINC runs in fullscreen mode to maximize the screen area available to display the web session. To be able to use all of the screen on the PDA and still allow the user to control and interact with it, pTHINC reuses the typical shortcut buttons found on PDAs to perform all the control functions available to the user. The buttons used by pTHINC do not require any OS environment changes; they are simply intercepted by the pTHINC client application when they are pressed. Figure 1 shows how pTHINC utilizes the shortcut buttons to provide easy navigation and improve the overall user experience. These buttons are not device specific, and the layout shown is common to widelyused PocketPC devices. pTHINC provides six shortcuts to support its usage model: • Rotate Screen: The record button on the left edge is used to rotate the screen between portrait and landscape mode each time the button is pressed. • Zoom Out: The leftmost button on the bottom front is used to zoom out the display of the web session providing a bird"s eye view of the web session. • Zoom In: The second leftmost button on the bottom front is used to zoom in the display of the web session to more clearly view content of interest. • Directional Scroll: The middle button on the bottom front is used to scroll around the display using a single control button in a way that is already familiar to PDA users. This feature is particularly useful when the user has zoomed in to a region of the display such that only part of the display is visible on the screen. • Show/Hide Keyboard: The second rightmost button on the bottom front is used to bring up a virtual keyboard drawn on the screen for devices which have no physical keyboard. The virtual keyboard uses standard PDA OS mechanisms, providing portability across different PDA environments. • Close Session: The rightmost button on the bottom front is used to disconnect from the pTHINC session. pTHINC uses the PDA touch screen, stylus, and standard user interface mechanisms to provide a user interface pointand-click metaphor similar to that provided by the mouse in a traditional desktop computing environment. pTHINC does not use a cursor since PDA environments do not provide one. Instead, a user can use the stylus to tap on different sections of the touch screen to indicate input focus. A single tap on the touch screen generates a corresponding single click mouse event. A double tap on the touch screen generates a corresponding double click mouse event. pTHINC provides two-button mouse emulation by using the stylus to press down on the screen for one second to generate a right mouse click. All of these actions are identical to the way users already interact with PDA applications in the common PocketPC environment. In web browsing, users can click on hyperlinks and focus on input boxes by simply tapping on the desired screen area of interest. Unlike local PDA web browsers and other PDA applications, pTHINC leverages more powerful desktop user interface metaphors to enable users to manipulate multiple open application windows instead of being limited to a single application window at any given moment. This provides increased browsing flexibility beyond what is currently available on PDA devices. Similar to a desktop environment, browser windows and other application windows can be moved around by pressing down and dragging the stylus similar to a mouse.
pTHINC builds on the THINC [1] remote display architecture to provide a thin-client system for PDAs. pTHINC virtualizes the display at the server by leveraging the video device abstraction layer, which sits below the window server and above the framebuffer. This is a well-defined, low-level, device-dependent layer that exposes the video hardware to the display system. pTHINC accomplishes this through a simple virtual display driver that intercepts drawing commands, packetizes, and sends them over the network. 145 While other thin-client approaches intercept display commands at other layers of the display subsystem, pTHINC"s display virtualization approach provides some key benefits in efficiently supporting PDA clients. For example, intercepting display commands at a higher layer between applications and the window system as is done by X [17] requires replicating and running a great deal of functionality on the PDA that is traditionally provided by the desktop window system. Given both the size and complexity of traditional window systems, attempting to replicate this functionality in the restricted PDA environment would have proven to be a daunting, and perhaps unfeasible task. Furthermore, applications and the window system often require tight synchronization in their operation and imposing a wireless network between them by running the applications on the server and the window system on the client would significantly degrade performance. On the other hand, intercepting at a lower layer by extracting pixels out of the framebuffer as they are rendered provides a simple solution that requires very little functionality on the PDA client, but can also result in degraded performance. The reason is that by the time the remote display server attempts to send screen updates, it has lost all semantic information that may have helped it encode efficiently, and it must resort to using a generic and expensive encoding mechanism on the server, as well as a potentially expensive decoding mechanism on the limited PDA client. In contrast to both the high and low level interception approaches, pTHINC"s approach of intercepting at the device driver provides an effective balance between client and server simplicity, and the ability to efficiently encode and decode screen updates.
By using a low-level virtual display approach, pTHINC can efficiently encode application display commands using only a small set of low-level commands. In a PDA environment, this set of commands provides a crucial component in maintaining the simplicity of the client in the resourceconstrained PDA environment. The display commands are shown in Table 1, and work as follows. COPY instructs the client to copy a region of the screen from its local framebuffer to another location. This command improves the user experience by accelerating scrolling and opaque window movement without having to resend screen data from the server.
SFILL, PFILL, and BITMAP are commands that paint a fixed-size region on the screen. They are useful for accelerating the display of solid window backgrounds, desktop patterns, backgrounds of web pages, text drawing, and certain operations in graphics manipulation programs. SFILL fills a sizable region on the screen with a single color. PFILL replicates a tile over a screen region. BITMAP performs a fill using a bitmap of ones and zeros as a stipple to apply a foreground and background color. Finally, RAW is used to transmit unencoded pixel data to be displayed verbatim on a region of the screen. This command is invoked as a last resort if the server is unable to employ any other command, and it is the only command that may be compressed to mitigate its impact on network bandwidth. pTHINC delivers its commands using a non-blocking, serverpush update mechanism, where as soon as display updates are generated on the server, they are sent to the client.
Clients are not required to explicitly request display updates, thus minimizing the impact that the typical varying network latency of wireless links may have on the responsiveness of the system. Keeping in mind that resource Command Description COPY Copy a frame buffer area to specified coordinates SFILL Fill an area with a given pixel color value PFILL Tile an area with a given pixel pattern BITMAP Fill a region using a bit pattern RAW Display raw pixel data at a given location Table 1: pTHINC Protocol Display Commands constrained PDAs and wireless networks may not be able to keep up with a fast server generating a large number of updates, pTHINC is able to coalesce, clip, and discard updates automatically if network loss or congestion occurs, or the client cannot keep up with the rate of updates. This type of behavior proves crucial in a web browsing environment, where for example, a page may be redrawn multiple times as it is rendered on the fly by the browser. In this case, the PDA will only receive and render the final result, which clearly is all the user is interesting in seeing. pTHINC prioritizes the delivery of updates to the PDA using a Shortest-Remaining-Size-First (SRSF) preemptive update scheduler. SRSF is analogous to Shortest-RemainingProcessing-Time scheduling, which is known to be optimal for minimizing mean response time in an interactive system.
In a web browsing environment, short jobs are associated with text and basic page layout components such as the page"s background, which are critical web content for the user. On the other hand, large jobs are often lower priority beautifying elements, or, even worse, web page banners and advertisements, which are of questionable value to the user as he or she is browsing the page. Using SRSF, pTHINC is able to maximize the utilization of the relatively scarce bandwidth available on the wireless connection between the PDA and the server.
To enable users to just as easily access their web browser and helper applications from a desktop computer at home as from a PDA while on the road, pTHINC provides a resize mechanism to zoom in and out of the display of a web session. pTHINC resizing is completely supported by the server, not the client. The server resamples updates to fit within the PDAs viewport before they are transmitted over the network. pTHINC uses Fant"s resampling algorithm to resize pixel updates. This provides smooth, visually pleasing updates with properly antialiasing and has only modest computational requirements. pTHINC"s resizing approach has a number of advantages.
First, it allows the PDA to leverage the vastly superior computational power of the server to use high quality resampling algorithms and produce higher quality updates for the PDA to display. Second, resizing the screen does not translate into additional resource requirements for the PDA, since it does not need to perform any additional work. Finally, better utilization of the wireless network is attained since rescaling the updates reduces their bandwidth requirements.
To enable users to orient their displays on a PDA to provide a viewing experience that best accommodates user preferences and the layout of web pages or applications, pTHINC provides a display rotation mechanism to switch between landscape and portrait viewing modes. pTHINC display rotation is completely supported by the client, not the server. pTHINC does not explicitly recalculate the ge146 ometry of display updates to perform rotation, which would be computationally expensive. Instead, pTHINC simply changes the way data is copied into the framebuffer to switch between display modes. When in portrait mode, data is copied along the rows of the framebuffer from left to right.
When in landscape mode, data is copied along the columns of the framebuffer from top to bottom. These very fast and simple techniques replace one set of copy operations with another and impose no performance overhead. pTHINC provides its own rotation mechanism to support a wide range of devices without imposing additional feature requirements on the PDA. Although some newer PDA devices provide native support for different orientations, this mechanism is not dynamic and requires the user to rotate the PDA"s entire user interface before starting the pTHINC client. Windows Mobile provides native API mechanisms for PDA applications to rotate their UI on the fly, but these mechanisms deliver poor performance and display quality as the rotation is performed naively and is not completely accurate.
Video has gradually become an integral part of the World Wide Web, and its presence will only continue to increase.
Web sites today not only use animated graphics and flash to deliver web content in an attractive manner, but also utilize streaming video to enrich the web interface. Users are able to view pre-recorded and live newscasts on CNN, watch sports highlights on ESPN, and even search through large collection of videos on Google Video. To allow applications to provide efficient video playback, interfaces have been created in display systems that allow video device drivers to expose their hardware capabilities back to the applications. pTHINC takes advantage of these interfaces and its virtual device driver approach to provide a virtual bridge between the remote client and its hardware and the applications, and transparently support video playback.
On top of this architecture, pTHINC uses the YUV colorspace to encode the video content, which provides a number of benefits. First, it has become increasingly common for PDA video hardware to natively support YUV and be able to perform the colorspace conversion and scaling automatically. As a result, pTHINC is able to provide fullscreen video playback without any performance hits. Second, the use of YUV allows for a more efficient representation of RGB data without loss of quality, by taking advantage of the human eye"s ability to better distinguish differences in brightness than in color. In particular, pTHINC uses the YV12 format, which allows full color RGB data to be encoded using just 12 bits per pixel. Third, YUV data is produced as one of the last steps of the decoding process of most video codecs, allowing pTHINC to provide video playback in a manner that is format independent. Finally, even if the PDA"s video hardware is unable to accelerate playback, the colorspace conversion process is simple enough that it does not impose unreasonable requirements on the PDA.
A more concrete example of how pTHINC leverages the PDA video hardware to support video playback can be seen in our prototype implementation on the popular Dell Axim X51v PDA, which is equipped with the Intel 2700G multimedia accelerator. In this case, pTHINC creates an offscreen buffer in video memory and writes and reads from this memory region data on the YV12 format. When a new video frame arrives, video data is copied from the buffer to Figure 2: Experimental Testbed an overlay surface in video memory, which is independent of the normal surface used for traditional drawing. As the YV12 data is put onto the overlay, the Intel accelerator automatically performs both colorspace conversion and scaling.
By using the overlay surface, pTHINC has no need to redraw the screen once video playback is over since the overlapped surface is unaffected. In addition, specific overlay regions can be manipulated by leveraging the video hardware, for example to perform hardware linear interpolation to smooth out the frame and display it fullscreen, and to do automatic rotation when the client runs in landscape mode.
We have implemented a pTHINC prototype that runs the client on widely-used Windows Mobile-based Pocket PC devices and the server on both Windows and Linux operating systems. To demonstrate its effectiveness in supporting mobile wireless web applications, we have measured its performance on web applications. We present experimental results on different PDA devices for two popular web applications, browsing web pages and playing video content from the web.
We compared pTHINC against native web applications running locally on the PDA to demonstrate the improvement that pTHINC can provide over the traditional fat-client approach. We also compared pTHINC against three of the most widely used thin clients that can run on PDAs, Citrix Meta-FrameXP [2], Microsoft Remote Desktop [3] and VNC (Virtual Network Computing) [16]. We follow common practice and refer to Citrix MetaFrameXP and Microsoft Remote Desktop by their respective remote display protocols, ICA (Independent Computing Architecture) and RDP (Remote Desktop Protocol).
We conducted our web experiments using two different wireless Pocket PC PDAs in an isolated Wi-Fi network testbed, as shown in Figure 2. The testbed consisted of two PDA client devices, a packet monitor, a thin-client server, and a web server. Except for the PDAs, all of the other machines were IBM Netfinity 4500R servers with dual 933 MHz Intel PIII CPUs and 512 MB RAM and were connected on a switched 100 Mbps FastEthernet network. The web server used was Apache 1.3.27, the network emulator was NISTNet 2.0.12, and the packet monitor was Ethereal 0.10.9. The PDA clients connected to the testbed through a 802.11b Lucent Orinoco AP-2000 wireless access point. All experiments using the wireless network were conducted within ten feet of the access point, so we considered the amount of packet loss to be negligible in our experiments.
Two Pocket PC PDAs were used to provide results across both older, less powerful models and newer higher performance models. The older model was a Dell Axim X5 with 147 Client 1024×768 640×480 Depth Resize Clip RDP no yes 8-bit no yes VNC yes yes 16-bit no no ICA yes yes 16-bit yes no pTHINC yes yes 24-bit yes no Table 2: Thin-client Testbed Configuration Setting a 400 MHz Intel XScale PXA255 CPU and 64 MB RAM running Windows Mobile 2003 and a Dell TrueMobile 1180
newer model was a Dell Axim X51v with a 624 MHz Intel XScale XPA270 CPU and 64 MB RAM running Windows Mobile 5.0 and integrated 802.11b wireless networking. The X51v has an Intel 2700G multimedia accelerator with 16MB video memory. Both PDAs are capable of 16-bit color but have different screen sizes and display resolutions. The X5 has a 3.5 inch diagonal screen with 240×320 resolution. The X51v has a 3.7 inch diagonal screen with 480×640.
The four thin clients that we used support different levels of display quality as summarized in Table 2. The RDP client only supports a fixed 640×480 display resolution on the server with 8-bit color depth, while other platforms provide higher levels of display quality. To provide a fair comparison across all platforms, we conducted our experiments with thin-client sessions configured for two possible resolutions, 1024×768 and 640×480. Both ICA and VNC were configured to use the native PDA resolution of 16-bit color depth. The current pTHINC prototype uses 24-bit color directly and the client downsamples updates to the 16-bit color depth available on the PDA. RDP was configured using only 8-bit color depth since it does not support any better color depth. Since both pTHINC and ICA provide the ability to view the display resized to fit the screen, we measured both clients with and without the display resized to fit the PDA screen. Each thin client was tested using landscape rather than portrait mode when available. All systems run on the X51v could run in landscape mode because the hardware provides a landscape mode feature. However, the X5 does not provide this functionality. Only pTHINC directly supports landscape mode, so it was the only system that could run in landscape mode on both the X5 and X51v.
To provide a fair comparison, we also standardized on common hardware and operating systems whenever possible.
All of the systems used the Netfinity server as the thin-client server. For the two systems designed for Windows servers,
ICA and RDP, we ran Windows 2003 Server on the server.
For the other systems which support X-based servers, VNC and pTHINC, we ran the Debian Linux Unstable distribution with the Linux 2.6.10 kernel on the server. We used the latest thin-client server versions available on each platform at the time of our experiments, namely Citrix MetaFrame XP Server for Windows Feature Release 3, Microsoft Remote Desktop built into Windows XP and Windows 2003 using RDP 5.2, and VNC 4.0.
We used two web application benchmarks for our experiments based on two common application scenarios, browsing web pages and playing video content from the web. Since many thin-client systems including two of the ones tested are closed and proprietary, we measured their performance in a noninvasive manner by capturing network traffic with a packet monitor and using a variant of slow-motion benchmarking [13] previously developed to measure thin-client performance in PDA environments [10]. This measurement methodology accounts for both the display decoupling that can occur between client and server in thin-client systems as well as client processing time, which may be significant in the case of PDAs.
To measure web browsing performance, we used a web browsing benchmark based on the Web Text Page Load Test from the Ziff-Davis i-Bench benchmark suite [7]. The benchmark consists of JavaScript controlled load of 55 pages from the web server. The pages contain both text and graphics with pages varying in size. The graphics are embedded images in GIF and JPEG formats. The original i-Bench benchmark was modified for slow-motion benchmarking by introducing delays of several seconds between the pages using JavaScript. Then two tests were run, one where delays where added between each page, and one where pages where loaded continuously without waiting for them to be displayed on the client. In the first test, delays were sufficiently adjusted in each case to ensure that each page could be received and displayed on the client completely without temporal overlap in transferring the data belonging to two consecutive pages. We used the packet monitor to record the packet traffic for each run of the benchmark, then used the timestamps of the first and last packet in the trace to obtain our latency measures [10]. The packet monitor also recorded the amount of data transmitted between the client and the server. The ratio between the data traffic in the two tests yields a scale factor. This scale factor shows the loss of data between the server and the client due to inability of the client to process the data quickly enough. The product of the scale factor with the latency measurement produces the true latency accounting for client processing time.
To run the web browsing benchmark, we used Mozilla Firefox 1.0.4 running on the thin-client server for the thin clients, and Windows Internet Explorer (IE) Mobile for 2003 and Mobile for 5.0 for the native browsers on the X5 and X51v PDAs, respectively. In all cases, the web browser used was sized to fill the entire display region available.
To measure video playback performance, we used a video benchmark that consisted of playing a 34.75s MPEG-1 video clip containing a mix of news and entertainment programming at full-screen resolution. The video clip is 5.11 MB and consists of 834 352x240 pixel frames with an ideal frame rate of 24 frames/sec. We measured video performance using slow-motion benchmarking by monitoring resulting packet traffic at two playback rates, 1 frames/second (fps) and 24 fps, and comparing the results to determine playback delays and frame drops that occur at 24 fps to measure overall video quality [13]. For example, 100% quality means that all video frames were played at real-time speed. On the other hand, 50% quality could mean that half the video data was dropped, or that the clip took twice as long to play even though all of the video data was displayed.
To run the video benchmark, we used Windows Media Player 9 for Windows-based thin-client servers, MPlayer 1.0 pre 6 for X-based thin-client servers, and Windows Media Player 9 Mobile and 10 Mobile for the native video players running locally on the X5 and X51v PDAs, respectively. In all cases, the video player used was sized to fill the entire display region available.
Figures 3 and 4 show the results of running the web brows148 0 1 10 100 pTHINC Resized pTHINCICA Resized ICAVNCRDPLOCAL Latency(s) Platform Axim X5 (640x480 or less) Axim X51v (640x480) Axim X5 (1024x768) Axim X51v (1024x768) Figure 3: Browsing Benchmark: Average Page Latency ing benchmark. For each platform, we show results for up to four different configurations, two on the X5 and two on the X51v, depending on whether each configuration was supported. However, not all platforms could support all configurations. The local browser only runs at the display resolution of the PDA, 480×680 or less for the X51v and the X5. RDP only runs at 640×480. Neither platform could support 1024×768 display resolution. ICA only ran on the X5 and could not run on the X51v because it did not work on Windows Mobile 5.
Figure 3 shows the average latency per web page for each platform. pTHINC provides the lowest average web browsing latency on both PDAs. On the X5, pTHINC performs up to 70 times better than other thin-client systems and 8 times better than the local browser. On the X51v, pTHINC performs up to 80 times better than other thin-client systems and 7 times better than the native browser. In fact, all of the thin clients except VNC outperform the local PDA browser, demonstrating the performance benefits of the thin-client approach. Usability studies have shown that web pages should take less than one second to download for the user to experience an uninterrupted web browsing experience [14]. The measurements show that only the thin clients deliver subsecond web page latencies. In contrast, the local browser requires more than 3 seconds on average per web page. The local browser performs worse since it needs to run a more limited web browser to process the HTML,
JavaScript, and do all the rendering using the limited capabilities of the PDA. The thin clients can take advantage of faster server hardware and a highly tuned web browser to process the web content much faster.
Figure 3 shows that RDP is the next fastest platform after pTHINC. However, RDP is only able to run at a fixed resolution of 640×480 and 8-bit color depth. Furthermore, RDP also clips the display to the size of the PDA screen so that it does not need to send updates that are not visible on the PDA screen. This provides a performance benefit assuming the remaining web content is not viewed, but degrades performance when a user scrolls around the display to view other web content. RDP achieves its performance with significantly lower display quality compared to the other thin clients and with additional display clipping not used by other systems. As a result, RDP performance alone does not provide a complete comparison with the other platforms. In contrast, pTHINC provides the fastest performance while at the same time providing equal or better display quality than the other systems. 0 1 10 100 1000 pTHINC Resized pTHINCICA Resized ICAVNCRDPLOCAL DataSize(KB) Platform Axim X5 (640x480 or less) Axim X51v (640x480) Axim X5 (1024x768) Axim X51v (1024x768) Figure 4: Browsing Benchmark: Average Page Data Transferred Since VNC and ICA provide similar display quality to pTHINC, these systems provide a more fair comparison of different thin-client approaches. ICA performs worse in part because it uses higher-level display primitives that require additional client processing costs. VNC performs worse in part because it loses display data due to its client-pull delivery mechanism and because of the client processing costs in decompressing raw pixel primitives. In both cases, their performance was limited in part because their PDA clients were unable to keep up with the rate at which web pages were being displayed.
Figure 3 also shows measurements for those thin clients that support resizing the display to fit the PDA screen, namely ICA and pTHINC. Resizing requires additional processing, which results in slower average web page latencies.
The measurements show that the additional delay incurred by ICA when resizing versus not resizing is much more substantial than for pTHINC. ICA performs resizing on the slower PDA client. In contrast, pTHINC leverage the more powerful server to do resizing, reducing the performance difference between resizing and not resizing. Unlike ICA, pTHINC is able to provide subsecond web page download latencies in both cases.
Figure 4 shows the data transferred in KB per page when running the slow-motion version of the tests. All of the platforms have modest data transfer requirements of roughly
bandwidth capacity of Wi-Fi networks. The measurements show that the local browser does not transfer the least amount of data. This is surprising as HTML is often considered to be a very compact representation of content. Instead, RDP is the most bandwidth efficient platform, largely as a result of using only 8-bit color depth and screen clipping so that it does not transfer the entire web page to the client. pTHINC overall has the largest data requirements, slightly more than VNC. This is largely a result of the current pTHINC prototype"s lack of native support for 16-bit color data in the wire protocol. However, this result also highlights pTHINC"s performance as it is faster than all other systems even while transferring more data. Furthermore, as newer PDA models support full 24-bit color, these results indicate that pTHINC will continue to provide good web browsing performance.
Since display usability and quality are as important as performance, Figures 5 to 8 compare screenshots of the different thin clients when displaying a web page, in this case from the popular BBC news website. Except for ICA, all of the screenshots were taken on the X51v in landscape mode 149 Figure 5: Browser Screenshot: RDP 640x480 Figure 6: Browser Screenshot: VNC 1024x768 Figure 7: Browser Screenshot: ICA Resized 1024x768 Figure 8: Browser Screenshot: pTHINC Resized 1024x768 using the maximum display resolution settings for each platform given in Table 2. The ICA screenshot was taken on the X5 since ICA does not run on the X51v. While the screenshots lack the visual fidelity of the actual device display, several observations can be made. Figure 5 shows that RDP does not support fullscreen mode and wastes lots of screen space for controls and UI elements, requiring the user to scroll around in order to access the full contents of the web browsing session. Figure 6 shows that VNC makes better use of the screen space and provides better display quality, but still forces the user to scroll around to view the web page due to its lack of resizing support. Figure 7 shows ICA"s ability to display the full web page given its resizing support, but that its lack of landscape capability and poorer resize algorithm significantly compromise display quality. In contrast, Figure 8 shows pTHINC using resizing to provide a high quality fullscreen display of the full width of the web page. pTHINC maximizes the entire viewing region by moving all controls to the PDA buttons. In addition, pTHINC leverages the server computational power to use a high quality resizing algorithm to resize the display to fit the PDA screen without significant overhead.
Figures 9 and 10 show the results of running the video playback benchmark. For each platform except ICA, we show results for an X5 and X51v configuration. ICA could not run on the X51v as noted earlier. The measurements were done using settings that reflected the environment a user would have to access a web session from both a desktop computer and a PDA. As such, a 1024×768 server display resolution was used whenever possible and the video was shown at fullscreen. RDP was limited to 640×480 display resolution as noted earlier. Since viewing the entire video display is the only really usable option, we resized the display to fit the PDA screen for those platforms that supported this feature, namely ICA and pTHINC.
Figure 9 shows the video quality for each platform. pTHINC is the only thin client able to provide perfect video playback quality, similar to the native PDA video player. All of the other thin clients deliver very poor video quality. With the exception of RDP on the X51v which provided unacceptable 35% video quality, none of the other systems were even able to achieve 10% video quality. VNC and ICA have the worst quality at 8% on the X5 device. pTHINC"s native video support enables superior video performance, while other thin clients suffer from their inability to distinguish video from normal display updates.
They attempt to apply ineffective and expensive compression algorithms on the video data and are unable to keep up with the stream of updates generated, resulting in dropped frames or long playback times. VNC suffers further from its client-pull update model because video frames are generated faster than the rate at which the client can process and send requests to the server to obtain the next display update. Figure 10 shows the total data transferred during 150 0% 20% 40% 60% 80% 100% pTHINCICAVNCRDPLOCAL VideoQuality Platform Axim X5 Axim X51v Figure 9: Video Benchmark: Fullscreen Video Quality 0 1 10 100 pTHINCICAVNCRDPLOCAL VideoDataSize(MB) Platform Axim X5 Axim X51v Figure 10: Video Benchmark: Fullscreen Video Data video playback for each system. The native player is the most bandwidth efficient platform, sending less than 6 MB of data, which corresponds to about 1.2 Mbps of bandwidth. pTHINC"s 100% video quality requires about 25 MB of data which corresponds to a bandwidth usage of less than 6 Mbps.
While the other thin clients send less data than THINC, they do so because they are dropping video data, resulting in degraded video quality.
Figures 11 to 14 compare screenshots of the different thin clients when displaying the video clip. Except for ICA, all of the screenshots were taken on the X51v in landscape mode using the maximum display resolution settings for each platform given in Table 2. The ICA screenshot was taken on the X5 since ICA does not run on the X51v. Figures 11 and 12 show that RDP and VNC are unable to display the entire video frame on the PDA screen. RDP wastes screen space for UI elements and VNC only shows the top corner of the video frame on the screen. Figure 13 shows that ICA provides resizing to display the entire video frame, but did not proportionally resize the video data, resulting in strange display artifacts. In contrast, Figure 14 shows pTHINC using resizing to provide a high quality fullscreen display of the entire video frame. pTHINC provides visually more appealing video display than RDP, VNC, or ICA.
Several studies have examined the web browsing performance of thin-client computing [13, 19, 10]. The ability for thin clients to improve web browsing performance on wireless PDAs was first quantitatively demonstrated in a previous study by one of the authors [10]. This study demonstrated that thin clients can provide both faster web browsing performance and greater web browsing functionality.
The study considered a wide range of web content including content from medical information systems. Our work builds on this previous study and consider important issues such as how usable existing thin clients are in PDA environments, the trade-offs between thin-client usability and performance, performance across different PDA devices, and the performance of thin clients on common web-related applications such as video.
Many thin clients have been developed and some have PDA clients, including Microsoft"s Remote Desktop [3],
Citrix MetraFrame XP [2], Virtual Network Computing [16, 12], GoToMyPC [5], and Tarantella [18]. These systems were first designed for desktop computing and retrofitted for PDAs. Unlike pTHINC, they do not address key system architecture and usability issues important for PDAs.
This limits their display quality, system performance, available screen space, and overall usability on PDAs. pTHINC builds on previous work by two of the authors on THINC [1], extending the server architecture and introducing a client interface and usage model to efficiently support PDA devices for mobile web applications.
Other approaches to improve the performance of mobile wireless web browsing have focused on using transcoding and caching proxies in conjunction with the fat client model [11, 9, 4, 8]. They work by pushing functionality to external proxies, and using specialized browsing applications on the PDA device that communicate with the proxy. Our thinclient approach differs fundamentally from these fat-client approaches by pushing all web browser logic to the server, leveraging existing investments in desktop web browsers and helper applications to work seamlessly with production systems without any additional proxy configuration or web browser modifications.
With the emergence of web browsing on small display devices, web sites have been redesigned using mechanisms like WAP and specialized native web browsers have been developed to tailor the needs of these devices. Recently, Opera has developed the Opera Mini [15] web browser, which uses an approach similar to the thin-client model to provide access across a number of mobile devices that would normally be incapable of running a web browser. Instead of requiring the device to process web pages, it uses a remote server to pre-process the page before sending it to the phone.
We have introduced pTHINC, a thin-client architecture for wireless PDAs. pTHINC provides key architectural and usability mechanisms such as server-side screen resizing, clientside screen rotation using simple copy techniques, YUV video support, and maximizing screen space for display updates and leveraging existing PDA control buttons for UI elements. pTHINC transparently supports traditional desktop browsers and their helper applications on PDA devices and desktop machines, providing mobile users with ubiquitous access to a consistent, personalized, and full-featured web environment across heterogeneous devices. We have implemented pTHINC and measured its performance on web applications compared to existing thin-client systems and native web applications. Our results on multiple mobile wireless devices demonstrate that pTHINC delivers web browsing performance up to 80 times better than existing thin-client systems, and 8 times better than a native PDA browser. In addition, pTHINC is the only PDA thin client 151 Figure 11: Video Screenshot: RDP 640x480 Figure 12: Video Screenshot: VNC 1024x768 Figure 13: Video Screenshot: ICA Resized 1024x768 Figure 14: Video Screenshot: pTHINC Resized 1024x768 that transparently provides full-screen, full frame rate video playback, making web sites with multimedia content accessible to mobile web users.
This work was supported in part by NSF ITR grants CCR0219943 and CNS-0426623, and an IBM SUR Award.
[1] R. Baratto, L. Kim, and J. Nieh. THINC: A Virtual Display Architecture for Thin-Client Computing. In Proceedings of the 20th ACM Symposium on Operating Systems Principles (SOSP), Oct. 2005. [2] Citrix Metaframe. http://www.citrix.com. [3] B. C. Cumberland, G. Carius, and A. Muir. Microsoft Windows NT Server 4.0, Terminal Server Edition: Technical Reference. Microsoft Press, Redmond, WA, 1999. [4] A. Fox, I. Goldberg, S. D. Gribble, and D. C. Lee.
Experience With Top Gun Wingman: A Proxy-Based Graphical Web Browser for the 3Com PalmPilot. In Proceedings of Middleware "98, Lake District, England,
September 1998, 1998. [5] GoToMyPC. http://www.gotomypc.com/. [6] Health Insurance Portability and Accountability Act. http://www.hhs.gov/ocr/hipaa/. [7] i-Bench version 1.5. http: //etestinglabs.com/benchmarks/i-bench/i-bench.asp. [8] A. Joshi. On proxy agents, mobility, and web access.
Mobile Networks and Applications, 5(4):233-241, 2000. [9] J. Kangasharju, Y. G. Kwon, and A. Ortega. Design and Implementation of a Soft Caching Proxy. Computer Networks and ISDN Systems, 30(22-23):2113-2121, 1998. [10] A. Lai, J. Nieh, B. Bohra, V. Nandikonda, A. P. Surana, and S. Varshneya. Improving Web Browsing on Wireless PDAs Using Thin-Client Computing. In Proceedings of the 13th International World Wide Web Conference (WWW),
May 2004. [11] A. Maheshwari, A. Sharma, K. Ramamritham, and P. Shenoy. TranSquid: Transcoding and caching proxy for heterogenous ecommerce environments. In Proceedings of the 12th IEEE Workshop on Research Issues in Data Engineering (RIDE "02), Feb. 2002. [12] .NET VNC Viewer for PocketPC. http://dotnetvnc.sourceforge.net/. [13] J. Nieh, S. J. Yang, and N. Novik. Measuring Thin-Client Performance Using Slow-Motion Benchmarking. ACM Trans. Computer Systems, 21(1):87-115, Feb. 2003. [14] J. Nielsen. Designing Web Usability. New Riders Publishing, Indianapolis, IN, 2000. [15] Opera Mini Browser. http://www.opera.com/products/mobile/operamini/. [16] T. Richardson, Q. Stafford-Fraser, K. R. Wood, and A. Hopper. Virtual Network Computing. IEEE Internet Computing, 2(1), Jan./Feb. 1998. [17] R. W. Scheifler and J. Gettys. The X Window System.
ACM Trans. Gr., 5(2):79-106, Apr. 1986. [18] Sun Secure Global Desktop. http://www.sun.com/software/products/sgd/. [19] S. J. Yang, J. Nieh, S. Krishnappa, A. Mohla, and M. Sajjadpour. Web Browsing Performance of Wireless Thin-Client Computing. In Proceedings of the 12th International World Wide Web Conference (WWW), May

A wireless sensor network (WSN) consists of a large number of in-situ battery-powered sensor nodes. A WSN can collect the data about physical phenomena of interest [1]. There are many potential applications of WSNs, including environmental monitoring and surveillance, etc. [1][11].
In many application scenarios, WSNs are employed to conduct surveillance tasks in adverse, or even worse, in hostile working environments. One major problem caused is that sensor nodes are subjected to failures. Therefore, fault tolerance of a WSN is critical.
One way to achieve fault tolerance is that a WSN should contain a large number of redundant nodes in order to tolerate node failures. It is vital to provide a mechanism that redundant nodes can be working in sleeping mode (i.e., major power-consuming units such as the transceiver of a redundant sensor node can be shut off) to save energy, and thus to prolong the network lifetime. Redundancy should be exploited as much as possible for the set of sensors that are currently taking charge in the surveillance work of the network area [6].
We find that the minimum distance between each pair of points normalized by the average distance between each pair of points serves as a good index to evaluate the distribution of the points. We call this index, denoted by ι, the normalized minimum distance. If points are moveable, we find that maximizing ι results in a honeycomb structure. The honeycomb structure poses that the coverage efficiency is the best if each point represents a sensor node that is providing surveillance work. Employing ι in coverage-related problems is thus deemed promising.
This enlightens us that maximizing ι is a good approach to select a set of sensors that are currently taking charge in the surveillance work of the network area. To explore the effectiveness of employing ι in coverage-related problems, we formulate a sensorgrouping problem for high-redundancy WSNs. An algorithm called Maximizing-ι Node-Deduction (MIND) is proposed in which redundant sensor nodes are removed to obtain a large ι for each set of sensors that are currently taking charge in the surveillance work of the network area. We also introduce another greedy solution called Incremental Coverage Quality Algorithm (ICQA) for this problem, which serves as a benchmark to evaluate MIND.
The main contribution of this paper is twofold. First, we introduce a novel index ι for evaluation of point-distribution. We show that maximizing ι of a WSN results in low redundancy of the network. Second, we formulate a general sensor-grouping problem for WSNs and provide a general sensing model. With the MIND algorithm we show that locally maximizing ι among each sensor node and its neighbors is a good approach to solve this problem.
This demonstrates a good application of employing ι in coveragerelated problems.
The rest of the paper is organized as follows. In Section 2, we introduce our point-distribution index ι. We survey related work and formulate a sensor-grouping problem together with a general sensing model in Section 3. Section 4 investigates the application of ι in this grouping problem. We propose MIND for this problem 1171 and introduce ICQA as a benchmark. In Section 5, we present our simulation results in which MIND and ICQA are compared.
Section 6 provides conclusion remarks.
ι: A POINT-DISTRIBUTION INDEX Suppose there are n points in a Euclidean space Ω. The coordinates of these points are denoted by xi (i = 1, ..., n).
It may be necessary to evaluate how the distribution of these points is. There are many metrics to achieve this goal. For example, the Mean Square Error from these points to their mean value can be employed to calculate how these points deviate from their mean (i.e., their central). In resource-sharing evaluation, the Global Fairness Index (GFI) is often employed to measure how even the resource distributes among these points [8], when xi represents the amount of resource that belong to point i. In WSNs, GFI is usually used to calculate how even the remaining energy of sensor nodes is.
When n is larger than 2 and the points do not all overlap (That points all overlap means xi = xj, ∀ i, j = 1, 2, ..., n). We propose a novel index called the normalized minimum distance, namely ι, to evaluate the distribution of the points. ι is the minimum distance between each pair of points normalized by the average distance between each pair of points. It is calculated by: ι = min(||xi − xj||) µ (∀ i, j = 1, 2, ..., n; and i = j) (1) where ||xi − xj|| denotes the Euclidean distance between point i and point j in Ω, the min(·) function calculates the minimum distance between each pair of points, and µ is the average distance between each pair of points, which is: µ = ( Pn i=1 Pn j=1,j=i ||xi − xj||) n(n − 1) (2) ι measures how well the points separate from one another.
Obviously, ι is in interval [0, 1]. ι is equal to 1 if and only if n is equal to 3 and these three points forms an equilateral triangle. ι is equal to zero if any two points overlap. ι is a very interesting value of a set of points. If we consider each xi (∀i = 1, ..., n) is a variable in Ω, how these n points would look like if ι is maximized?
An algorithm is implemented to generate the topology in which ι is locally maximized (The algorithm can be found in [19]). We consider a 2-dimensional space. We select n = 10, 20, 30, ..., 100 and perform this algorithm. In order to avoid that the algorithm converge to local optimum, we select different random seeds to generate the initial points for 1000 time and obtain the best one that results in the largest ι when the algorithm converges. Figure 1 demonstrates what the resulting topology looks like when n = 20 as an example.
Suppose each point represents a sensor node. If the sensor coverage model is the Boolean coverage model [15][17][18][14] and the coverage radius of each node is the same. It is exciting to see that this topology results in lowest redundancy because the Vonoroi diagram [2] formed by these nodes (A Vonoroi diagram formed by a set of nodes partitions a space into a set of convex polygons such that points inside a polygon are closest to only one particular node) is a honeycomb-like structure1 .
This enlightens us that ι may be employed to solve problems related to sensor-coverage of an area. In WSNs, it is desirable 1 This is how base stations of a wireless cellular network are deployed and why such a network is called a cellular one.
0 20 40 60 80 100 120 140 160 X Y Figure 1: Node Number = 20, ι = 0.435376 that the active sensor nodes that are performing surveillance task should separate from one another. Under the constraint that the sensing area should be covered, the more each node separates from the others, the less the redundancy of the coverage is. ι indicates the quality of such separation. It should be useful for approaches on sensor-coverage related problems.
In our following discussions, we will show the effectiveness of employing ι in sensor-grouping problem.
In many application scenarios, to achieve fault tolerance, a WSN contains a large number of redundant nodes in order to tolerate node failures. A node sleeping-working schedule scheme is therefore highly desired to exploit the redundancy of working sensors and let as many nodes as possible sleep.
Much work in the literature is on this issue [6]. Yan et al introduced a differentiated service in which a sensor node finds out its responsible working duration with cooperation of its neighbors to ensure the coverage of sampled points [17]. Ye et al developed PEAS in which sensor nodes wake up randomly over time, probe their neighboring nodes, and decide whether they should begin to take charge of surveillance work [18]. Xing et al exploited a probabilistic distributed detection model with a protocol called Coordinating Grid (Co-Grid) [16]. Wang et al designed an approach called Coverage Configuration Protocol (CCP) which introduced the notion that the coverage degree of intersection-points of the neighboring nodes" sensing-perimeters indicates the coverage of a convex region [15]. In our recent work [7], we also provided a sleeping configuration protocol, namely SSCP, in which sleeping eligibility of a sensor node is determined by a local Voronoi diagram. SSCP can provide different levels of redundancy to maintain different requirements of fault tolerance.
The major feature of the aforementioned protocols is that they employ online distributed and localized algorithms in which a sensor node determines its sleeping eligibility and/or sleeping time based on the coverage requirement of its sensing area with some information provided by its neighbors.
Another major approach for sensor node sleeping-working scheduling issue is to group sensor nodes. Sensor nodes in a network are divided into several disjoint sets. Each set of sensor nodes are able to maintain the required area surveillance work. The sensor nodes are scheduled according to which set they belong to. These sets work successively. Only one set of sensor nodes work at any time.
We call the issue sensor-grouping problem.
The major advantage of this approach is that it avoids the overhead caused by the processes of coordination of sensor nodes to make decision on whether a sensor node is a candidate to sleep or 1172 work and how long it should sleep or work. Such processes should be performed from time to time during the lifetime of a network in many online distributed and localized algorithms. The large overhead caused by such processes is the main drawback of the online distributed and localized algorithms. On the contrary, roughly speaking, this approach groups sensor nodes in one time and schedules when each set of sensor nodes should be on duty. It does not require frequent decision-making on working/sleeping eligibility2 .
In [13] by Slijepcevic et al, the sensing area is divided into regions. Sensor nodes are grouped with the most-constrained leastconstraining algorithm. It is a greedy algorithm in which the priority of selecting a given sensor is determined by how many uncovered regions this sensor covers and the redundancy caused by this sensor. In [5] by Cardei et al, disjoint sensor sets are modeled as disjoint dominating sets. Although maximum dominating sets computation is NP-complete, the authors proposed a graphcoloring based algorithm. Cardei et al also studied similar problem in the domain of covering target points in [4]. The NP-completeness of the problem is proved and a heuristic that computes the sets are proposed. These algorithms are centralized solutions of sensorgrouping problem.
However, global information (e.g., the location of each in-network sensor node) of a large scale WSN is also very expensive to obtained online. Also it is usually infeasible to obtain such information before sensor nodes are deployed. For example, sensor nodes are usually deployed in a random manner and the location of each in-network sensor node is determined only after a node is deployed.
The solution of sensor-grouping problem should only base on locally obtainable information of a sensor node. That is to say, nodes should determine which group they should join in a fully distributed way. Here locally obtainable information refers to a node"s local information and the information that can be directly obtained from its adjacent nodes, i.e., nodes within its communication range.
In Subsection 3.1, we provide a general problem formulation of the sensor-grouping problem. Distributed-solution requirement is formulated in this problem. It is followed by discussion in Subsection 3.2 on a general sensing model, which serves as a given condition of the sensor-grouping problem formulation.
To facilitate our discussions, the notations in our following discussions are described as follows. • n: The number in-network sensor nodes. • S(j) (j = 1, 2, ..., m): The jth set of sensor nodes where m is the number of sets. • L(i) (i = 1, 2, ..., n): The physical location of node i. • φ: The area monitored by the network: i.e., the sensing area of the network. • R: The sensing radius of a sensor node. We assume that a sensor node can only be responsible to monitor a circular area centered at the node with a radius equal to R. This is a usual assumption in work that addresses sensor-coverage related problems. We call this circular area the sensing area of a node.
We assume that each sensor node can know its approximate physical location. The approximate location information is obtainable if each sensor node carries a GPS receiver or if some localization algorithms are employed (e.g., [3]). 2 Note that if some nodes die, a re-grouping process might also be performed to exploit the remaining nodes in a set of sensor nodes.
How to provide this mechanism is beyond the scope of this paper and yet to be explored.
Problem 1. Given: • The set of each sensor node i"s sensing neighbors N(i) and the location of each member in N(i); • A sensing model which quantitatively describes how a point P in area φ is covered by sensor nodes that are responsible to monitor this point. We call this quantity the coverage quality of P. • The coverage quality requirement in φ, denoted by s. When the coverage of a point is larger than this threshold, we say this point is covered.
For each sensor node i, make a decision on which group S(j) it should join so that: • Area φ can be covered by sensor nodes in each set S(j) • m, the number of sets S(j) is maximized.
In this formulation, we call sensor nodes within a circular area centered at a sensor node i with a radius equal to 2 · R the sensing neighbors of node i. This is because sensors nodes in this area, together with node i, may be cooperative to ensure the coverage of a point inside node i"s sensing area.
We assume that the communication range of a sensor node is larger than 2 · R, which is also a general assumption in work that addresses sensor-coverage related problems. That is to say, the first given condition in Problem 1 is the information that can be obtained directly from a node"s adjacent nodes. It is therefore locally obtainable information. The last two given conditions in this problem formulation can be programmed into a node before it is deployed or by a node-programming protocol (e.g., [9]) during network runtime. Therefore, the given conditions can all be easily obtained by a sensor-grouping scheme with fully distributed implementation.
We reify this problem with a realistic sensing model in next subsection.
As WSNs are usually employed to monitor possible events in a given area, it is therefore a design requirement that an event occurring in the network area must/may be successfully detected by sensors.
This issue is usually formulated as how to ensure that an event signal omitted in an arbitrary point in the network area can be detected by sensor nodes. Obviously, a sensing model is required to address this problem so that how a point in the network area is covered can be modeled and quantified. Thus the coverage quality of a WSN can be evaluated.
Different applications of WSNs employ different types of sensors, which surely have widely different theoretical and physical characteristics. Therefore, to fulfill different application requirements, different sensing models should be constructed based on the characteristics of the sensors employed.
A simple theoretical sensing model is the Boolean sensing model [15][18][17][14]. Boolean sensing model assumes that a sensor node can always detect an event occurring in its responsible sensing area. But most sensors detect events according to the signal strength sensed. Event signals usually fade in relation to the physical distance between an event and the sensor. The larger the distance, the weaker the event signals that can be sensed by the sensor, which results in a reduction of the probability that the event can be successfully detected by the sensor.
As in WSNs, event signals are usually electromagnetic, acoustic, or photic signals, they fade exponentially with the increasing of 1173 their transmit distance. Specifically, the signal strength E(d) of an event that is received by a sensor node satisfies: E(d) = α dβ (3) where d is the physical distance from the event to the sensor node; α is related to the signal strength omitted by the event; and β is signal fading factor which is typically a positive number larger than or equal to 2. Usually, α and β are considered as constants.
Based on this notion, to be more reasonable, researchers propose collaborative sensing model to capture application requirements: Area coverage can be maintained by a set of collaborative sensor nodes: For a point with physical location L, the point is considered covered by the collaboration of i sensors (denoted by k1, ..., ki) if and only if the following two equations holds [7][10][12]. ∀j = 1, ..., i; L(kj) − L < R. (4) C(L) = iX j=1 (E( L(kj) − L ) > s. (5) C(L) is regarded as the coverage quality of location L in the network area [7][10][12].
However, we notice that defining the sensibility as the sum of the sensed signal strength by each collaborative sensor implies a very special application: Applications must employ the sum of the signal strength to achieve decision-making. To capture generally realistic application requirement, we modify the definition described in Equation (5). The model we adopt in this paper is described in details as follows.
We consider the probability P(L, kj ) that an event located at L can be detected by sensor kj is related to the signal strength sensed by kj. Formally,
P(L, kj) = γE(d) = δ ( L(kj) − L / + 1)β , (6) where γ is a constant and δ = γα is a constant too. normalizes the distance to a proper scale and the +1 item is to avoid infinite value of P(L, kj).
The probability that an event located at L can be detected by any collaborative sensors that satisfied Equation (4) is: P (L) = 1 − iY j=1 (1 − P(L, kj )). (7) As the detection probability P (L) reasonably determines how an event occurring at location L can be detected by the networks, it is a good measure of the coverage quality of location L in a WSN.
Specifically, Equation (5) is modified to: C(L) = P (L) = 1 − iY j=1 [1 − δ ( L(kj) − L / + 1)β ] > s. (8) To sum it up, we consider a point at location L is covered if Equation (4) and (8) hold.
ALGORITHM FOR SENSOR-GROUPING PROBLEM Before we process to introduce algorithms to solve the sensor grouping problem, let us define the margin (denoted by θ) of an area φ monitored by the network as the band-like marginal area of φ and all the points on the outer perimeter of θ is ρ distance away from all the points on the inner perimeter of θ. ρ is called the margin length.
In a practical network, sensor nodes are usually evenly deployed in the network area. Obviously, the number of sensor nodes that can sense an event occurring in the margin of the network is smaller than the number of sensor nodes that can sense an event occurring in other area of the network. Based on this consideration, in our algorithm design, we ensure the coverage quality of the network area except the margin. The information on φ and ρ is networkbased. Each in-network sensor node can be pre-programmed or on-line informed about φ and ρ, and thus calculate whether a point in its sensing area is in the margin or not.
The node-deduction process of our Maximizing-ι Node-Deduction Algorithm (MIND) is simple. A node i greedily maximizes ι of the sub-network composed by itself, its ungrouped sensing neighbors, and the neighbors that are in the same group of itself. Under the constraint that the coverage quality of its sensing area should be ensured, node i deletes nodes in this sub-network one by one. The candidate to be pruned satisfies that: • It is an ungrouped node. • The deletion of the node will not result in uncovered-points inside the sensing area of i.
A candidate is deleted if the deletion of the candidate results in largest ι of the sub-network compared to the deletion of other candidates. This node-deduction process continues until no candidate can be found. Then all the ungrouped sensing neighbors that are not deleted are grouped into the same group of node i. We call the sensing neighbors that are in the same group of node i the group sensing neighbors of node i. We then call node i a finished node, meaning that it has finished the above procedure and the sensing area of the node is covered. Those nodes that have not yet finished this procedure are called unfinished nodes.
The above procedure initiates at a random-selected node that is not in the margin. The node is grouped to the first group. It calculates the resulting group sensing neighbors of it based on the above procedure. It informs these group sensing neighbors that they are selected in the group. Then it hands over the above procedure to an unfinished group sensing neighbors that is the farthest from itself. This group sensing neighbor continues this procedure until no unfinished neighbor can be found. Then the first group is formed (Algorithmic description of this procedure can be found at [19]).
After a group is formed, another random-selected ungrouped node begins to group itself to the second group and initiates the above procedure. In this way, groups are formed one by one. When a node that involves in this algorithm found out that the coverage quality if its sensing area, except what overlaps the network margin, cannot be ensured even if all its ungrouped sensing neighbors are grouped into the same group as itself, the algorithm stops. MIND is based on locally obtainable information of sensor nodes. It is a distributed algorithm that serves as an approximate solution of Problem 1.
A Benchmark for MIND To evaluate the effectiveness of introducing ι in the sensor-group problem, another algorithm for sensor-group problem called Incremental Coverage Quality Algorithm (ICQA) is designed. Our aim 1174 is to evaluate how an idea, i.e., MIND, based on locally maximize ι performs.
In ICQA, a node-selecting process is as follows. A node i greedily selects an ungrouped sensing neighbor in the same group as itself one by one, and informs the neighbor it is selected in the group.
The criterion is: • The selected neighbor is responsible to provide surveillance work for some uncovered parts of node i"s sensing area. (i.e., the coverage quality requirement of the parts is not fulfilled if this neighbor is not selected.) • The selected neighbor results in highest improvement of the coverage quality of the neighbor"s sensing area.
The improvement of the coverage quality, mathematically, should be the integral of the the improvements of all points inside the neighbor"s sensing area. A numerical approximation is employed to calculate this improvement. Details are presented in our simulation study.
This node-selecting process continues until the sensing area of node i is entirely covered. In this way, node i"s group sensing neighbors are found. The above procedure is handed over as what MIND does and new groups are thus formed one by one. And the condition that ICQA stops is the same as MIND. ICQA is also based on locally obtainable information of sensor nodes. ICQA is also a distributed algorithm that serves as an approximate solution of Problem 1.
To evaluate the effectiveness of employing ι in sensor-grouping problem, we build simulation surveillance networks. We employ MIND and ICQA to group the in-network sensor nodes. We compare the grouping results with respect to how many groups both algorithms find and how the performance of the resulting groups are.
Detailed settings of the simulation networks are shown in Table
a uniform manner in the network area.
Table 1: The settings of the simulation networks Area of sensor field 400m*400m ρ 20m R 80m α, β, γ and 1.0, 2.0, 1.0 and 100.0 s 0.6 For evaluating the coverage quality of the sensing area of a node, we divide the sensing area of a node into several regions and regard the coverage quality of the central point in each region as a representative of the coverage quality of the region. This is a numerical approximation. Larger number of such regions results in better approximation. As sensor nodes are with low computational capacity, there is a tradeoff between the number of such regions and the precision of the resulting coverage quality of the sensing area of a node. In our simulation study, we set this number 12. For evaluating the improvement of coverage quality in ICQA, we sum up all the improvements at each region-center as the total improvement.
ICQA We set the total in-network node number to different values and let the networks perform MIND and ICQA. For each n, simulations run with several random seeds to generate different networks.
Results are averaged. Figure 2 shows the group numbers found in networks with different n"s.
0 5 10 15 20 25 30 35 40 45 50 Total in−network node number Totalnumberofgroupsfound ICQA MMNP Figure 2: The number of groups found by MIND and ICQA We can see that MIND always outperforms ICQA in terms of the number of groups formed. Obviously, the larger the number of groups can be formed, the more the redundancy of each group is exploited. This output shows that an approach like MIND that aim to maximize ι of the resulting topology can exploits redundancy well.
As an example, in case that n = 1500, the results of five networks are listed in Table 2.
Table 2: The grouping results of five networks with n = 1500 Net MIND ICQA MIND ICQA Group Number Group Number Average ι Average ι
The difference between the average ι of the groups in each network shows that groups formed by MIND result in topologies with larger ι"s. It demonstrates that ι is good indicator of redundancy in different networks.
Although MIND forms more groups than ICQA does, which implies longer lifetime of the networks, another importance consideration is how these groups formed by MIND and ICQA perform.
We let 10000 events randomly occur in the network area except the margin. We compare how many events happen at the locations where the quality is less than the requirement s = 0.6 when each resulting group is conducting surveillance work (We call the number of such events the failure number of group). Figure 3 shows the average failure numbers of the resulting groups when different node numbers are set.
We can see that the groups formed by MIND outperform those formed by ICQA because the groups formed by MIND result in lower failure numbers. This further demonstrates that MIND is a good approach for sensor-grouping problem. 1175
0 10 20 30 40 50 60 Total in−network node number averagefailurenumbers ICQA MMNP Figure 3: The failure numbers of MIND and ICQA
This paper proposes ι, a novel index for evaluation of pointdistribution. ι is the minimum distance between each pair of points normalized by the average distance between each pair of points.
We find that a set of points that achieve a maximum value of ι result in a honeycomb structure. We propose that ι can serve as a good index to evaluate the distribution of the points, which can be employed in coverage-related problems in wireless sensor networks (WSNs). We set out to validate this idea by employing ι to a sensorgrouping problem. We formulate a general sensor-grouping problem for WSNs and provide a general sensing model. With an algorithm called Maximizing-ι Node-Deduction (MIND), we show that maximizing ι at sensor nodes is a good approach to solve this problem. Simulation results verify that MIND outperforms a greedy algorithm that exploits sensor-redundancy we design in terms of the number and the performance of the groups formed. This demonstrates a good application of employing ι in coverage-related problems.
The work described in this paper was substantially supported by two grants, RGC Project No. CUHK4205/04E and UGC Project No. AoE/E-01/99, of the Hong Kong Special Administrative Region, China.
[1] I. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci.
A survey on wireless sensor networks. IEEE Communications Magazine, 40(8):102-114, 2002. [2] F. Aurenhammer. Vononoi diagram - a survey of a fundamental geometric data structure. ACM Computing Surveys, 23(2):345-405, September 1991. [3] N. Bulusu, J. Heidemann, and D. Estrin. GPS-less low-cost outdoor localization for very small devices. IEEE Personal Communication, October 2000. [4] M. Cardei and D.-Z. Du. Improving wireless sensor network lifetime through power aware organization. ACM Wireless Networks, 11(3), May 2005. [5] M. Cardei, D. MacCallum, X. Cheng, M. Min, X. Jia, D. Li, and D.-Z. Du. Wireless sensor networks with energy efficient organization. Journal of Interconnection Networks, 3(3-4),
December 2002. [6] M. Cardei and J. Wu. Coverage in wireless sensor networks.
In Handbook of Sensor Networks, (eds. M. Ilyas and I.
Magboub), CRC Press, 2004. [7] X. Chen and M. R. Lyu. A sensibility-based sleeping configuration protocol for dependable wireless sensor networks. CSE Technical Report, The Chinese University of Hong Kong, 2005. [8] R. Jain, W. Hawe, and D. Chiu. A quantitative measure of fairness and discrimination for resource allocation in shared computer systems. Technical Report DEC-TR-301,
September 1984. [9] S. S. Kulkarni and L. Wang. MNP: Multihop network reprogramming service for sensor networks. In Proc. of the 25th International Conference on Distributed Computing Systems (ICDCS), June 2005. [10] B. Liu and D. Towsley. A study on the coverage of large-scale sensor networks. In Proc. of the 1st IEEE International Conference on Mobile ad-hoc and Sensor Systems, Fort Lauderdale, FL, October 2004. [11] A. Mainwaring, J. Polastre, R. Szewczyk, D. Culler, and J. Anderson. Wireless sensor networks for habitat monitoring. In Proc. of the ACM International Workshop on Wireless Sensor Networks and Applications, 2002. [12] S. Megerian, F. Koushanfar, G. Qu, G. Veltri, and M. Potkonjak. Explosure in wirless sensor networks: Theory and pratical solutions. Wireless Networks, 8, 2002. [13] S. Slijepcevic and M. Potkonjak. Power efficient organization of wireless sensor networks. In Proc. of the IEEE International Conference on Communications (ICC), volume 2, Helsinki, Finland, June 2001. [14] D. Tian and N. D. Georganas. A node scheduling scheme for energy conservation in large wireless sensor networks.
Wireless Communications and Mobile Computing, 3:272-290, May 2003. [15] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless, and C. Gill.
Integrated coverage and connectivity configuration in wireless sensor networks. In Proc. of the 1st ACM International Conference on Embedded Networked Sensor Systems (SenSys), Los Angeles, CA, November 2003. [16] G. Xing, C. Lu, R. Pless, and J. A. O´ Sullivan. Co-Grid: an efficient converage maintenance protocol for distributed sensor networks. In Proc. of the 3rd International Symposium on Information Processing in Sensor Networks (IPSN), Berkeley, CA, April 2004. [17] T. Yan, T. He, and J. A. Stankovic. Differentiated surveillance for sensor networks. In Proc. of the 1st ACM International Conference on Embedded Networked Sensor Systems (SenSys), Los Angeles, CA, November 2003. [18] F. Ye, G. Zhong, J. Cheng, S. Lu, and L. Zhang. PEAS: A robust energy conserving protocol for long-lived sensor networks. In Proc. of the 23rd International Conference on Distributed Computing Systems (ICDCS), Providence, Rhode Island, May 2003. [19] Y. Zhou, H. Yang, and M. R. Lyu. A point-distribution index and its application in coverage-related problems. CSE Technical Report, The Chinese University of Hong Kong,

There has recently been a huge surge in the growth of wireless technology, driven primarily by the availability of unlicensed spectrum. However, this has come at the cost of increased RF interference, which has caused the Federal Communications Commission (FCC) in the United States to re-evaluate its strategy on spectrum allocation. Currently, the FCC has licensed RF spectrum to a variety of public and private institutions, termed primary users. New spectrum allocation regimes implemented by the FCC use dynamic spectrum access schemes to either negotiate or opportunistically allocate RF spectrum to unlicensed secondary users Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific D1 D2 D5 D3 D4 Primary User Shadowed Secondary Users Secondary Users detect Primary's Signal Shadowed Secondary User Figure 1: Without cooperation, shadowed users are not able to detect the presence of the primary user. that can use it when the primary user is absent. The second type of allocation scheme is termed opportunistic spectrum sharing. The FCC has already legislated this access method for the 5 GHz band and is also considering the same for TV broadcast bands [1]. As a result, a new wave of intelligent radios, termed cognitive radios (or software defined radios), is emerging that can dynamically re-tune their radio parameters based on interactions with their surrounding environment.
Under the new opportunistic allocation strategy, secondary users are obligated not to interfere with primary users (senders or receivers). This can be done by sensing the environment to detect the presence of primary users.
However, local sensing is not always adequate, especially in cases where a secondary user is shadowed from a primary user, as illustrated in Figure 1. Here, coordination between secondary users is the only way for shadowed users to detect the primary. In general, cooperation improves sensing accuracy by an order of magnitude when compared to not cooperating at all [5].
To realize this vision of dynamic spectrum access, two fundamental problems must be solved: (1) Efficient and coordinated spectrum sensing and (2) Distributed spectrum allocation. In this paper, we propose strategies for coordinated spectrum sensing that are low cost, operate on timescales comparable to the agility of the RF environment, and are resilient to network failures and alterations. We defer the problem of spectrum allocation to future work.
Spectrum sensing techniques for cognitive radio networks [4, 17] are broadly classified into three regimes; (1) centralized coordinated techniques, (2) decentralized coordinated techniques, and (3) decentralized uncoordinated techniques.
We advocate a decentralized coordinated approach, similar in spirit to OSPF link-state routing used in the Internet.
This is more effective than uncoordinated approaches because making decisions based only on local information is fallible (as shown in Figure 1). Moreover, compared to cen12 tralized approaches, decentralized techniques are more scalable, robust, and resistant to network failures and security attacks (e.g. jamming).
Coordinating sensory data between cognitive radio devices is technically challenging because accurately assessing spectrum usage requires exchanging potentially large amounts of data with many radios at very short time scales. Data size grows rapidly due to the large number (i.e. thousands) of spectrum bands that must be scanned. This data must also be exchanged between potentially hundreds of neighboring secondary users at short time scales, to account for rapid changes in the RF environment.
This paper presents GUESS, a novel approach to coordinated spectrum sensing for cognitive radio networks. Our approach is motivated by the following key observations:
devices have limited sensing resolution because they are low-cost and low duty-cycle devices and thus cannot perform complex RF signal processing (e.g. matched filtering). Many are typically equipped with simple energy detectors that gather only approximate information.
Approximate statistical summaries of sensed data are sufficient for correlating sensed information between radios, as relative usage information is more important than absolute usage data. Thus, exchanging exact RF information may not be necessary, and more importantly, too costly for the purposes of spectrum sensing.
RF spectrum utilization changes infrequently.
Moreover, utilization of a specific RF band affects only that band and not the entire spectrum. Therefore, if the usage pattern of a particular band changes substantially, nodes detecting that change can initiate an update protocol to update the information for that band alone, leaving in place information already collected for other bands. This allows rapid detection of change while saving the overhead of exchanging unnecessary information.
Based on these observations, GUESS makes the following contributions:
algorithms to the problem of coordinated spectrum sensing. These algorithms are well suited to coordinated spectrum sensing due to the unique characteristics of the problem: i.e. radios are power-limited, mobile and have limited bandwidth to support spectrum sensing capabilities.
dissemination of spectrum summaries. We argue that approximate summaries are adequate for performing accurate radio parameter tuning.
randomized gossiping to support incremental maintenance of spectrum summaries. Compared to standard gossiping approaches, incremental techniques can further reduce overhead and protocol execution time by requiring fewer radio resources.
The rest of the paper is organized as follows. Section 2 motivates the need for a low cost and efficient approach to coordinated spectrum sensing. Section 3 discusses related work in the area, while Section 4 provides a background on in-network aggregation and randomized gossiping. Sections
techniques for coordinated spectrum sensing. Section 7 presents simulation results showcasing the benefits of GUESS, and Section 8 presents a discussion and some directions for future work.
To estimate the scale of the problem, In-stat predicts that the number of WiFi-enabled devices sold annually alone will grow to 430 million by 2009 [2]. Therefore, it would be reasonable to assume that a typical dense urban environment will contain several thousand cognitive radio devices in range of each other. As a result, distributed spectrum sensing and allocation would become both important and fundamental.
Coordinated sensing among secondary radios is essential due to limited device sensing resolution and physical RF effects such as shadowing. Cabric et al. [5] illustrate the gains from cooperation and show an order of magnitude reduction in the probability of interference with the primary user when only a small fraction of secondary users cooperate.
However, such coordination is non-trivial due to: (1) the limited bandwidth available for coordination, (2) the need to communicate this information on short timescales, and (3) the large amount of sensory data that needs to be exchanged.
Limited Bandwidth: Due to restrictions of cost and power, most devices will likely not have dedicated hardware for supporting coordination. This implies that both data and sensory traffic will need to be time-multiplexed onto a single radio interface. Therefore, any time spent communicating sensory information takes away from the device"s ability to perform its intended function. Thus, any such coordination must incur minimal network overhead.
Short Timescales: Further compounding the problem is the need to immediately propagate updated RF sensory data, in order to allow devices to react to it in a timely fashion. This is especially true due to mobility, as rapid changes of the RF environment can occur due to device and obstacle movements. Here, fading and multi-path interference heavily impact sensing abilities. Signal level can drop to a deep null with just a λ/4 movement in receiver position (3.7 cm at 2 GHz), where λ is the wavelength [14]. Coordination which does not support rapid dissemination of information will not be able to account for such RF variations.
Large Sensory Data: Because cognitive radios can potentially use any part of the RF spectrum, there will be numerous channels that they need to scan. Suppose we wish to compute the average signal energy in each of 100 discretized frequency bands, and each signal can have up to 128 discrete energy levels. Exchanging complete sensory information between nodes would require 700 bits per transmission (for
Exchanging this information among even a small group of 50 devices each second would require (50 time-steps × 50 devices × 700 bits per transmission) = 1.67 Mbps of aggregate network bandwidth.
Contrast this to the use of a randomized gossip protocol to disseminate such information, and the use of FM bit vectors to perform in-network aggregation. By applying gossip and FM aggregation, aggregate bandwidth requirements drop to (c·logN time-steps × 50 devices × 700 bits per transmission) = 0.40 Mbps, since 12 time-steps are needed to propagate the data (with c = 2, for illustrative purpoes1 ). This is explained further in Section 4.
Based on these insights, we propose GUESS, a low-overhead approach which uses incremental extensions to FM aggregation and randomized gossiping for efficient coordination within a cognitive radio network. As we show in Section 7, 1 Convergence time is correlated with the connectivity topology of the devices, which in turn depends on the environment. 13 X A A X B B X Figure 2: Using FM aggregation to compute average signal level measured by a group of devices. these incremental extensions can further reduce bandwidth requirements by up to a factor of 2.4 over the standard approaches discussed above.
Research in cognitive radio has increased rapidly [4, 17] over the years, and it is being projected as one of the leading enabling technologies for wireless networks of the future [9].
As mentioned earlier, the FCC has already identified new regimes for spectrum sharing between primary users and secondary users and a variety of systems have been proposed in the literature to support such sharing [4, 17].
Detecting the presence of a primary user is non-trivial, especially a legacy primary user that is not cognitive radio aware. Secondary users must be able to detect the primary even if they cannot properly decode its signals. This has been shown by Sahai et al. [16] to be extremely difficult even if the modulation scheme is known. Sophisticated and costly hardware, beyond a simple energy detector, is required to improve signal detection accuracy [16]. Moreover, a shadowed secondary user may not even be able to detect signals from the primary. As a result, simple local sensing approaches have not gained much momentum. This has motivated the need for cooperation among cognitive radios [16].
More recently, some researchers have proposed approaches for radio coordination. Liu et al. [11] consider a centralized access point (or base station) architecture in which sensing information is forwarded to APs for spectrum allocation purposes. APs direct mobile clients to collect such sensing information on their behalf. However, due to the need of a fixed AP infrastructure, such a centralized approach is clearly not scalable.
In other work, Zhao et al. [17] propose a distributed coordination approach for spectrum sensing and allocation.
Cognitive radios organize into clusters and coordination occurs within clusters. The CORVUS [4] architecture proposes a similar clustering method that can use either a centralized or decentralized approach to manage clusters. Although an improvement over purely centralized approaches, these techniques still require a setup phase to generate the clusters, which not only adds additional delay, but also requires many of the secondary users to be static or quasi-static. In contrast, GUESS does not place such restrictions on secondary users, and can even function in highly mobile environments.
This section provides the background for our approach.
We present the FM aggregation scheme that we use to generate spectrum summaries and perform in-network aggregation. We also discuss randomized gossiping techniques for disseminating aggregates in a cognitive radio network.
Aggregation is the process where nodes in a distributed network combine data received from neighboring nodes with their local value to generate a combined aggregate. This aggregate is then communicated to other nodes in the network and this process repeats until the aggregate at all nodes has converged to the same value, i.e. the global aggregate. Double-counting is a well known problem in this process, where nodes may contribute more than once to the aggregate, causing inaccuracy in the final result. Intuitively, nodes can tag the aggregate value they transmit with information about which nodes have contributed to it. However, this approach is not scalable. Order and Duplicate Insensitive (ODI) techniques have been proposed in the literature [10, 15]. We adopt the ODI approach pioneered by Flajolet and Martin (FM) for the purposes of aggregation. Next we outline the FM approach; for full details, see [7].
Suppose we want to compute the number of nodes in the network, i.e. the COUNT query. To do so, each node performs a coin toss experiment as follows: toss an unbiased coin, stopping after the first head is seen. The node then sets the ith bit in a bit vector (initially filled with zeros), where i is the number of coin tosses it performed. The intuition is that as the number of nodes doing coin toss experiments increases, the probability of a more significant bit being set in one of the nodes" bit vectors increases.
These bit vectors are then exchanged among nodes. When a node receives a bit vector, it updates its local bit vector by bitwise OR-ing it with the received vector (as shown in Figure 2 which computes AVERAGE). At the end of the aggregation process, every node, with high probability, has the same bit vector. The actual value of the count aggregate is then computed using the following formula, AGGF M = 2j−1 /0.77351, where j represents the bit position of the least significant zero in the aggregate bit vector [7].
Although such aggregates are very compact in nature, requiring only O(logN) state space (where N is the number of nodes), they may not be very accurate as they can only approximate values to the closest power of 2, potentially causing errors of up to 50%. More accurate aggregates can be computed by maintaining multiple bit vectors at each node, as explained in [7]. This decreases the error to within O(1/ √ m), where m is the number of such bit vectors.
Queries other than count can also be computed using variants of this basic counting algorithm, as discussed in [3] (and shown in Figure 2). Transmitting FM bit vectors between nodes is done using randomized gossiping, discussed next.
Gossip-based protocols operate in discrete time-steps; a time-step is the required amount of time for all transmissions in that time-step to complete. At every time-step, each node having something to send randomly selects one or more neighboring nodes and transmits its data to them. The randomized propagation of information provides fault-tolerance and resilience to network failures and outages. We emphasize that this characteristic of the protocol also allows it to operate without relying on any underlying network structure. Gossip protocols have been shown to provide exponentially fast convergence2 , on the order of O(log N) [10], where N is the number of nodes (or radios). These protocols can therefore easily scale to very dense environments. 2 Convergence refers to the state in which all nodes have the most up-to-date view of the network. 14 Two types of gossip protocols are: • Uniform Gossip: In uniform gossip, at each timestep, each node chooses a random neighbor and sends its data to it. This process repeats for O(log(N)) steps (where N is the number of nodes in the network).
Uniform gossip provides exponentially fast convergence, with low network overhead [10]. • Random Walk: In random walk, only a subset of the nodes (termed designated nodes) communicate in a particular time-step. At startup, k nodes are randomly elected as designated nodes. In each time-step, each designated node sends its data to a random neighbor, which becomes designated for the subsequent timestep (much like passing a token). This process repeats until the aggregate has converged in the network.
Random walk has been shown to provide similar convergence bounds as uniform gossip in problems of similar context [8, 12].
One limitation of FM aggregation is that it does not support updates. Due to the probabilistic nature of FM, once bit vectors have been ORed together, information cannot simply be removed from them as each node"s contribution has not been recorded. We propose the use of delete vectors, an extension of FM to support updates. We maintain a separate aggregate delete vector whose value is subtracted from the original aggregate vector"s value to obtain the resulting value as follows.
AGGINC = (2a−1 /0.77351) − (2b−1 /0.77351) (1) Here, a and b represent the bit positions of the least significant zero in the original and delete bit vectors respectively.
Suppose we wish to compute the average signal level detected in a particular frequency. To compute this, we compute the SUM of all signal level measurements and divide that by the COUNT of the number of measurements. A SUM aggregate is computed similar to COUNT (explained in Section 4.1), except that each node performs s coin toss experiments, where s is the locally measured signal level.
Figure 2 illustrates the sequence by which the average signal energy is computed in a particular band using FM aggregation.
Now suppose that the measured signal at a node changes from s to s . The vectors are updated as follows. • s > s: We simply perform (s − s) more coin toss experiments and bitwise OR the result with the original bit vector. • s < s: We increase the value of the delete vector by performing (s − s ) coin toss experiments and bitwise OR the result with the current delete vector.
Using delete vectors, we can now support updates to the measured signal level. With the original implementation of FM, the aggregate would need to be discarded and a new one recomputed every time an update occurred. Thus, delete vectors provide a low overhead alternative for applications whose data changes incrementally, such as signal level measurements in a coordinated spectrum sensing environment.
Next we discuss how these aggregates can be communicated between devices using incremental routing protocols.
We use the following incremental variants of the routing protocols presented in Section 4.2 to support incremental updates to previously computed aggregates.
Update Received OR Local Update Occurs Recovered Susceptible Time-stamp Expires Initial State Additional Update Received Infectious Clean Up Figure 3: State diagram each device passes through as updates proceed in the system • Incremental Gossip Protocol (IGP): When an update occurs, the updated node initiates the gossiping procedure. Other nodes only begin gossiping once they receive the update. Therefore, nodes receiving the update become active and continue communicating with their neighbors until the update protocol terminates, after O(log(N)) time steps. • Incremental Random Walk Protocol (IRWP): When an update (or updates) occur in the system, instead of starting random walks at k random nodes in the network, all k random walks are initiated from the updated node(s). The rest of the protocol proceeds in the same fashion as the standard random walk protocol. The allocation of walks to updates is discussed in more detail in [3], where the authors show that the number of walks has an almost negligible impact on network overhead.
Using incremental routing protocols to disseminate incremental FM aggregates is a natural fit for the problem of coordinated spectrum sensing. Here we outline the implementation of such techniques for a cognitive radio network.
We continue with the example from Section 5.1, where we wish to perform coordination between a group of wireless devices to compute the average signal level in a particular frequency band.
Using either incremental random walk or incremental gossip, each device proceeds through three phases, in order to determine the global average signal level for a particular frequency band. Figure 3 shows a state diagram of these phases.
Susceptible: Each device starts in the susceptible state and becomes infectious only when its locally measured signal level changes, or if it receives an update message from a neighboring device. If a local change is observed, the device updates either the original or delete bit vector, as described in Section 5.1, and moves into the infectious state. If it receives an update message, it ORs the received original and delete bit vectors with its local bit vectors and moves into the infectious state.
Note, because signal level measurements may change sporadically over time, a smoothing function, such as an exponentially weighted moving average, should be applied to these measurements.
Infectious: Once a device is infectious it continues to send its up-to-date bit vectors, using either incremental random walk or incremental gossip, to neighboring nodes. Due to FM"s order and duplicate insensitive (ODI) properties, simultaneously occurring updates are handled seamlessly by the protocol.
Update messages contain a time stamp indicating when the update was generated, and each device maintains a lo15 0 200 400 600 800 1000
Number of Measured Signal Changes Executiontime(ms) Incremental Gossip Uniform Gossip (a) Incremental Gossip and Uniform Gossip on Clique 0 200 400 600 800 1000
Number of Measured Signal Changes ExecutionTime(ms).
Incremental Random Walk Random Walk (b) Incremental Random Walk and Random Walk on Clique 0 400 800 1200 1600 2000
Number of Measured Signal Changes ExecutionTime(ms).
Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 4: Execution times of Incremental Protocols
Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoUniformGossip) Incremental Gossip Uniform Gossip (a) Incremental Gossip and Uniform Gossip on Clique
Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Incremental Random Walk Random Walk (b) Incremental Random Walk and Random Walk on Clique
Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 5: Network overhead of Incremental Protocols cal time stamp of when it received the most recent update.
Using this information, a device moves into the recovered state once enough time has passed for the most recent update to have converged. As discussed in Section 4.2, this happens after O(log(N)) time steps.
Recovered: A recovered device ceases to propagate any update information. At this point, it performs clean-up and prepares for the next infection by entering the susceptible state. Once all devices have entered the recovered state, the system will have converged, and with high probability, all devices will have the up-to-date average signal level. Due to the cumulative nature of FM, even if all devices have not converged, the next update will include all previous updates.
Nevertheless, the probability that gossip fails to converge is small, and has been shown to be O(1/N) [10].
For coordinated spectrum sensing, non-incremental routing protocols can be implemented in a similar fashion.
Random walk would operate by having devices periodically drop the aggregate and re-run the protocol. Each device would perform a coin toss (biased on the number of walks) to determine whether or not it is a designated node. This is different from the protocol discussed above where only updated nodes initiate random walks. Similar techniques can be used to implement standard gossip.
We now provide a preliminary evaluation of GUESS in simulation. A more detailed evaluation of this approach can be found in [3]. Here we focus on how incremental extensions to gossip protocols can lead to further improvements over standard gossiping techniques, for the problem of coordinated spectrum sensing.
Simulation Setup: We implemented a custom simulator in C++. We study the improvements of our incremental gossip protocols over standard gossiping in two dimensions: execution time and network overhead. We use two topologies to represent device connectivity: a clique, to eliminate the effects of the underlying topology on protocol performance, and a BRITE-generated [13] power-law random graph (PLRG), to illustrate how our results extend to more realistic scenarios. We simulate a large deployment of 1,000 devices to analyze protocol scalability.
In our simulations, we compute the average signal level in a particular band by disseminating FM bit vectors. In each run of the simulation, we induce a change in the measured signal at one or more devices. A run ends when the new average signal level has converged in the network.
For each data point, we ran 100 simulations and 95% confidence intervals (error bars) are shown.
Simulation Parameters: Each transmission involves sending 70 bits of information to a neighboring node. To compute the AVERAGE aggregate, four bit vectors need to be transmitted: the original SUM vector, the SUM delete vector, the original COUNT vector, and the COUNT delete vector. Non-incremental protocols do not transmit the delete vectors. Each transmission also includes a time stamp of when the update was generated.
We assume nodes communicate on a common control channel at 2 Mbps. Therefore, one time-step of protocol execution corresponds to the time required for 1,000 nodes to sequentially send 70 bits at 2 Mbps. Sequential use of the control channel is a worst case for our protocols; in practice, multiple control channels could be used in parallel to reduce execution time. We also assume nodes are loosely time synchronized, the implications of which are discussed further in [3]. Finally, in order to isolate the effect of protocol operation on performance, we do not model the complexities of the wireless channel in our simulations.
Incremental Protocols Reduce Execution Time: Figure 4(a) compares the performance of incremental gossip (IGP) with uniform gossip on a clique topology. We observe that both protocols have almost identical execution times.
This is expected as IGP operates in a similar fashion to 16 uniform gossip, taking O(log(N)) time-steps to converge.
Figure 4(b) compares the execution times of incremental random walk (IRWP) and standard random walk on a clique. IRWP reduces execution time by a factor of 2.7 for a small number of measured signal changes. Although random walk and IRWP both use k random walks (in our simulations k = number of nodes), IRWP initiates walks only from updated nodes (as explained in Section 5.2), resulting in faster information convergence. These improvements carry over to a PLRG topology as well (as shown in Figure 4(c)), where IRWP is 1.33 times faster than random walk.
Incremental Protocols Reduce Network Overhead: Figure 5(a) shows the ratio of data transmitted using uniform gossip relative to incremental gossip on a clique. For a small number of signal changes, incremental gossip incurs
in the early steps of protocol execution, only devices which detect signal changes communicate. As more signal changes are introduced into the system, gossip and incremental gossip incur approximately the same overhead.
Similarly, incremental random walk (IRWP) incurs much less overhead than standard random walk. Figure 5(b) shows a 2.7 fold reduction in overhead for small numbers of signal changes on a clique. Although each protocol uses the same number of random walks, IRWP uses fewer network resources than random walk because it takes less time to converge. This improvement also holds true on more complex PLRG topologies (as shown in Figure 5(c)), where we observe a 33% reduction in network overhead.
From these results it is clear that incremental techniques yield significant improvements over standard approaches to gossip, even on complex topologies. Because spectrum utilization is characterized by incremental changes to usage, incremental protocols are ideally suited to solve this problem in an efficient and cost effective manner.
We have only just scratched the surface in addressing the problem of coordinated spectrum sensing using incremental gossiping. Next, we outline some open areas of research.
Spatial Decay: Devices performing coordinated sensing are primarily interested in the spectrum usage of their local neighborhood. Therefore, we recommend the use of spatially decaying aggregates [6], which limits the impact of an update on more distant nodes. Spatially decaying aggregates work by successively reducing (by means of a decay function) the value of the update as it propagates further from its origin. One challenge with this approach is that propagation distance cannot be determined ahead of time and more importantly, exhibits spatio-temporal variations.
Therefore, finding the optimal decay function is non-trivial, and an interesting subject of future work.
Significance Threshold: RF spectrum bands continually experience small-scale changes which may not necessarily be significant. Deciding if a change is significant can be done using a significance threshold β, below which any observed change is not propagated by the node. Choosing an appropriate operating value for β is application dependent, and explored further in [3].
Weighted Readings: Although we argued that most devices will likely be equipped with low-cost sensing equipment, there may be situations where there are some special infrastructure nodes that have better sensing abilities than others. Weighting their measurements more heavily could be used to maintain a higher degree of accuracy.
Determining how to assign such weights is an open area of research.
Implementation Specifics: Finally, implementing gossip for coordinated spectrum sensing is also open. If implemented at the MAC layer, it may be feasible to piggy-back gossip messages over existing management frames (e.g. networking advertisement messages). As well, we also require the use of a control channel to disseminate sensing information. There are a variety of alternatives for implementing such a channel, some of which are outlined in [4]. The trade-offs of different approaches to implementing GUESS is a subject of future work.
Spectrum sensing is a key requirement for dynamic spectrum allocation in cognitive radio networks. The nature of the RF environment necessitates coordination between cognitive radio devices. We propose GUESS, an approximate yet low overhead approach to perform efficient coordination between cognitive radios. The fundamental contributions of GUESS are: (1) an FM aggregation scheme for efficient innetwork aggregation, (2) a randomized gossiping approach which provides exponentially fast convergence and robustness to network alterations, and (3) incremental variations of FM and gossip which we show can reduce the communication time by up to a factor of 2.7 and reduce network overhead by up to a factor of 2.4. Our preliminary simulation results showcase the benefits of this approach and we also outline a set of open problems that make this a new and exciting area of research.
[1] Unlicensed Operation in the TV Broadcast Bands and Additional Spectrum for Unlicensed Devices Below 900 MHz in the 3 GHz band, May 2004. Notice of Proposed Rule-Making 04-186, Federal Communications Commission. [2] In-Stat: Covering the Full Spectrum of Digital Communications Market Research, from Vendor to End-user, December 2005. http://www.in-stat.com/catalog/scatalogue.asp?id=28. [3] N. Ahmed, D. Hadaller, and S. Keshav. Incremental Maintenance of Global Aggregates. UW. Technical Report CS-2006-19, University of Waterloo, ON, Canada, 2006. [4] R. W. Brodersen, A. Wolisz, D. Cabric, S. M. Mishra, and D. Willkomm. CORVUS: A Cognitive Radio Approach for Usage of Virtual Unlicensed Spectrum. Technical report, July
[5] D. Cabric, S. M. Mishra, and R. W. Brodersen. Implementation Issues in Spectrum Sensing for Cognitive Radios. In Asilomar Conference, 2004. [6] E. Cohen and H. Kaplan. Spatially-Decaying Aggregation Over a Network: Model and Algorithms. In Proceedings of SIGMOD 2004, pages 707-718, New York, NY, USA, 2004. ACM Press. [7] P. Flajolet and G. N. Martin. Probabilistic Counting Algorithms for Data Base Applications. J. Comput. Syst. Sci., 31(2):182-209, 1985. [8] C. Gkantsidis, M. Mihail, and A. Saberi. Random Walks in Peer-to-Peer Networks. In Proceedings of INFOCOM 2004, pages 1229-1240, 2004. [9] E. Griffith. Previewing Intel"s Cognitive Radio Chip, June 2005. http://www.internetnews.com/wireless/article.php/3513721. [10] D. Kempe, A. Dobra, and J. Gehrke. Gossip-Based Computation of Aggregate Information. In FOCS 2003, page 482, Washington, DC, USA, 2003. IEEE Computer Society. [11] X. Liu and S. Shankar. Sensing-based Opportunistic Channel Access. In ACM Mobile Networks and Applications (MONET) Journal, March 2005. [12] Q. Lv, P. Cao, E. Cohen, K. Li, and S. Shenker. Search and Replication in Unstructured Peer-to-Peer Networks. In Proceedings of ICS, 2002. [13] A. Medina, A. Lakhina, I. Matta, and J. Byers. BRITE: an Approach to Universal Topology Generation. In Proceedings of MASCOTS conference, Aug. 2001. [14] S. M. Mishra, A. Sahai, and R. W. Brodersen. Cooperative Sensing among Cognitive Radios. In ICC 2006, June 2006. [15] S. Nath, P. B. Gibbons, S. Seshan, and Z. R. Anderson.
Synopsis Diffusion for Robust Aggregation in Sensor Networks.

With the increasing popularity of mobile devices and their widespread adoption, there is a clear need to allow the development of a broad spectrum of applications that operate effectively over such an environment. Unfortunately, this is far from simple: mobile devices are increasingly heterogeneous in terms of processing capabilities, memory size, battery capacity, and network interfaces. Each such configuration has substantially different characteristics that are both statically different - for example, there is a major difference in capability between a Berkeley mote and an 802.11g-equipped laptop - and that vary dynamically, as in situations of fluctuating bandwidth and intermittent connectivity. Mobile ad hoc environments have an additional element of complexity in that they are entirely decentralised.
In order to craft applications for such complex environments, an appropriate form of middleware is essential if cost effective development is to be achieved. In this paper, we examine one of the foundational aspects of middleware for mobile ad-hoc environments: that of the communication primitives.
Traditionally, the most frequently used middleware primitives for communication assume the simultaneous presence of both end points on a network, since the stability and pervasiveness of the networking infrastructure is not an unreasonable assumption for most wired environments. In other words, most communication paradigms are synchronous: object oriented middleware such as CORBA and Java RMI are typical examples of middleware based on synchronous communication.
In recent years, there has been growing interest in platforms based on asynchronous communication paradigms, such as publish-subscribe systems [6]: these have been exploited very successfully where there is application level asynchronicity. From a Gartner Market Report [7]: Given messageoriented-middleware"s (MOM) popularity, scalability, flexibility, and affinity with mobile and wireless architectures, by 2004, MOM will emerge as the dominant form of communication middleware for linking mobile and enterprise applications (0.7 probability).... Moreover, in mobile ad-hoc systems, the likelihood of network fragmentation means that synchronous communication may in any case be impracticable, giving situations in which delay tolerant asynchronous traffic is the only form of traffic that could be supported.
Middleware for mobile ad-hoc environments must therefore support semi-synchronous or completely asynchronous communication primitives if it is to avoid substantial limitations to its utility. Aside from the intellectual challenge in supporting this model, this work is also interesting because there are a number of practical application domains in allowing inter-community communication in undeveloped areas of the globe. Thus, for example, projects that have been carried out to help populations that live in remote places of the globe such as Lapland [3] or in poor areas that lack fixed connectivity infrastructure [9].
There have been attempts to provide mobile middleware with these properties, including STEAM, LIME,
XMIDDLE, Bayou (see [11] for a more complete review of mobile middleware). These models differ quite considerably from the existing traditional middleware in terms of primitives provided. Furthermore, some of them fail in providing a solution for the true ad-hoc scenarios.
If the projected success of MOM becomes anything like a reality, there will be many programmers with experience of it. The ideal solution to the problem of middleware for ad-hoc systems is, then, to allow programmers to utilise the same paradigms and models presented by common forms of MOM and to ensure that these paradigms are supportable within the mobile environment. This approach has clear advantages in allowing applications developed on standard middleware platforms to be easily deployed on mobile devices. Indeed, some research has already led to the adaptation of traditional middleware platforms to mobile settings, mainly to provide integration between mobile devices and existing fixed networks in a nomadic (i.e., mixed) environment [4]. With respect to message oriented middleware, the current implementations, however, either assume the existence of a backbone network to which the mobile hosts connect from time to time while roaming [10], or assume that nodes are always somehow reachable through a path [18].
No adaptation to heterogeneous or completely ad-hoc scenarios, with frequent disconnection and periodically isolated clouds of hosts, has been attempted.
In the remainder of this paper we describe an initial attempt to adapt message oriented middleware to suit mobile and, more specifically, mobile ad-hoc networks. In our case, we elected to examine JMS, as one of the most widely known MOM systems. In the latter part of this paper, we explore the limitations of our results and describe the plans we have to take the work further.
AND JAVA MESSAGE SERVICE (JMS) Message-oriented middleware systems support communication between distributed components via message-passing: the sender sends a message to identified queues, which usually reside on a server. A receiver retrieves the message from the queue at a different time and may acknowledge the reply using the same asynchronous mechanism. Message-oriented middleware thus supports asynchronous communication in a very natural way, achieving de-coupling of senders and receivers. A sender is able to continue processing as soon as the middleware has accepted the message; eventually, the receiver will send an acknowledgment message and the sender will be able to collect it at a convenient time.
However, given the way they are implemented, these middleware systems usually require resource-rich devices, especially in terms of memory and disk space, where persistent queues of messages that have been received but not yet processed, are stored. Sun Java Message Service [5], IBM WebSphere MQ [6], Microsoft MSMQ [12] are examples of very successful message-oriented middleware for traditional distributed systems.
The Java Messaging Service (JMS) is a collection of interfaces for asynchronous communication between distributed components. It provides a common way for Java programs to create, send and receive messages. JMS users are usually referred to as clients. The JMS specification further defines providers as the components in charge of implementing the messaging system and providing the administrative and control functionality (i.e., persistence and reliability) required by the system. Clients can send and receive messages, asynchronously, through the JMS provider, which is in charge of the delivery and, possibly, of the persistence of the messages.
There are two types of communication supported: point to point and publish-subscribe models. In the point to point model, hosts send messages to queues. Receivers can be registered with some specific queues, and can asynchronously retrieve the messages and then acknowledge them. The publish-subscribe model is based on the use of topics that can be subscribed to by clients. Messages are sent to topics by other clients and are then received in an asynchronous mode by all the subscribed clients. Clients learn about the available topics and queues through Java Naming and Directory Interface (JNDI) [14]. Queues and topics are created by an administrator on the provider and are registered with the JNDI interface for look-up.
In the next section, we introduce the challenges of mobile networks, and show how JMS can be adapted to cope with these requirements.
Mobile networks vary very widely in their characteristics, from nomadic networks in which modes relocate whilst offline through to ad-hoc networks in which modes move freely and in which there is no infrastructure. Mobile ad-hoc networks are most generally applicable in situations where survivability and instant deployability are key: most notably in military applications and disaster relief. In between these two types of "mobile" networks, there are, however, a number of possible heterogeneous combinations, where nomadic and ad-hoc paradigms are used to interconnect totally unwired areas to more structured networks (such as a LAN or the Internet).
Whilst the JMS specification has been extensively implemented and used in traditional distributed systems, adaptations for mobile environments have been proposed only recently. The challenges of porting JMS to mobile settings are considerable; however, in view of its widespread acceptance and use, there are considerable advantages in allowing the adaptation of existing applications to mobile environments and in allowing the interoperation of applications in the wired and wireless regions of a network.
In [10], JMS was adapted to a nomadic mobile setting, where mobile hosts can be JMS clients and communicate through the JMS provider that, however, sits on a backbone network, providing reliability and persistence. The client prototype presented in [10] is very lightweight, due to the delegation of all the heavyweight functionality to the Middleware for Pervasive and ad-hoc Computing 122 provider on the wired network. However, this approach is somewhat limited in terms of widespread applicability and scalability as a consequence of the concentration of functionality in the wired portion of the network.
If JMS is to be adapted to completely ad-hoc environments, where no fixed infrastructure is available, and where nodes change location and status very dynamically, more issues must be taken into consideration. Firstly, discovery needs to use a resilient but distributed model: in this extremely dynamic environment, static solutions are unacceptable. As discussed in Section 2, a JMS administrator defines queues and topics on the provider. Clients can then learn about them using the Java Naming and Directory Interface (JNDI). However, due to the way JNDI is designed, a JNDI node (or more than one) needs to be in reach in order to obtain a binding of a name to an address (i.e., knowing where a specific queue/topic is). In mobile ad-hoc environments, the discovery process cannot assume the existence of a fixed set of discovery servers that are always reachable, as this would not match the dynamicity of ad-hoc networks.
Secondly, a JMS Provider, as suggested by the JMS specification, also needs to be reachable by each node in the network, in order to communicate. This assumes a very centralised architecture, which again does not match the requirements of a mobile ad-hoc setting, in which nodes may be moving and sparse: a more distributed and dynamic solution is needed. Persistence is, however, essential functionality in asynchronous communication environments as hosts are, by definition, connected at different times.
In the following section, we will discuss our experience in designing and implementing JMS for mobile ad-hoc networks.
Networks Developing applications for mobile networks is yet more challenging: in addition to the same considerations as for infrastructured wireless environments, such as the limited device capabilities and power constraints, there are issues of rate of change of network connectivity, and the lack of a static routing infrastructure. Consequently, we now describe an initial attempt to adapt the JMS specification to target the particular requirements related to ad-hoc scenarios. As discussed in Section 3, a JMS application can use either the point to point and the publish-subscribe styles of messaging.
Point to Point Model The point to point model is based on the concept of queues, that are used to enable asynchronous communication between the producer of a message and possible different consumers. In our solution, the location of queues is determined by a negotiation process that is application dependent. For example, let us suppose that it is possible to know a priori, or it is possible to determine dynamically, that a certain host is the receiver of the most part of messages sent to a particular queue. In this case, the optimum location of the queue may well be on this particular host. In general, it is worth noting that, according to the JMS specification and suggested design patterns, it is common and preferable for a client to have all of its messages delivered to a single queue.
Queues are advertised periodically to the hosts that are within transmission range or that are reachable by means of the underlying synchronous communication protocol, if provided. It is important to note that, at the middleware level, it is logically irrelevant whether or not the network layer implements some form of ad-hoc routing (though considerably more efficient if it does); the middleware only considers information about which nodes are actively reachable at any point in time. The hosts that receive advertisement messages add entries to their JNDI registry. Each entry is characterized by a lease (a mechanism similar to that present in Jini [15]). A lease represents the time of validity of a particular entry. If a lease is not refreshed (i.e, its life is not extended), it can expire and, consequently, the entry is deleted from the registry. In other words, the host assumes that the queue will be unreachable from that point in time. This may be caused, for example, if a host storing the queue becomes unreachable. A host that initiates a discovery process will find the topics and the queues present in its connected portion of the network in a straightforward manner.
In order to deliver a message to a host that is not currently in reach1 , we use an asynchronous epidemic routing protocol that will be discussed in detail in Section 4.2. If two hosts are in the same cloud (i.e., a connected path exists between them), but no synchronous protocol is available, the messages are sent using the epidemic protocol. In this case, the delivery latency will be low as a result of the rapidity of propagation of the infection in the connected cloud (see also the simulation results in Section 5). Given the existence of an epidemic protocol, the discovery mechanism consists of advertising the queues to the hosts that are currently unreachable using analogous mechanisms.
Publish-Subscribe Model In the publish-subscribe model, some of the hosts are similarly designated to hold topics and store subscriptions, as before. Topics are advertised through the registry in the same way as are queues, and a client wishing to subscribe to a topic must register with the client holding the topic. When a client wishes to send a message to the topic list, it sends it to the topic holder (in the same way as it would send a message to a queue). The topic holder then forwards the message to all subscribers, using the synchronous protocol if possible, the epidemic protocol otherwise. It is worth noting that we use a single message with multiple recipients, instead of multiple messages with multiple recipients. When a message is delivered to one of the subscribers, this recipient is deleted from the list. In order to delete the other possible replicas, we employ acknowledgment messages (discussed in Section 4.4), returned in the same way as a normal message.
We have also adapted the concepts of durable and non durable subscriptions for ad-hoc settings. In fixed platforms, durable subscriptions are maintained during the disconnections of the clients, whether these are intentional or are the result of failures. In traditional systems, while a durable subscriber is disconnected from the server, it is responsible for storing messages. When the durable subscriber reconnects, the server sends it all unexpired messages. The problem is that, in our scenario, disconnections are the norm 1 In theory, it is not possible to send a message to a peer that has never been reachable in the past, since there can be no entry present in the registry. However, to overcome this possible limitation, we provide a primitive through which information can be added to the registry without using the normal channels.
rather than the exception. In other words, we cannot consider disconnections as failures. For these reasons, we adopt a slightly different semantics. With respect to durable subscriptions, if a subscriber becomes disconnected, notifications are not stored but are sent using the epidemic protocol rather than the synchronous protocol. In other words, durable notifications remain valid during the possible disconnections of the subscriber.
On the other hand, if a non-durable subscriber becomes disconnected, its subscription is deleted; in other words, during disconnections, notifications are not sent using the epidemic protocol but exploit only the synchronous protocol. If the topic becomes accessible to this host again, it must make another subscription in order to receive the notifications.
Unsubscription messages are delivered in the same way as are subscription messages. It is important to note that durable subscribers have explicitly to unsubscribe from a topic in order to stop the notification process; however, all durable subscriptions have a predefined expiration time in order to cope with the cases of subscribers that do not meet again because of their movements or failures. This feature is clearly provided to limit the number of the unnecessary messages sent around the network.
In this section, we examine one possible mechanism that will allow the delivery of messages in a partially connected network. The mechanism we discuss is intended for the purposes of demonstrating feasibility; more efficient communication mechanisms for this environment are themselves complex, and are the subject of another paper [13].
The asynchronous message delivery described above is based on a typical pure epidemic-style routing protocol [16].
A message that needs to be sent is replicated on each host in reach. In this way, copies of the messages are quickly spread through connected networks, like an infection. If a host becomes connected to another cloud of mobile nodes, during its movement, the message spreads through this collection of hosts. Epidemic-style replication of data and messages has been exploited in the past in many fields starting with the distributed database systems area [2].
Within epidemic routing, each host maintains a buffer containing the messages that it has created and the replicas of the messages generated by the other hosts. To improve the performance, a hash-table indexes the content of the buffer. When two hosts connect, the host with the smaller identifier initiates a so-called anti-entropy session, sending a list containing the unique identifiers of the messages that it currently stores. The other host evaluates this list and sends back a list containing the identifiers it is storing that are not present in the other host, together with the messages that the other does not have. The host that has started the session receives the list and, in the same way, sends the messages that are not present in the other host. Should buffer overflow occur, messages are dropped.
The reliability offered by this protocol is typically best effort, since there is no guarantee that a message will eventually be delivered to its recipient. Clearly, the delivery ratio of the protocol increases proportionally to the maximum allowed delay time and the buffer size in each host (interesting simulation results may be found in [16]).
In this section, we will analyse the aspects of our adaptation of the specification related to the so-called JMS Message Model [5]. According to this, JMS messages are characterised by some properties defined using the header field, which contains values that are used by both clients and providers for their delivery. The aspects discussed in the remainder of this section are valid for both models (point to point and publish-subscribe).
A JMS message can be persistent or non-persistent.
According to the JMS specification, persistent messages must be delivered with a higher degree of reliability than the nonpersistent ones. However, it is worth noting that it is not possible to ensure once-and-only-once reliability for persistent messages as defined in the specification, since, as we discussed in the previous subsection, the underlying epidemic protocol can guarantee only best-effort delivery. However, clients maintain a list of the identifiers of the recently received messages to avoid the delivery of message duplicates.
In other words, we provide the applications with at-mostonce reliability for both types of messages.
In order to implement different levels of reliability, EMMA treats persistent and non-persistent messages differently, during the execution of the anti-entropy epidemic protocol. Since the message buffer space is limited, persistent messages are preferentially replicated using the available free space. If this is insufficient and non-persistent messages are present in the buffer, these are replaced. Only the successful deliveries of the persistent messages are notified to the senders.
According to the JMS specification, it is possible to assign a priority to each message. The messages with higher priorities are delivered in a preferential way. As discussed above, persistent messages are prioritised above the non-persistent ones. Further selection is based on their priorities. Messages with higher priorities are treated in a preferential way. In fact, if there is not enough space to replicate all the persistent messages, a mechanism based on priorities is used to delete and replicate non-persistent messages (and, if necessary, persistent messages).
Messages are deleted from the buffers using the expiration time value that can be set by senders. This is a way to free space in the buffers (one preferentially deletes older messages in cases of conflict); to eliminate stale replicas in the system; and to limit the time for which destinations must hold message identifiers to dispose of duplicates.
Mechanisms As already discussed, at-most-once message delivery is the best that can be achieved in terms of delivery semantics in partially connected ad-hoc settings. However, it is possible to improve the reliability of the system with efficient acknowledgment mechanisms. EMMA provides a mechanism for failure notification to applications if the acknowledgment is not received within a given timeout (that can be configured by application developers). This mechanism is the one that distinguishes the delivery of persistent and non-persistent messages in our JMS implementation: the deliveries of the former are notified to the senders, whereas the latter are not.
We use acknowledgment messages not only to inform senders about the successful delivery of messages but also to delete the replicas of the delivered messages that are still present in the network. Each host maintains a list of the messages Middleware for Pervasive and ad-hoc Computing 124 successfully delivered that is updated as part of the normal process of information exchange between the hosts. The lists are exchanged during the first steps of the anti-entropic epidemic protocol with a certain predefined frequency. In the case of messages with multiple recipients, a list of the actual recipients is also stored. When a host receives the list, it checks its message buffer and updates it according to the following rules: (1) if a message has a single recipient and it has been delivered, it is deleted from the buffer; (2) if a message has multiple recipients, the identifiers of the delivered hosts are deleted from the associated list of recipients.
If the resulting length of the list of recipients is zero, the message is deleted from the buffer.
These lists have, clearly, finite dimensions and are implemented as circular queues. This simple mechanism, together with the use of expiration timestamps, guarantees that the old acknowledgment notifications are deleted from the system after a limited period of time.
In order to improve the reliability of EMMA, a design mechanism for intelligent replication of queues and topics based on the context information could be developed.
However this is not yet part of the current architecture of EMMA.
EVALUATION We implemented a prototype of our platform using the J2ME Personal Profile. The size of the executable is about 250KB including the JMS 1.1 jar file; this is a perfectly acceptable figure given the available memory of the current mobile devices on the market. We tested our prototype on HP IPaq PDAs running Linux, interconnected with WaveLan, and on a number of laptops with the same network interface.
We also evaluated the middleware platform using the OMNET++ discrete event simulator [17] in order to explore a range of mobile scenarios that incorporated a more realistic number of hosts than was achievable experimentally. More specifically, we assessed the performance of the system in terms of delivery ratio and average delay, varying the density of population and the buffer size, and using persistent and non-persistent messages with different priorities.
The simulation results show that the EMMA"s performance, in terms of delivery ratio and delay of persistent messages with higher priorities, is good. In general, it is evident that the delivery ratio is strongly related to the correct dimensioning of the buffers to the maximum acceptable delay. Moreover, the epidemic algorithms are able to guarantee a high delivery ratio if one evaluates performance over a time interval sufficient for the dissemination of the replicas of messages (i.e., the infection spreading) in a large portion of the ad-hoc network.
One consequence of the dimensioning problem is that scalability may be seriously impacted in peer-to-peer middleware for mobile computing due to the resource poverty of the devices (limited memory to store temporarily messages) and the number of possible interconnections in ad-hoc settings. What is worse is that common forms of commercial and social organisation (six degrees of separation) mean that even modest TTL values on messages will lead to widespread flooding of epidemic messages. This problem arises because of the lack of intelligence in the epidemic protocol, and can be addressed by selecting carrier nodes for messages with greater care. The details of this process are, however, outside the scope of this paper (but may be found in [13]) and do not affect the foundation on which the EMMA middleware is based: the ability to deliver messages asynchronously.
THE ART The design of middleware platforms for mobile computing requires researchers to answer new and fundamentally different questions; simply assuming the presence of wired portions of the network on which centralised functionality can reside is not generalisable. Thus, it is necessary to investigate novel design principles and to devise architectural patterns that differ from those traditionally exploited in the design of middleware for fixed systems.
As an example, consider the recent cross-layering trend in ad-hoc networking [1]. This is a way of re-thinking software systems design, explicitly abandoning the classical forms of layering, since, although this separation of concerns afford portability, it does so at the expense of potential efficiency gains. We believe that it is possible to view our approach as an instance of cross-layering. In fact, we have added the epidemic network protocol at middleware level and, at the same time, we have used the existing synchronous network protocol if present both in delivering messages (traditional layering) and in informing the middleware about when messages may be delivered by revealing details of the forwarding tables (layer violation). For this reason, we prefer to consider them jointly as the communication layer of our platform together providing more efficient message delivery.
Another interesting aspect is the exploitation of context and system information to improve the performance of mobile middleware platforms. Again, as a result of adopting a cross-layering methodology, we are able to build systems that gather information from the underlying operating system and communication components in order to allow for adaptation of behaviour. We can summarise this conceptual design approach by saying that middleware platforms must be not only context-aware (i.e., they should be able to extract and analyse information from the surrounding context) but also system-aware (i.e., they should be able to gather information from the software and hardware components of the mobile system).
A number of middleware systems have been developed to support ad-hoc networking with the use of asynchronous communication (such as LIME, XMIDDLE, STEAM [11]).
In particular, the STEAM platform is an interesting example of event-based middleware for ad-hoc networks, providing location-aware message delivery and an effective solution for event filtering.
A discussion of JMS, and its mobile realisation, has already been conducted in Sections 4 and 2. The Swiss company Softwired has developed the first JMS middleware for mobile computing, called iBus Mobile [10]. The main components of this typically infrastructure-based architecture are the JMS provider, the so-called mobile JMS gateway, which is deployed on a fixed host and a lightweight JMS client library. The gateway is used for the communication between the application server and mobile hosts. The gateway is seen by the JMS provider as a normal JMS client. The JMS provider can be any JMS-enabled application server, such as BEA Weblogic. Pronto [19] is an example of mid125 Middleware 2004 Companion dleware system based on messaging that is specifically designed for mobile environments. The platform is composed of three classes of components: mobile clients implementing the JMS specification, gateways that control traffic, guaranteeing efficiency and possible user customizations using different plug-ins and JMS servers. Different configurations of these components are possible; with respect to mobile ad hoc networks applications, the most interesting is Serverless JMS. The aim of this configuration is to adapt JMS to a decentralized model. The publish-subscribe model exploits the efficiency and the scalability of the underlying IP multicast protocol. Unreliable and reliable message delivery services are provided: reliability is provided through a negative acknowledgment-based protocol. Pronto represents a good solution for infrastructure-based mobile networks but it does not adequately target ad-hoc settings, since mobile nodes rely on fixed servers for the exchange of messages.
Other MOM implemented for mobile environments exist; however, they are usually straightforward extensions of existing middleware [8]. The only implementation of MOM specifically designed for mobile ad-hoc networks was developed at the University of Newcastle [18]. This work is again a JMS adaptation; the focus of that implementation is on group communication and the use of application level routing algorithms for topic delivery of messages. However, there are a number of differences in the focus of our work. The importance that we attribute to disconnections makes persistence a vital requirement for any middleware that needs to be used in mobile ad-hoc networks. The authors of [18] signal persistence as possible future work, not considering the fact that routing a message to a non-connected host will result in delivery failure. This is a remarkable limitation in mobile settings where unpredictable disconnections are the norm rather than the exception.
Asynchronous communication is a useful communication paradigm for mobile ad-hoc networks, as hosts are allowed to come, go and pick up messages when convenient, also taking account of their resource availability (e.g., power, connectivity levels). In this paper we have described the state of the art in terms of MOM for mobile systems. We have also shown a proof of concept adaptation of JMS to the extreme scenario of partially connected mobile ad-hoc networks.
We have described and discussed the characteristics and differences of our solution with respect to traditional JMS implementations and the existing adaptations for mobile settings. However, trade-offs between application-level routing and resource usage should also be investigated, as mobile devices are commonly power/resource scarce. A key limitation of this work is the poorly performing epidemic algorithm and an important advance in the practicability of this work requires an algorithm that better balances the needs of efficiency and message delivery probability. We are currently working on algorithms and protocols that, exploiting probabilistic and statistical techniques on the basis of small amounts of exchanged information, are able to improve considerably the efficiency in terms of resources (memory, bandwidth, etc) and the reliability of our middleware platform [13].
One futuristic research development, which may take these ideas of adaptation of messaging middleware for mobile environments further is the introduction of more mobility oriented communication extensions, for instance the support of geocast (i.e., the ability to send messages to specific geographical areas).
[1] M. Conti, G. Maselli, G. Turi, and S. Giordano.
Cross-layering in Mobile ad-hoc Network Design. IEEE Computer, 37(2):48-51, February 2004. [2] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson,
S. Shenker, H. Sturgis, D. Swinehart, and D. Terry.
Epidemic Algorithms for Replicated Database Maintenance. In Sixth Symposium on Principles of Distributed Computing, pages 1-12, August 1987. [3] A. Doria, M. Uden, and D. P. Pandey. Providing connectivity to the Saami nomadic community. In Proceedings of the Second International Conference on Open Collaborative Design for Sustainable Innovation,
December 2002. [4] M. Haahr, R. Cunningham, and V. Cahill. Supporting CORBA applications in a Mobile Environment. In 5th International Conference on Mobile Computing and Networking (MOBICOM99), pages 36-47. ACM, August
[5] M. Hapner, R. Burridge, R. Sharma, J. Fialli, and K. Stout. Java Message Service Specification Version 1.1.
Sun Microsystems, Inc., April 2002. http://java.sun.com/products/jms/. [6] J. Hart. WebSphere MQ: Connecting your applications without complex programming. IBM WebSphere Software White Papers, 2003. [7] S. Hayward and M. Pezzini. Marrying Middleware and Mobile Computing. Gartner Group Research Report,
September 2001. [8] IBM. WebSphere MQ EveryPlace Version 2.0, November
[9] ITU. Connecting remote communities. Documents of the World Summit on Information Society, 2003. http://www.itu.int/osg/spu/wsis-themes. [10] S. Maffeis. Introducing Wireless JMS. Softwired AG, www.sofwired-inc.com, 2002. [11] C. Mascolo, L. Capra, and W. Emmerich. Middleware for Mobile Computing. In E. Gregori, G. Anastasi, and S. Basagni, editors, Advanced Lectures on Networking, volume 2497 of Lecture Notes in Computer Science, pages 20-58. Springer Verlag, 2002. [12] Microsoft. Microsoft Message Queuing (MSMQ) Version
[13] M. Musolesi, S. Hailes, and C. Mascolo. Adaptive routing for intermittently connected mobile ad-hoc networks.
Technical report, UCL-CS Research Note, July 2004.
Submitted for Publication. [14] Sun Microsystems. Java Naming and Directory Interface (JNDI) Documentation Version 1.2. 2003. http://java.sun.com/products/jndi/. [15] Sun Microsystems. Jini Specification Version 2.0, 2003. http://java.sun.com/products/jini/. [16] A. Vahdat and D. Becker. Epidemic routing for Partially Connected ad-hoc Networks. Technical Report CS-2000-06,
Department of Computer Science, Duke University, 2000. [17] A. Vargas. The OMNeT++ discrete event simulation system. In Proceedings of the European Simulation Multiconference (ESM"2001), Prague, June 2001. [18] E. Vollset, D. Ingham, and P. Ezhilchelvan. JMS on Mobile ad-hoc Networks. In Personal Wireless Communications (PWC), pages 40-52, Venice, September 2003. [19] E. Yoneki and J. Bacon. Pronto: Mobilegateway with publish-subscribe paradigm over wireless network.
Technical Report 559, University of Cambridge, Computer Laboratory, February 2003.

Solutions for integrating heterogeneous IDSs (Intrusion Detection Systems) have been proposed by several groups [6],[7],[11],[2].
Some reasons for integrating IDSs are described by the IDWG (Intrusion Detection Working Group) from the IETF (Internet Engineering Task Force) [12] as follows: • Many IDSs available in the market have strong and weak points, which generally make necessary the deployment of more than one IDS to provided an adequate solution. • Attacks and intrusions generally originate from multiple networks spanning several administrative domains; these domains usually utilize different IDSs. The integration of IDSs is then needed to correlate information from multiple networks to allow the identification of distributed attacks and or intrusions. • The interoperability/integration of different IDS components would benefit the research on intrusion detection and speed up the deployment of IDSs as commercial products.
DIDSs (Distributed Intrusion Detection Systems) therefore started to emerge in early 90s [9] to allow the correlation of intrusion information from multiple hosts, networks or domains to detect distributed attacks. Research on DIDSs has then received much interest, mainly because centralised IDSs are not able to provide the information needed to prevent such attacks [13].
However, the realization of a DIDS requires a high degree of coordination. Computational Grids are appealing as they enable the development of distributed application and coordination in a distributed environment. Grid computing aims to enable coordinate resource sharing in dynamic groups of individuals and/or organizations. Moreover, Grid middleware provides means for secure access, management and allocation of remote resources; resource information services; and protocols and mechanisms for transfer of data [4].
According to Foster et al. [4], Grids can be viewed as a set of aggregate services defined by the resources that they share. OGSA (Open Grid Service Architecture) provides the foundation for this service orientation in computational Grids. The services in OGSA are specified through well-defined, open, extensible and platformindependent interfaces, which enable the development of interoperable applications.
This article proposes a model for integration of IDSs by using computational Grids. The proposed model enables heterogeneous IDSs to work in a cooperative way; this integration is termed DIDSoG (Distributed Intrusion Detection System on Grid). Each of the integrated IDSs is viewed by others as a resource accessed through the services that it exposes. A Grid middleware provides several features for the realization of a DIDSoG, including [3]: decentralized coordination of resources; use of standard protocols and interfaces; and the delivery of optimized QoS (Quality of Service).
The service oriented architecture followed by Grids (OGSA) allows the definition of interfaces that are adaptable to different platforms. Different implementations can be encapsulated by a service interface; this virtualisation allows the consistent access to resources in heterogeneous environments [3]. The virtualisation of the environment through service interfaces allows the use of services without the knowledge of how they are actually implemented. This characteristic is important for the integration of IDSs as the same service interfaces can be exposed by different IDSs.
Grid middleware can thus be used to implement a great variety of services. Some functions provided by Grid middleware are [3]: (i) data management services, including access services, replication, and localisation; (ii) workflow services that implement coordinate execution of multiple applications on multiple resources; (iii) auditing services that perform the detection of frauds or intrusions; (iv) monitoring services which implement the discovery of sensors in a distributed environment and generate alerts under determined conditions; (v) services for identification of problems in a distributed environment, which implement the correlation of information from disparate and distributed logs.
These services are important for the implementation of a DIDSoG.
A DIDS needs services for the location of and access to distributed data from different IDSs. Auditing and monitoring services take care of the proper needs of the DIDSs such as: secure storage, data analysis to detect intrusions, discovery of distributed sensors, and sending of alerts. The correlation of distributed logs is also relevant because the detection of distributed attacks depends on the correlation of the alert information generated by the different IDSs that compose the DIDSoG.
The next sections of this article are organized as follows. Section
Section 3. Section 4 describes the development and a case study.
Results and discussion are presented in Section 5. Conclusions and future work are discussed in Section 6.
DIDMA [5] is a flexible, scalable, reliable, and platformindependent DIDS. DIDMA architecture allows distributed analysis of events and can be easily extended by developing new agents. However, the integration with existing IDSs and the development of security components are presented as future work [5]. The extensibility of DIDS DIDMA and the integration with other IDSs are goals pursued by DIDSoG. The flexibility, scalability, platform independence, reliability and security components discussed in [5] are achieved in DIDSoG by using a Grid platform.
More efficient techniques for analysis of great amounts of data in wide scale networks based on clustering and applicable to DIDSs are presented in [13]. The integration of heterogeneous IDSs to increase the variety of intrusion detection techniques in the environment is mentioned as future work [13] DIDSoG thus aims at integrating heterogeneous IDSs [13].
Ref. [10] presents a hierarchical architecture for a DIDS; information is collected, aggregated, correlated and analysed as it is sent up in the hierarchy. The architecture comprises of several components for: monitoring, correlation, intrusion detection by statistics, detection by signatures and answers. Components in the same level of the hierarchy cooperate with one another. The integration proposed by DIDSoG also follows a hierarchical architecture. Each IDS integrated to the DIDSoG offers functionalities at a given level of the hierarchy and requests functionalities from IDSs from another level. The hierarchy presented in [10] integrates homogeneous IDSs whereas the hierarchical architecture of DIDSoG integrates heterogeneous IDSs.
There are proposals on integrating computational Grids and IDSs [6],[7],[11],[2]. Ref. [6] and [7] propose the use of Globus Toolkit for intrusion detection, especially for DoS (Denial of Service) and DDoS (Distributed Denial of Service) attacks; Globus is used due to the need to process great amounts of data to detect these kinds of attack. A two-phase processing architecture is presented. The first phase aims at the detection of momentary attacks, while the second phase is concerned with chronic or perennial attacks.
Traditional IDSs or DIDSs are generally coordinated by a central point; a characteristic that leaves them prone to attacks. Leu et al. [6] point out that IDSs developed upon Grids platforms are less vulnerable to attacks because of the distribution provided for such platforms. Leu et al. [6],[7] have used tools to generate several types of attacks - including TCP, ICMP and UDP flooding - and have demonstrated through experimental results the advantages of applying computational Grids to IDSs.
This work proposes the development of a DIDS upon a Grid platform. However, the resulting DIDS integrates heterogeneous IDSs whereas the DIDSs upon Grids presented by Leu et al. [6][7] do not consider the integration of heterogeneous IDSs. The processing in phases [6][7] is also contemplated by DIDSoG, which is enabled by the specification of several levels of processing allowed by the integration of heterogeneous IDSs.
The DIDS GIDA (Grid Intrusion Detection Architecture) targets at the detection of intrusions in a Grid environment [11]. GridSim Grid simulator was used for the validation of DIDS GIDA.
Homogeneous resources were used to simplify the development [11]. However, the possibility of applying heterogeneous detection systems is left for future work Another DIDS for Grids is presented by Choon and Samsudim [2]. Scenarios demonstrating how a DIDS can execute on a Grid environment are presented.
DIDSoG does not aim at detecting intrusions in a Grid environment. In contrast, DIDSoG uses the Grid to compose a DIDS by integrating specific IDSs; the resulting DIDS could however be used to identify attacks in a Grid environment. Local and distributed attacks can be detected through the integration of traditional IDSs while attacks particular to Grids can be detected through the integration of Grid IDSs.
DIDSoG presents a hierarchy of intrusion detection services; this hierarchy is organized through a two-dimensional vector defined by Scope:Complexity. The IDSs composing DIDSoG can be organized in different levels of scope or complexity, depending on its functionalities, the topology of the target environment and expected results.
Figure 1 presents a DIDSoG composed by different intrusion detection services (i.e. data gathering, data aggregation, data correlation, analysis, intrusion response and management) provided by different IDSs. The information flow and the relationship between the levels of scope and complexity are presented in this figure.
Information about the environment (host, network or application) is collected by Sensors located both in user 1"s and user 2"s computers in domain 1. The information is sent to both simple Analysers that act on the information from a single host (level 1:1), and to aggregation and correlation services that act on information from multiple hosts from the same domain (level 2:1).
Simple Analysers in the first scope level send the information to more complex Analysers in the next levels of complexity (level 1: N). When an Analyser detects an intrusion, it communicates with Countermeasure and Monitoring services registered to its scope.
An Analyser can invoke a Countermeasure service that replies to a detected attack, or informs a Monitoring service about the ongoing attack, so the administrator can act accordingly.
Aggregation and correlation resources in the second scope receive information from Sensors from different users" computers (user 1"s and user 2"s) in the domain 1. These resources process the received information and send it to the analysis resources registered to the first level of complexity in the second scope (level 2:1). The information is also sent to the aggregation and correlation resources registered in the first level of complexity in the next scope (level 3:1).
User 1 Domain 1 Analysers Level 1:1 Local Sensors Analysers Level 1:N Aggreg.
Correlation Level 2:1 User 2 Domain 1 Local Sensors Analysers Level 2:1 Analysers Level 2:N Aggreg.
Correlation Level 3:1 Domain 2 Monitor Level 1 Monitor Level 2 Analysers Level 3:1 Analysers Level 3:N Monitor Level 3 Response Level 1 Response Level 2 Response Level 3 Fig. 1. How DIDSoG works.
The analysis resources in the second scope act like the analysis resources in the first scope, directing the information to a more complex analysis resource and putting the Countermeasure and Monitoring resources in action in case of detected attacks.
Aggregation and correlation resources in the third scope receive information from domains 1 and 2. These resources then carry out the aggregation and correlation of the information from different domains and send it to the analysis resources in the first level of complexity in the third scope (level 3:1). The information could also be sent to the aggregate service in the next scope in case of any resources registered to such level.
The analysis resources in the third scope act similar to the analysis resources in the first and second scopes, except that the analysis resources in the third scope act on information from multiple domains.
The functionalities of the registered resources in each of the scopes and complexity level can vary from one environment to another. The model allows the development of N levels of scope and complexity.
Figure 2 presents the architecture of a resource participating in the DIDSoG. Initially, the resource registers itself to GIS (Grid Information Service) so other participating resources can query the services provided. After registering itself, the resource requests information about other intrusion detection resources registered to the GIS.
A given resource of DIDSoG interacts with other resources, by receiving data from the Source Resources, processing it, and sending the results to the Destination Resources, therefore forming a grid of intrusion detection resources.
Grid Resource BaseNative IDS Grid Origin Resources Grid Destination Resources Grid Information Service Descri ptor Connec tor Fig. 2. Architecture of a resource participating of the DIDSoG.
A resource is made up of four components: Base, Connector,
Descriptor and Native IDS. Native IDS corresponds to the IDS being integrated to the DIDSoG. This component process the data received from the Origin Resources and generates new data to be sent to the Destination Resources. A Native IDS component can be any tool processes information related to intrusion detection, including analysis, data gathering, data aggregation, data correlation, intrusion response or management.
The Descriptor is responsible for the information that identifies a resource and its respective Destination Resources in the DIDSoG.
Figure 3 presents the class diagram of the stored information by the Descriptor. The ResourceDescriptor class has Feature, Level,
DataType and Target Resources type members. Feature class represents the functionalities that a resource has. Type, name and version attributes refer to the functions offered by the Native IDS component, its name and version, respectively. Level class identifies the level of target and complexity in which the resource acts. DataType class represents the data format that the resource accepts to receive. DataType class is specialized by classes Text,
XML and Binary. Class XML contains the DTDFile attribute to specify the DTD file that validates the received XML. -ident -version -description ResourceDescriptor -featureType -name -version Feature 1 1 -type -version DataType -escope -complex Level 1 1 Text Binary -DTDFile XML 1 1 TargetResources 1
-featureType Resource11 1 1 Fig. 3. Class Diagram of the Descriptor component.
TargetResources class represents the features of the Destination Resources of a determined resource. This class aggregates Resource. The Resource class identifies the characteristics of a Destination Resource. This identification is made through the featureType attribute and the Level and DataType classes.
A given resource analyses the information from Descriptors from other resources, and compares this information with the information specified in TargetResources to know to which resources to send the results of its processing.
The Base component is responsible for the communication of a resource with other resources of the DIDSoG and with the Grid Information Service. It is this component that registers the resource and the queries other resources in the GIS.
The Connector component is the link between Base and Native IDS. The information that Base receives from Origin Resources is passed to Connector component. The Connector component performs the necessary changes in the data so that it is understood by Native IDS, and sends this data to Native IDS for processing.
The Connector component also has the responsibility of collecting the information processed by Native IDS, and making the necessary changes so the information can pass through the DIDSoG again. After these changes, Connector sends the information to the Base, which in turn sends it to the Destination Resources in accordance with the specifications of the Descriptor component.
We have used GridSim Toolkit 3 [1] for development and evaluation of the proposed model. We have used and extended GridSim features to model and simulate the resources and components of DIDSoG.
Figure 4 presents the Class diagram of the simulated DIDSoG.
The Simulation_DIDSoG class starts the simulation components.
The Simulation_User class represents a user of DIDSoG. This class" function is to initiate the processing of a resource Sensor, from where the gathered information will be sent to other resources. DIDSoG_GIS keeps a registry of the DIDSoG resources.The DIDSoG_BaseResource class implements the Base component (see Figure 2). DIDSoG_BaseResource interacts with DIDSoG_Descriptor class, which represents the Descriptor component. The DIDSoG_Descriptor class is created from an XML file that specifies a resource descriptor (see Figure 3).
DIDSoG_BaseResource DIDSoG_Descriptor 11 DIDSoG_GIS Simulation_User Simulation_DIDSoG 1 *1* 1 1 GridInformationService GridSim GridResource Fig. 4. Class Diagram of the simulatated DIDSoG.
A Connector component must be developed for each Native IDS integrated to DIDSoG. The Connector component is implemented by creating a class derived from DIDSoG_BaseResource. The new class will implement new functionalities in accordance with the needs of the corresponding Native IDS.
In the simulation environment, data collection resources, analysis, aggregation/correlation and generation of answers were integrated. Classes were developed to simulate the processing of each Native IDS components associated to the resources. For each simulated Native IDS a class derived from DIDSoG_BaseResource was developed. This class corresponds to the Connector component of the Native IDS and aims at the integrating the IDS to DIDSoG.
A XML file describing each of the integrated resources is chosen by using the Connector component. The resulting relationship between the resources integrated to the DIDSoG, in accordance with the specification of its respective descriptors, is presented in Figure 5.
The Sensor_1 and Sensor_2 resources generate simulated data in the TCPDump [8] format. The generated data is directed to Analyser_1 and Aggreg_Corr_1 resources, in the case of Sensor_1, and to Aggreg_Corr_1 in the case of Sensor_2, according to the specification of their descriptors.
User_1 Analyser_ 1 Level 1:1 Sensor_1 Aggreg_ Corr_1 Level 2:1 User_2 Sensor_2 Analyser_2 Level 2:1 Analyser_3 Level 2:2 TCPDump TCPDump TCPDumpAg TCPDumpAg IDMEF IDMEF IDMEF TCPDump  Countermeasure_1 Level 1  Countermeasure_2 Level 2 Fig. 5. Flow of the execution of the simulation.
The Native IDS of Analyser_1 generates alerts for any attempt of connection to port 23. The data received from Analyser_1 had presented such features, generating an IDMEF (Intrusion Detection Message Exchange Format) alert [14]. The generated alert was sent to Countermeasure_1 resource, where a warning was dispatched to the administrator informing him of the alert received.
The Aggreg_Corr_1 resource received the information generated by sensors 1 and 2. Its processing activities consist in correlating the source IP addresses with the received data. The resultant information of the processing of Aggreg_Corr_1 was directed to the Analyser_2 resource.
The Native IDS component of the Analyser_2 generates alerts when a source tries to connect to the same port number of multiple destinations. This situation is identified by the Analyser_2 in the data received from Aggreg_Corr_1 and an alert in IDMEF format is then sent to the Countermeasures_2 resource.
In addition to generating alerts in IDMEF format, Analyser_2 also directs the received data to the Analyser_3, in the level of complexity 2. The Native IDS component of Analyser_3 generates alerts when the transmission of ICMP messages from a given source to multiple destinations is detected. This situation is detected in the data received from Analyser_2, and an IDMEF alert is then sent to the Countermeasure_2 resource.
The Countermeasure_2 resource receives the alerts generated by analysers 3 and 2, in accordance with the implementation of its Native IDS component. Warnings on alerts received are dispatched to the administrator.
The simulation carried out demonstrates how DIDSoG works.
Simulated data was generated to be the input for a grid of intrusion detection systems composed by several distinct resources. The resources carry out tasks such as data collection, aggregation and analysis, and generation of alerts and warnings in an integrated manner.
The hierarchic organization of scope and complexity provides a high degree of flexibility to the model. The DIDSoG can be modelled in accordance with the needs of each environment. The descriptors define data flow desired for the resulting DIDS.
Each Native IDS is integrated to the DIDSoG through a Connector component. The Connector component is also flexible in the DIDSoG. Adaptations, conversions of data types and auxiliary processes that Native IDSs need are provided by the Connector. Filters and generation of Specific logs for each Native IDS or environment can also be incorporated to the Connector.
If the integration of a new IDS to an environment already configured is desired, it is enough to develop the Connector for the desired IDS and to specify the resource Descriptor. After the specification of the Connector and the Descriptor the new IDS is integrated to the DIDSoG.
Through the definition of scopes, resources can act on data of different source groups. For example, scope 1 can be related to a given set of hosts, scope 2 to another set of hosts, while scope 3 can be related to hosts from scopes 1 and 2. Scopes can be defined according to the needs of each environment.
The complexity levels allow the distribution of the processing between several resources inside the same scope. In an analysis task, for example, the search for simple attacks can be made by resources of complexity 1, whereas the search for more complex attacks, that demands more time, can be performed by resources of complexity 2. With this, the analysis of the data is made by two resources.
The distinction between complexity levels can also be organized in order to integrate different techniques of intrusion detection.
The complexity level 1 could be defined for analyses based on signatures, which are simpler techniques; the complexity level 2 for techniques based on behaviour, that require greater computational power; and the complexity level 3 for intrusion detection in applications, where the techniques are more specific and depend on more data.
The division of scopes and the complexity levels make the processing of the data to be carried out in phases. No resource has full knowledge about the complete data processing flow. Each resource only knows the results of its processing and the destination to which it sends the results. Resources of higher complexity must be linked to resources of lower complexity.
Therefore, the hierarchic structure of the DIDSoG is maintained, facilitating its extension and integration with other domains of intrusion detection.
By carrying out a hierarchic relationship between the several chosen analysers for an environment, the sensor resource is not overloaded with the task to send the data to all the analysers. An initial analyser will exist (complexity level 1) to which the sensor will send its data, and this analyser will then direct the data to the next step of the processing flow. Another feature of the hierarchical organization is the easy extension and integration with other domains. If it is necessary to add a new host (sensor) to the DIDSoG, it is enough to plug it to the first hierarchy of resources. If it is necessary to add a new analyser, it will be in the scope of several domains, it is enough to relate it to another resource of same scope.
The DIDSoG allows different levels to be managed by different entities. For example, the first scope can be managed by the local user of a host. The second scope, comprising several hosts of a domain can be managed by the administrator of the domain. A third entity can be responsible for managing the security of several domains in a joint way. This entity can act in the scope 3 independently from others.
With the proposed model for integration of IDSs in Grids, the different IDSs of an environment (or multiple IDSs integrated) act in a cooperative manner improving the intrusion detection services, mainly in two aspects. First, the information from multiple sources are analysed in an integrated way to search for distributed attacks. This integration can be made under several scopes. Second, there is a great diversity of data aggregation techniques, data correlation and analysis, and intrusion response that can be applied to the same environment; these techniques can be organized under several levels of complexity.
The integration of heterogeneous IDSs is important. However, the incompatibility and diversity of IDS solutions make such integration extremely difficult. This work thus proposed a model for composition of DIDS by integrating existing IDSs on a computational Grid platform (DIDSoG). IDSs in DIDSoG are encapsulated as Grid services for intrusion detection. A computational Grid platform is used for the integration by providing the basic requirements for communication, localization, resource sharing and security mechanisms.
The components of the architecture of the DIDSoG were developed and evaluated using the GridSim Grid simulator.
Services for communication and localization were used to carry out the integration between components of different resources.
Based on the components of the architecture, several resources were modelled forming a grid of intrusion detection. The simulation demonstrated the usefulness of the proposed model.
Data from the sensor resources was read and this data was used to feed other resources of DIDSoG.
The integration of distinct IDSs could be observed through the simulated environment. Resources providing different intrusion detection services were integrated (e.g. analysis, correlation, aggregation and alert). The communication and localization services provided by GridSim were used to integrate components of different resources. Various resources were modelled following the architecture components forming a grid of intrusion detection.
The components of DIDSoG architecture have served as base for the integration of the resources presented in the simulation.
During the simulation, the different IDSs cooperated with one another in a distributed manner; however, in a coordinated way with an integrated view of the events, having, thus, the capability to detect distributed attacks. This capability demonstrates that the IDSs integrated have resulted in a DIDS.
Related work presents cooperation between components of a specific DIDS. Some work focus on either the development of DIDSs on computational Grids or the application of IDSs to computational Grids. However, none deals with the integration of heterogeneous IDSs. In contrast, the proposed model developed and simulated in this work, can shed some light into the question of integration of heterogeneous IDSs.
DIDSoG presents new research opportunities that we would like to pursue, including: deployment of the model in a more realistic environment such as a Grid; incorporation of new security services; parallel analysis of data by Native IDSs in multiple hosts.
In addition to the integration of IDSs enabled by a grid middleware, the cooperation of heterogeneous IDSs can be viewed as an economic problem. IDSs from different organizations or administrative domains need incentives for joining a grid of intrusion detection services and for collaborating with other IDSs. The development of distributed strategy proof mechanisms for integration of IDSs is a challenge that we would like to tackle.

In huge networks a single fault can cause a burst of failure events. To handle the flood of events and to find the root cause of a fault, event correlation approaches like rule-based reasoning, case-based reasoning or the codebook approach have been developed. The main idea of correlation is to condense and structure events to retrieve meaningful information. Until now, these approaches address primarily the correlation of events as reported from management tools or devices. Therefore, we call them device-oriented.
In this paper we define a service as a set of functions which are offered by a provider to a customer at a customer provider interface. The definition of a service is therefore more general than the definition of a Web Service, but a Web Service is included in this service definition. As a consequence, the results are applicable for Web Services as well as for other kinds of services. A service level agreement (SLA) is defined as a contract between customer and provider about guaranteed service performance.
As in today"s IT environments the offering of such services with an agreed service quality becomes more and more important, this change also affects the event correlation. It has become a necessity for providers to offer such guarantees for a differentiation from other providers. To avoid SLA violations it is especially important for service providers to identify the root cause of a fault in a very short time or even act proactively. The latter refers to the case of recognizing the influence of a device breakdown on the offered services.
As in this scenario the knowledge about services and their SLAs is used we call it service-oriented. It can be addressed from two directions.
Top-down perspective: Several customers report a problem in a certain time interval. Are these trouble reports correlated? How to identify a resource as being the problem"s root cause? 183 Bottom-up perspective: A device (e.g., router, server) breaks down. Which services, and especially which customers, are affected by this fault?
The rest of the paper is organized as follows. In Section
present a selection of the state-of-the-art event correlation techniques. Section 3 describes the motivation for serviceoriented event correlation and its benefits. After having motivated the need for such type of correlation we use two well-known IT service management models to gain requirements for an appropriate workflow modeling and present our proposal for it (see Section 4). In Section 5 we present our information modeling which is derived from the MNM Service Model. An application of the approach for a web hosting scenario is performed in Section 6. The last section concludes the paper and presents future work.
TECHNIQUES In [11] the task of event correlation is defined as a conceptual interpretation procedure in the sense that a new meaning is assigned to a set of events that happen in a certain time interval. We can distinguish between three aspects for event correlation.
Functional aspect: The correlation focuses on functions which are provided by each network element. It is also regarded which other functions are used to provide a specific function.
Topology aspect: The correlation takes into account how the network elements are connected to each other and how they interact.
Time aspect: When explicitly regarding time constraints, a start and end time has to be defined for each event.
The correlation can use time relationships between the events to perform the correlation. This aspect is only mentioned in some papers [11], but it has to be treated in an event correlation system.
In the event correlation it is also important to distinguish between the knowledge acquisition/representation and the correlation algorithm. Examples of approaches to knowledge acquisition/representation are Gruschke"s dependency graphs [6] and Ensel"s dependency detection by neural networks [3]. It is also possible to find the dependencies by analyzing interactions [7]. In addition, there is an approach to manage service dependencies with XML and to define a resource description framework [4].
To get an overview about device-oriented event correlation a selection of several event correlation techniques being used for this kind of correlation is presented.
Model-based reasoning: Model-based reasoning (MBR) [15, 10, 20] represents a system by modeling each of its components. A model can either represent a physical entity or a logical entity (e.g., LAN, WAN, domain, service, business process). The models for physical entities are called functional model, while the models for all logical entities are called logical model. A description of each model contains three categories of information: attributes, relations to other models, and behavior. The event correlation is a result of the collaboration among models.
As all components of a network are represented with their behavior in the model, it is possible to perform simulations to predict how the whole network will behave.
A comparison in [20] showed that a large MBR system is not in all cases easy to maintain. It can be difficult to appropriately model the behavior for all components and their interactions correctly and completely.
An example system for MBR is NetExpert[16] from OSI which is a hybrid MBR/RBR system (in 2000 OSI was acquired by Agilent Technologies).
Rule-based reasoning: Rule-based reasoning (RBR) [15, 10] uses a set of rules for event correlation. The rules have the form conclusion if condition. The condition uses received events and information about the system, while the conclusion contains actions which can either lead to system changes or use system parameters to choose the next rule.
An advantage of the approach is that the rules are more or less human-readable and therefore their effect is intuitive. The correlation has proved to be very fast in practice by using the RETE algorithm.
In the literature [20, 1] it is claimed that RBR systems are classified as relatively inflexible. Frequent changes in the modeled IT environment would lead to many rule updates. These changes would have to be performed by experts as no automation has currently been established. In some systems information about the network topology which is needed for the event correlation is not used explicitly, but is encoded into the rules. This intransparent usage would make rule updates for topology changes quite difficult. The system brittleness would also be a problem for RBR systems.
It means that the system fails if an unknown case occurs, because the case cannot be mapped onto similar cases. The output of RBR systems would also be difficult to predict, because of unforeseen rule interactions in a large rule set. According to [15] an RBR system is only appropriate if the domain for which it is used is small, nonchanging, and well understood.
The GTE IMPACT system [11] is an example of a rulebased system. It also uses MBR (GTE has merged with Bell Atlantic in 1998 and is now called Verizon [19]).
Codebook approach: The codebook approach [12, 21] has similarities to RBR, but takes a further step and encodes the rules into a correlation matrix.
The approach starts using a dependency graph with two kinds of nodes for the modeling. The first kind of nodes are the faults (denoted as problems in the cited papers) which have to be detected, while the second kind of nodes are observable events (symptoms in the papers) which are caused by the faults or other events. The dependencies between the nodes are denoted as directed edges. It is possible to choose weights for the edges, e.g., a weight for the probability that 184 fault/event A causes event B. Another possible weighting could indicate time dependencies. There are several possibilities to reduce the initial graph. If, e.g., a cyclic dependency of events exists and there are no probabilities for the cycles" edges, all events can be treated as one event.
After a final input graph is chosen, the graph is transformed into a correlation matrix where the columns contain the faults and the rows contain the events.
If there is a dependency in the graph, the weight of the corresponding edge is put into the according matrix cell. In case no weights are used, the matrix cells get the values 1 for dependency and 0 otherwise.
Afterwards, a simplification can be done, where events which do not help to discriminate faults are deleted.
There is a trade-off between the minimization of the matrix and the robustness of the results. If the matrix is minimized as much as possible, some faults can only be distinguished by a single event. If this event cannot be reliably detected, the event correlation system cannot discriminate between the two faults. A measure how many event observation errors can be compensated by the system is the Hamming distance. The number of rows (events) that can be deleted from the matrix can differ very much depending on the relationships [15].
The codebook approach has the advantage that it uses long-term experience with graphs and coding. This experience is used to minimize the dependency graph and to select an optimal group of events with respect to processing time and robustness against noise.
A disadvantage of the approach could be that similar to RBR frequent changes in the environment make it necessary to frequently edit the input graph.
SMARTS InCharge [12, 17] is an example of such a correlation system.
Case-based reasoning: In contrast to other approaches case-based reasoning (CBR) [14, 15] systems do not use any knowledge about the system structure. The knowledge base saves cases with their values for system parameters and successful recovery actions for these cases. The recovery actions are not performed by the CBR system in the first place, but in most cases by a human operator.
If a new case appears, the CBR system compares the current system parameters with the system parameters in prior cases and tries to find a similar one. To identify such a match it has to be defined for which parameters the cases can differ or have to be the same.
If a match is found, a learned action can be performed automatically or the operator can be informed with a recovery proposal.
An advantage of this approach is that the ability to learn is an integral part of it which is important for rapid changing environments.
There are also difficulties when applying the approach [15]. The fields which are used to find a similar case and their importance have to be defined appropriately.
If there is a match with a similar case, an adaptation of the previous solution to the current one has to be found.
An example system for CBR is SpectroRx from Cabletron Systems. The part of Cabletron that developed SpectroRx became an independent software company in 2002 and is now called Aprisma Management Technologies [2].
In this section four event correlation approaches were presented which have evolved into commercial event correlation systems. The correlation approaches have different focuses.
MBR mainly deals with the knowledge acquisition and representation, while RBR and the codebook approach propose a correlation algorithm. The focus of CBR is its ability to learn from prior cases.
EVENT CORRELATION Fig. 1 shows a general service scenario upon which we will discuss the importance of a service-oriented correlation.
Several services like SSH, a web hosting service, or a video conference service are offered by a provider to its customers at the customer provider interface. A customer can allow several users to use a subscribed service. The quality and cost issues of the subscribed services between a customer and a provider are agreed in SLAs. On the provider side the services use subservices for their provisioning. In case of the services mentioned above such subservices are DNS, proxy service, and IP service. Both services and subservices depend on resources upon which they are provisioned. As displayed in the figure a service can depend on more than one resource and a resource can be used by one or more services.
SSH DNS proxy IP service dependency resource dependency user a user b user c customer SLA web a video conf.
SSH sun1 provider video conf. web services subservices resources Figure 1: Scenario To get a common understanding, we distinguish between different types of events: Resource event: We use the term resource event for network events and system events. A network event refers to events like node up/down or link up/down whereas system events refer to events like server down or authentication failure.
Service event: A service event indicates that a service does not work properly. A trouble ticket which is generated from a customer report is a kind of such an 185 event. Other service events can be generated by the provider of a service, if the provider himself detects a service malfunction.
In such a scenario the provider may receive service events from customers which indicate that SSH, web hosting service, and video conference service are not available. When referring to the service hierarchy, the provider can conclude in such a case that all services depend on DNS. Therefore, it seems more likely that a common resource which is necessary for this service does not work properly or is not available than to assume three independent service failures. In contrast to a resource-oriented perspective where all of the service events would have to be processed separately, the service events can be linked together. Their information can be aggregated and processed only once. If, e.g., the problem is solved, one common message to the customers that their services are available again is generated and distributed by using the list of linked service events. This is certainly a simplified example. However, it shows the general principle of identifying the common subservices and common resources of different services.
It is important to note that the service-oriented perspective is needed to integrate service aspects, especially QoS aspects. An example of such an aspect is that a fault does not lead to a total failure of a service, but its QoS parameters, respectively agreed service levels, at the customer-provider interface might not be met. A degradation in service quality which is caused by high traffic load on the backbone is another example. In the resource-oriented perspective it would be possible to define events which indicate that there is a link usage higher than a threshold, but no mechanism has currently been established to find out which services are affected and whether a QoS violation occurs.
To summarize, the reasons for the necessity of a serviceoriented event correlation are the following: Keeping of SLAs (top-down perspective): The time interval between the first symptom (recognized either by provider, network management tools, or customers) that a service does not perform properly and the verified fault repair needs to be minimized. This is especially needed with respect to SLAs as such agreements often contain guarantees like a mean time to repair.
Effort reduction (top-down perspective): If several user trouble reports are symptoms of the same fault, fault processing should be performed only once and not several times. If the fault has been repaired, the affected customers should be informed about this automatically.
Impact analysis (bottom-up perspective): In case of a fault in a resource, its influence on the associated services and affected customers can be determined. This analysis can be performed for short term (when there is currently a resource failure) or long term (e.g., network optimization) considerations.
In the following we examine the established IT process management frameworks IT Infrastructure Library (ITIL) and enhanced Telecom Operations Map (eTOM). The aim is find out where event correlation can be found in the process structure and how detailed the frameworks currently are.
After that we present our solution for a workflow modeling for the service-oriented event correlation.
The British Office of Government Commerce (OGC) and the IT Service Management Forum (itSMF) [9] provide a collection of best practices for IT processes in the area of IT service management which is called ITIL. The service management is described by 11 modules which are grouped into Service Support Set (provider internal processes) and Service Delivery Set (processes at the customer-provider interface). Each module describes processes, functions, roles, and responsibilities as well as necessary databases and interfaces. In general, ITIL describes contents, processes, and aims at a high abstraction level and contains no information about management architectures and tools.
The fault management is divided into Incident Management process and Problem Management process.
Incident Management: The Incident Management contains the service desk as interface to customers (e.g., receives reports about service problems). In case of severe errors structured queries are transferred to the Problem Management.
Problem Management: The Problem Management"s tasks are to solve problems, take care of keeping priorities, minimize the reoccurrence of problems, and to provide management information. After receiving requests from the Incident Management, the problem has to be identified and information about necessary countermeasures is transferred to the Change Management.
The ITIL processes describe only what has to be done, but contain no information how this can be actually performed.
As a consequence, event correlation is not part of the modeling. The ITIL incidents could be regarded as input for the service-oriented event correlation, while the output could be used as a query to the ITIL Problem Management.
(eTOM) The TeleManagement Forum (TMF) [18] is an international non-profit organization from service providers and suppliers in the area of telecommunications services. Similar to ITIL a process-oriented framework has been developed at first, but the framework was designed for a narrower focus, i.e., the market of information and communications service providers. A horizontal grouping into processes for customer care, service development & operations, network & systems management, and partner/supplier is performed. The vertical grouping (fulfillment, assurance, billing) reflects the service life cycle.
In the area of fault management three processes have been defined along the horizontal process grouping.
Problem Handling: The purpose of this process is to receive trouble reports from customers and to solve them by using the Service Problem Management. The aim is also to keep the customer informed about the current status of the trouble report processing as well as about the general network status (e.g., planned maintenance). It is also a task of this process to inform the 186 QoS/SLA management about the impact of current errors on the SLAs.
Service Problem Management: In this process reports about customer-affecting service failures are received and transformed. Their root causes are identified and a problem solution or a temporary workaround is established. The task of the Diagnose Problem subprocess is to find the root cause of the problem by performing appropriate tests. Nothing is said how this can be done (e.g., no event correlation is mentioned).
Resource Trouble Management: A subprocess of the Resource Trouble Management is responsible for resource failure event analysis, alarm correlation & filtering, and failure event detection & reporting.
Another subprocess is used to execute different tests to find a resource failure. There is also another subprocess which keeps track about the status of the trouble report processing. This subprocess is similar to the functionality of a trouble ticket system.
The process description in eTOM is not very detailed. It is useful to have a check list which aspects for these processes have to be taken into account, but there is no detailed modeling of the relationships and no methodology for applying the framework. Event correlation is only mentioned in the resource management, but it is not used in the service level.
Service-Oriented Event Correlation Fig. 2 shows a general service scenario which we will use as basis for the workflow modeling for the service-oriented event correlation. We assume that the dependencies are already known (e.g., by using the approaches mentioned in Section 2). The provider offers different services which depend on other services called subservices (service dependency). Another kind of dependency exists between services/subservices and resources. These dependencies are called resource dependencies. These two kinds of dependencies are in most cases not used for the event correlation performed today. This resource-oriented event correlation deals only with relationships on the resource level (e.g., network topology). service dependency resources subservices provider services resource dependency Figure 2: Different kinds of dependencies for the service-oriented event correlation The dependencies depicted in Figure 2 reflect a situation with no redundancy in the service provisioning. The relationships can be seen as AND relationships. In case of redundancy, if e.g., a provider has 3 independent web servers, another modeling (see Figure 3) should be used (OR relationship). In such a case different relationships are possible.
The service could be seen as working properly if one of the servers is working or a certain percentage of them is working. services ) AND relationship b) OR relationship resources Figure 3: Modeling of no redundancy (a) and of redundancy (b) As both ITIL and eTOM contain no description how event correlation and especially service-oriented event correlation should actually be performed, we propose the following design for such a workflow (see Fig. 4). The additional components which are not part of a device-oriented event correlation are depicted with a gray background. The workflow is divided into the phases fault detection, fault diagnosis, and fault recovery.
In the fault detection phase resource events and service events can be generated from different sources. The resource events are issued during the use of a resource, e.g., via SNMP traps. The service events are originated from customer trouble reports, which are reported via the Customer Service Management (see below) access point. In addition to these two passive ways to get the events, a provider can also perform active tests. These tests can either deal with the resources (resource active probing) or can assume the role of a virtual customer and test a service or one of its subservices by performing interactions at the service access points (service active probing).
The fault diagnosis phase is composed of three event correlation steps. The first one is performed by the resource event correlator which can be regarded as the event correlator in today"s commercial systems. Therefore, it deals only with resource events. The service event correlator does a correlation of the service events, while the aggregate event correlator finally performs a correlation of both resource and service events. If the correlation result in one of the correlation steps shall be improved, it is possible to go back to the fault detection phase and start the active probing to get additional events. These events can be helpful to confirm a correlation result or to reduce the list of possible root causes.
After the event correlation an ordered list of possible root causes is checked by the resource management. When the root cause is found, the failure repair starts. This last step is performed in the fault recovery phase.
The next subsections present different elements of the event correlation process.
Intelligent Assistant The Customer Service Management (CSM) access point was proposed by [13] as a single interface between customer 187 fault detection fault diagnosis resource active probing resource event resource event correlator resource management candidate list fault recovery resource usage service active probing intelligent assistant service event correlator aggregate event correlator service eventCSM AP Figure 4: Event correlation workflow and provider. Its functionality is to provide information to the customer about his subscribed services, e.g., reports about the fulfillment of agreed SLAs. It can also be used to subscribe services or to allow the customer to manage his services in a restricted way. Reports about problems with a service can be sent to the customer via CSM. The CSM is also contained in the MNM Service Model (see Section 5).
To reduce the effort for the provider"s first level support, an Intelligent Assistant can be added to the CSM. The Intelligent Assistant structures the customer"s information about a service problem. The information which is needed for a preclassification of the problem is gathered from a list of questions to the customer. The list is not static as the current question depends on the answers to prior questions or from the result of specific tests. A decision tree is used to structure the questions and tests. The tests allow the customer to gain a controlled access to the provider"s management. At the LRZ a customer of the E-Mail Service can, e.g., use the Intelligent Assistant to perform ping requests to the mail server. But also more complex requests could be possible, e.g., requests of a combination of SNMP variables.
Active probing is useful for the provider to check his offered services. The aim is to identify and react to problems before a customer notices them. The probing can be done from a customer point of view or by testing the resources which are part of the services. It can also be useful to perform tests of subservices (own subservices or subservices offered by suppliers).
Different schedules are possible to perform the active probing. The provider could select to test important services and resources in regular time intervals. Other tests could be initiated by a user who traverses the decision tree of the Intelligent Assistant including active tests. Another possibility for the use of active probing is a request from the event correlator, if the current correlation result needs to be improved. The results of active probing are reported via service or resource events to the event correlator (or if the test was demanded by the Intelligent Assistant the result is reported to it, too). While the events that are received from management tools and customers denote negative events (something does not work), the events from active probing should also contain positive events for a better discrimination.
The event correlation should not be performed by a single event correlator, but by using different steps. The reason for this are the different characteristics of the dependencies (see Fig. 1).
On the resource level there are only relationships between resources (network topology, systems configuration). An example for this could be a switch linking separate LANs. If the switch is down, events are reported that other network components which are located behind the switch are also not reachable. When correlating these events it can be figured out that the switch is the likely error cause. At this stage, the integration of service events does not seem to be helpful.
The result of this step is a list of resources which could be the problem"s root cause. The resource event correlator is used to perform this step.
In the service-oriented scenario there are also service and resource dependencies. As next step in the event correlation process the service events should be correlated with each other using the service dependencies, because the service dependencies have no direct relationship to the resource level. The result of this step, which is performed by the service event correlator, is a list of services/subservices which could contain a failure in a resource. If, e.g., there are service events from customers that two services do not work and both services depend on a common subservice, it seems more likely that the resource failure can be found inside the subservice. The output of this correlation is a list of services/subservices which could be affected by a failure in an associated resource.
In the last step the aggregate event correlator matches the lists from resource event correlator and service event correlator to find the problem"s possible root cause. This is done by using the resource dependencies.
The event correlation techniques presented in Section 2 could be used to perform the correlation inside the three event correlators. If the dependencies can be found precisely, an RBR or codebook approach seems to be appropriate. A case database (CBR) could be used if there are cases which could not be covered by RBR or the codebook approach.
These cases could then be used to improve the modeling in a way that RBR or the codebook approach can deal with them in future correlations.
In this section we use a generic model for IT service management to derive the information necessary for the event correlation process.
The MNM Service Model [5] is a generic model for IT service management. A distinction is made between customer side and provider side. The customer side contains the basic roles customer and user, while the provider side contains the role provider. The provider makes the service available to the customer side. The service as a whole is divided into usage which is accessed by the role user and management which is used by the role customer.
The model consists of two main views. The Service View (see Fig. 5) shows a common perspective of the service for customer and provider. Everything that is only important 188 for the service realization is not contained in this view. For these details another perspective, the Realization View, is defined (see Fig. 6). customer domain supplies supplies provider domain «role» provider accesses uses concludes accesses implements observesrealizes provides directs substantiates usesuses manages implementsrealizes manages service concludes QoS parameters usage functionality service access point management functionality service implementation service management implementation service agreement customersideprovidersidesideindependent «role» user «role» customer CSM access point service client CSM client Figure 5: Service View The Service View contains the service for which the functionality is defined for usage as well as for management. There are two access points (service access point and CSM access point) where user and customer can access the usage and management functionality, respectively. Associated to each service is a list of QoS parameters which have to be met by the service at the service access point. The QoS surveillance is performed by the management. provider domain implements observesrealizes provides directs implementsrealizes accesses uses concludes accesses usesuses manages side independent side independent manages manages concludes acts as service implementation service management implementation manages uses acts as service logic sub-service client service client CSM client uses resources usesuses service management logic sub-service management client basic management functionality «role» customer «role» provider «role» user Figure 6: Realization View In the Realization View the service implementation and the service management implementation are described in detail.
For both there are provider-internal resources and subservices. For the service implementation a service logic uses internal resources (devices, knowledge, staff) and external subservices to provide the service. Analogous, the service management implementation includes a service management logic using basic management functionalities [8] and external management subservices.
The MNM Service Model can be used for a similar modeling of the used subservices, i.e., the model can be applied recursively.
As the service-oriented event correlation has to use dependencies of a service from subservices and resources, the model is used in the following to derive the needed information for service events.
Today"s event correlation deals mainly with events which are originated from resources. Beside a resource identifier these events contain information about the resource status, e.g., SNMP variables. To perform a service-oriented event correlation it is necessary to define events which are related to services. These events can be generated from the provider"s own service surveillance or from customer reports at the CSM interface. They contain information about the problems with the agreed QoS. In our information modeling we define an event superclass which contains common attributes (e.g., time stamp). Resource event and service event inherit from this superclass.
Derived from the MNM Service Model we define the information necessary for a service event.
Service: As a service event shall represent the problems of a single service, a unique identification of the affected service is contained here.
Event description: This field has to contain a description of the problem. Depending on the interactions at the service access point (Service View) a classification of the problem into different categories should be defined.
It should also be possible to add an informal description of the problem.
QoS parameters: For each service QoS parameters (Service View) are defined between the provider and the customer. This field represents a list of these QoS parameters and agreed service levels. The list can help the provider to set the priority of a problem with respect to the service levels agreed.
Resource list: This list contains the resources (Realization View) which are needed to provide the service. This list is used by the provider to check if one of these resources causes the problem.
Subservice service event identification: In the service hierarchy (Realization View) the service, for which this service event has been issued, may depend on subservices. If there is a suspicion that one of these subservices causes the problem, child service events are issued from this service event for the subservices. In such a case this field contains links to the corresponding events.
Other event identifications: In the event correlation process the service event can be correlated with other service events or with resource events. This field then contains links to other events which have been correlated to this service event. This is useful to, e.g., send a common message to all affected customers when their subscribed services are available again.
Issuer"s identification: This field can either contain an identification of the customer who reported the problem, an identification of a service provider"s employee 189 (in case the failure has been detected by the provider"s own service active probing) or a link to a parent service event. The identification is needed, if there are ambiguities in the service event or the issuer should be informed (e.g., that the service is available again).
The possible issuers refer to the basic roles (customer, provider) in the Service Model.
Assignee: To keep track of the processing the name and address of the provider"s employee who is solving or solved the problem is also noted. This is a specialization of the provider role in the Service Model.
Dates: This field contains key dates in the processing of the service event such as initial date, problem identification date, resolution date. These dates are important to keep track how quick the problems have been solved.
Status: This field represents the service event"s actual status (e.g., active, suspended, solved).
Priority: The priority shows which importance the service event has from the provider"s perspective. The importance is derived from the service agreement, especially the agreed QoS parameters (Service View).
The fields date, status, and other service events are not derived directly from the Service Model, but are necessary for the event correlation process.
SERVICE-ORIENTED EVENT CORRELATION FOR A WEB HOSTING SCENARIO The Leibniz Supercomputing Center is the joint computing center for the Munich universities and research institutions. It also runs the Munich Scientific Network and offers related services. One of these services is the Virtual WWW Server, a web hosting offer for smaller research institutions.
It currently has approximately 200 customers.
A subservice of the Virtual WWW Server is the Storage Service which stores the static and dynamic web pages and uses caching techniques for a fast access. Other subservices are DNS and IP service. When a user accesses a hosted web site via one of the LRZ"s Virtual Private Networks the VPN service is also used. The resources of the Virtual WWW Server include a load balancer and 5 redundant servers. The network connections are also part of the resources as well as the Apache web server application running on the servers. Figure 7 shows the dependencies of the Virtual WWW Server.
Intelligent Assistant The Intelligent Assistant that is available at the Leibniz Supercomputing Center can currently be used for connectivity or performance problems or problems with the LRZ E-Mail Service. A selection of possible customer problem reports for the Virtual WWW Server is given in the following: • The hosted web site is not reachable. • The web site access is (too) slow. • The web site contains outdated content. server serverserver server server server server server server outgoing connection hosting of LRZ"s own pages content caching server emergency server webmail server dynamic web pages static web pages DNS ProxyIP Storage Resources: Services: Virtual WWW Server five redundant servers AFS NFS DBload balancer Figure 7: Dependencies of the Virtual WWW Server • The transfer of new content to the LRZ does not change the provided content. • The web site looks strange (e.g., caused by problems with HTML version) This customer reports have to be mapped onto failures in resources. For, e.g., an unreachable web site different root causes are possible like a DNS problem, connectivity problem, wrong configuration of the load balancer.
In general, active probing can be used for services or resources. For the service active probing of the Virtual WWW Server a virtual customer could be installed. This customer does typical HTTP requests of web sites and compares the answer with the known content. To check the up-to-dateness of a test web site, the content could contain a time stamp.
The service active probing could also include the testing of subservices, e.g., sending requests to the DNS.
The resource active probing performs tests of the resources.
Examples are connectivity tests, requests to application processes, and tests of available disk space.
Server Figure 8 shows the example processing. At first, a customer who takes a look at his hosted web site reports that the content that he had changed is not displayed correctly.
This report is transferred to the service management via the CSM interface. An Intelligent Assistant could be used to structure the customer report. The service management translates the customer report into a service event.
Independent from the customer report the service provider"s own service active probing tries to change the content of a test web site. Because this is not possible, a service event is issued.
Meanwhile, a resource event has been reported to the event correlator, because an access of the content caching server to one of the WWW servers failed. As there are no other events at the moment the resource event correlation 190 customer CSM service mgmt event correlator resource mgmt customer reports: "web site content not up−to−date" service active probing reports: "web site content change not possible" event: "retrieval of server content failed"event forward resource event correlation service event correlation aggregate event correlation link failure report event forward check WWW server check link result display link repair result display result forward customer report Figure 8: Example processing of a customer report cannot correlate this event to other events. At this stage it would be possible that the event correlator asks the resource management to perform an active probing of related resources.
Both service events are now transferred to the service event correlator and are correlated. From the correlation of these events it seems likely that either the WWW server itself or the link to the WWW server is the problem"s root cause. A wrong web site update procedure inside the content caching server seems to be less likely as this would only explain the customer report and not the service active probing result. At this stage a service active probing could be started, but this does not seem to be useful as this correlation only deals with the Web Hosting Service and its resources and not with other services.
After the separate correlation of both resource and service events, which can be performed in parallel, the aggregate event correlator is used to correlate both types of events.
The additional resource event makes it seem much more likely that the problems are caused by a broken link to the WWW server or by the WWW server itself and not by the content caching server. In this case the event correlator asks the resource management to check the link and the WWW server. The decision between these two likely error causes can not be further automated here.
Later, the resource management finds out that a broken link is the failure"s root cause. It informs the event correlator about this and it can be determined that this explains all previous events. Therefore, the event correlation can be stopped at this point.
Depending on the provider"s customer relationship management the finding of the root cause and an expected repair time could be reported to the customers. After the link has been repaired, it is possible to report this event via the CSM interface.
Even though many details of this event correlation process could also be performed differently, the example showed an important advantage of the service-oriented event correlation. The relationship between the service provisioning and the provider"s resources is explicitly modeled. This allows a mapping of the customer report onto the provider-internal resources.
If a provider like the LRZ offers several services the serviceoriented event correlation can be used to reveal relationships that are not obvious in the first place. If the LRZ E-Mail Service and its events are viewed in relationship with the events for the Virtual WWW Server, it is possible to identify failures in common subservices and resources. Both services depend on the DNS which means that customer reports like I cannot retrieve new e-mail and The web site of my research institute is not available can have a common cause, e.g., the DNS does not work properly.
In our paper we showed the need for a service-oriented event correlation. For an IT service provider this new kind of event correlation makes it possible to automatically map problems with the current service quality onto resource failures. This helps to find the failure"s root cause earlier and to reduce costs for SLA violations. In addition, customer reports can be linked together and therefore the processing effort can be reduced.
To receive these benefits we presented our approach for performing the service-oriented event correlation as well as a modeling of the necessary correlation information. In the future we are going to apply our workflow and information modeling for services offered by the Leibniz Supercomputing Center going further into details.
Several issues have not been treated in detail so far, e.g., the consequences for the service-oriented event correlation if a subservice is offered by another provider. If a service does not perform properly, it has to be determined whether this is caused by the provider himself or by the subservice. In the latter case appropriate information has to be exchanged between the providers via the CSM interface. Another issue is the use of active probing in the event correlation process which can improve the result, but can also lead to a correlation delay.
Another important point is the precise definition of dependency which has also been left out by many other publications. To avoid having to much dependencies in a certain situation one could try to check whether the dependencies currently exist. In case of a download from a web site there is only a dependency from the DNS subservice at the beginning, but after the address is resolved a download failure is unlikely to have been caused by the DNS. Another possibility to reduce the dependencies is to divide a service into its possible user interactions (e.g., an e-mail service into transactions like get mail, sent mail, etc) and to define the dependencies for each user interaction.
Acknowledgments The authors wish to thank the members of the Munich Network Management (MNM) Team for helpful discussions and valuable comments on previous versions of the paper. The MNM Team, directed by Prof. Dr. Heinz-Gerd Hegering, is a 191 group of researchers of the Munich Universities and the Leibniz Supercomputing Center of the Bavarian Academy of Sciences. Its web server is located at wwwmnmteam.informatik. uni-muenchen.de.
[1] K. Appleby, G. Goldszmidt, and M. Steinder.
Yemanja - A Layered Event Correlation Engine for Multi-domain Server Farms. In Proceedings of the Seventh IFIP/IEEE International Symposium on Integrated Network Management, pages 329-344.
IFIP/IEEE, May 2001. [2] Spectrum, Aprisma Corporation. http://www.aprisma.com. [3] C. Ensel. New Approach for Automated Generation of Service Dependency Models. In Network Management as a Strategy for Evolution and Development; Second Latin American Network Operation and Management Symposium (LANOMS 2001). IEEE, August 2001. [4] C. Ensel and A. Keller. An Approach for Managing Service Dependencies with XML and the Resource Description Framework. Journal of Network and Systems Management, 10(2), June 2002. [5] M. Garschhammer, R. Hauck, H.-G. Hegering,
B. Kempter, M. Langer, M. Nerb, I. Radisic,
H. Roelle, and H. Schmidt. Towards generic Service Management Concepts - A Service Model Based Approach. In Proceedings of the Seventh IFIP/IEEE International Symposium on Integrated Network Management, pages 719-732. IFIP/IEEE, May 2001. [6] B. Gruschke. Integrated Event Management: Event Correlation using Dependency Graphs. In Proceedings of the 9th IFIP/IEEE International Workshop on Distributed Systems: Operations & Management (DSOM 98). IEEE/IFIP, October 1998. [7] M. Gupta, A. Neogi, M. Agarwal, and G. Kar.
Discovering Dynamic Dependencies in Enterprise Environments for Problem Determination. In Proceedings of the 14th IFIP/IEEE Workshop on Distributed Sytems: Operations and Management.
IFIP/IEEE, October 2003. [8] H.-G. Hegering, S. Abeck, and B. Neumair. Integrated Management of Networked Systems - Concepts,
Architectures and their Operational Application.
Morgan Kaufmann Publishers, 1999. [9] IT Infrastructure Library, Office of Government Commerce and IT Service Management Forum. http://www.itil.co.uk. [10] G. Jakobson and M. Weissman. Alarm Correlation.
IEEE Network, 7(6), November 1993. [11] G. Jakobson and M. Weissman. Real-time Telecommunication Network Management: Extending Event Correlation with Temporal Constraints. In Proceedings of the Fourth IEEE/IFIP International Symposium on Integrated Network Management, pages 290-301. IEEE/IFIP, May 1995. [12] S. Kliger, S. Yemini, Y. Yemini, D. Ohsie, and S. Stolfo. A Coding Approach to Event Correlation. In Proceedings of the Fourth IFIP/IEEE International Symposium on Integrated Network Management, pages 266-277. IFIP/IEEE, May 1995. [13] M. Langer, S. Loidl, and M. Nerb. Customer Service Management: A More Transparent View To Your Subscribed Services. In Proceedings of the 9th IFIP/IEEE International Workshop on Distributed Systems: Operations & Management (DSOM 98),
Newark, DE, USA, October 1998. [14] L. Lewis. A Case-based Reasoning Approach for the Resolution of Faults in Communication Networks. In Proceedings of the Third IFIP/IEEE International Symposium on Integrated Network Management.
IFIP/IEEE, 1993. [15] L. Lewis. Service Level Management for Enterprise Networks. Artech House, Inc., 1999. [16] NETeXPERT, Agilent Technologies. http://www.agilent.com/comms/OSS. [17] InCharge, Smarts Corporation. http://www.smarts.com. [18] Enhanced Telecom Operations Map, TeleManagement Forum. http://www.tmforum.org. [19] Verizon Communications. http://www.verizon.com. [20] H. Wietgrefe, K.-D. Tuchs, K. Jobmann, G. Carls,
P. Froelich, W. Nejdl, and S. Steinfeld. Using Neural Networks for Alarm Correlation in Cellular Phone Networks. In International Workshop on Applications of Neural Networks to Telecommunications (IWANNT), May 1997. [21] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie. High Speed and Robust Event Correlation.

A distributed computation consists of a set of processes that cooperate to achieve a common goal. A main characteristic of these computations lies in the fact that the processes do not share a common global memory, and communicate only by exchanging messages over a communication network. Moreover, message transfer delays are finite but unpredictable. This computation model defines what is known as the asynchronous distributed system model. It is particularly important as it includes systems that span large geographic areas, and systems that are subject to unpredictable loads. Consequently, the concepts, tools and mechanisms developed for asynchronous distributed systems reveal to be both important and general.
Causality is a key concept to understand and master the behavior of asynchronous distributed systems [18]. More precisely, given two events e and f of a distributed computation, a crucial problem that has to be solved in a lot of distributed applications is to know whether they are causally related, i.e., if the occurrence of one of them is a consequence of the occurrence of the other. The causal past of an event e is the set of events from which e is causally dependent.
Events that are not causally dependent are said to be concurrent. Vector clocks [5, 16] have been introduced to allow processes to track causality (and concurrency) between the events they produce. The timestamp of an event produced by a process is the current value of the vector clock of the corresponding process. In that way, by associating vector timestamps with events it becomes possible to safely decide whether two events are causally related or not.
Usually, according to the problem he focuses on, a designer is interested only in a subset of the events produced by a distributed execution (e.g., only the checkpoint events are meaningful when one is interested in determining consistent global checkpoints [12]). It follows that detecting causal dependencies (or concurrency) on all the events of the distributed computation is not desirable in all applications [7, 15]. In other words, among all the events that may occur in a distributed computation, only a subset of them are relevant. In this paper, we are interested in the restriction of the causality relation to the subset of events defined as being the relevant events of the computation.
Being a strict partial order, the causality relation is transitive. As a consequence, among all the relevant events that causally precede a given relevant event e, only a subset are its immediate predecessors: those are the events f such that there is no relevant event on any causal path from f to e.
Unfortunately, given only the vector timestamp associated with an event it is not possible to determine which events of its causal past are its immediate predecessors. This comes from the fact that the vector timestamp associated with e determines, for each process, the last relevant event belong210 ing to the causal past of e, but such an event is not necessarily an immediate predecessor of e. However, some applications [4, 6] require to associate with each relevant event only the set of its immediate predecessors. Those applications are mainly related to the analysis of distributed computations.
Some of those analyses require the construction of the lattice of consistent cuts produced by the computation [15, 16].
It is shown in [4] that the tracking of immediate predecessors allows an efficient on the fly construction of this lattice.
More generally, these applications are interested in the very structure of the causal past. In this context, the determination of the immediate predecessors becomes a major issue [6]. Additionally, in some circumstances, this determination has to satisfy behavior constraints. If the communication pattern of the distributed computation cannot be modified, the determination has to be done without adding control messages. When the immediate predecessors are used to monitor the computation, it has to be done on the fly.
We call Immediate Predecessor Tracking (IPT) the problem that consists in determining on the fly and without additional messages the immediate predecessors of relevant events. This problem consists actually in determining the transitive reduction (Hasse diagram) of the causality graph generated by the relevant events of the computation.
Solving this problem requires tracking causality, hence using vector clocks. Previous works have addressed the efficient implementation of vector clocks to track causal dependence on relevant events. Their aim was to reduce the size of timestamps attached to messages. An efficient vector clock implementation suited to systems with fifo channels is proposed in [19]. Another efficient implementation that does not depend on channel ordering property is described in [11]. The notion of causal barrier is introduced in [2, 17] to reduce the size of control information required to implement causal multicast. However, none of these papers considers the IPT problem. This problem has been addressed for the first time (to our knowledge) in [4, 6] where an IPT protocol is described, but without correctness proof. Moreover, in this protocol, timestamps attached to messages are of size n. This raises the following question which, to our knowledge, has never been answered: Are there efficient vector clock implementation techniques that are suitable for the IPT problem?.
This paper has three main contributions: (1) a positive answer to the previous open question, (2) the design of a family of efficient IPT protocols, and (3) a formal correctness proof of the associated protocols. From a methodological point of view the paper uses a top-down approach. It states abstract properties from which more concrete properties and protocols are derived. The family of IPT protocols is defined by a general condition that allows application messages to piggyback control information whose size can be smaller than the system size (i.e., smaller than the number of processes composing the system). In that sense, this family defines low cost IPT protocols when we consider the message size. In addition to efficiency, the proposed approach has an interesting design property. Namely, the family is incrementally built in three steps. The basic vector clock protocol is first enriched by adding to each process a boolean vector whose management allows the processes to track the immediate predecessor events. Then, a general condition is stated to reduce the size of the control information carried by messages. Finally, according to the way this condition is implemented, three IPT protocols are obtained.
The paper is composed of seven sections. Sections 2 introduces the computation model, vector clocks and the notion of relevant events. Section 3 presents the first step of the construction that results in an IPT protocol in which each message carries a vector clock and a boolean array, both of size n (the number of processes). Section 4 improves this protocol by providing the general condition that allows a message to carry control information whose size can be smaller than n. Section 5 provides instantiations of this condition. Section 6 provides a simulation study comparing the behaviors of the proposed protocols. Finally, Section 7 concludes the paper. (Due to space limitations, proofs of lemmas and theorems are omitted. They can be found in [1].)
A distributed program is made up of sequential local programs which communicate and synchronize only by exchanging messages. A distributed computation describes the execution of a distributed program. The execution of a local program gives rise to a sequential process. Let {P1, P2, . . . ,
Pn} be the finite set of sequential processes of the distributed computation. Each ordered pair of communicating processes (Pi, Pj ) is connected by a reliable channel cij through which Pi can send messages to Pj. We assume that each message is unique and a process does not send messages to itself1 .
Message transmission delays are finite but unpredictable.
Moreover, channels are not necessarily fifo. Process speeds are positive but arbitrary. In other words, the underlying computation model is asynchronous.
The local program associated with Pi can include send, receive and internal statements. The execution of such a statement produces a corresponding send/receive/internal event. These events are called primitive events. Let ex i be the x-th event produced by process Pi. The sequence hi = e1 i e2 i . . . ex i . . . constitutes the history of Pi, denoted Hi. Let H = ∪n i=1Hi be the set of events produced by a distributed computation. This set is structured as a partial order by Lamport"s happened before relation [14] (denoted hb →) and defined as follows: ex i hb → ey j if and only if (i = j ∧ x + 1 = y) (local precedence) ∨ (∃m : ex i = send(m) ∧ ey j = receive(m)) (msg prec.) ∨ (∃ ez k : ex i hb → ez k ∧ e z k hb → ey j ) (transitive closure). max(ex i , ey j ) is a partial function defined only when ex i and ey j are ordered. It is defined as follows: max(ex i , ey j ) = ex i if ey j hb → ex i , max(ex i , ey j ) = ey i if ex i hb → ey j .
Clearly the restriction of hb → to Hi, for a given i, is a total order. Thus we will use the notation ex i < ey i iff x < y.
Throughout the paper, we will use the following notation: if e ∈ Hi is not the first event produced by Pi, then pred(e) denotes the event immediately preceding e in the sequence Hi. If e is the first event produced by Pi, then pred(e) is denoted by ⊥ (meaning that there is no such event), and ∀e ∈ Hi : ⊥ < e. The partial order bH = (H, hb →) constitutes a formal model of the distributed computation it is associated with. 1 This assumption is only in order to get simple protocols. 211 P1 P2 P3 [1, 1, 2] [1, 0, 0] [3, 2, 1] [1, 1, 0] (2, 1) [0, 0, 1] (3, 1) [2, 0, 1] (1, 1) (1, 3)(1, 2) (2, 2) (2, 3) (3, 2) [2, 2, 1] [2, 3, 1] (1, 1) (1, 2) (1, 3) (2, 1) (2, 2) (2, 3) (3, 1) (3, 2) Figure 1: Timestamped Relevant Events and Immediate Predecessors Graph (Hasse Diagram)
For a given observer of a distributed computation, only some events are relevant2 [7, 9, 15]. An interesting example of what an observation is, is the detection of predicates on consistent global states of a distributed computation [3, 6, 8, 9, 13, 15]. In that case, a relevant event corresponds to the modification of a local variable involved in the global predicate. Another example is the checkpointing problem where a relevant event is the definition of a local checkpoint [10, 12, 20].
The left part of Figure 1 depicts a distributed computation using the classical space-time diagram. In this figure, only relevant events are represented. The sequence of relevant events produced by process Pi is denoted by Ri, and R = ∪n i=1Ri ⊆ H denotes the set of all relevant events. Let → be the relation on R defined in the following way: ∀ (e, f) ∈ R × R : (e → f) ⇔ (e hb → f).
The poset (R, →) constitutes an abstraction of the distributed computation [7]. In the following we consider a distributed computation at such an abstraction level.
Moreover, without loss of generality we consider that the set of relevant events is a subset of the internal events (if a communication event has to be observed, a relevant internal event can be generated just before a send and just after a receive communication event occurred). Each relevant event is identified by a pair (process id, sequence number) (see Figure 1).
Definition 1. The relevant causal past of an event e ∈ H is the (partially ordered) subset of relevant events f such that f hb → e. It is denoted ↑ (e). We have ↑ (e) = {f ∈ R | f hb → e}.
Note that, if e ∈ R then ↑ (e) = {f ∈ R | f → e}. In the computation described in Figure 1, we have, for the event e identified (2, 2): ↑ (e) = {(1, 1), (1, 2), (2, 1), (3, 1)}.
The following properties are immediate consequences of the previous definitions. Let e ∈ H.
CP1 If e is not a receive event then ↑ (e) = 8 < : ∅ if pred(e) = ⊥, ↑ (pred(e)) ∪ {pred(e)} if pred(e) ∈ R, ↑ (pred(e)) if pred(e) ∈ R.
CP2 If e is a receive event (of a message m) then ↑ (e) = 8 >>< >>: ↑ (send(m)) if pred(e) = ⊥, ↑ (pred(e))∪ ↑ (send(m)) ∪ {pred(e)} if pred(e) ∈ R, ↑ (pred(e))∪ ↑ (send(m)) if pred(e) ∈ R. 2 Those events are sometimes called observable events.
Definition 2. Let e ∈ Hi. For every j such that ↑ (e) ∩ Rj = ∅, the last relevant event of Pj with respect to e is: lastr(e, j) = max{f | f ∈↑ (e) ∩ Rj}. When ↑ (e) ∩ Rj = ∅, lastr(e, j) is denoted by ⊥ (meaning that there is no such event).
Let us consider the event e identified (2,2) in Figure 1. We have lastr(e, 1) = (1, 2), lastr(e, 2) = (2, 1), lastr(e, 3) = (3, 1). The following properties relate the events lastr(e, j) and lastr(f, j) for all the predecessors f of e in the relation hb →. These properties follow directly from the definitions.
Let e ∈ Hi.
LR0 ∀e ∈ Hi: lastr(e, i) = 8 < : ⊥ if pred(e) = ⊥, pred(e) if pred(e) ∈ R, lastr(pred(e),i) if pred(e) ∈ R.
LR1 If e is not a receipt event: ∀j = i : lastr(e, j) = lastr(pred(e),j).
LR2 If e is a receive event of m: ∀j = i : lastr(e, j) = max(lastr(pred(e),j), lastr(send(m),j)).
Definition As a fundamental concept associated with the causality theory, vector clocks have been introduced in 1988, simultaneously and independently by Fidge [5] and Mattern [16]. A vector clock system is a mechanism that associates timestamps with events in such a way that the comparison of their timestamps indicates whether the corresponding events are or are not causally related (and, if they are, which one is the first). More precisely, each process Pi has a vector of integers V Ci[1..n] such that V Ci[j] is the number of relevant events produced by Pj, that belong to the current relevant causal past of Pi. Note that V Ci[i] counts the number of relevant events produced so far by Pi. When a process Pi produces a (relevant) event e, it associates with e a vector timestamp whose value (denoted e.V C) is equal to the current value of V Ci.
Vector Clock Implementation The following implementation of vector clocks [5, 16] is based on the observation that ∀i, ∀e ∈ Hi, ∀j : e.V Ci[j] = y ⇔ lastr(e, j) = ey j where e.V Ci is the value of V Ci just after the occurrence of e (this relation results directly from the properties LR0,
LR1, and LR2). Each process Pi manages its vector clock V Ci[1..n] according to the following rules: VC0 V Ci[1..n] is initialized to [0, . . . , 0].
VC1 Each time it produces a relevant event e, Pi increments its vector clock entry V Ci[i] (V Ci[i] := V Ci[i] + 1) to 212 indicate it has produced one more relevant event, then Pi associates with e the timestamp e.V C = V Ci.
VC2 When a process Pi sends a message m, it attaches to m the current value of V Ci. Let m.V C denote this value.
VC3 When Pi receives a message m, it updates its vector clock as follows: ∀k : V Ci[k] := max(V Ci[k], m.V C[k]).
In this section, the Immediate Predecessor Tracking (IPT) problem is stated (Section 3.1). Then, some technical properties of immediate predecessors are stated and proved (Section 3.2). These properties are used to design the basic IPT protocol and prove its correctness (Section 3.3). This IPT protocol, previously presented in [4] without proof, is built from a vector clock protocol by adding the management of a local boolean array at each process.
As indicated in the introduction, some applications (e.g., analysis of distributed executions [6], detection of distributed properties [7]) require to determine (on-the-fly and without additional messages) the transitive reduction of the relation → (i.e., we must not consider transitive causal dependency). Given two relevant events f and e, we say that f is an immediate predecessor of e if f → e and there is no relevant event g such that f → g → e.
Definition 3. The Immediate Predecessor Tracking (IPT) problem consists in associating with each relevant event e the set of relevant events that are its immediate predecessors. Moreover, this has to be done on the fly and without additional control message (i.e., without modifying the communication pattern of the computation).
As noted in the Introduction, the IPT problem is the computation of the Hasse diagram associated with the partially ordered set of the relevant events produced by a distributed computation.
In order to design a protocol solving the IPT problem, it is useful to consider the notion of immediate relevant predecessor of any event, whether relevant or not. First, we observe that, by definition, the immediate predecessor on Pj of an event e is necessarily the lastr(e, j) event.
Second, for lastr(e, j) to be immediate predecessor of e, there must not be another lastr(e, k) event on a path between lastr(e, j) and e. These observations are formalized in the following definition: Definition 4. Let e ∈ Hi. The set of immediate relevant predecessors of e (denoted IP(e)), is the set of the relevant events lastr(e, j) (j = 1, . . . , n) such that ∀k : lastr(e, j) ∈↑ (lastr(e, k)).
It follows from this definition that IP(e) ⊆ {lastr(e, j)|j = 1, . . . , n} ⊂↑ (e). When we consider Figure 1, The graph depicted in its right part describes the immediate predecessors of the relevant events of the computation defined in its left part, more precisely, a directed edge (e, f) means that the relevant event e is an immediate predecessor of the relevant event f (3 ).
The following lemmas show how the set of immediate predecessors of an event is related to those of its predecessors in the relation hb →. They will be used to design and prove the protocols solving the IPT problem. To ease the reading of the paper, their proofs are presented in Appendix A.
The intuitive meaning of the first lemma is the following: if e is not a receive event, all the causal paths arriving at e have pred(e) as next-to-last event (see CP1). So, if pred(e) is a relevant event, all the relevant events belonging to its relevant causal past are separated from e by pred(e), and pred(e) becomes the only immediate predecessor of e. In other words, the event pred(e) constitutes a reset w.r.t. the set of immediate predecessors of e. On the other hand, if pred(e) is not relevant, it does not separate its relevant causal past from e.
Lemma 1. If e is not a receive event, IP(e) is equal to: ∅ if pred(e) = ⊥, {pred(e)} if pred(e) ∈ R,
IP(pred(e)) if pred(e) ∈ R.
The intuitive meaning of the next lemma is as follows: if e is a receive event receive(m), the causal paths arriving at e have either pred(e) or send(m) as next-to-last events.
If pred(e) is relevant, as explained in the previous lemma, this event hides from e all its relevant causal past and becomes an immediate predecessor of e. Concerning the last relevant predecessors of send(m), only those that are not predecessors of pred(e) remain immediate predecessors of e.
Lemma 2. Let e ∈ Hi be the receive event of a message m. If pred(e) ∈ Ri, then, ∀j, IP(e) ∩ Rj is equal to: {pred(e)} if j = i, ∅ if lastr(pred(e),j) ≥ lastr(send(m),j),
IP(send(m)) ∩ Rj if lastr(pred(e),j) < lastr(send(m),j).
The intuitive meaning of the next lemma is the following: if e is a receive event receive(m), and pred(e) is not relevant, the last relevant events in the relevant causal past of e are obtained by merging those of pred(e) and those of send(m) and by taking the latest on each process. So, the immediate predecessors of e are either those of pred(e) or those of send(m). On a process where the last relevant events of pred(e) and of send(m) are the same event f, none of the paths from f to e must contain another relevant event, and thus, f must be immediate predecessor of both events pred(e) and send(m).
Lemma 3. Let e ∈ Hi be the receive event of a message m. If pred(e) ∈ Ri, then, ∀j, IP(e) ∩ Rj is equal to: IP(pred(e)) ∩ Rj if lastr(pred(e),j) > lastr(send(m),j),
IP(send(m)) ∩ Rj if lastr(pred(e),j) < lastr(send(m),j) IP(pred(e))∩IP(send(m))∩Rj if lastr(pred(e),j) = lastr (send(m), j).
The basic protocol proposed here associates with each relevant event e, an attribute encoding the set IP(e) of its immediate predecessors. From the previous lemmas, the set 3 Actually, this graph is the Hasse diagram of the partial order associated with the distributed computation. 213 IP(e) of any event e depends on the sets IP of the events pred(e) and/or send(m) (when e = receive(m)). Hence the idea to introduce a data structure allowing to manage the sets IPs inductively on the poset (H, hb →). To take into account the information from pred(e), each process manages a boolean array IPi such that, ∀e ∈ Hi the value of IPi when e occurs (denoted e.IPi) is the boolean array representation of the set IP(e). More precisely, ∀j : IPi[j] =
knowledge of lastr(e,j) (for every e and every j) is based on the management of vectors V Ci. Thus, the set IP(e) is determined in the following way: IP(e) = {ey j | e.V Ci[j] = y ∧ e.IPi[j] = 1, j = 1, . . . , n} Each process Pi updates IPi according to the Lemmas 1, 2, and 3:
the current value of IPi is sufficient to determine e.IPi.
It results from Lemmas 2 and 3 that, if e is a receive event (e = receive(m)), then determining e.IPi involves information related to the event send(m). More precisely, this information involves IP(send(m)) and the timestamp of send(m) (needed to compare the events lastr(send(m),j) and lastr(pred(e),j), for every j). So, both vectors send(m).V Cj and send(m).IPj (assuming send(m) produced by Pj ) are attached to message m.
of each event. In fact, the value of IPi just after an event e is used to determine the value succ(e).IPi. In particular, as stated in the Lemmas, the determination of succ(e).IPi depends on whether e is relevant or not.
Thus, the value of IPi just after the occurrence of event e must  keep track of this event.
The following protocol, previously presented in [4] without proof, ensures the correct management of arrays V Ci (as in Section 2.3) and IPi (according to the Lemmas of Section
denoted e.TS.
R0 Initialization: Both V Ci[1..n] and IPi[1..n] are initialized to [0, . . . , 0].
R1 Each time it produces a relevant event e: - Pi associates with e the timestamp e.TS defined as follows e.TS = {(k, V Ci[k]) | IPi[k] = 1}, - Pi increments its vector clock entry V Ci[i] (namely it executes V Ci[i] := V Ci[i] + 1), - Pi resets IPi: ∀ = i : IPi[ ] := 0; IPi[i] := 1.
R2 When Pi sends a message m to Pj, it attaches to m the current values of V Ci (denoted m.V C) and the boolean array IPi (denoted m.IP).
R3 When it receives a message m from Pj , Pi executes the following updates: ∀k ∈ [1..n] : case V Ci[k] < m.V C[k] thenV Ci[k] := m.V C[k]; IPi[k] := m.IP[k] V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k]) V Ci[k] > m.V C[k] then skip endcase The proof of the following theorem directly follows from Lemmas 1, 2 and 3.
Theorem 1. The protocol described in Section 3.3 solves the IPT problem: for any relevant event e, the timestamp e.TS contains the identifiers of all its immediate predecessors and no other event identifier.
This section addresses a previously open problem, namely, How to solve the IPT problem without requiring each application message to piggyback a whole vector clock and a whole boolean array?. First, a general condition that characterizes which entries of vectors V Ci and IPi can be omitted from the control information attached to a message sent in the computation, is defined (Section 4.1). It is then shown (Section 4.2) that this condition is both sufficient and necessary.
However, this general condition cannot be locally evaluated by a process that is about to send a message. Thus, locally evaluable approximations of this general condition must be defined. To each approximation corresponds a protocol, implemented with additional local data structures. In that sense, the general condition defines a family of IPT protocols, that solve the previously open problem. This issue is addressed in Section 5.
Information Let us consider the previous IPT protocol (Section 3.3).
Rule R3 shows that a process Pj does not systematically update each entry V Cj[k] each time it receives a message m from a process Pi: there is no update of V Cj[k] when V Cj[k] ≥ m.V C[k]. In such a case, the value m.V C[k] is useless, and could be omitted from the control information transmitted with m by Pi to Pj.
Similarly, some entries IPj[k] are not updated when a message m from Pi is received by Pj. This occurs when
m.V C[k], or when m.V C[k] = 0 (in the latest case, as m.IP[k] = IPi[k] = 0 then no update of IPj[k] is necessary).
Differently, some other entries are systematically reset to 0 (this occurs when 0 < V Cj [k] = m.V C[k] ∧ m.IP[k] = 0).
These observations lead to the definition of the condition K(m, k) that characterizes which entries of vectors V Ci and IPi can be omitted from the control information attached to a message m sent by a process Pi to a process Pj: Definition 5. K(m, k) ≡ (send(m).V Ci[k] = 0) ∨ (send(m).V Ci[k] < pred(receive(m)).V Cj[k]) ∨ ; (send(m).V Ci[k] = pred(receive(m)).V Cj[k]) ∧(send(m).IPi[k] = 1) .
We show here that the condition K(m, k) is both necessary and sufficient to decide which triples of the form (k, send(m).V Ci[k], send(m).IPi[k]) can be omitted in an outgoing message m sent by Pi to Pj. A triple attached to m will also be denoted (k, m.V C[k], m.IP[k]). Due to space limitations, the proofs of Lemma 4 and Lemma 5 are given in [1]. (The proof of Theorem 2 follows directly from these lemmas.) 214 Lemma 4. (Sufficiency) If K(m, k) is true, then the triple (k, m.V C[k], m.IP[k]) is useless with respect to the correct management of IPj[k] and V Cj [k].
Lemma 5. (Necessity) If K(m, k) is false, then the triple (k, m.V C[k], m.IP[k]) is necessary to ensure the correct management of IPj[k] and V Cj [k].
Theorem 2. When a process Pi sends m to a process Pj, the condition K(m, k) is both necessary and sufficient not to transmit the triple (k, send(m).V Ci[k], send(m).IPi[k]).
ON EVALUABLE CONDITIONS It results from the previous theorem that, if Pi could evaluate K(m, k) when it sends m to Pj, this would allow us improve the previous IPT protocol in the following way: in rule R2, the triple (k, V Ci[k], IPi[k]) is transmitted with m only if ¬K(m, k). Moreover, rule R3 is appropriately modified to consider only triples carried by m.
However, as previously mentioned, Pi cannot locally evaluate K(m, k) when it is about to send m. More precisely, when Pi sends m to Pj , Pi knows the exact values of send(m).V Ci[k] and send(m).IPi[k] (they are the current values of V Ci[k] and IPi[k]). But, as far as the value of pred(receive(m)).V Cj[k] is concerned, two cases are possible. Case (i): If pred(receive(m)) hb → send(m), then Pi can know the value of pred(receive(m)).V Cj[k] and consequently can evaluate K(m, k). Case (ii): If pred(receive(m)) and send(m) are concurrent, Pi cannot know the value of pred(receive(m)).V Cj[k] and consequently cannot evaluate K(m, k). Moreover, when it sends m to Pj , whatever the case (i or ii) that actually occurs, Pi has no way to know which case does occur. Hence the idea to define evaluable approximations of the general condition. Let K (m, k) be an approximation of K(m, k), that can be evaluated by a process Pi when it sends a message m. To be correct, the condition K must ensure that, every time Pi should transmit a triple (k, V Ci[k], IPi[k]) according to Theorem 2 (i.e., each time ¬K(m, k)), then Pi transmits this triple when it uses condition K . Hence, the definition of a correct evaluable approximation: Definition 6. A condition K , locally evaluable by a process when it sends a message m to another process, is correct if ∀(m, k) : ¬K(m, k) ⇒ ¬K (m, k) or, equivalently, ∀(m, k) : K (m, k) ⇒ K(m, k).
This definition means that a protocol evaluating K to decide which triples must be attached to messages, does not miss triples whose transmission is required by Theorem 2.
Let us consider the constant condition (denoted K1), that is always false, i.e., ∀(m, k) : K1(m, k) = false. This trivially correct approximation of K actually corresponds to the particular IPT protocol described in Section 3 (in which each message carries a whole vector clock and a whole boolean vector). The next section presents a better approximation of K (denoted K2).
Condition Condition K2 is based on the observation that condition K is composed of sub-conditions. Some of them can be Pj send(m) Pi V Ci[k] = x IPi[k] = 1 V Cj[k] ≥ x receive(m) Figure 2: The Evaluable Condition K2 locally evaluated while the others cannot. More precisely, K ≡ a ∨ α ∨ (β ∧ b), where a ≡ send(m).V Ci[k] = 0 and b ≡ send(m).IPi[k] = 1 are locally evaluable, whereas α ≡ send(m).V Ci[k] < pred(receive(m)).V Cj[k] and β ≡ send(m).V Ci[k] = pred(receive(m)).V Cj[k] are not.
But, from easy boolean calculus, a∨((α∨β)∧b) =⇒ a∨α∨ (β ∧ b) ≡ K. This leads to condition K ≡ a ∨ (γ ∧ b), where γ = α ∨ β ≡ send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k] , i.e., K ≡ (send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k] ∧ send(m).IPi[k] = 1) ∨ send(m).V Ci[k] = 0.
So, Pi needs to approximate the predicate send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k]. To be correct, this approximation has to be a locally evaluable predicate ci(j, k) such that, when Pi is about to send a message m to Pj, ci(j, k) ⇒ (send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k]). Informally, that means that, when ci(j, k) holds, the local context of Pi allows to deduce that the receipt of m by Pj will not lead to V Cj[k] update (Pj knows as much as Pi about Pk). Hence, the concrete condition K2 is the following: K2 ≡ send(m).V Ci[k] = 0 ∨ (ci(j, k) ∧ send(m).IPi[k] = 1).
Let us now examine the design of such a predicate (denoted ci). First, the case j = i can be ignored, since it is assumed (Section 2.1) that a process never sends a message to itself. Second, in the case j = k, the relation send(m).V Ci[j] ≤ pred(receive(m)).V Cj [j] is always true, because the receipt of m by Pj cannot update V Cj[j]. Thus, ∀j = i : ci(j, j) must be true. Now, let us consider the case where j = i and j = k (Figure 2). Suppose that there exists an event e = receive(m ) with e < send(m), m sent by Pj and piggybacking the triple (k, m .V C[k], m .IP[k]), and m .V C[k] ≥ V Ci[k] (hence m .V C[k] = receive(m ).V Ci[k]).
As V Cj[k] cannot decrease this means that, as long as V Ci[k] does not increase, for every message m sent by Pi to Pj we have the following: send(m).V Ci[k] = receive(m ).V Ci[k] = send(m ).V Cj[k] ≤ receive(m).V Cj [k], i.e., ci(j, k) must remain true. In other words, once ci(j, k) is true, the only event of Pi that could reset it to false is either the receipt of a message that increases V Ci[k] or, if k = i, the occurrence of a relevant event (that increases V Ci[i]). Similarly, once ci(j, k) is false, the only event that can set it to true is the receipt of a message m from Pj, piggybacking the triple (k, m .V C[k], m .IP[k]) with m .V C[k] ≥ V Ci[k].
In order to implement the local predicates ci(j, k), each process Pi is equipped with a boolean matrix Mi (as in [11]) such that M[j, k] = 1 ⇔ ci(j, k). It follows from the previous discussion that this matrix is managed according to the following rules (note that its i-th line is not significant (case j = i), and that its diagonal is always equal to 1): M0 Initialization: ∀ (j, k) : Mi[j, k] is initialized to 1. 215 M1 Each time it produces a relevant event e: Pi resets4 the ith column of its matrix: ∀j = i : Mi[j, i] := 0.
M2 When Pi sends a message: no update of Mi occurs.
M3 When it receives a message m from Pj , Pi executes the following updates: ∀ k ∈ [1..n] : case V Ci[k] < m.V C[k] then ∀ = i, j, k : Mi[ , k] := 0; Mi[j, k] := 1 V Ci[k] = m.V C[k] then Mi[j, k] := 1 V Ci[k] > m.V C[k] then skip endcase The following lemma results from rules M0-M3. The theorem that follows shows that condition K2(m, k) is correct. (Both are proved in [1].) Lemma 6. ∀i, ∀m sent by Pi to Pj, ∀k, we have: send(m).Mi[j, k] = 1 ⇒ send(m).V Ci[k] ≤ pred(receive(m)).V Cj [k].
Theorem 3. Let m be a message sent by Pi to Pj . Let K2(m, k) ≡ ((send(m).Mi[j, k] = 1) ∧ (send(m).IPi[k] = 1)∨(send(m).V Ci[k] = 0)). We have: K2(m, k) ⇒ K(m, k).
The complete text of the IPT protocol based on the previous discussion follows.
RM0 Initialization: - Both V Ci[1..n] and IPi[1..n] are set to [0, . . . , 0], and ∀ (j, k) : Mi[j, k] is set to 1.
RM1 Each time it produces a relevant event e: - Pi associates with e the timestamp e.TS defined as follows: e.TS = {(k, V Ci[k]) | IPi[k] = 1}, - Pi increments its vector clock entry V Ci[i] (namely, it executes V Ci[i] := V Ci[i] + 1), - Pi resets IPi: ∀ = i : IPi[ ] := 0; IPi[i] := 1. - Pi resets the ith column of its boolean matrix: ∀j = i : Mi[j, i] := 0.
RM2 When Pi sends a message m to Pj, it attaches to m the set of triples (each made up of a process id, an integer and a boolean): {(k, V Ci[k], IPi[k]) | (Mi[j, k] = 0 ∨ IPi[k] = 0) ∧ (V Ci[k] > 0)}.
RM3 When Pi receives a message m from Pj , it executes the following updates: ∀(k,m.V C[k], m.IP[k]) carried by m: case V Ci[k] < m.V C[k] then V Ci[k] := m.V C[k]; IPi[k] := m.IP[k]; ∀ = i, j, k : Mi[ , k] := 0; 4 Actually, the value of this column remains constant after its first update. In fact, ∀j, Mi[j, i] can be set to 1 only upon the receipt of a message from Pj, carrying the value V Cj[i] (see R3). But, as Mj [i, i] = 1, Pj does not send V Cj[i] to Pi. So, it is possible to improve the protocol by executing this reset of the column Mi[∗, i] only when Pi produces its first relevant event.
Mi[j, k] := 1 V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k]); Mi[j, k] := 1 V Ci[k] > m.V C[k] then skip endcase
The condition K2(m, k) shows that a triple has not to be transmitted when (Mi[j, k] = 1 ∧ IPi[k] = 1) ∨ (V Ci[k] > 0). Let us first observe that the management of IPi[k] is governed by the application program. More precisely, the IPT protocol does not define which are the relevant events, it has only to guarantee a correct management of IPi[k]. Differently, the matrix Mi does not belong to the problem specification, it is an auxiliary variable of the IPT protocol, which manages it so as to satisfy the following implication when Pi sends m to Pj : (Mi[j, k] = 1) ⇒ (pred(receive(m)).V Cj [k] ≥ send(m).V Ci[k]). The fact that the management of Mi is governed by the protocol and not by the application program leaves open the possibility to design a protocol where more entries of Mi are equal to 1. This can make the condition K2(m, k) more often satisfied5 and can consequently allow the protocol to transmit less triples.
We show here that it is possible to transmit less triples at the price of transmitting a few additional boolean vectors. The previous IPT matrix-based protocol (Section 5.2) is modified in the following way. The rules RM2 and RM3 are replaced with the modified rules RM2" and RM3" (Mi[∗, k] denotes the kth column of Mi).
RM2" When Pi sends a message m to Pj, it attaches to m the following set of 4-uples (each made up of a process id, an integer, a boolean and a boolean vector): {(k, V Ci[k], IPi[k], Mi[∗, k]) | (Mi[j, k] = 0 ∨ IPi[k] = 0) ∧ V Ci[k] > 0}.
RM3" When Pi receives a message m from Pj , it executes the following updates: ∀(k,m.V C[k], m.IP[k], m.M[1..n, k]) carried by m: case V Ci[k] < m.V C[k] then V Ci[k] := m.V C[k]; IPi[k] := m.IP[k]; ∀ = i : Mi[ , k] := m.M[ , k] V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k]); ∀ =i : Mi[ , k] := max(Mi[ , k], m.M[ , k]) V Ci[k] > m.V C[k] then skip endcase Similarly to the proofs described in [1], it is possible to prove that the previous protocol still satisfies the property proved in Lemma 6, namely, ∀i, ∀m sent by Pi to Pj, ∀k we have (send(m).Mi[j, k] = 1) ⇒ (send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k]). 5 Let us consider the previously described protocol (Section
equal to 0. The reader can easily verify that this setting correctly implements the matrix. Moreover, K2(m, k) is then always false: it actually coincides with K1(k, m) (which corresponds to the case where whole vectors have to be transmitted with each message). 216 Intuitively, the fact that some columns of matrices M are attached to application messages allows a transitive transmission of information. More precisely, the relevant history of Pk known by Pj is transmitted to a process Pi via a causal sequence of messages from Pj to Pi. In contrast, the protocol described in Section 5.2 used only a direct transmission of this information. In fact, as explained Section 5.1, the predicate c (locally implemented by the matrix M) was based on the existence of a message m sent by Pj to Pi, piggybacking the triple (k, m .V C[k], m .IP[k]), and m .V C[k] ≥ V Ci[k], i.e., on the existence of a direct transmission of information (by the message m ).
The resulting IPT protocol (defined by the rules RM0,
RM1, RM2" and RM3") uses the same condition K2(m, k) as the previous one. It shows an interesting tradeoff between the number of triples (k, V Ci[k], IPi[k]) whose transmission is saved and the number of boolean vectors that have to be additionally piggybacked. It is interesting to notice that the size of this additional information is bounded while each triple includes a non-bounded integer (namely a vector clock value).
This section compares the behaviors of the previous protocols. This comparison is done with a simulation study.
IPT1 denotes the protocol presented in Section 3.3 that uses the condition K1(m, k) (which is always equal to false).
IPT2 denotes the protocol presented in Section 5.2 that uses the condition K2(m, k) where messages carry triples.
Finally, IPT3 denotes the protocol presented in Section 5.3 that also uses the condition K2(m, k) but where messages carry additional boolean vectors.
This section does not aim to provide an in-depth simulation study of the protocols, but rather presents a general view on the protocol behaviors. To this end, it compares IPT2 and IPT3 with regard to IPT1. More precisely, for IPT2 the aim was to evaluate the gain in terms of triples (k, V Ci[k], IPi[k]) not transmitted with respect to the systematic transmission of whole vectors as done in IPT1. For IPT3, the aim was to evaluate the tradeoff between the additional boolean vectors transmitted and the number of saved triples. The behavior of each protocol was analyzed on a set of programs.
The simulator provides different parameters enabling to tune both the communication and the processes features.
These parameters allow to set the number of processes for the simulated computation, to vary the rate of communication (send/receive) events, and to alter the time duration between two consecutive relevant events. Moreover, to be independent of a particular topology of the underlying network, a fully connected network is assumed. Internal events have not been considered.
Since the presence of the triples (k, V Ci[k], IPi[k]) piggybacked by a message strongly depends on the frequency at which relevant events are produced by a process, different time distributions between two consecutive relevant events have been implemented (e.g., normal, uniform, and Poisson distributions). The senders of messages are chosen according to a random law. To exhibit particular configurations of a distributed computation a given scenario can be provided to the simulator. Message transmission delays follow a standard normal distribution. Finally, the last parameter of the simulator is the number of send events that occurred during a simulation.
To compare the behavior of the three IPT protocols, we performed a large number of simulations using different parameters setting. We set to 10 the number of processes participating to a distributed computation. The number of communication events during the simulation has been set to
is the average number of relevant events in a given time interval) has been set so that the relevant events are generated at the beginning of the simulation. With the uniform time distribution, a relevant event is generated (in the average) every 10 communication events. The location parameter of the standard normal time distribution has been set so that the occurrence of relevant events is shifted around the third part of the simulation experiment.
As noted previously, the simulator can be fed with a given scenario. This allows to analyze the worst case scenarios for IPT2 and IPT3. These scenarios correspond to the case where the relevant events are generated at the maximal frequency (i.e., each time a process sends or receives a message, it produces a relevant event).
Finally, the three IPT protocols are analyzed with the same simulation parameters.
The results are displayed on the Figures 3.a-3.d. These figures plot the gain of the protocols in terms of the number of triples that are not transmitted (y axis) with respect to the number of communication events (x axis). From these figures, we observe that, whatever the time distribution followed by the relevant events, both IPT2 and IPT3 exhibit a behavior better than IPT1 (i.e., the total number of piggybacked triples is lower in IPT2 and IPT3 than in IPT1), even in the worst case (see Figure 3.d).
Let us consider the worst scenario. In that case, the gain is obtained at the very beginning of the simulation and lasts as long as it exists a process Pj for which ∀k : V Cj[k] = 0.
In that case, the condition ∀k : K(m, k) is satisfied. As soon as ∃k : V Cj[k] = 0, both IPT2 and IPT3 behave as IPT1 (the shape of the curve becomes flat) since the condition K(m, k) is no longer satisfied.
Figure 3.a shows that during the first events of the simulation, the slope of curves IPT2 and IPT3 are steep. The same occurs in Figure 3.d (that depicts the worst case scenario). Then the slope of these curves decreases and remains constant until the end of the simulation. In fact, as soon as V Cj[k] becomes greater than 0, the condition ¬K(m, k) reduces to (Mi[j, k] = 0 ∨ IPi[k] = 0).
Figure 3.b displays an interesting feature. It considers λ =
beginning of the simulation, this figure exhibits a very steep slope as the other figures. The figure shows that, as soon as no more relevant events are taken, on average, 45% of the triples are not piggybacked by the messages. This shows the importance of matrix Mi. Furthermore, IPT3 benefits from transmitting additional boolean vectors to save triple transmissions. The Figures 3.a-3.c show that the average gain of IPT3 with respect to IPT2 is close to 10%.
Finally, Figure 3.c underlines even more the importance 217 of matrix Mi. When very few relevant events are taken,
IPT2 and IPT3 turn out to be very efficient. Indeed, this figure shows that, very quickly, the gain in number of triples that are saved is very high (actually, 92% of the triples are saved).
Of course, all simulation results are consistent with the theoretical results. IPT3 is always better than or equal to IPT2, and IPT2 is always better than IPT1. The simulation results teach us more: • The first lesson we have learnt concerns the matrix Mi.
Its use is quite significant but mainly depends on the time distribution followed by the relevant events. On the one hand, when observing Figure 3.b where a large number of relevant events are taken in a very short time, IPT2 can save up to 45% of the triples. However, we could have expected a more sensitive gain of IPT2 since the boolean vector IP tends to stabilize to [1, ..., 1] when no relevant events are taken. In fact, as discussed in Section 5.3, the management of matrix Mi within IPT2 does not allow a transitive transmission of information but only a direct transmission of this information. This explains why some columns of Mi may remain equal to 0 while they could potentially be equal to 1. Differently, as IPT3 benefits from transmitting additional boolean vectors (providing a transitive transmission information) it reaches a gain of 50%.
On the other hand, when very few relevant events are taken in a large period of time (see Figure 3.c), the behavior of IPT2 and IPT3 turns out to be very efficient since the transmission of up to 92% of the triples is saved. This comes from the fact that very quickly the boolean vector IPi tends to stabilize to [1, ..., 1] and that matrix Mi contains very few
direct transmission of the information is sufficient to quickly get matrices Mi equal to [1, ..., 1], . . . , [1, ..., 1]. • The second lesson concerns IPT3, more precisely, the tradeoff between the additional piggybacking of boolean vectors and the number of triples whose transmission is saved.
With n = 10, adding 10 booleans to a triple does not substantially increases its size. The Figures 3.a-3.c exhibit the number of triples whose transmission is saved: the average gain (in number of triples) of IPT3 with respect to IPT2 is about 10%.
This paper has addressed an important causality-related distributed computing problem, namely, the Immediate Predecessors Tracking problem. It has presented a family of protocols that provide each relevant event with a timestamp that exactly identify its immediate predecessors. The family is defined by a general condition that allows application messages to piggyback control information whose size can be smaller than n (the number of processes). In that sense, this family defines message size-efficient IPT protocols.
According to the way the general condition is implemented, different IPT protocols can be obtained. Three of them have been described and analyzed with simulation experiments.
Interestingly, it has also been shown that the efficiency of the protocols (measured in terms of the size of the control information that is not piggybacked by an application message) depends on the pattern defined by the communication events and the relevant events.
Last but not least, it is interesting to note that if one is not interested in tracking the immediate predecessor events, the protocols presented in the paper can be simplified by suppressing the IPi booleans vectors (but keeping the boolean matrices Mi). The resulting protocols, that implement a vector clock system, are particularly efficient as far as the size of the timestamp carried by each message is concerned.
Interestingly, this efficiency is not obtained at the price of additional assumptions (such as fifo channels).
[1] Anceaume E., H´elary J.-M. and Raynal M., Tracking Immediate Predecessors in Distributed Computations. Res.
Report #1344, IRISA, Univ. Rennes (France), 2001. [2] Baldoni R., Prakash R., Raynal M. and Singhal M.,
Efficient ∆-Causal Broadcasting. Journal of Computer Systems Science and Engineering, 13(5):263-270, 1998. [3] Chandy K.M. and Lamport L., Distributed Snapshots: Determining Global States of Distributed Systems, ACM Transactions on Computer Systems, 3(1):63-75, 1985. [4] Diehl C., Jard C. and Rampon J.-X., Reachability Analysis of Distributed Executions, Proc. TAPSOFT"93,
Springer-Verlag LNCS 668, pp. 629-643, 1993. [5] Fidge C.J., Timestamps in Message-Passing Systems that Preserve Partial Ordering, Proc. 11th Australian Computing Conference, pp. 56-66, 1988. [6] Fromentin E., Jard C., Jourdan G.-V. and Raynal M.,
On-the-fly Analysis of Distributed Computations, IPL, 54:267-274, 1995. [7] Fromentin E. and Raynal M., Shared Global States in Distributed Computations, JCSS, 55(3):522-528, 1997. [8] Fromentin E., Raynal M., Garg V.K. and Tomlinson A.,
On-the-Fly Testing of Regular Patterns in Distributed Computations. Proc. ICPP"94, Vol. 2:73-76, 1994. [9] Garg V.K., Principles of Distributed Systems, Kluwer Academic Press, 274 pages, 1996. [10] H´elary J.-M., Most´efaoui A., Netzer R.H.B. and Raynal M., Communication-Based Prevention of Useless Ckeckpoints in Distributed Computations. Distributed Computing, 13(1):29-43, 2000. [11] H´elary J.-M., Melideo G. and Raynal M., Tracking Causality in Distributed Systems: a Suite of Efficient Protocols. Proc. SIROCCO"00, Carleton University Press, pp. 181-195, L"Aquila (Italy), June 2000. [12] H´elary J.-M., Netzer R. and Raynal M., Consistency Issues in Distributed Checkpoints. IEEE TSE, 25(4):274-281, 1999. [13] Hurfin M., Mizuno M., Raynal M. and Singhal M., Efficient Distributed Detection of Conjunction of Local Predicates in Asynch Computations. IEEE TSE, 24(8):664-677, 1998. [14] Lamport L., Time, Clocks and the Ordering of Events in a Distributed System. Comm. ACM, 21(7):558-565, 1978. [15] Marzullo K. and Sabel L., Efficient Detection of a Class of Stable Properties. Distributed Computing, 8(2):81-91, 1994. [16] Mattern F., Virtual Time and Global States of Distributed Systems. Proc. Int. Conf. Parallel and Distributed Algorithms, (Cosnard, Quinton, Raynal, Robert Eds),
North-Holland, pp. 215-226, 1988. [17] Prakash R., Raynal M. and Singhal M., An Adaptive Causal Ordering Algorithm Suited to Mobile Computing Environment. JPDC, 41:190-204, 1997. [18] Raynal M. and Singhal S., Logical Time: Capturing Causality in Distributed Systems. IEEE Computer, 29(2):49-57, 1996. [19] Singhal M. and Kshemkalyani A., An Efficient Implementation of Vector Clocks. IPL, 43:47-52, 1992. [20] Wang Y.M., Consistent Global Checkpoints That Contain a Given Set of Local Checkpoints. IEEE TOC, 46(4):456-468, 1997. 218 0 1000 2000 3000 4000 5000 6000
gaininnumberoftriples communication events number IPT1 IPT2 IPT3 relevant events (a) The relevant events follow a uniform distribution (ratio=1/10) -5000 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
gaininnumberoftriples communication events number IPT1 IPT2 IPT3 relevant events (b) The relevant events follow a Poisson distribution (λ = 100) 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
gaininnumberoftriples communication events number IPT1 IPT2 IPT3 relevant events (c) The relevant events follow a normal distribution 0 50 100 150 200 250 300 350 400 450

In recent years we have seen the continuous improvement of technologies that are relevant for the construction of distributed embedded systems, including trustworthy visual, auditory, and location sensing [11], communication and processing. We believe that in a future networked physical world a new class of applications will emerge, composed of a myriad of smart sensors and actuators to assess and control aspects of their environments and autonomously act in response to it. The anticipated challenging characteristics of these applications include autonomy, responsiveness and safety criticality, large scale, geographical dispersion, mobility and evolution.
In order to deal with these challenges, it is of fundamental importance to use adequate high-level models, abstractions and interaction paradigms. Unfortunately, when facing the specific characteristics of the target systems, the shortcomings of current architectures and middleware interaction paradigms become apparent. Looking to the basic building blocks of such systems we may find components which comprise mechanical parts, hardware, software and a network interface. However, classical event/object models are usually software oriented and, as such, when trans28 ported to a real-time, embedded systems setting, their harmony is cluttered by the conflict between, on the one side, send/receive of software events (message-based), and on the other side, input/output of hardware or real-world events, register-based. In terms of interaction paradigms, and although the use of event-based models appears to be a convenient solution [10, 22], these often lack the appropriate support for non-functional requirements like reliability, timeliness or security.
This paper describes an architectural framework and a middleware, supporting a component-based system and an integrated view on event-based communication comprising the real world events and the events generated in the system.
When choosing the appropriate interaction paradigm it is of fundamental importance to address the challenging issues of the envisaged sentient applications. Unlike classical approaches that confine the possible interactions to the application boundaries, i.e. to its components, we consider that the environment surrounding the application also plays a relevant role in this respect. Therefore, the paper starts by clarifying several issues concerning our view of the system, about the interactions that may take place and about the information flows. This view is complemented by providing an outline of the component-based system construction and, in particular, by showing that it is possible to compose larger applications from basic components, following an hierarchical composition approach.
This provides the necessary background to introduce the Generic-Events Architecture (GEAR), which describes the event-based interaction between the components via a generic event layer while allowing the seamless integration of physical and computer information flows. In fact, the generic event layer hides the different communication channels, including the interactions through the environment.
Additionally, the event layer abstraction is also adequate for the proper handling of the non-functional requirements, namely reliability and timeliness, which are particularly stringent in real-time settings. The paper devotes particular attention to this issue by discussing the temporal aspects of interactions and the needs for predictability.
An appropriate middleware is presented which reflects these needs and allows to specify events which have quality attributes to express temporal constraints. This is complemented by the notion of Event Channels (EC), which are abstractions of the underlying network while being abstracted by the event layer. In fact, event channels play a fundamental role in securing the functional and non-functional (e.g. reliability and timeliness) properties of the envisaged applications, that is, in allowing the enforcement of quality attributes. They are established prior to interaction to reserve the needed computational and network resources for highly predictable event dissemination.
The paper is organized as follows. In Section 3 we introduce the fundamental notions and abstractions that we adopt in this work to describe the interactions taking place in the system. Then, in Section 4, we describe the componentbased approach that allows composition of objects. GEAR is then described in Section 5 and Section 6 focuses on temporal aspects of the interactions. Section 7 describes the COSMIC middleware, which may be used to specify the interaction between sentient objects. A simple example to highlight the ideas presented in the paper appears in Section 8 and Section 9 concludes the paper.
Our work considers a wired physical world in which a very large number of autonomous components cooperate.
It is inspired by many research efforts in very different areas. Event-based systems in general have been introduced to meet the requirements of applications in which entities spontaneously generate information and disseminate it [1, 25, 22]. Intended for large systems and requiring quite complex infrastructures, these event systems do not consider stringent quality aspects like timeliness and dependability issues.
Secondly, they are not created to support inter-operability between tiny smart devices with substantial resource constraints.
In [10] a real-time event system for CORBA has been introduced. The events are routed via a central event server which provides scheduling functions to support the real-time requirements. Such a central component is not available in an infrastructure envisaged in our system architecture and the developed middleware TAO (The Ace Orb) is quite complex and unsuitable to be directly integrated in smart devices.
There are efforts to implement CORBA for control networks, tailored to connect sensor and actuator components [15, 19]. They are targeted for the CAN-Bus [9], a popular network developed for the automotive industry. However, in these approaches the support for timeliness or dependability issues does not exist or is only very limited.
A new scheme to integrate smart devices in a CORBA environment is proposed in [17] and has lead to the proposal of a standard by the Object Management Group (OMG) [26].
Smart transducers are organized in clusters that are connected to a CORBA system by a gateway.
The clusters form isolated subnetworks. A special master node enforces the temporal properties in the cluster subnet.
A CORBA gateway allows to access sensor data and write actuator data by means of an interface file system (IFS).
The basic structure is similar to the WAN-of-CANs structure which has been introduced in the CORTEX project [4].
Islands of tight control may be realized by a control network and cooperate via wired or wireless networks covering a large number of these subnetworks. However, in contrast to the event channel model introduced in this paper, all communication inside a cluster relies on a single technical solution of a synchronous communication channel. Secondly, although the temporal behaviour of a single cluster is rigorously defined, no model to specify temporal properties for clusterto-CORBA or cluster-to-cluster interactions is provided.
INTERACTION MODEL In this paper we consider a component-based system model that incorporates previous work developed in the context of the IST CORTEX project [5]. As mentioned above, a fundamental idea underlying the approach is that applications can be composed of a large number of smart components that are able to sense their surrounding environment and interact with it. These components are referred to as sentient objects, a metaphor elaborated in CORTEX and inspired on the generic concept of sentient computing introduced in [12]. Sentient objects accept input events from a variety of different sources (including sensors, but not constrained to that), process them, and produce output events, whereby 29 they actuate on the environment and/or interact with other objects. Therefore, the following kinds of interactions can take place in the system: Environment-to-object interactions: correspond to a flow of information from the environment to application objects, reporting about the state of the former, and/or notifying about events taking place therein.
Object-to-object interactions: correspond to a flow of information among sentient objects, serving two purposes. The first is related with complementing the assessment of each individual object about the state of the surrounding space. The second is related to collaboration, in which the object tries to influence other objects into contributing to a common goal, or into reacting to an unexpected situation.
Object-to-environment interactions: correspond to a flow of information from an object to the environment, with the purpose of forcing a change in the state of the latter.
Before continuing, we need to clarify a few issues with respect to these possible forms of interaction. We consider that the environment can be a producer or consumer of information while interacting with sentient objects. The environment is the real (physical) world surrounding an object, not necessarily close to the object or limited to certain boundaries. Quite clearly, the information produced by the environment corresponds to the physical representation of real-time entities, of which typical examples include temperature, distance or the state of a door. On the other hand, actuation on the environment implies the manipulation of these real-time entities, like increasing the temperature (applying more heat), changing the distance (applying some movement) or changing the state of the door (closing or opening it). The required transformations between system representations of these real-time entities and their physical representations is accomplished, generically, by sensors and actuators. We further consider that there may exist dumb sensors and actuators, which interact with the objects by disseminating or capturing raw transducer information, and smart sensors and actuators, with enhanced processing capabilities, capable of speaking some more elaborate event dialect (see Sections 5 and 6.1). Interaction with the environment is therefore done through sensors and actuators, which may, or may not be part of sentient objects, as discussed in Section 4.2.
State or state changes in the environment are considered as events, captured by sensors (in the environment or within sentient objects) and further disseminated to other potentially interested sentient objects in the system. In consequence, it is quite natural to base the communication and interaction among sentient objects and with the environment on an event-based communication model. Moreover, typical properties of event-based models, such as anonymous and non-blocking communication, are highly desirable in systems where sentient objects can be mobile and where interactions are naturally very dynamic.
A distinguishing aspect of our work from many of the existing approaches, is that we consider that sentient objects may indirectly communicate with each other through the environment, when they act on it. Thus the environment constitutes an interaction and communication channel and is in the control and awareness loop of the objects. In other words, when a sentient object actuates on the environment it will be able to observe the state changes in the environment by means of events captured by the sensors. Clearly, other objects might as well capture the same events, thus establishing the above-mentioned indirect communication path.
In systems that involve interactions with the environment it is very important to consider the possibility of communication through the environment. It has been shown that the hidden channels developing through the latter (e.g., feedback loops) may hinder software-based algorithms ignoring them [30]. Therefore, any solution to the problem requires the definition of convenient abstractions and appropriate architectural constructs.
On the other hand, in order to deal with the information flow through the whole computer system and environment in a seamless way, handling software and hardware events uniformly, it is also necessary to find adequate abstractions.
As discussed in Section 5, the Generic-Events Architecture introduces the concept of Generic Event and an Event Layer abstraction which aim at dealing, among others, with these issues.
In this section we analyze the most relevant issues related with the sentient object paradigm and the construction of systems composed of sentient objects.
Sentient objects can take several different forms: they can simply be software-based components, but they can also comprise mechanical and/or hardware parts, amongst which the very sensorial apparatus that substantiates sentience, mixed with software components to accomplish their task.
We refine this notion by considering a sentient object as an encapsulating entity, a component with internal logic and active processing elements, able to receive, transform and produce new events. This interface hides the internal hardware/software structure of the object, which may be complex, and shields the system from the low-level functional and temporal details of controlling a specific sensor or actuator.
Furthermore, given the inherent complexity of the envisaged applications, the number of simultaneous input events and the internal size of sentient objects may become too large and difficult to handle. Therefore, it should be possible to consider the hierarchical composition of sentient objects so that the application logic can be separated across as few or as many of these objects as necessary. On the other hand, composition of sentient objects should normally be constrained by the actual hardware component"s structure, preventing the possibility of arbitrarily composing sentient objects. This is illustrated in Figure 1, where a sentient object is internally composed of a few other sentient objects, each of them consuming and producing events, some of which only internally propagated.
Observing the figure, and recalling our previous discussion about the possible interactions, we identify all of them here: an object-to-environment interaction occurs between the object controlling a WLAN transmitter and some WLAN receiver in the environment; an environment-to-object interaction takes place when the object responsible for the GPS 30 G P S r e c e p t i o n W i r e l e s s t r a n s m i s s i o n D o p p l e r r a d a r P h y s i c a l f e e d b a c k O b j e c t ' s b o d y I n t e r n a l N e t w o r k Figure 1: Component-aware sentient object composition. signal reception uses the information transmitted by the satellites; finally, explicit object-to-object interactions occur internally to the container object, through an internal communication network. Additionally, it is interesting to observe that implicit communication can also occur, whether the physical feedback develops through the environment internal to the container object (as depicted) or through the environment external to this object. However, there is a subtle difference between both cases. While in the former the feedback can only be perceived by objects internal to the container, bounding the extent to which consistency must be ensured, such bounds do not exist in the latter. In fact, the notion of sentient object as an encapsulating entity may serve other purposes (e.g., the confinement of feedback and of the propagation of events), beyond the mere hierarchical composition of objects.
To give a more concrete example of such component-aware object composition we consider a scenario of cooperating robots. Each robot is made of several components, corresponding, for instance, to axis and manipulator controllers.
Together with the control software, each of these controllers may be a sentient object. On the other hand, a robot itself is a sentient object, composed of the objects materialized by the controllers, and the environment internal to its own structure, or body.
This means that it should be possible to define cooperation activities using the events produced by robot sentient objects, without the need to know the internal structure of robots, or the events produced by body objects or by smart sensors within the body. From an engineering point of view, however, this also means that robot sentient object may have to generate new events that reflect its internal state, which requires the definition of a gateway to make the bridge between the internal and external environments.
Now an important question is about how to represent and disseminate events in a large scale networked world. As we have seen above, any event generated by a sentient object could, in principle, be visible anywhere in the system and thus received by any other sentient object. However, there are substantial obstacles to such universal interactions, originating from the components heterogeneity in such a largescale setting.
Firstly, the components may have severe performance constraints, particularly because we want to integrate smart sensors and actuators in such an architecture. Secondly, the bandwidth of the participating networks may vary largely.
Such networks may be low power, low bandwidth fieldbuses, or more powerful wireless networks as well as high speed backbones. Thirdly, the networks may have widely different reliability and timeliness characteristics. Consider a platoon of cooperating vehicles. Inside a vehicle there may be a field-bus like CAN [8, 9], TTP/A [17] or LIN [20], with a comparatively low bandwidth. On the other hand, the vehicles are communicating with others in the platoon via a direct wireless link. Finally, there may be multiple platoons of vehicles which are coordinated by an additional wireless network layer.
At the abstraction level of sentient objects, such heterogeneity is reflected by the notion of body-vs-environment.
At the network level, we assume the WAN-of-CANs structure [27] to model the different networks. The notion of body and environment is derived from the recursively defined component-based object model. A body is similar to a cell membrane and represents a quality of service container for the sentient objects inside. On the network level, it may be associated with the components coupled by a certain CAN. A CAN defines the dissemination quality which can be expected by the cooperating objects.
In the above example, a vehicle may be a sentient object, whose body is composed of the respective lower level objects (sensors and actuators) which are connected by the internal network (see Figure 1). Correspondingly, the platoon can be seen itself as an object composed of a collection of cooperating vehicles, its body being the environment encapsulated by the platoon zone. At the network level, the wireless network represents the respective CAN. However, several platoons united by their CANs may interact with each other and objects further away, through some wider-range, possible fixed networking substrate, hence the concept of WAN-of-CANs.
The notions of body-environment and WAN-of-CANs are very useful when defining interaction properties across such boundaries. Their introduction obeyed to our belief that a single mechanism to provide quality measures for interactions is not appropriate. Instead, a high level construct for interaction across boundaries is needed which allows to specify the quality of dissemination and exploits the knowledge about body and environment to assess the feasibility of quality constraints. As we will see in the following section, the notion of an event channel represents this construct in our architecture. It disseminates events and allows the network independent specification of quality attributes. These attributes must be mapped to the respective properties of the underlying network structure.
In order to successfully apply event-based object-oriented models, addressing the challenges enumerated in the introduction of this paper, it is necessary to use adequate architectural constructs, which allow the enforcement of fundamental properties such as timeliness or reliability.
We propose the Generic-Events Architecture (GEAR), depicted in Figure 2, which we briefly describe in what follows (for a more detailed description please refer to [29]).
The L-shaped structure is crucial to ensure some of the properties described.
Environment: The physical surroundings, remote and close, solid and etherial, of sentient objects. 31 C o m m ' sC o m m ' sC o m m ' s T r a n s l a t i o n L a y e r T r a n s l a t i o n L a y e r B o d y E n v i r o n m e n t B o d y E n v i r o n m e n t B o d y E n v i r o n m e n t ( i n c l u d i n g o p e r a t i o n a l n e t w o r k ) ( o f o b j e c t o r o b j e c t c o m p o u n d ) T r a n s l a t i o n L a y e r T r a n s l a t i o n S e n t i e n t O b j e c t S e n t i e n t O b j e c t S e n t i e n t O b j e c t R e g u l a r N e t w o r k c o n s u m ep r o d u c e E v e n t L a y e r E v e n t L a y e r E v e n t L a y e r S e n t i e n t O b j e c t Figure 2: Generic-Events architecture.
Body: The physical embodiment of a sentient object (e.g., the hardware where a mechatronic controller resides, the physical structure of a car). Note that due to the compositional approach taken in our model, part of what is environment to a smaller object seen individually, becomes body for a larger, containing object.
In fact, the body is the internal environment of the object. This architecture layering allows composition to take place seamlessly, in what concerns information flow.
Inside a body there may also be implicit knowledge, which can be exploited to make interaction more efficient, like the knowledge about the number of cooperating entities, the existence of a specific communication network or the simple fact that all components are co-located and thus the respective events do not need to specify location in their context attributes. Such intrinsic information is not available outside a body and, therefore, more explicit information has to be carried by an event.
Translation Layer: The layer responsible for physical event transformation from/to their native form to event channel dialect, between environment/body and an event channel. Essentially one doing observation and actuation operations on the lower side, and doing transactions of event descriptions on the other. On the lower side this layer may also interact with dumb sensors or actuators, therefore talking the language of the specific device. These interactions are done through operational networks (hence the antenna symbol in the figure).
Event Layer: The layer responsible for event propagation in the whole system, through several Event Channels (EC):. In concrete terms, this layer is a kind of middleware that provides important event-processing services which are crucial for any realistic event-based system.
For example, some of the services that imply the processing of events may include publishing, subscribing, discrimination (zoning, filtering, fusion, tracing), and queuing.
Communication Layer: The layer responsible for wrapping events (as a matter of fact, event descriptions in EC dialect) into carrier event-messages, to be transported to remote places. For example, a sensing event generated by a smart sensor is wrapped in an event-message and disseminated, to be caught by whoever is concerned. The same holds for an actuation event produced by a sentient object, to be delivered to a remote smart actuator. Likewise, this may apply to an event-message from one sentient object to another. Dumb sensors and actuators do not send event-messages, since they are unable to understand the EC dialect (they do not have an event layer neither a communication layer- they communicate, if needed, through operational networks).
Regular Network: This is represented in the horizontal axis of the block diagram by the communication layer, which encompasses the usual LAN, TCP/IP, and realtime protocols, desirably augmented with reliable and/or ordered broadcast and other protocols.
The GEAR introduces some innovative ideas in distributed systems architecture. While serving an object model based on production and consumption of generic events, it treats events produced by several sources (environment, body, objects) in a homogeneous way. This is possible due to the use of a common basic dialect for talking about events and due to the existence of the translation layer, which performs the necessary translation between the physical representation of a real-time entity and the EC compliant format. Crucial to the architecture is the event layer, which uses event channels to propagate events through regular network infrastructures.
The event layer is realized by the COSMIC middleware, as described in Section 7.
The flow of information (external environment and computational part) is seamlessly supported by the L-shaped architecture. It occurs in a number of different ways, which demonstrates the expressiveness of the model with regard to the necessary forms of information encountered in real-time cooperative and embedded systems.
Smart sensors produce events which report on the environment. Body sensors produce events which report on the body. They are disseminated by the local event layer module, on an event channel (EC) propagated through the regular network, to any relevant remote event layer modules where entities showed an interest on them, normally, sentient objects attached to the respective local event layer modules.
Sentient objects consume events they are interested in, process them, and produce other events. Some of these events are destined to other sentient objects. They are published on an EC using the same EC dialect that serves, e.g., sensor originated events. However, these events are semantically of a kind such that they are to be subscribed by the relevant sentient objects, for example, the sentient objects composing a robot controller system, or, at a higher level, the sentient objects composing the actual robots in 32 a cooperative application. Smart actuators, on the other hand, merely consume events produced by sentient objects, whereby they accept and execute actuation commands.
Alternatively to talking to other sentient objects, sentient objects can produce events of a lower level, for example, actuation commands on the body or environment. They publish these exactly the same way: on an event channel through the local event layer representative. Now, if these commands are of concern to local actuator units (e.g., body, including internal operational networks), they are passed on to the local translation layer. If they are of concern to a remote smart actuator, they are disseminated through the distributed event layer, to reach the former. In any case, if they are also of interest to other entities, such as other sentient objects that wish to be informed of the actuation command, then they are also disseminated through the EC to these sentient objects.
A key advantage of this architecture is that event-messages and physical events can be globally ordered, if necessary, since they all pass through the event layer. The model also offers opportunities to solve a long lasting problem in realtime, computer control, and embedded systems: the inconsistency between message passing and the feedback loop information flow subsystems.
INTERACTIONS Any interaction needs some form of predictability. If safety critical scenarios are considered as it is done in CORTEX, temporal aspects become crucial and have to be made explicit. The problem is how to define temporal constraints and how to enforce them by appropriate resource usage in a dynamic ad-hoc environment. In an system where interactions are spontaneous, it may be also necessary to determine temporal properties dynamically. To do this, the respective temporal information must be stated explicitly and available during run-time. Secondly, it is not always ensured that temporal properties can be fulfilled. In these cases, adaptations and timing failure notification must be provided [2, 28]. In most real-time systems, the notion of a deadline is the prevailing scheme to express and enforce timeliness.
However, a deadline only weakly reflect the temporal characteristics of the information which is handled. Secondly, a deadline often includes implicit knowledge about the system and the relations between activities. In a rather well defined, closed environment, it is possible to make such implicit assumptions and map these to execution times and deadlines.
E.g. the engineer knows how long a vehicle position can be used before the vehicle movement outdates this information.
Thus he maps this dependency between speed and position on a deadline which then assures that the position error can be assumed to be bounded. In a open environment, this implicit mapping is not possible any more because, as an obvious reason, the relation between speed and position, and thus the error bound, cannot easily be reverse engineered from a deadline. Therefore, our event model includes explicit quality attributes which allow to specify the temporal attributes for every individual event. This is of course an overhead compared to the use of implicit knowledge, but in a dynamic environment such information is needed.
To illustrate the problem, consider the example of the position of a vehicle. A position is a typical example for time, value entity [30]. Thus, the position is useful if we can determine an error bound which is related to time, e.g. if we want a position error below 10 meters to establish a safety property between cooperating cars moving with 5 m/sec, the position has a validity time of 2 seconds. In a time, value entity entity we can trade time against the precision of the value. This is known as value over time and time over value [18]. Once having established the time-value relation and captured in event attributes, subscribers of this event can locally decide about the usefulness of an information. In the GEAR architecture temporal validity is used to reason about safety properties in a event-based system [29]. We will briefly review the respective notions and see how they are exploited in our COSMIC event middleware.
Consider the timeline of generating an event representing some real-time entity [18] from its occurrence to the notification of a certain sentient object (Figure 3). The real-time entity is captured at the sensor interface of the system and has to be transformed in a form which can be treated by a computer. During the time interval t0 the sensor reads the real-time entity and a time stamp is associated with the respective value. The derived time, value entity represents an observation. It may be necessary to perform substantial local computations to derive application relevant information from the raw sensor data. However, it should be noted that the time stamp of the observation is associated with the capture time and thus independent from further signal processing and event generation. This close relationship between capture time and the associated value is supported by smart sensors described above.
The processed sensor information is assembled in an event data structure after ts to be published to an event channel.
As is described later, the event includes the time stamp of generation and the temporal validity as attributes.
The temporal validity is an application defined measure for the expiration of a time, value . As we explained in the example of a position above, it may vary dependent on application parameters. Temporal validity is a more general concept than that of a deadline. It is independent of a certain technical implementation of a system. While deadlines may be used to schedule the respective steps in an event generation and dissemination, a temporal validity is an intrinsic property of a time, value entity carried in an event.
A temporal validity allows to reason about the usefulness of information and is beneficial even in systems in which timely dissemination of events cannot be enforced because it enables timing failure detection at the event consumer. It is obvious that deadlines or periods can be derived from the temporal validity of an event. To set a deadline, knowledge of an implementation, worst case execution times or message dissemination latencies is necessary. Thus, in the timeline of Figure 3 every interval may have a deadline. Event dissemination through soft real-time channels in COSMIC exploits the temporal validity to define dissemination deadlines. Quality attributes can be defined, for instance, in terms of validity interval, omission degree pairs. These allow to characterize the usefulness of the event for a certain application, in a certain context. Because of that, quality attributes of an event clearly depend on higher level issues, such as the nature of the sentient object or of the smart sensor that produced the event. For instance, an event containing an indication of some vehicle speed must have different quality attributes depending on the kind of vehicle 33 real-world event observation: <time stamp, value> event generated ready to be transmitted event received notification , to t event producer communication network event consumer event channel push <event> , ts , tm , tt , tn , t o : t i m e t o o b t a i n a n o b s e r v a t i o n , t s : t i m e t o p r o c e s s s e n s o r r e a d i n g , t m : t i m e t o a s s e m b l e a n e v e n t m e s s a g e , t t : t i m e t o t r a n s f e r t h e e v e n t o n t h e r e g u l a r n e t w o r k , t n : t i m e f o r n o t i f i c a t i o n o n t h e c o n s u m e r s i t e Figure 3: Event processing and dissemination. from which it originated, or depending on its current speed.
The same happens with the position event of the car example above, whose validity depends on the current speed and on a predefined required precision. However, since quality attributes are strictly related with the semantics of the application or, at least, with some high level knowledge of the purpose of the system (from which the validity of the information can be derived), the definition of these quality attributes may be done by exploiting the information provided at the programming interface. Therefore, it is important to understand how the system programmer can specify non-functional requirements at the API, and how these requirements translate into quality attributes assigned to events. While temporal validity is identified as an intrinsic event property, which is exploited to decide on the usefulness of data at a certain point in time, it is still necessary to provide a communication facility which can disseminate the event before the validity is expired.
In a WAN-of-CANs network structure we have to cope with very different network characteristics and quality of service properties. Therefore, when crossing the network boundaries the quality of service guarantees available in a certain network will be lost and it will be very hard, costly and perhaps impossible to achieve these properties in the next larger area of the WAN-of CANs structure. CORTEX has a couple of abstractions to cope with this situation (network zones, body/environment) which have been discussed above. From the temporal point of view we need a high level abstraction like the temporal validity for the individual event now to express our quality requirements of the dissemination over the network. The bound, coverage pair, introduced in relation with the TCB [28] seems to be an appropriate approach. It considers the inherent uncertainty of networks and allows to trade the quality of dissemination against the resources which are needed. In relation with the event channel model discussed later, the bound, coverage pair allows to specify the quality properties of an event channel independently of specific technical issues. Given the typical environments in which sentient applications will operate, where it is difficult or even impossible to provide timeliness or reliability guarantees, we proposed an alternative way to handle non-functional application requirements, in relation with the TCB approach [28]. The proposed approach exploits intrinsic characteristics of applications, such as fail-safety, or time-elasticity, in order to secure QoS specifications of the form bound, coverage . Instead of constructing systems that rely on guaranteed bounds, the idea is to use (possibly changing) bounds that are secured with a constant probability all over the execution. This obviously requires an application to be able to adapt to changing conditions (and/or changing bounds) or, if this is not possible, to be able to perform some safety procedures when the operational conditions degrade to an unbearable level. The bounds we mentioned above refer essentially to timeliness bounds associated to the execution of local or distributed activities, or combinations thereof. From these bounds it is then possible to derive the quality attributes, in particular validity intervals, that characterize the events published in the event channel.
Smart devices encapsulate hardware, software and mechanical components and provide information and a set of well specified functions and which are closely related to the interaction with the environment. The built-in computational components and the network interface enable the implementation of a well-defined high level interface that does not just provide raw transducer data, but a processed, application-related set of events. Moreover, they exhibit an autonomous spontaneous behaviour. They differ from general purpose nodes because they are dedicated to a certain functionality which complies to their sensing and actuating capabilities while general purpose node may execute any program.
Concerning the sentient object model, smart sensors and actuators may be basic sentient objects themselves, consuming events from the real-world environment and producing the respective generic events for the system"s event layer or, 34 vice versa consuming a generic event and converting it to a real-world event by an actuation. Smart components therefore constitute the periphery, i.e. the real-world interface of a more complex sentient object. The model of sentient objects also constitutes the framework to built more complex virtual sensors by relating multiple (primary, i.e. sensors which directly sense a physical entity) sensors.
Smart components translate events of the environment to an appropriate form available at the event layer or, vice versa, transform a system event into an actuation. For smart components we can assume that: • Smart components have dedicated resources to perform a specific function. • These resources are not used for other purposes during normal real-time operation. • No local temporal conflicts occur that will change the observable temporal behaviour. • The functions of a component can usually only be changed during a configuration procedure which is not performed when the component is involved in critical operations. • An observation of the environment as a time,value pair can be obtained with a bounded jitter in time.
Many predictability and scheduling problems arise from the fact, that very low level timing behaviours have to be handled on a single processor. Here, temporal encapsulation of activities is difficult because of the possible side effects when sharing a single processor resource. Consider the control of a simple IR-range detector which is used for obstacle avoidance. Dependent on its range and the speed of a vehicle, it has to be polled to prevent the vehicle from crashing into an obstacle. On a single central processor, this critical activity has to be coordinated with many similar, possibly less critical functions. It means that a very fine grained schedule has to be derived based purely on the artifacts of the low level device control. In a smart sensor component, all this low level timing behaviour can be optimized and encapsulated. Thus we can assume temporal encapsulation similar to information hiding in the functional domain. Of course, there is still the problem to guarantee that an event will be disseminated and recognized in due time by the respective system components, but this relates to application related events rather than the low artifacts of a device timing. The main responsibility to provide timeliness guarantees is shifted to the event layer where these events are disseminated. Smart sensors thus lead to network centric system model. The network constitute the shared resource which has to be scheduled in a predictable way. The COSMIC middleware introduced in the next section is an approach to provide predictable event dissemination for a network of smart sensors and actuators.
FOR COOPERATING SMART DEVICES An event model and a middleware suitable for smart components must support timely and reliable communication and also must be resource efficient. COSMIC (COoperating Smart devices) is aimed at supporting the interaction between those components according to the concepts introduced so far. Based on the model of a WAN-of-CANs, we assume that the components are connected to some form of CAN as a fieldbus or a special wireless sensor network which provides specific network properties. E.g. a fieldbus developed for control applications usually includes mechanisms for predictable communication while other networks only support a best effort dissemination. A gateway connects these CANs to the next level in the network hierarchy.
The event system should allow the dynamic interaction over a hierarchy of such networks and comply with the overall CORTEX generic event model. Events are typed information carriers and are disseminated in a publisher/ subscriber style [24, 7], which is particularly suitable because it supports generative, anonymous communication [3] and does not create any artificial control dependencies between producers of information and the consumers. This decoupling in space (no references or names of senders or receivers are needed for communication) and the flow decoupling (no control transfer occurs with a data transfer) are well known [24, 7, 14] and crucial properties to maintain autonomy of components and dynamic interactions.
It is obvious that not all networks can provide the same QoS guarantees and secondly, applications may have widely differing requirements for event dissemination.
Additionally, when striving for predictability, resources have to be reserved and data structures must be set up before communication takes place. Thus, these things can not predictably be made on the fly while disseminating an event. Therefore, we introduced the notion of an event channel to cope with differing properties and requirements and have an object to which we can assign resources and reservations. The concept of an event channel is not new [10, 25], however, it has not yet been used to reflect the properties of the underlying heterogeneous communication networks and mechanisms as described by the GEAR architecture. Rather, existing event middleware allows to specify the priorities or deadlines of events handled in an event server. Event channels allow to specify the communication properties on the level of the event system in a fine grained way. An event channel is defined by: event channel := subject, quality attributeList, handlers The subject determines the types of events event which may be issued to the channel. The quality attributes model the properties of the underlying communication network and dissemination scheme. These attributes include latency specifications, dissemination constraints and reliability parameters. The notion of zones which represent a guaranteed quality of service in a subnetwork support this approach.
Our goal is to handle the temporal specifications as bound, coverage pairs [28] orthogonal to the more technical questions of how to achieve a certain synchrony property of the dissemination infrastructure. Currently, we support quality attributes of event channels in a CAN-Bus environment represented by explicit synchrony classes.
The COSMIC middleware maps the channel properties to lower level protocols of the regular network. Based on our previous work on predictable protocols for the CAN-Bus,
COSMIC defines an abstract network which provides hard, soft and non real-time message classes [21].
Correspondingly, we distinguish three event channel classes according to their synchrony properties: hard real-time channels, soft real-time channels and non-real-time channels.
Hard real-time channels (HRTC) guarantee event propagation within the defined time constraints in the presence 35 of a specified number of omission faults. HRTECs are supported by a reservation scheme which is similar to the scheme used in time-triggered protocols like TTP [16][31], TTP/A [17], and TTCAN [8]. However, a substantial advantage over a TDMA scheme is that due to CAN-Bus properties, bandwidth which was reserved but is not needed by a HRTEC can be used by less critical traffic [21].
Soft real-time channels (SRTC) exploit the temporal validity interval of events to derive deadlines for scheduling.
The validity interval defines the point in time after which an event becomes temporally inconsistent. Therefore, in a real-time system an event is useless after this point and may me discarded. The transmission deadline (DL) is defined as the latest point in time when a message has to be transmitted and is specified in a time interval which is derived from the expiration time: tevent ready < DL < texpiration − ∆notification texpiration defines the point in time when the temporal validity expires. ∆notification is the expected end-to-end latency which includes the transfer time over the network and the time the event may be delayed by the local event handling in the nodes. As said before, event deadlines are used to schedule the dissemination by SRTECs. However, deadlines may be missed in transient overload situations or due to arbitrary arrival times of events. On the publisher side the application"s exception handler is called whenever the event deadline expires before event transmission. At this point in time the event is also not expected to arrive at the subscriber side before the validity expires. Therefore, the event is removed from the sending queue. On the subscriber side the expiration time is used to schedule the delivery of the event. If the event cannot be delivered until its expiration time it is removed from the respective queues allocated by the COSMIC middleware. This prevents the communication system to be loaded by outdated messages.
Non-real-time channels do not assume any temporal specification and disseminate events in a best effort manner. An instance of an event channel is created locally, whenever a publisher makes an announcement for publication or a subscriber subscribes for an event notification. When a publisher announces publication, the respective data structures of an event channel are created by the middleware. When a subscriber subscribes to an event channel, it may specify context attributes of an event which are used to filter events locally. E.g. a subscriber may only be interested in events generated at a certain location. Additionally the subscriber specifies quality properties of the event channel. A more detailed description of the event channels can be found in [13].
Currently, COSMIC handles all event channels which disseminate events beyond the CAN network boundary as non real-time event channels. This is mainly because we use the TCP/IP protocol to disseminate events over wireless links or to the standard Ethernet. However, there are a number of possible improvements which can easily be integrated in the event channel model. The Timely Computing Base (TCB) [28] can be exploited for timing failure detection and thus would provide awareness for event dissemination in environments where timely delivery of events cannot be enforced. Additionally, there are wireless protocols which can provide timely and reliable message delivery [6, 23] which may be exploited for the respective event channel classes.
Events are the information carriers which are exchanged between sentient objects through event channels. To cope with the requirements of an ad-hoc environment, an event includes the description of the context in which it has been generated and quality attributes defining requirements for dissemination. This is particularly important in an open, dynamic environment where an event may travel over multiple networks. An event instance is specified as: event := subject, context attributeList, quality attributeList, contents A subject defines the type of the event and is related to the event contents. It supports anonymous communication and is used to route an event. The subject has to match to the subject of the event channel through which the event is disseminated. Attributes are complementary to the event contents. They describe individual functional and non-functional properties of the event. The context attributes describe the environment in which the event has been generated, e.g. a location, an operational mode or a time of occurrence. The quality attributes specify timeliness and dependability aspects in terms of validity interval, omission degree pairs. The validity interval defines the point in time after which an event becomes temporally inconsistent [18]. As described above, the temporal validity can be mapped to a deadline. However, usually a deadline is an engineering artefact which is used for scheduling while the temporal validity is a general property of a time, value entity. In a environment where a deadline cannot be enforced, a consumer of an event eventually must decide whether the event still is temporally consistent, i.e. represents a valid time, value entity.
Middleware On the architectural level, COSMIC distinguish three layers roughly depicted in Figure 4. Two of them, the event layer and the abstract network layer are implemented by the COSMIC middleware. The event layer provides the API for the application and realizes the abstraction of event and event channels.
The abstract network implements real-time message classes and adapts the quality requirements to the underlying real network. An event channel handler resides in every node. It supports the programming interface and provides the necessary data structures for event-based communication.
Whenever an object subscribes to a channel or a publisher announces a channel, the event channel handler is involved. It initiates the binding of the channel"s subject, which is represented by a network independent unique identifier to an address of the underlying abstract network to enable communication [14]. The event channel handler then tightly cooperates with the respective handlers of the abstract network layer to disseminate events or receive event notifications. It should be noted that the QoS properties of the event layer in general depend on what the abstract network layer can provide. Thus, it may not always be possible to e.g. support hard real-time event channels because the abstract network layer cannot provide the respective guarantees. In [13], we describe the protocols and services of the abstract network layer particularly for the CAN-Bus.
As can be seen in Figure 4, the hard real-time (HRT) message class is supported by a dedicated handler which is able to provide the time triggered message dissemination. 36  event notifications HRT-msg list SRT-msg queue NRT-msg queue HRT-msg calendar HRTC Handler S/NRTC Handler Abstract Network Layer CAN Layer RX Buffer TX Buffer RX, TX, error interrupts Event Channel Specs.
Event Layer send messages exception notification exceptions, notifications ECH: Event Channel Handler p u b l i s h a n n o u n c e s u b s c r i b e b i n d i n g p r o t o c o l c o n f i g . p r o t o c o l Global Time Service event notifications HRT-msg list SRT-msg queue NRT-msg queue HRT-msg calendar HRTC Handler S/NRTC Handler Abstract Network Layer CAN Layer RX Buffer TX Buffer RX, TX, error interrupts Event Channel Specs.
Event Layer send messages exception notification exceptions, notifications ECH: Event Channel Handler p u b l i s h a n n o u n c e s u b s c r i b e b i n d i n g p r o t o c o l c o n f i g . p r o t o c o l Global Time Service Figure 4: Architecture layers of COSMIC.
The HRT handler maintains the HRT message list, which contains an entry for each local HRT message to be sent.
The entry holds the parameters for the message, the activation status and the binding information. Messages are scheduled on the bus according to the HRT message calendar which comprises the precise start time for each time slot allocated for a message. Soft real-time message queues order outgoing messages according to their transmission deadlines derived from the temporal validity interval. If the transmission deadline is exceeded, the event message is purged out of the queue. The respective application is notified via the exception notification interface and can take actions like trying to publish the event again or publish it to a channel of another class. Incoming event messages are ordered according to their temporal validity. If an event message arrive, the respective applications are notified. At the moment, an outdated message is deleted from the queue and if the queue runs out of space, the oldest message is discarded.
However, there are other policies possible depending on event attributes and available memory space. Non real-time messages are FIFO ordered in a fixed size circular buffer.
The goal for developing COSMIC was to provide a platform to seamlessly integrate smart tiny components in a large system. Therefore, COSMIC should run also on the small, resource constraint devices which are built around 16Bit or even 8-Bit micro-controllers. The distributed COSMIC middleware has been implemented and tested on various platforms. Under RT-Linux, we support the real-time channels over the CAN Bus as described above. The RTLinux version runs on Pentium processors and is currently evaluated before we intent to port it to a smart sensor or actuator. For the interoperability in a WAN-of-CANs environment, we only provide non real-time channels at the moment. This version includes a gateway between the CANbus and a TCP/IP network. It allows us to use a standard wireless 802.11 network. The non real-time version of COSMIC is available on Linux, RT-Linux and on the microcontroller families C167 (Infineon) and 68HC908 (Motorola).
Both micro-controllers have an on-board CAN controller and thus do not require additional hardware components for the network. The memory footprint of COSMIC is about 13 Kbyte on a C167 and slightly more on the 68HC908 where it fits into the on-board flash memory without problems.
Because only a few channels are required on such a smart sensor or actuator component, the requirement of RAM (which is a scarce resource on many single chip systems) to hold the dynamic data structures of a channel is low. The COSMIC middleware makes it very easy to include new smart sensors in an existing system. Particularly, the application running on a smart sensor to condition and process the raw physical data must not be aware of any low level network specific details. It seamlessly interacts with other components of the system exclusively via event channels.
The demo example, briefly described in the next chapter, is using a distributed infrastructure of tiny smart sensors and actuators directly cooperating via event channels over heterogeneous networks.
A simple example for many important properties of the proposed system showing the coordination through the environment and events disseminated over the network is the demo of two cooperating robots depicted in Figure 5.
Each robot is equipped with smart distance sensors, speed sensors, acceleration sensors and one of the robots (the guide (KURT2) in front (Figure 5)) has a tracking camera allowing to follow a white line. The robots form a WAN-of-CANs system in which their local CANs are interconnected via a wireless 802.11 network. COSMIC provides the event layer for seamless interaction. The blind robot (N.N.) is searching the guide randomly. Whenever the blind robot detects (by its front distance sensors) an obstacle, it checks whether this may be the guide. For this purpose, it dynamically subscribes to the event channel disseminating distance events from rear distance sensors of the guide(s) and compares these with the distance events from its local front sensors.
If the distance is approximately the same it infers that it is really behind a guide. Now N.N. also subscribes to the event channels of the tracking camera and the speed sensors 37 Figure 5: Cooperating robots. to follow the guide. The demo application highlights the following properties of the system:
advance. In principle, any two a priori unknown robots can cooperate. All what publishers and subscribers have to know to dynamically interact in this environment is the subject of the respective event class. A problem will be to receive only the events of the robot which is closest. A robot identity does not help much to solve this problem. Rather, the position of the event generation entity which is captured in the respective attributes can be evaluated to filter the relevant event out of the event stream. A suitable wireless protocol which uses proximity to filter events has been proposed by Meier and Cahill [22] in the CORTEX project.
cooperation between the robots is controlled by sensing the distance between the robots. If the guide detects that the distance grows, it slows down. Respectively, if the blind robot comes too close it reduces its speed. The local distance sensors produce events which are disseminated through a low latency, highly predictable event channel. The respective reaction time can be calculated as function of the speed and the distance of the robots and define a dynamic dissemination deadline for events. Thus, the interaction through the environment will secure the safety properties of the application, i.e. the follower may not crash into the guide and the guide may not loose the follower. Additionally, the robots have remote subscriptions to the respective distance events which are used to check it with the local sensor readings to validate that they really follow the guide which they detect with their local sensors.
Because there may be longer latencies and omissions, this check occasionally will not be possible. The unavailability of the remote events will decrease the quality of interaction and probably and slow down the robots, but will not affect safety properties.
events of the line tracking camera. Thus it can see through the eye of the guide. Because it knows the distance to the guide and the speed as well, it can foresee the necessary movements. The proposed system provides the architectural framework for such a cooperation. The respective sentient object controlling the actuation of the robot receives as input the position and orientation of the white line to be tracked. In the case of the guide robot, this information is directly delivered as a body event with a low latency and a high reliability over the internal network. For the follower robot, the information comes also via an event channel but with different quality attributes. These quality attributes are reflected in the event channel description.
The sentient object controlling the actuation of the follower is aware of the increased latency and higher probability of omission.
The paper addresses problems of building large distributed systems interacting with the physical environment and being composed from a huge number of smart components.
We cannot assume that the network architecture in such a system is homogeneous. Rather multiple edge- networks are fused to a hierarchical, heterogeneous wide area network. They connect the tiny sensors and actuators perceiving the environment and providing sentience to the application. Additionally, mobility and dynamic deployment of components require the dynamic interaction without fixed, a priori known addressing and routing schemes. The work presented in the paper is a contribution towards the seamless interaction in such an environment which should not be restricted by technical obstacles. Rather it should be possible to control the flow of information by explicitly specifying functional and temporal dissemination constraints.
The paper presented the general model of a sentient object to describe composition, encapsulation and interaction in such an environment and developed the Generic Event Architecture GEAR which integrates the interaction through the environment and the network. While appropriate abstractions and interaction models can hide the functional heterogeneity of the networks, it is impossible to hide the quality differences. Therefore, one of the main concerns is to define temporal properties in such an open infrastructure. The notion of an event channel has been introduced which allows to specify quality aspects explicitly. They can be verified at subscription and define a boundary for event dissemination. The COSMIC middleware is a first attempt to put these concepts into operation. COSMIC allows the interoperability of tiny components over multiple network boundaries and supports the definition of different real-time event channel classes.
There are many open questions that emerged from our work. One direction of future research will be the inclusion of real-world communication channels established between sensors and actuators in the temporal analysis and the ordering of such events in a cause-effect chain. Additionally, the provision of timing failure detection for the adaptation of interactions will be in the focus of our research. To reduce network traffic and only disseminate those events to the subscribers which they are really interested in and which have a chance to arrive timely, the encapsulation and scoping schemes have to be transformed into respective multi-level filtering rules. The event attributes which describe aspects of the context and temporal constraints for the dissemination will be exploited for this purpose. Finally, it is intended to integrate the results in the COSMIC middleware to enable experimental assessment. 38
[1] J. Bacon, K. Moody, J. Bates, R. Hayton, C. Ma,
A. McNeil, O. Seidel, and M. Spiteri. Generic support for distributed applications. IEEE Computer, 33(3):68-76, 2000. [2] L. B. Becker, M. Gergeleit, S. Schemmer, and E. Nett.
Using a flexible real-time scheduling strategy in a distributed embedded application. In Proc. of the 9th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA), Lisbon,
Portugal, Sept. 2003. [3] N. Carriero and D. Gelernter. Linda in context.
Communications of the ACM, 32(4):444-458, apr 1989. [4] A. Casimiro (Ed.). Preliminary definition of cortex system architecture. CORTEX project,
IST-2000-26031, Deliverable D4, Apr. 2002. [5] CORTEX project Annex 1, Description of Work.
Technical report, CORTEX project, IST-2000-26031,
Oct. 2000. http://cortex.di.fc.ul.pt. [6] R. Cunningham and V. Cahill. Time bounded medium access control for ad-hoc networks. In Proceedings of the Second ACM International Workshop on Principles of Mobile Computing (POMC"02), pages 1-8, Toulouse,
France, Oct. 2002. ACM Press. [7] P. T. Eugster, P. Felber, R. Guerraoui, and A.-M.
Kermarrec. The many faces of publish/subscribe.
Technical Report DSC ID:200104, EPFL, Lausanne,
Switzerland, 2001. [8] T. F¨uhrer, B. M¨uller, W. Dieterle, F. Hartwich,
R. Hugel, and M.Walther. Time triggered communication on CAN, 2000. http://www.can-cia.org/can/ttcan/fuehrer.pdf. [9] R. B. GmbH. CAN Specification Version 2.0. Technical report, Sept. 1991. [10] T. Harrison, D. Levine, and D. Schmidt. The design and performance of a real-time corba event service. In Proceedings of the 1997 Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA), pages 184-200, Atlanta, Georgia, USA,
[11] J. Hightower and G. Borriello. Location systems for ubiquitous computing. IEEE Computer, 34(8):57-66, aug 2001. [12] A. Hopper. The Clifford Paterson Lecture, 1999 Sentient Computing. Philosophical Transactions of the Royal Society London, 358(1773):2349-2358, Aug. 2000. [13] J. Kaiser, C. Mitidieri, C. Brudna, and C. Pereira.
COSMIC: A Middleware for Event-Based Interaction on CAN. In Proc. 2003 IEEE Conference on Emerging Technologies and Factory Automation, Lisbon,
Portugal, Sept. 2003. [14] J. Kaiser and M. Mock. Implementing the real-time publisher/subscriber model on the controller area network (CAN). In Proceedings of the 2nd International Symposium on Object-oriented Real-time distributed Computing (ISORC99), Saint-Malo, France, May 1999. [15] K. Kim, G. Jeon, S. Hong, T. Kim, and S. Kim.
Integrating subscription-based and connection-oriented communications into the embedded CORBA for the CAN Bus. In Proceedings of the IEEE Real-time Technology and Application Symposium, May 2000. [16] H. Kopetz and G. Gr¨unsteidl. TTP - A Time-Triggered Protocol for Fault-Tolerant Real-Time Systems. Technical Report rr-12-92, Institut f¨ur Technische Informatik, Technische Universit¨at Wien,
Treilstr. 3/182/1, A-1040 Vienna, Austria, 1992. [17] H. Kopetz, M. Holzmann, and W. Elmenreich. A Universal Smart Transducer Interface: TTP/A.
International Journal of Computer System, Science Engineering, 16(2), Mar. 2001. [18] H. Kopetz and P. Ver´ıssimo. Real-time and Dependability Concepts. In S. J. Mullender, editor,
Distributed Systems, 2nd Edition, ACM-Press, chapter 16, pages 411-446. Addison-Wesley, 1993. [19] S. Lankes, A. Jabs, and T. Bemmerl. Integration of a CAN-based connection-oriented communication model into Real-Time CORBA. In Workshop on Parallel and Distributed Real-Time Systems, Nice, France, Apr.
[20] Local Interconnect Network: LIN Specification Package Revision 1.2. Technical report, Nov. 2000. [21] M. Livani, J. Kaiser, and W. Jia. Scheduling hard and soft real-time communication in the controller area network. Control Engineering, 7(12):1515-1523, 1999. [22] R. Meier and V. Cahill. Steam: Event-based middleware for wireless ad-hoc networks. In Proceedings of the International Workshop on Distributed Event-Based Systems (ICDCS/DEBS"02), pages 639-644, Vienna, Austria, 2002. [23] E. Nett and S. Schemmer. Reliable real-time communication in cooperative mobile applications.
IEEE Transactions on Computers, 52(2):166-180, Feb.
[24] B. Oki, M. Pfluegl, A. Seigel, and D. Skeen. The information bus - an architecture for extensible distributed systems. Operating Systems Review, 27(5):58-68, 1993. [25] O. M. G. (OMG). CORBAservices: Common Object Services Specification - Notification Service Specification, Version 1.0, 2000. [26] O. M. G. (OMG). Smart transducer interface, initial submission, June 2001. [27] P. Ver´ıssimo, V. Cahill, A. Casimiro, K. Cheverst,
A. Friday, and J. Kaiser. Cortex: Towards supporting autonomous and cooperating sentient entities. In Proceedings of European Wireless 2002, Florence, Italy,
Feb. 2002. [28] P. Ver´ıssimo and A. Casimiro. The Timely Computing Base model and architecture. Transactions on Computers - Special Issue on Asynchronous Real-Time Systems, 51(8):916-930, Aug. 2002. [29] P. Ver´ıssimo and A. Casimiro. Event-driven support of real-time sentient objects. In Proceedings of the 8th IEEE International Workshop on Object-oriented Real-time Dependable Systems, Guadalajara, Mexico,

Mobile ad-hoc networks (MANETs) consist of mobile nodes that autonomously establish connectivity via multi-hop wireless communications. Without relying on any existing, pre-configured network infrastructure or centralized control, MANETs are useful in situations where impromptu communication facilities are required, such as battlefield communications and disaster relief missions. As MANET applications demand collaborative processing and information sharing among mobile nodes, resource (service) discovery and distribution have become indispensable capabilities.
One approach to designing resource discovery and distribution schemes over MANETs is to construct a peer-to-peer (P2P) system (or an overlay) which organizes peers of the system into a logical structure, on top of the actual network topology. However, deploying such P2P systems over MANETs may result in either a large number of flooding operations triggered by the reactive routing process, or inefficiency in terms of bandwidth utilization in proactive routing schemes. Either way, constructing an overlay will potentially create a scalability problem for large-scale MANETs.
Due to the dynamic nature of MANETs, P2P systems should be robust by being scalable and adaptive to topology changes. These systems should also provide efficient and effective ways for peers to interact, as well as other desirable application specific features.
This paper describes a design paradigm that uses the following two functional primitives to design robust resource discovery and distribution schemes over MANETs.
explore a route to other peers holding resources of interest.
Optionally, advertisement packets are sent out to advertise routes from other peers about available resources. When traversing a route, these control packets measure goodness of the route and leave feedback information on each node along the way to guide subsequent control packets to appropriate directions.
the availability of resources change, existing routes may become stale while better routes become available. Sporadic random walk allows a control packet to explore different paths and opportunistically discover new and/or better routes.
Adopting this paradigm, the whole MANET P2P system operates as a collection of autonomous entities which consist of different types of control packets such as query and advertisement packets.
These packets work collaboratively, but indirectly, to achieve common tasks, such as resource discovery, routing, and load balancing. With collaboration among these entities, a MANET P2P system is able to ‘learn" the network dynamics by itself and adjust its behavior accordingly, without the overhead of organizing peers into an overlay.
The remainder of this paper is organized as follows. Related work is described in the next section. Section 3 describes the resource discovery scheme. Section 4 describes the resource distribution scheme. The replica invalidation scheme is described in Section 5, followed by it performance evaluation in Section 6. Section 7 concludes the paper.
For MANETs, P2P systems can be classified based on the design principle, into layered and cross-layer approaches. A layered approach adopts a P2P-like [1] solution, where resource discovery is facilitated as an application layer protocol and query/reply messages are delivered by the underlying MANET routing protocols.
For instance, Konark [2] makes use of a underlying multicast protocol such that service providers and queriers advertise and search services via a predefined multicast group, respectively.
Proem [3] is a high-level mobile computing platform for P2P systems over MANETs. It defines a transport protocol that sits on top of the existing TCP/IP stack, hence relying on an existing routing protocol to operate. With limited control over how control and data packets are routed in the network, it is difficult to avoid the inefficiency of the general-purpose routing protocols which are often reactive and flooding-based.
In contrast, cross-layer approaches either relies on its own routing mechanism or augments existing MANET routing algorithms to support resource discovery. 7DS [4], which is the pioneering work deploying P2P system on mobile devices, exploits data locality and node mobility to dissemination data in a single-hop fashion. Hence, long search latency may be resulted as a 7DS node can get data of interest only if the node that holds the data is in its radio coverage. Mohan et al. [5] propose an adaptive service discovery algorithm that combines both push and pull models.
Specifically, a service provider/querier broadcasts advertisement/query only when the number of nodes advertising or querying, which is estimated by received control packets, is below a threshold during a period of time. In this way, the number of control packets on the network is constrained, thus providing good scalability. Despite the mechanism to reduce control packets, high overhead may still be unavoidable, especially when there are many clients trying to locate different services, due to the fact that the algorithm relies on flooding,
For resource replication, Yin and Cao [6] design and evaluate cooperative caching techniques for MANETs. Caching, however, is performed reactively by intermediate nodes when a querier requests data from a server. Data items or resources are never pushed into other nodes proactively. Thanedar et al. [7] propose a lightweight content replication scheme using an expanding ring technique. If a server detects the number of requests exceed a threshold within a time period, it begins to replicate its data onto nodes capable of storing replicas, whose hop counts from the server are of certain values. Since data replication is triggered by the request frequency alone, it is possible that there are replicas unnecessarily created in a large scope even though only nodes within a small range request this data. Our proposed resource replication mechanism, in contrast, attempts to replicate a data item in appropriate areas, instead of a large area around the server, where the item is requested frequently.
We propose a cross-layer, hybrid resource discovery scheme that relies on the multiple interactions of query, reply and advertisement packets. We assume that each resource is associated with a unique ID1 . Initially, when a node wants to discover a resource, it deploys query packets, which carry the corresponding resource ID, and randomly explore the network to search for the requested resource. Upon receiving such a query packet, a reply packet is generated by the node providing the requested resource.
Advertisement packets can also be used to proactively inform other nodes about what resources are available at each node. In addition to discovering the ‘identity" of the node providing the requested resource, it may be also necessary to discover a ‘route" leading to this node for further interaction.
To allow intermediate nodes to make a decision on where to forward query packets, each node maintains two tables: neighbor 1 The assumption of unique ID is made for brevity in exposition, and resources could be specified via attribute-value assertions. table and pheromone table. The neighbor table maintains a list of all current neighbors obtained via a neighbor discovery protocol.
The pheromone table maintains the mapping of a resource ID and a neighbor ID to a pheromone value. This table is initially empty, and is updated by a reply packet generated by a successful query.
Figure 1 illustrates an example of a neighbor table and a pheromone table maintained by node A having four neighbors. When node A receives a query packet searching for a resource, it makes a decision to which neighbor it should forward the query packet by computing the desirability of each of the neighbors that have not been visited before by the same query packet. For a resource ID r, the desirability of choosing a neighbor n, δ(r,n), is obtained from the pheromone value of the entry whose neighbor and resource ID fields are n and r, respectively. If no such entry exists in the pheromone table, δ(r,n) is set to zero.
Once the desirabilities of all valid next hops have been calculated, they are normalized to obtain the probability of choosing each neighbor. In addition, a small probability is also assigned to those neighbors with zero desirability to exercise the sporadic random walk primitive. Based on these probabilities, a next hop is selected to forward the query packet to. When a query packet encounters a node with a satisfying resource, a reply packet is returned to the querying node. The returning reply packet also updates the pheromone table at each node on its return trip by increasing the pheromone value in the entry whose resource ID and neighbor ID fields match the ID of the discovered resource and the previous hop, respectively. If such an entry does not exist, a new entry is added into the table. Therefore, subsequent query packets looking for the same resource, when encountering this pheromone information, are then guided toward the same destination with a small probability of taking an alternate path.
Since the hybrid discovery scheme neither relies on a MANET routing protocol nor arranges nodes into a logical overlay, query packets are to traverse the actual network topology. In dense networks, relatively large nodal degrees can have potential impacts on this random exploring mechanism. To address this issue, the hybrid scheme also incorporates proactive advertisement in addition to the reactive query. To perform proactive advertisement, each node periodically deploys an advertising packet containing a list of its available resources" IDs. These packets will traverse away from the advertising node in a random walk manner up to a limited number of hops and advertise resource information to surrounding nodes in the same way as reply packets. In the hybrid scheme, an increase of pheromone serves as a positive feedback which indirectly guides query packets looking for similar resources. Intuitively, the amount of pheromone increased is inversely proportional to the distance the reply packet has traveled back, and other metrics, such as quality of the resource, could contribute to this amount as well. Each node also performs an implicit negative feedback for resources that have not been given a positive feedback for some time by regularly decreasing the pheromone in all of its pheromone table entries over time. In addition, pheromone can be reduced by an explicit negative response, for instance, a reply packet returning from a node that is not willing to provide a resource due to excessive workload. As a result, load balancing can be achieved via positive and negative feedback. A node serving too many nodes can either return fewer responses to query packets or generate negative responses.
Figure 1: Example illustrating neighbor and pheromone tables maintained by node A: (a) wireless connectivity around A showing that it currently has four neighbors, (b) A"s neighbor table, and (c) a possible pheromone table of A Figure 2: Sample scenarios illustrating the three mechanisms supporting load-balancing: (a) resource replication, (b) resource relocation, and (c) resource division
In addition to resource discovery, a querying node usually attempts to access and retrieve the contents of a resource after a successful discovery. In certain situations, it is also beneficial to make a resource readily available at multiple nodes when the resource can be relocated and/or replicated, such as data files.
Furthermore, in MANETs, we should consider not only the amount of load handled by a resource provider, but also the load on those intermediate nodes that are located on the communication paths between the provider and other nodes as well. Hence, we describe a cross-layer, hybrid resource distribution scheme to achieve load balancing by incorporating the functionalities of resource relocation, resource replication, and resource division.
Multiple replicas of a resource in the network help prevent a single node, as well as nodes surrounding it, from being overloaded by a large number of requests and data transfers. An example is when a node has obtained a data file from another node, the requesting node and the intermediate nodes can cache the file and start sharing that file with other surrounding nodes right away. In addition, replicable resources can also be proactively replicated at other nodes which are located in certain strategic areas. For instance, to help nodes find a resource quickly, we could replicate the resource so that it becomes reachable by random walk for a specific number of hops from any node with some probability, as depicted in Figure 2(a).
To realize this feature, the hybrid resource distribution scheme employs a different type of control packet, called resource replication packet, which is responsible for finding an appropriate place to create a replica of a resource. A resource replication packet of type R is deployed by a node that is providing the resource R itself. Unlike a query packet which follows higher pheromone upstream toward a resource it is looking for, a resource replication packet tends to be propelled away from similar resources by moving itself downstream toward weaker pheromone. When a resource replication packet finds itself in an area with sufficiently low pheromone, it makes a decision whether it should continue exploring or turn back. The decision depends on conditions such as current workload and/or remaining energy of the node being visited, as well as popularity of the resource itself.
In certain situations, a resource may be required to transfer from one node to another. For example, a node may no longer want to possess a file due to the shortage of storage space, but it cannot simply delete the file since other nodes may still need it in the future. In this case, the node can choose to create replicas of the file by the aforementioned resource replication mechanism and then delete its own copy. Let us consider a situation where a majority of nodes requesting for a resource are located far away from a resource provider, as shown on the top of Figure 2(b). If the resource R is relocatable, it is preferred to be relocated to another area that is closer to those nodes, similar to the bottom of the same figure. Hence network bandwidth is more efficiently utilized.
The 3rd Conference on Mobile Technology, Applications and Systems - Mobility 2006 3 The hybrid resource distribution scheme incorporates resource relocation algorithms that are adaptive to user requests and aim to reduce communication overhead. Specifically, by following the same pheromone maintenance concept, the hybrid resource distribution scheme introduces another type of pheromone which corresponds to user requests, instead of resources. This type of pheromone, called request pheromone, is setup by query packets that are in their exploring phases (not returning ones) to guide a resource to a new location.
Certain types of resources can be divided into smaller subresources (e.g., a large file being broken into smaller files) and distributed to multiple locations to avoid overloading a single node, as depicted in Figure 2(c). The hybrid resource distribution scheme incorporates a resource division mechanism that operates at a thin layer right above all the other mechanisms described earlier. The resource division mechanism is responsible for decomposing divisible resources into sub-resources, and then adds an extra keyword to distinguish each sub-resource from one another. Therefore, each of these sub-resources will be seen by the other mechanisms as one single resource which can be independently discovered, replicated, and relocated. The resource division mechanism is also responsible for combining data from these subresources together (e.g., merging pieces of a file) and delivering the final result to the application.
Although replicas improve accessibility and balance load, replica invalidation becomes a critical issue when nodes caching updatable resources may concurrently update their own replicas, which renders replicas held by other nodes obsolete. Most existing solutions to the replica invalidation problem either impose constrains that only the data source could perform update and invalidate other replicas, or resort to network-wide flooding which results in heavy network traffic and leads to scalability problem, or both. The lack of infrastructure supports and frequent topology changes in MANETs further challenge the issue.
We apply the same cross-layer paradigm to invalidating replicas in MANETs which allows concurrent updates performed by multiple replicas. To coordinate concurrent updates and disseminate replica invalidations, a special infrastructure, called validation mesh or mesh for short, is adaptively maintained among nodes possessing ‘valid" replicas of a resource. Once a node has updated its replica, an invalidation packet will only be disseminated over the validation mesh to inform other replica-possessing nodes that their replicas become invalid and should be deleted. The structure (topology) of the validation mesh keeps evolving (1) when nodes request and cache a resource, (2) when nodes update their respective replicas and invalidate other replicas, and (3) when nodes move. To accommodate the dynamics, our scheme integrates the components of swarm intelligence to adaptively maintain the validation mesh without relying on any underlying MANET routing protocol. In particular, the scheme takes into account concurrent updates initiated by multiple nodes to ensure the consistency among replicas. In addition, version number is used to distinguish new from old replicas when invalidating any stale replica.
Simulation results show that the proposed scheme effectively facilitates concurrent replica updates and efficiently perform replica invalidation without incurring network-wide flooding.
Figure 3 depicts the idea of ‘validation mesh" which maintains connectivity among nodes holding valid replicas of a resource to avoid network-wide flooding when invalidating replicas.
Figure 3: Examples showing maintenance of validation mesh There are eight nodes in the sample network, and we start with only node A holding the valid file, as shown in Figure 3(a). Later on, node G issues a query packet for the file and eventually obtains the file from A via nodes B and D. Since intermediate nodes are allowed to cache forwarded data, nodes B, D, and G will now hold valid replicas of the file. As a result, a validation mesh is established among nodes A, B, D, and G, as depicted in Figure 3(b). In Figure 3(c), another node, H, has issued a query packet for the same file and obtained it from node B"s cache via node E.
At this point, six nodes hold valid replicas and are connected through the validation mesh. Now we assume node G updates its replica of the file and informs the other nodes by sending an invalidation packet over the validation mesh. Consequently, all other nodes except G remove their replicas of the file from their storage and the validation mesh is torn down. However, query forwarding pheromone, as denoted by the dotted arrows in Figure 3(d), is setup at these nodes via the ‘reverse paths" in which the invalidation packets have traversed, so that future requests for this file will be forwarded to node G. In Figure 3(e), node H makes a new request for the file again. This time, its query packet follows the pheromone toward node G, where the updated file can be obtained. Eventually, a new validation mesh is established over nodes G, B, D, E, and H.
To maintain a validation mesh among the nodes holding valid replicas, one of them is designated to be the focal node. Initially, the node that originally holds the data is the focal node. As nodes update replicas, the node that last (or most recently) updates a
corresponding replica assumes the role of focal node. We also name nodes, such as G and H, who originate requests to replicate data as clients, and nodes B, D, and E who locally cache passing data as data nodes. For instance, in Figures 3(a), 3(b), and 3(c), node A is the focal node; in Figures 3(d), 3(e), and 3(f), node G becomes the focal node. In addition, to accommodate newly participating nodes and mobility of nodes, the focal node periodically floods the validation mesh with a keep-alive packet, so that nodes who can hear this packet are considered themselves to be part of the validation mesh. If a node holding a valid/updated replica doesn"t hear a keep-alive packet for a certain time interval, it will deploy a search packet using the resource discovery mechanism described in Section 3 to find another node (termed attachment point) currently on the validation mesh so that it can attach itself to. Once an attachment point is found, a search_reply packet is returned to the disconnected node who originated the search.
Intermediate nodes who forward the search_reply packet will become part of the validation mesh as well. To illustrate the effect of node mobility, in Figure 3(f), node H has moved to a location where it is not directly connected to the mesh. Via the resource discovery mechanism, node H relies on an intermediate node F to connect itself to the mesh. Here, node F, although part of the validation mesh, doesn"t hold data replica, and hence is termed nondata node.
Client and data node who keep hearing the keep-alive packets from the focal node act as if they are holding a valid replica, so that they can reply to query packets, like node B in Figure 3(c) replying a request from node H. While a disconnected node attempting to discover an attachment point to reattach itself to the mesh, the disconnected node can"t reply to a query packet. For instance, in Figure 3(f), node H does not reply to any query packet before it reattaches itself to the mesh.
Although validation mesh provides a conceptual topology that (1) connects all replicas together, (2) coordinates concurrent updates, and (3) disseminates invalidation packets, the technical issue is how such a mesh topology could be effectively and efficiently maintained and evolved when (a) nodes request and cache a resource, (b) when nodes update their respective replicas and invalidate other replicas, and (c) when nodes move. Without relying on any MANET routing protocols, the two primitives work together to facilitate efficient search and adaptive maintenance.
We have conducted simulation experiments using the QualNet simulator to evaluate the performance of the described resource discovery, resource distribution, and replica invalidation schemes.
However, due to space limitation only the performance of the replica invalidation is reported. In our experiments, eighty nodes are uniformly distributed over a terrain of size 1000×1000 m2 .
Each node has a communication range of approximately 250 m over a 2 Mbps wireless channel, using IEEE802.11 as the MAC layer. We use the random-waypoint mobility model with a pause time of 1 second. Nodes may move at the minimum and maximum speeds of 1 m/s and 5 m/s, respectively. Table 1 lists other parameter settings used in the simulation. Initially, there is one resource server node in network. Two nodes are randomly picked up every 10 seconds as clients. Every β seconds, we check the number of nodes, N, which have gotten data. Then we randomly pickup Min(γ,N) nodes from them to initiate data update. Each experiment is run for 10 minutes.
Table 1: Simulation Settings HOP_LIMIT 10 ADVERTISE_HOP_LIMIT 1 KEEPALIVE_INTERVAL 3 second NUM_SEARCH 1 ADVERTISE_INTERVAL 5 second EXPIRATION_INTERVAL 10 second Average Query Generation Rate 2 query/ 10 sec Max # of Concurrent Update (γ) 2 Frequency of Update (β) 3s We evaluate the performance under different mobility speed, the density, the maximum number of concurrent update nodes, and update frequency using two metrics: • Average overhead per update measures the average number of packets transmitted per update in the network. • Average delay per update measures how long our approach takes to finish an update on average.
All figures shown present the results with a 70% confidence interval.
Figure 4: Overhead vs. speed for 80 nodes Figure 5: Overhead vs. density Figure 6: Overhead vs. max #concurrent updates Figure 7: Overhead vs. freq.
Figure 8: Delay vs. speed Figure 9: Delay vs. density The 3rd Conference on Mobile Technology, Applications and Systems - Mobility 2006 5 Figure 10: Delay vs. max #concurrent updates Figure 11: Delay vs. freq.
Figures 4, 5, 6, and 7 show the overhead versus various parameter values. In Figure 4, the overhead increases as the speed increase, which is due to the fact that as the speed increase, nodes move out of mesh more frequently, and will send out more search packets.
However, the overhead is not high, and even in speed 10m/sec, the overhead is below 100 packets. In contrast, the packets will be expected to be more than 200 packets at various speeds when flooding is used.
Figure 5 shows that the overhead almost remains the same under various densities. That is attributed to only flooding over the mesh instead of the whole network. The size of mesh doesn"t vary much on various densities, so that the overhead doesn"t vary much.
Figure 6 shows that overhead also almost remains the same under various maximum number of concurrent updates. That"s because one more node just means one more flood over the mesh during update process so that the impact is limited.
Figure 7 shows that if updates happen more frequently, the overhead is higher. This is because the more quickly updates happen, (1) there will be more keep_alive message over the mesh between two updates, and (2) nodes move out of mesh more frequently and send out more search packets.
Figures 8, 9, 10, and 11 show the delay versus various parameter values. From Figure 8, we know the delay increases as the speed increases, which is due to the fact that with increasing speed, clients will move out of mesh with higher probability. When these clients want to update data, they will spend time to first search the mesh. The faster the speed, the more time clients need to spend to search the mesh.
Figure 9 shows that delay is negligibly affected by the density.
Delay decreases slightly as the number of nodes increases, due to the fact that the more nodes in the network, the more nodes receives the advertisement packets which helps the search packet find the target so that the delay of update decreases.
Figure 10 shows that delay decreases slightly as the maximum number of concurrent updates increases. The larger the maximum number of concurrent updates is, the more nodes are picked up to do update. Then with higher probability, one of these nodes is still in mesh and finishes the update immediately (don"t need to search mesh first), which decreases the delay.
Figure 11 shows how the delay varies with the update frequency.
When updates happen more frequently, the delay will higher.
Because the less frequently, the more time nodes in mesh have to move out of mesh then they need to take time to search the mesh when they do update, which increases the delay.
The simulation results show that the replica invalidation scheme can significantly reduce the overhead with an acceptable delay.
To facilitate resource discovery and distribution over MANETs, one approach is to designing peer-to-peer (P2P) systems over MANETs which constructs an overlay by organizing peers of the system into a logical structure on the top of MANETs" physical topology. However, deploying overlay over MANETs may result in either a large number of flooding operations triggered by the routing process, or inefficiency in terms of bandwidth usage.
Specifically, overlay routing relies on the network-layer routing protocols. In the case of a reactive routing protocol, routing on the overlay may cause a large number of flooded route discovery message since the routing path in each routing step must be discovered on demand. On the other hand, if a proactive routing protocol is adopted, each peer has to periodically broadcast control messages, which leads to poor efficiency in terms of bandwidth usage. Either way, constructing an overlay will potentially suffer from the scalability problem. The paper describes a design paradigm that uses the functional primitives of positive/negative feedback and sporadic random walk to design robust resource discovery and distribution schemes over MANETs. In particular, the scheme offers the features of (1) cross-layer design of P2P systems, which allows the routing process at both the P2P and the network layers to be integrated to reduce overhead, (2) scalability and mobility support, which minimizes the use of global flooding operations and adaptively combines proactive resource advertisement and reactive resource discovery, and (3) load balancing, which facilitates resource replication, relocation, and division to achieve load balancing.
[1] A. Oram, Peer-to-Peer: Harnessing the Power of Disruptive Technologies. O"Reilly, March 2000. [2] S. Helal, N. Desai, V. Verma, and C. Lee, Konark - A Service Discovery and Delivery Protocol for ad-hoc Networks, in the Third IEEE Conference on Wireless Communication Networks (WCNC), New Orleans, Louisiana, 2003. [3] G. Krotuem, Proem: A Peer-to-Peer Computing Platform for Mobile ad-hoc Networks, in Advanced Topic Workshop Middleware for Mobile Computing, Germany, 2001. [4] M. Papadopouli and H. Schulzrinne, A Performance Analysis of 7DS a Peer-to-Peer Data Dissemination and Prefetching Tool for Mobile Users, in Advances in wired and wireless communications, IEEE Sarnoff Symposium Digest,
Ewing, NJ, 2001, (Best student paper & poster award). [5] U. Mohan, K. Almeroth, and E. Belding-Royer, Scalable Service Discovery in Mobile ad-hoc Networks, in IFIP Networking Conference, Athens, Greece, May 2004. [6] L. Yin and G. Cao, Supporting Cooperative Caching in Ad Hoc Networks, in IEEE INFOCOM, 2004. [7] V. Thanedar, K. Almeroth, and E. Belding-Royer, A Lightweight Content Replication Scheme for Mobile ad-hoc Environments, in IFIP Networking Conference, Athens, Greece,
May 2004.

Operational Transformation (OT) was originally invented for consistency maintenance in plain-text group editors [4].
In over 15 years, OT has evolved to support an increasing number of applications, including group undo [15, 19, 18, 21], group-awareness [28], operation notification and compression [20], spreadsheet and table-centric applications [14, 27], HTML/XML and tree-structured document editing [3, 7], word processing and slide creation [29, 25, 24], transparent and heterogenous application-sharing [1, 10, 24], and mobile replicated computing and database systems [6, 16].
To effectively and efficiently support existing and new applications, we must continue to improve the capability and quality of OT in solving both old and new problems. The soundness of the theoretical foundation for OT is crucial in this process. One theoretical underpinning of all existing OT algorithms is causality/concurrency [9, 17, 4, 22]: causally related operations must be executed in their causal order; concurrent operations must be transformed before their execution. However, the theory of causality is inadequate to capture essential OT conditions for correct transformation.
The limitation of the causality theory had caused correctness problems from the very beginning of OT. The dOPT algorithm was the first OT algorithm and was based solely on the concurrency relationships among operations [4]: a pair of operations are transformable as long as they are concurrent. However, later research discovered that the concurrency condition alone is not sufficient to ensure the correctness of transformation. Another condition is that the two concurrent operations must be defined on the same document state. In fact, the failure to meet the second condition was the root of the dOPT-puzzle [22]. This puzzle was solved in various ways, but the theory of causality as well as its limitation were inherited by all follow-up OT algorithms.
The causality theory limitation became even more prominent when OT was applied to solve the undo problem in group editors. The concept of causality is unsuitable to capture the relationships between an inverse operation (as an interpretation of a meta-level undo command) and other normal editing operations. In fact, the causality relation is not defined for inverse operations (see Section 2). Various patches were invented to work around this problem, resulting in more intricate complicated OT algorithms [18, 21].
After having designed, implemented, and experimented with a series of OT algorithms of increased complexity, we reflected on what had been learned and set out to develop a uniformed theoretical framework for better understanding and resolving OT problems, reducing its complexity, and 279 supporting its continual evolution. In this paper, we report the main results of this effort: the theory of operation context and the COT (Context-based OT) algorithm.
The rest of this paper is organized as follows. First, we define causal-dependency/-independency and briefly describe their limitations in Section 2. Then, we present the key elements of the operation context theory, including the definition of operation context, context-dependency/-independency relations, context-based conditions, and context vectors in Section 3. In Section 4, we present the basic COT algorithm for supporting consistency maintenance (do) and group undo under the assumption that underlying transformation functions are able to preserve some important transformation properties. Then, these transformation properties and their pre-conditions are discussed in Section 5. The COT solutions to these transformation properties are presented in Section 6. Comparison of the COT work to prior OT work, OT correctness issues, and future work are discussed in Section 7. Finally, major contributions of this work are summarized in Section 8.
The theory of causality is central to distributed computing and to the design of all existing OT algorithms.
Following Lamport [9], causal-dependency/-independency relations among editing operations can be defined in terms of their generation and execution sequences [4, 23].
Definition 1. Causal-dependency relation → Given two operations Oa and Ob, generated at sites i and j, Ob is causal-dependent on Oa, denoted by Oa → Ob, iff: (1) i = j and the generation of Oa happened before the generation of Ob; or (2) i = j and the execution of Oa at site j happened before the generation of Ob; or (3) there exists an operation Ox, such that Oa → Ox and Ox → Ob. 2 Definition 2. Causal-independency relation Given two operations Oa and Ob, Oa and Ob are causalindependent or concurrent, denoted by Oa Ob, iff neither Oa → Ob, nor Ob → Oa. 2 Just as Vector Logical Clocks are used for capturing casuality in distributed systems [17], State Vectors have been used for capturing causal relationships among operations and for representing document states in OT systems [4, 19, 23].
To illustrate causal relations among operations, consider a real-time group editing session with two sites in Figure 1.
There are three editing operations in this scenario (the undo command Undo(O2) and its relation with other operations shall be explained later): O1 generated at site 0, and O2 and O3 generated at site 1. According to Definitions 1 and 2, we have O2 → O3 because the generation of O2 happened before the generation of O3; O1 O2 and O1 O3 because for each pair, neither operation"s execution happened before the other operation"s generation.
In the following discussion, we shall use the term ITtransform to mean the use of the IT (Inclusion Transformation) function: IT(Oa, Ob), which transforms operation Oa against operation Ob in such a way that the impact of Ob is effectively included in Oa [23]. This term is introduced to differentiate this special transformation function from other steps involved in a transformation process.
Figure 1: A real-time group editing scenario.
The scenario in Figure 1 (without the undo command) has often been used to illustrate the dOPT-puzzle. Under the dOPT algorithm [4], when O2 arrives at site 0, it will be ITtransformed against O1 since O2 O1; this is correct because O2 and O1 are defined on the same (initial) document state.
When O3 arrives at site 0, it will also be IT-transformed against O1 since O3 O1; but this is incorrect because O3 is defined on the document state that contains the effect of O2, whereas O1 is defined on the initial document state. In this case, the parameters of O3 and O1 are not comparable and hence may not be IT-transformed correctly. The solution to this puzzle is first to IT-transform O1 against O2 to produce O1, which is defined on the document state including the effect of O2 (the same state on which O3 is defined), and then to IT-transform O3 against O1 [22].
From Definitions 1 and 2, it is clear that the causaldependency relation is only defined for original operations (e.g. O1, O2 and O3) directly generated by users, but not for transformed operations (e.g. O1). Furthermore, the concurrency relation does not capture the essential condition for correct IT-transformation: the two input operations must be defined on the same document state [23].
Another major limitation of causality is its unsuitability for capturing OT conditions for inverse operations. The Undo(O2) command in Figure 1 is interpreted as an inverse operation O2. The correct undo effect for O2 is to eliminate the effect of O2 but retain the effects of other operations (i.e. O1 and O3) [21]. To achieve this effect, O2 needs to be treated as an operation defined on the document state including the effect of O2 but not O1 and O3, so that O2 can be transformed against O1 and O3 before its execution.
However, according to Lamport"s happen-before relation [9],
Undo(O2) is causally dependent on O1, O2, and O3. If O2 was to inherit the causal relation of Undo(O2), then it would be effectively treated as an operation defined on the document state with the effects of all three operations O1, O2, and O3, which would prohibit O2 from being transformed against any operation, thus failing to achieve the correct undo effect. Moreover, after executing an inverse operation like O2, the document state can no longer be properly represented by the state vector, which is only capable of representing original normal editing operations.
Conceptually, each operation O is associated with a context, denoted by C(O), which corresponds to the document 280 state on which the operation is defined. The significance of operation context is twofold: (1) an operation can be correctly executed only if its context and the current document state are the same; and (2) an operation can be correctly ITtransformed against another operation only if the contexts of these two operations are the same.
In Figure 1, both O1 and O2 are defined on the same initial document so they are associated with the same context; O3 is defined on the document state which includes the effect of O2, so C(O3) is different from C(O1) or C(O2). When O2 arrives at site 0, it cannot be executed as-is since C(O2) does not match the current document state at site 0 which includes the effect of O1. O2 can be correctly IT-transformed against O1 since their contexts corresponds to the same initial document state. When O3 arrives at site 0, it cannot be executed as-is either since C(O3) does not match the current document state at site 0 which includes the effects of both O1 and O2. O3 cannot be correctly IT-transformed against O1 since their contexts are different, which is the root of the dOPT-puzzle. As discussed in Section 2, Undo(O2) should be interpreted as an inverse O2 defined on the document state with the effect of O2 only.
To facilitate comparison and manipulation of operation contexts for correct execution and transformation, it is necessary to explicitly represent operation context.
In OT systems, there are two different kinds of operation: original operations which are generated by users, and transformed operations which are the outcomes of some transformations. Original operations can be further divided into two classes: normal operations which are generated to do something, and inverse operations which are generated to undo some executed operations. For any operation O, its inverse is denoted by O. Since every transformed operation must come from an original operation, we use the notation org(O) to denote the original operation of O. If O is an original operation, then org(O) = O.
Since the context of an operation corresponds to the document state on which the operation is defined, the problem of context representation can be reduced into the problem of document state representation. In an OT-based group editor, each document state can be uniquely represented by the set of original operations executed so far on the document. These original operations may be executed in different orders or in different (original or transformed) forms at different sites, but the same document state must be achieved (according to the convergence requirement [23]). We use original (normal and inverse) operations, rather than their transformed versions, to represent a document state.
Definition 3. Document state representation A document state can be represented by DS as follows:
document state represented by DS, the new document state is represented by DS = DS ∪ {org(O)}. 2 This presentation does not specify what execution forms the original operations in DS should take to bring the document to the current state, but it captures essential and sufficient information for detecting whether two document states are the same and for deriving their differences in terms of original operations.
Based on the document state representation, the context of an original normal operation should be the same as the representation of the document state from which this operation was generated. To achieve the undo effect in [21], an original inverse operation O should be defined on the document state DS = C(O) ∪ {O}, which is the state after executing the original operation O on the state C(O).
According to the definition of the IT function [23], a transformed operation O , where O = IT(O, Ox), should be defined on the document state DS = C(O)∪{org(Ox)}, which is the state achievable by executing Ox on the state C(O).
More precisely, the context of an operation is defined blow.
Definition 4. The context of an operation
where DS is the representation of the document state from which O was generated.
{O}, where O is the operation to be undone.
{org(Ox)}, where O = IT(O, Ox). 2 According to the above definition, the context of any type of operation can be represented as a set of original operations.
For the scenario in Figure 1, we have C(O1) = {}, C(O2) = {}, and C(O3) = {O2} according to Definition 4-Item 1.
According to Definition 4-Item 2, we have C(O2) = {O2}.
From O2 = IT(O2, O1), we have C(O2) = {O1} according to Definition 4-Item 3.
We define the context-dependency/-independency relation among operations in terms of whether an original operation is included in the context of another operation of any type.
Definition 5. Context-dependency relation c → Given an original operation Oa and an operation Ob of any type, Ob is context-dependent on Oa, denoted by Oa c → Ob, iff: (1) Oa ∈ C(Ob); or (2) there exists an original operation Ox, such that Oa ∈ C(Ox) and Ox ∈ C(Ob). 2 It should be noted that the context-dependency relation is defined only between an original (either normal or inverse) operation and another operation of any type (original or transformed). This is because any operation has a context, but only original operations can be included in a context.
Definition 6. Context-independency relation c  Given two original operations Oa and Ob, Oa and Ob are context-independent, denoted by Oa c Ob, iff neither Oa c → Ob, nor Ob c → Oa. 2 It can be shown that if both Oa and Ob are original normal operations, then Oa c → Ob is equivalent to Oa → Ob; and Oa c Ob is equivalent to Oa Ob. In other words, the causal-dependency/-independency relation is a special case of the context-dependency/-independency relation.
The following Context-based Conditions (CC) capture essential requirements for operation execution and transformation in OT systems: 281 CC1: C(O) ⊆ DS is a necessary condition for an original operation O to be transformed to the document state DS for execution.
CC1 ensures that O is always executed after the contextdependent operations included in C(O). In other words, for any original operation Ox, if Ox c → O, then Ox must be executed before O. When O is an original normal operation, all operations which are causally before O must be included in C(O) (according to Definition 1 and Definition 5), so CC1 preserves the causal ordering among original normal operations [4, 22]. When O is an original inverse operation, C(O) must include the operation to be undone by O (see Definition 4-Item 2), so CC1 preserves the do-undo ordering among normal and inverse operations [21].
CC2: DS − C(O)1 is the set of operations that O must be transformed against before O is executed on the document state DS.
CC2 ensures that O is transformed against all contextindependent operations in DS before its execution. It can be shown that, for any Ox in DS − C(O), it must be that Ox c O. When O is an original normal operation, DS − C(O) must include all executed operations which are concurrent with O, so CC2 covers the condition that O should be transformed against concurrent operations [4, 22]. When O is an inverse operation, CC2 covers the condition that O should be transformed against all operations which are executed after the operation to be undone by O [21].
CC3: C(O) = DS is a necessary condition for O to be executed on the document state DS.
CC3 is required for correctly executing operations.
CC4: C(Oa) ⊆ C(Ob) is a necessary condition for Oa to be IT-transformable to the new context given by C(Ob).
CC4 is required because if C(Oa) ⊆ C(Ob), then there must be an operation Ox ∈ C(Oa) but Ox ∈ C(Ob), which means Oa cannot be IT-transformed to the new context C(Ob) since IT-transformation cannot remove this Ox from C(Oa) (see Definition 4-item3).
CC5: C(Ob) − C(Oa) is the set of operations that Oa must be transformed against before IT-transformed against Ob.
CC5 ensures that Oa is transformed against contextindependent operations in C(Ob) before IT-transformed against Ob. It can be shown that, for any Ox in C(Ob) − C(Oa), it must be that Ox c Oa,
CC6: C(Oa) = C(Ob) is a necessary condition for Oa to be IT-transformed against Ob.
CC6 is required for correctly applying IT functions.
In summary, CC1 and CC4 are required for ensuring correct ordering of operation execution/transformation; CC2 and CC5 are required for selecting correct transformation target operations; and CC3 and CC6 are required for ensuring correct operation execution/transformation. These context-based conditions form the foundation for the COT algorithm to be presented in Section 4 and Section 6. 1 DS − C(O) is the set difference between DS and C(O).
An important element of the operation context theory is the context vector, which represents the set of operations of a context in an efficient way. For notational convenience, we assume that a collaborative editing session consists of N collaborating sites, identified by 0, 1, . . . , N − 1.
Original normal operations generated at each site are strictly sequential, so each of them can be uniquely identified by a pair of integers (sid, ns), where sid is the site identifier and ns is the local sequence number of this operation.
Let Oij be an original normal operation generated at site i with a sequence number j. If Oij is included in a context C(O), then Oi1, Oi2, . . . , Oij−1 must also be included in C(O) according to Definition 3 and Definition 4.
Therefore, all normal operations generated at the same site can be sufficiently characterized by the largest sequence number of these operations. All original normal operations in a context can be partitioned into N groups according to their generation sites, so N integers are needed for representing original normal operations in a context.
An original inverse operation can be generated to undo an original normal operation, or to redo an undone operation. Each original inverse operation directly or indirectly corresponds to exactly one original normal operation. For example, inverse operation O may be generated to undo O, and O may be generated to undo O. Both O and O correspond to the same normal operation O. Based on this observation, all original inverse operations in an operation context can be grouped by their corresponding original normal operations: one inverse group for each undone original normal operation.
Inverse operations in the same inverse group can be further differentiated by a sequence number based on their execution order within this group. For example, O and O are in the same inverse group corresponding to O, so O has the sequence number 1, and O has the sequence number 2.
In general, an inverse can be identified by a triple (sid, ns, is), where sid and ns are the site identifier and sequence number of the corresponding normal operation, and is is the inverse sequence number within the group. Since inverses are sequentially executed, the largest sequence number in the group can be used to represent all inverses in the group.
Inverse groups can be further partitioned into N inverse clusters according to the site identifiers of their corresponding normal operations. The inverse cluster at site i - icican be expressed as follows: ici = [(ns0, is0), (ns1, is1), . . . , (nsk−1, isk−1)], where each pair (nsj, isj), 0 ≤ j < k, represents an inverse group with isj inverse operations corresponding to the original normal operation with sequence number nsj at site i. If no normal operation at site i has been undone, ici is empty.
To represent an operation context with both original normal and inverse operations, an N-dimensional context vector is defined below. 282 Definition 7. Context Vector Given an operation O, its context C(O) can be represented by the following context vector CV (O): CV (O) = [ (ns0, ic0), (ns1, ic1), . . . , (nsN−1, icN−1) ], where, for 0 ≤ i ≤ N − 1,
at site i, and
represents all inverse operations for undoing normal operations generated at site i, where (nsj, isj), 0 ≤ j < k, represents an inverse group with isj inverses related to the normal operation with sequence number nsj. 2 In the absence of inverse operations in the operation context, all ici, 0 ≤ i ≤ N − 1, would be empty and a Context Vector would be reduced to a State Vector [4].
The vector representation of operation context can also be used as the vector representation of the document state. As an example, consider the document state after interpreting the undo command Undo(O2) in Figure 1. Since Undo(O2) is interpreted as an inverse O2 (see Section 4.2), the document state after executing (the transformed) O2 shall be DS = {O1, O2, O3, O2}. This document state cannot be represented by a state vector but can be represented as a context vector as follows: CV (DS) = [(1, [ ]), (2, [(1, 1)]].
Based on Definition 7, it is straightforward to derive the scheme for maintaining the vector representation for the document state after executing each operation (according to Definition 3). Moreover, the vector representation of operation context can also be used to efficiently detect contextdependency/-independency relations. Due to space limitation, these technical details are omitted in this paper.
In the basic COT algorithm, we assume each site maintains a document state DS, which contains the set of original operations executed so-far. This is different from the log or the History Buffer (HB) schemes in prior OT algorithms [4, 22, 23], which record a list of transformed operations. We deliberately leave the internal data structure of DS unspecified to keep the COT algorithm independent of the operation buffering strategy.
In algorithm description, we shall use the context set representation C(O), rather than the context vector representation CV (O). When an operation O is propagated from the local site to remote sites, however, it is the context vector, not the context set, that is actually piggy-backed on O for propagation. The set of operations in C(O) can be easily determined from DS based on the information in CV (O).
The COT algorithm has two parts: the COT-DO part for supporting consistency maintenance (do), and the COTUNDO part for supporting undo. Both parts share the same core context-based transformation procedure.
Operation context and context-based conditions are central to the whole COT algorithm.
COT-DO takes two parameters: O - an original operation to be executed, and DS - the current document state representation. COT-DO is invoked only if C(O) ⊆ DS (CC1), which ensures that all operations included in the context of O have already been executed on DS.
Algorithm 1. COT-DO(O, DS)
Procedure 1. transform(O, CD) Repeat until CD = { }:
COT-DO first invokes procedure transform() to transform O against operations in DS − C(O) (CC2). This is to upgrade the context of O to DS. In Step 2, it must be that C(O) = DS (CC3), so O is executed as-is, and the original of O is added to DS (according to Definition 3-Item 2).
The heart of COT-DO is transform(O, CD), whose task is to transform O against operations in CD, which represents the context difference between C(O) and a new context on which O is to be defined. This procedure repeats the following three steps until CD becomes empty:
C(O) (CC4). An operation Ox meeting this condition can be determined if all operations in CD are sorted in the order of their execution and sequentially retrieved.
transform Ox against operations in C(O)−C(Ox) (CC5).
This is to upgrade Ox to the context of O, so that they can be used for IT transformation in the next step.
C(O) = C(Ox) (CC6), so O is IT-transformed against Ox, and the context of O is updated by adding the original of Ox (according to Definition 4-Item 3).
To show how COT-DO works, we examine how it resolves the dOPT-puzzle in Figure 1. Consider the operation executions at site 0, with the initial document state DS0 = { }.
executed as-is and DS0 is updated to DS1 = {O1}.
C(O2)) is called, where DS1 − C(O2) = {O1}.
Inside transform(O2, {O1}), since C(O1) = C(O2), we have O2 := IT(O2, O1), and C(O2) = {O1}.
Returning from transform(O2, {O1}), we have C(O2) = DS1, so O2 is executed, and DS1 is updated to DS2 = {O1, O2}, where O2 = org(O2).
C(O3)) is called, where DS2 − C(O3) = {O1}.
Inside transform(O3, {O1}), transform(O1, C(O3)−C(O1)) is recursively called, with C(O3) − C(O1) = {O2}, which is the key step in detecting the dOPT-puzzle.
In the recursive transform(O1, {O2}), since C(O2) = C(O1), we have O1 := IT(O1, O2), and C(O1) = {O2}.
Returning from the recursion, we have C(O1) = C(O3), so C(O3) := IT(O3, O1) (the dOPT-puzzle resolved here), and C(O3) = {O1, O2}, where O1 = org(O1).
After returning from transform(O3, {O1}), C(O3) = DS2; so O3 is executed, and DS2 is updated to DS3 = {O1, O2, O3}, where O3 = org(O3). 283
To undo an operation O, a meta-level undo command Undo(O) must be issued by a user. How to generate the undo command for selecting any operation to undo is part of the undo policy [21]. This paper is confined to the discussion of the undo mechanism, which determines how to undo the selected operation in a given context.
In COT-UNDO, Undo(O) is interpreted as an inverse O, that is context-dependent on operations in C(O) and O itself.
COT-UNDO takes two input parameters: O is the operation selected to be undone, which can be any operation done sofar, and DS is the current document state representation.
Algorithm 2. COT-UNDO(O, DS)
COT-UNDO works by first creating an inverse O by invoking makeInverse(O)2 , with its context C(O) := C(O) ∪ {O} (according to Definition 4-Item 2), and then invoking COTDO to handle O.
For example, to interpret Undo(O2) in Figure 1,
COTUNDO is invoked with parameters O2 and DS = {O1, O2, O3}.
First, O2 and C(O2) = {O2} are created. Then, COT-DO is invoked with parameters O2 and DS. Inside COT-DO, transform(O2, DS − C(O2)) shall be invoked, and O2 shall be correctly transformed against O1 and O3 since CD = DS − C(O2) = {O1, O3}. This example shows that an inverse operation can be handled by COT-DO in the same way as other normal operations. This is because context-based conditions CC1 - CC6 are uniformly applicable to both normal and inverse operations.
The basic COT algorithm is simple yet powerful - capable of doing and undoing any operations at anytime. Among all prior OT systems, only the combination of GOTO and ANYUNDO (referred as GOTO-ANYUNDO) has similar capabilities [22, 21].
COT is a high-level control algorithm responsible for determining which operation should be transformed against other operations and in which order according to contextbased conditions. Another important component of an OT system is the low-level transformation functions responsible for transforming operations according to their types and parameters. Past research has identified a range of transformation properties/conditions that must be maintained for ensuring the correctness of an OT system. Different OT systems may have different control algorithms, different transformation functions, and different divisions of responsibilities among these components.
Unlike GOTO-ANYUNDO, the basic COT algorithm does not use ET (Exclusion Transformation) functions [21], thus avoiding the requirement of the Reversibility Property (RP) between IT and ET functions [21].
Similar to GOTO-ANYUNDO, the basic COT algorithm assumes that underlying transformation functions are capable of preserving the following properties [4, 15, 19, 23, 21]: 2 The reader is referred to [25] for precise definitions of three primitive operations Insert, Delete and Update and their corresponding inverses. The makeInverse(O) procedure directly follows these definitions.
. Given a document state DS, and operations Oa, Ob, if Oa = IT(Oa, Ob), and Ob = IT(Ob, Oa), then it must be: DS ◦ [Oa, Ob] = DS ◦ [Ob, Oa], which means that [Oa, Ob] and [Ob, Oa] are equivalent with respect to the effect on the document state DS.
operations O, Oa and Ob, if Oa = IT(Oa, Ob) and Ob = IT(Ob, Oa), then it must be: IT(IT(O, Oa), Ob) = IT(IT(O, Ob), Oa), which means that [Oa, Ob] and [Ob, Oa] are equivalent with respect to the effect in transformation.
. Given any operation Ox and a pair of operations [O, O], it must be: IT(IT(Ox, O), O) = IT(Ox, I) = Ox, which means that [O, O] and I are equivalent with respect to the effect in transformation.
and Ob, if Oa := IT(Oa, Ob), Ob := IT(Ob, Oa), and Oa := IT(Oa, Ob), then it must be: Oa = Oa, which means the transformed inverse operation Oa is equal to the inverse of the transformed operation Oa.
The above transformation properties are important discoveries of past research, but they are not unconditionally required. The pre-conditions for requiring them, however, were never explicitly stated in their specifications, which has unfortunately caused quite some misconceptions in OT literature. To explore alternative solutions to these properties, we explicitly state the Pre-Conditions (PC) for CP1, CP2,
IP2, and IP3 as follows:
allows the same group of context-independent operations to be executed in different orders.
allows an operation to be transformed against the same group of context-independent operations in different orders.
an operation Ox to be transformed against a pair of do and undo operations (O and O) one-by-one.
an inverse operation Oa to be transformed against another operation Ob that is context-independent of Oa. 3 Convergence Property 1 & 2 in this paper (and in [21]) are the same as Transformation Property 1 & 2 in [19]. 4 There is another Inverse Property 1 (IP1) that is required in an OT system for achieving the correct undo effect [21], but IP1 is not related to IT functions. 284 There are generally two ways to achieve OT correctness with respect to these transformation properties: one is to design transformation functions capable of preserving these properties; the other is to design control algorithms capable of breaking the pre-conditions for requiring these properties.
Past research has shown that it is relatively easy to design transformation functions capable of preserving CP1, but non-trivial to design and formally prove transformation functions capable of preserving CP2, IP2 and IP3.
Counterexamples illustrating the violation of these properties in some early published transformation functions can be found in [23, 21, 8, 11]. IT functions capable of preserving IP2 and IP3 had been devised in the context of ANYUNDO [21], but our experience in implementing these functions revealed that those solutions are quick intricate and inefficient (more analysis can be found in Section 7).
Clearly, solving CP2, IP2 and IP3 at the control algorithm level has the benefit of simplifying the design of transformation functions and the OT system as a whole. In the following section, we extend the basic COT algorithm to provide simple and efficient solutions to CP2, IP2 and IP3 at the control algorithm level.
A distinctive feature of COT is that in every transformation process (i.e. an invocation of transform(O, CD)), the whole set of transformation target operations are determined in advance, and available in the context-difference parameter CD (calculated by using context-based conditions CC2 and CC5). With the knowledge of all operations involved in the transformation process, we are able to properly arrange these operations to break the pre-conditions for CP2, IP2, and IP3.
We extend the core procedure transform(O, CD) to take advantage of the global knowledge of operations in the contextdifference parameter CD for breaking PC-CP2, PC-IP2 and PC-IP3. The extended transform(), as shown in Procedure 2, retains the structure and main elements of Procedure 1, but adds solutions to CP2, IP2, and IP3 in Step 1 (ensure TPsafety()) and in Step 2-(c) (the if-then part).
Procedure 2. transform(O, CD)
(a) Remove the first operation Ox from CD; (b) transform(Ox, C(O) − C(Ox)); (c) If Ox is a do-undo-pair, then C(O) := C(O) ∪ {org(Ox), org(Ox)}; else O := IT(O, Ox); C(O) := C(O) ∪ {org(Ox)}.
Procedure 3. ensure TPsafety(O, CD)
order that respects their context-dependency order.
then mark Ox as a do-undo-pair, remove Ox from CD.
make IP3safe Inverse(O, CD).
Procedure 4. make IP3safe Inverse(O, CD)
c O};
The COT solution to CP2 is to sort all operations in CD in a total order which respects their context-dependency order (in Step 1 of ensure TPsafety()). If an operation O is transformed against the same group of context-independent operations in multiple invocations to transform(O, CD), this group of operations must be included in CD and sorted in the same total order. Therefore, O can never be transformed against the same group of operations in different orders, thus breaking PC-CP2.
It should be noted that CD becomes an ordered set after the sorting. The first Ox in CD must meet the condition C(Ox) ⊆ C(O) in Step 2(a) of transform(O, CD) (Procedure 1), so this condition is no longer explicitly specified in Procedure 2. A correct total order for breaking PCCP2 can be conveniently determined by using the contextdependency relations among all operations plus the site identifiers of context-independent operations.
There have been several prior OT systems capable of breaking PC-CP2, including the GOT system (by an undo/redo scheme based on total ordering) [23], the SOCT4 system (by a control strategy based on global sequencing) [26], the NICE system (by a central transformation-based notifier) [20], and the TIBOT system (by a distributed synchronization protocol based on time-internal) [12]. The COT solution to CP2 is unique and avoids the use of any undo/redo or global sequencing/synchronization.
The basic idea of the COT solution to IP2 is to make sure that an operation is never transformed against a pair of do and undo operations one by one, thus breaking PCIP2. This solution consists of two parts: (1) Step 2 of ensure TPsafety(CD) couples operations with their corresponding inverses if they are all included in the context difference CD, and remove these inverses from CD; (2) In Step 2-(c) of transform(), if Ox is found to be a do-undo-pair, the IT-transformation of O against Ox is skipped (effectively treating this pair as an identity operation) and the context of O is updated by adding two operations: {org(Ox), org(Ox)}.
The COT solution to IP3 is encapsulated in the procedure make IP3safe Inverse(O, CD), which makes O an IP3-safe inverse with respect to the context difference CD. An inverse O is IP3-safe with respect to CD if it is made from a transformed version of O, which has included all operations in CD that are context-independent of O. Under the control of COT, the IP3-safe inverse O shall never be transformed against operations that are context-independent of O, thus breaking PC-IP3.
The make IP3safe Inverse procedure works as follows: (1) create operation O (the inverse of O) and C(O) = C(O) − 285 {O}; (2) select all operations from CD which are contextindependent of O and create a new context difference NCD; (3) transform O against operations in NCD (by recursively invoking transform()); (4) create a new inverse from the transformed O; and (5) create a new CD by subtracting NCD from the old CD (the new CD must maintain the total order as required for solving CP2). This new inverse O must be IP3-safe because it is created from a transformed operation whose context has included all operations in NCD. The IP3-safe inverse O shall never be transformed against the operations in NCD since these operations have been removed from the new CD in Step (5).
The notion of operation context was first proposed in the GOT algorithm [23] and used in conjunction with the theory of causality in follow-up GOTO and ANYUNDO algorithms [22, 21]. In prior work, the context of an operation O was defined as a sequence of transformed operations which can be executed to bring the document from its initial state to the state on which O is defined. This definition is directly coupled to the sequential history buffering strategy, which saves executed operations in their execution forms and orders. There was no explicit representation of an operation context. Context relationships among operations are derived from the causality relationships plus the history buffer position relationships among operations [23, 21].
In this paper, the concept of operation context is defined as a set of original operations corresponding to the document state on which this operation is defined. This new concept of operation context is independent of the underlying operation buffering strategy and is explicitly represented as an operation set. Based on the set representation of operation context, essential OT conditions (CC1 - CC6) have been precisely and concisely captured. Moreover, the context vector has been devised to efficiently represent both normal and inverse operations in a context. The context vector is more general than the state vector and potentially applicable to other distributed computing systems as well.
Based on the theory of causality, prior OT algorithms have used state vectors to capture causal-dependency relationships among original normal operations and to represent document states in terms of original normal operations. However, causal-dependency relationships are not defined for inverse or transformed operations, and state vectors cannot represent document states with original inverse operations. The theory of causality is unable to capture essential OT conditions (CC1 - CC6) for all types of operation - original and transformed, normal and inverse operations.
Both COT and GOTO-ANYUNDO are capable of doing and undoing any operations at anytime. The main difference is that COT achieves this capability without using ET functions (thus eliminating the RP requirement for IT functions), and without requiring IT functions to preserve CP2,
IP2 and IP3. The avoidance of RP, CP2, IP2, and IP3 has significantly simplified the design of transformation functions and the OT system as a whole.
COT is simpler than GOTO-ANYUNDO (and prior OT algorithms based on the causality theory) because of the use of a single theory of operation context for capturing all OTrelated conditions (CC1-CC6), the uniformity of contextbased conditions for treating all types of operation, and the conciseness of these context-based conditions.
The COT-based system is more efficient than the GOTOANYUNDO-based system in solving IP2 and IP3. In GOTOANYUNDO, the do-part (a normal operation) and the undopart (an inverse operation) need to be coupled for the purpose of preserving IP2 [21]. An eager coupling strategy was adopted: an inverse operation is coupled with its corresponding normal operation immediately after its execution.
Under this scheme, inverse operations are not explicitly represented in the history buffer. When a normal operation is to be executed, however, it may need to be transformed against only the undo-part of a do-undo-pair. To cope with this problem, an extra DeCouple-GOTO-ReCouple scheme has to be used to decouple a do-undo-pair before invoking GOTO and then recouple them afterwards [21].
However, the implementation of this decouple-recouple scheme revealed it was rather intricate and causing many repeated transformations.
In the COT algorithm, COT-DO and COT-UNDO are seamlessly integrated. Inverse operations are explicitly represented in the operation context, and a lazy coupling strategy is adopted: the coupling of a do-undo-pair occurs not immediately after executing each inverse, but only when both the do-part and the undo-part appear in the same transformation process at some late stage. These strategies help to avoid overhead transformations caused by the eager coupling scheme and the decouple-recouple scheme.
In the GOTO-ANYUNDO-based system, the solution to IP3 is encapsulated in an IP3-preserving IT function, called IP3P-IT [21]. Inside this function, an extended ET function has to be used, which may invoke the expensive GOTO algorithm to ensure RP with the corresponding IT function.
In contrast, the COT solution to IP3 is encapsulated in the high-level procedure make IP3safe Inverse(O, CD), which is more efficient since (1) it avoids converting O to O back and forth multiple times for each Ox ∈ NCD (if IP3P-IT(O, Ox) were used instead); and (2) the transform() procedure is much cheaper than GOTO.
Another distinctive feature of the COT algorithm is the separation of the algorithm from the underlying operation buffering strategy. This has not only resulted in a cleaner and simpler logical structure to the algorithm itself, but also allowed a range of performance optimizations at the operation buffering level.
We have devised and implemented a buffering structure in which not only original operations but also transformed versions can be saved; and all transformed operations from the same original operation are organized in the same version group. When an original operation is required at the COT algorithm level, the corresponding version group is searched for a version that matches the context requirement. If such a version already exists, it is used to represent the original operation in the transformation process, thus saving the overhead to transform the original operation into this version. Under this buffering structure, various heuristics can be used to selectively save transformed versions to maximize their reuse and minimize their space usage. By experimentation, we have identified some useful heuristics that are 286 effective in saving transformations for a number of common patterns of operation sequence.
COT is not the first OT algorithm that buffers and uses original operations for transformation. Several prior OT algorithms, including CCU [2], adOPTed [19], and GOTOANYUNDO [21], have also buffered original operations. COT is unique in its way of buffering and using original, as well as transformed, operations.
OT correctness is a central topic of discussion in OT research. In this section, we provide our observations and opinions on some important OT correctness issues.
OT is a complex system with multiple interrelated components. A system-oriented approach is needed for addressing OT issues. An experimental method, called puzzle-detectionresolution, has commonly been used in exploring and refining OT solutions. Puzzles are subtle but representative scenarios in which certain OT properties/conditions may be violated and the system may produce incorrect results. The ability to solve all known puzzles is a necessary condition and an important indicator of the soundness of an OT system. In research literature, simple puzzle scenarios are often used to illustrate the key reasons why an OT system works or fails. In real OT system design, however, a real implementation and comprehensive testing cases based on complex puzzle scenarios are crucial in validating a design.
Theoretical methods have also been used to formally verify OT correctness with respect to some identified transformation properties/conditions. Formal verification can be effective if the correctness issues have been well-understood and the verification criteria and boundary conditions have been well-defined. In this regard, experimental methods like puzzle-detection-resolution can play an important role in gaining the necessary insights into the real correctness issues, and establishing suitable criteria and conditions for formal verification.
A systematic approach is needed in conducting both experimental and theoretic OT research. Many OT components and issues are intimately related, and a solution to one issue, if examined in isolation, is unlikely to be correct or complete. For example, a solution that works well for consistency maintenance (do), may fail when both do and undo problems are considered; and an undo solution (e.g. preserving IP2) may violate the solution to consistency maintenance [21]. A complete OT solution to both do and undo problems is significantly more difficult to design than a partial solution to only one of them.
On the other hand, a difficult issue in one OT component may be resolved easily, or avoided altogether, if this issue is addressed from a different OT component. For example, it is known that devising and proving transformation functions capable of preserving properties CP2, IP2, and IP3 are difficult. However, these difficulties can be avoided by devising control algorithms (like COT) capable of breaking the pre-conditions for requiring these properties; it is also easier to prove a control algorithm is capable of breaking the pre-conditions for these properties, than to prove transformation functions are capable of preserving them.
Different OT systems may have different divisions of responsibility among their components and hence different correctness requirements for these components. Caution must be taken in interpreting correctness results. For example, CP1 and CP2 were proven to be necessary and sufficient for adOPTed-based systems to converge [19, 13], but this result cannot be generalized to all OT systems. In fact,
CP1 and CP2 are neither sufficient nor necessary for many OT systems. They are insufficient because an OT system may need to preserve additional properties/conditions, such as IP2, IP3, and those summarized in [21]. They are unnecessary if the pre-conditions for requiring them have been broken. For example, neither CP1 nor CP2 is required in the REDUCE system based on the GOT algorithm for ensuring convergence [23]. CP2 is also not required by OT systems based on COT or some prior OT algorithms [26, 20, 12].
One OT correctness issue, which is often discussed in relation to the CP2-violation problem, is the false-tie problem: when two (or more) insert operations with the same position are IT-transformed with each other, the position tie may be false if it was not original but caused by previous transformations. An OT system may fail to produce correct results if the normal tie-breaking rule (e.g. based on site identifiers) is used to break false-ties. This problem was long discovered in early OT work and a concrete scenario related to this problem was illustrated in Fig. 6 of [23]. It is beyond the scope of this paper to discuss solutions to this problem, but it is worth pointing out that the false-tie problem is different from the CP2-violation problem: a false-tie may occur without violating CP2. In our view, the false-tie problem is an issue at the transformation function level and its solution could and should be localized at this level as well. For alternative views and approaches to this problem, the reader is referred to [8, 11, 5].
The COT algorithm has been implemented and validated by a comprehensive testing suite covering all known OT puzzle scenarios. In this paper, informal analysis and simple puzzle scenarios have been used to show the correctness of COT with respect to various transformation properties/conditions. Formal verification of COT correctness with respect to these properties/conditions, and quantitative analysis of the time and space complexity of COT, shall be reported in a journal version of this paper.
We have contributed the theory of operation context and the COT (Context-based OT) algorithm. The theory of operation context is capable of capturing essential relationships and conditions for all types of operation in an OT system; it provides a new foundation for better understanding and resolving OT problems. The COT algorithm provides uniformed solutions to both consistency maintenance and undo problems; it is simpler and more efficient than prior OT control algorithms with similar capabilities; and it significantly simplifies the design of transformation functions. The COT algorithm has been implemented in a generic collaboration engine and used for supporting a range of novel collaborative applications [24].
Real-world applications provide exciting opportunities and challenges to future OT research. The theory of operation context and the COT algorithm shall serve as new foundations for addressing the technical challenges in existing and emerging OT applications.
Acknowledgments The authors are grateful to Bo Begole and anonymous reviewers for their valuable comments and suggestions which have helped improve the presentation of the paper. 287
[1] J. Begole, M. Rosson, and C. Shaffer. Flexible collaboration transparency: supporting worker independence in replicated application-sharing systems. ACM Trans. on Computer-Human Interaction, 6(2):95-132, 1999. [2] G. Cormack. A calculus for concurrent update. In Research Report CS-95-06, Dept. of Computer Science, University of Waterloo, Canada, 1995. [3] A. Davis, C. Sun, and J. Lu. Generalizing operational transformation to the standard general markup language. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 58 - 67,
Nov. 2002. [4] C. A. Ellis and S. J. Gibbs. Concurrency control in groupware systems. In Proc. of the ACM Conf. on Management of Data, pages 399-407, May 1989. [5] N. Gu, J. Yang, and Q.Zhang. Consistency maintenance based on the mark & retrace technique in groupware systems. In Proc. of ACM Conf. on Supporting Group Work, pages 264-273, Nov. 2005. [6] R. Guerraoui and Corine Hari. On the consistency problem in mobile distributed computing. In Proceedings of the Second ACM International Workshop on Principles of Mobile Computing, pages 51-57, New York, Octo 2002. ACM. [7] C. Ignat and M.C. Norrie. Customizable collaborative editor relying on treeOPT algorithm. In Proc. of the European Conf. of Computer-supported Cooperative Work, pages 315-324, Sept. 2003. [8] A. Imine, P. Molli, G. Oster, and M. Rusinowitch.
Proving correctness of transformation functions in real-time groupware. In Proc. of the European Conf. on Computer-Supported Cooperative Work, Sept. 2003. [9] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communication of ACM, 21(7):558-565, 1978. [10] D. Li and R. Li. Transparent sharing and interoperation of heterogeneous single-user applications. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 246-255, Nov. 2002. [11] D. Li and R. Li. Preserving operation effects relation in group editors. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 457-466, Nov. 2004. [12] R. Li, D. Li, and C. Sun. A time interval based consistency control algorithm for interactive groupware applications. In Proc. of International Conference on Parallel and Distributed Systems, pages 429-436, July. 2004. [13] B. Lushman and G. Cormack. Proof of correctness of Ressels adOPTed algorithm. Information Processing Letters, (86):303-310, 2003. [14] C. Palmer and G. Cormack. Operation transforms for a distributed shared spreadsheet. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 69-78, Nov. 1998. [15] A. Prakash and M. Knister. A framework for undoing actions in collaborative systems. ACM Trans. on Computer-Human Interaction, 4(1):295-330, Dec.
[16] N. Preguica, M. Shapiro, and J. Legatheaux Martins.
Automating semantics-based reconciliation for mobile databases. In Proceedings of the 3th Conference Francaise sur les Systems d"Exploitation, Octo 2003. [17] M. Raynal and M. Singhal. Logical time: capturing causality in distributed systems. IEEE Computer Magazine, 29(2):49-56, Feb. 1996. [18] M. Ressel and R. Gunzenh¨auser. Reducing the problems of group undo. In Proc. of the ACM Conf. on Supporting Group Work, pages 131-139, Nov. 1999. [19] M. Ressel, D. Nitsche-Ruhland, and R. Gunzenh¨auser.
An integrating, transformation-oriented approach to concurrency control and undo in group editors. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 288-297, Nov. 1996. [20] H.F. Shen and C. Sun. A flexible notification framework for collaborative systems. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 77-86, Nov. 2002. [21] C. Sun. Undo as concurrent inverse in group editors.
ACM Trans. on Computer-Human Interaction, 9(4):309-361, December 2002. [22] C. Sun and C. A. Ellis. Operational transformation in real-time group editors: issues, algorithms, and achievements. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 59-68,
Nov. 1998. [23] C. Sun, X. Jia, Y. Zhang, Y. Yang, and D. Chen.
Achieving convergence, causality-preservation, and intention-preservation in real-time cooperative editing systems. ACM Trans. on Computer-Human Interaction, 5(1):63-108, March 1998. [24] C. Sun, Q. Xia, D. Sun, D. Chen, H.F. Shen, and W. Cai. Transparent adaptation of single-user applications for multi-user real-time collaboration.
ACM Trans. on Computer-Human Interaction, 2006. [25] D. Sun, S. Xia, C. Sun, and D. Chen. Operational transformation for collaborative word processing. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 437-446, Nov. 2004. [26] N. Vidot, M. Cart, J. Ferri´e, and M. Suleiman. Copies convergence in a distributed real-time collaborative environment. In Proc. of the ACM Conf. on Computer-Supported Cooperative Work, pages 171-180, Dec. 2000. [27] S. Xia, D. Sun, C. Sun, and D. Chen. A collaborative table editing technique based on transparent adaptation. In Proc. of the International Conf. on Cooperative Information Systems, LNCS Vol. 3760,
Springer Verlag, pages 576-592, Nov. 2005. [28] S. Xia, D. Sun, C. Sun, and D. Chen.
Object-associated telepointer for real-time collaborative document editing systems. In Proc. of the IEEE Conf. on Collaborative Computing: Networking, Applications and Worksharing, Dec. 2005. [29] S. Xia, D. Sun, C. Sun, D. Chen, and H.F. Shen.

An increasing fraction of web content is dynamically generated from back-end relational databases. Even when database content remains unchanged, temporal locality of access cannot be exploited because dynamic content is not cacheable by web browsers or by intermediate caching servers such as Akamai mirrors. In a multitiered architecture, each web request can stress the WAN link between the web server and the database. This causes user experience to be highly variable because there is no caching to insulate the client from bursty loads. Previous attempts in caching dynamic database content have generally weakened transactional semantics [3, 4] or required application modifications [15, 34].
We report on a new solution that takes the form of a databaseagnostic middleware layer called Ganesh. Ganesh makes no effort to semantically interpret the contents of queries or their results.
Instead, it relies exclusively on cryptographic hashing to detect similarities with previous results. Hash-based similarity detection has seen increasing use in distributed file systems [26, 36, 37] for improving performance on low-bandwidth networks. However, these techniques have not been used for relational databases. Unlike previous approaches that use generic methods to detect similarity,
Ganesh exploits the structure of relational database results to yield superior performance improvement.
One faces at least three challenges in applying hash-based similarity detection to back-end databases. First, previous work in this space has traditionally viewed storage content as uninterpreted bags of bits with no internal structure. This allows hash-based techniques to operate on long, contiguous runs of data for maximum effectiveness. In contrast, relational databases have rich internal structure that may not be as amenable to hash-based similarity detection. Second, relational databases have very tight integrity and consistency constraints that must not be compromised by the use of hash-based techniques. Third, the source code of commercial databases is typically not available. This is in contrast to previous work which presumed availability of source code.
Our experiments show that Ganesh, while conceptually simple, can improve performance significantly at bandwidths representative of today"s commercial Internet. On benchmarks modeling multitiered web applications, the throughput improvement was as high as tenfold for data-intensive workloads. For workloads that were not data-intensive, throughput improvements of up to twofold were observed. Even when bandwidth was not a constraint, Ganesh had low overhead and did not hurt performance. Our experiments also confirm that exploiting the structure present in database results is crucial to this performance improvement.
As the World Wide Web has grown, many web sites have decentralized their data and functionality by pushing them to the edges of the Internet. Today, eBusiness systems often use a three-tiered architecture consisting of a front-end web server, an application server, and a back-end database server. Figure 1 illustrates this architecture. The first two tiers can be replicated close to a concentration of clients at the edge of the Internet. This improves user experience by lowering end-to-end latency and reducing exposure WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 311 Back-End Database Server Front-End Web and Application Servers Figure 1: Multi-Tier Architecture to backbone traffic congestion. It can also increase the availability and scalability of web services.
Content that is generated dynamically from the back-end database cannot be cached in the first two tiers. While databases can be easily replicated in a LAN, this is infeasible in a WAN because of the difficult task of simultaneously providing strong consistency, availability, and tolerance to network partitions [7]. As a result, databases tend to be centralized to meet the strong consistency requirements of many eBusiness applications such as banking, finance, and online retailing [38]. Thus, the back-end database is usually located far from many sets of first and second-tier nodes [2].
In the absence of both caching and replication, WAN bandwidth can easily become a limiting factor in the performance and scalability of data-intensive applications.
Ganesh"s focus is on efficient transmission of results by discovering similarities with the results of previous queries. As SQL queries can generate large results, hash-based techniques lend themselves well to the problem of efficiently transferring these large results across bandwidth constrained links.
The use of hash-based techniques to reduce the volume of data transmitted has emerged as a common theme of many recent storage systems, as discussed in Section 8.2. These techniques rely on some basic assumptions. Cryptographic hash functions are assumed to be collision-resistant. In other words, it is computationally intractable to find two inputs that hash to the same output. The functions are also assumed to be one-way; that is, finding an input that results in a specific output is computationally infeasible.
Menezes et al. [23] provide more details about these assumptions.
The above assumptions allow hash-based systems to assume that collisions do not occur. Hence, they are able to treat the hash of a data item as its unique identifier. A collection of data items effectively becomes content-addressable, allowing a small hash to serve as a codeword for a much larger data item in permanent storage or network transmission.
The assumption that collisions are so rare as to be effectively non-existent has recently come under fire [17]. However, as explained by Black [5], we believe that these issues do not form a concern for Ganesh. All communication is between trusted parts of the system and an adversary has no way to force Ganesh to accept invalid data. Further, Ganesh does not depend critically on any specific hash function. While we currently use SHA-1, replacing it with a different hash function would be simple. There would be no impact on performance as stronger hash functions (e.g.
SHA256) only add a few extra bytes and the generated hashes are still orders of magnitude smaller than the data items they represent. No re-hashing of permanent storage is required since Ganesh only uses hashing on volatile data.
Ganesh exploits redundancy in the result stream to avoid transmitting result fragments that are already present at the query site.
Redundancy can arise naturally in many different ways. For example, a query repeated after a certain interval may return a different result because of updates to the database; however, there may be significant commonality between the two results. As another example, a user who is refining a search may generate a sequence of queries with overlapping results. When Ganesh detects redundancy, it suppresses transmission of the corresponding result fragments. Instead, it transmits a much smaller digest of those fragments and lets the query site reconstruct the result through hash lookup in a cache of previous results. In effect, Ganesh uses computation at the edges to reduce Internet communication.
Our description of Ganesh focuses on four aspects. We first explain our approach to detecting similarity in query results. Next, we discuss how the Ganesh architecture is completely invisible to all components of a multi-tier system. We then describe Ganesh"s proxy-based approach and the dataflow for detecting similarity.
One of the key design decisions in Ganesh is how similarity is detected. There are many potential ways to decompose a result into fragments. The optimal way is, of course, the one that results in the smallest possible object for transmission for a given query"s results.
Finding this optimal decomposition is a difficult problem because of the large space of possibilities and because the optimal choice depends on many factors such as the contents of the query"s result, the history of recent results, and the cache management algorithm.
When an object is opaque, the use of Rabin fingerprints [8, 30] to detect common data between two objects has been successfully shown in the past by systems such as LBFS [26] and CASPER [37].
Rabin fingerprinting uses a sliding window over the data to compute a rolling hash. Assuming that the hash function is uniformly distributed, a chunk boundary is defined whenever the lower order bits of the hash value equal some predetermined value. The number of lower order bits used defines the average chunk size. These sub-divided chunks of the object become the unit of comparison for detecting similarity between different objects.
As the locations of boundaries found by using Rabin fingerprints is stochastically determined, they usually fail to align with any structural properties of the underlying data. The algorithm therefore deals well with in-place updates, insertions and deletions.
However, it performs poorly in the presence of any reordering of data.
Figure 2 shows an example where two results, A and B, consisting of three rows, have the same data but have different sort attributes. In the extreme case, Rabin fingerprinting might be unable to find any similar data due to the way it detects chunk boundaries.
Fortunately, Ganesh can use domain specific knowledge for more precise boundary detection. The information we exploit is that a query"s result reflects the structure of a relational database where all data is organized as tables and rows. It is therefore simple to check for similarity with previous results at two granularities: first the entire result, and then individual rows. The end of a row in a result serves as a natural chunk boundary. It is important to note that using the tabular structure in results only involves shallow interpretation of the data. Ganesh does not perform any deeper semantic interpretation such as understanding data types, result schema, or integrity constraints.
Tuning Rabin fingerprinting for a workload can also be difficult.
If the average chunk size is too large, chunks can span multiple result rows. However, selecting a smaller average chunk size increases the amount of metadata required to the describe the results.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 312 Figure 2: Rabin Fingerprinting vs. Ganesh"s Chunking This, in turn, would decrease the savings obtained via its use.
Rabin fingerprinting also needs two computationally-expensive passes over the data: once to determine chunk boundaries and one again to generate cryptographic hashes for the chunks. Ganesh only needs a single pass for hash generation as the chunk boundaries are provided by the data"s natural structure.
The performance comparison in Section 6 shows that Ganesh"s row-based algorithm outperforms Rabin fingerprinting. Given that previous work has already shown that Rabin fingerprinting performs better than gzip [26], we do not compare Ganesh to compression algorithms in this paper.
The key factor influencing our design was the need for Ganesh to be completely transparent to all components of a typical eBusiness system: web servers, application servers, and database servers.
Without this, Ganesh stands little chance of having a significant real-world impact. Requiring modifications to any of the above components would raise the barrier for entry of Ganesh into an existing system, and thus reduce its chances of adoption. Preserving transparency is simplified by the fact that Ganesh is purely a performance enhancement, not a functionality or usability enhancement.
We chose agent interposition as the architectural approach to realizing our goal. This approach relies on the existence of a compact programming interface that is already widely used by target software. It also relies on a mechanism to easily add new code without disrupting existing module structure.
These conditions are easily met in our context because of the popularity of Java as the programming language for eBusiness systems. The Java Database Connectivity (JDBC) API [32] allows Java applications to access a wide variety of databases and even other tabular data repositories such as flat files. Access to these data sources is provided by JDBC drivers that translate between the JDBC API and the database communication mechanism.
Figure 3(a) shows how JDBC is typically used in an application.
As the JDBC interface is standardized, one can substitute one JDBC driver for another without application modifications. The JDBC driver thus becomes the natural module to exploit for code interposition. As shown in Figure 3(b), the native JDBC driver is replaced with a Ganesh JDBC driver that presents the same standardized interface. The Ganesh driver maintains an in-memory cache of result fragments from previous queries and performs reassembly of results. At the database, we add a new process called the Ganesh proxy. This proxy, which can be shared by multiple front-end nodes, consists of two parts: code to detect similarity in result fragments and the original native JDBC driver that communicates with the database. The use of a proxy at the database makes Ganesh database-agnostic and simplifies prototyping and experimentation. Ganesh is thus able to work with a wide range of databases and applications, requiring no modifications to either.
The native JDBC driver shown in Figure 3(a) is a lightweight code component supplied by the database vendor. Its main funcClient Database Web and Application Server Native JDBC Driver WAN (a) Native Architecture Client Database Ganesh Proxy Native JDBC Driver WAN Web and Application Server Ganesh JDBC Driver (b) Ganesh"s Interposition-based Architecture Figure 3: Native vs. Ganesh Architecture tion is to mediate communication between the application and the remote database. It forwards queries, buffers entire results, and responds to application requests to view parts of results.
The Ganesh JDBC driver shown in Figure 3(b) presents the application with an interface identical to that provided by the native driver. It provides the ability to reconstruct results from compact hash-based descriptions sent by the proxy. To perform this reconstruction, the driver maintains an in-memory cache of recentlyreceived results. This cache is only used as a source of result fragments in reconstructing results. No attempt is made by the Ganesh driver or proxy to track database updates. The lack of cache consistency does not hurt correctness as a description of the results is always fetched from the proxy - at worst, there will be no performance benefit from using Ganesh. Stale data will simply be paged out of the cache over time.
The Ganesh proxy accesses the database via the native JDBC driver, which remains unchanged between Figures 3(a) and (b).
The database is thus completely unaware of the existence of the proxy. The proxy does not examine any queries received from the Ganesh driver but passes them to the native driver. Instead, the proxy is responsible for inspecting database output received from the native driver, detecting similar results, and generating hash-based encodings of these results whenever enough similarity is found. While this architecture does not decrease the load on a database, as mentioned earlier in Section 2.1, it is much easier to replicate databases for scalability in a LAN than in a WAN.
To generate a hash-based encoding, the proxy must be aware of what result fragments are available in the Ganesh driver"s cache.
One approach is to be optimistic, and to assume that all result fragments are available. This will result in the smallest possible initial transmission of a result. However, in cases where there is little overlap with previous results, the Ganesh driver will have to make many calls to the proxy during reconstruction to fetch missing result fragments. To avoid this situation, the proxy loosely tracks the state of the Ganesh driver"s cache. Since both components are under our control, it is relatively simple to do this without resorting to gray-box techniques or explicit communication for maintaining cache coherence. Instead, the proxy simulates the Ganesh driver"s cache management algorithm and uses this to maintain a list of hashes for which the Ganesh driver is likely to possess the result fragments. In case of mistracking, there will be no loss of correctness but there will be extra round-trip delays to fetch the missing fragments. If the client detects loss of synchronization with the proxy, it can ask the proxy to reset the state shared between them.
Also note that the proxy does not need to keep the result fragments themselves, only their hashes. This allows the proxy to remain scalable even when it is shared by many front-end nodes.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 313 Object Output Stream Convert ResultSet Object Input Stream Convert ResultSet All Data Recipe ResultSet All Data ResultSet Network Ganesh Proxy Ganesh JDBC Driver Result Set Recipe Result Set Yes Yes No No GaneshInputStream GaneshOutputStream Figure 4: Dataflow for Result Handling
The Ganesh proxy receives database output as Java objects from the native JDBC driver. It examines this output to see if a Java object of type ResultSet is present. The JDBC interface uses this data type to store results of database queries. If a ResultSet object is found, it is shrunk as discussed below. All other Java objects are passed through unmodified.
As discussed in Section 3.1, the proxy uses the row boundaries defined in the ResultSet to partition it into fragments consisting of single result rows. All ResultSet objects are converted into objects of a new type called RecipeResultSet. We use the term recipe for this compact description of a database result because of its similarity to a file recipe in the CASPER file system [37]. The conversion replaces each result fragment that is likely to be present in the Ganesh driver"s cache by a SHA-1 hash of that fragment. Previously unseen result fragments are retained verbatim. The proxy also retains hashes for the new result fragments as they will be present in the driver"s cache in the future.
Note that the proxy only caches hashes for result fragments and does not cache recipes.
The proxy constructs a RecipeResultSet by checking for similarity at the entire result and then the row level. If the entire result is predicted to be present in the Ganesh driver"s cache, the RecipeResultSet is simply a single hash of the entire result.
Otherwise, it contains hashes for those rows predicted to be present in that cache; all other rows are retained verbatim. If the proxy estimates an overall space savings, it will transmit the RecipeResultSet. Otherwise the original ResultSet is transmitted.
The RecipeResultSet objects are transformed back into ResultSet objects by the Ganesh driver. Figure 4 illustrates ResultSet handling at both ends. Each SHA-1 hash found in a RecipeResultSet is looked up in the local cache of result fragments. On a hit, the hash is replaced by the corresponding fragment. On a miss, the driver contacts the Ganesh proxy to fetch the fragment. All previously unseen result fragments that were retained verbatim by the proxy are hashed and added to the result cache.
There should be very few misses if the proxy has accurately tracked the Ganesh driver"s cache state. A future optimization would be to batch the fetch of missing fragments. This would be valuable when there are many small missing fragments in a high-latency WAN. Once the transformation is complete, the fully reconstructed ResultSet object is passed up to the application.
Three questions follow from the goals and design of Ganesh: • First, can performance can be improved significantly by exploiting similarity across database results?
Benchmark Dataset Details 500,000 Users, 12,000 Stories BBOARD 2.0 GB 3,298,000 Comments AUCTION 1.3 GB 1,000,000 Users, 34,000 Items Table 1: Benchmark Dataset Details • Second, how important is Ganesh"s structural similarity detection relative to Rabin fingerprinting"s similarity detection? • Third, is the overhead of the proxy-based design acceptable?
Our evaluation answers these question through controlled experiments with the Ganesh prototype. This section describes the benchmarks used, our evaluation procedure, and the experimental setup.
Results of the experiments are presented in Sections 5, 6, and 7.
Our evaluation is based on two benchmarks [18] that have been widely used by other researchers to evaluate various aspects of multi-tier [27] and eBusiness architectures [9]. The first benchmark, BBOARD, is modeled after Slashdot, a technology-oriented news site. The second benchmark, AUCTION, is modeled after eBay, an online auction site. In both benchmarks, most content is dynamically generated from information stored in a database.
Details of the datasets used can be found in Table 1.
The BBOARD benchmark, also known as RUBBoS [18], models Slashdot, a popular technology-oriented web site. Slashdot aggregates links to news stories and other topics of interest found elsewhere on the web. The site also serves as a bulletin board by allowing users to comment on the posted stories in a threaded conversation form. It is not uncommon for a story to gather hundreds of comments in a matter of hours. The BBOARD benchmark is similar to the site and models the activities of a user, including readonly operations such as browsing the stories of the day, browsing story categories, and viewing comments as well as write operations such as new user registration, adding and moderating comments, and story submission.
The benchmark consists of three different phases: a short warmup phase, a runtime phase representing the main body of the workload, and a short cool-down phase. In this paper we only report results from the runtime phase. The warm-up phase is important in establishing dynamic system state, but measurements from that phase are not significant for our evaluation. The cool-down phase is solely for allowing the benchmark to shut down.
The warm-up, runtime, and cool-down phases are 2, 15, and 2 minutes respectively. The number of simulated clients were 400, 800, 1200, and 1600. The benchmark is available in a Java Servlets and PHP version and has different datasets; we evaluated Ganesh using the Java Servlets version and the Expanded dataset.
The BBOARD benchmark defines two different workloads. The first, the Authoring mix, consists of 70% read-only operations and 30% read-write operations. The second, the Browsing mix, contains only read-only operations and does not update the database.
The AUCTION benchmark, also known as RUBiS [18], models eBay, the online auction site. The eBay web site is used to buy and sell items via an auction format. The main activities of a user include browsing, selling, or bidding for items. Modeling the activities on this site, this benchmark includes read-only activities such as browsing items by category and by region, as well as read-write WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 314 NetEm Router Ganesh Proxy Clients Web and Application Server Database Server Figure 5: Experimental Setup activities such as bidding for items, buying and selling items, and leaving feedback.
As with BBOARD, the benchmark consists of three different phases.
The warm-up, runtime, and cool-down phases for this experiment are 1.5, 15, and 1 minutes respectively. We tested Ganesh with four client configurations where the number of test clients was set to 400, 800, 1200, and 1600. The benchmark is available in a Enterprise Java Bean (EJB), Java Servlets, and PHP version and has different datasets; we evaluated Ganesh with the Java Servlets version and the Expanded dataset.
The AUCTION benchmark defines two different workloads. The first, the Bidding mix, consists of 70% read-only operations and 30% read-write operations. The second, the Browsing mix, contains only read-only operations and does not update the database.
Both benchmarks involve a synthetic workload of clients accessing a web server. The number of clients emulated is an experimental parameter. Each emulated client runs an instance of the benchmark in its own thread, using a matrix to transition between different benchmark states. The matrix defines a stochastic model with probabilities of transitioning between the different states that represent typical user actions. An example transition is a user logging into the AUCTION system and then deciding on whether to post an item for sale or bid on active auctions. Each client also models user think time between requests. The think time is modeled as an exponential distribution with a mean of 7 seconds.
We evaluate Ganesh along two axes: number of clients and WAN bandwidth. Higher loads are especially useful in understanding Ganesh"s performance when the CPU or disk of the database server or proxy is the limiting factor. A previous study has shown that approximately 50% of the wide-area Internet bottlenecks observed had an available bandwidth under 10 Mb/s [1]. Based on this work, we focus our evaluation on the WAN bandwidth of 5 Mb/s with
network paths, and 20 Mb/s with 33 ms of round-trip latency, representative of a moderately constrained network path. We also report Ganesh"s performance at 100 Mb/s with no added round-trip latency. This bandwidth, representative of an unconstrained network, is especially useful in revealing any potential overhead of Ganesh in situations where WAN bandwidth is not the limiting factor. For each combination of number of clients and WAN bandwidth, we measured results from the two configurations listed below: • Native: This configuration corresponds to Figure 3(a).
Native avoids Ganesh"s overhead in using a proxy and performing Java object serialization. • Ganesh: This configuration corresponds to Figure 3(b). For a given number of clients and WAN bandwidth, comparing these results to the corresponding Native results gives the performance benefit due to the Ganesh middleware system.
The metric used to quantify the improvement in throughput is the number of client requests that can be serviced per second. The metric used to quantify Ganesh"s overhead is the average response time for a client request. For all of the experiments, the Ganesh driver used by the application server used a cache size of 100,000 items1 . The proxy was effective in tracking the Ganesh driver"s cache state; for all of our experiments the miss rate on the driver never exceeded 0.7%.
The experimental setup used for the benchmarks can be seen in Figure 5. All machines were 3.2 GHz Pentium 4s (with HyperThreading enabled.) With the exception of the database server, all machines had 2 GB of SDRAM and ran the Fedora Core Linux distribution. The database server had 4 GB of SDRAM.
We used Apache"s Tomcat as both the application server that hosted the Java Servlets and the web server. Both benchmarks used Java Servlets to generate the dynamic content. The database server used the open source MySQL database. For the native JDBC drivers, we used the Connector/J drivers provided by MySQL. The application server used Sun"s Java Virtual Machine as the runtime environment for the Java Servlets. The sysstat tool was used to monitor the CPU, network, disk, and memory utilization on all machines.
The machines were connected by a switched gigabit Ethernet network. As shown in Figure 5, the front-end web and application server was separated from the proxy and database server by a NetEm router [16]. This router allowed us to control the bandwidth and latency settings on the network. The NetEm router is a standard PC with two network cards running the Linux Traffic Control and Network Emulation software. The bandwidth and latency constraints were only applied to the link between the application server and the database for the native case and between the application server and the proxy for the Ganesh case. There is no communication between the application server and the database with Ganesh as all data flows through the proxy. As our focus was on the WAN link between the application server and the database, there were no constraints on the link between the simulated test clients and the web server.
In this section, we address the first question raised in Section 4: Can performance can be improved significantly by exploiting similarity across database results? To answer this question, we use results from the BBOARD and AUCTION benchmarks. We use two metrics to quantify the performance improvement obtainable through the use of Ganesh: throughput, from the perspective of the web server, and average response time, from the perspective of the client. Throughput is measured in terms of the number of client requests that can be serviced per second.
Figures 6 (a) and (b) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for BBOARD"s Authoring Mix.
As Figure 6 (a) shows, Native easily saturates the 5 Mb/s link.
At 400 clients, the Native solution delivers 29 requests/sec with an average response time of 8.3 seconds. Native"s throughput drops with an increase in test clients as clients timeout due to congestion at the application server. Usability studies have shown that response times above 10 seconds cause the user to move on to 1 As Java lacks a sizeof() operator, Java caches therefore limit their size based on the number of objects. The size of cache dumps taken at the end of the experiments never exceeded 212 MB.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 315 0 50 100 150 200 250 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Requests/sec Native Ganesh
1 10 100 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Avg.Resp.Time(sec) Native Ganesh Note Logscale (a) Throughput: Authoring Mix (b) Response Time: Authoring Mix 0 50 100 150 200 250 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Requests/sec Native Ganesh
1 10 100 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Avg.Resp.Time(sec) Native Ganesh Note Logscale (c) Throughput: Browsing Mix (d) Response Time: Browsing Mix Mean of three trials. The maximum standard deviation for throughput and response time was 9.8% and 11.9% of the corresponding mean.
Figure 6: BBOARD Benchmark - Throughput and Average Response Time other tasks [24]. Based on these numbers, increasing the number of test clients makes the Native system unusable. Ganesh at
clients and a fivefold improvement at 1200 clients. Ganesh"s performance drops slightly at 1200 and 1600 clients as the network is saturated. Compared to Native, Figure 6 (b) shows that Ganesh"s response times are substantially lower with sub-second response times at 400 clients.
Figure 6 (a) also shows that for 400 and 800 test clients Ganesh at 5 Mb/s has the same throughput and average response time as Native at 20 Mb/s. Only at 1200 and 1600 clients does Native at 20 Mb/s deliver higher throughput than Ganesh at 5 Mb/s.
Comparing both Ganesh and Native at 20 Mb/s, we see that Ganesh is no longer bandwidth constrained and delivers up to a twofold improvement over Native at 1600 test clients. As Ganesh does not saturate the network with higher test client configurations, at 1600 test clients, its average response time is 0.1 seconds rather than Native"s 7.7 seconds.
As expected, there are no visible gains from Ganesh at the higher bandwidth of 100 Mb/s where the network is no longer the bottleneck. Ganesh, however, still tracks Native in terms of throughput.
Figures 6 (c) and (d) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for BBOARD"s Browsing Mix.
Regardless of the test client configuration, Figure 6 (c) shows that Native"s throughput at 5 Mb/s is limited to 10 reqs/sec. Ganesh at 5 Mb/s with 400 test clients, delivers more than a sixfold increase in throughput. The improvement increases to over a elevenfold increase at 800 test clients before Ganesh saturates the network. Further, Figure 6 (d) shows that Native"s average response time of 35 seconds at 400 test clients make the system unusable.
These high response times further increase with the addition of test clients. Even with the 1600 test client configuration Ganesh delivers an acceptable average response time of 8.2 seconds.
Due to the data-intensive nature of the Browsing mix, Ganesh at
Further, as shown in Figure 6 (d), while the average response time for Native at 20 Mb/s is acceptable at 400 test clients, it is unusable with 800 test clients with an average response time of 15.8 seconds.
Like the 5 Mb/s case, this response time increases with the addition of extra test clients.
Ganesh at 20 Mb/s and both Native and Ganesh at 100 Mb/s are not bandwidth limited. However, performance plateaus out after
We were surprised by the Native performance from the BBOARD benchmark. At the bandwidth of 5 Mb/s, Native performance was lower than what we had expected. It turned out the benchmark code that displays stories read all the comments associated with the particular story from the database and only then did some postprocessing to select the comments to be displayed. While this is exactly the behavior of SlashCode, the code base behind the Slashdot web site, we decided to modify the benchmark to perform some pre-filtering at the database. This modified benchmark, named the Filter Variant, models a developer who applies optimizations at the SQL level to transfer less data. In the interests of brevity, we only briefly summarize the results from the Authoring mix.
For the Authoring mix, at 800 test clients at 5 Mb/s, Figure 7 (a) shows that Native"s throughput increase by 85% when compared to the original benchmark while Ganesh"s improvement is smaller at 15%. Native"s performance drops above 800 clients as the test clients time out due to high response times. The most significant gain for Native is seen at 20 Mb/s. At 1600 test clients, when compared to the original benchmark, Native sees a 73% improvement in throughput and a 77% reduction in average response time. While WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 316 0 50 100 150 200 250 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Requests/sec Native Ganesh
1 10 100 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Avg.Resp.Time(sec) Native Ganesh Note Logscale (a) Throughput: Authoring Mix (b) Response Time: Authoring Mix Mean of three trials. The maximum standard deviation for throughput and response time was 7.2% and 11.5% of the corresponding mean.
Figure 7: BBOARD Benchmark - Filter Variant - Throughput and Average Response Time Ganesh sees no improvement when compared to the original, it still processes 19% more requests/sec than Native. Thus, while the optimizations were more helpful to Native, Ganesh still delivers an improvement in performance.
Figures 8 (a) and (b) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for AUCTION"s Bidding Mix. As mentioned earlier, the Bidding mix consists of a mixture of read and write operations.
The AUCTION benchmark is not as data intensive as BBOARD.
Therefore, most of the gains are observed at the lower bandwidth of 5 Mb/s. Figure 8 (a) shows that the increase in throughput due to Ganesh ranges from 8% at 400 test clients to 18% with 1600 test clients. As seen in Figure 8 (b), the average response times for Ganesh are significantly lower than Native ranging from a decrease of 84% at 800 test clients to 88% at 1600 test clients.
Figure 8 (a) also shows that with a fourfold increase of bandwidth from 5 Mb/s to 20 Mb/s, Native is no longer bandwidth constrained and there is no performance difference between Ganesh and Native. With the higher test client configurations, we did observe that the bandwidth used by Ganesh was lower than Native.
Ganesh might still be useful in these non-constrained scenarios if bandwidth is purchased on a metered basis. Similar results are seen for the 100 Mb/s scenario.
For AUCTION"s Browsing Mix, Figures 8 (c) and (d) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients.
Again, most of the gains are observed at lower bandwidths. At 5 Mb/s, Native and Ganesh deliver similar throughput and response times with 400 test clients. While the throughput for both remains the same at 800 test clients, Figure 8 (d) shows that Ganesh"s average response time is 62% lower than Native. Native saturates the link at 800 clients and adding extra test clients only increases the average response time. Ganesh, regardless of the test client configuration, is not bandwidth constrained and maintains the same response time. At 1600 test clients, Figure 8 (c) shows that Ganesh"s throughput is almost twice that of Native.
At the higher bandwidths of 20 and 100 Mb/s, neither Ganesh nor Native is bandwidth limited and deliver equivalent throughput and response times.
Benchmark Orig. Size Ganesh Size Rabin Size SelectSort1 223.6 MB 5.4 MB 219.3 MB SelectSort2 223.6 MB 5.4 MB 223.6 MB Table 2: Similarity Microbenchmarks
In this section, we address the second question raised in Section 4: How important is Ganesh"s structural similarity detection relative to Rabin fingerprinting-based similarity detecting? To answer this question, we used microbenchmarks and the BBOARD and AUCTION benchmarks. As Ganesh always performed better than Rabin fingerprinting, we only present a subset of the results here in the interests of brevity.
Two microbenchmarks show an example of the effects of data reordering on Rabin fingerprinting algorithm. In the first microbenchmark, SelectSort1, a query with a specified sort order selects
query is then repeated with a different sort attribute. While the same number of rows and the same data is returned, the order of rows is different. In such a scenario, one would expect a large amount of similarity to be detected between both results. As Table 2 shows, Ganesh"s row-based algorithm achieves a 97.6% reduction while the Rabin fingerprinting algorithm, with the average chunk size parameter set to 4 KB, only achieves a 1% reduction.
The reason, as shown earlier in Figure 2, is that with Rabin fingerprinting, the spans of data between two consecutive boundaries usually cross row boundaries. With the order of the rows changing in the second result and the Rabin fingerprints now spanning different rows, the algorithm is unable to detect significant similarity.
The small gain seen is mostly for those single rows that are large enough to be broken into multiple chunks.
SelectSort2, another micro-benchmark executed the same queries but increased the minimum chunk size of the Rabin fingerprinting algorithm. As can be seen in Table 2, even the small gain from the previous microbenchmark disappears as the minimum chunk size was greater than the average row size. While one can partially address these problems by dynamically varying the parameters of the Rabin fingerprinting algorithm, this can be computationally expensive, especially in the presence of changing workloads.
We ran the BBOARD benchmark described in Section 4.1.1 on two versions of Ganesh: the first with Rabin fingerprinting used as WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 317 0 50 100 150 200 250 300 350 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Requests/sec Native Ganesh
1 10 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Avg.Resp.Time(sec) Native Ganesh Note Logscale (a) Throughput: Bidding Mix (b) Response Time: Bidding Mix 0 50 100 150 200 250 300 350 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Requests/sec Native Ganesh
1 10 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Avg.Resp.Time(sec) Native Ganesh Note Logscale (c) Throughput: Browsing Mix (d) Response Time: Browsing Mix Mean of three trials. The maximum standard deviation for throughput and response time was 2.2% and 11.8% of the corresponding mean.
Figure 8: AUCTION Benchmark - Throughput and Average Response Time the chunking algorithm and the second with Ganesh"s row-based algorithm. Rabin"s results for the Browsing Mix are normalized to Ganesh"s results and presented in Figure 9.
As Figure 9 (a) shows, at 5 Mb/s, independent of the test client configuration, Rabin significantly underperforms Ganesh. This happens because of a combination of two reasons. First, as outlined in Section 3.1, Rabin finds less similarity as it does not exploit the result"s structural information. Second, this benchmark contained some queries that generated large results. In this case,
Rabin, with a small average chunk size, generated a large number of objects that evicted other useful data from the cache. In contrast,
Ganesh was able to detect these large rows and correspondingly increase the size of the chunks. This was confirmed as cache statistics showed that Ganesh"s hit ratio was roughly three time that of Rabin. Throughput measurements at 20 Mb/s were similar with the exception of Rabin"s performance with 400 test clients. In this case, Ganesh was not network limited and, in fact, the throughput was the same as 400 clients at 5 Mb/s. Rabin, however, took advantage of the bandwidth increase from 5 to 20 Mb/s to deliver a slightly better performance. At 100 Mb/s, Rabin"s throughput was almost similar to Ganesh as bandwidth was no longer a bottleneck.
The normalized response time, presented in Figure 9 (b), shows similar trends. At 5 and 20 Mb/s, the addition of test clients decreases the normalized response time as Ganesh"s average response time increases faster than Rabin"s. However, at no point does Rabin outperform Ganesh. Note that at 400 and 800 clients at 100 Mb/s,
Rabin does have a higher overhead even when it is not bandwidth constrained. As mentioned in Section 3.1, this is due to the fact that Rabin has to hash each ResultSet twice. The overhead disappears with 1200 and 1600 clients as the database CPU is saturated and limits the performance of both Ganesh and Rabin.
In this section, we address the third question raised in Section 4: Is the overhead of Ganesh"s proxy-based design acceptable? To answer this question, we concentrate on its performance at the higher bandwidths. Our evaluation in Section 5 showed that Ganesh, when compared to Native, can deliver a substantial throughput improvement at lower bandwidths. It is only at higher bandwidths that latency, measured by the average response time for a client request, and throughput, measured by the number of client requests that can be serviced per second, overheads would be visible.
Looking at the Authoring mix of the original BBOARD benchmark, there are no visible gains from Ganesh at 100 Mb/s. Ganesh, however, still tracks Native in terms of throughput. While the average response time is higher for Ganesh, the absolute difference is in between 0.01 and 0.04 seconds and would be imperceptible to the end-user. The Browsing mix shows an even smaller difference in average response times. The results from the filter variant of the BBOARD benchmarks are similar. Even for the AUCTION benchmark, the difference between Native and Ganesh"s response time at
to the above results was seen in the filter variant of the BBOARD benchmark where Ganesh at 1600 test clients added 0.85 seconds to the average response time. Thus, even for much faster networks where the WAN link is not the bottleneck, Ganesh always delivers throughput equivalent to Native. While some extra latency is added by the proxy-based design, it is usually imperceptible.
To the best of our knowledge, Ganesh is the first system that combines the use of hash-based techniques with caching of database results to improve throughput and response times for applications with dynamic content. We also believe that it is also the first system to demonstrate the benefits of using structural information for WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 318
400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Norm.Throughput
0 5 10 15 20 25 30 35 400 800 1200 1600 400 800 1200 1600 400 800 1200 1600
Test Clients Norm.ResponseTime (a) Normalized Throughput: Higher is better (b) Normalized Response Time: Higher is worse For throughput, a normalized result greater than 1 implies that Rabin is better, For response time, a normalized result greater than 1 implies that Ganesh is better. Mean of three trials. The maximum standard deviation for throughput and response time was 9.1% and 13.9% of the corresponding mean.
Figure 9: Normalized Comparison of Ganesh vs. Rabin - BBOARD Browsing Mix detecting similarity. In this section, we first discuss alternative approaches to caching dynamic content and then examine other uses of hash-based primitives in distributed systems.
At the database layer, a number of systems have advocated middletier caching where parts of the database are replicated at the edge or server [3, 4, 20]. These systems either cache entire tables in what is essentially a replicated database or use materialized views from previous query replies [19]. They require tight integration with the back-end database to ensure a time bound on the propagation of updates. These systems are also usually targeted towards workloads that do not require strict consistency and can tolerate stale data. Further, unlike Ganesh, some of these mid-tier caching solutions [2, 3], suffer from the complexity of having to participate in query planing and distributed query processing.
Gao et al. [15] propose using a distributed object replication architecture where the data store"s consistency requirements are adapted on a per-application basis. These solutions require substantial developer resources and detailed understanding of the application being modified. While systems that attempt to automate the partitioning and replication of an application"s database exist [34], they do not provide full transaction semantics. In comparison, Ganesh does not weaken any of the semantics provided by the underlying database.
Recent work in the evaluation of edge caching options for dynamic web sites [38] has suggested that, without careful planning, employing complex offloading strategies can hurt performance.
Instead, the work advocates for an architecture in which all tiers except the database should be offloaded to the edge. Our evaluation of Ganesh has shown that it would benefit these scenarios. To improve database scalability, C-JDBC [10], SSS [22], and Ganymed [28] also advocate the use of an interposition-based architecture to transparently cluster and replicate databases at the middleware level.
The approaches of these architectures and Ganesh are complementary and they would benefit each other.
Moving up to the presentation layer, there has been widespread adoption of fragment-based caching [14], which improves cache utilization by separately caching different parts of generated web pages. While fragment-based caching works at the edge, a recent proposal has proposed moving web page assembly to the clients to optimize content delivery [31]. While Ganesh is not used at the presentation layer, the same principles have been applied in Duplicate Transfer Detection [25] to increase web cache efficiency as well as for web access across bandwidth limited links [33].
The past few years have seen the emergence of many systems that exploit hash-based techniques. At the heart of all these systems is the idea of detecting similarity in data without requiring interpretation of that data. This simple yet elegant idea relies on cryptographic hashing, as discussed earlier in Section 2. Successful applications of this idea span a wide range of storage systems.
Examples include peer-to-peer backup of personal computing files [11], storage-efficient archiving of data [29], and finding similar files [21].
Spring and Wetherall [35] apply similar principles at the network level. Using synchronized caches at both ends of a network link, duplicated data is replaced by smaller tokens for transmission and then restored at the remote end. This and other hash-based systems such as the CASPER [37] and LBFS [26] filesystems, and Layer-2 bandwidth optimizers such as Riverbed and Peribit use Rabin fingerprinting [30] to discover spans of commonality in data. This approach is especially useful when data items are modified in-place through insertions, deletions, and updates. However, as Section 6 shows, the performance of this technique can show a dramatic drop in the presence of data reordering. Ganesh instead uses row boundaries as dividers for detecting similarity.
The most aggressive use of hash-based techniques is by systems that use hashes as the primary identifiers for objects in persistent storage. Storage systems such as CFS [12] and PAST [13] that have been built using distributed hash tables fall into this category.
Single Instance Storage [6] and Venti [29] are other examples of such systems. As discussed in Section 2.2, the use of cryptographic hashes for addressing persistent data represents a deeper level of faith in their collision-resistance than that assumed by Ganesh. If time reveals shortcomings in the hash algorithm, the effort involved in correcting the flaw is much greater. In Ganesh, it is merely a matter of replacing the hash algorithm.
The growing use of dynamic web content generated from relational databases places increased demands on WAN bandwidth.
Traditional caching solutions for bandwidth and latency reduction are often ineffective for such content. This paper shows that the impact of WAN accesses to databases can be substantially reduced through the Ganesh architecture without any compromise of the database"s strict consistency semantics. The essence of the Ganesh architecture is the use of computation at the edges to reduce communication through the Internet. Ganesh is able to use cryptographic hashes to detect similarity with previous results and send WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content 319 compact recipes of results rather than full results. Our design uses interposition to achieve complete transparency: clients, application servers, and database servers are all unaware of Ganesh"s presence and require no modification.
Our experimental evaluation confirms that Ganesh, while conceptually simple, can be highly effective in improving throughput and response time. Our results also confirm that exploiting the structure present in database results to detect similarity is crucial to this performance improvement.
[1] AKELLA, A., SESHAN, S., AND SHAIKH, A. An empirical evaluation of wide-area internet bottlenecks. In Proc. 3rd ACM SIGCOMM Conference on Internet Measurement (Miami Beach, FL, USA, Oct. 2003), pp. 101-114. [2] ALTINEL, M., BORNH ¨OVD, C., KRISHNAMURTHY, S.,
MOHAN, C., PIRAHESH, H., AND REINWALD, B. Cache tables: Paving the way for an adaptive database cache. In Proc. of 29th VLDB (Berlin, Germany, 2003), pp. 718-729. [3] ALTINEL, M., LUO, Q., KRISHNAMURTHY, S., MOHAN,
C., PIRAHESH, H., LINDSAY, B. G., WOO, H., AND BROWN, L. Dbcache: Database caching for web application servers. In Proc. 2002 ACM SIGMOD (2002), pp. 612-612. [4] AMIRI, K., PARK, S., TEWARI, R., AND PADMANABHAN,
S. Dbproxy: A dynamic data cache for web applications. In Proc. IEEE International Conference on Data Engineering (ICDE) (Mar. 2003). [5] BLACK, J. Compare-by-hash: A reasoned analysis. In Proc.
May 2006), pp. 85-90. [6] BOLOSKY, W. J., CORBIN, S., GOEBEL, D., , AND DOUCEUR, J. R. Single instance storage in windows 2000.
In Proc. 4th USENIX Windows Systems Symposium (Seattle,
WA, Aug. 2000), pp. 13-24. [7] BREWER, E. A. Lessons from giant-scale services. IEEE Internet Computing 5, 4 (2001), 46-55. [8] BRODER, A., GLASSMAN, S., MANASSE, M., AND ZWEIG, G. Syntactic clustering of the web. In Proc. 6th International WWW Conference (1997). [9] CECCHET, E., CHANDA, A., ELNIKETY, S.,
MARGUERITE, J., AND ZWAENEPOEL, W. Performance comparison of middleware architectures for generating dynamic web content. In Proc. Fourth ACM/IFIP/USENIX International Middleware Conference (Rio de Janeiro,
Brazil, June 2003). [10] CECCHET, E., MARGUERITE, J., AND ZWAENEPOEL, W.
C-JDBC: Flexible database clustering middleware. In Proc.
June 2004). [11] COX, L. P., MURRAY, C. D., AND NOBLE, B. D. Pastiche: Making backup cheap and easy. In OSDI: Symposium on Operating Systems Design and Implementation (2002). [12] DABEK, F., KAASHOEK, M. F., KARGER, D., MORRIS,
R., AND STOICA, I. Wide-area cooperative storage with CFS. In 18th ACM Symposium on Operating Systems Principles (Banff, Canada, Oct. 2001). [13] DRUSCHEL, P., AND ROWSTRON, A. PAST: A large-scale, persistent peer-to-peer storage utility. In HotOS VIII (Schloss Elmau, Germany, May 2001), pp. 75-80. [14] Edge side includes. http://www.esi.org. [15] GAO, L., DAHLIN, M., NAYATE, A., ZHENG, J., AND IYENGAR, A. Application specific data replication for edge services. In WWW "03: Proc. Twelfth International Conference on World Wide Web (2003), pp. 449-460. [16] HEMMINGER, S. Netem - emulating real networks in the lab.
In Proc. 2005 Linux Conference Australia (Canberra,
Australia, Apr. 2005). [17] HENSON, V. An analysis of compare-by-hash. In Proc. 9th Workshop on Hot Topics in Operating Systems (HotOS IX) (May 2003), pp. 13-18. [18] Jmob benchmarks. http://jmob.objectweb.org/. [19] LABRINIDIS, A., AND ROUSSOPOULOS, N. Balancing performance and data freshness in web database servers. In Proc. 29th VLDB Conference (Sept. 2003). [20] LARSON, P.-A., GOLDSTEIN, J., AND ZHOU, J.
Transparent mid-tier database caching in sql server. In Proc.
[21] MANBER, U. Finding similar files in a large file system. In Proc. USENIX Winter 1994 Technical Conference (San Fransisco, CA, 17-21 1994), pp. 1-10. [22] MANJHI, A., AILAMAKI, A., MAGGS, B. M., MOWRY,
T. C., OLSTON, C., AND TOMASIC, A. Simultaneous scalability and security for data-intensive web applications.
In Proc. 2006 ACM SIGMOD (June 2006), pp. 241-252. [23] MENEZES, A. J., VANSTONE, S. A., AND OORSCHOT, P.
C. V. Handbook of Applied Cryptography. CRC Press, 1996. [24] MILLER, R. B. Response time in man-computer conversational transactions. In Proc. AFIPS Fall Joint Computer Conference (1968), pp. 267-277. [25] MOGUL, J. C., CHAN, Y. M., AND KELLY, T. Design, implementation, and evaluation of duplicate transfer detection in http. In Proc. First Symposium on Networked Systems Design and Implementation (San Francisco, CA,
Mar. 2004). [26] MUTHITACHAROEN, A., CHEN, B., AND MAZIERES, D. A low-bandwidth network file system. In Proc. 18th ACM Symposium on Operating Systems Principles (Banff, Canada,
Oct. 2001). [27] PFEIFER, D., AND JAKSCHITSCH, H. Method-based caching in multi-tiered server applications. In Proc. Fifth International Symposium on Distributed Objects and Applications (Catania, Sicily, Italy, Nov. 2003). [28] PLATTNER, C., AND ALONSO, G. Ganymed: Scalable replication for transactional web applications. In Proc. 5th ACM/IFIP/USENIX International Conference on Middleware (2004), pp. 155-174. [29] QUINLAN, S., AND DORWARD, S. Venti: A new approach to archival storage. In Proc. FAST 2002 Conference on File and Storage Technologies (2002). [30] RABIN, M. Fingerprinting by random polynomials. In Harvard University Center for Research in Computing Technology Technical Report TR-15-81 (1981). [31] RABINOVICH, M., XIAO, Z., DOUGLIS, F., AND KALMANEK, C. Moving edge side includes to the real edge - the clients. In Proc. 4th USENIX Symposium on Internet Technologies and Systems (Seattle, WA, Mar. 2003). [32] REESE, G. Database Programming with JDBC and Java, 1st ed. O"Reilly, June 1997. [33] RHEA, S., LIANG, K., AND BREWER, E. Value-based web caching. In Proc. Twelfth International World Wide Web Conference (May 2003). [34] SIVASUBRAMANIAN, S., ALONSO, G., PIERRE, G., AND VAN STEEN, M. Globedb: Autonomic data replication for web applications. In WWW "05: Proc. 14th International World-Wide Web conference (May 2005). [35] SPRING, N. T., AND WETHERALL, D. A protocol-independent technique for eliminating redundant network traffic. In Proc. of ACM SIGCOMM (Aug. 2000). [36] TOLIA, N., HARKES, J., KOZUCH, M., AND SATYANARAYANAN, M. Integrating portable and distributed storage. In Proc. 3rd USENIX Conference on File and Storage Technologies (San Francisco, CA, Mar. 2004). [37] TOLIA, N., KOZUCH, M., SATYANARAYANAN, M., KARP,
B., PERRIG, A., AND BRESSOUD, T. Opportunistic use of content addressable storage for distributed file systems. In Proc. 2003 USENIX Annual Technical Conference (San Antonio, TX, June 2003), pp. 127-140. [38] YUAN, C., CHEN, Y., AND ZHANG, Z. Evaluation of edge caching/offloading for dynamic content delivery. In WWW "03: Proc. Twelfth International Conference on World Wide Web (2003), pp. 461-471.

Energy supply has always been a crucial issue in designing battery-powered wireless sensor networks because the lifetime and utility of the systems are limited by how long the batteries are able to sustain the operation. The fidelity of the data produced by a sensor network begins to degrade once sensor nodes start to run out of battery power. Therefore, harvesting energy from the environment has been proposed to supplement or completely replace battery supplies to enhance system lifetime and reduce the maintenance cost of replacing batteries periodically.
However, metrics for evaluating energy harvesting systems are different from those used for battery powered systems.
Environmental energy is distinct from battery energy in two ways.
First it is an inexhaustible supply which, if appropriately used, can allow the system to last forever, unlike the battery which is a limited resource. Second, there is an uncertainty associated with its availability and measurement, compared to the energy stored in the battery which can be known deterministically. Thus, power management methods based on battery status are not always applicable to energy harvesting systems. In addition, most power management schemes designed for battery-powered systems only account for the dynamics of the energy consumers (e.g., CPU, radio) but not the dynamics of the energy supply. Consequently, battery powered systems usually operate at the lowest performance level that meets the minimum data fidelity requirement in order to maximize the system life. Energy harvesting systems, on the other hand, can provide enhanced performance depending on the available energy.
In this paper, we will study how to adapt the performance of the available energy profile. There exist many techniques to accomplish performance scaling at the node level, such as radio transmit power adjustment [1], dynamic voltage scaling [2], and the use of low power modes [3]. However, these techniques require hardware support and may not always be available on resource constrained sensor nodes. Alternatively, a common performance scaling technique is duty cycling. Low power devices typically provide at least one low power mode in which the node is shut down and the power consumption is negligible. In addition, the rate of duty cycling is directly related to system performance metrics such as network latency and sampling frequency. We will use duty cycle adjustment as the primitive performance scaling technique in our algorithms.
Energy harvesting has been explored for several different types of systems, such as wearable computers [4], [5], [6], sensor networks [7], etc. Several technologies to extract energy from the environment have been demonstrated including solar, motion-based, biochemical, vibration-based [8], [9], [10], [11], and others are being developed [12], [13]. While several energy harvesting sensor node platforms have been prototyped [14], [15], [16], there is a need for systematic power management techniques that provide performance guarantees during system operation. The first work to take environmental energy into account for data routing was [17], followed by [18].
While these works did demonstrate that environment aware decisions improve performance compared to battery aware decisions, their objective was not to achieve energy neutral operation. Our proposed techniques attempt to maximize system performance while maintaining energy-neutral operation.
The energy usage considerations in a harvesting system vary significantly from those in a battery powered system, as mentioned earlier. We propose the model shown in Figure 1 for designing energy management methods in a harvesting system. The functions of the various blocks shown in the figure are discussed below. The precise methods used in our system to achieve these functions will be discussed in subsequent sections.
Harvested Energy Tracking: This block represents the mechanisms used to measure the energy received from the harvesting device, such as the solar panel. Such information is useful for determining the energy availability profile and adapting system performance based on it. Collecting this information requires that the node hardware be equipped with the facility to measure the power generated from the environment, and the Heliomote platform [14] we used for evaluating the algorithms has this capability.
Energy Generation Model: For wireless sensor nodes with limited storage and processing capabilities to be able to use the harvested energy data, models that represent the essential components of this information without using extensive storage are required. The purpose of this block is to provide a model for the energy available to the system in a form that may be used for making power management decisions. The data measured by the energy tracking block is used here to predict future energy availability. A good prediction model should have a low prediction error and provide predicted energy values for durations long enough to make meaningful performance scaling decisions. Further, for energy sources that exhibit both long-term and short-term patterns (e.g., diurnal and climate variations vs. weather patterns for solar energy), the model must be able to capture both characteristics. Such a model can also use information from external sources such as local weather forecast service to improve its accuracy.
Energy Consumption Model: It is also important to have detailed information about the energy usage characteristics of the system, at various performance levels. For general applicability of our design, we will assume that only one sleep mode is available. We assume that the power consumption in the sleep and active modes is known.
It may be noted that for low power systems with more advanced capabilities such as dynamic voltage scaling (DVS), multiple low power modes, and the capability to shut down system components selectively, the power consumption in each of the states and the resultant effect on application performance should be known to make power management decisions.
Energy Storage Model: This block represents the model for the energy storage technology. Since all the generated energy may not be used instantaneously, the harvesting system will usually have some energy storage technology. Storage technologies (e.g., batteries and ultra-capacitors) are non-ideal, in that there is some energy loss while storing and retrieving energy from them. These characteristics must be known to efficiently manage energy usage and storage. This block also includes the system capability to measure the residual stored energy.
Most low power systems use batteries to store energy and provide residual battery status. This is commonly based on measuring the battery voltage which is then mapped to the residual battery energy using the known charge to voltage relationship for the battery technology in use. More sophisticated methods which track the flow of energy into and out of the battery are also available.
Harvesting-aware Power Management: The inputs provided by the previously mentioned blocks are used here to determine the suitable power management strategy for the system. Power management could be carried to meet different objectives in different applications.
For instance, in some systems, the harvested energy may marginally supplement the battery supply and the objective may be to maximize the system lifetime. A more interesting case is when the harvested energy is used as the primary source of energy for the system with the objective of achieving indefinitely long system lifetime. In such cases, the power management objective is to achieve energy neutral operation. In other words, the system should only use as much energy as harvested from the environment and attempt to maximize performance within this available energy budget.
MANAGEMENT We develop the following theory to understand the energy neutral mode of operation. Let us define Ps(t) as the energy harvested from the environment at time t, and the energy being consumed by the load at that time is Pc(t). Further, we model the non-ideal storage buffer by its round-trip efficiency η (strictly less than 1) and a constant leakage power Pleak. Using this notation, applying the rule of energy conservation leads to the following inequality:
[ ( ) ( )] [ ( ) ( )] 0 T T T s c c s leakP t P t dt P t P t dt P dtB η + + − − − ≥+ −∫ ∫ ∫ (1) where B0 is the initial battery level and the function [X]+ = X if X > 0 and zero otherwise.
DEFINITION 1 (ρ,σ1,σ2) function: A non-negative, continuous and bounded function P (t) is said to be a (ρ,σ1,σ2) function if and only if for any value of finite real number T , the following are satisfied:
T T P t dt Tρ σ ρ σ− ≤ ≤ +∫ (2) This function can be used to model both energy sources and loads.
If the harvested energy profile Ps(t) is a (ρ1,σ1,σ2) function, then the average rate of available energy over long durations becomes ρ1, and the burstiness is bounded by σ1 and σ2 . Similarly, Pc(t) can be modeled as a (ρ2,σ3) function, when ρ2 and σ3 are used to place an upper bound on power consumption (the inequality on the right side) while there are no minimum power consumption constraints.
The condition for energy neutrality, equation (1), leads to the following theorem, based on the energy production, consumption, and energy buffer models discussed above.
THEOREM 1 (ENERGY NEUTRAL OPERATION): Consider a harvesting system in which the energy production profile is characterized by a (ρ1, σ1, σ2) function, the load is characterized by a (ρ2, σ3) function and the energy buffer is characterized by parameters η for storage efficiency, and Pleak for leakage power. The following conditions are sufficient for the system to achieve energy neutrality: ρ2 ≤ ηρ1 − Pleak (3) B0 ≥ ησ2 + σ3 (4) B ≥ B0 (5) where B0 is the initial energy stored in the buffer and provides a lower bound on the capacity of the energy buffer B. The proof is presented in our prior work [19].
To adjust the duty cycle D using our performance scaling algorithm, we assume the following relation between duty cycle and the perceived utility of the system to the user: Suppose the utility of the application to the user is represented by U(D) when the system operates at a duty cycle D. Then, min
( ) 0, ( ) , ( ) ,
U D if D D U D k D if D D D U D k if D D β = < = + ≤ ≤ = > This is a fairly general and simple model and the specific values of Dmin and Dmax may be determined as per application requirements. As an example, consider a sensor node designed to detect intrusion across a periphery. In this case, a linear increase in duty cycle translates into a linear increase in the detection probability. The fastest and the slowest speeds of the intruders may be known, leading to a minimum and Harvested Energy Tracking Energy Consumption Model Energy Storage Model  Harvestingaware Power Mangement Energy Generation Model LOAD Figure 1. System model for an energy harvesting system. 181 maximum sensing delay tolerable, which results in the relevant Dmax and Dmin for the sensor node. While there may be cases where the relationship between utility and duty cycle may be non-linear, in this paper, we restrict our focus on applications that follow this linear model. In view of the above models for the system components and the required performance, the objective of our power management strategy is adjust the duty cycle D(i) dynamically so as to maximize the total utility U(D) over a period of time, while ensuring energy neutral operation for the sensor node.
Before discussing the performance scaling methods for harvesting aware duty cycle adaptation, let us first consider the optimal power management strategy that is possible for a given energy generation profile. For the calculation of the optimal strategy, we assume complete knowledge of the energy availability profile at the node, including the availability in the future. The calculation of the optimal is a useful tool for evaluating the performance of our proposed algorithm.
This is particularly useful for our algorithm since no prior algorithms are available to serve as a baseline for comparison.
Suppose the time axis is partitioned into discrete slots of duration ΔT, and the duty cycle adaptation calculation is carried out over a window of Nw such time slots. We define the following energy profile variables, with the index i ranging over {1,…, Nw}: Ps(i) is the power output from the harvested source in time slot i, averaged over the slot duration, Pc is the power consumption of the load in active mode, and D(i) is the duty cycle used in slot i, whose value is to be determined.
B(i) is the residual battery energy at the beginning of slot i. Following this convention, the battery energy left after the last slot in the window is represented by B(Nw+1). The values of these variables will depend on the choice of D(i).
The energy used directly from the harvested source and the energy stored and used from the battery must be accounted for differently.
Figure 2 shows two possible cases for Ps(i) in a time slot. Ps(i) may either be less than or higher than Pc , as shown on the left and right respectively. When Ps(i) is lower than Pc, some of the energy used by the load comes from the battery, while when Ps(i) is higher than Pc, all the energy used is supplied directly from the harvested source. The crosshatched area shows the energy that is available for storage into the battery while the hashed area shows the energy drawn from the battery. We can write the energy used from the battery in any slot i as: ( ) ( ) ( ) ( )[ ] ( ) ( ){ } ( ) ( )[ ]1 1c cs s sB i B i TD i P P i TP i D i TD i P i Pη η + + − + = Δ − − Δ − − − (6) In equation (6), the first term on the right hand side measures the energy drawn from the battery when Ps(i) < Pc, the next term measures the energy stored into the battery when the node is in sleep mode, and the last term measures the energy stored into the battery in active mode if Ps(i) > Pc. For energy neutral operation, we require the battery at the end of the window of Nw slots to be greater than or equal to the starting battery. Clearly, battery level will go down when the harvested energy is not available and the system is operated from stored energy.
However, the window Nw is judiciously chosen such that over that duration, we expect the environmental energy availability to complete a periodic cycle. For instance, in the case of solar energy harvesting,
Nw could be chosen to be a twenty-four hour duration, corresponding to the diurnal cycle in the harvested energy. This is an approximation since an ideal choice of the window size would be infinite, but a finite size must be used for analytical tractability. Further, the battery level cannot be negative at any time, and this is ensured by having a large enough initial battery level B0 such that node operation is sustained even in the case of total blackout during a window period. Stating the above constraints quantitatively, we can express the calculation of the optimal duty cycles as an optimization problem below: 1 max ( ) wN i D i = ∑ (7) ( ) ( ) ( ) ( ) ( ) ( ){ } ( ) ( )1 1c s s s cB i B i TD i P P i TP i D i TD i P i Pη η + + ⎡ ⎤ ⎡ ⎤− + = Δ − − Δ − − −⎣ ⎦ ⎣ ⎦ (8) 0(1)B B= (9) 0( 1)wB N B+ ≥ (10) min w( ) i {1,...,N }D i D≥ ∀ ∈ (11) max w( ) i {1,...,N }D i D≤ ∀ ∈ (12) The solution to the optimization problem yields the duty cycles that must be used in every slot and the evolution of residual battery over the course of Nw slots. Note that while the constraints above contain the non-linear function [x]+ , the quantities occurring within that function are all known constants. The variable quantities occur only in linear terms and hence the above optimization problem can be solved using standard linear programming techniques, available in popular optimization toolboxes.
MANAGEMENT We now present a practical algorithm for power management that may be used for adapting the performance based on harvested energy information. This algorithm attempts to achieve energy neutral operation without using knowledge of the future energy availability and maximizes the achievable performance within that constraint.
The harvesting-aware power management strategy consists of three parts. The first part is an instantiation of the energy generation model which tracks past energy input profiles and uses them to predict future energy availability. The second part computes the optimal duty cycles based on the predicted energy, and this step uses our computationally tractable method to solve the optimization problem. The third part consists of a method to dynamically adapt the duty cycle in response to the observed energy generation profile in real time. This step is required since the observed energy generation may deviate significantly from the predicted energy availability and energy neutral operation must be ensured with the actual energy received rather than the predicted values.
We use a prediction model based on Exponentially Weighted Moving-Average (EWMA). The method is designed to exploit the diurnal cycle in solar energy but at the same time adapt to the seasonal variations. A historical summary of the energy generation profile is maintained for this purpose. While the storage data size is limited to a vector length of Nw values in order to minimize the memory overheads of the power management algorithm, the window size is effectively infinite as each value in the history window depends on all the observed data up to that instant. The window size is chosen to be 24 hours and each time slot is taken to be 30 minutes as the variation in generated power by the solar panel using this setting is less than 10% between each adjacent slots. This yields Nw = 48. Smaller slot durations may be used at the expense of a higher Nw.
The historical summary maintained is derived as follows. On a typical day, we expect the energy generation to be similar to the energy generation at the same time on the previous days. The value of energy generated in a particular slot is maintained as a weighted average of the energy received in the same time-slot during all observed days. The weights are exponential, resulting in decaying contribution from older Figure 2. Two possible cases for energy calculations Slot i Slot k Pc Pc P(i) P(i) Active Sleep 182 data. More specifically, the historical average maintained for each slot is given by:
where α is the value of the weighting factor, kx is the observed value of energy generated in the slot, and 1kx − is the previously stored historical average. In this model, the importance of each day relative to the previous one remains constant because the same weighting factor was used for all days.
The average value derived for a slot is treated as an estimate of predicted energy value for the slot corresponding to the subsequent day. This method helps the historical average values adapt to the seasonal variations in energy received on different days. One of the parameters to be chosen in the above prediction method is the parameter α, which is a measure of rate of shift in energy pattern over time. Since this parameter is affected by the characteristics of the energy and sensor node location, the system should have a training period during which this parameter will be determined. To determine a good value of α, we collected energy data over 72 days and compared the average error of the prediction method for various values of α. The error based on the different values of α is shown in Figure 3. This curve suggests an optimum value of α = 0.15 for minimum prediction error and this value will be used in the remainder of this paper.
The energy values predicted for the next window of Nw slots are used to calculated the desired duty cycles for the next window, assuming the predicted values match the observed values in the future.
Since our objective is to develop a practical algorithm for embedded computing systems, we present a simplified method to solve the linear programming problem presented in Section 4. To this end, we define the sets S and D as follows: { } { } | ( ) 0 | ( ) 0 s c c s S i P i P D i P P i = − ≥ = − > The two sets differ by the condition that whether the node operation can be sustained entirely from environmental energy. In the case that energy produced from the environment is not sufficient, battery will be discharged to supplement the remaining energy. Next we sum up both sides of (6) over the entire Nw window and rewrite it with the new notation. 1
( )[ ( )] ( ) ( ) ( ) ( )[ ( ) ] Nw Nw Nw i i c s s s s c i i D i i i S B B TD i P P i TP i TP i D i TD i P i Pη η η+ = ∈ = = ∈ − = Δ − − Δ + Δ − Δ −∑ ∑ ∑ ∑ ∑ The term on the left hand side is actually the battery energy used over the entire window of Nw slots, which can be set to 0 for energy neutral operation. After some algebraic manipulation, this yields: 1 1 ( ) ( ) ( ) 1 ( ) Nw c s s c i i D i S P P i D i P i P D i η η= ∈ ∈ ⎛ ⎞⎛ ⎞ = + − +⎜ ⎟⎜ ⎟ ⎝ ⎠⎝ ⎠ ∑ ∑ ∑ (13) The term on the left hand side is the total energy received in Nw slots. The first term on the right hand side can be interpreted as the total energy consumed during the D slots and the second term is the total energy consumed during the S slots. We can now replace three constraints (8), (9), and (10) in the original problem with (13), restating the optimization problem as follows: 1 max ( ) wN i D i = ∑ 1 1 ( ) ( ) ( ) 1 ( ) Nw c s s c i i D i S P P i D i P i P D i η η= ∈ ∈ ⎛ ⎞⎛ ⎞ = + − +⎜ ⎟⎜ ⎟ ⎝ ⎠⎝ ⎠ ∑ ∑ ∑ min max D(i) D {1,...,Nw) D(i) D {1,...,Nw) i i ≥ ∀ ∈ ≤ ∀ ∈ This form facilitates a low complexity solution that doesn"t require a general linear programming solver. Since our objective is to maximize the total system utility, it is preferable to set the duty cycle to Dmin for time slots where the utility per unit energy is the least. On the other hand, we would also like the time slots with the highest Ps to operate at Dmax because of better efficiency of using energy directly from the energy source. Combining these two characteristics, we define the utility co-efficient for each slot i as follows: 1 ( ) 1
c c s P for i S W i P P i for i D η η ∈⎧ ⎪ = ⎛ ⎞⎛ ⎞⎨ + − ∈⎜ ⎟⎜ ⎟⎪ ⎝ ⎠⎝ ⎠⎩ where W(i) is a representation of how efficient the energy usage in a particular time slot i is. A larger W(i) indicates more system utility per unit energy in slot i and vice versa. The algorithm starts by assuming D(i) =Dmin for i = {1…NW} because of the minimum duty cycle requirement, and computes the remaining system energy R by: 1 1 ( ) ( ) ( ) 1 ( ) (14) Nw c s s c i i D i S P R P i D i P i P D i η η= ∈ ∈ ⎛ ⎞⎛ ⎞ = − + − −⎜ ⎟⎜ ⎟ ⎝ ⎠⎝ ⎠ ∑ ∑ ∑ A negative R concludes that the optimization problem is infeasible, meaning the system cannot achieve energy neutrality even at the minimum duty cycle. In this case, the system designer is responsible for increasing the environment energy availability (e.g., by using larger solar panels). If R is positive, it means the system has excess energy that is not being used, and this may be allocated to increase the duty cycle beyond Dmin for some slots. Since our objective is to maximize the total system utility, the most efficient way to allocate the excess energy is to assign duty cycle Dmax to the slots with the highest W(i).
So, the coefficients W(i) are arranged in decreasing order and duty cycle Dmax is assigned to the slots beginning with the largest coefficients until the excess energy available, R (computed by (14) in every iteration), is insufficient to assign Dmax to another slot. The remaining energy, RLast, is used to increase the duty cycle to some value between Dmin and Dmax in the slot with the next lower coefficient.
Denoting this slot with index j, the duty cycle is given by: D(j)= min / ( ( ) ) / ( ) Last c Last s c s R P if j D DR if j S P j P P jη ∈⎧ ⎫ ⎪ ⎪ +⎨ ⎬ ∈⎪ ⎪− −⎩ ⎭ The above solution to the optimization problem requires only simple arithmetic calculations and one sorting step which can be easily implemented on an embedded platform, as opposed to implementing a general linear program solver.
The observed energy values may vary greatly from the predicted ones, such as due to the effect of clouds or other sudden changes. It is thus important to adapt the duty cycles calculated using the predicted values, to the actual energy measurements in real time to ensure energy neutrality. Denote the initial duty cycle assignments for each time slot i computed using the predicted energy values as D(i) = {1, ...,Nw}. First we compute the difference between predicted power level Ps(i) and actual power level observed, Ps"(i) in every slot i. Then, the excess energy in slot i, denoted by X, can be obtained as follows: ( ) '( ) '( ) 1 ( ) '( ) ( )[ ( ) '( )](1 ) '( ) s s s c s s s s s c P i P i if P i P X P i P i D i P i P i if P i P η − >⎧ ⎪ = ⎨ − − − − ≤⎪ ⎩
3 alpha AvgError(mA) Figure 3. Choice of prediction parameter. 183 The upper term accounts for the energy difference when actual received energy is more than the power drawn by the load. On the other hand, if the energy received is less than Pc, we will need to account for the extra energy used from the battery by the load, which is a function of duty cycle used in time slot i and battery efficiency factor η. When more energy is received than predicted, X is positive and that excess energy is available for use in the subsequent solutes, while if X is negative, that energy must be compensated from subsequent slots.
CASE I: X<0. In this case, we want to reduce the duty cycles used in the future slots in order to make up for this shortfall of energy. Since our objective function is to maximize the total system utility, we have to reduce the duty cycles for time slots with the smallest normalized utility coefficient, W(i). This is accomplished by first sorting the coefficient W(j) ,where j>i. in decreasing order, and then iteratively reducing Dj to Dmin until the total reduction in energy consumption is the same as X.
CASE II: X>0. Here, we want to increase the duty cycles used in the future to utilize this excess energy we received in recent time slot. In contrast to Case I, the duty cycles of future time slots with highest utility coefficient W(i) should be increased first in order to maximize the total system utility.
Suppose the duty cycle is changed by d in slot j. Define a quantity R(j,d) as follows: ⎪ ⎩ ⎪ ⎨ ⎧ <=⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ ⎟⎟ ⎠ ⎞ ⎜⎜ ⎝ ⎛ −+ >⋅ = lji l ljl PPifP P d PPifP djR 　　　 1 1 　　d　　 ),( ηη The precise procedure to adapt the duty cycle to account for the above factors is presented in Algorithm 1. This calculation is performed at the end of every slot to set the duty cycle for the next slot. We claim that our duty cycling algorithm is energy neutral because an surplus of energy at the previous time slot will always translate to additional energy opportunity for future time slots, and vice versa. The claim may be violated in cases of severe energy shortages especially towards the end of window. For example, a large deficit in energy supply can"t be restored if there is no future energy input until the end of the window. In such case, this offset will be carried over to the next window so that long term energy neutrality is still maintained.
Our adaptive duty cycling algorithm was evaluated using an actual solar energy profile measured using a sensor node called Heliomote, capable of harvesting solar energy [14]. This platform not only tracks the generated energy but also the energy flow into and out of the battery to provide an accurate estimate of the stored energy.
The energy harvesting platform was deployed in a residential area in Los Angeles from the beginning of June through the middle of August for a total of 72 days. The sensor node used is a Mica2 mote running at a fixed 40% duty cycle with an initially full battery. Battery voltage and net current from the solar panels are sampled at a period of
by tracking the output current from the solar cell is shown in Figure 4, both on continuous and diurnal scales. We can observe that although the energy profile varies from day to day, it still exhibits a general pattern over several days.
0 10 20 30 40 50 60 70 Day mA
0 10 20 30 40 50 60 70 Hour mA
We first evaluate the performance of the prediction model, which is judged by the amount of absolute error it made between the predicted and actual energy profile. Figure 5 shows the average error of each time slot in mA over the entire 72 days. Generally, the amount of error is larger during the day time because that"s when the factor of weather can cause deviations in received energy, while the prediction made for night time is mostly correct.
Prior methods to optimize performance while achieving energy neutral operation using harvested energy are scarce. Instead, we compare the performance of our algorithm against two extremes: the theoretical optimal calculated assuming complete knowledge about future energy availability and a simple approach which attempts to achieve energy neutrality using a fixed duty cycle without accounting for battery inefficiency.
The optimal duty cycles are calculated for each slot using the future knowledge of actual received energy for that slot. For the simple approach, the duty cycle is kept constant within each day and is Figure 4 Solar Energy Profile (Left: Continuous, Right: Diurnal) Input: D: Initial duty cycle, X: Excess energy due to error in the prediction, P: Predicted energy profile, i: index of current time slot Output: D: Updated duty cycles in one or more subsequent slots AdaptDutyCycle() Iteration: At each time slot do: if X > 0 Wsorted = W{1, ...,Nw} sorted in decending order.
Q := indices of Wsorted for k = 1 to |Q| if Q(k) ≤ i or D(Q(k)) ≥ Dmax //slot is already passed continue if R(Q(k), Dmax − D(Q(k))) < X D(Q(k)) = Dmax X = X − R(j, Dmax − D(Q(k))) else //X insufficient to increase duty cycle to Dmax if P (Q(k)) > Pl D(Q(k)) = D(Q(k)) + X/Pl else D(Q(k)) = D(Q(k)) + ( ( ))(1 1 ))c s X P P Q kη η+ − if X < 0 Wsorted = W{1, ...,Nw} sorted in ascending order.
Q := indices of Wsorted for k = 1 to |Q| if Q(k) ≤ I or D(Q(k)) ≤ Dmin continue if R(Q(k), Dmax − D(Q(k))) > X D(Q(k)) = Dmin X = X − R(j, Dmin − D(Q(k))) else if P (Q(k)) > Pc D(Q(k)) = D(Q(k)) + X/Pc else D(Q(k)) = D(Q(k)) + ( ( ))(1 1 ))c s X P P Q kη η+ − ALGORITHM 1 Pseudocode for the duty-cycle adaptation algorithm Figure 5. Average Predictor Error in mA
0 2 4 6 8 10 12 Time(H) abserror(mA) 184 computed by taking the ratio of the predicted energy availability and the maximum usage, and this guarantees that the senor node will never deplete its battery running at this duty cycle. {1.. ) ( )s w c i Nw D P i N Pη ∈ = ⋅ ⋅∑ We then compare the performance of our algorithm to the two extremes with varying battery efficiency. Figure 6 shows the results, using Dmax = 0.8 and Dmin = 0.3. The battery efficiency was varied from 0.5 to 1 on the x-axis and solar energy utilizations achieved by the three algorithms are shown on the y-axis. It shows the fraction of net received energy that is used to perform useful work rather than lost due to storage inefficiency.
As can be seen from the figure, battery efficiency factor has great impact on the performance of the three different approaches. The three approaches all converges to 100% utilization if we have a perfect battery (η=1), that is, energy is not lost by storing it into the batteries.
When battery inefficiency is taken into account, both the adaptive and optimal approach have much better solar energy utilization rate than the simple one. Additionally, the result also shows that our adaptive duty cycle algorithm performs extremely close to the optimal.
1 Eta-batery roundtrip efficiency SolarEnergyUtilization(%) Optimal Adaptive Simple We also compare the performance of our algorithm with different values of Dmin and Dmax for η=0.7, which is typical of NiMH batteries.
These results are shown in Table 1 as the percentage of energy saved by the optimal and adaptive approaches, and this is the energy which would normally be wasted in the simple approach. The figures and table indicate that our real time algorithm is able to achieve a performance very close to the optimal feasible. In addition, these results show that environmental energy harvesting with appropriate power management can achieve much better utilization of the environmental energy.
Dmax Dmin
Adaptive 51.0% 48.2% 42.3% 29.4% 54.7% 58.7% Optimal 52.3% 49.6% 43.7% 36.7% 56.6% 60.8%
We discussed various issues in power management for systems powered using environmentally harvested energy. Specifically, we designed a method for optimizing performance subject to the constraint of energy neutral operation. We also derived a theoretically optimal bound on the performance and showed that our proposed algorithm operated very close to the optimal. The proposals were evaluated using real data collected using an energy harvesting sensor node deployed in an outdoor environment.
Our method has significant advantages over currently used methods which are based on a conservative estimate of duty cycle and can only provide sub-optimal performance. However, this work is only the first step towards optimal solutions for energy neutral operation. It is designed for a specific power scaling method based on adapting the duty cycle. Several other power scaling methods, such as DVS, submodule power switching and the use of multiple low power modes are also available. It is thus of interest to extend our methods to exploit these advanced capabilities.
This research was funded in part through support provided by DARPA under the PAC/C program, the National Science Foundation (NSF) under award #0306408, and the UCLA Center for Embedded Networked Sensing (CENS). Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA, NSF, or CENS.
REFERENCES [1] R Ramanathan, and R Hain, Toplogy Control of Multihop Wireless Networks Using Transmit Power Adjustment in Proc. Infocom. Vol 2. 26-30 pp. 404-413. March 2000 [2] T.A. Pering, T.D. Burd, and R. W. Brodersen,  The simulation and evaluation of dynamic voltage scaling algorithms, in Proc. ACM ISLPED, pp. 76-81, 1998 [3] L. Benini and G. De Micheli, Dynamic Power Management: Design Techniques and CAD Tools. Kluwer Academic Publishers, Norwell, MA,
[4] John Kymisis, Clyde Kendall, Joseph Paradiso, and Neil Gershenfeld.
Parasitic power harvesting in shoes. In ISWC, pages 132-139. IEEE Computer Society press, October 1998. [5] Nathan S. Shenck and Joseph A. Paradiso. Energy scavenging with shoemounted piezoelectrics. IEEE Micro, 21(3):30ñ42, May-June 2001. [6] T Starner. Human-powered wearable computing. IBM Systems Journal, 35(3-4), 1996. [7] Mohammed Rahimi, Hardik Shah, Gaurav S. Sukhatme, John Heidemann, and D. Estrin. Studying the feasibility of energy harvesting in a mobile sensor network. In ICRA, 2003. [8] ChrisMelhuish. The ecobot project. www.ias.uwe.ac.uk/energy autonomy/EcoBot web page.html. [9] Jan M.Rabaey, M. Josie Ammer, Julio L. da Silva Jr., Danny Patel, and Shad Roundy. Picoradio supports ad-hoc ultra-low power wireless networking. IEEE Computer, pages 42-48, July 2000. [10] Joseph A. Paradiso and Mark Feldmeier. A compact, wireless, selfpowered pushbutton controller. In ACM Ubicomp, pages 299-304,
Atlanta, GA, USA, September 2001. Springer-Verlag Berlin Heidelberg. [11] SE Wright, DS Scott, JB Haddow, andMA Rosen. The upper limit to solar energy conversion. volume 1, pages 384 - 392, July 2000. [12] Darpa energy harvesting projects. http://www.darpa.mil/dso/trans/energy/projects.html. [13] Werner Weber. Ambient intelligence: industrial research on a visionary concept. In Proceedings of the 2003 international symposium on Low power electronics and design, pages 247-251. ACM Press, 2003. [14] V Raghunathan, A Kansal, J Hsu, J Friedman, and MB Srivastava, "Design Considerations for Solar Energy Harvesting Wireless Embedded Systems," (IPSN/SPOTS), April 2005. [15] Xiaofan Jiang, Joseph Polastre, David Culler, Perpetual Environmentally Powered Sensor Networks, (IPSN/SPOTS), April 25-27, 2005. [16] Chulsung Park, Pai H. Chou, and Masanobu Shinozuka, "DuraNode: Wireless Networked Sensor for Structural Health Monitoring," to appear in Proceedings of the 4th IEEE International Conference on Sensors,
Irvine, CA, Oct. 31 - Nov. 1, 2005. [17] Aman Kansal and Mani B. Srivastava. An environmental energy harvesting framework for sensor networks. In International symposium on Low power electronicsand design, pages 481-486. ACM Press, 2003. [18] Thiemo Voigt, Hartmut Ritter, and Jochen Schiller. Utilizing solar power in wireless sensor networks. In LCN, 2003. [19] A. Kansal, J. Hsu, S. Zahedi, and M. B. Srivastava. Power management in energy harvesting sensor networks. Technical Report TR-UCLA-NESL200603-02, Networked and Embedded Systems Laboratory, UCLA,
March 2006.

Text documents are a valuable resource for virtually any enterprise and organization. Documents like papers, reports and general business documentations contain a large part of today"s (business) knowledge. Documents are mostly stored in a hierarchical folder structure on file servers and it is difficult to organize them in regard to classification, versioning etc., although it is of utmost importance that users can find, retrieve and edit up-to-date versions of documents whenever they want and, in a user-friendly way.
With most of the commonly used word-processing applications documents can be manipulated by only one user at a time: tools for pervasive collaborative document editing and management, are rarely deployed in today"s world. Despite the fact, that people strive for location- and time- independence, the importance of pervasive collaborative work, i.e. collaborative document editing and management is totally neglected. Documents could therefore be seen as a vulnerable source in today"s world, which demands for an appropriate solution: The need to store, retrieve and edit these documents collaboratively anytime, everywhere and with almost every suitable device and with guaranteed mechanisms for security, consistency, availability and access control, is obvious.
In addition, word processing systems ignore the fact that the history of a text document contains crucial information for its management.
Such meta data includes creation date, creator, authors, version, location-based information such as time and place when/where a user reads/edits a document and so on. Such meta data can be gathered during the documents creation process and can be used versatilely. Especially in the field of pervasive document management, meta data is of crucial importance since it offers totally new ways of organizing and classifying documents: On the one hand, the user"s actual situation influences the user"s objectives.
Meta data could be used to give the user the best possible view on the documents, dependent of his actual information. On the other hand, as soon as the user starts to work, i.e. reads or edits a document, new meta data can be gathered in order to make the system more adaptable and in a sense to the users situation and, to offer future users a better view on the documents.
As far as we know, no system exists, that satisfies the aforementioned requirements. A very good overview about realtime communication and collaboration system is described in [7].
We therefore strive for a pervasive document editing and management system, which enables pervasive (and collaborative) document editing and management: users should be able to read and edit documents whenever, wherever, with whomever and with whatever device.
In this paper, we present collaborative database-based real-time word processing, which provides pervasive document editing and management functionality. It enables the user to work on documents collaboratively and offers sophisticated document management facility: the user is always served with up-to-date documents and can organize and manage documents on the base of meta data. Additionally document data is treated as ‘first class citizen" of the database as demanded in [1].
The concept of our pervasive document editing and management system requires an appropriate architectural foundation. Our concept and implementation are based on the TeNDaX [3] collaborative database-based document editing and management system, which enables pervasive document editing and managing.
TeNDaX is a Text Native Database eXtension. It enables the storage of text in databases in a native form so that editing text is finally represented as real-time transactions. Under the term ‘text editing" we understand the following: writing and deleting text (characters), copying & pasting text, defining text layout & structure, inserting notes, setting access rights, defining business processes, inserting tables, pictures, and so on i.e. all the actions regularly carried out by word processing users. With ‘real-time transaction" we mean that editing text (e.g. writing a character/word) invokes one or several database transactions so that everything, which is typed appears within the editor as soon as these objects are stored persistently. Instead of creating files and storing them in a file system, the content and all of the meta data belonging to the documents is stored in a special way in the database, which enables very fast real-time transactions for all editing tasks [2].
The database schema and the above-mentioned transactions are created in such a way that everything can be done within a multiuser environment, as is usual done by database technology. As a consequence, many of the achievements (with respect to data organization and querying, recovery, integrity and security enforcement, multi-user operation, distribution management, uniform tool access, etc.) are now, by means of this approach, also available for word processing.
Our pervasive editing and management system is based on the above-mentioned database-based TeNDaX approach, where document data is stored natively in the database and supports pervasive collaborative text editing and document management.
We define the pervasive document editing and management system, as a system, where documents can easily be accessed and manipulated everywhere (within the network), anytime (independently of the number of users working on the same document) and with any device (desktop, notebook, PDA, mobile phone etc.).
DB 3 RTSC 4 RTSC 1 RTSC 2 RTSC 3 AS 1 AS 3 DB 1 DB 2 AS 2 AS 4 DB 4 A B C D E F G Figure 1. TeNDaX Application Architecture In contrast to documents stored locally on the hard drive or on a file server, our system automatically serves the user with the up-to-date version of a document and changes done on the document are stored persistently in the database and immediately propagated to all clients who are working on the same document. Additionally, meta data gathered during the whole document creation process enables sophisticated document management. With the TeXt SQL API as abstract interface, this approach can be used by any tool and for any device.
The system is built on the following components (see Figure 1): An editor in Java implements the presentation layer (A-G in Figure 1).
The aim of this layer is the integration in a well-known wordprocessing application such as OpenOffice.
The business logic layer represents the interface between the database and the word-processing application. It consists of the following three components: The application server (marked as AS 1-4 in Figure 1) enables text editing within the database environment and takes care of awareness, security, document management etc., all within a collaborative, real-time and multi-user environment. The real-time server component (marked as RTSC
updates between all of the connected editors.
The storage engine (data layer) primarily stores the content of documents as well as all related meta data within the database Databases can be distributed in a peer-to-peer network (DB 1-4 in Figure 1)..
In the following, we will briefly present the database schema, the editor and the real-time server component as well as the concept of dynamic folders, which enables sophisticated document management on the basis of meta data.
A database-based real-time collaborative editor allows the same document to be opened and edited simultaneously on the same computer or over a network of several computers and mobile devices. All concurrency issues, as well as message propagation, are solved within this approach, while multiple instances of the same document are being opened [3]. Each insert or delete action is a database transaction and as such, is immediately stored persistently in the database and propagated to all clients working on the same document.
As it was mentioned earlier that text is stored in a native way. Each character of a text document is stored as a single object in the database [3]. When storing text in such a native form, the performance of the employed database system is of crucial importance. The concept and performance issues of such a text database are described in [3], collaborative layouting in [2], dynamic collaborative business processes within documents in [5], the text editing creation time meta data model in [6] and the relation to XML databases in [7].
Figure 2 depicts the core database schema. By connecting a client to the database, a Session instance is created. One important attribute of the Session is the DocumentSession. This attribute refers to DocumentSession instances, which administrates all opened documents. For each opened document, a DocumentSession instance is created. The DocumentSession is important for the realtime server component, which, in case of a 42 is beforeis after Char (ID) has TextElement (ID) starts with is used by InternalFile (ID) is in includes created at has inserted by inserted is active ir ir CharacterValue (Unicode) has List (ID) starts starts with ends ends with FileSize has User (ID) last read by last written by created at created by Style DTD (ID) is used by uses uses is used by Authors arehas Description Password Picture UserColors UserListSecurity has has has has has has FileNode (ID) references/isreferencedby is dynamic DynStructure NodeDetails has has is NodeType is parent of has parent has Role (ID) created at created created by Name has Description is user Name has has main role FileNodeAccessMatrix (ID) has is AccessMatrix read option grand option write option contains has access Times opened … times with … by contains/ispartof ir ir is...andincludes Lineage (ID) references is after is before CopyPaste (ID) references is in is copy of is a copy from hasCopyPaste (ID) is activeLength has Str (Stream) has inserted by / inserted RegularChar StartChar EndChar File ExternalFile is from URL Type (extension) is of Title has DocumentSession (ID) is opened by has opened has opened Session (ID) isconnectedwith launched by VersionNumber uses has read option grand option write option ends with is used by is in has is unique DTD (Stream) has has Name Column (ID) has set on On/off isvisible…for false LanguageProfile (ID) has contains Name Profile Marking (ID) has parent internal is copy from hasRank is onPosition starts with ends with is logical style is itemized is italic is enumerated is underline is is part of Alignment Size has Font has hasColor is bold has uses ElementName StylesheetName isused by Process (ID) is running by OS is web session MainRoles Roles has has Timestamp (Date, Time) created at Timestamp (Date, Time) Timestamp (Date, Time) Timestamp (Date, Time) Timestamp (Date, Time)created at Type has Port IP has has MessagePropagator (ID) Picture (Stream) Name Picture (ID) has contains LayoutBlock WorkflowBlockLogicalBlock contains BlockDataType has property BlockData is of WorkflowInstance (ID) isin TaskInstance (ID) has parent Timestamp (Date, Time) Timestamp (Date, Time) Timestamp (Date, Time) Timestamp (Date, Time) last modified at completed at started at created at is on has Name created by has attached Comment Typeis of Timestamp (Date, Time) Timestamp (Date, Time) Timestamp (Date, Time) created at started at << last modified at is Category Editors has Status has Timestamp (Date, Time) << status last modified Timestamp (Date, Time) is due at DueType has Timezone has Notes has SecurityLevel hasset Timestamp (Date, Time) << is completed at isfollowedby Task (Code) Description has Indent references hasbeenopenedat...by Timestamp RedoHistory is before is after references hasCharCounter is inhas has Offset ActionID (Code) Timestamp (Date, Time) invoked at invoked by Version (ID) isbuild from has created byarchived has Comment Timestamp (Date, Time) <<createdat UndoHistory (ID) starts ends has Name created by Name has is before is after << references CharCounter has is in created at Timestamp is active created by is used by Offset has created at Timestamp Index (ID) lastmodifiedby Lexicon (ID) isof Frequency is occurring is stop word Term is is in ends with starts with << original starts with WordNumber SentenceNumber ParagraphNumber Citatons has is in is is in istemporary is in has Structure has ElementPath createdat Timestamp << describes SpiderBuild (ID) is updated is deleted Timestamp (Date, Time) <<lastupdatedat has validated structure <<neededtoindex Time (ms) IndexUpdate nextupdatein hasindexed isrunningbyOS lastupdate enabled Timestamp Time (s) Documents StopCharacter Description Character Value (ASCII) is sentence stop is paragraph stop Name has is is OptionsSettings show information show warningsshow exceptions do lineage recording do internal lineage recording ask for unknown source show intra document lineage information are set for X X X VirtualBorder (ID) isonhas {1, 2} {1, 2} ir ir UserMode (Code) UserMode (Code) Figure 2. TeNDaX Database Schema (Object Role Modeling Diagram) change on a document done by a client, is responsible for sending update information to all the clients working on the same document. The DocumentId in the class DocumentSession points to a FileNode instance, and corresponds to the ID of the opened document. Instances of the class FileNode either represent a folder node or a document node. The folder node corresponds to a folder of a file system and the document node to that of a file.
Instances of the class Char represent the characters of a document. The value of a character is stored in the attribute CharacterValue. The sequence is defined by the attributes After and Before of the class Char. Particular instances of Char mark the beginning and the end of a document. The methods InsertChars and RemoveChars are used to add and delete characters.
As seen above, each document is natively stored in the database.
Our editor does not have a replica of one part of the native text database in the sense of database replicas. Instead, it has a so-called image as its replica. Even if several authors edit the same text at the same time, they work on one unique document at all times. The system guarantees this unique view.
Editing a document involves a number of steps: first, getting the required information out of the image, secondly, invoking the corresponding methods within the database, thirdly, changing the image, and fourthly, informing all other clients about the changes.
The real-time server component is responsible for the real-time propagation of any changes on a document done within an editor to all the editors who are working or have opened the same document.
When an editor connects to the application server, which in turn connects to the database, the database also establishes a connection to the real-time server component (if there isn"t already a connection). The database system informs the real-time server component about each new editor session (session), which the realtime server component administrates in his SessionManager. Then, the editor as well connects to the real-time server component. The real-time server component adds the editor socket to the client"s data structure in the SessionManager and is then ready to communicate.
Each time a change on a document from an editor is persistently stored in the database, the database sends a message to the real-time server component, which in turns, sends the changes to all the 43 editors working on the same document. Therefore, a special communication protocol is used: the update protocol.
Update Protocol The real-time server component uses the update protocol to communicate with the database and the editors. Messages are sent from the database to the real-time server component, which sends the messages to the affected editors. The update protocol consists of different message types. Messages consist of two packages: package one contains information for the real-time server component whereas package two is passed to the editors and contains the update information, as depicted in Figure 3. || RTSC || Parameter | … | Parameter|| || Editor Data || Protocol between database system and real-time server component Protocol between real -time server component and editors Figure 3. Update Protocol In the following, two message types are presented: ||u|sessionId,...,sessionId||||editor data|| u: update message, sessionId: Id of the client session With this message type the real-time server component sends the editor data package to all editors specified in the sessionId list. ||ud|fileId||||editor data|| ud: update document message, fileId: Id of the file With this message type, the real-time server component sends the editor data to all editors who have opened the document with the indicated file-Id.
Class Model Figure 4 depicts the class model as well as the environment of the real-time server component. The environment consists mainly of the editor and the database, but any other client application that could make use of the real-time server component can connect.
ConnectionListener: This class is responsible for the connection to the clients, i.e. to the database and the editors. Depending on the connection type (database or editor) the connection is passed to an EditorWorker instance or DatabaseMessageWorker instance respectively.
EditorWorker: This class manages the connections of type ‘editor".
The connection (a socket and its input and output stream) is stored in the SessionManager.
SessionManager: This class is similar to an ‘in-memory database": all editor session information, e.g. the editor sockets, which editor has opened which document etc. are stored within this data structure.
DatabaseMessageWorker: This class is responsible for the connections of type ‘database". At run-time, only one connection exists for each database. Update messages from the database are sent to the DatabaseMessageWorker and, with the help of additional information from the SessionManager, sent to the corresponding clients.
ServiceClass: This class offers a set of methods for reading, writing and logging messages. tdb.mp.editor tdb.mp.database tdb.mp.mgmt EditorWorker DatabaseMessageWorker SessionManager MessageHandler ConnectionListener ServiceClass MessageQueue tdb.mp.listener tdb.mp.service junit.tests 1 * 1 *
1 * 1* 1 * Editors Datenbanksystem 1 2 1 * 1 * 1 * TCP/IP Figure 4. Real-Time Server Component Class Diagram
As mentioned above, every editing action invoked by a user is immediately transferred to the database. At the same time, more information about the current transaction is gathered.
As all information is stored in the database, one character can hold a multitude of information, which can later be used for the retrieval of documents. Meta data is collected at character level, from document structure (layout, workflow, template, semantics, security, workflow and notes), on the level of a document section and on the level of the whole document [6].
All of the above-mentioned meta data is crucial information for creating content and knowledge out of word processing documents.
This meta data can be used to create an alternative storage system for documents. In any case, it is not an easy task to change users" familiarity to the well known hierarchical file system. This is also the main reason why we do not completely disregard the classical file system, but rather enhance it. Folders which correspond to the classical hierarchical file system will be called static folders.
Folders where the documents are organized according to meta data, will be called dynamic folders. As all information is stored in the database, the file system, too, is based on the database.
The dynamic folders build up sub-trees, which are guided by the meta data selected by the user. Thus, the first step in using a dynamic folder is the definition of how it should be built. For each level of a dynamic folder, exactly one meta data item is used to. The following example illustrates the steps which have to be taken in order to define a dynamic folder, and the meta data which should be used.
As a first step, the meta data which will be used for the dynamic folder must be chosen (see Table 1): The sequence of the meta data influences the structure of the folder. Furthermore, for each meta data used, restrictions and granularity must be defined by the user; if no restrictions are defined, all accessible documents are listed.
The granularity therefore influences the number of sub-folders which will be created for the partitioning of the documents. 44 As the user enters the tree structure of the dynamic folder, he can navigate through the branches to arrive at the document(s) he is looking for. The directory names indicate which meta data determines the content of the sub-folder in question. At each level, the documents, which have so far been found to match the meta data, can be inspected.
Table 1. Defining dynamic folders (example) Level Meta data Restrictions Granularity
which have been created by the users Leone or Hodel or Gall One folder per creator
location Only show documents which where read at my current location One folder per task status
where at least 40% was written by user ‘Leone" Each 20% one folder ad-hoc changes of granularity and restrictions are possible in order to maximize search comfort for the user. It is possible to predefine dynamic folders for frequent use, e.g. a location-based folder, as well as to create and modify dynamic folders on an ad-hoc basis.
Furthermore, the content of such dynamic folders can change from one second to another, depending on the changes made by other users at that moment.
The proposed architecture is validated on the example of a character insertion. Insert operations are the mostly used operations in a (collaborative) editing system. The character insertion is based on the TeNDaX Insert Algorithm which is formally described in the following. The algorithm is simplified for this purpose.
The symbol c stands for the object character, p stands for the previous character, n stands for the next character of a character object c and the symbol l stands for a list of character objects. c = character p=previous character n = next character l = list of characters The symbol c1 stands for the first character in the list l, ci stands for a character in the list l at the position i, whereas i is a value between 1 and the length of the list l, and cn stands for the last character in the list l. c1 = first character in list l ci = character at position i in list l cn = last character in list l The symbol β stands for the special character that marks the beginning of a document and ε stands for the special character that marks the end of a document. β=beginning of document ε=end of document The function startTA starts a transaction. startTA = start transaction The function commitTA commits a transaction that was started. commitTA = commit transaction The function checkWriteAccess checks if the write access for a document session s is granted. checkWriteAccess(s) = check if write access for document session s is granted The function lock acquires an exclusive lock for a character c and returns 1 for a success and 0 for no success. lock(c) = acquire the lock for character c success : return 1, no success : return 0 The function releaseLocks releases all locks that a transaction has acquired so far. releaseLocks = release all locks The function getPrevious returns the previous character and getNext returns the next character of a character c. getPrevious(c) = return previous character of character c getNext(c) = return next character of character c The function linkBefore links a preceding character p with a succeeding character x and the function linkAfter links a succeeding character n with a preceding character y. linkBefore(p,x) = link character p to character x linkAfter(n,y) = link character n to character y The function updateString links a character p with the first character c1 of a character list l and a character n with the last character cn of a character list l updateString(l, p, n) = linkBefore(p cl)∧ linkAfter(n, cn ) The function insertChar inserts a character c in the table Char with the fields After set to a character p and Before set to a character n. insertChar(c, p, n) = linkAfter(c,p) ∧ linkBefore(c,n) ∧ linkBefore(p,c) ∧ linkAfter(n,c) The function checkPreceding determines the previous character's CharacterValue of a character c and if the previous character's status is active. checkPreceding(c) = return status and CharacterValue of the previous character The function checkSucceeding determines the next character's CharacterValue of a character c and if the next character's status is active. 45 checkSucceeding(c) = return status and CharacterValue of the next character The function checkCharValue determines the CharacterValue of a character c. checkCharValue(c) = return CharacterValue of character c The function sendUpdate sends an update message (UpdateMessage) from the database to the real-time server component. sendUpdate(UpdateMessage) The function Read is used in the real-time server component to read the UpdateMessage.
Read(UpdateInformationMessage) The function AllocatEditors checks on the base of the UpdateMessage and the SessionManager, which editors have to be informed.
AllocateEditors(UpdateInformationMessage, SessionManager) = returns the affected editors The function SendMessage(EditorData) sends the editor part of the UpdateMessage to the editors SendMessage(EditorData) In TeNDaX, the Insert Algorithm is implemented in the class method InsertChars of the class Char which is depicted in Figure
introduced in the following list: - nextCharacterOID: OID of the character situated next to the string to be inserted - previousCharacterOID: OID of the character situated previously to the string to be inserted - characterOIDs (List): List of character which have to be inserted Thus, the insertion of characters can be defined stepwise as follows: Start a transaction. startTA Select the character that is situated before the character that follows the string to be inserted. getPrevious(nextCharacterOID) = PrevChar(prevCharOID) ⇐ Π After ϑOID = nextCharacterOID(Char)) Acquire the lock for the character that is situated in the document before the character that follows the string which shall be inserted. lock(prevCharId) At this time the list characterOIDs contains the characters c1 to cn that shall be inserted. characterOIDs={ c1, …, cn } Each character of the string is inserted at the appropriate position by linking the preceding and the succeeding character to it.
For each character ci of characterOIDs: insertChar(ci, p, n) Whereas ci ∈ { c1,…, cn } Check if the preceding and succeeding characters are active or if it is the beginning or the end of the document. checkPreceding(prevCharOID) = IsOK(IsActive,
CharacterValue) ⇐ Π IsActive, CharacterValue (ϑ OID = nextCharacterOID(Char)) checkSucceeding(nextCharacterOID) = IsOK(IsActive,
CharacterValue)⇐ Π IsActive, CharacterValue (ϑ OID = nextCharacterOID(Char)) Update characters before and after the string to be inserted. updateString(characterOIDs, prevCharOID, nextCharacterOID) Release all locks and commit Transaction. releaseLocks commitTA Send update information to the real-time server component sendUpdate(UpdatenMessage) Read update message and inform affected editors of the change Read(UpdateMessage) Allocate Editors(UpdateMessage, SessionManager) SendMessage(EditorData)
Figure 1 gives a snapshot the system, i.e. of its architecture: four databases are distributed over a peer-to-peer network. Each database is connected to an application server (AS) and each application server is connected to a real-time server component (RTSC). Editors are connected to one or more real-time server components and to the corresponding databases.
Considering that editor A (connected to database 1 and 4) and editor B (connected to database 1 and 2) are working on the same document stored in database 1. Editor B now inserts a character into this document. The insert operation is passed to application server 1, which in turns, passes it to the database 1, where an insert operation is invoked; the characters are inserted according to the algorithm discussed in the previous section. After the insertion, database 1 sends an update message (according to the update protocol discussed before) to real-time server component 1 (via AS 1). RTCS 1 combines the received update information with the information in his SessionManager and sends the editor data to the affected editors, in this case to editor A and B, where the changes are immediately shown.
Occurring collaboration conflicts are solved and described in [3].
With the approach presented in this paper and the implemented prototype, we offer real-time collaborative editing and management of documents stored in a special way in a database. With this approach we provide security, consistency and availability of documents and consequently offer pervasive document editing and management. Pervasive document editing and management is enabled due to the proposed architecture with the embedded real46 time server component, which propagates changes to a document immediately and consequently offers up-to-date documents.
Document editing and managing is consequently enabled anywhere, anytime and with any device.
The above-descried system is implemented in a running prototype.
The system will be tested soon in line with a student workshop next autumn.
REFERENCES [1] Abiteboul, S., Agrawal, R., et al.: The Lowell Database Research Self Assessment. Massachusetts, USA, 2003. [2] Hodel, T. B., Businger, D., and Dittrich, K. R.: Supporting Collaborative Layouting in Word Processing. IEEE International Conference on Cooperative Information Systems (CoopIS), Larnaca, Cyprus, IEEE, 2004. [3] Hodel, T. B. and Dittrich, K. R.: "Concept and prototype of a collaborative business process environment for document processing." Data & Knowledge Engineering 52, Special Issue: Collaborative Business Process Technologies(1): 61120, 2005. [4] Hodel, T. B., Dubacher, M., and Dittrich, K. R.: Using Database Management Systems for Collaborative Text Editing. ACM European Conference of Computersupported Cooperative Work (ECSCW CEW 2003),
Helsinki, Finland, 2003. [5] Hodel, T. B., Gall, H., and Dittrich, K. R.: Dynamic Collaborative Business Processes within Documents. ACM Special Interest Group on Design of Communication (SIGDOC) , Memphis, USA, 2004. [6] Hodel, T. B., R. Hacmac, and Dittrich, K. R.: Using Text Editing Creation Time Meta Data for Document Management. Conference on Advanced Information Systems Engineering (CAiSE'05), Porto, Portugal, Springer Lecture Notes, 2005. [7] Hodel, T. B., Specker, F. and Dittrich, K. R.: Embedded SOAP Server on the Operating System Level for ad-hoc Automatic Real-Time Bidirectional Communication.
Information Resources Management Association (IRMA),

Wide-area peer-to-peer file systems [2,5,22,32,33], peer-to-peer caches [15, 16], and web caches [6, 10] have become popular over the last few years. Caching1 of files in selected servers is widely used to enhance the performance, availability, and reliability of these systems. However, most such systems assume that servers cooperate with one another by following protocols optimized for overall system performance, regardless of the costs incurred by each server.
In reality, servers may behave selfishly - seeking to maximize their own benefit. For example, parties in different administrative domains utilize their local resources (servers) to better support clients in their own domains. They have obvious incentives to cache objects2 that maximize the benefit in their domains, possibly at the expense of globally optimum behavior. It has been an open question whether these caching scenarios and protocols maintain their desirable global properties (low total social cost, for example) in the face of selfish behavior.
In this paper, we take a game-theoretic approach to analyzing the problem of caching in networks of selfish servers through theoretical analysis and simulations. We model selfish caching as a non-cooperative game. In the basic model, the servers have two possible actions for each object. If a replica of a requested object is located at a nearby node, the server may be better off accessing the remote replica. On the other hand, if all replicas are located too far away, the server is better off caching the object itself. Decisions about caching the replicas locally are arrived at locally, taking into account only local costs. We also define a more elaborate payment model, in which each server bids for having an object replicated at another site. Each site now has the option of replicating an object and collecting the related bids. Once all servers have chosen a strategy, each game specifies a configuration, that is, the set of servers that replicate the object, and the corresponding costs for all servers.
Game theory predicts that such a situation will end up in a Nash equilibrium, that is, a set of (possibly randomized) strategies with the property that no player can benefit by changing its strategy while the other players keep their strategies unchanged [28].
Foundational considerations notwithstanding, it is not easy to accept randomized strategies as the behavior of rational agents in a distributed system (see [28] for an extensive discussion) - but this is what classical game theory can guarantee. In certain very fortunate situations, however (see [9]), the existence of pure (that is, deterministic) Nash equilibria can be predicted.
With or without randomization, however, the lack of coordination inherent in selfish decision-making may incur costs well beyond what would be globally optimum. This loss of efficiency is 1 We will use caching and replication interchangeably. 2 We use the term object as an abstract entity that represents files and other data objects. 21 quantified by the price of anarchy [21]. The price of anarchy is the ratio of the social (total) cost of the worst possible Nash equilibrium to the cost of the social optimum. The price of anarchy bounds the worst possible behavior of a selfish system, when left completely on its own. However, in reality there are ways whereby the system can be guided, through seeding or incentives, to a preselected Nash equilibrium. This optimistic version of the price of anarchy [3] is captured by the smallest ratio between a Nash equilibrium and the social optimum.
In this paper we address the following questions : • Do pure strategy Nash equilibria exist in the caching game? • If pure strategy Nash equilibria do exist, how efficient are they (in terms of the price of anarchy, or its optimistic counterpart) under different placement costs, network topologies, and demand distributions? • What is the effect of adopting payments? Will the Nash equilibria be improved?
We show that pure strategy Nash equilibria always exist in the caching game. The price of anarchy of the basic game model can be O(n), where n is the number of servers; the intuitive reason is undersupply. Under certain topologies, the price of anarchy does have tighter bounds. For complete graphs and stars, it is O(1). For D-dimensional grids, it is O(n D D+1 ). Even the optimistic price of anarchy can be O(n). In the payment model, however, the game can always implement a Nash equilibrium that is same as the social optimum, so the optimistic price of anarchy is one.
Our simulation results show several interesting phases. As the placement cost increases from zero, the price of anarchy increases.
When the placement cost first exceeds the maximum distance between servers, the price of anarchy is at its highest due to undersupply problems. As the placement cost further increases, the price of anarchy decreases, and the effect of replica misplacement dominates the price of anarchy.
The rest of the paper is organized as follows. In Section 2 we discuss related work. Section 3 discusses details of the basic game and analyzes the bounds of the price of anarchy. In Section 4 we discuss the payment game and analyze its price of anarchy. In Section 5 we describe our simulation methodology and study the properties of Nash equilibria observed. We discuss extensions of the game and directions for future work in Section 6.
There has been considerable research on wide-area peer-to-peer file systems such as OceanStore [22], CFS [5], PAST [32],
FARSITE [2], and Pangaea [33], web caches such as NetCache [6] and SummaryCache [10], and peer-to-peer caches such as Squirrel [16].
Most of these systems use caching for performance, availability, and reliability. The caching protocols assume obedience to the protocol and ignore participants" incentives. Our work starts from the assumption that servers are selfish and quantifies the cost of the lack of coordination when servers behave selfishly.
The placement of replicas in the caching problem is the most important issue. There is much work on the placement of web replicas, instrumentation servers, and replicated resources. All protocols assume obedience and ignore participants" incentives. In [14],
Gribble et al. discuss the data placement problem in peer-to-peer systems. Ko and Rubenstein propose a self-stabilizing, distributed graph coloring algorithm for the replicated resource placement [20].
Chen, Katz, and Kubiatowicz propose a dynamic replica placement algorithm exploiting underlying distributed hash tables [4].
Douceur and Wattenhofer describe a hill-climbing algorithm to exchange replicas for reliability in FARSITE [8]. RaDar is a system that replicates and migrates objects for an Internet hosting service [31]. Tang and Chanson propose a coordinated en-route web caching that caches objects along the routing path [34].
Centralized algorithms for the placement of objects, web proxies, mirrors, and instrumentation servers in the Internet have been studied extensively [18,19,23,30].
The facility location problem has been widely studied as a centralized optimization problem in theoretical computer science and operations research [27]. Since the problem is NP-hard, approximation algorithms based on primal-dual techniques, greedy algorithms, and local search have been explored [17, 24, 26]. Our caching game is different from all of these in that the optimization process is performed among distributed selfish servers.
There is little research in non-cooperative facility location games, as far as we know. Vetta [35] considers a class of problems where the social utility is submodular (submodularity means decreasing marginal utility). In the case of competitive facility location among corporations he proves that any Nash equilibrium gives an expected social utility within a factor of 2 of optimal plus an additive term that depends on the facility opening cost. Their results are not directly applicable to our problem, however, because we consider each server to be tied to a particular location, while in their model an agent is able to open facilities in multiple locations. Note that in that paper the increase of the price of anarchy comes from oversupply problems due to the fact that competing corporations can open facilities at the same location. On the other hand, the significant problems in our game are undersupply and misplacement.
In a recent paper, Goemans et al. analyze content distribution on ad-hoc wireless networks using a game-theoretic approach [12]. As in our work, they provide monetary incentives to mobile users for caching data items, and provide tight bounds on the price of anarchy and speed of convergence to (approximate) Nash equilibria.
However, their results are incomparable to ours because their payoff functions neglect network latencies between users, they consider multiple data items (markets), and each node has a limited budget to cache items.
Cost sharing in the facility location problem has been studied using cooperative game theory [7, 13, 29]. Goemans and Skutella show strong connections between fair cost allocations and linear programming relaxations for facility location problems [13]. P´al and Tardos develop a method for cost-sharing that is approximately budget-balanced and group strategyproof and show that the method recovers 1/3 of the total cost for the facility location game [29].
Devanur, Mihail, and Vazirani give a strategyproof cost allocation for the facility location problem, but cannot achieve group strategyproofness [7].
The caching problem we study is to find a configuration that meets certain objectives (e.g., minimum total cost). Figure 1 shows examples of caching among four servers. In network (a), A stores an object. Suppose B wants to access the object. If it is cheaper to access the remote replica than to cache it, B accesses the remote replica as shown in network (b). In network (c), C wants to access the object. If C is far from A, C caches the object instead of accessing the object from A. It is possible that in an optimal configuration it would be better to place replicas in A and B. Understanding the placement of replicas by selfish servers is the focus of our study.
The caching problem is abstracted as follows. There is a set N of n servers and a set M of m objects. The distance between servers can be represented as a distance matrix D (i.e., dij is the distance 22 Server Server Server Server A B C D (a) Server Server Server Server A B C D (b) Server Server Server Server A B C D (c) Figure 1: Caching. There are four servers labeled A, B, C, and D. The rectangles are object replicas. In (a), A stores an object. If B incurs less cost accessing A"s replica than it would caching the object itself, it accesses the object from A as in (b). If the distance cost is too high, the server caches the object itself, as C does in (c). This figure is an example of our caching game model. from server i to server j). D models an underlying network topology. For our analysis we assume that the distances are symmetric and the triangle inequality holds on the distances (for all servers i, j, k: dij + djk ≥ dik). Each server has demand from clients that is represented by a demand matrix W (i.e., wij is the demand of server i for object j). When a server caches objects, the server incurs some placement cost that is represented by a matrix α (i.e., αij is a placement cost of server i for object j).
In this study, we assume that servers have no capacity limit. As we discuss in the next section, this fact means that the caching behavior with respect to each object can be examined separately.
Consequently, we can talk about configurations of the system with respect to a given object: DEFINITION 1. A configuration X for some object O is the set of servers replicating this object.
The goal of the basic game is to find configurations that are achieved when servers optimize their cost functions locally.
We take a game-theoretic approach to analyzing the uncapacitated caching problem among networked selfish servers. We model the selfish caching problem as a non-cooperative game with n players (servers/nodes) whose strategies are sets of objects to cache. In the game, each server chooses a pure strategy that minimizes its cost. Our focus is to investigate the resulting configuration, which is the Nash equilibrium of the game. It should be emphasized that we consider only pure strategy Nash equilibria in this paper.
The cost model is an important part of the game. Let Ai be the set of feasible strategies for server i, and let Si ∈ Ai be the strategy chosen by server i. Given a strategy profile S = (S1, S2, ..., Sn), the cost incurred by server i is defined as: Ci(S) = j∈Si αij + j /∈Si wij di (i,j). (1) where αij is the placement cost of object j, wij is the demand that server i has for object j, (i, j) is the closest server to i that caches object j, and dik is the distance between i and k. When no server caches the object, we define distance cost di (i,j) to be dM -large enough that at least one server will choose to cache the object.
The placement cost can be further divided into first-time installation cost and maintenance cost: αij = k1i + k2i UpdateSizej ObjectSizej 1 T Pj k wkj , (2) where k1i is the installation cost, k2i is the relative weight between the maintenance cost and the installation cost, Pj is the ratio of the number of writes over the number of reads and writes,
UpdateSizej is the size of an update, ObjectSizej is the size of the object, and T is the update period. We see tradeoffs between different parameters in this equation. For example, placing replicas becomes more expensive as UpdateSizej increases, Pj increases, or T decreases. However, note that by varying αij itself we can capture the full range of behaviors in the game. For our analysis, we use only αij .
Since there is no capacity limit on servers, we can look at each single object as a separate game and combine the pure strategy equilibria of these games to obtain a pure strategy equilibrium of the multi-object game. Fabrikant, Papadimitriou, and Talwar discuss this existence argument: if two games are known to have pure equilibria, and their cost functions are cross-monotonic, then their union is also guaranteed to have pure Nash equilibria, by a continuity argument [9]. A Nash equilibrium for the multi-object game is the cross product of Nash equilibria for single-object games.
Therefore, we can focus on the single object game in the rest of this paper.
For single object selfish caching, each server i has two strategies - to cache or not to cache. The object under consideration is j.
We define Si to be 1 when server i caches j and 0 otherwise. The cost incurred by server i is Ci(S) = αij Si + wij di (i,j)(1 − Si). (3) We refer to this game as the basic game. The extent to which Ci(S) represents actual cost incurred by server i is beyond the scope of this paper; we will assume that an appropriate cost function of the form of Equation 3 can be defined.
In principle, we can start with a random configuration and let this configuration evolve as each server alters its strategy and attempts to minimize its cost. Game theory is interested in stable solutions called Nash equilibria. A pure strategy Nash equilibrium is reached when no server can benefit by unilaterally changing its strategy. A Nash equilibrium3 (S∗ i , S∗ −i) for the basic game specifies a configuration X such that ∀i ∈ N, i ∈ X ⇔ S∗ i = 1.
Thus, we can consider a set E of all pure strategy Nash equilibrium configurations: X ∈ E ⇔ ∀i ∈ N, ∀Si ∈ Ai, Ci(S∗ i , S∗ −i) ≤ Ci(Si, S∗ −i) (4) By this definition, no server has incentive to deviate in the configurations since it cannot reduce its cost.
For the basic game, we can easily see that: X ∈ E ⇔ ∀i ∈ N, ∃j ∈ X s.t. dji ≤ α and ∀j ∈ X, ¬∃k ∈ X s.t. dkj < α (5) The first condition guarantees that there is a server that places the replica within distance α of each server i. If the replica is not placed 3 The notation for strategy profile (S∗ i , S∗ −i) separates node i s strategy (S∗ i ) from the strategies of other nodes (S∗ −i). 23 A B1−α 0 0 0 0
0 0 0 0 2 n nodes 2 n nodes (a) A B1−α 0 0 0 0
0 0 0 0 2 n nodes 2 n nodes (b) A B1−α 2 n nodes 2 n nodes n2 n2 n2 n2 n2 n2 n2 n2 n2 n2 (c) Figure 2: Potential inefficiency of Nash equilibria illustrated by two clusters of n 2 servers. The intra-cluster distances are all zero and the distance between clusters is α − 1, where α is the placement cost. The dark nodes replicate the object. Network (a) shows a Nash equilibrium in the basic game, where one server in a cluster caches the object. Network (b) shows the social optimum where two replicas, one for each cluster, are placed. The price of anarchy is O(n) and even the optimistic price of anarchy is O(n). This high price of anarchy comes from the undersupply of replicas due to the selfish nature of servers. Network (c) shows a Nash equilibrium in the payment game, where two replicas, one for each cluster, are placed. Each light node in each cluster pays 2/n to the dark node, and the dark node replicates the object. Here, the optimistic price of anarchy is one. at i, then it is placed at another server within distance α of i, so i has no incentive to cache. If the replica is placed at i, then the second condition ensures there is no incentive to drop the replica because no two servers separated by distance less than α both place replicas.
The social cost of a given strategy profile is defined as the total cost incurred by all servers, namely: C(S) = n−1 i=0 Ci(S) (6) where Ci(S) is the cost incurred by server i given by Equation 1.
The social optimum cost, referred to as C(SO) for the remainder of the paper, is the minimum social cost. The social optimum cost will serve as an important base case against which to measure the cost of selfish caching. We define C(SO) as: C(SO) = min S C(S) (7) where S varies over all possible strategy profiles. Note that in the basic game, this means varying configuration X over all possible configurations. In some sense, C(SO) represents the best possible caching behavior - if only nodes could be convinced to cooperate with one another.
The social optimum configuration is a solution of a mini-sum facility location problem, which is NP-hard [11]. To find such configurations, we formulate an integer programming problem: minimize Èi Èj ¢αij xij + Èk wij dikyijk £ subject to ∀i, j Èk yijk = I(wij) ∀i, j, k xij − ykji ≥ 0 ∀i, j xij ∈ {0, 1} ∀i, j, k yijk ∈ {0, 1} (8) Here, xij is 1 if server i replicates object j and 0 otherwise; yijk is 1 if server i accesses object j from server k and 0 otherwise; I(w) returns 1 if w is nonzero and 0 otherwise. The first constraint specifies that if server i has demand for object j, then it must access j from exactly one server. The second constraint ensures that server i replicates object j if any other server accesses j from i.
To analyze the basic game, we first give a proof of the existence of pure strategy Nash equilibria. We discuss the price of anarchy in general and then on specific underlying topologies. In this analysis we use simply α in place of αij , since we deal with a single object and we assume placement cost is the same for all servers. In addition, when we compute the price of anarchy, we assume that all nodes have the same demand (i.e., ∀i ∈ N wij = 1).
THEOREM 1. Pure strategy Nash equilibria exist in the basic game.
PROOF. We show a constructive proof. First, initialize the set V to N. Then, remove all nodes with zero demand from V . Each node x defines βx, where βx = α wxj . Furthermore, let Z(y) = {z : dzy ≤ βz, z ∈ V }; Z(y) represents all nodes z for which y lies within βz from z.
Pick a node y ∈ V such that βy ≤ βx for all x ∈ V . Place a replica at y and then remove y and all z ∈ Z(y) from V . No such z can have incentive to replicate the object because it can access y"s replica at lower (or equal) cost. Iterate this process of placing replicas until V is empty. Because at each iteration y is the remaining node with minimum β, no replica will be placed within distance βy of any such y by this process. The resulting configuration is a pure-strategy Nash equilibrium of the basic game.
The Price of Anarchy (POA): To quantify the cost of lack of coordination, we use the price of anarchy [21] and the optimistic price of anarchy [3]. The price of anarchy is the ratio of the social costs of the worst-case Nash equilibrium and the social optimum, and the optimistic price of anarchy is the ratio of the social costs of the best-case Nash equilibrium and the social optimum.
We show general bounds on the price of anarchy. Throughout our discussion, we use C(SW ) to represent the cost of worst case Nash equilibrium, C(SO) to represent the cost of social optimum, and PoA to represent the price of anarchy, which is C(SW ) C(SO) .
The worst case Nash equilibrium maximizes the total cost under the constraint that the configuration meets the Nash condition.
Formally, we can define C(SW ) as follows.
C(SW ) = max X∈E (α|X| + i min j∈X dij) (9) where minj∈X dij is the distance to the closest replica (including i itself) from node i and X varies through Nash equilibrium configurations.
Bounds on the Price of Anarchy: We show bounds of the price of anarchy varying α. Let dmin = min(i,j)∈N×N,i=j dij and dmax = max(i,j)∈N×N dij . We see that if α ≤ dmin, PoA = 1 24 Topology PoA Complete graph 1 Star ≤ 2 Line O( √ n) D-dimensional grid O(n D D+1 ) Table 1: PoA in the basic game for specific topologies trivially, since every server caches the object for both Nash equilibrium and social optimum. When α > dmax, there is a transition in Nash equilibria: since the placement cost is greater than any distance cost, only one server caches the object and other servers access it remotely. However, the social optimum may still place multiple replicas. Since α ≤ C(SO) ≤ α+minj∈N Èi dij when α > dmax, we obtain α+maxj∈N Èi dij α+minj∈N Èi dij ≤ PoA ≤ α+maxj∈N Èi dij α .
Note that depending on the underlying topology, even the lower bound of PoA can be O(n). Finally, there is a transition when α > maxj∈N Èi dij. In this case, PoA = α+maxj∈N Èi dij α+minj∈N Èi dij and it is upper bounded by 2.
Figure 2 shows an example of the inefficiency of a Nash equilibrium. In the network there are two clusters of servers whose size is n 2 . The distance between two clusters is α − 1 where α is the placement cost. Figure 2(a) shows a Nash equilibrium where one server in a cluster caches the object. In this case, C(SW ) = α + (α − 1)n 2 , since all servers in the other cluster accesses the remote replica. However, the social optimum places two replicas, one for each cluster, as shown in Figure 2(b). Therefore, C(SO) = 2α.
PoA = α+(α−1) n 2 2α , which is O(n). This bad price of anarchy comes from an undersupply of replicas due to the selfish nature of the servers. Note that all Nash equilibria have the same cost; thus even the optimistic price of anarchy is O(n).
In Appendix A, we analyze the price of anarchy with specific underlying topologies and show that PoA can have tighter bounds than O(n) for the complete graph, star, line, and D-dimensional grid. In these topologies, we set the distance between directly connected nodes to one. We describe the case where α > 1, since PoA = 1 trivially when α ≤ 1. A summary of the results is shown in Table 1.
In this section, we present an extension to the basic game with payments and analyze the price of anarchy and the optimistic price of anarchy of the game.
The new game, which we refer to as the payment game, allows each player to offer a payment to another player to give the latter incentive to replicate the object. The cost of replication is shared among the nodes paying the server that replicates the object.
The strategy for each player i is specified by a triplet (vi, bi, ti) ∈ {N, Ê+, Ê+}. vi specifies the player to whom i makes a bid, bi ≥ 0 is the value of the bid, and ti ≥ 0 denotes a threshold for payments beyond which i will replicate the object. In addition, we use Ri to denote the total amount of bids received by a node i (Ri = Èj:vj =i bj).
A node i replicates the object if and only if Ri ≥ ti, that is, the amount of bids it receives is greater than or equal to its threshold.
Let Ii denote the corresponding indicator variable, that is, Ii equals
if a node i makes a bid to another node j and j replicates the object, then i must pay j the amount bi. If j does not replicate the object, i does not pay j.
Given a strategy profile, the outcome of the game is the set of tuples {(Ii, vi, bi, Ri)}. Ii tells us whether player i replicates the object or not, bi is the payment player i makes to player vi, and Ri is the total amount of bids received by player i. To compute the payoffs given the outcome, we must now take into account the payments a node makes, in addition to the placement costs and access costs of the basic game.
By our rules, a server node i pays bi to node vi if vi replicates the object, and receives a payment of Ri if it replicates the object itself. Its net payment is biIvi − RiIi. The total cost incurred by each node is the sum of its placement cost, access cost, and net payment. It is defined as Ci(S) = αij Ii + wij di (i,j)(1 − Ii) + biIvi − RiIi. (10) The cost of social optimum for the payment game is same as that for the basic game, since the net payments made cancel out.
In analyzing the payment model, we first show that a Nash equilibrium in the basic game is also a Nash equilibrium in the payment game. We then present an important positive result - in the payment game the socially optimal configuration can always be implemented by a Nash equilibrium. We know from the counterexample in Figure 2 that this is not guaranteed in the the basic game. In this analysis we use α to represent αij .
THEOREM 2. Any configuration that is a pure strategy Nash equilibrium in the basic game is also a pure strategy Nash equilibrium in the payment game. Therefore, the price of anarchy of the payment game is at least that of the basic game.
PROOF. Consider any Nash equilibrium configuration in the basic game. For each node i replicating the object, set its threshold ti to 0; everyone else has threshold α. Also, for all i, bi = 0.
A node that replicates the object does not have incentive to change its strategy: changing the threshold does not decrease its cost, and it would have to pay at least α to access a remote replica or incentivize a nearby node to cache. Therefore it is better off keeping its threshold and bid at 0 and replicating the object.
A node that is not replicating the object can access the object remotely at a cost less than or equal to α. Lowering its threshold does not decrease its cost, since all bi are zero. The payment necessary for another server to place a replica is at least α.
No player has incentive to deviate, so the current configuration is a Nash equilibrium.
In fact, Appendix B shows that the PoA of the payment game can be more than that of the basic game in a given topology.
Now let us look at what happens to the example shown in Figure 2 in the best case. Suppose node B"s neighbors each decide to pay node B an amount 2/n. B does not have an incentive to deviate, since accessing the remote replica does not decrease its cost. The same argument holds for A because of symmetry in the graph. Since no one has an incentive to deviate, the configuration is a Nash equilibrium. Its total cost is 2α, the same as in the socially optimal configuration shown in Figure 2(b). Next we prove that indeed the payment game always has a strategy profile that implements the socially optimal configuration as a Nash equilibrium. We first present the following observation, which is used in the proof, about thresholds in the payment game.
OBSERVATION 1. If node i replicates the object, j is the nearest node to i among the other nodes that replicate the object, and dij < α in a Nash equilibrium, then i should have a threshold at 25 least (α − dij). Otherwise, it cannot collect enough payment to compensate for the cost of replicating the object and is better off accessing the replica at j.
THEOREM 3. In the payment game, there is always a pure strategy Nash equilibrium that implements the social optimum configuration. The optimistic price of anarchy in the payment game is therefore always one.
PROOF. Consider the socially optimal configuration φopt. Let No be the set of nodes that replicate the object and Nc = N − No be the rest of the nodes. Also, for each i in No, let Qi denote the set of nodes that access the object from i, not including i itself. In the socially optimal configuration, dij ≤ α for all j in Qi.
We want to find a set of payments and thresholds that makes this configuration implementable. The idea is to look at each node i in No and distribute the minimum payment needed to make i replicate the object among the nodes that access the object from i. For each i in No, and for each j in Qi, we define δj = min{α, min k∈No−{i} djk} − dji (11) Note that δj is the difference between j"s cost for accessing the replica at i and j"s next best option among replicating the object and accessing some replica other than i. It is clear that δj ≥ 0.
CLAIM 1. For each i ∈ No, let be the nearest node to i in No. Then,
Èj∈Qi δj ≥ α − di .
PROOF. (of claim) Assume the contrary, that is,
Èj∈Qi δj < α − di . Consider the new configuration φnew wherein i does not replicate and each node in Qi chooses its next best strategy (either replicating or accessing the replica at some node in No − {i}). In addition, we still place replicas at each node in No − {i}. It is easy to see that cost of φopt minus cost of φnew is at least: (α + j∈Qi dij) − (di + j∈Qi min{α, min k∈No−{i} dik}) = α − di − j∈Qi δj > 0, which contradicts the optimality of φopt.
We set bids as follows. For each i in No, bi = 0 and for each j in Qi, j bids to i (i.e., vj = i) the amount: bj = max{0, δj − i/(|Qi| + 1)}, j ∈ Qi (12) where i = Èj∈Qi δj − α + di ≥ 0 and |Qi| is the cardinality of Qi. For the thresholds, we have: ti = α if i ∈ Nc;Èj∈Qi bj if i ∈ No. (13) This fully specifies the strategy profile of the nodes, and it is easy to see that the outcome is indeed the socially optimal configuration.
Next, we verify that the strategies stipulated constitute a Nash equilibrium. Having set ti to α for i in Nc means that any node in N is at least as well off lowering its threshold and replicating as bidding α to some node in Nc to make it replicate, so we may disregard the latter as a profitable strategy. By observation 1, to ensure that each i in No does not deviate, we require that if is the nearest node to i in No, then Èj∈Qi bj is at least (α − di ).
Otherwise, i will raise ti above Èj∈Qi bj so that it does not replicate and instead accesses the replica at . We can easily check that j∈Qi bj ≥ j∈Qi δj − |Qi| i |Qi| + 1 = α − di + i |Qi| + 1 ≥ α − di . 1
2
3
4
5
1 10 100 C(NE)/C(SO) AverageNumberofReplicas alpha PoA Ratio OPoA Replica (SO) Replica (NE) Figure 3: We present P oA, Ratio, and OP oA results for the basic game, varying α on a 100-node line topology, and we show number of replicas placed by the Nash equilibria and by the optimal solution.
We see large peaks in P oA and OP oA at α = 100, where a phase transition causes an abrupt transition in the lines.
Therefore, each node i ∈ No does not have incentive to change ti since i loses its payments received or there is no change, and i does not have incentive to bi since it replicates the object. Each node j in Nc has no incentive to change tj since changing tj does not reduce its cost. It also does not have incentive to reduce bj since the node where j accesses does not replicate and j has to replicate the object or to access the next closest replica, which costs at least the same from the definition of bj . No player has incentive to deviate, so this strategy profile is a Nash equilibrium.
We run simulations to compare Nash equilibria for the singleobject caching game with the social optimum computed by solving the integer linear program described in Equation 8 using Mosek [1].
We examine price of anarchy (PoA), optimistic price of anarchy (OPoA), and the average ratio of the costs of Nash equilibria and social optima (Ratio), and when relevant we also show the average numbers of replicas placed by the Nash equilibrium (Replica(NE)) and the social optimum (Replica(SO)). The PoA and OPoA are taken from the worst and best Nash equilibria, respectively, that we observe over the runs. Each data point in our figures is based on
order. The details of the simulations including protocols and a discussion of convergence are presented in Appendix C.
In our evaluation, we study the effects of variation in four categories: placement cost, underlying topology, demand distribution, and payments. As we vary the placement cost α, we directly influence the tradeoff between caching and not caching. In order to get a clear picture of the dependency of PoA on α in a simple case, we first analyze the basic game with a 100-node line topology whose edge distance is one.
We also explore transit-stub topologies generated using the GTITM library [36] and power-law topologies (Router-level BarabasiAlbert model) generated using the BRITE topology generator [25].
For these topologies, we generate an underlying physical graph of
average, and maximum physical node distances. The average distance is 0.42. We create an overlay of 100 server nodes and use the same overlay for all experiments with the given topology.
In the game, each server has a demand whose distribution is Bernoulli(p), where p is the probability of having demand for the object; the default unless otherwise specified is p = 1.0. 26
1
2
3
1 10 100 C(NE)/C(SO) AverageNumberofReplicas alpha PoA Ratio OPoA Replica (SO) Replica (NE) (a)
1
2
3
1 10 100 C(NE)/C(SO) AverageNumberofReplicas alpha PoA Ratio OPoA Replica (SO) Replica (NE) (b) Figure 4: Transit-stub topology: (a) basic game, (b) payment game. We show the P oA, Ratio, OP oA, and the number of replicas placed while varying α between 0 and 2 with 100 servers on a 3050-physical-node transit-stub topology.
1
2
1 10 100 C(NE)/C(SO) AverageNumberofReplicas alpha PoA Ratio OPoA Replica (SO) Replica (NE) (a)
1
2
1 10 100 C(NE)/C(SO) AverageNumberofReplicas alpha PoA Ratio OPoA Replica (SO) Replica (NE) (b) Figure 5: Power-law topology: (a) basic game, (b) payment game. We show the P oA, Ratio, OP oA, and the number of replicas placed while varying α between 0 and 2 with 100 servers on a 3050-physical-node power-law topology.
Figure 3 shows PoA, OPoA, and Ratio, as well as number of replicas placed, for the line topology as α varies. We observe two phases. As α increases the PoA rises quickly to a peak at
behavior similar to PoA.
These behaviors can be explained by examining the number of replicas placed by Nash equilibria and by optimal solutions. We see that when α is above one, Nash equilibrium solutions place fewer replicas than optimal on average. For example, when α is 100, the social optimum places four replicas, but the Nash equilibrium places only one. The peak in PoA at α = 100 occurs at the point for a 100-node line where the worst-case cost of accessing a remote replica is slightly less than the cost of placing a new replica, so selfish servers will never place a second replica. The optimal solution, however, places multiple replicas to decrease the high global cost of access. As α continues to increase, the undersupply problem lessens as the optimal solution places fewer replicas.
In Figure 4(a) we examine an overlay graph on the more realistic transit-stub topology. The trends for the PoA, OPoA, and Ratio are similar to the results for the line topology, with a peak in PoA at α = 0.8 due to maximal undersupply.
In Figure 5(a) we examine an overlay graph on the power-law topology. We observe several interesting differences between the power-law and transit-stub results. First, the PoA peaks at a lower level in the power-law graph, around 2.3 (at α = 0.9) while the peak PoA in the transit-stub topology is almost 3.0 (at α = 0.8).
After the peak, PoA and Ratio decrease more slowly as α increases. OPoA is close to one for the whole range of α values.
This can be explained by the observation in Figure 5(a) that there is no significant undersupply problem here like there was in the transit-stub graph. Indeed the high PoA is due mostly to misplacement problems when α is from 0.7 to 2.0, since there is little decrease in PoA when the number of replicas in social optimum changes from two to one. The OPoA is equal to one in the figure when the same number of replicas are placed.
Now we examine the effects of varying the demand distribution.
The set of servers with demand is random for p < 1, so we calculate the expected PoA by averaging over 5 trials (each data point is based on 5000 runs). We run simulations for demand levels of p ∈ {0.2, 0.6, 1.0} as α is varied on the 100 servers on top of the transit-stub graph. We observe that as demand falls, so does expected PoA. As p decreases, the number of replicas placed in the social optimum decreases, but the number in Nash equilibria changes little. Furthermore, when α exceeds the overlay diameter, the number in Nash equilibria stays constant when p varies.
Therefore, lower p leads to a lesser undersupply problem, agreeing with intuition. We do not present the graph due to space limitations and redundancy; the PoA for p = 1.0 is identical to PoA in Figure 4(a), and the lines for p = 0.6 and p = 0.2 are similar but lower and flatter. 27
Finally, we discuss the effects of payments on the efficiency of Nash equilibria. The results are presented in Figure 4(b) and Figure 5(b). As shown in the analysis, the simulations achieve OPoA close to one (it is not exactly one because of randomness in the simulations). The Ratio for the payment game is much lower than the Ratio for the basic game, since the protocol for the payment game tends to explore good regions in the space of Nash equilibria. We observe in Figure 4 that for α ≥ 0.4, the average number of replicas of Nash equilibria gets closer with payments to that of the social optimum than it does without. We observe in Figure 5 that more replicas are placed with payments than without when α is between 0.7 and 1.3, the only range of significant undersupply in the power-law case. The results confirm that payments give servers incentive to replicate the object and this leads to better equilibria.
We suggest several interesting extensions and directions. One extension is to consider multiple objects in the capacitated caching game, in which servers have capacity limits when placing objects.
Since caching one object affects the ability to cache another, there is no separability of a multi-object game into multiple single object games. As studied in [12], one way to formulate this problem is to find the best response of a server by solving a knapsack problem and to compute Nash equilibria.
In our analyses, we assume that all nodes have the same demand.
However, nodes could have different demand depending on objects.
We intend to examine the effects of heterogeneous demands (or heterogeneous placement costs) analytically. We also want to look at the following aggregation effect. Suppose there are n − 1 clustered nodes with distance of α−1 from a node hosting a replica.
All nodes have demands of one. In that case, the price of anarchy is O(n). However, if we aggregate n − 1 nodes into one node with demand n − 1, the price of anarchy becomes O(1), since α should be greater than (n − 1)(α − 1) to replicate only one object. Such aggregation can reduce the inefficiency of Nash equilibria.
We intend to compute the bounds of the price of anarchy under different underlying topologies such as random graphs or growthrestricted metrics. We want to investigate whether there are certain distance constraints that guarantee O(1) price of anarchy. In addition, we want to run large-scale simulations to observe the change in the price of anarchy as the network size increases.
Another extension is to consider server congestion. Suppose the distance is the network distance plus γ × (number of accesses) where γ is an extra delay when an additional server accesses the replica. Then, when α > γ, it can be shown that PoA is bounded by α γ . As γ increases, the price of anarchy bound decreases, since the load of accesses is balanced across servers.
While exploring the caching problem, we made several observations that seem counterintuitive. First, the PoA in the payment game can be worse than the PoA in the basic game. Another observation we made was that the number of replicas in a Nash equilibrium can be more than the number of replicas in the social optimum even without payments. For example, a graph with diameter slightly more than α may have a Nash equilibrium configuration with two replicas at the two ends. However, the social optimum may place one replica at the center. We leave the investigation of more examples as an open issue.
In this work we introduce a novel non-cooperative game model to characterize the caching problem among selfish servers without any central coordination. We show that pure strategy Nash equilibria exist in the game and that the price of anarchy can be O(n) in general, where n is the number of servers, due to undersupply problems. With specific topologies, we show that the price of anarchy can have tighter bounds. More importantly, with payments, servers are incentivized to replicate and the optimistic price of anarchy is always one. Non-cooperative caching is a more realistic model than cooperative caching in the competitive Internet, hence this work is an important step toward viable federated caching systems.
We thank Kunal Talwar for enlightening discussions regarding this work.
[1] http://www.mosek.com. [2] A. Adya et al. FARSITE: Federated, Available, and Reliable Storage for an Incompletely Trusted Environment. In Proc. of USENIX OSDI, 2002. [3] E. Anshelevich, A. Dasgupta, E. Tardos, and T. Wexler.
Near-optimal Network Design with Selfish Agents. In Proc. of ACM STOC, 2003. [4] Y. Chen, R. H. Katz, and J. D. Kubiatowicz. SCAN: A Dynamic, Scalable, and Efficient Content Distribution Network. In Proc. of Intl. Conf. on Pervasive Computing,
[5] F. Dabek et al. Wide-area Cooperative Storage with CFS. In Proc. of ACM SOSP, Oct. 2001. [6] P. B. Danzig. NetCache Architecture and Deploment. In Computer Networks and ISDN Systems, 1998. [7] N. Devanur, M. Mihail, and V. Vazirani. Strategyproof cost-sharing Mechanisms for Set Cover and Facility Location Games. In Proc. of ACM EC, 2003. [8] J. R. Douceur and R. P. Wattenhofer. Large-Scale Simulation of Replica Placement Algorithms for a Serverless Distributed File System. In Proc. of MASCOTS, 2001. [9] A. Fabrikant, C. H. Papadimitriou, and K. Talwar. The Complexity of Pure Nash Equilibria. In Proc. of ACM STOC,
[10] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. Summary Cache: A Scalable Wide-area Web Cache Sharing Protocol.
IEEE/ACM Trans. on Networking, 8(3):281-293, 2000. [11] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness.
W. H. Freeman and Co., 1979. [12] M. X. Goemans, L. Li, V. S. Mirrokni, and M. Thottan.
Market Sharing Games Applied to Content Distribution in ad-hoc Networks. In Proc. of ACM MOBIHOC, 2004. [13] M. X. Goemans and M. Skutella. Cooperative Facility Location Games. In Proc. of ACM-SIAM SODA, 2000. [14] S. Gribble et al. What Can Databases Do for Peer-to-Peer? In WebDB Workshop on Databases and the Web, June 2001. [15] K. P. Gummadi et al. Measurement, Modeling, and Analysis of a Peer-to-Peer File-Sharing Workload. In Proc. of ACM SOSP, October 2003. [16] S. Iyer, A. Rowstron, and P. Druschel. Squirrel: A Decentralized Peer-to-Peer Web Cache. In Proc. of ACM PODC, 2002. [17] K. Jain and V. V. Vazirani. Primal-Dual Approximation Algorithms for Metric Facility Location and k-Median Problems. In Proc. of IEEE FOCS, 1999. 28 [18] S. Jamin et al. On the Placement of Internet Instrumentation.
In Proc. of IEEE INFOCOM, pages 295-304, 2000. [19] S. Jamin et al. Constrained Mirror Placement on the Internet.
In Proc. of IEEE INFOCOM, pages 31-40, 2001. [20] B.-J. Ko and D. Rubenstein. A Distributed, Self-stabilizing Protocol for Placement of Replicated Resources in Emerging Networks. In Proc. of IEEE ICNP, 2003. [21] E. Koutsoupias and C. Papadimitriou. Worst-Case Equilibria.
In STACS, 1999. [22] J. Kubiatowicz et al. OceanStore: An Architecture for Global-scale Persistent Storage. In Proc. of ACM ASPLOS.
ACM, November 2000. [23] B. Li, M. J. Golin, G. F. Italiano, and X. Deng. On the Optimal Placement of Web Proxies in the Internet. In Proc. of IEEE INFOCOM, 1999. [24] M. Mahdian, Y. Ye, and J. Zhang. Improved Approximation Algorithms for Metric Facility Location Problems. In Proc. of Intl. Workshop on Approximation Algorithms for Combinatorial Optimization Problems, 2002. [25] A. Medina, A. Lakhina, I. Matta, and J. Byers. BRITE: Universal Topology Generation from a User"s Perspective.
Technical Report 2001-003, 1 2001. [26] R. R. Mettu and C. G. Plaxton. The Online Median Problem.
In Proc. of IEEE FOCS, 2000. [27] P. B. Mirchandani and R. L. Francis. Discrete Location Theory. Wiley-Interscience Series in Discrete Mathematics and Optimization, 1990. [28] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
MIT Press, 1994. [29] M. Pal and E. Tardos. Group Strategyproof Mechanisms via Primal-Dual Algorithms. In Proc. of IEEE FOCS, 2003. [30] L. Qiu, V. N. Padmanabhan, and G. M. Voelker. On the Placement of Web Server Replicas. In Proc. of IEEE INFOCOM, 2001. [31] M. Rabinovich, I. Rabinovich, R. Rajaraman, and A. Aggarwal. A Dynamic Object Replication and Migration Protocol for an Internet Hosting Service. In Proc. of IEEE ICDCS, 1999. [32] A. Rowstron and P. Druschel. Storage Management and Caching in PAST, A Large-scale, Persistent Peer-to-peer Storage Utility. In Proc. of ACM SOSP, October 2001. [33] Y. Saito, C. Karamanolis, M. Karlsson, and M. Mahalingam.
Taming Aggressive Replication in the Pangaea Wide-Area File System. In Proc. of USENIX OSDI, 2002. [34] X. Tang and S. T. Chanson. Coordinated En-route Web Caching. In IEEE Trans. Computers, 2002. [35] A. Vetta. Nash Equilibria in Competitive Societies, with Applications to Facility Location, Traffic Routing, and Auctions. In Proc. of IEEE FOCS, 2002. [36] E. W. Zegura, K. L. Calvert, and S. Bhattacharjee. How to Model an Internetwork. In Proc. of IEEE INFOCOM, 1996.
APPENDIX A. ANALYZING SPECIFIC TOPOLOGIES We now analyze the price of anarchy (PoA) for the basic game with specific underlying topologies and show that PoA can have better bounds. We look at complete graph, star, line, and Ddimensional grid. In all these topologies, we set the distance between two directly connected nodes to one. We describe the case where α > 1, since PoA = 1 trivially when α ≤ 1.
A BC D3 α 3 α 4 3α 4 α 4 α Figure 6: Example where the payment game has a Nash equilibrium which is worse than any Nash equilibrium in the basic game. The unlabeled distances between the nodes in the cluster are all 1. The thresholds of white nodes are all α and the thresholds of dark nodes are all α/4. The two dark nodes replicate the object in this payment game Nash equilibrium.
For a complete graph, PoA = 1, and for a star, PoA ≤ 2.
For a complete graph, when α > 1, both Nash equilibria and social optima place one replica at one server, so PoA = 1. For star, when 1 < α < 2, the worst case Nash equilibrium places replicas at all leaf nodes. However, the social optimum places one replica at the center node. Therefore, PoA = (n−1)α+1 α+(n−1) ≤ 2(n−1)+1 1+(n−1) ≤ 2. When α > 2, the worst case Nash equilibrium places one replica at a leaf node and the other nodes access the remote replica, and the social optimum places one replica at the center. PoA = α+1+2(n−2) α+(n−1) = 1 + n α+(n−1) ≤ 2.
For a line, the price of anarchy is O( √ n). When 1 < α < n, the worst case Nash equilibrium places replicas every 2α so that there is no overlap between areas covered by two adjacent servers that cache the object. The social optimum places replicas at least every √ 2α. The placement of replicas for the social optimum is as follows. Suppose there are two replicas separated by distance d. By placing an additional replica in the middle, we want to have the reduction of distance to be at least α. The distance reduction is d/2 + 2{((d/2 − 1) − 1) + ((d/2 − 2) − 2) + ... + ((d/2 − d/4) − d/4)} ≥ d2 /8. d should be at most 2 √ 2α. Therefore, the distance between replicas in the social optimum is at most √ 2α.
C(SW ) = α(n−1) 2α + α(α+1) 2 (n−1) 2α = Θ(αn). C(SO) ≥ α n−1√ 2α + 2 √ 2α/2( √ 2α/2+1) 2 n−1√ 2α . C(SO) = Ω( √ αn). Therefore, PoA = O( √ α). When α > n − 1, the worst case Nash equilibrium places one replica at a leaf node and C(SW ) = α + (n−1)n 2 . However, the social optimum still places replicas every √ 2α. If we view PoA as a continuous function of α and compute a derivative of PoA, the derivative becomes 0 when α is Θ(n2 ), which means the function decreases as α increases from n. Therefore, PoA is maximum when α is n, and PoA = Θ(n2 ) Ω( √ nn) = O( √ n). When α > (n−1)n 2 , the social optimum also places only one replica, and PoA is trivially bounded by 2. This result holds for the ring and it can be generalized to the D-dimensional grid. As the dimension in the grid increases, the distance reduction of additional replica placement becomes Ω(dD+1 ) where d is the distance between two adjacent replicas. Therefore, PoA = Θ(n2) Ω(n 1 D+1 n) = O(n D D+1 ).
B. PAYMENT CAN DO WORSE Consider the network in Figure 6 where α > 1+α/3. Any Nash equilibrium in the basic game model would have exactly two replicas - one in the left cluster, and one in the right. It is easy to verify that the worst placement (in terms of social cost) of two replicas occurs when they are placed at nodes A and B. This placement can be achieved as a Nash equilibrium in the payment game, but not in the basic game since A and B are a distance 3α/4 apart. 29 Algorithm 1 Initialization for the Basic Game L1 = a random subset of servers for each node i in N do if i ∈ L1 then Si = 1 ; replicate the object else Si = 0 Algorithm 2 Move Selection of i for the Basic Game Cost1 = α Cost2 = minj∈X−{i} dij ; X is the current configuration Costmin = min{Cost1, Cost2} if Costnow > Costmin then if Costmin == Cost1 then Si = 1 else Si = 0 C. NASH DYNAMICS PROTOCOLS The simulator initializes the game according to the given parameters and a random initial strategy profile and then iterates through rounds. Initially the order of player actions is chosen randomly. In each round, each server performs the Nash dynamics protocol that adjusts its strategies greedily in the chosen order. When a round passes without any server changing its strategy, the simulation ends and a Nash equilibrium is reached.
In the basic game, we pick a random initial subset of servers to replicate the object as shown in Algorithm 1. After the initialization, each player runs the move selection procedure described in Algorithm 2 (in algorithms 2 and 4, Costnow represents the current cost for node i). This procedure chooses greedily between replication and non-replication. It is not hard to see that this Nash dynamics protocol converges in two rounds.
In the payment game, we pick a random initial subset of servers to replicate the object by setting their thresholds to 0. In addition, we initialize a second random subset of servers to replicate the object with payments from other servers. The details are shown in Algorithm 3. After the initialization, each player runs the move selection procedure described in Algorithm 4. This procedure chooses greedily between replication and accessing a remote replica, with the possibilities of receiving and making payments, respectively.
In the protocol, each node increases its threshold value by incr if it does not replicate the object. By this ramp up procedure, the cost of replicating an object is shared fairly among the nodes that access a replica from a server that does cache. If incr is small, cost is shared more fairly, and the game tends to reach equilibria that encourages more servers to store replicas, though the convergence takes longer.
If incr is large, the protocol converges quickly, but it may miss efficient equilibria. In the simulations we set incr to 0.1. Most of our A B C a b c α/3+1 2α/3−1 2α/3 Figure 7: An example where the Nash dynamics protocol does not converge in the payment game.
Algorithm 3 Initialization for the Payment Game L1 = a random subset of servers for each node i in N do bi = 0 if i ∈ L1 then ti = 0 ; replicate the object else ti = α L2 = {} for each node i in N do if coin toss == head then Mi = {j : d(j, i) < mink∈L1∪L2 d(j, k)} if Mi != ∅ then for each node j ∈ Mi do bj = max{ α+ Èk∈Mi d(i,k) |Mi| − d(i, j), 0} L2 = L2 ∪ {i} Algorithm 4 Move Selection of i for the Payment Game Cost1 = α − Ri Cost2 = minj∈N−{i}{tj − Rj + dij } Costmin = min{Cost1, Cost2} if Costnow > Costmin then if Costmin == Cost1 then ti = Ri else ti = Ri + incr vi = argminj{tj − Rj + dij} bi = tvi − Rvi simulation runs converged, but there were a very few cases where the simulation did not converge due to the cycles of dynamics. The protocol does not guarantee convergence within a certain number of rounds like the protocol for the basic game.
We provide an example graph and an initial condition such that the Nash dynamics protocol does not converge in the payment game if started from this initial condition. The graph is represented by a shortest path metric on the network shown in Figure 7. In the starting configuration, only A replicates the object, and a pays it an amount α/3 to do so. The thresholds for A, B and C are α/3 each, and the thresholds for a, b and c are 2α/3. It is not hard to verify that the Nash dynamics protocol will never converge if we start with this condition.

Market-based systems have long been proposed as solutions for resource allocation in distributed systems including computational Grids [2, 20], wide-area network testbeds [9], and peer-to-peer systems [17]. Yet, while the theoretical underpinnings of market-based schemes have made significant strides in recent years, practical integration of market-based mechanisms into real computer systems and empirical observations of such systems under real workloads has remained an elusive goal. Towards this end, we have designed, implemented, and deployed a microeconomic resource allocation system called Mirage [3] for scheduling testbed time on a 148-node wireless sensor network (SensorNet) testbed at Intel Research. The system, which employs a repeated combinatorial auction [5, 14] to schedule allocations, has been in production use for over four months and has scheduled over 312,148 node hours across 11 research projects to date.
In designing and deploying Mirage, we had three primary goals. First, we wanted to validate whether a market-based resource allocation scheme was necessary at all. An economic problem only exists when resources are scarce.
Therefore, a key goal was to first measure both resource contention and the range of underlying valuations users place on the resources during periods of resource scarcity.
Second, we wanted to observe how users would actually behave in a market-based environment. Much of economic theory is predicated on rational user behavior, which forms the basis for motivating research efforts such as strategyproof mechanism design [4, 6, 15, 16, 19]. With Mirage, we wanted to observe to what extent rationality held and in what ways users would attempt to strategize and game the system.
Finally, we wanted to identify what other practical problems would emerge in a deployment of a market based system. In this paper, we report briefly on our first goal while focusing primarily on the second. The third is left for future work.
Empirical results based on four months of usage have validated the key motivating factors in using an auction-based scheme (i.e., significant resource contention and widely varying valuations) but have also pointed to real-world observations of strategic user behavior. In deploying Mirage, we made the early decision to base the system on a repeated combinatorial auction known not to be strategyproof. That is, self-interested users could attempt to increase their personal gain, at the expense of others, by not revealing their true value to the system. We made this decision mainly because designing a strategyproof mechanism remains an open, challenging problem and we wanted to deploy a working system and gain experience with real users to address our three goals in a timely manner. Deploying a non-strategyproof mechanism also had the benefit of testing rationality and seeing how and to what extent users would try to game the system. The key contribution of this paper is an analysis of such strategic behavior as observed over a four-month time period and proposed refinements for mitigating such behavior en route to building an approximately strategyproof repeated combinatorial auction.
The rest of this paper is organized as follows. In Section 2, 99 we present an overview of Mirage including high-level observations on usage over a four-month period. In Section 3, we examine strategic user behavior, focusing on the four primary types of strategies employed by users in the system.
Based on these results, Section 4 presents a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. As a first step in addressing some of these challenges, we describe refinements to Mirage"s current auction scheme that mitigate the strategies observed to date and also comment on some initial steps towards building an approximately strategyproof repeated combinatorial auction for Mirage. Finally, in Section 5, we conclude the paper.
SensorNet testbeds are a critical tool for developing and evaluating SensorNet technology in a controlled and instrumented environment. As with many large-scale systems, however, resource management is a key problem given that it is not economical for users to each build and operate their own testbed. In Mirage [3], testbed resources are spaceshared and allocated using a repeated combinatorial auction in a closed virtual currency environment. Users compete for testbed resources by submitting bids which specify resource combinations of interest in space/time (e.g., any 32 MICA2 motes for 8 hours anytime in the next two days) along with a maximum value amount the user is willing to pay. A combinatorial auction is then periodically run to determine the winning bids based on supply and demand while maximizing aggregate utility delivered to users. 0 20 40 60 80 100 0 20 40 60 80 100 120 TotalMICA2Utilization(%) Days since Dec 9, 2004 Figure 1: Testbed utilization for 97 MICA2 motes.
In Mirage, resources are allocated using a first-price combinatorial auction which clears every hour. In each round of the auction, a rolling window of future testbed resources is available for allocation with subsets of that window being removed from the pool as resources get allocated. In our initial deployment, we used a 72-hour window and deployed the system on a testbed consisting of 148 nodes (97 MICA2 [1] and 51 MICA2DOT sensor nodes or motes).
In each round of the auction, users bid for subsets of resources available in the current window. When the system is first brought online, a full 148-node × 72-hour window is available, where each row of the window represents the availability of a particular node across time, and each column represents the availability of the testbed for a given hour. The leftmost column of the window represents node availability for the hour immediately following the auction; these node/hours will never again be available for auction.
All other node/hours not allocated at this or previous auctions continue to be offered for sale at subsequent auctions.
In each subsequent round (i.e., every hour), portions of the current window are allocated as bids are matched to available resources and a new rightmost 148-node × 1-hour column of resources rolls in and replaces the leftmost column of resources which expires. There is no time sharing of nodes: given limited local computation and communication power, once a sensor is allocated to a user for a particular time period, it is unavailable to all other users.
In Mirage, users place combinatorial bids specifying resource combinations of interest in space/time along with a maximum value amount the user is willing to pay. More specifically, a bid bi = (vi, si, ti, di, fmin, fmax, ni, oki) indicates the user wants any combination of ni motes from the set oki simultaneously for a duration of di hours (di ∈ {1, 2, . . . , 32}), a start time anywhere between si and ti, and a radio frequency in the range [fmin, fmax].1 The user also is willing to pay up to vi units of virtual currency for these resources. In essence, each bid specifies in a succinct manner what subsets of the resource window would serve as acceptable resources that meet the user"s constraints and how important the desired resource allocation is to the user.
We deployed Mirage on December 9, 2004 and the system has been in continuous production use for over four months.
In the process, its lifetime has overlapped with several periods of significant resource contention including the SIGCOMM "05 and SenSys "05 conference deadlines. Overall, the system has 18 research projects registered to use the system spanning a variety of academic and commercial institutions. Of these, 11 have actively bid and received time on the system. As of April 8, 2005, the system has received 322 bids, and allocated 312,148 node hours over the testbed"s 148 nodes. 0
1
Cumulativefractionofbids Bid value per node hour U1 U2 U3 U4 U5 U6 U7 Figure 2: Bid value distributions by user.
As a measure of contention, Figure 1 shows the utilization of the 97 MICA2 motes over the past four months.
It depicts periods of significant contention extending over multiple consecutive days, in particular near major deadlines.2 To quantify user valuations for resources, Figure 2 1 The frequency constraints are used to schedule testbed allocations such that allocations co-scheduled in time do not collide by using the same radio frequency. In practice, distinct frequencies have not been a scarce resource. 2 Results for the 51 MICA2DOT motes are similar and omitted for space. 100 plots distributions of bid values per node hour for the seven most active users in the system. This graph shows that user valuations for testbed resources varied substantially, spanning over four orders of magnitude. Valuations are also distributed relatively evenly across each order of magnitude, suggesting that these ranges are not due to a few anomalous bids but rather to a wide range of underlying user valuations for testbed resources. These dual observations-significant resource contention and a wide range of valuations-support the use of an auction, which is designed precisely to harness such widely varying valuations to compute an efficient and user utility-maximizing node allocation.
Lastly, as another measure of resource contention and the utility of driving resource allocation via user-specified valuations, Figure 3 plots the median per-node clearing price for both MICA2 and MICA2DOT motes over time. To compute these prices, we price an allocated node-hour for a winning bid with value v for n nodes for k hours as v/nk. Unallocated node-hours are assigned a price of 0. For a given hour, we examine all MICA2 motes and plot the median node-hour price for that hour and do the same for MICA2DOT motes.
Of particular interest in this graph are the sequence of prices from days 45-60 and days 105-120 (i.e., periods leading up to conference deadlines). These sequences show that the value of testbed resources, as measured by market prices for motes, increases exponentially (logarithmic y-axis) during times of peak contention. This suggests that allowing users to express valuations for resources to drive the resource allocation process is important for making effective use of the testbed (e.g., to distinguish important use from low priority activities). However, it also suggests that users become exponentially desperate to acquire resources as deadlines approach. As it turns out, it is precisely during these times that users will try their hardest to game the system and, therefore, when the efficacy of a market-based mechanism can be best evaluated. 1e-05
1 10 0 20 40 60 80 100 120 Valuepernodehour Days since Dec 9, 2004 MICA2 MICA2DOT Figure 3: Median node-hour market prices.
During the past four months of operation, Mirage has employed two distinct auction mechanisms and observed four primary types of strategic behavior from users. The first auction mechanism, A1, was deployed from December 9, 2004 to March 28, 2005. During this time period, we observed three different types of strategic behavior (S1-S3), the most recent of which (S3) resulted in significant gaming of the system. In response to the impact of S3, we deployed a second mechanism, A2, on March 29, 2005 (Day 111 in Figures 1 and 3). While A2 mitigated or eliminated the known shortcomings of A1-in particular the vulnerability strategy S3 exploited that prompted the change in the first place-it was soon discovered that A2 remained vulnerable to another strategy, S4, which was predictably discovered and exploited by a motivated user community. We are currently in the process of designing a mechanism to address the weakness in A2 that is abused by S4. Of course, ideally we would develop a provably strategyproof mechanism.
However, this remains an open research problem for repeated combinatorial auctions.
In this section, we describe the two auction mechanisms A1 and A2, Mirage"s virtual currency policy, the four types of observed strategic behavior S1-S4, and their impact on aggregate utility delivered.
Our first auction mechanism, A1 was a first-price, openbid (i.e., users can see all outstanding bids from competing users) combinatorial auction that cleared every hour based on a greedy algorithm. In each round of auction, the current set of bids was sorted by value per node hour and bids were greedily fit into the remaining portion of the current window of available resources. Like A1, our second auction, A2, was also based on a greedy clearing algorithm. Its key differences were that (i) it was a sealed-bid auction and (ii) it allocated resources over a 148-node × 104-hour window with bid start times constrained to be within the next 72 hours (the reason for this will become apparent when we discuss strategy S3).
In both auctions, winning bids from previous auctions were publicly visible for price feedback and the same virtual currency policy was used. Our virtual currency policy assigns two numbers to each user"s bank account: a baseline value and a number of shares. When created, each bank account is initialized to its baseline value. Once funded, a user can then begin to bid and acquire testbed resources through Mirage. In each round of the auction, accounts for winning bids are debited and the proceeds are redistributed through a proportional profit-sharing policy based on bank account share values. The primary purpose of this policy is to reward users who refrain from using the system during times of peak demand and penalize those who use resources aggressively during periods of scarcity. These rewards result in transient bursts of credit and are balanced by another mechanism, a savings tax, to prevent idle users from sitting on large amounts of excess credit forever (a use it or lose it policy). In our deployment, an administrator set the virtual currency policy. Bank accounts for external users were assigned baseline and shares value set to 1000, while bank accounts for internal users (U4 and U5) were assigned larger allocations with baseline and share values set to 2000.
The following are descriptions of the four primary bidding strategies observed over the past four-months.
S1: underbidding based on current demand. In A1, all outstanding bids were publicly visible. Consequently, when users would observe a lack of demand, some users would bid correspondingly low amounts rather than their true values.
For example, one user would frequently bid 1 or 2 when no other bids were present. While underbidding in the absence of competition is not a problem per se, it does raise two 101 issues. First, if a seller was collecting revenue for profit, such bidding leads to suboptimal outcomes for the seller.
Second, should other users enter competing bids before the auction clears, users will need to refine their bids to allow the system to compute an allocation that maximizes aggregate utility. This second problem then leads to strategy S2.
S2: iterative bidding. Because users are allowed to modify their bids and A1 was an open auction, iteratively refining one"s bid value in response to other users" bid values should, in theory, have no effect on who wins the auction; users with higher valuations-who may also be underbiddingshould eventually outbid those with lower valuations after sufficient iteration. The problem is that users do not behave this (rational) way. Usability overhead matters: users in Mirage bid once and perhaps modify their bid a second time. The end result is that inefficiencies may arise since the auction may clear with bid values that are understated.
While bidding proxies that automatically adjust user bids in response to other bids are effective in single-good auctions, it is unclear how such proxies could be generally effective in a combinatorial auction without actually implementing the same clearing algorithm used by Mirage (which could be computationally expensive). In summary, S1 and S2 both point toward the need for a strategyproof auction mechanism in Mirage. In such an auction, a user"s optimal strategy is always to bid truthfully the first time. Thus, rational users will never underbid and iterative bidding is unnecessary.
S3: rolling window manipulation. Unlike auctions for tangible goods, resource allocation in computer systems fundamentally involves time, since sharing of resources implies that a resource cannot be assigned to a given user/process forever. In Mirage, we addressed the issue of time by selling resources over a rolling window 72 hours into the future with users able to bid for blocks 1, 2, . . . , or 32 hours in length. What we did not anticipate, however, was what would happen when the entire window of resources becomes fully allocated. In this scenario, which was the norm near the recent SenSys "05 deadline, the entire 148-node × 72hour window is allocated. A user bidding for, say, 32 hours thus needs to minimally wait 32 hours for 32 new 148-node × 1-hour columns of resources to become available.
The problem here is that a user can game the system by observing other bids and simply requesting fewer hours.
Since 16 columns will roll into the resource window before 32 columns, a user bidding for 16 hours outbids a 32-hour bid independent of each bid"s value because resources for the 32-hour bid are not available when the auction clears. Of course, if other users also begin bidding for 16 hours, this opportunity disappears but then moves to durations shorter than 16 hours. In the limit, all users bid for 1-hour blocks, thereby eliminating the possibility of obtaining longer resource allocations which may be critical to the underlying SensorNet experiment. In practice, we observed this type of gaming push winning bid durations down to 2 hours.
With rampant gaming of the system occurring through S3, we responded by implementing and deploying auction A2. As mentioned, a key difference of A2 compared to A1 is that it allocates resources over a 104-hour window with bid start times constrained to be within the next 72 hours. In expanding the window and expanding (while still constraining) the range of start times, A2 eliminates strategy S3.
When the entire 148-node × 72-hour window is allocated, a pending 16-hour bid and a pending 32-hour bid will both Time Project Value Nodes Hours 04-02-2005 03:58:04 U2 1590 97 32 04-02-2005 05:05:45 U1 5 24 4 04-02-2005 05:28:23 U1 130 40 4 04-02-2005 06:12:12 U1 1 33 4 Table 1: Strategy S4 on 97 MICA2 motes. have their first opportunity for an allocation when 32 new columns become available. At that point, both the 16-hour bid and the 32-hour bid will have an opportunity to obtain an allocation. Such allocations are then determined by the usual greedy clearing algorithm.
S4: auction sandwich attack. While A2 eliminated S3 and significantly reduced S1 and S2, it still retained a weakness of A1 that had yet to be discovered and exploited. In the auction sandwich attack, a user exploits two pieces of information: (i) historical information on previous winning bids to estimate the current workload and (ii) the greedy nature of the auction clearing algorithm. In this particular case, a user employs a strategy of splitting a bid for 97 MICA2 motes across several bids, only one of which has a high value per node hour. Since the high value bid is likely to win due to the greedy nature of the auction clearing algorithm and since all other users at the time were all requesting 97 motes (based on the historical information and the fact that the SenSys "05 deadline was imminent requiring experiments at scale), no other bids could backfill the remaining slots; the user"s remaining bids would then fit those slots at a low price. An actual occurrence is shown in Table 1. Here, user U1 uses three bids, the main one being a bid with value 130 (value per node hour 130/(4 · 40) = 0.813) which is used to outbid a bid with value 1590 (value per node hour 1590/(32 · 97) = 0.0512). Once the high valued 40-node bid has occupied its portion of the resource window, no other 97-node bids can be matched. Consequently, the user backfills on the remaining 57 nodes using two bids: a 24-node bid and a 33-node bid, both at low valuations.
Designing an appropriate auction mechanism is key to addressing the above strategies. Specifically, our goals for such a mechanism include: (i) strategyproofness, (ii) computational tractability, and (iii) optimal allocation. The Generalized Vickrey Auction (GVA) [8, 18] is the only known combinatorial mechanism that provides both strategyproofness and optimal allocation. However, it also is computationally intractable as it is NP-hard to calculate the allocations as well as individual payments. Other VCG-based mechanisms exist that replace the allocation algorithms in GVA with approximate ones to provide tractability. In this case, however, strategyproofness is no longer available [16]. These goals are in conflict for VCG and in general [10]. We thus must make certain trade-offs.
With this in mind, we now present a two-phase roadmap for improving Mirage: (i) short-term improvements to the current mechanism that mitigate the effects of existing strategies; and (ii) designing a new mechanism that approximately achieves our three goals simultaneously.
Our first improvement is a mixed-integer programming 102 (MIP) formulation as an alternative to the greedy algorithm.
This is aimed directly at eliminating strategy S4. While the MIP does not provide strategyproofness, it is able to compute approximately-optimal allocations. Like the GVA, however, the MIP is computationally demanding and thus careful formulation of the MIP and optimizations based on the observed workloads from Mirage will be required to ensure timely clearing of the auction. Our first step is to test and optimize our MIP-based algorithm on auction data from the past four months. We can then run both the MIP alongside the greedy algorithm in parallel and select the higher quality result each time the auction clears.
Second, we can also augment the auction with additional rules and fees to further mitigate strategic behavior. To eliminate S4, two possibilities are to restrict each user to having either one outstanding bid at a time or to mandate that users are not allowed to have multiple overlapping allocations in time. To mitigate S1 and S2, we could add transaction fees. With such fees in place, a user who understates a bid and intends to iteratively refine it will have a disincentive to do so given that each iteration incurs a fee.
Finally, another approach to eliminating S4 is to modify the greedy algorithm such that if users do have bids whose allocations could overlap in time, then those potential allocations are considered from lowest to highest value per node hour. In effect, this allows bids for overlapping allocations but creates a disincentive for users to place such bids.
Clearly, we need to evaluate our goals and identify where we can make trade-offs in designing a new mechanism.
Computational tractability is a fundamental requirement for operational reasons. Strategyproofness or, minimally, making the system hard to manipulate is also key given the behavior we have observed. Finally, our mechanism should compute near-optimal allocations given our compute time budget.
Among the potential mechanisms we can extend, the LOS scheme [12] seems to be a good starting point. It is a fast algorithm as the allocation rule is a greedy mechanism, ranking bids with some norm such as value per node hour.
The advantage of LOS is its special payment scheme that is tightly linked to the greedy allocation. Essentially, a winner i pays the norm of the first bidder denied access times the amount of units (i.e. node hours) that i won. This feature makes it strategyproof. The main downside, however, is that it assumes users are single-minded, meaning that each bidder only cares about a specific set of goods (e.g., a specific list of nodes for specific durations) and they do not value anything else. Unfortunately, this is highly restrictive and contradicts what Mirage currently offers its users, namely the ability to select any subset of nodes for any slots and submit multiple bids. Thus, LOS is vulnerable to S4 and to avoid it we must find a way to extend LOS and its strategyproof property to satisfy complex-bidders.
Realistically, even with a strategyproof LOS scheme for complex bidders there will likely be further strategies we have yet to encounter and that we should consider in our design. For instance, our discussion so far focuses on strategyproofness within a single auction. Across auctions, however, there may be temporal strategies that are possible. For example, in a particular auction, suppose the highest bidder wants all nodes and pays, using GVA payment scheme for simplicity, the next bidder"s value. This same bidder may be better off by waiting until the next auction, if the user can still win and face bidders that have even lower values.
In this case, the user will gain additional utility due to a lower payment. This, however, may create various problems as total revenue, total value, as well as allocative efficiency across the auctions may be adversely affected.
There are two techniques we can use to address temporal strategies. The first is a wrapper scheme such as the one employed by Virtual Worlds (VW) [13] that makes sequences of individually strategyproof auctions (e.g., LOS) strategyproof. What VW does is, after bidder i wins, it tracks what would have happened if i had submitted in a subsequent auction instead. Specifically, it tracks what i would have paid in all following auctions during i"s patience (i.e., the maximum time i is willing to wait for an allocation) and keeps track of the lowest possible payment. i will instead be charged the lowest payment and will thus have no incentive to temporally game the system. Alternatively, the new class of online mechanisms[7, 11] assumes dynamic arrival and departure of bidders and does not hold auctions at fixed intervals. Instead, the mechanism is a continuous scheme that accepts bids as they arrive and makes allocation decisions immediately, thus removing any need to clear auctions. The challenge is that the current literature is still restricted to non-combinatorial settings.
Despite initially using a repeated combinatorial auction known not to be strategyproof, Mirage has shown significant promise as a vehicle for SensorNet testbed allocation. The dual observations of significant resource contention and a wide range of valuations suggest that auction-based schemes can deliver large improvements in aggregate utility when compared to traditional approaches such as proportional share allocation or batch scheduling. Fully realizing these gains, however, requires addressing key problems in strategyproof mechanism design and combinatorial optimization.
The temporal nature of computational resources and the combinatorial resource demands of distributed applications adds an additional layer of complexity. Nevertheless, we remain optimistic and believe that a pragmatic mix of theory and practice combined with iterative improvements on real deployments provides one promising avenue toward bringing market-based resource allocation into the mainstream.
[1] Crossbow corporation. http://www.xbow.com. [2] Buyya, R., Abramson, D., and Giddy, J.
NimrodG: An Architecture of a Resource Management and Scheduling System in a Global Computational Grid. In Proceedings of the 4th International Conference on High Performance Computing in Asia-Pacific Region (May 2000). [3] Chun, B. N., Buonadonna, P., AuYoung, A., Ng,
C., Parkes, D. C., Shneidman, J., Snoeren,
A. C., and Vahdat, A. Mirage: A Microeconomic Resource Allocation System for SensorNet Testbeds.
In Proceedings of the 2nd IEEE Workshop on Embedded Networked Sensors (May 2005). [4] Clarke, E. H. Multipart pricing of public goods.
Public Choice 2 (1971), 19-33. 103 [5] de Vries, S., and Vohra, R. V. Combinatorial Auctions: A Survey. INFORMS Journal on Computing 15 (2003), 284-309. [6] Groves, T. Incentives in Teams. Econometrica 41 (1973), 617-631. [7] Hajiaghayi, M. T., Kleinberg, R., and Parkes,
D. C. Adaptive Limited-Supply Online Auctions. In Proceedings of the 5th ACM Conference on Electronic Commerce (2004). [8] Jackson, M. O. Mechanism Theory. In The Encyclopedia of Life Support Systems. EOLSS Publishers, 2000. [9] Lai, K., Huberman, B. A., and Fine, L. Tycoon: A Distributed Market-based Resource Allocation System. Tech. rep., Hewlett Packard, 2004. [10] Lavi, R., Mu"alem, A., and Nisan, N. Towards a Characterization of Truthful Combinatorial Auctions.
In Proceedings of the 44th Annual Symposium on Foundations of Computer Science (2003). [11] Lavi, R., and Nisan, N. Competitive Analysis of Incentive Compatible On-line Auctions. In Proceedings of the 2nd ACM Conference on Electronic Commerce (2000), pp. 233-241. [12] Lehmann, D., O"Callaghan, L. I., and Shoham,
Y. Truth Revelation in Approximately Efficient Combinatorial Auctions. Journal of the ACM 49, 5 (September 2002), 577-602. [13] Ng, C., Parkes, D. C., and Seltzer, M. Virtual Worlds: Fast and Strategyproof Auctions for Dynamic Resou rce Allocation. In Proceedings of the 4th ACM Conference on Electronic Commerce (2003). [14] Nisan, N. Bidding and Allocation in Combinatorial Auctions. In Proceedings of the 2nd ACM Conference on Electronic Commerce (2000). [15] Nisan, N., and Ronen, A. Algorithmic Mechanism Design. In Proceedings of the 31st Annual ACM Symposium on Theory of Computing (May 1999). [16] Nisan, N., and Ronen, A. Computationally Feasible VCG Mechanisms. In Proceedings of the 2nd ACM Conference on Electronic Commerce (October 2000). [17] Regev, O., and Nisan, N. The POPCORN Market - an Online Market for Computational Resources. In Proceedings of the 1st International Conference on Information and Computation Economies (October 1998). [18] Varian, H., and MacKie-Mason, J. K. Generalized Vickrey auctions. Tech. rep., University of Michigan,
[19] Vickrey, W. Counterspeculation, Auctions and Competitive Sealed Tenders. Journal of Finance (1961), 8-37. [20] Wolski, R., Plank, J. S., Brevik, J., and Bryan,

Infrastructures for grid computing aim at virtualizing a group of computers, servers, and storage as one large computing system. Resource management is a key issue in such systems, needed for an efficient and automated distribution of tasks on the grid. Such grid infrastructures are often deployed at enterprise level, but projects like SETI@home [1] have demonstrated the feasibility of more decentralized grids as well. Current grid computing infrastructures don"t provide sufficient support for the execution of distributed, useraccessed, long-term services as they are designed to solve compute- or data-intensive tasks with a more or less fixed set of parameters. The common three-phase approach of resource discovery, system selection and job execution fails for services that change their resource demand over time due to interactive user access and run for a long period of time. Instead an infrastructure for long-term services has to place services based on their current demand and their estimated future requirements. If the distribution turns out to be wrong (e.g. a node gets overloaded) the service has to be migrated within the grid (e.g. to a more powerful and less loaded node). Migration however is expensive as the whole state of a service has to be transfered. Additionally a non-replicated service is not accessible during migration.
Therefore the resource management has to avoid migration if possible. Furthermore a service concept has to be provided that evades overload in the first place, and secondly inhibits service unavailability if migration can"t be avoided.
EDAS [2] aims at providing a grid-like infrastructure for user-accessed, long-term services that allows the dynamic adaptation at run-time, provides a management infrastructure, and offers system-level support for scalability and fault tolerance. Nodes can dynamically join and leave the infrastructure, and all management tasks, especially the resource management, are decentralized. The environment is built upon our AspectIX [3] middleware infrastructure, which directly supports QoS-based, dynamic reconfiguration of services.
The resource management focuses on the execution of services that have a long, potentially infinite, operating time.
Theses services are organized in projects. Each project has a distributed execution scope called a service environment.
Such an environment possibly spans multiple institutions.
Each institution represents an administrative domain that can support a project with a fixed set of resources. Our approach supports the adaptive resource management of all projects in scope of an institution based on an algorithm inspired by the diffusive algorithms for decentralized loadbalancing [4]. It is not known how to optimally subdivide these resources for the services as the resource demand of services can change over time or even frequently fluctuate.
To provide resources as needed, our approach automatically rededicates evenly free or not needed resources between service instances across projects and nodes. The whole process Article 5 of rededication is scalable as it is decentralized and respects certain limits like the physically available resources of a node and the amount of resources dedicated to a project. In cases where rededication is not possible, the migration of the demanding service is initiated.
EDAS further supports flexible service models, including a fully centralized client/server structure, completely peer-topeer based systems, and various configurations in between that allow a controlled use of peer resources based on the fragmented object model [5]. The overall goal is to provide a generic service architecture that allows to implement the service functionality once, and then, ideally, run this service with different service models and adapt it at run-time, thereby scaling from a single user local instance to a multidomain-spanning scalable service.
To reduce the implementation effort of such services a framework has been developed that supports the run-time evolution from a traditional client/server scenario to an active replicated server with clients interacting in a hybrid peer-to-peer architecture as known from Napster. In a longterm-service grid infrastructure, active replication has various benefits: Replicas can join and leave the object group and therefore replicas can be migrated without service unavailability. Load of non-modifying requests can be evenly distributed across the replicas making overload situations less likely. Finally a certain amount of node crashes can be tolerated.
The following section describes the used features of AspectIX followed by a brief overview of the core components and concepts of EDAS. Section 4 explains the self-managing and rededication concepts of distributed adaptive resource management. Section 5 describes the framework for decentralized adaptive services. Section 6 describes related work and finally Section 7 concludes the paper.
The EDAS environment is based on the AspectIX middleware. At its core, it provides a CORBA-compliant ORB and, as such, supports heterogeneous distributed systems.
Furthermore AspectIX supports the fragmented object model [5] that is used to implement and provide decentralized adaptive services.
In the fragmented object model, the distinction between client stubs and the server object is no longer present (Fig. 1). From an abstract point of view, a fragmented object is a unit with unique identity, interface, behavior, and state, like in classic object-oriented design. The implementation of these properties however is not bound to a specific location, but may be distributed arbitrarily on various fragments.
Any client that wants to access the fragmented object needs a local fragment, which provides an interface identical to that of a traditional stub. However internal distribution and interaction is not only transparent on the outer interface of the distributed object, but may even change dynamically at runtime. This allows the fragmented object model to adapt to changing environment conditions or quality of service requirements. It offers to change the service model on demand from traditional client-server to a peer-to-peer based approach and all kind of intermediate stages by migration and exchanging of fragments.
Figure 1: Fragmented object on three nodes EDAS has three major components: Every node that actively supports decentralized adaptive services provides a home environment. The home environment basically manages resources of one or more nodes belonging to the same administrative domain or institution. The service environment is spread over a set of domains that support a certain project and relies on basic services from the corresponding home environments. The service environment supports the execution of services belonging to the same project. Finally, the decentralized adaptive service is dynamically distributed within the scope of an associated service environment.
The home environment has the role of a mediator between the nodes of an institution and one or more service environments, each running a set of services. Fig. 2 shows three domains each running a home environment that spans all nodes of the respective domains. Every node provides a set of resources. Each domain has a manager who can use that home environment to assign resources to service environments and to revoke them. Apart from providing system load and all kinds of resource-usage information to the service environment, the home environment also notifies about all important system events like a node shutdown or crash.
A service environment represents a scope of distribution for one or more services. Usually, a service environment is owned by one organization or community and dedicated to one project. A service manager can start, stop, and configure services through the interface of the service environment and decides which resources provided by home environments are accepted.
In most cases a service environment is spread over more than one administrative domain as shown in Fig. 2. One of the main tasks of the service environment is to support the migration of services or service components especially between different home environments. The service environment thereby takes available resources, the requirements of the services, and the policies provided by the service manager into account. The migration of service components can be necessary for various reasons, like node shutdown, resource constraints, and the growth or shrinkage of a service environment.
RESOURCEMANAGEMENT Resource management for long-term services has other requirements than resource management in common grid computing environments. For instance even in the context of traditional grid systems it is very difficult to determine or even only estimate the resource requirements of a task [6].
For long-term services this is even harder, and it is likely that the resource demand frequently changes. This turns Article 5 Host FHost D Host E Host A Host C Service Environment Service B Service A Home Env.
Home Environment Home Environment Boundary of the fragmented object Service Manager Domain Manager Domain Manager Figure 2: EDAS Scenario the optimal distribution of services over a set of nodes into a difficult problem. In general the migration of services is a prerequisite of EDAS as it offers a solution if the initial distribution decision was wrong (e.g. initial start of previously unknown service) or the demand of services has changed substantially. But migration is costly, since the whole state of a service possibly including program code has to be transfered. If the service is not replicated it will be unavailable during migration. Taking this into account a resource management should place services and adaptively rededicate resources between services and nodes as needed to prevent migration. As EDAS aims at providing a grid-like infrastructure for a large set of nodes that can join and leave the system all resource management tasks have to be decentralized to be scalable and should not require global knowledge.
The resource management can be structured into the following different tasks: • Adding and changing the assigned resources of a service environment • Automatic placement of service at startup time and during migration due to overload • Keeping all kind of resource limits, especially the limits of service environments • Compensate leaving and crashed nodes In the next sections we will describe what kind of resource limits there are and how to do resource management based on these basic conditions.
Our approach manages resources on two stages, the node level and the service-environment level. At the node level we monitor usage of all kind of physical resources like disk space, memory, CPU and network bandwidth but also logical ones like open files, sockets and threads. The entity of monitoring is a service or a service component in case of a decentralized adaptive service. Every service has so called local limits that restrict the resource usage in context of the current node. If a service runs the risk of exceeding such a local limit the home environment tries to extend the limits or notifies the responsible service environment if rededication is not possible. Reaching a local limit can be caused by two reasons: an overloaded node or an overloaded service environment. In the first case service migration might help, in the second case migration to another administrative domain might be an option, or simply reducing resource demand by stopping the service. Of course there could be more than one service of the same service environment at a node.
Therefore the assigned resources of a service environment at a node can be easily computed by summing up all local limits of its services.
Furthermore each node has node limits that restrict the overall usage of certain resources. A node limit must never exceed the physical resource (e.g. disk space) of a node and might be much smaller, e.g., for supporting local users.
The sum of all local limits at a node must never exceed the node limit of a resource. Therefore observing and controlling the local limits will keep the node limits and preventing overload.
NodeLimitNode ≥ numberOfLocalLimitsNodeX i=1 LocalLimiti At the institution level the resource usage of a service environment and its associated services is also restricted by so-called global limits. These limits determine the maximum resource usage of a project in scope of a home environment.
The sum of all local limits on all nodes of the institution for a certain project therefore never exceeds its global limit.
GlobaleLimitSE ≥ numberOfLocalLimitsSEX i=1 LocalLimiti
We start with a straight-forward implementation to describe the principal workflow. Then we propose an approach for a more efficient solution and discuss its problems.
If a new project should be supported by a home environment it is first necessary to identify the nodes that offer sufficient unassigned resources to start a service. This can be achieved in a naive implementation by using a flooding approach like it is done by the Gnutella protocol assuming the nodes are connected in a random graph. These resources then can be assigned to the service environment of the new project which further on can start services on these nodes.
Of course a home environment supports usually numerous projects. Each of these projects has resource shares on various nodes, some of them occupied by services, other free and unused.
As the resource demand of a service changes it might be possible that a service reaches its local limit if the service is under high demand. What happens next depends on the overall resource usage of the service environment and the resource consumption at the local node. If the service environment has not reached its global limit and the node is not overloaded the dependent local limit of the service should be extended simply by reducing a local limit at another node of the same service environment. When all resources of the node are assigned to other service environments there are two possibilities. All resources are used by services, so we have to migrate a service, or the resources are assigned but not used. In the later case we shall rededicate resources and assign them to the demanding service environment. Finally the service environment might have reached its global limit.
In this case the resource consumption has to be reduced either by migrating the service to another domain and its depended home environment or simply by bounding resource usage and if this is not possible, stopping the service.
In contrary to the setup of a new service environment which is not time critical and a less frequent task the adaptation of local limits occurs frequently and needs to be done Article 5 almost immediately. Thus it is not an option to use broadcast searches for rededication. Instead a more efficient approach with a bounded complexity is necessary. The same applies for detecting if a global limit is reached by a service environment.
Currently we investigate if this can be achieved by using a diffusive algorithm[4] like it is used for decentralized load balancing. Thereby all nodes of a system are partitioned in groups that overlap partially. The union of all groups achieves a full coverage. Group members frequently exchange load information and balance the load by migration.
In our case we aim not at balancing the load but the amount of available free resources of a service environment.
Each node that supports a certain service environment is at least connected to another node that supports the same project. This way it always should be known if a service environment has still enough resources and therefore if a service can grow. There still remain open issues like if the diffusively balanced free resources should be tightly connected to the real resources, comparable to reservations. In this case there might be problems if a node supports several service environments which all have services running at the node and a node limit is exceeded which would require service migration. In fact it can be needless as the services might not use all the assigned resources but the diffusive algorithm caused the limit overrun by equally balancing the free resources of all supported service environments. If we remove the mapping between free resources and real resources of a node we can evade these situations. However it gets more complicated to determine the free and unassigned resources of a home environment.
Independent of the mapping of free resources the placement of a service is, as already stated, a difficult problem.
Distributing the services equally over all nodes would surely prevent migration in the average case even if resource demand of services changes. However if the resource demand of services varies highly and the grid is clogged by many projects it might be that a service can"t be placed because the free resources are too scattered.
A different approach would be to consider it as an variant of the bin-packing problem that aims at packing items in bins by optimizing the number of used bins. In our case we need an online approach as the items are not known in advance and we have a multi-dimensional problem since a service has various resource requirements. The number of bins is bounded as we have a finite number of nodes in our grid. An algorithm for this problem has recently been proposed by Epstein and van Stee in [7].
On the downside this algorithm needs to know all nodes and their actual usage. As the placement of a service is not a time critical problem again a flooding based approach might offer a solution. To reduce the number of answers only nodes that provide sufficient resources need to reply. It has also to be considered to transform the algorithm to a distributed one. Another problem might be that the algorithm optimizes the occupancy too strong. Therefore demand changes of service can lead to overloaded nodes and causing migration. We believe this can be prevented by not only considering the actual resource consumption to determine the resource demand of a service but taking the previous demand into account.
MODEL In EDAS a decentralized, adaptive service normally matches a traditional service accessed by users like a web server, an instant messaging server or a source code repository. Such a service is represented by a fragmented object. This object expands or shrinks in the scope spanned by the associated service environment depending on the service demands and for fault-tolerance reasons. Usually every part of the object is mobile and can be migrated if necessary. Each service has at least two interfaces: one for management tasks and another service specific for the end user. The management interface offers methods to start, stop, and configure service instances.
As this set of features requires an enormous implementation effort to do it anew for each service implementation we support the development of decentralized adaptive services through a framework and an extended version of IDL in combination with a special IDL-compiler [8]. The core idea is to develop a service in usual client/server fashion as it is done in plain CORBA. This service then can be started and executed on the grid as a common servant.
Additionally it should be possible to migrate the service. This can be achieved by using value type based approach to describe the service state as done in [9] or using the language supplied serialization mechanisms.
As we would like to tolerate node crashes and the service should be available during migration we support the active replication of the service. This is achieved by generating special client-side stubs that communicate with one of the replicas. To keep the connection between clients and the replicated object we use time-bounded references [10] that restrict the migration but make the usage of location services (to cope with outdate references) obsolete. The replicas are synchronized via a group communication framework.
The IDL extension consists of additional modifiers that affect code generation for client and server side. These are retain to mark non-modifying operations which allows faster responses and load balancing of those requests.
Furthermore one can mark methods as local which indicates that they can be locally processed. In this case the IDLcompiler creates placeholder for local execution. Apart form methods that are usual static this is useful to implement client-side contribution and interaction. For example if a client-stub offers a method which results in a file transfer it is possible to integrate a custom protocol that forwards a modified request to the replicated object which returns not the file as in the common case but URLs that point to clients that previously requested the file. Now the clientstub fetches the data from the offered location and responds as if it was supplied by the server object. This peer-to-peer based behavior as known from Napster is transparent to the client and can be switched on and off depending on environment conditions like load and community as needed. Finally we provide another modifier to mark administrative operations. If a method is marked with admin an authentication is necessary. The method to authenticate is pluggable and might be by pass-phrase, internet address or any other authentication scheme. This modifier facilitates the creation of service management methods.
Article 5
Grid infrastructures like the Globus-Toolkit [11] provide services and mechanisms for distributed heterogeneous environments to combine resources on demand to solve resource consuming and compute intensive tasks. Due to this orientation they focus on different service models, provide no support for object mobility if even supporting a distributed object approach at all. But most important they follow a different resource management approach as they target the parallel execution of a large number of short and midterm tasks.
JavaSymphony [12] and Ibis [13] provide object mobility but are limited to the Java programming language and focus on object oriented high performance computing.
Actively replicated objects are provided by Jgroup [14] based on RMI. On top of this basic middleware a replication management layer has been implemented called ARM [15].
JGroup focus on the active replication of objects but lacks support for more flexible services like EDAS does. ARM can be compared to EDAS but supports no resource aware distribution.
Fog [16] and Globe [17] are basic middleware environments that support the fragmented object approach. Globe considers replication and caching. Both systems lack support for resource aware distribution.
Based on the fragmented object model and the architecture of the EDAS environment, decentralized adaptive services can be easily designed, implemented and executed.
As described the resource management can be decomposed in two main problems that have to be solved.
Controlling and managing of resource limits including ensuring that the assigned resources are available (even in the context of node crashes) and the autonomous placement of services.
For both problems we offer a solution, a currently implemented simulation environment will verify their feasibility.
In a next step the resource management will be integrate in an already implemented prototype of the EDAS architecture.
As described we have already an early implementation of the framework for the decentralized adaptive services. This framework has to be extended to smoothly interact with the resource management and the EDAS architecture. In a final step we need to implement some services that verify the usability of the whole EDAS project.
[1] D. Werthimer S. Bowyer J. Cobb D. Gedye D. Anderson W. T. Sullivan, III. A new major seti project based on project serendip data and 100,000 personal computers. In Proc. of the Fifth Intl. Conf. on Bioastronomy, 1997. [2] Hans Reiser R¨udiger Kapitza, Franz J. Hauck.
Decentralized, Adaptive Services: The AspectIX Approach for a Flexible and Secure Grid Environment.
In Grid Services Engineering and Management (GSEM 2004), Erfurt, Germany, 2004. Springer. [3] Hans P. Reiser, Franz J. Hauck, R¨udiger Kapitza, and Andreas I. Schmied. Integrating fragmented objects into a CORBA environment. In Proc. of the Net.ObjectDays, 2003. [4] Tiberiu Rotaru and Hans-Heinrich N¨ageli. Dynamic load balancing by diffusion in heterogeneous systems.
J. Parallel Distrib. Comput., 64(4):481-497, 2004. [5] M. Makpangou, Y. Gourhant, J.-P. Narzul, and M. Shapiro. Fragmented objects for distributed abstractions. [6] Jennifer M. Schopf. Ten actions when grid scheduling: the user as a grid scheduler. pages 15-23, 2004. [7] Leah Epstein and Rob van Stee. Optimal online bounded space multidimensional packing. In SODA "04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pages 214-223,
Philadelphia, PA, USA, 2004. Society for Industrial and Applied Mathematics. [8] Hans P. Reiser, Martin Steckermeier, and Franz J.
Hauck. IDLflex: a flexible and generic compiler for CORBA IDL. In Proc. of the Net.ObjectDays (Erfurt,
Germany, Sep. 10-13, 2001), 2001. [9] R¨udiger Kapitza, Holger Schmidt, and Franz J.
Hauck. Platform-independent object migration in CORBA. In Proc. of the OTM"05 Conferences (DOA,
Agia Napa, Cyprus, Oct 31-Nov. 04, 2005), 2005. [10] R¨udiger Kapitza, Hans P. Reiser, and Franz J. Hauck.
Stable, time-bound references in context of dynamically changing environments. In MDC"05: Proc. of the 25th IEEE Int. Conf. on Distributed Computing Systems - Workshops (ICDCS 2005 Workshops), 2005. [11] Ian Foster, Carl Kesselman, and Steven Tuecke. The anatomy of the Grid: Enabling scalable virtual organizations. Lecture Notes in Computer Science, 2150, 2001. [12] Thomas Fahringer and Alexandru Jugravu.
Javasymphony: new directives to control and synchronize locality, parallelism, and load balancing for cluster and grid-computing. In JGI "02: Proceedings of the 2002 joint ACM-ISCOPE conference on Java Grande, pages 8-17, New York,
NY, USA, 2002. ACM Press. [13] Rob V. van Nieuwpoort, Jason Maassen, Rutger Hofman, Thilo Kielmann, and Henri E. Bal. Ibis: an efficient java-based grid programming environment. In JGI "02: Proceedings of the 2002 joint ACM-ISCOPE conference on Java Grande, pages 18-27, New York,
NY, USA, 2002. ACM Press. [14] Alberto Montresor, Renzo Davoli, and Ozalp Babaoglu. Middleware for dependable network services in partitionable distributed systems. SIGOPS Oper. Syst. Rev., 35(1):73-96, 2001. [15] H. Meling and B. Helvik. Arm: Autonomous replication management in jgroup, 2001. [16] Mesaac Makpangou, Yvon Gourhant, Jean-Pierre Le Narzul, and Marc Shapiro. Fragmented objects for distributed abstractions. In Readings in Distributed Computing Systems. [17] Philip Homburg, Leendert van Doorn, Maarten van Steen, Andrew S. Tanenbaum, and Wiebren de Jonge.
An object model for flexible distributed systems. In Proceedings of the 1st Annual ASCI Conference, pages 69-78, 1995.

