<?xml version="1.0" encoding="UTF-8"?>
<document language="en">
<meta>
<genre/>
<title>H-87</title>
<authors/>
<date>10-06-2017</date>
<source/>
<complexity_level/>
<uri/>
</meta>
<body>
<p id="0">Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.</p>
<p id="1">The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.</p>
<p id="2">None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].</p>
<p id="3">Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper.</p>
<p id="4">We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.</p>
<p id="5">Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.</p>
<p id="6">This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.</p>
<p id="7">In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].</p>
<p id="8">Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14]. It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.</p>
<p id="9">The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively.</p>
<p id="10">Section 5 reports the experiments and results. Section 6 concludes the main findings in this study.</p>
<p id="11">We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets.</p>
<p id="12">TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]. The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 &amp; ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.</p>
<p id="13">TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].</p>
<p id="14">TDT3 was the evaluation benchmark in the TDT2001 dry run1 .</p>
<p id="15">The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT,</p>
<p id="16">CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.</p>
<p id="17">Machine-translated versions of the non-English stories (Xinhua,</p>
<p id="18">Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT.</p>
<p id="19">TDT5 was the evaluation benchmark in TDT2004 [4]. The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper.</p>
<p id="20">The TDT topics differ from TREC topics both conceptually and statistically. Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.</p>
<p id="21">The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0</p>
<p id="22">Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics</p>
<p id="23">To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation.</p>
<p id="24">Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max</p>
<p id="25">CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002). For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging).</p>
<p id="26">The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).</p>
<p id="27">To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( ,</p>
<p id="28">N DB TP + =− )(1 ,</p>
<p id="29">CA C Pmiss + = ,</p>
<p id="30">DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.</p>
<p id="31">In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric. To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.</p>
<p id="32">Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01</p>
<p id="33">From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.</p>
<p id="34">The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.</p>
<p id="35">More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.</p>
<p id="36">That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w . However, this is not true if</p>
<p id="37">on average for the test corpus. Using TDT3 as an example, the true percentage is:</p>
<p id="38">37770</p>
<p id="39">)( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: )1.010( 1</p>
<p id="40">)3.7937770(37770</p>
<p id="41">))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10</p>
<p id="42">)( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows:</p>
<p id="43">991,207/3.71</p>
<p id="44">)( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.</p>
<p id="45">The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.</p>
<p id="46">Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. This was a challenging part of the TDT2004 evaluation for AF.</p>
<p id="47">Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge.</p>
<p id="48">We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| ' |)(| )()( )(')( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights. The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.</p>
<p id="49">The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the system"s prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype. Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF). To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.</p>
<p id="50">The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].</p>
<p id="51">It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.</p>
<p id="52">Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.</p>
<p id="53">This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).</p>
<p id="54">Results of more complex variants of Rocchio are also discussed when relevant.</p>
<p id="55">Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].</p>
<p id="56">Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.</p>
<p id="57">We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix. Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5]. How to find an effective μ r is an open issue for research, depending on the user"s belief about the parameter space and the optimal range.</p>
<p id="58">The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ .</p>
<p id="59">We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback.</p>
<p id="60">The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004. Multiple research teams participated and multiple runs from each team were allowed.</p>
<p id="61">Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure</p>
<p id="62">with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison.</p>
<p id="63">Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better)</p>
<p id="64">0</p>
<p id="65">Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.) We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better)</p>
<p id="66">0</p>
<p id="67">1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).</p>
<p id="68">CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004</p>
<p id="69">0</p>
<p id="70">1</p>
<p id="71">Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made. Such a setting has been conventional for the Topic Tracking task in TDT until</p>
<p id="72">each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers.</p>
<p id="73">How much the strong performance of our systems depends on parameter tuning is an important question.</p>
<p id="74">Both Rocchio and LR have parameters that must be prespecified before the AF process. The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed.</p>
<p id="75">Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004. The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.</p>
<p id="76">Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.</p>
<p id="77">Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ. Table 2 summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0. This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.</p>
<p id="78">Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers" parameters do not.</p>
<p id="79">Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.</p>
<p id="80">Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964</p>
<p id="81">3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0</p>
<p id="82">Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.</p>
<p id="83">The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between</p>
<p id="84">performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.</p>
<p id="85">Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments.</p>
<p id="86">How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.</p>
<p id="87">To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.</p>
<p id="88">Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.</p>
<p id="89">Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF.</p>
<p id="90">After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback.</p>
<p id="91">We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.</p>
<p id="92">For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.</p>
<p id="93">Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.</p>
</body>
</document>
