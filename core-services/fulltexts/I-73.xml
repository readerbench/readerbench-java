<?xml version="1.0" encoding="UTF-8"?>
<document language="en">
<meta>
<genre/>
<title>I-73</title>
<authors/>
<date>10-06-2017</date>
<source/>
<complexity_level/>
<uri/>
</meta>
<body>
<p id="0">Open multiagent systems (MAS) are composed of autonomous distributed agents that may enter and leave the agent society at their will because open systems have no centralized control over the development of its parts [1]. Since agents are considered as autonomous entities, we cannot assume that there is a way to control their internal behavior. These features are interesting to obtain flexible and adaptive systems but they also create new risks about the reliability and the robustness of the system. Solutions to this problem have been proposed by the way of trust models where agents are endowed with a model of other agents that allows them to decide if they can or cannot trust another agent.</p>
<p id="1">Such trust decision is very important because it is an essential condition to the formation of agents' cooperation. The trust decision processes use the concept of reputation as the basis of a decision. Reputation is a subject that has been studied in several works [4][5][8][9] with different approaches, but also with different semantics attached to the reputation concept. Casare and Sichman [2][3] proposed a Functional Ontology of Reputation (FORe) and some directions about how it could be used to allow the interoperability among different agent reputation models. This paper describes how the FORe can be applied to allow interoperability among agents that have different reputation models. An outline of this approach is sketched in the context of a testbed for the experimentation and comparison of trust models, the ART testbed [6].</p>
<p id="2">REPUTATION (FORe) In the last years several computational models of reputation have been proposed [7][10][13][14]. As an example of research produced in the MAS field we refer to three of them: a cognitive reputation model [5], a typology of reputation [7] and the reputation model used in the ReGret system [9][10]. Each model includes its own specific concepts that may not exist in other models, or exist with a different name. For instance, Image and Reputation are two central concepts in the cognitive reputation model. These concepts do not exist in the typology of reputation or in the ReGret model. In the typology of reputation, we can find some similar concepts such as direct reputation and indirect reputation but there are some slight semantic differences. In the same way, the ReGret model includes four kinds of reputation (direct, witness, neighborhood and system) that overlap with the concepts of other models but that are not exactly the same.</p>
<p id="3">The Functional Ontology of Reputation (FORe) was defined as a common semantic basis that subsumes the concepts of the main reputation models. The FORe includes, as its kernel, the following concepts: reputation nature, roles involved in reputation formation and propagation, information sources for reputation, evaluation of reputation, and reputation maintenance. The ontology concept ReputationNature is composed of concepts such as IndividualReputation, GroupReputation and ProductReputation.</p>
<p id="4">Reputation formation and propagation involves several roles, played by the entities or agents that participate in those processes.</p>
<p id="5">The ontology defines the concepts ReputationProcess and ReputationRole. Moreover, reputation can be classified according to the origin of beliefs and opinions that can derive from several sources. The ontology defines the concept ReputationType which can be PrimaryReputation or SecondaryReputation.</p>
<p id="6">PrimaryReputation is composed of concepts ObservedReputation and DirectReputation and the concept SecondaryReputation is composed of concepts such as PropagatedReputation and CollectiveReputation. More details about the FORe can be found on [2][3].</p>
<p id="7">MODELS TO THE FORe Visser et al [12] suggest three different ways to support semantic integration of different sources of information: a centralized approach, where each source of information is related to one common domain ontology; a decentralized approach, where every source of information is related to its own ontology; and a hybrid approach, where every source of information has its own ontology and the vocabulary of these ontologies are related to a common ontology. This latter organizes the common global vocabulary in order to support the source ontologies comparison. Casare and Sichman [3] used the hybrid approach to show that the FORe serves as a common ontology for several reputation models.</p>
<p id="8">Therefore, considering the ontologies which describe the agent reputation models we can define a mapping between these ontologies and the FORe whenever the ontologies use a common vocabulary. Also, the information concerning the mappings between the agent reputation models and the FORe can be directly inferred by simply classifying the resulting ontology from the integration of a given reputation model ontology and the FORe in an ontology tool with reasoning engine.</p>
<p id="9">For instance, a mapping between the Cognitive Reputation Model ontology and the FORe relates the concepts Image and Reputation to PrimaryReputation and SecondaryReputation from FORe, respectively. Also, a mapping between the Typology of Reputation and the FORe relates the concepts Direct Reputation and Indirect Reputation to PrimaryReputation and SecondaryReputation from FORe, respectively. Nevertheless, the concepts Direct Trust and Witness Reputation from the Regret System Reputation Model are mapped to PrimaryReputation and PropagatedReputation from FORe. Since PropagatedReputation is a sub-concept of SecondaryReputation, it can be inferred that Witness Reputation is also mapped to SecondaryReputation.</p>
<p id="10">THE ART TESTBED To exemplify the use of mappings from last section, we define a scenario where several agents are implemented using different agent reputation models. This scenario includes the agents" interaction during the simulation of the game defined by ART [6] in order to describe the ways interoperability is possible between different trust models using the FORe.</p>
<p id="11">The ART testbed provides a simulation engine on which several agents, using different trust models, may run. The simulation consists in a game where the agents have to decide to trust or not other agents. The game"s domain is art appraisal, in which agents are required to evaluate the value of paintings based on information exchanged among other agents during agents" interaction. The information can be an opinion transaction, when an agent asks other agents to help it in its evaluation of a painting; or a reputation transaction, when the information required is about the reputation of another agent (a target) for a given era.</p>
<p id="12">More details about the ART testbed can be found in [6].</p>
<p id="13">The ART common reputation model was enhanced with semantic data obtained from FORe. A general agent architecture for interoperability was defined [11] to allow agents to reason about the information received from reputation interactions. This architecture contains two main modules: the Reputation Mapping Module (RMM) which is responsible for mapping concepts between an agent reputation model and FORe; and the Reputation Reasoning Module (RRM) which is responsible for deal with information about reputation according to the agent reputation model.</p>
<p id="14">While including the FORe to the ART common reputation model, we have incremented it to allow richer interactions that involve reputation transaction. In this section we describe scenarios concerning reputation transactions in the context of ART testbed, but the first is valid for any kind of reputation transaction and the second is specific for the ART domain.</p>
<p id="15">Suppose that agents A, B and C are implemented according to the aforementioned general agent architecture with the enhanced ART common reputation model, using different reputation models.</p>
<p id="16">Agent A uses the Typology of Reputation model, agent B uses the Cognitive Reputation Model and agent C uses the ReGret System model. Consider the interaction about reputation where agents A and B receive from agent C information about the reputation of agent Y. A big picture of this interaction is showed in Figure 2.</p>
<p id="17">ReGret Ontology (Y, value=0.8, witnessreputation) C Typol.</p>
<p id="18">Ontology (Y, value=0.8, propagatedreputation) A CogMod.</p>
<p id="19">Ontology (Y, value=0.8, reputation) B (Y, value=0.8,</p>
<p id="20">PropagatedReputation) (Y, value=0.8,</p>
<p id="21">PropagatedReputation) ReGret Ontology (Y, value=0.8, witnessreputation) C ReGret Ontology (Y, value=0.8, witnessreputation) ReGret Ontology (Y, value=0.8, witnessreputation) (Y, value=0.8, witnessreputation) C Typol.</p>
<p id="22">Ontology (Y, value=0.8, propagatedreputation) A Typol.</p>
<p id="23">Ontology (Y, value=0.8, propagatedreputation) Typol.</p>
<p id="24">Ontology (Y, value=0.8, propagatedreputation) (Y, value=0.8, propagatedreputation) A CogMod.</p>
<p id="25">Ontology (Y, value=0.8, reputation) B CogMod.</p>
<p id="26">Ontology (Y, value=0.8, reputation) CogMod.</p>
<p id="27">Ontology (Y, value=0.8, reputation) (Y, value=0.8, reputation) B (Y, value=0.8,</p>
<p id="28">PropagatedReputation) (Y, value=0.8,</p>
<p id="29">PropagatedReputation) (Y, value=0.8,</p>
<p id="30">PropagatedReputation) (Y, value=0.8,</p>
<p id="31">PropagatedReputation) Figure 1. Interaction about reputation The information witness reputation from agent C is treated by its RMM and is sent as PropagatedReputation to both agents. The corresponding information in agent A reputation model is propagated reputation and in agent B reputation model is reputation. The way agents A and B make use of the information depends on their internal reputation model and their RRM implementation.</p>
<p id="32">Considering the same agents A and B and the art appraisal domain of ART, another interesting scenario describes the following situation: agent A asks to agent B information about agents it knows that have skill on some specific painting era. In this case agent A wants information concerning the direct reputation agent B has about agents that have skill on an specific era, such as cubism. Following the same steps of the previous scenario, agent A message is prepared in its RRM using information from its internal model. A big picture of this interaction is in Figure 2.</p>
<p id="33">Typol.</p>
<p id="34">Ontology (agent = ?, value = ?, skill = cubism, reputation = directreputation) A (agent = ?, value = ?, skill = cubism, reputation = PrimaryReputation) CogMod.</p>
<p id="35">Ontology (agent = ?, value = ?, skill = cubism, reputation = image) B Typol.</p>
<p id="36">Ontology (agent = ?, value = ?, skill = cubism, reputation = directreputation) A (agent = ?, value = ?, skill = cubism, reputation = PrimaryReputation) CogMod.</p>
<p id="37">Ontology (agent = ?, value = ?, skill = cubism, reputation = image) B Figure 2. Interaction about specific types of reputation values Agent B response to agent A is processed in its RRM and it is composed of tuples (agent, value, cubism, image) , where the pair (agent, value) is composed of all agents and associated reputation values whose agent B knows their expertise about cubism by its own opinion. This response is forwarded to the RMM in order to be translated to the enriched common model and to be sent to agent A. After receiving the information sent by agent B, agent A processes it in its RMM and translates it to its own reputation model to be analyzed by its RRM.</p>
<p id="38">In this paper we present a proposal for reducing the incompatibility between reputation models by using a general agent architecture for reputation interaction which relies on a functional ontology of reputation (FORe), used as a globally shared reputation model. A reputation mapping module allows agents to translate information from their internal reputation model into the shared model and vice versa. The ART testbed has been enriched to use the ontology during agent transactions. Some scenarios were described to illustrate our proposal and they seem to be a promising way to improve the process of building reputation just using existing technologies.</p>
<p id="39">Anarosa A. F. Brand√£o is supported by CNPq/Brazil grant 310087/2006-6 and Jaime Sichman is partially supported by CNPq/Brazil grants 304605/2004-2, 482019/2004-2 and 506881/2004-1. Laurent Vercouter was partially supported by FAPESP grant 2005/02902-5.</p>
</body>
</document>
