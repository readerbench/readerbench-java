Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan cannot be guaranteed [13].
Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence [21].
In agent architectures inspired by the Belief-Desire-Intention (BDI) model [16], these properties are often characterized by the interactions between beliefs, goals, and plans [2].1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success [12, 14]. Failures can occur for a number of reasons, and it is often not possible to predict these in advance, either because of the complexity of the system or because changes in the environment invalidate some earlier decisions.
Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent"s execution cycle.
Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. This decision may be due to an internal deliberation (such as the agent believing the task can no longer be achieved, or that some conflicting task now has a higher priority) or due to an external factor (such as another agent altering a commitment, or a change in the environment).
Aborting a task or plan is distinct from its failure. Failure reflects an inability to perform and does not negate the need to perform - for example, a reasonable response to failure may be to try again. In contrast, aborting says nothing about the ability to perform; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up.
There has been a considerable amount of work on plan failures (such as detecting and resolving resource conflicts [20, 10]) and most agent systems incorporate some notion of failure handling.
However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required.
As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct 1 One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of the task having been performed, and a goal can be considered a task of bringing about that state of the world.
We adopt the latter view and use task to also refer to goals. 8 978-81-904262-7-5 (RPS) c 2007 IFAAMAS fails (common approaches include either letting the other branch run to completion unhindered or dropping it completely).
In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN [23, 17]. This allows us to specify an appropriate level of detail for the execution model, without focusing on the specific constructs of any one agent system such as JACK [2], Jadex [14], Jason [6], or SPARK [9]. Our focus is on a single agent, complementary to related work that considers exception handling for single- and multiagent systems (e.g., [22, 5, 6]).
This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work.
Alice is a knowledge worker assisted by a learning, personal assistive agent such as CALO [11]. Alice plans to attend the IJCAI conference later in the year, and her CALO agent adopts the task of Support Meeting Submission (SMS) to assist her. CALO"s plan for an SMS task in the case of a conference submission consists of the following sub-tasks:
purposes in the company.
in preparing an abstract.
manager based on the abstract and conference details.
writing the paper.
procedures for submitting a paper to a conference.
These steps must be performed in order, with the exception of steps 3 (AFC) and 4 (TWP), which may be performed in parallel.
Similarly, CALO can perform the task Apply For Clearance (AFC) by a plan consisting of:
Now suppose that a change in circumstances causes Alice to reconsider her travel plans while she is writing the paper. Alice will no longer be able to attend IJCAI. She therefore instructs her CALO agent to abort the SMS task. Aborting the task implies aborting both the SMS plan and the AFC subplan. Aborting the first plan requires CALO to notify the paper number registry that the allocated paper number is obsolete, which it can achieve by the Cancel Paper Number (CPN) task.2 Aborting the second plan requires CALO to notify Alice"s manager that Alice no longer requires clearance for publication, which CALO can achieve by invoking the Cancel Clearance Request (CCR) task.
We note a number of important observations from the example. First, the decision to abort a particular course of action can come from the internal deliberations of the agent (such as reasoning about priorities in a conflict over resources), or from external sources (such as another agent cancelling a commitment), as in this example. In this paper we only touch on the problem of determining whether a task or plan should be aborted, instead concentrating on determining the appropriate actions once this decision is made.
Hence, our objective is to determine how to incorporate aborting mechanisms into the standard execution cycle rather than determine what should be aborted and when.
Second, once the decision is made to abort the attempt to submit a paper, there are some actions the agent should take, such as cancelling the clearance request. In other words, aborting a task is not simply a matter of dropping the task and associated active plans: there are some clean up actions that may need to be done. This is similar to the case for failure, in that there may also be actions to take when a task or plan fails. In both cases, note that it is not simply a matter of the agent undo-ing its actions to date; indeed, this may be neither possible (since the agent acts in a situated world and its actions change world state) nor desirable (depending on the semantics of the task). Rather, cleaning up involves compensation via forward recovery actions [3].
Third, there is a distinction between aborting a task and aborting a plan. In the former case, it is clear that all plans being executed to perform the task should be aborted; in the latter case, it may be that there are better alternatives to the current plan and one of these should be attempted. Hence, plan aborting or failure does not necessarily lead to task aborting or failure.
Fourth, given that tasks may contain sub-tasks, which may contain further sub-tasks, it is necessary for a parent task to wait until its children have finished their abort methods. This is the source of one of the technical challenges that we address: determining the precise sequence of actions once a parent task or plan is aborted.
As we have alluded to, failure and aborting are related concepts.
They both cause the execution of existing plans to cease and, consequentially, the agent to reflect over its current tasks and intentions.
Failure and aborting, however, differ in the way they arise. In the case of failure, the trigger to cease execution of a task or plan comes from below, that is, the failure of sub-tasks or lower-level plans. In the case of aborting, the trigger comes from above, that is, the tasks and the parent plans that initiated a plan.
In BDI-style systems such as JACK and SPARK, an agent"s domain knowledge includes a pre-defined plan library of plan clauses.
Each plan clause has a plan body, which is a program (i.e., combination of primitive actions, sub-tasks, etc.) that can be executed in response to a task or other event should the plan clause"s context condition be satisfied. The agent selects and executes instances of plan clauses to perform its tasks. There can be more than one applicable plan clause and, in the event that one fails, another applicable one may be attempted. Plans may have sub-tasks that must succeed 2 CALO needs only drop the TWA and TWP tasks to abort them: for the sake of simplicity we suppose no explicit clean up of its internal state is necessary.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 9 for the plan to succeed. In such systems, a plan failure occurs if one of the actions or sub-tasks within the plan fails.
The agent"s action upon plan failure depends on its nature: for example, the agent may declare the task to have failed if one plan has been tried and resulted in failure, or it may retry alternate plans and declare (indeed, must declare) task failure only if all possible alternate plans to perform the task have been tried and resulted in failure. Observe that, while task failure can follow from plan failure or a sequence of plan failures, plan failure need not lead to task failure provided the agent can successfully complete an alternate plan. Moreover, task failure can also arise separately from plan failure, if the agent decides to abort the task.
Our approach associates an abort-method with each plan. This enables the programmer to specify dedicated compensation actions according to how the agent is attempting to perform the task. Note that our abort-methods can be arbitrary programs and so can invoke tasks that may be performed dynamically in the usual BDI fashion, i.e., the clean-up is not limited to executing a predetermined set of actions. The question remains which abort-method should be invoked, and in what manner. Given the complexity of agent action spaces, it is not possible nor desirable to enumerate a static set of rules. Rather, the agent will invoke its abort-methods dynamically according to the state of execution and its own internal events.
An alternative to attaching an abort-method to each plan is to attach such methods to each atomic action. We choose the former because: (1) action-level abort-methods would incur a greater overhead, (2) plans are meant to be designed as single cohesive units and are the unit of deliberation in BDI systems, and (3) the cleanup methods for failure in current systems are attached to plans.
In order to understand how the agent"s abort processing should function, we consider three situations where it is sensible for an agent to consider aborting some of its tasks and plans:
other than the agent itself, the plan currently executed to perform the task should be aborted. For example, suppose company policy changes so that employees of Alice"s seniority automatically have clearance for publishing papers. Since Alice now has clearance for publishing her paper, CALO can abort the plan for Apply For Clearance. In doing so it must invoke the abort-method, in this case thus performing Cancel Clearance Request.3
one fails then the others should be aborted, given that the failure of one branch leads to the failure of the overall task. For example, suppose that part-way through writing the paper,
Alice realizes that there is a fatal flaw in her results, and so notifies CALO that she will not be able to complete the paper by the deadline. The failure of the Track Writing Paper task should cause the Apply For Clearance task being executed in parallel to be aborted.
task or intention, the agent should deliberate over whether the existing plan(s) should continue. For example, suppose that Alice tasks CALO with a new, high-priority task to purchase a replacement laptop, but that Alice lacks enough funds to both purchase the laptop and to attend IJCAI. Reasoning over resource requirements [20, 10] will cause the agent to realize 3 If there is any difference between how to abort a task that is externally performed versus how to abort one that is now known to be impossible, the abort-method can detect the circumstances and handle the situation as appropriate. that it cannot successfully complete both tasks. Given that the new task has greater importance, a rational agent will evaluate its best course of action and may decide to abortor at least suspend - the existing task of submitting a paper and intentions derived from it [12].
The operational semantics we provide in Section 4 for aborting tasks and plans captures the first two situations above. The third situation involves deliberating over the importance of a task, which depends on various factors such as task priority. Although this deliberation is beyond the scope of this paper, it is a complementary topic of our future work.
Note that the above situations apply to achievement goals, for which the task is completed when a particular state of the world is brought about (e.g., ensure we have clearance). Different forms of reasoning apply to other goal types [4] such as maintenance goals [1], where the goal is satisfied by maintaining a state of the world for some period of time (e.g., maintain $100 in cash).
Abort Method Representation The intent of aborting a task or plan is that the task or plan and all its children cease to execute, and that appropriate clean-up methods are performed as required. In contrast to offline planning systems,
BDI agents are situated: they perform online deliberation and their actions change the state of the world. As a result, the effects of many actions cannot be simply undone. Moreover, the undo process may cause adverse effects. Therefore, the clean-up methods that we specify are forward recovery procedures that attempt to ensure a stable state and that also may, if possible, recover resources.
The common plan representation in BDI-style systems such as JACK and SPARK includes a failure-method, which is the designated clean-up method invoked when the plan fails. To this, we add the abort-method, which is invoked if the plan is to be aborted.
In our example, the abort-method for the plan for Support Meeting Submission consists of invoking the sub-task Cancel Paper Number. The abort-method need not explicitly abort Apply For Clearance, because the agent will invoke the abort-method for the subtask appropriately, as we outline below.
The assumption here is that, like the failure-method, the programmer of the agent system has the opportunity to specify a sensible abort-method that takes into consideration the point in the plan at which the abort is to be executed. For any plan, the abort-method is optional: if no abort-method is specified, the agent takes no specific action for this plan. However, the agent"s default behavioural rules still apply, for example, whether to retry an alternate plan for the parent task.
Note that an explicit representation of the clean-up methods for tasks is not required, since tasks are performed by executing some plan or plans. Hence, aborting a task means aborting the current plan that is executed to perform that task, as we next describe.
Abort Method Invocation We now informally lay out the agent"s action upon aborting plans and tasks. When a plan P is aborted:
child is one that was triggered by P and is currently in execution.
method of plan P.
here that if the parent task TP is not to be aborted then the agent may choose another applicable plan to satisfy TP . 10 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) When a task (or sub-task) T is aborted:
The agent thus no longer pursues T.
is aborted, no other applicable plans to perform T should be tried as it is the task that is to be aborted.
In order to prevent infinitely cascading clean-up efforts, we assume that abort-methods will never be aborted nor fail. In reality, however, an abort-method may fail. In this case, lacking a more sophisticated handling mechanism, the agent simply stops executing the failed abort-method with no further deliberation. The assumption we make is thus not a reflection of the full complexity of reality, but one that is pragmatic in terms of the agent execution cycle; the approach to failure-handling of [21] makes the same assumption. In systems such as SPARK, the programmer can specify an alternative behaviour for a failed failure- or abort-method by means of meta-level procedures. We also assume that failure- and abort-methods terminate in finite time.
We provide the semantics for the task and plan failure and aborting processes outlined above. We use the CAN language initially defined in [23] and later extended as CANPLAN in [17] to include a planning component and then as CANPLAN2 in [18] to improve the goal adoption and dropping mechanisms. The extensions also simplified the semantics in the earlier work. We use some of these simplifications for providing a brief summary of the CAN language in Section 4.1. Following a presentation of the operational semantics of our approach in Section 4.2, in Section 4.3 we provide a worked example to clarify the semantics that we present.
CAN is a high-level agent language, in a spirit similar to that of AgentSpeak [15] and Kinny"s Ψ [7], both of which attempt to extract the essence of a class of implemented BDI agent systems. CAN provides an explicit goal construct that captures both the declarative and procedural aspects of a goal. Goals are persistent in CAN in that, when a plan fails, another applicable plan is attempted. This equates to the default failure handling mechanism typically found in implemented BDI systems such as JACK [2].
In practical systems, tasks are typically translated into events that trigger the execution of some plans. This is also true in the CAN language, but, in order to maintain the persistence of goals, a goal construct is introduced. This is denoted by Goal ` φs, P, φf ´ , where φs is the success condition that determines when the goal is considered achieved, φf is a fail condition under which it is considered the goal is no longer achievable or relevant, and P is a program for achieving the goal, which will be aborted once φs or φf become true.
An agent"s behavior is specified by a plan library, denoted by Π, that consists of a collection of plan clauses of the form e : c ← P, where e is an event, c is a context condition (a logical formula over the agent"s beliefs that must be true in order for the plan to be applicable)4 and P is the plan body. The plan body is a program that is defined recursively as follows: P ::= act | +b | −b | ?φ | !e | P1; P2 | P1 P2 | Goal ` φs, P1, φf ´ | P1 P2 | {ψ1 : P1, . . . , ψn : Pn} | nil 4 An omitted c is equivalent to true.
Δ = {ψiθ : Piθ | e : ψi ← Pi ∈ Π ∧ θ = mgu(e, e )} B, !e −→ B, Δ Event ψi : Pi ∈ Δ B |= ψi B, Δ −→ B, Pi Δ \ {ψi : Pi} Select B, P1 −→ B, (P1 P2) −→ B, P2 fail B, P1 −→ B , P1 B, (P1; P2) −→ B , (P ; P2) Sequence B, P1 −→ B , P B, (P1 P2) −→ B , (P P2) Parallel1 B, P2 −→ B , P B, (P1 P2) −→ B , (P P1) Parallel2 Figure 1: Operational rules of CAN. where P1, . . . , Pn are themselves programs, act is a primitive action that is not further specified, and +b and −b are operations to add and delete beliefs. The belief base contains ground belief atoms in the form of first-order relations but could be orthogonally extended to other logics. It is assumed that well-defined operations are provided to check whether a condition follows from a belief set (B |= c), to add a belief to a belief set (B ∪ {b}), and to delete a belief from a belief set (B \ {b}). ?φ is a test for condition φ, and !e5 is an event6 that is posted from within the program. The compound constructs are sequencing (P1; P2), parallel execution (P1 P2), and goals (Goal ` φs, P, φf ´ ).
The above defines the user language. In addition, a set of auxiliary compound forms are used internally when assigning semantics to constructs. nil is the basic (terminating) program. When an event matches a set of plan clauses these are collected into a set of guarded alternatives ( c1 : P1, . . . , cn : Pn ). The other auxiliary compound form, , is a choice operator dual to sequencing: P1 P2 executes P1 and then executes P2 only if P1 failed.
A summary of the operational semantics for CAN in line with [23] and following some of the simplifications of [17] is as follows.
A basic configuration S = B, G, Γ consists of the current belief base B of the agent, the current set of goals G being pursued (i.e., set of formulae), and the current program P being executed (i.e., the current intention).
A transition S0 −→ S1 specifies that executing S0 for a single step yields configuration S1. S0 −→∗ Sn is the usual reflexive transitive closure of −→: Sn is the result of one or more singlestep transitions. A derivation rule S −→ Sr S −→ Sr consists of a (possibly empty) set of premises, which are transitions together with some auxiliary conditions (numerator), and a single transition conclusion derivable from these premises (denominator).
Figure 1 gives some of the operational rules. The Event rule handles task events by collecting all relevant plan clauses for the event in question: for each plan clause e : ψi ← Pi, if there is a most general unifier, θ = mgu(e, e ) of e and the event in 5 Where it is obvious that e is an event we will sometimes exclude the exclamation mark for readability. 6 Typically an achievement goal.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 11 B |= φs B, Goal ` φs, P, φf ´ −→ B, true Gs B |= φf B, Goal ` φs, P, φf ´ −→ B, fail Gf P = Goal ` φs, P , φf ´ P = P1 £ P2 B |= φs ∨ φf B, P −→ B, Goal ` φs, P £ P , φf ´ GI P = P1 £ P2 B |= φs ∨ φf B, P1 −→ B , P B, Goal ` φs, P, φf ´ −→ B , Goal ` φs, P £ P2, φf ´ GS P = P1 £ P2 B |= φs ∨ φf P1 ∈ {true, fail} B, Goal ` φs, P, φf ´ −→ B, Goal ` φs, P2 £ P2, φf ´ GR Figure 2: Rules for goals in CAN. question, then the rule constructs a guarded alternative ψiθ : Piθ.
The Select rule then selects one applicable plan body from a set of (remaining) relevant alternatives: program P Δ states that program P should be tried first, falling back to the remaining alternatives, Δ \ P, if necessary. This rule and the fail rule together are used for failure handling: if the current program Pi from a plan clause for a task fails, rule fail is applied first, and then if possible, rule Select will choose another applicable alternative for the task if one exists. Rule Sequence handles sequencing of programs in the usual way. Rules Parallel1 and Parallel2 define the possible interleaving when executing two programs in parallel.
Figure 2 gives simplified rules for dealing with goals, in line with those presented in [17]. The first rule states that a goal succeeds when φs become true; the second rule states that a goal fails when φf become true. The third rule GI initializes the execution of a goal-program by updating the goal base and setting the program in the goal to P £ P; the first P is to be executed and the second P is used to keep track of the original program for the goal. The fourth rule GS executes a single step of the goal-program. The final rule GR restarts the original program (encoded as P2 of pair P1 £ P2) whenever the current program is finished but the desired and still possible goal has not yet been achieved.
We next introduce the ability to specify handler programs, in the form of failure- and abort-methods, that deal with the clean-up required when a given program respectively fails or is aborted. We do not associate failure- and abort- methods with plan clauses or with tasks (events), but rather we introduce a new program construct that specifies failure- and abort- methods for an arbitrary program. The FAb(P, PF , PA) construct executes the program P. Should P fail, it executes the failure handling program PF ; should P need to be aborted, it executes the abort handling program PA. Thus to add failure- and abort- methods PF and PA to a plan clause e : c ← P, we write e : c ← FAb(P, PF , PA).
With the introduction of the ability to abort programs, we modify the parallel construct to allow the failure of one branch to abort the other. We must take into consideration the possible existence of abort-methods in the aborted branch. Similarly, with the Goal construct we can no longer completely abandon the program the goal contains as soon as the success or failure condition holds; we must now take into consideration the existence of any abort-methods applicable to the program.
We provide the semantics of an augmented agent language containing the FAb construct by defining a source transformation, similar to macro-expansion, that maps a plan library containing the FAb(P, PF , PA) construct into (almost) standard CAN. The one non-standard extension to CAN is a wait-until-condition construct.
We explain this simple modification of the parallel construct below when we come to translation of the Goal construct. First we describe the general nature of the source transformation, which proves to be quite simple for most of the language constructs, and then we concentrate on the three more complex cases: the FAb, parallel, and Goal constructs.
A key issue is that the FAb constructs may be nested, either directly or indirectly. Let us call each instantiation of the construct at execution time a possible abort point (pap). Where these constructs are nested, it is important that before the failure- or abort-method of a parent pap is executed, the failure- or abort-methods programs of the children paps are executed first, as described earlier in Section 3. The need to coordinate the execution of the abort-methods of nested paps requires that there be some way to identify the parents and children of a particular pap. We achieve this as part of the source transformation by explicitly keeping track of the context of execution as an extra parameter on the events and an extra variable within each plan body.7 The source transformation replaces each plan clause of the form e : c ← P with a plan clause e(v) : c ← μv(P) where v is a free variable, not previously present in the plan clause. This variable is used to keep track of the context of execution.
The value of the context variable is a list of identifiers, where each new pap is represented by prepending a new identifier to the context. For example, if the identifiers are integers, the context of one pap may be represented by a list [42, 1] and the context introduced by a new pap may be represented by [52, 42, 1]. We will refer to paps by the context rather than by the new identifier added, e.g., by [51, 42, 1] not 51. This enables us to equate the ancestor relationship between paps with the list suffix relationship on the relevant contexts, i.e., v is an ancestor of v if and only if v is a suffix of v .
For most CAN constructs, the context variable is unused or passed unchanged: μv(act) = act μv(+b) = +b μv(−b) = −b μv(nil) = nil μv(!e) = !e(v) μv(P1; P2) = μv(P1); μv(P2) μv(P1 P2) = μv(P1) μv(P2) μv( ψ1 : P1, . . . , ψn : Pn ) = ψ1 : μv(P1), . . . , ψn : μv(Pn) It remains to specify the transformation μv(·) in three cases: the FAb, parallel, and Goal constructs. These are more complex in that the transformed source needs to create a new pap identifier dynamically, for use as a new context within the construct, and to keep track of when the pap is active (i.e., currently in execution) by adding and removing beliefs about the context.
Let us introduce the primitive action prependID(v, v ) that creates a new pap identifier and prepends it to list v giving list v . We also introduce the following predicates: • a(v) - the pap v is currently active. • abort(v) - the pap v should be aborted (after aborting all of its descendants). 7 An alternative would be to use meta-level predicates that reflect the current state of the intention structure. 12 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • f(v) - the program of pap v has failed. • ancestorof(v, v ) ≡ v = v ∨ ancestorof(v, tail(v ))the pap v is an ancestor of pap v . • nac(v) ≡ ¬∃v .(a(v ) ∧ ancestorof(v, v ) ∧ v = v ) - v has no active children. • sa(v) ≡ ∃v .abort(v ) ∧ ancestorof(v , v) - we should abort v, i.e., abort is true of v or some ancestor; however, we need to wait until no children of v are active. • san(v) ≡ sa(v) ∧ nac(v) - we should abort v now if we should abort v and v has no active children.
First let us consider the case of the FAb construct. The idea is that, whenever a new pap occurs, the prependID(v, v ) action is used to create a new pap identifier list v from the existing list v.
We then add the belief that v is the active context, i.e., +a(v ), and start processing the program within the pap using v instead of v as the context. We need to make sure that we retract the belief that v is active at the end, i.e., −a(v ).
We use the Goal construct to allow us to drop the execution of a program within a pap v when it is necessary to abort. While executing the program P, we know that we need to drop P and invoke its abort-method if some ancestor of P has been told to abort.
This is represented by the predicate sa(v ) being true. However, we need to make sure that we do this only after every child pap has had the chance to invoke its abort-method and all these abort-methods have completed: if we drop the program too soon, then execution of the abort-methods of the children will also be dropped. Therefore, the condition we actually use in the Goal construct to test when to drop the program is san(v ). This condition relies on the fact that as the children paps complete, they remove the relevant a facts.
Our use of the Goal construct is for its ability to drop the execution of a program when conditions are met. To leave aside the repeat execution until a condition is met aspect, we must ensure that the success or failure condition of the construct is satisfied once the execution of the program succeeds or fails. We make sure of this by retracting the fact a(v ) on success and asserting the fact f(v ) on failure, and by having the appropriate success and failure conditions on the Goal. Hence, if the Goal construct fails, then the program either was aborted or it failed. We invoke the relevant failure- or abort- method, retract the a(v ) fact, and then fail.
Putting all this together, we formally define μv(FAb(P, PA, PF )) to be the following, where v is a new variable distinct from any other in the agent"s plan library: prependID(v, v ); +a(v ); Goal ( ¬a(v ), (μv (P); −a(v ) +f(v )), san(v ) ∨ f(v ) ) (((?sa(v ); μv(PA)) μv(PF )); −a(v ); ?false) Second, we must transform the parallel operator to ensure that the failure of one branch safely aborts the other. Here we construct two new contexts, v and v , from the existing context v. If one branch fails, it must abort the other branch. At the end, if either branch was aborted, then we must fail.
Let v and v be new variables distinct from any other in the agent"s plan library. We define μv(P1 P2) to be: prependID(v, v ); prependID(v, v ); +a(v ); +a(v ); ( Goal (¬a(v ), (μv (P1); −a(v ) +f(v )), san(v ) ∨ f(v ) ) (+abort(v ); −a(v )) Goal (¬a(v ), (μv (P2); −a(v ) +f(v )), san(v ) ∨ f(v ) ) (+abort(v ); −a(v )) ); ?¬abort(v ) ∧ ¬abort(v ) Finally, we need to modify occurrences of the Goal construct in two ways: first, to make sure that the abort handling methods are not bypassed when the success or failure conditions are satisfied, and second, to trigger the aborting of the contained program when either the success or failure conditions are satisfied.
To transform the Goal construct we need to extend standard CAN with a wait-until-condition construct. The construct φ : P does not execute P until φ becomes true. We augment the CAN language with the following rules for the guard operator ‘:": B |= φ B, G, (φ : P −→ B, G, P :true B |= φ B, G, (φ : P) −→ B, G, (φ : P) :false In order to specify μv(Goal ` φs, P, φf ´ ), we generate a new pap and execute the program within the Goal construct in this new context. We must ensure that belief a(v ) is removed whether the Goal succeeds or fails. We shift the success and failure condition of the Goal construct into a parallel branch using the wait-until-condition construct, and modify the Goal to use the should abort now condition san(v ) as the success condition. The waiting branch will trigger the abort of the program should either the success or failure condition be met. To avoid any problems with terminating the wait condition, we also end the wait if the pap is no longer active.
Let v be a new variable distinct from any other in the agent"s plan library. We define μv(Goal ` φs, P, φf ´ ) to be: prependID(v, v ); +a(v ); ( Goal ( san(v ), μv (P), false) ; −a(v ); ?φs ) φs ∨ φf ∨ ¬a(v ) : +abort(v ) ) The program P will be repeatedly executed until san(v ) becomes true. There are two ways this can occur. First, if either the success condition φs or the failure condition φf becomes true, then the second branch of the parallel construct executes. This causes abort(v ) to become true, and, after the descendant paps" abortmethods are executed, san(v ) becomes true. In this case, P is now dropped, the a(v ) is removed, and the entire construct succeeds or fails based on φs. The second way for san(v ) to become true is if v or one of its ancestors is aborted. In this case, once the descendant paps" abort-methods are executed, san(v ) becomes true, P is dropped, the a(v ) belief is removed (allowing the second parallel branch to execute, vacuously instructing v to abort), and the first parallel branch fails (assuming φs is false).
Let us look at translation of the IJCAI submission example of Section 2. We will express tasks by events, for example, the task Allocate a Paper Number we express as the event APN. Let the output of the Apply For Clearance task be Y or N, indicating the approval or not of Alice"s manager, respectively. Then we have (at least) the following two plan clauses in CAN, for the Support Meeting Submission and Apply For Clearance tasks, respectively: SMS(m) : isconf(m) ← FAb(!APN; !TWA; (!AFC !TWP); !HPS, !CPN, !CPN) AFC : true ← FAb(!SCR; !WFR(r); ?r = Y, nil, !CCR) Note that Support Meeting Submission has a parameter m, the meeting of interest (IJCAI, in our example), while Apply For Clearance has no parameters.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 13 Let us look first at the translation of the second plan clause, for AFC, since it is the simpler of the two. Let v and v denote new variables. Then we have as the translated plan clause: AFC(v ) : true ← prependID(v , v ); +a(v ); Goal ( ¬a(v ), (!SCR(v ); !WFR(r, v ); ?r = Y; −a(v ) +f(v )), san(v ) ∨ f(v ) ) (((?sa(v ); !CCR(v )) nil); −a(v ); ?false) We can see that an extra context parameter has been added to each task and that the old plan body now appears inside a Goal construct. Should the old plan body succeed, belief a(v ) is retracted, causing the Goal to succeed. If the old plan body fails, or if the task is to be aborted, the Goal construct fails. This is followed by the execution of CCR (in the case of an abort), the retraction of a(v ), and failure.
The translation of the first plan clause, for SMS, is more complex, because of the parallel construct that introduces nested paps: SMS(m, v) : isconf(m) ← prependID(v, v ); +a(v ); Goal ( ¬a(v ), ((!APN(v ); !TWA(v ); prependID(v , v ); prependID(v , v ); +a(v ); +a(v ); ( Goal ( ¬a(v ), (!AFC(v ); −a(v ) +f(v )), san(v ) ∨ f(v ) ) (+abort(v ); −a(v )) Goal ( ¬a(v ), (!TWP(v ); −a(v ) +f(v )), san(v ) ∨ f(v ) ) (+abort(v ); −a(v )) ) ; ?¬abort(v ) ∧ ¬abort(v ); !HPS(v ); −a(v )) +f(v )), san(v ) ∨ f(v ) ) (((?sa(v ); !CPN(v)) !CPN(v)); −a(v ); ?false) Here we can see that if the task !TWP(v ) fails then f(v ) will be asserted, failing the Goal construct that contains it, and leading to abort(v ) being asserted. If the !WFR(r, v ) task in the expansion of !AFC(v ) is still executing and has no active child paps, then sa(v ) and sa(v ) will be true; however, only san(v ) and not san(v ) will be true. This set of conditions will cause the Goal construct in the first plan clause to fail, dropping execution of !WFR(r, v ). The task !CCR(v ) will be executed. Once this task completes, belief a(v ) is retracted, causing san(v ) to become true, leading to the first Goal construct of the second plan clause to fail.
While the translated plan clauses appear complicated, observe that the translation from the initial plan clauses is entirely automated, according to the rules set out in Section 4.2. The translated plan clauses, with the semantics of CAN augmented by our waituntil-condition construct, thus specify the operation of the agent to handle both failure and aborting for the example.
Plan failure is handled in the extended version of AgentSpeak found in the Jason system [6]. Failure clean-up plans are triggered from goal deletion events −!g. Such plans, similar to our failure methods, are designed for the agent to effect state changes (act to undo its earlier actions) prior to possibly attempting another plan to achieve the failed goal g.
Given Jason"s constructs for dropping a goal with an indication of whether or not to try an alternate plan for it, H¨ubner et al. [6] provide an informal description of how a Jason agent modifies its intention structure when a goal failure event occurs. In a goal deletion plan, the programmer can specify any undo actions and whether to attempt the goal again. If no goal deletion plan is provided,
Jason"s default behaviour is to not reattempt the goal. Failure handling is applied only to plans triggered by addition of an achievement or test goal; in particular, goal deletion events are not posted for failure of a goal deletion plan. Further, the informal semantics of [6] do not consider parallel sub-goals (i.e., the CAN construct), since such execution is not part of Jason"s language.
The implementation of H¨ubner et al. [6] requires Jason"s internal actions. A requirement for implementing our approach is a reflective capability in the BDI agent implementation. Suitable implementations of the BDI formalism are JACK [2], Jadex [14], and SPARK [9]. All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states.
Such meta level facilities are also required by the approach of Unruh et al. [21], who define goal-based semantic compensation for an agent. Failure-handling goals are invoked according to failurehandling strategy rules, by a dedicated agent Failure Handling Component (FHC) that tracks task execution. These goals are specified by the agent programmer and attached to tasks, much like our FAb(P, PF , PA) construct associates failure and abort methods with a plan P. Note, however, that in contrast to both [6] and our semantics, [21] attach the failure-handling knowledge at the goal, not plan, level. Their failure-handling goals may consist of stabilization goals that perform localized, immediate clean-up to restore the agent"s state to a known, stable state, and compensation goals that perform undo actions. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure (i.e., if the FHC directs the agent to retry the failed goal and the retry is successful).
The FHC approach is defined at the goal level in order to facilitate abstract specification of failure-handling knowledge; the FHC decides when to address a failure and what to do (i.e., what failurehandling goals to invoke), separating this knowledge from the how of implementing corrective actions (i.e., what plan to execute to meet the adopted failure-handling goal). This contrasts with simplistic plan-level failure handling in which the what and how are intermingled in domain task knowledge. While our approach is defined at the plan level, our extended BDI semantics provides for the separation of execution and failure handling. Further, the FHC explicitly maintains data structures to track agent execution. We leverage the existing execution structures and self-reflective ability of a BDI agent to accomplish both aborting and failure handling without additional overhead. FHC"s failure-handling strategy rules (e.g., whether to retry a failed goal) are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent"s nature (e.g., blindly committed).
The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism (although not tied to any one agent system). Thus no formal semantics are developed in [21]; the FHC"s operation is given as 14 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a state-based protocol. This approach, together with state checkpointing, is used for multi-agent systems in [22]. The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery.
Other work on multi-agent exception handling includes AOEX"s distributed exception handling agents [5], and the similar sentinels of [8]. In both cases, failure-handling logic and knowledge are decoupled from the agents; by contrast, while separating exception handling from domain-specific knowledge, Unruh et al."s FHC and our approach both retain failure-handling logic within an agent.
The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them (perhaps at the request of another agent to do so), or by unbidden factors that lead to failure. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN.
We are planning to implement an instance of our approach in the SPARK agent system [9]; in particular, the work of this paper will be the basis for SPARK"s abort handling mechanism. We are also developing an analysis tool for our extended version of CAN as a basis for experimentation.
An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action. We have assumed the default behaviour of a BDI-style agent, according to its nature: for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain (in which case to fail the task). Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish [19, 12].
Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process. This would in particular influence the commitment the agent has towards a particular task: the higher the cost, the greater the commitment.
Our assumption that abort-methods do not fail, as discussed above, is a pragmatic one. However, this is an issue worthy of further exploration, either to develop weaker assumptions that are also practical, or to analyze conditions under which our assumption is realistic. A further item of interest is extending our approach to failure and abort to maintenance goals [1]. For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves.
Acknowledgements We thank Lin Padgham and the anonymous reviewers for their comments.
The first author acknowledges the support of the Australian Research Council and Agent Oriented Software under grant LP0453486. The work of the two authors at SRI International was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA or the Department of Interior-National Business Center.

This article deals with the problem of collaborative concept learning in a multi-agent system. [6] introduces a characterisation of learning in multi-agent system according to the level of awareness of the agents. At level 1, agents learn ∗The primary author of this paper is a student. in the system without taking into account the presence of other agents, except through the modification brought upon the environment by their action. Level 2 implies direct interaction between the agents as they can exchange messages to improve their learning. Level 3 would require agents to take into account the competencies of other agents, and be able to learn from observation of the other agents" behaviour (while considering them as independant entities and not indetermined part of the environment as in level 1). We focus in this paper on level 2, studying direct interaction between agents involved in a learning process.
Each agent is assumed to be able to learn incrementally from the data he receives, meaning that each agent can update his belief set B to keep it consistent with the whole set of information K that he has received from the environment or from other agents. In such a case, we will say that he is a-consistent. Here, the belief set B represents hypothetical knowledge that can therefore be revised, whereas the set of information K represents certain knowledge, consisting of non revisable observations and facts. Moreover, we suppose that at least a part Bc of the beliefs of each agent is common to all agents and must stay that way. Therefore, an update of this common set Bc by agent r must provoke an update of Bc for the whole community of agents. It leads us to define what is the mas-consistency of an agent with respect to the community. The update process of the community beliefs when one of its members gets new information can then be defined as the consistency maintenance process ensuring that every agent in the community will stay masconsistent. This mas-consistency maintenance process of an agent getting new information gives him the role of a learner and implies communication with other agents acting as critics. However, agents are not specialised and can in turn be learners or critics, none of them being kept to a specific role.
Pieces of information are distributed among the agents, but can be redundant. There is no central memory.
The work described here has its origin in a former work concerning learning in an intentional multi-agent system using a BDI formalism [6]. In that work, agents had plans, each of them being associated with a context defining in which conditions it can be triggered. Plans (each of them having its own context) were common to the whole set of agents in the community. Agents had to adapt their plan contexts depending on the failure or success of executed plans, using a learning mechanism and asking other agents for examples (plans successes or failures). However this work lacked a collective learning protocol enabling a real autonomy of the multi-agent system. The study of such a protocol is the object of the present paper.
In section 2 we formally define the mas-consistency of an update mechanism for the whole MAS and we propose a generic update mechanism proved to be mas consistent. In section 3 we describe SMILE, an incremental multi agent concept learner applying our mas consistent update mechanism to collaborative concept learning. Section 4 describes various experiments on SMILE and discusses various issues including how the accuracy and the simplicity of the current hypothesis vary when comparing single agent learning and mas learning. In section 5 we briefly present some related works and then conclude in section 6 by discussing further investigations on mas consistent learning.
In this section, we present a general formulation of collective incremental learning in a cognitive multi agent system.
We represent a MAS as a set of agents r1, ..., rn. Each agent ri has a belief set Bi consisting of all the revisable knowledge he has. Part of these knowledges must be shared with other agents. The part of Bi that is common to all agents is denoted as BC . This common part provokes a dependency between the agents. If an agent ri updates his belief set Bi to Bi, changing in the process BC into BC , all other agents rk must then update their belief set Bk to Bk so that BC ⊆ Bk.
Moreover, each agent ri has stored some certain information Ki. We suppose that some consistency property Cons(Bi, Ki) can be verified by the agent itself between its beliefs Bi and its information Ki. As said before, Bi represents knowledge that might be revised whereas Ki represents observed facts, taken as being true, and which can possibly contradict Bi.
Definition 1. a-consistency of an agent An agent ri is a-consistent iff Cons(Bi, Ki) is true.
Example 1. Agent r1 has a set of plans which are in the common part BC of B1. Each plan P has a triggering context d(P) (which acts as a pre-condition) and a body. Some piece of information k could be plan P, triggered in situation s, has failed in spite of s being an instance of d(P).
If this piece of information is added to K1, then agent r1 is not a-consistent anymore: Cons(B1, K1 ∪ k) is false.
We also want to define some notion of consistency for the whole MAS depending on the belief and information sets of its constituting elements. We will first define the consistency of an agent ri with respect to its belief set Bi and its own information set Ki together with all information sets K1...Kn from the other agents of the MAS. We will simply do that by considering what would be the a-consistency of the agent if he has the information of all the other agents.
We call this notion the mas-consistency: Definition 2. mas-consistency of an agent An agent ri is mas-consistent iff Cons(Bi, Ki ∪ K) is true, where K = ∪j∈{1,..,n}−{i}Kj 1 is the set of all information from other agents of the MAS. 1 We will note this ∪ Kj when the context is similar.
Example 2. Using the previous example, suppose that the piece of information k is included in the information K2 of agent r2. As long as the piece of information is not transmitted to r1, and so added to K1 , r1 remains a-consistent.
However, r1 is not mas-consistent as k is in the set K of all information of the MAS.
The global consistency of the MAS is then simply the mas-consistency of all its agents.
Definition 3. Consistency of a MAS A MAS r1,...,rn is consistent iff all its agents ri are masconsistent.
We now define the required properties for a revision mechanism M updating an agent ri when it gets a piece of information k. In the following, we will suppose that: • Update is always possible, that is, an agent can always modify its belief set Bi in order to regain its a-consistency. We will say that each agent is locally efficient. • Considering two sets of information Cons(Bi, K1) and Cons(Bi, K2), we also have Cons(Bi, K1 ∪ K2). That is, a-consistency of the agents is additive. • If a piece of information k concerning the common set BC is consistent with an agent, it is consistent with all agents: for all pair of agents (ri,rj) such that Cons(Bi, Ki) and Cons(Bj, Kj) are true, we have, for all piece of information k: Cons(Bi, Ki ∪ k) iff Cons(Bj, Kj ∪ k). In such a case, we will say that the MAS is coherent.
This last condition simply means that the common belief set BC is independent of the possible differences between the belief sets Bi of each agent ri. In the simplest case,
B1 = ... = Bn = BC .
M will also be viewed as an incremental learning mechanism and represented as an application changing Bi in Bi.
In the following, we shall note ri(Bi, Ki) for ri when it is useful.
Definition 4. a-consistency of a revision An update mechanism M is a-consistent iff for any agent ri and any piece of information k reaching ri, the a-consistency of this agent is preserved. In other words, iff: ri(Bi, Ki) a-consistent ⇒ ri(Bi, Ki) a-consistent, where Bi = M(Bi) and Ki = Ki ∪ k is the set of all information from other agents of the MAS.
In the same way, we define the mas-consistency of a revision mechanism as the a-consistency of this mechanism should the agents dispose of all information in the MAS. In the following, we shall note, if needed, ri(Bi, Ki, K) for the agent ri in MAS r1 . . . rn.
Definition 5. mas-consistency of a revision An update mechanism Ms is mas-consistent iff for all agent ri and all pieces of information k reaching ri, the masconsistency of this agent is preserved. In other words, if: ri(Bi, Ki, K) mas-consistent ⇒ ri(Bi, Ki, K) mas-consistent, where Bi = Ms(Bi), Ki = Ki ∪ k, and K = ∪Kj is the set of all information from the MAS.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 165 At last, when a mas-consistent mechanism is applied by an agent getting a new piece of information, a desirable sideeffect of the mechanism should be that all others agents remains mas-consistent after any modification of the common part BC , that is, the MAS itself should become consistent again. This property is defined as follows: Definition 6. Strong mas-consistency of a revision An update mechanism Ms is strongly mas-consistent iff - Ms is mas-consistent, and - the application of Ms by an agent preserves the consistency of the MAS.
mechanism The general idea is that, since information is distributed among all the agents of the MAS, there must be some interaction between the learner agent and the other agents in a strongly mas-consistent update mechanism Ms. In order to ensure its mas-consistency, Ms will be constituted of reiterated applications by the learner agent ri of an internal a-consistent mechanism M, followed by some interactions between ri and the other agents, until ri regain its masconsistency. We describe below such a mechanism, first with a description of an interaction, then an iteration, and finally a statement of the termination condition of the mechanism.
The mechanism is triggered by an agent ri upon receipt of a piece of information k disrupting the mas-consistency.
We shall note M(Bi) the belief set of the learner agent ri after an update, BC the common part modified by ri, and Bj the belief set of another agent rj induced by the modification of its common part BC in BC .
An interaction I(ri, rj) between the learner agent ri and another agent rj, acting as critic is constituted of the following steps: • agent ri sends the update BC of the common part of its beliefs. Having applied its update mechanism, ri is a-consistent. • agent rj checks the modification Bj of its beliefs induced by the update BC . If this modification preserve its a-consistency, rj adopts this modification. • agent rj sends either an acceptation of BC or a denial along with one (or more) piece(s) of information k such that Cons(Bj, k ) is false.
An iteration of Ms will then be composed of: • the reception by the learner agent ri of a piece of information and the update M(Bi) restoring its aconsistency • a set of interactions I(ri, rj) (in which several critic agents can possibly participate). If at least one piece of information k is transmitted to ri, the addition of k will necessarily make ri a-inconsistent and a new iteration will then occur.
This mechanism Ms ends when no agent can provide such a piece of information k . When it is the case, the masconsistency of the learner agent ri is restored.
Proposition 1. Let r1,...,rn be a consistent MAS in which agent ri receives a piece of information k breaking its aconsistency, and M an a-consistent internal update mechanism. The update mechanism Ms described above is strongly mas-consistent.
Proof. The proof directly derives from the mechanism description. This mechanism ensures that each time an agent receives an event, its mas-consistency will be restored.
As the other agents all adopt the final update BC , they are all mas-consistent, and the MAS is consistent. Therefore Ms is a strongly consistent update mechanism.
In the mechanism Ms described above, the learner agent is the only one that receives and memorizes information during the mechanism execution. It ensures that Ms terminates. The pieces of information transmitted by other agents and memorized by the learner agent are redundant as they are already present in the MAS, more precisely in the memory of the critic agents that transmitted them.
Note that the mechanism Ms proposed here does not explicitly indicate the order nor the scope of the interactions.
We will consider in the following that the modification proposal BC is sent sequentially to the different agents (synchronous mechanism). Moreover, the response of a critic agent will only contain one piece of information inconsistent with the proposed modification. We will say that the response of the agent is minimal. This mechanism Ms, being synchronous with minimal response, minimizes the amount of information transmitted by the agents. We will now illustrate it in the case of multi-agent concept learning.
LEARNING
We experiment the mechanism proposed above in the case of incremental MAS concept learning. We consider here a hypothesis language in which a hypothesis is a disjunction of terms. Each term is a conjunction of atoms from a set A. An example is represented by a tag + or − and a description 2 composed of a subset of atoms e ⊆ A. A term covers an example if its constituting atoms are included in the example. A hypothesis covers an example if one of its term covers it.
This representation will be used below for learning boolean formulae. Negative literals are here represented by additional atoms, like not − a. The boolean formulae f =(a ∧ b) ∨ (b ∧ ¬c) will then be written (a ∧ b) ∨ (b ∧ not − c). A positive example of f, like {not − a, b, not − c}, represents a model for f.
The learning process is an update mechanism that, given a current hypothesis H, a memory E = E+ ∪ E− filled with the previously received examples, and a new positive or negative example e, produces a new updated hypothesis. Before this update, the given hypothesis is complete, meaning that it covers all positive examples of E+ , and 2 When no confusion is possible, the word example will be used to refer to the pair (tag, description) as well as the description alone. 166 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) coherent, meaning that it does not cover any negative example of E− . After the update, the new hypothesis must be complete and coherent with the new memory state E ∪ {e}.
We describe below our single agent update mechanism, inspired from a previous work on incremental learning[7].
In the following, a hypothesis H for the target formula f is a list of terms h, each of them being a conjunction of atoms.
H is coherent if all terms h are coherent, and H is complete if each element of E+ is covered by at least one term h of H. Each term is by construction the lgg (least general generalization) of a subset of positives instances {e1, ..., en}[5], that is the most specific term covering {e1, ..., en}. The lgg operator is defined by considering examples as terms, so we denote as lgg(e) the most specific term that covers e, and as lgg(h, e) the most specific term which is more general than h and that covers e. Restricting the term to lgg is the basis of a lot of Bottom-Up learning algorithms (for instance [5]). In the typology proposed by [9], our update mechanism is an incremental learner with full instance memory: learning is made by successive updates and all examples are stored.
The update mechanism depends of the ongoing hypothesis H, the ongoing examples E+ and E− , and the new example e. There are three possible cases: • e is positive and H covers e, or e is negative and H does not cover e. No update is needed, H is already complete and coherent with E ∪ {e}. • e is positive and H does not cover e: e is denoted as a positive counterexample of H. Then we seek to generalize in turn the terms h of H. As soon as a correct generalization h = lgg(h, e) is found, h replaces h in H. If there is a term that is less general that h , it is discarded. If no generalization is correct (meaning here coherent), H ∪ lgg(e) replaces H. • e is negative and H covers e: e is denoted as a negative counterexample of H. Each term h covering e is then discarded from H and replaced by a set of terms {h1, ...., hn} that is, as a whole, coherent with E− ∪ {e} and that covers the examples of E+  uncovered by H − {h}. Terms of the final hypothesis H that are less general than others are discarded from H.
We will now describe the case where e = e− is a covered negative example. The following functions are used here: • coveredOnlyBy(h, E+) gives the subset of E+ covered by h and no other term of H. • bestCover(h1, h2) gives h1 if h1 covers more examples from uncoveredPos than h2, otherwise it gives h2. • covered(h) gives the elements of uncoveredPos covered by h. // Specialization of each h covering e− for each h of H covering e− do H = H − {h} uncoveredPos = coveredOnlyBy(h, E+ ) Ar= atoms that are neither in e− nor in h while (uncoveredPos = ∅) do // seeking the best specialization of h hc=h best=⊥ // ⊥ covers no example for each a of Ar do hc= h ∧ a best = bestCover(hc, best) endfor Ar=Ar−{best} hi=lgg(covered(best)) H = H ∪ {hi} uncoveredPos=uncoveredPos - covered(best) endwhile endfor Terms of H that are less general than others are discarded.
Note that this mechanism tends to both make a minimal update of the current hypothesis and minimize the number of terms in the hypothesis, in particular by discarding terms less general than other ones after updating a hypothesis.
If H is the current hypothesis, Ei the current example memory of agent ri and E the set of all the examples received by the system, the notation of section 2 becomes Bi = BC = H, Ki = Ei and K = E. Cons(H, Ei) states that H is complete and coherent with Ei. In such a case, ri is a-consistent. The piece of information k received by agent ri is here simply an example e along with its tag.
If e is such that the current hypothesis H is not complete or coherent with Ei ∪ {e}, e contradicts H: ri becomes a-inconsistent, and therefore the MAS is not consistent anymore.
The update of a hypothesis when a new example arrives is an a- consistent mechanism. Following proposition 1 this mechanism can be used to produce a strong mas-consistent mechanism: upon reception of a new example in the MAS by an agent r, an update is possibly needed and, after a set of interactions between r and the other agents, results in a new hypothesis shared by all the agents and that restores the consistency of the MAS, that is which is complete and coherent with the set ES of all the examples present in the MAS.
It is clear that by minimizing the number of hypothesis modifications, this synchronous and minimal mechanism minimize the number of examples received by the learner from other agents, and therefore, the total number of examples stored in the system.
In the following, we will learn a boolean formula that is a difficult test for the learning method: the 11-multiplexer (see [4]). It concerns 3 address boolean attributes a0, a1, a2 and 8 data boolean attributes d0, ..., d7. Formulae f11 is satisfied if the number coded by the 3 address attributes is the number of a data attribute whose value is 1. Its formula is the following: f11 = (a0 ∧a1 ∧a2 ∧d7)∨(a0 ∧a1 ∧¬a2 ∧d6)∨(a0 ∧¬a1 ∧ a2 ∧d5)∨(a0 ∧¬a1 ∧¬a2 ∧d4)∨(¬a0 ∧a1 ∧a2 ∧d3)∨(¬a0 ∧ a1 ∧¬a2 ∧d2)∨(¬a0 ∧¬a1 ∧a2 ∧d1)∨(¬a0 ∧¬a1 ∧¬a2 ∧d0).
There are 2048 = 211 possible examples, half of whom are positive (meaning they satisfy f11) while the other half is negative.
An experiment is typically composed of 50 trials. Each run corresponds to a sequence of 600 examples that are incrementally learned by a Multi Agent System with n agents The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 167 (n-MAS). A number of variables such as accuracy, (i.e. the frequency of correct classification of a set of unseen examples), hypothesis size (i.e. the number of terms in the current formula) or number of stored examples, is recorded each time 25 examples are received by the system during those runs.
In the protocol that is used here, a new example is sent to a random agent when the MAS is consistent. The next example will be sent in turn to an other agent when the MAS consistency will have been restored. In such a way we simulate a kind of slow learning: the frequency of example arrivals is slow compared to the time taken by an update.
We briefly discuss here execution time of learning in the MAS. Note that the whole set of action and interaction in the MAS is simulated on a single processor. Figure 1 shows that time linearly depends on the number of agents. At the end of the most active part of learning (200 examples), a 16MAS has taken 4 times more learning time than a 4-MAS.
This execution time represents the whole set of learning and Figure 1: Execution time of a n-MAS (from n = 2 at the bottom to n = 20 on the top). communication activity and hints at the cost of maintaining a consistent learning hypothesis in a MAS composed of autonomous agents.
We study now the distribution of the examples in the MAS memory. Redundancy is written RS = nS/ne, where nS is the total number of examples stored in the MAS, that is the sum of the sizes of agents examples memories Ei, and ne is the total number of examples received from the environment in the MAS. In figure 2, we compare redundancies in 2 to 20 agents MAS. There is a peak, slowly moving from 80 to 100 examples, that represents the number of examples for which the learning is most active. For 20 agents, maximal redundancy is no more than 6, which is far less than the maximal theoretical value of 20. Note that when learning becomes less active, redundancy tends towards its minimal value 1: when there is no more updates, examples are only Figure 2: Redundancy of examples stored in a nMAS (from n = 2 at the bottom to n = 20 on the top) . stored by the agent that receives them.
single agent The proposed mechanism tends to minimize the number of terms in the selected hypothesis. During learning, the size of the current hypothesis grows up beyond the optimum, and then decreases when the MAS converges. In the Multiplexer 11 testbed, the optimal number of terms is 8, but there also exist equivalent formulas with more terms. It is interesting to note that in this case the 10-MAS converges towards an exact solution closer to the optimal number of terms (here 8) (see Figure 3). After 1450 examples have been presented both 1-MAS and 10-MAS have exactly learned the concept (the respective accuracies are 0.9999 and 1) but the single agent expresses in average the result as a 11.0 terms DNF whereas the 10-MAS expresses it as a 8.8 terms DNF.
However for some other boolean functions we found that during learning 1-MAS always produces larger hypotheses than 10-MAS but that both MAS converge to hypotheses with similar size results.
Figure 4 shows the improvement brought by a MAS with n agents compared to a single agent. This improvement was not especially expected, because whether we have one or n agents, when N examples are given to the MAS it has access to the same amount of information, maintains only on ongoing hypothesis and uses the same basic revision algorithm whenever an agent has to modify the current hypothesis.
Note that if the accuracy of 1, 2, 4 and 10-MAS are significantly different, getting better as the number of agents increases, there is no clear difference beyond this point: the accuracy curve of the 100 agents MAS is very close to the one of the 10 agents MAS.
To evaluate this accuracy improvement, we have experimented our protocol on other problems of boolean function learning, As in the Multiplexer-11 case, these functions 168 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Size of the hypothesis built by 1 and 10MAS: the M11 case.
Figure 4: Accuracy of a n-MAS: the M11 case (from bottom to top, n = 1, 2, 4, 10, 100). are learnt in the form of more or less syntactically complex DNF3 (that is with more or less conjunctive terms in the DNF), but are also more or less difficult to learn as it can be difficult to get its way in the hypothesis space to reach them. Furthermore, the presence in the description of irrelevant attributes (that is attributes that does not belong to the target DNF) makes the problem more difficult. The following problems have been selected to experiment our protocol: (i) the multiplexer-11 with 9 irrelevant attributes: M11 9, (ii) the 20-multiplexer M20 (with 4 address bits and 16 data bits), (iii) a difficult parity problem (see [4]) the Xorp m: there must be an odd number of bits with value 1 in the p first attributes for the instance to be positive, the p others bits being irrelevant, and (iv) a simple DNF formula (a ∧ b ∧ c) ∨ (c ∧ d ∧ e)(e ∧ f ∧ g) ∧ (g ∧ h ∧ i) with 19 irrelevant attributes. The following table sums up some information about these problems, giving the total number of attributes including irrelevant ones, the number of irrelevant 3 Disjunctive Normal Forms attributes, the minimal number of terms of the corresponding DNF, and the number of learning examples used.
Pb att. irre. att. terms ex.
M11 11 0 8 200 M11 9 20 9 8 200 M20 20 0 16 450 Xor3 25 28 25 4 200 Xor5 5 10 5 16 180 Xor5 15 20 15 16 600 Simple4-9 19 28 19 4 200 Below are given the accuracy results of our learning mechanism with a single agent and a 10 agents MAS, along with the results of two standard algorithms implemented with the learning environment WEKA[16]: JRip (an implementation of RIPPER[2]) and Id3[12]. For the experiments with JRip and Id3, we measured the mean accuracy on 50 trials, each time randomly separating examples in a learning set and a test set. JRip and Id3 parameters are default parameters, except that JRip is used without pruning. The following table shows the results: Pb JRip Id3 Sm 1 Sm 10 M11 88.3 80.7 88.7 95.5 M11 9 73.4 67.9 66.8 83.5 M20 67.7 62.7 64.6 78.2 Xor3 25 54.4 55.2 71.4 98.5 Xor5 5 52.6 60.8 71.1 78.3 Xor5 15 50.9 51.93 62.4 96.1 Simple4-9 19 99.9 92.3 87.89 98.21 It is clear that difficult problems are better solved with more agents (see for instance xor5 15). We think that these benefits, which can be important with an increasing number of agents, are due to the fact that each agent really memorizes only part of the total number of examples, and this part is partly selected by other agents as counter examples, which cause a greater number of current hypothesis updates and therefore, a better exploration of the hypotheses space.
We did also experiments with some non boolean problems.
We considered only two classes (positive/negative) problems, taken from the UCI"s learning problems database[3].
In all these problems, examples are described as a vector of couples (attribute, value). The value domains can be either boolean, numeric (wholly ordered set), or nominal (non-ordered set). An adequate set of atoms A must be constituted for each problem. For instance, if a is a numeric attribute, we define at most k threshold si, giving k+1 intervals of uniform density4 . Therefore, each distinct threshold si gives two atoms a ≤ si and a > si. In our experiments, we took a maximal number of threshold k = 8. For instance, in the iono problem case, there were 34 numeric attributes, and an instance is described with 506 atoms.
Below are given the accuracy results of our system along with previous results. The column Nb ex. refer to the 4 The probability for the value of a to be in any interval is constant The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 169 number of examples used for learning5 . Column (1) represents minimal and maximal accuracy values for the thirty three classifiers tested in [8]. Column (2) represents the results of [13], where various learning methods are compared to ensemble learning methods using weighted classifiers sets.
Column S-1 and S-10 gives the accuracy of SMILE with respectively 1 and 10 agents.
Pb Nb ex. (1) (2) S-1 S-10 ttt 862/574 // 76.2-99.7 99.7 99.9 kr-vs-kp 2876/958 // 91.4-99.4 96.8 97.3 iono 315 // 88.0-91.8 87.2 88.1 bupa 310 57-72 58-69.3 62.5 63.3 breastw 614 91-97 94.3-97.3 94.7 94.7 vote 391 94-96 95.3-96 91.9 92.6 pima 691 // 71.5- 73.4 65.0 65.0 heart 243 66-86 77.1-84.1 69.5 70.7 This table shows that the incremental algorithm corresponding to the single agent case, gives honorable results relatively to non-incremental classical methods using larger and more complex hypotheses. In some cases, there is an accuracy improvement with a 10 agents MAS. However, with such benchmarks data, which are often noisy, the difficulty does not really come from the way in which the search space is explored, and therefore the improvement observed is not always significant. The same kind of phenomenon have been observed with methods dedicated to hard boolean problems [4].
Here we consider that n single agents learn without interactions and at a given time start interacting thus forming a MAS. The purpose is to observe how the agents take advantage of collaboration when they start from different states of beliefs and memories. We compare in this section a 1-MAS, a 10-MAS (ref) and a 10-MAS (100sync) whose agents did not communicate during the arrival of the first 100 examples (10 by agents). The three accuracy curves are shown in figure 5. By comparing the single agent curve and the synchronized 10-MAS, we can observe that after the beginning of the synchronization, that is at 125 examples, accuracies are identical. This was expected since as soon as an example e received by the MAS contradicts the current hypothesis of the agent ra receiving it, this agent makes an update and its new hypothesis is proposed to the others agents for criticism.
Therefore, this first contradictory example brings the MAS to reach consistency relatively to the whole set of examples present in agents" memories. A higher accuracy, corresponding to a 10-MAS is obtained later, from the 175th example.
In other words, the benefit of a better exploration of the research space is obtained slightly later in the learning process. Note that this synchronization happens naturally in all situations where agents have, for some reason, a divergence between their hypothesis and the system memory. This includes the fusion of two MAS into a single one or the arrival of new agents in an existing MAS.
the effect of a large data stream 5 For ttt and kr-vs-kp, our protocol did not use more than respectively 574 and 958 learning examples, so we put another number in the column.
Figure 5: Accuracies of a 1-MAS, a 10-MAS, and a 10-MAS synchronized after 100 examples.
In this experiment we relax our slow learning mode: the examples are sent at a given rate to the MAS. The resulting example stream is measured in ms−1 , and represents the number of examples sent to the MAS each ms.
Whenever the stream is too large, the MAS cannot reach MAS consistency on reception of an example from the environment before a new example arrives. This means that the update process, started by agent r0 as he received an example, may be unfinished when a new example is received by r0 or another agent r1. As a result, a critic agent may have at instant t to send counterexamples of hypotheses sent by various agents. However as far as the agents, in our setting, memorizes all the examples they receive whenever the stream ends, the MAS necessarily reaches MAS consistency with respect to all the examples received so far. In our experiments, though its learning curve is slowed down during the intense learning phase (corresponding to low accuracy of the current hypotheses), the MAS still reaches a satisfying hypothesis later on as there are less and less counterexamples in the example stream. In Figure 6 we compare the accuracies of two 11-MAS respectively submitted to example streams of different rates when learning the M11 formula.
The learning curve of the MAS receiving an example at a 1/33 ms−1 rate is almost not altered (see Figure 4) whereas the 1/16 ms−1 MAS is first severely slowed down before catching up with the first one.
Since 96 [15], various work have been performed on learning in MAS, but rather few on concept learning. In [11] the MAS performs a form of ensemble learning in which the agents are lazy learners (no explicit representation is maintained) and sell useless examples to other agents. In [10] each agent observes all the examples but only perceive a part of their representation. In mutual online concept learning [14] the agents converge to a unique hypothesis, but each agent produces examples from its own concept representation, thus resulting in a kind of synchronization rather than in pure concept learning. 170 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: Accuracies of two asynchronous 11-MAS (1/33ms−1 and 1/16ms−1 example rates) .
We have presented here and experimented a protocol for MAS online concept learning. The main feature of this collaborative learning mechanism is that it maintains a consistency property: though during the learning process each agent only receives and stores, with some limited redundancy, part of the examples received by the MAS, at any moment the current hypothesis is consistent with the whole set of examples. The hypotheses of our experiments do not address the issues of distributed MAS such as faults (for instance messages could be lost or corrupted) or other failures in general (crash, byzantine faults, etc.). Nevertheless, our framework is open, i.e., the agents can leave the system or enter it while the consistency mechanism is preserved. For instance if we introduce a timeout mechanism, even when a critic agent crashes or omits to answer, the consistency with the other critics (within the remaining agents) is entailed. In [1], a similar approach has been applied to MAS abduction problems: the hypotheses to maintain, given an incomplete information, are then facts or statements.
Further work concerns first coupling induction and abduction in order to perform collaborative concept learning when examples are only partially observed by each agent, and second, investigating partial memory learning: how learning is preserved whenever one agent or the whole MAS forgets some selected examples.
Aknowledgments We are very grateful to Dominique Bouthinon for implementing late modifications in SMILE, so much easing our experiments. Part of this work has been performed during the first author"s visit to the Atelier De BioInformatique of Paris VI university, France.

Reasoning about agents that we observe in the world must integrate two disparate levels. Our observations are often limited to the agent"s external behavior, which can frequently be summarized numerically as a trajectory in space-time (perhaps punctuated by actions from a fairly limited vocabulary). However, this behavior is driven by the agent"s internal state, which (in the case of a human) may involve high-level psychological and cognitive concepts such as intentions and emotions. A central challenge in many application domains is reasoning from external observations of agent behavior to an estimate of their internal state. Such reasoning is motivated by a desire to predict the agent"s behavior.
This problem has traditionally been addressed under the rubric of plan recognition or plan inference. Work to date focuses almost entirely on recognizing the rational state (as opposed to the emotional state) of a single agent (as opposed to an interacting community), and frequently takes advantage of explicit communications between agents (as in managing conversational protocols). Many realistic problems deviate from these conditions.
Increasing the number of agents leads to a combinatorial explosion that can swamp conventional analysis.
Environmental dynamics can frustrate agent intentions.
The agents often are trying to hide their intentions (and even their presence), rather than intentionally sharing information.
An agent"s emotional state may be at least as important as its rational state in determining its behavior.
Domains that exhibit these constraints can often be characterized as adversarial, and include military combat, competitive business tactics, and multi-player computer games.
BEE (Behavioral Evolution and Extrapolation) is a novel approach to recognizing the rational and emotional state of multiple interacting agents based solely on their behavior, without recourse to intentional communications from them. It is inspired by techniques used to predict the behavior of nonlinear dynamical systems, in which a representation of the system is continually fit to its recent past behavior. For nonlinear dynamical systems, the representation is a closed-form mathematical equation. In BEE, it is a set of parameters governing the behavior of software agents representing the individuals being analyzed. The current version of BEE characterizes and predicts the behavior of agents representing soldiers engaged in urban combat [8].
Section 2 reviews relevant previous work. Section 3 describes the architecture of BEE. Section 4 reports results from experiments with the system. Section 5 concludes. Further details that cannot be included here for the sake of space are available in an on-line technical report [16].
BEE bears comparison with previous research in AI (plan recognition), Hidden Markov Models, and nonlinear dynamics systems (trajectory prediction).
Agent theory commonly describes an agent"s cognitive state in terms of its beliefs, desires, and intentions (the so-called BDI model [5, 20]). An agent"s beliefs are propositions about the state of the world that it considers true, based on its perceptions. Its desires are propositions about the world that it would like to be true. Desires are not necessarily consistent with one another: an agent might desire both to be rich and not to work at the same time. An agent"s intentions, or goals, are a subset of its desires that it has selected, based on its beliefs, to guide its future actions.
Unlike desires, goals must be consistent with one another (or at least believed to be consistent by the agent).
An agent"s goals guide its actions. Thus one ought to be able to learn something about an agent"s goals by observing its past actions, and knowledge of the agent"s goals in turn enables conclusions about what the agent may do in the future.
This process of reasoning from an agent"s actions to its goals is known as plan recognition or plan inference. This body of work (surveyed recently at [3]) is rich and varied. It covers both single-agent and multi-agent (e.g., robot soccer team) plans, intentional vs. non-intentional actions, speech vs. non-speech behavior, adversarial vs. cooperative intent, complete vs. incomplete world knowledge, and correct vs. faulty plans, among other dimensions.
Plan recognition is seldom pursued for its own sake. It usually supports a higher-level function. For example, in humancomputer interfaces, recognizing a user"s plan can enable the system to provide more appropriate information and options for user action. In a tutoring system, inferring the student"s plan is a first step to identifying buggy plans and providing appropriate remediation. In many cases, the higher-level function is predicting likely future actions by the entity whose plan is being inferred.
We focus on plan recognition in support of prediction. An agent"s plan is a necessary input to a prediction of its future behavior, but hardly a sufficient one. At least two other influences, one internal and one external, need to be taken into account.
The external influence is the dynamics of the environment, which may include other agents. The dynamics of the real world impose significant constraints.
The environment may interfere with the desires of the agent [4, 10].
Most interactions among agents, and between agents and the world, are nonlinear. When iterated, these can generate chaos (extreme sensitivity to initial conditions).
A rational analysis of an agent"s goals may enable us to predict what it will attempt, but any nontrivial plan with several steps will depend sensitively at each step to the reaction of the environment, and our prediction must take this reaction into account as well. Actual simulation of futures is one way (the only one we know now) to deal with the impact of environmental dynamics on an agent"s actions.
Human agents are also subject to an internal influence. The agent"s emotional state can modulate its decision process and its focus of attention (and thus its perception of the environment). In extreme cases, emotion can lead an agent to choose actions that from the standpoint of a logical analysis may appear irrational.
Current work on plan recognition for prediction focuses on the rational plan, and does not take into account either external environmental influences or internal emotional biases. BEE integrates all three elements into its predictions.
BEE is superficially similar to Hidden Markov Models (HMM"s [19]). In both cases, the agent has hidden internal state (the agent"s personality) and observable state (its outward behavior), and we wish to learn the hidden state from the observable state (by evolution in BEE, by the Baum-Welch algorithm [1] in HMM"s) and then predict the agent"s future behavior (by extrapolation via ghosts in BEE, by the forward algorithm in HMM"s).
BEE offers two important benefits over HMM"s.
First, a single agent"s hidden variables do not satisfy the Markov property. That is, their values at t + 1 depend not only on their values at t, but also on the hidden variables of other agents.
One could avoid this limitation by constructing a single HMM over the joint state space of all of the agents, but this approach is combinatorially prohibitive. BEE combines the efficiency of independently modeling individual agents with the reality of taking into account interactions among them.
Second, Markov models assume that transition probabilities are stationary. This assumption is unrealistic in dynamic situations. BEE"s evolutionary process continually updates the agents" personalities based on actual observations, and thus automatically accounts for changes in the agents" personalities.
Many systems of interest can be described by a vector of real numbers that changes as a function of time. The dimensions of the vector define the system"s state space. One typically analyzes such systems as vector differential equations, e.g., )(xf dt xd .
When f is nonlinear, the system can be formally chaotic, and starting points arbitrarily close to one another can lead to trajectories that diverge exponentially rapidly. Long-range prediction of such a system is impossible. However, it is often useful to anticipate the system"s behavior a short distance into the future. A common technique is to fit a convenient functional form for f to the system"s trajectory in the recent past, then extrapolate this fit into the future (Figure 1, [7]). This process is repeated constantly, providing the user with a limited look-ahead.
This approach is robust and widely applied, but requires systems that can efficiently be described with mathematical equations. BEE extends this approach to agent behaviors, which it fits to observed behavior using a genetic algorithm.
BEE predicts the future by observing the emergent behavior of agents representing the entities of interest in a fine-grained agent simulation. Key elements of the BEE architecture include the model of an individual agent, the pheromone infrastructure through which agents interact, the information sources that guide them, and the overall evolutionary cycle that they execute.
The agents in BEE are inspired by two bodies of work: our previous work on fine-grained agents that coordinate their actions through digital pheromones in a shared environment [2, 13, 17, 18, 21], and the success of previous agentbased combat modeling.
Digital pheromones are scalar variables that agents deposit and sense at their current location a c b d a c b d Figure 1: Tracking a nonlinear dynamical system. a = system state space; b = system trajectory over time; c = recent measurements of system state; d = short-range prediction.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1427 in the environment. Agents respond to local concentrations of these variables tropistically, climbing or descending local gradients. Their movements change the deposit patterns. This feedback loop, together with processes of evaporation and propagation in the environment, support complex patterns of interaction and coordination among the agents [15]. Table 1 shows the BEE"s current pheromone flavors. For example, a living member of the adversary emits a RED-ALIVE pheromone, while roads emit a MOBILITY pheromone.
Our soldier agents are inspired by EINSTein and MANA.
EINSTein [6] represents an agent as a set of six weights, each in [-1, 1], describing the agent"s response to six kinds of information. Four of these describe the number of alive friendly, alive enemy, injured friendly, and injured enemy troops within the agent"s sensor range. The other two weights relate to the agent"s distance to its own flag and that of the adversary, representing objectives that it seeks to protect and attack, respectively. A positive weight indicates attraction to the entity described by the weight, while a negative weight indicates repulsion.
MANA [9] extends the concepts in EINSTein. Friendly and enemy flags are replaced by the waypoints pursued by each side.
MANA includes low, medium, and high threat enemies. In addition, it defines a set of triggers (e.g., reaching a waypoint, being shot at, making contact with the enemy, being injured) that shift the agent from one personality vector to another. A default state defines the personality vector when no trigger state is active.
The personality vectors in MANA and EINSTein reflect both rational and emotive aspects of decision-making. The notion of being attracted or repelled by friendly or adversarial forces in various states of health is an important component of what we informally think of as emotion (e.g., fear, compassion, aggression), and the use of the term personality in both EINSTein and MANA suggests that the system designers are thinking anthropomorphically, though they do not use emotion to describe the effect they are trying to achieve. The notion of waypoints to which an agent is attracted reflects goal-oriented rationality.
BEE uses an integrated rational-emotive personality model.
A BEE agent"s rationality is a vector of seven desires, which are values in [-1, +1]: ProtectRed (the adversary), ProtectBlue (friendly forces), ProtectGreen (civilians), ProtectKeySites,
AvoidCombat, AvoidDetection, and Survive. Negative values reverse the sense suggested by the label. For example, a negative value of ProtectRed indicates a desire to harm Red, and an agent with a high positive desire to ProtectRed will be attracted to REDALIVE, RED-CASUALTY, and MOBILITY pheromone, and will move at maximum speed.
The emotive component of a BEE"s personality is based on the Ortony-Clore-Collins (OCC) framework [11], and is described in detail elsewhere [12]. OCC define emotions as valanced reactions to agents, states, or events in the environment. This notion of reaction is captured in MANA"s trigger states. An important advance in BEE"s emotional model is the recognition that agents may differ in how sensitive they are to triggers. For example, threatening situations tend to stimulate the emotion of fear, but a given level of threat will produce more fear in a new recruit than in a seasoned veteran. Thus our model includes not only Emotions, but Dispositions. Each Emotion has a corresponding Disposition. Dispositions are relatively stable, and considered constant over the time horizon of a run of the BEE, while Emotions vary based on the agent"s disposition and the stimuli to which it is exposed.
Interviews with military domain experts identified the two most crucial emotions for combat behavior as Anger (with the corresponding disposition Irritability) and Fear (whose disposition is Cowardice). Table 2 shows which pheromones trigger which emotions. For example, RED-CASUALTY pheromone stimulates both Anger and Fear in a Red agent, but not in a Blue agent.
Emotions are modeled as agent hormones (internal pheromones) that are augmented in the presence of the triggering environmental condition and evaporate over time.
A non-zero emotion modifies the agent"s actions. Elevated level Anger increases movement likelihood, weapon firing likelihood, and tendency toward an exposed posture. Elevated Fear decreases these likelihoods.
Figure 2 summarizes the BEE"s personality model. The left side is a straightforward BDI model (we prefer the term goal to intention). The right side is the emotive component, where an appraisal of the agent"s beliefs, moderated by the disposition, leads to an emotion that in turn influences the BDI analysis.
Table 1. Pheromone flavors in BEE Pheromone Flavor Description RedAlive RedCasualty BlueAlive BlueCasualty GreenAlive GreenCasualty Emitted by a living or dead entity of the appropriate group (Red = enemy, Blue = friendly,
Green = neutral) WeaponsFire Emitted by a firing weapon KeySite Emitted by a site of particular importance to Red Cover Emitted by locations that afford cover from fire Mobility Emitted by roads and other structures that enhance agent mobility RedThreat BlueThreat Determined by external process (see Section
Table 2: Interactions of pheromones and dispositions/emotions Dispositions/Emotions Red Perspective Blue Perspective Green Perspective Pheromone Irritability /Anger Cowardice /Fear Irritability /Anger Cowardice /Fear Irritability /Anger Cowardice /FearRedAlive X X RedCasualty X X BlueAlive X X X X BlueCasualty X X GreenCasualty X X X X WeaponsFire X X X X X X KeySites X X 1428 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
BEE"s major innovation is extending the nonlinear systems technique of Section 2.2 to agent behaviors. This section describes this process at a high level, then details the multi-page pheromone infrastructure that implements it.
Figure 3 is an overview of Behavior Evolution and Extrapolation. Each active entity in the battlespace has an persistent avatar that continuously generates a stream of ghost agents representing itself. We call the combined modeling entity consisting of avatar and ghosts a polyagent [14].
Ghosts live on a timeline indexed by that begins in the past and runs into the future. is offset with respect to the current time t. The timeline is divided into discrete pages, each representing a successive value of . The avatar inserts the ghosts at the insertion horizon. In our current system, the insertion horizon is at - t = -30, meaning that ghosts are inserted into a page representing the state of the world 30 minutes ago. At the insertion horizon, each ghost"s behavioral parameters (desires and dispositions) are sampled from distributions to explore alternative personalities of the entity it represents.
Each page between the insertion horizon and = t (now) records the historical state of the world at the point in the past to which it corresponds. As ghosts move from page to page, they interact with this past state, based on their behavioral parameters.
These interactions mean that their fitness depends not just on their own actions, but also on the behaviors of the rest of the population, which is also evolving. Because advances faster than real time, eventually = t (actual time). At this point, each ghost is evaluated based on its location compared with the actual location of its corresponding real-world entity.
The fittest ghosts have three functions.
rest of the system as the likely personality of that entity. This information enables us to characterize individual warriors as unusually cowardly or brave.
to the insertion horizon to continue the fitting process.
population of ghosts that run past the avatar's present into the future. Each ghost that runs into the future explores a different possible future of the battle, analogous to how some people plan ahead by mentally simulating different ways that a situation might unfold. Analysis of the behaviors of these different possible futures yields predictions.
Thus BEE has three distinct notions of time, all of which may be distinct from real-world time.
modeled. If BEE is applied to a real-world situation, this time is the same as real-world time. In our experiments, we apply BEE to a simulated battle, and domain time is the time stamp published by the simulator. During actual runs, the simulator is often paused, so domain time runs slower than real time.
When we replay logs from simulation runs, we can speed them up so that domain time runs faster than real time.
domain time corresponding to the state of the world represented on that page, and is offset from the current domain time.
ghosts move from one page to the next.
The relation between shift time and real time depends on the processing resources available.
BEE must operate very rapidly, to keep pace with the ongoing battle. Thus we use simple agents coordinated using pheromone mechanisms.
We have described the basic dynamics of our pheromone infrastructure elsewhere [2]. This infrastructure runs on the nodes of a graph-structured environment (in the case of BEE, a rectangular lattice). Each node maintains a scalar value for each flavor of pheromone, and provides three functions: It aggregates deposits from individual agents, fusing information across multiple agents and through time.
It evaporates pheromones over time, providing an innovative alternative to traditional truth maintenance. Traditionally, knowledge bases remember everything they are told unless they have a reason to forget. Pheromone-based systems immediately begin to forget everything they learn, unless it is continually reinforced. Thus inconsistencies automatically remove themselves within a known period.
It diffuses pheromones to nearby places, disseminating information for access by nearby agents.
The distribution of each pheromone flavor over the environment forms a field that represents some aspect of the state of the world at an instant in time. Each page of the timeline is a complete pheromone field for the world at the BEE time represented by that page. The behavior of the pheromones on each page depends on whether the page represents the past or the future.
Environment Beliefs Desires Goal Emotion Disposition State Process Analysis Action Perception Appraisal Rational Emotive Figure 2: BEE"s Integrated Rational and Emotive Personality Model Ghost time =t(now) Avatar Insertion Horizon Measure Ghost fitness Prediction Horizon Observe Ghost prediction Ghosts ReadPersonality ReadPrediction Entity Ghost time =t(now) Avatar Insertion Horizon Measure Ghost fitness Prediction Horizon Observe Ghost prediction Ghosts ReadPersonality ReadPrediction Entity Figure 3: Behavioral Emulation and Extrapolation. Each avatar generates a stream of ghosts that sample the personality space of its entity. They evolve against the entity"s recent observed behavior, and the fittest ghosts run into the future to generate predictions.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1429 In pages representing the future ( > t), the usual pheromone mechanisms apply. Ghosts deposit pheromone each time they move to a new page, and pheromones evaporate and propagate from one page to the next.
In pages representing the past ( t), we have an observed state of the real world. This has two consequences for pheromone management. First, we can generate the pheromone fields directly from the observed locations of individual entities, so there is no need for the ghosts to make deposits. Second, we can adjust the pheromone intensities based on the changed locations of entities from page to page, so we do not need to evaporate or propagate the pheromones. Both of these simplifications reflect the fact that in our current system, we have complete knowledge of the past.
When we introduce noise and uncertainty, we will probably need to introduce dynamic pheromones in the past as well as the future.
Execution of the pheromone infrastructure proceeds on two time scales, running in separate threads.
The first thread updates the book of pages each time the domain time advances past the next page boundary. At each step,
The former now + 1page is replaced with a new current page, whose pheromones correspond to the locations and strengths of observed units; An empty page is added at the prediction horizon; The oldest page is discarded, since it has passed the insertion horizon.
The second thread moves the ghosts from one page to the next, as fast as the processor allows. At each step,
Ghosts reaching the = t page are evaluated for fitness and removed or evolved; New ghosts from the avatars and from the evolutionary process are inserted at the insertion horizon; A population of ghosts based on the fittest ghosts are inserted at = t to run into the future; Ghosts that have moved beyond the prediction horizon are removed; All ghosts plan their next actions based on the pheromone field in the pages they currently occupy; The system computes the next state of each page, including executing the actions elected by the ghosts, and (in future pages) evaporating pheromones and recording new deposits from the recently arrived ghosts.
Ghost movement based on pheromone gradients is a simple process, so this system can support realistic agent populations without excessive computer load. In our current system, each avatar generates eight ghosts per shift. Since there are about 50 entities in the battlespace (about 20 units each of Red and Blue and about 5 of Green), we must support about 400 ghosts per page, or about 24000 over the entire book.
How fast a processor do we need? Let p be the real-time duration of a page in seconds. If each page represents 60 seconds of domain time, and we are replaying a simulation at 2x domain time, p = 30. Let n be the number of pages between the insertion horizon and = t. In our current system, n = 30. Then a shift rate of n/p shifts per second will permit ghosts to run from the insertion horizon to the current time at least once before a new page is generated. Empirically, this level is a lower bound for reasonable performance, and easily achievable on stock WinTel platforms.
The flexibility of the BEE"s pheromone infrastructure permits the integration of numerous information sources as input to our characterizations of entity personalities and predictions of their future behavior. Our current system draws on three sources of information, but others can readily be added.
Real-world observations.-Observations from the real world are encoded into the pheromone field each increment of BEE time, as a new current page is generated. Table 1 identifies the entities that generate each flavor of pheromone.
Statistical estimates of threat regions.-Statistical techniques1 estimate the level of threat to each force (Red or Blue), based on the topology of the battlefield and the known disposition of forces. For example, a broad open area with no cover is threatening, especially if the opposite force occupies its margins. The results of this process are posted to the pheromone pages as RedThreat pheromone (representing a threat to red) and BlueThreat pheromone (representing a threat to Blue).
AI-based plan recognition.-While plan recognition is not sufficient for effective prediction, it is a valuable input. We dynamically configure a Bayes net based on heuristics to identify the likely goals that each entity may hold.2 The destinations of these goals function as virtual pheromones. Ghosts include their distance to such points in their action decisions, achieving the result of gradient following without the computational expense of maintaining a pheromone field.
We have tested BEE in a series of experiments in which human wargamers make decisions that are played out in a battlefield simulator. The commander for each side (Red and Blue) has at his disposal a team of pucksters, human operators who set waypoints for individual units in the simulator. Each puckster is responsible for four to six units. The simulator moves the units, determines firing actions, and resolves the outcome of conflicts. It is important to emphasize that this simulator is simply a surrogate for a sensor feed from a real-world battlefield
To test our ability to fit personalities based on behavior, one Red puckster responsible for four units is designated the emotional puckster. He selects two of his units to be cowardly (chickens) and two to be irritable (Rambos). He does not disclose this assignment during the run. He moves each unit according to the commander"s orders until the unit encounters circumstances that would trigger the emotion associated with the unit"s disposition. Then he manipulates chickens as though they are fearful (avoiding combat and moving away from Blue), and moves Rambos into combat as quickly as possible. Our software receives position reports on all units, every twenty seconds. 1 This process, known as SAD (Statistical Anomaly Detection), is developed by our colleagues Rafael Alonso, Hua Li, and John Asmuth at Sarnoff Corporation. Alonso and Li are now at SET Corporation. 2 This process, known as KIP (Knowledge-based Intention Projection), is developed by our colleagues Paul Nielsen, Jacob Crossman, and Rich Frederiksen at Soar Technology. 1430 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The difference between the two disposition values (Irritability - Cowardice) of the fittest ghosts proves a better indicator of the emotional state of the corresponding entity than either value by itself. Figure 4 shows the delta disposition for each of the eight fittest ghosts at each time step, plotted against the time in seconds, for a unit played as a chicken. The values clearly trend negative.
Figure 5 shows a similar plot for a Rambo. Rambos tend to die early, and often do not give their ghosts enough time to evolve a clear picture of their personality, but in this case the positive Delta Disposition is evident before the unit"s demise.
To characterize a unit"s personality, we maintain a 800-second exponentially weighted moving average of the Delta Disposition, and declare the unit to be a chicken or Rambo if this value passes a negative or positive threshold, respectively. Currently, this threshold is set at 0.25. We are exploring additional filters. For example, a rapid rate of increase enhances the likelihood of calling a Rambo; units that seek to avoid detection and avoid combat are more readily called chicken.
Table 1 shows the detection results for emotional units in a recent series of experiments. We never called a Rambo a chicken.
In the one case where we called a chicken a Rambo, logs show that in fact the unit was being played aggressively, rushing toward oncoming Blue forces. The brave die young, so we almost never detect units played intentionally as Rambos.
Figure 6 shows a comparison on a separate series of experiments of our emotion detector compared with humans. Two cowards were played in each of eleven games. Human observers in each game were able to detect a total of 13 of the cowards. BEE was able to detect cowards (= chickens) much earlier than the human, while missing only one chicken that the humans detected.
In addition to these results on units intentionally played as emotional, BEE sometimes detects other units as cowardly or brave. Analysis of these units shows that these characterizations were appropriate: units that flee in the face of enemy forces or weapons fire are detected as chickens, while those that stand their ground or rush the adversary are denominated as Rambos.
Each ghost that runs into the future generates a possible path that its unit might follow. The paths in the resulting set over all ghosts vary in how likely they are, the risk they pose to their own or the opposite side, and so forth. In the experiments reported here, we select the future whose ghost receives the most guidance from pheromones in the environment at each step along the way.
In this sense, it is the most likely future. In these experiments, we receive position reports only on units that have actually come within visual range of Blue units, or on average fewer than half of the live Red units at any time.
We evaluate predictions spatially, comparing an entity"s actual location with the location predicted for it 15 minutes earlier. We compare BEE with two baselines: a gametheoretic predictor based on linguistic geometry [22], and estimates by military officers. In both cases, we use a CEP (circular error probable) measure of accuracy, the radius of the circle that one would have to draw around each prediction to capture 50% of the actual unit locations. The higher the CEP measure, the worse the accuracy.
Figure 7 compares our accuracy with that of the gametheoretic predictor. Each point gives the median CEP measure over all predictions in a single run. Points above the diagonal favor BEE, while points below the line favor the game-theoretic predictor. In all but two missions, BEE is more accurate. In one mission, the two systems are comparable, while in one, the gameTable 1: Experimental Results on Fitting Disposition (16 runs) Called Correctly Called Incorrectly Not Called Chickens 68% 5% 27% Rambos 5% 0% 95% Figure 4: Delta Disposition for a Chicken"s Ghosts.
Figure 5: Delta Disposition for a Rambo.
Cowards Found vs Percent of Run Time 0 2 4 6 8 10 12 14 0% 20% 40% 60% 80% 100% Percent of Run Time (Wall Clock) CowardsFound(outof22) Human ARM-A Figure 6: BEE vs. Human.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1431 theoretic predictor is more accurate.
In 18 RAID runs, BEE generated 1405 predictions at each of two time horizons (0 and 15 minutes), while in 18 non-RAID runs, staff generated 102 predictions. Figure. 8 shows a box-andwhisker plot of the CEP measures, in meters, of these predictions.
The box covers the inter-quartile range with a line at the median, whiskers extend to the most distant data points within 1.5 of the interquartile range from the edge of the box, squares show outliers within 3 interquartile ranges, and stars show more distant outliers.
BEE"s median score even at 15 minutes is lower than either Staff median. The Wilcoxon test shows that the difference between the H15 scores is significant at the 99.76% level, while that between the H0 scores is significant at more than 99.999%.
In many domains, it is important to reason from an entity"s observed behavior to an estimate of its internal state, and then to extrapolate that estimate to predict the entity"s future behavior.
BEE performs this task using a faster-than-real-time simulation of swarming agents, coordinated through digital pheromones. This simulation integrates knowledge of threat regions, a cognitive analysis of the agent"s beliefs, desires, and intentions, a model of the agent"s emotional disposition and state, and the dynamics of interactions with the environment. By evolving agents in this rich environment, we can fit their internal state to their observed behavior. In realistic wargames, the system successfully detects deliberately played emotions and makes reasonable predictions about the entities" future behaviors.
BEE can only model internal state variables that impact the agent"s external behavior. It cannot fit variables that the agent does not manifest externally, since the basis for the evolutionary cycle is a comparison of the outward behavior of the simulated agent with that of the real entity. This limitation is serious if our purpose is to understand the entity"s internal state for its own sake. If our purpose of fitting agents is to predict their subsequent behavior, the limitation is much less serious. State variables that do not impact behavior, while invisible to a behavior-based analysis, are irrelevant to a behavioral prediction.
The BEE architecture lends itself to extension in several promising directions.
The various inputs being integrated by the BEE are only an example of the kinds of information that can be handled. The basic principle of using a dynamical simulation to integrate a wide range of influences can be extended to other inputs as well, requiring much less additional engineering than other more traditional ways of reasoning about how different knowledge sources come together in impacting an agent"s behavior.
With such a change in inputs, BEE could be applied more widely than its current domain of adversarial reasoning in urban warfare. Potential applications of interest include computer games, business strategy, and sensor fusion.
Our initial limited repertoire of emotions is a small subset of those that have been distinguished by psychologists, and that might be useful for understanding and projecting behavior. We expect to extend the set of emotions and supporting dispositions that BEE can detect.
The mapping between an agent"s psychological (cognitive and emotional) state and its outward behavior is not one-to-one.
Several different internal states might be consistent with a given observed behavior under one set of environmental conditions, but might yield distinct behaviors under other conditions.
If the environment in the recent past is one that confounds such distinct internal states, we will be unable to distinguish them.
As long as the environment stays in this state, our predictions will be accurate, whichever of the internal states we assign to the agent. If the environment then shifts to one under which the different internal states lead to different behaviors, using the previously chosen internal state will yield inaccurate predictions. One way to address these concerns is to probe the real world, perturbing it in ways that would stimulate distinct behaviors from entities whose psychological state is otherwise indistinguishable. Such probing is an important intelligence technique. BEE"s faster-than-real-time simulation may enable us to identify appropriate probing actions, greatly increasing the effectiveness of intelligence efforts.
This material is based in part upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHC040153. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA or the Department of Interior-National Business Center (DOI-NBC).
Distribution Statement A (Approved for Public Release,
Distribution Unlimited).

For many real-world scenarios, autonomous agents need to operate in dynamic, uncertain environments in which they have only incomplete information about the results of their actions and characteristics of other agents or people with whom they need to cooperate or collaborate. In such environments, agents can benefit from sharing information they gather, pooling their individual experiences to improve their estimations of unknown parameters required for reasoning about actions under uncertainty.
This paper addresses the problem of learning the distribution of the values of a probabilistic parameter that represents a characteristic of a person who is interacting with a computer agent. The characteristic to be learned is (or is clearly related to) an important factor in the agent"s decision making.1 The basic setting we consider is one in which an agent accumulates observations about a specific user characteristic and uses them to produce a timely estimate of some measure that depends on that characteristic"s distribution.
The mechanisms we develop are designed to be useful in a range of application domains, such as disaster rescue, that are characterized by environments in which conditions may be rapidly changing, actions (whether of autonomous agents or of people) and the overall operations occur at a fast pace, and decisions must be made within tightly constrained time frames. Typically, agents must make decisions in real time, concurrent with task execution, and in the midst of great uncertainty. In the remainder of this paper, we use the term fast-paced to refer to such environments. In fast-paced environments, information gathering may be limited, and it is not possible to learn offline or to wait until large amounts of data are collected before making decisions.
Fast-paced environments impose three constraints on any mechanism for learning a distribution function (including the large range of Bayesian update techniques [23]): (a) the no structure constraint: no a priori information about the structure of the estimated parameter"s distribution nor any initial data from which such structure can be inferred is available; (b) the limited use constraint: agents typically need to produce only a small number of estimations in total for this parameter; (c) the early use constraint: high accuracy is a critical requirement even in the initial stages of learning.
Thus, the goal of the estimation methods presented in this paper is to minimize the average error over time, rather than to determine an accurate value at the end of a long period of interaction. That is, the agent is expected to work with the user for a limited time, and it attempts to minimize the overall error in its estimations. In such environments, an agent"s individually acquired data (its own observations) are too sparse for it to obtain good estimations in the requisite time frame. Given the no-structure-constraint of the environment, approaches that depend on structured distributions may result in a significantly high estimation bias.
We consider this problem in the context of a multi-agent distributed system in which computer agents support people who are carrying out complex tasks in a dynamic environment. The fact that agents are part of a multi-agent setting, in which other agents may 1 Learning the distribution rather than just determining some value in the distribution is important whenever the overall shape of the distribution and not just such individual features as mean are important. also be gathering data to estimate a similar characteristic of their users, offers the possibility for an agent to augment its own observations with those of other agents, thus improving the accuracy of its learning process. Furthermore, in the environments we consider, agents are usually accumulating data at a relatively similar rate.
Nonetheless, the extent to which the observations of other agents will be useful to a given agent depends on the extent to which their users" characteristics" distributions are correlated with that of this agent"s user. There is no guarantee that the distribution for two different agents is highly, positively correlated, let alone that they are the same. Therefore, to use a data-sharing approach, a learning mechanism must be capable of effectively identifying the level of correlation between the data collected by different agents and to weigh shared data depending on the level of correlation.
The design of a coordination autonomy (CA) module within a coordination-manager system (as part of the DARPA Coordinators project [18]), in which agents support a distributed scheduling task, provided the initial motivation and a conceptual setting for this work. However, the mechanisms themselves are general and can be applied not only to other fast-paced domains, but also in other multi-agent settings in which agents are collecting data that overlaps to some extent, at approximately similar rates, and in which the environment imposes the no-structure, limited- and early-use constraints defined above (e.g., exploration of remote planets). In particular, our techniques would be useful in any setting in which a group of agents undertakes a task in a new environment, with each agent obtaining observations at a similar rate of individual parameters they need for their decision-making.
In this paper, we present a mechanism that was used for learning key user characteristics in fast-paced environments. The mechanism provides relatively accurate estimations within short time frames by augmenting an individual agent"s direct observations with observations obtained by other agents with which it is coordinating. In particular, we focus on the related problems of estimating the cost of interrupting a person and estimating the probability that that person will have the information required by the system. Our adaptive approach, which we will refer to throughout the paper as selective-sharing, allows our CA to improve the accuracy of its distribution-based estimations in comparison to relying only on the interactions with a specific user (subsequently, self-learning) or pooling all data unconditionally (average all), in particular when the number of available observations is relatively small.
The mechanism was successfully tested using a system that simulates a Coordinators environment. The next section of the paper describes the problem of estimating user-related parameters in fastpaced domains. Section 3 provides an overview of the methods we developed. The implementation, empirical setting, and results are given in Sections 4 and 5. A comparison with related methods is given in Section 6 and conclusions in section 7.
FASTPACED DOMAINS The CA module and algorithms we describe in this paper were developed and tested in the Coordinators domain [21]. In this domain, autonomous agents, called Coordinators, are intended to help maximize an overall team objective by handling changes in the task schedule as conditions of operation change. Each agent operates on behalf of its owner (e.g., the team leader of a firstresponse team or a unit commander) whose schedule it manages.
Thus, the actual tasks being scheduled are executed either by owners or by units they oversee, and the agent"s responsibility is limited to maintaining the scheduling of these tasks and coordinating with the agents of other human team members (i.e., other owners). In this domain, scheduling information and constraints are distributed.
Each agent receives a different view of the tasks and structures that constitute the full multi-agent problem-typically only a partial, local one. Schedule revisions that affect more than one agent must be coordinated, so agents thus must share certain kinds of information. (In a team context they may be designed to share other types as well.) However, the fast-paced nature of the domain constrains the amount of information they can share, precluding a centralized solution; scheduling problems must be solved distributively.
The agent-owner relationship is a collaborative one, with the agent needing to interact with its owner to obtain task and environment information relevant to scheduling. The CA module is responsible for deciding intelligently when and how to interact with the owner for improving the agent"s scheduling. As a result, the CA must estimate the expected benefit of any such interaction and the cost associated with it [19]. In general, the net benefit of a potential interaction is PV − C, where V is the value of the information the user may have, P is the probability that the user has this information, and C is the cost associated with an interaction. The values of P, V , and C are time-varying, and the CA estimates their value at the intended time of initiating the interaction with its owner. This paper focuses on the twin problems of estimating the parameters P and C, both of which are user-centric in the sense of being determined by characteristics of the owner and the environment in which the owner is operating); it presumes a mechanism for determining V [18].
The cost of interrupting owners derives from the potential degradation in performance of tasks they are doing caused by the disruption [1; 9, inter alia]. Research on interaction management has deployed sensor-based statistical models of human interruptibility to infer the degree of distraction likely to be caused by an interruption.
This work aims to reduce interruption costs by delaying interruptions to times that are convenient. It typically uses Bayesian models to learn a user"s current or likely future focus of attention from an ongoing stream of actions. By using sensors to provide continuous incoming indications of the user"s attentional state, these models attempt to provide a means for computing probability distributions over a user"s attention and intentions [9]. Work which examines such interruptibility-cost factors as user frustration and distractability [10] includes work on the cost of repeatedly bothering the user which takes into account the fact that recent interruptions and difficult questions should carry more weight than interruptions in the distant past or straightforward questions [5].
Although this prior work uses interruptibility estimates to balance the interaction"s estimated importance against the degree of distraction likely to be caused, it differs from the fast-paced environments problem we address in three ways that fundamentally change the nature of the problem and hence alter the possible solutions. First, it considers settings in which the computer system has information that may be relevant to its user rather than the user (owner) having information needed by the system, which is the complement of the information exchange situation we consider.
Second, the interruptibility-estimation models are task-based. Lastly, it relies on continuous monitoring of a user"s activities.
In fast-paced environments, there usually is no single task structure, and some of the activities themselves may have little internal structure. As a result, it is difficult to determine the actual attentional state of agent-owners [15]. In such settings, owners must make complex decisions that typically involve a number of other members of their units, while remaining reactive to events that diverge from expectations [24]. For instance, during disaster rescue, a first-response unit may begin rescuing survivors trapped in a burning house, when a wall collapses suddenly, forcing the unit to The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 203 retract and re-plan their actions.
Prior work has tracked users" focus of attention using a range of devices, including those able to monitor gestures [8] and track eyegaze to identify focus of visual attention [13, 20], thus enabling estimations of cognitive load and physical indicators of performance degradation. The mechanisms described in this paper also presume the existence of such sensors. However, in contrast to prior work, which relies on these devices operating continuously, our mechanism presumes that fast-paced environments only allow for the activation of sensors for short periods of time on an ad-hoc basis, because agents" resources are severely limited.
Methods that depend on predicting what a person will do next based only on what the user is currently doing (e.g., MDPs) are not appropriate for modeling focus of attention in fast-paced domains, because an agent cannot rely on a person"s attentional state being well structured and monitoring can only be done on a sporadic, non-continuous basis. Thus, at any given time, the cost of interaction with the user is essentially probabilistic, as reflected over a single random monitoring event, and can be assigned a probability distribution function. Consequently, in fast-paced environments, an agent needs a sampling strategy by which the CA samples its owner"s interruptibility level (with some cost) and decides whether to initiate an interaction at this specific time or to delay until a lower cost is observed in future samplings. The method we describe in the remainder of this subsection applies concepts from economic search theory [16] to this problem. The CA"s cost estimation uses a mechanism that integrates the distribution of an owner"s interruptibility level (as estimated by the CA) into an economic search strategy, in a way that the overall combined cost of sensor costs and interaction costs is minimized.
In its most basic form, the economic search problem aims to identify an opportunity that will minimize expected cost or maximize expected utility. The search process itself is associated with a cost, and opportunities (in our case, interruption opportunities) are associated with a stationary distribution function. We use a sequential search strategy [16] in which one observation is drawn at a time, over multiple search stages. The dominating strategy in this model is a reservation-value based strategy which determines a lower bound, and keeps drawing samples as long as no opportunity above the bound was drawn.
In particular, we consider the situation in which an agent"s owner has an interruption cost described by a probability distribution function (pdf) f(x) and a cumulative distribution function (cdf) F(x).
The agent can activate sensing devices to get an estimation of the interruption cost, x, at the current time, but there is a cost c of operating the sensing devices for a single time unit. The CA module sets a reservation value and as long as the sensor-based observation x is greater than this reservation value, the CA will wait and re-sample the user for a new estimation.
The expected cost, V (xrv), using such a strategy with reservation value xrv is described by Equation 1,
V (xrv) = c + R xrv y=0 yf(y) F(xrv) , (1) which decomposes into two parts. The first part, c divided by F(xrv), represents the expected sampling cost. The second, the integral divided by F(xrv), represents the expected cost of interruption, because the expected number of search cycles is (random) geometric and the probability of success is F(xrv). Taking the derivative of the left-hand-side of Equation 1 and equating it to zero, yields the characteristics of the optimal reservation value, namely x∗ rv must satisfy,
V (x∗ rv) = x∗ rv. (2) Substituting (2) in Equation 1 yields Equation 3 (after integration by parts) from which the optimal reservation value, x∗ rv, and consequently (from Equation 2) V (x∗ rv) can be computed. c = Z x∗ rv y=0 F(y) (3) This method, which depends on extracting the optimal sequence of sensor-based user sampling, relies heavily on the structure of the distribution function, f(x). However, we need only a portion of the distribution function, namely from the origin to the reservation value. (See Equation 1 and Figure 1.) Thus, when we consider sharing data, it is not necessary to rely on complete similarity in the distribution function of different users. For some parameters, including the user"s interruptibility level, it is enough to rely on similarity in the relevant portion of the distribution function. The implementation described in Sections 4-5 relies on this fact.
Figure 1: The distribution structure affecting the expected cost"s calculation
Information One way an agent can estimate the probability a user will have information it needs (e.g., will know at a specific interruption time, with some level of reliability, the actual outcome of a task currently being executed) is to rely on prior interactions with this user, calculating the ratio between the number of times the user had the information and the total number of interactions. Alternatively, the agent can attempt to infer this probability from measurable characteristics of the user"s behavior, which it can assess without requiring an interruption. This indirect approach, which does not require interrupting the user, is especially useful in fast-paced domains.
The CA module we designed uses such an indirect method: ownerenvironment interactions are used as a proxy for measuring whether the owner has certain information. For instance, in Coordinatorslike scenarios, owners may obtain a variety of information through occasional coordination meetings of all owners, direct communication with other individual owners participating in the execution of a joint task (through which they may learn informally about the existence or status of other actions they are executing), open communications they overhear (e.g. if commanders leave their radios open, they can listen to messages associated with other teams in their area), and other formal or informal communication channels [24]. Thus, owners" levels of communication with others, which can be obtained without interrupting them, provide some indication of the frequency with which they obtain new information. Given occasional updates about its owner"s level of communication, the CA can estimate the probability that a random interaction with the owner will yield the information it needs. Denoting the probability distribution function of the amount of communication the user generally maintains with its environment by g(x), and using the transformation function Z(x), mapping from a level of communication, x, to a probability of having the information, the expected probability of getting the information that is needed from the owner when interrupting at a given time can be calculated from P = Z ∞ 0 Z(x)g(x)dy. (4) 204 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The more observations an agent can accumulate about the distribution of the frequency of an owner"s interaction with the environment at a given time, the better it can estimate the probability the owner has the information needed by the system.
This section presents the selective-sharing mechanism by which the CA learns the distribution function of a probabilistic parameter by taking advantage of data collected by other CAs in its environment. We first explain the need for increasing the number of observations used as the basis of estimation and then present a method for determining how much data to adopt from other agents.
The most straightforward method for the CA to learn the distribution functions associated with the different parameters characterizing an owner is by building a histogram based on the observations it has accumulated up to the estimation point. Based on this histogram, the CA can estimate the parameter either by taking into account the entire range of values (e.g., to estimate the mean) or a portion of it (e.g., to find the expected cost when using a reservation-value-based strategy). The accuracy of the estimation will vary widely if it is based on only a small number of observations.
For example, Figure 2 illustrates the reservation-value-based cost calculated according to observations received from an owner with a uniform interruption cost distribution U(0, 100) as a function of the number of accumulated observations used for generating the distribution histogram. (In this simulation, device activation cost was taken to be c = 0.5).
Figure 2: The convergence of a single CA to its optimal strategy These deviations from the actual (true) value (which is 10 in this case, according to Equation 3) is because the sample used in each stage cannot accurately capture the actual structure of the distribution function. Eventually this method yields a very accurate estimation for the expected interruption cost. However, in the initial stages of the process, its estimation deviates significantly from the true value. This error could seriously degrade the CA"s decision making process: underestimating the cost may result in initiating costly, non-beneficial interactions, and overestimating the cost might result in missing opportunities for valuable interactions. Any improvement that can be achieved in predicting the cost values, especially in the initial stages of learning, can make a significant difference in performance, especially because the agent is severely limited in the number of times it can interact with its owner in fastpaced domains.
One way to decrease the deviation from the actual value is by augmenting the data the CA acquires by observing its owner with observations made by other owners" agents. Such an approach depends on identifying other owners with distribution functions for the characteristic of interest similar to the CA"s owner. This dataaugmentation idea is simple: different owners may exhibit similar basic behaviors or patterns in similar fast-paced task scenarios.
Since they are all coordinating on a common overall task and are operating in the same environment, it is reasonable to assume some level of similarity in the distribution function of their modeled parameters. People vary in their behavior, so, obviously, there may be different types of owners: some will emphasize communication with their teams, and some will spend more time on map-based planning; some will dislike being disturbed while trying to evaluate their team"s progress, while others may be more open to interruptions. Consequently, an owner"s CA is likely to be able to find some CAs that are working with owners who are similar to its owner.
When adopting data collected by other agents, the two main questions are which agents the CA should rely on and to what extent it should rely on each of them. The selective-sharing mechanism relies on a statistical measure of similarity that allows the CA of any specific user to identify the similarity between its owner and other owners dynamically. Based on this similarity level, the CA decides if and to what degree to import other CAs" data in order to augment its direct observations, and thus to enable better modeling of its owner"s characteristics.
It is notable that the cost of transferring observations between different CA modules of different agents is relatively small. This information can be transferred as part of regular negotiation communication between agents. The volume of such communication is negligible: it involves just the transmission of new observations" values.
In our learning mechanism, the CA constantly updates its estimation of the level of similarity between its owner and the owners represented by other CAs in the environment. Each new observation obtained either by that CA or any of the other CAs updates this estimation. The similarity level is determined using the Wilcoxon rank-sum test (Subsection 3.1).
Whenever it is necessary to produce a parameter estimate, the CA decides on the number of additional observations it intends to rely on for extracting its estimation. The number of additional observations to be taken from each other agent is a function of the number of observations it currently has from former interactions with its owner and the level of confidence the CA has in the similarity between its owner and other owners. In most cases, the number of observations the CA will want to take from another agent is smaller than the overall number of observations the other agent has; thus, it randomly samples (without repetitions) the required number of observations from this other agent"s database. The additional observations the CA takes from other agents are used only to model its owner"s characteristics. Future similarity level determination is not affected by this information augmentation procedure.
We use a nonparametric method (i.e., one that makes no assumptions about the parametric form of the distributions each set is drawn from), because user characteristics in fast-paced domains do not have the structure needed for parametric approaches. Two additional advantages of a non-parametric approach are their usefulness for dealing with unexpected, outlying observations (possibly problematic for a parametric approach), and the fact that non-parametric approaches are computationally very simple and thus ideal for settings in which computational resources are scarce.
The Wilcoxon rank-sum test we use is a nonparametric alternative to the two-sample t-test [22, 14]2 . While the t-test compares means, the Wilcoxon test can be used to test the null hypothesis that two populations X and Y have the same continuous distribution. We assume that we have independent random samples {x1, x2, ..., xm} and {y1, y2, ..., yn}, of sizes m and n respectively, from each population. We then merge the data and rank each measurement from lowest to highest. All sequences of ties are assigned an average rank. From the sum of the ranks of the smaller 2 Chi-Square Goodness-of-Fit Test is for a single sample and thus not suitable.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 205 sample, we calculate the test statistic and extract the level of confidence for rejecting the null hypothesis. This level of confidence becomes the measure for the level of similarity between the two owners. The Wilcoxon test does not require that the data originates from a normally distributed population or that the distribution is characterized by a finite set of parameters.
Correctly identifying the right number of additional observations to gather is a key determinant of success of the selective-sharing mechanism. Obviously, if the CA can identify another owner who has identical characteristics to the owner it represents, then it should use all of the observations collected by that owner"s agent.
However, cases of identical matches are likely to be very uncommon.
Furthermore, even to establish that another user is identical to its own, the CA would need substantial sample sizes to have a relatively high level of confidence. Thus, usually the CA needs to decide how much to rely on another agent"s data while estimating various levels of similarity with a changing level of confidence.
At the beginning of its process, the selective-sharing mechanism has almost no data to rely on, and thus no similarity measure can be used. In this case, the CA module relies heavily on other agents, in the expectation that all owners have some basic level of similarity in their distribution (see Section 2). As the number of its direct observations increases, the CA module refines the number of additional observations required. Again, there are two conflicting effects. On one hand, the more data the CA has, the better it can determine its level of confidence in the similarity ratings it has for other owners. On the other hand, assuming there is some difference among owners (even if not noticed yet), as the number of its direct observations increases, the owner"s own data should gain weight in its analysis. Therefore, when CAi decides how many additional observations, Oi j should be adopted from CAj"s database, it calculates Oi j as follows: Oi j = N ∗ (1 − αi,j) √ N + 2 + ln(N) N (5) where N is the number of observations CAi already has (which is similar in magnitude to the number of observations CAj has) and αi,j is the confidence of rejecting the Wilcoxon null hypothesis.
The function in Equation 5 ensures that the number of additional observations to be taken from another CA module increases as the confidence in the similarity with the source for these additional observations increases. At the same time, it ensures that the level of dependency on external observations decreases as the number of direct observations increases. When calculating the parameter αi,j, we always perform the test over the interval relevant to the originating CA"s distribution function. For example, when estimating the cost of interrupting the user, we apply the Wilcoxon test only for observations in the interval that starts from zero and ends slightly to the right of the formerly estimated RV (see Figure 1).
We tested the selective-sharing mechanism in a system that simulates a distributed, Coordinators-like MAS. This testbed environment includes a variable number of agents, each corresponding to a single CA module. Each agent is assigned an external source (simulating an owner) which it periodically samples to obtain a value from the distribution being estimated. The simulation system enabled us to avoid unnecessary inter-agent scheduling and communication overhead (which are an inherent part of the Coordinators environment) and thus to better isolate the performance and effectiveness of the estimation and decision-making mechanisms.
The distribution functions used in the experiments (i.e., the distribution functions assigned to each user in the simulated environment) are multi-rectangular shaped. This type of function is ideal for representing empirical distribution functions. It is composed of k rectangles, where each rectangle i is defined over the interval (xi−1, xi), and represents a probability pi, ( Pk i=1 pi =1). For any value x in rectangle i, we can formulate F(x) and f(x) as: f(x) = pi xi − xi−1 F(x) = i−1X j=1 pj + (x − xi−1)pi xi − xi−1 (6) For example, the multi-rectangular function in Figure 3 depicts a possible interruption cost distribution for a specific user. Each rectangle is associated with one of the user"s typical activities, characterized by a set of typical interruption costs. (We assume the distribution of cost within each activity is uniform.) The rectangular area represents the probability of the user being engaged in this type of activity when she is randomly interrupted. Any overlap between the interruption costs of two or more activities results in a new rectangle for the overlapped interval. The user associated with the above distribution function spends most of her time in reporting (notice that this is the largest rectangle in terms of area), an activity associated with a relatively high cost of interruption. The user also spends a large portion of her time in planning (associated with a very high cost of interruption), monitoring his team (with a relatively small interruption cost) and receiving reports (mid-level cost of interruption). The user spends a relatively small portion of her time in scouting the enemy (associated with relatively high interruption cost) and resting.
Figure 3: Representing interruption cost distribution using a multi-rectangular function Multi-rectangular functions are modular and allow the representation of any distribution shape by controlling the number and dimensions of the rectangles used. Furthermore, these functions have computational advantages, mostly due to the ability to re-use many of their components when calculating the optimal reservation value in economical search models. They also fit well the parameters the CA is trying to estimate in fast-paced domains, because these parameters are mostly influenced by activities in which the user is engaged.
The testbed system enabled us to define either hand-crafted or automatically generated multi-rectangular distribution functions. At each step of a simulation, each of the CAs samples its owner (i.e., all CAs in the system collect data at a similar rate) and then estimates the parameter (either the expected cost when using the sequential interruption technique described in Section 2 or the probability that the owner will have the required information) using one of the following methods: (a) relying solely on direct observation (self-learning) data; (b) relying on the combined data of all other agents (average all); and, (c) relying on its own data and selective portions of the other agents" data based on the selective-sharing mechanism described in Section 3.
We present the results in two parts: (1) using a specific sample environment for illustrating the basic behavior of the selectivesharing mechanism; and (2) using general environments that were automatically generated. 206 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
To illustrate the gain obtained by using the selective-sharing mechanism, we used an environment of 10 agents, associated with 5 different interruptibility cost distribution function types. The table in Figure 4 details the division of the 10 agents into types, the dimensions of the rectangles that form the distribution functions, and the theoretical mean and reservation value (RV) (following Equation 3) with a cost c = 2 for sensing the interruption cost. Even though the means of the five types are relatively similar, the use of a reservation-value based interruption strategy yields relatively different expected interruption costs (RV , following Equation 2).
The histogram in this figure depicts the number of observations obtained for each bin of size 1 out of a sample of 100000 observations taken from each type"s distribution function.
Type Agents Rect. Range prob mean RV I 1,2 1 0-20 0.40 50 14.1 2 20-80 0.20 3 80-100 0.40 II 3,4,5,6 1 0-40 0.25 50 25.3 2 40-60 0.50 3 60-100 0.25 III 7 1 0-80 0.10 85 56.6 2 80-100 0.90 IV 8,9 1 0-60 0.60 48 20.0 2 60-90 0.40 V 10 1 0-100 1.00 50 20.0 0 500 1000 1500 2000 2500 1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 type I type II type III type IV type V #ofobservations range Figure 4: Users" interruptibility cost distribution functions (5 types) Figure 5 gives CA performance in estimating the expected cost of interruption when using the reservation-value based interruption initiation technique. Each graph presents the average prediction accuracy (in terms of the absolute deviation from the theoretical value, so the lower the curve the better the performance) of a different type, based on 10000 simulation runs. The three curves in each graph represent the methods being compared (self-learning, average all, and selective-sharing). The data is given as a function of the accumulated number of observations collected. The sixth graph in the figure is the average for all types, weighted according to the number of agents of each type. Similarly, the following table summarizes the overall average performance in terms of the absolute deviation from the theoretical value of each of the different methods: Iterations Self-Learning Averaging-All Selective-Sharing % Improvement3 5 20.08 8.70 9.51 53% 15 12.62 7.84 8.14 36% 40 8.16 7.42 6.35 22% Table 1: Average absolute error along time Several observations may be made from Figure 5. First, although the average-all method may produce relatively good results, it quickly reaches stagnation, while the other two methods exhibit continuous improvement as a function of the amount of accumulated data. For the Figure 4 environment, average-all is a good strategy for agents of type II, IV and V, because the theoretical reservation value of each of these types is close to the one obtained based on the aggregated distribution function (i.e., 21.27).4 However, for types I and III for which the optimal RV differs from that value, the average-all method performs significantly worse. Overall, the sixth graph and the table above show that while in this specific environment the average-all method works well in the first interactions, it 3 The improvement is measured in percentages relative to the self-learning method. 4 The value is obtained by constructing the weighted aggregated distributed function according to the different agents" types and extracting the optimal RV using Equation 3. 0 4 8 12 16 20 1 6 11 16 21 26 31 36 Type I 0 4 8 12 16 20 1 6 11 16 21 26 31 36 selective sharing self-learning average all 0 4 8 12 16 20 1 6 11 16 21 26 31 36 Type II 0 8 16 24 32 40 1 6 11 16 21 26 31 36 Type III 0 4 8 12 16 20 1 6 11 16 21 26 31 36 Type IV 0 4 8 12 16 20 1 6 11 16 21 26 31 36 Type V 0 4 8 12 16 20 1 6 11 16 21 26 31 36 Weighted Average Figure 5: Average absolute deviation from the theoretical RV in each method (10000 runs) is quickly outperformed by the selective-sharing mechanism.
Furthermore, the more user observations the agents accumulate (i.e., as we extend the horizontal axis), the better the other two methods are in comparison to average-all. In the long run (and as shown in the following subsection for the general case), the average-all method exhibits the worst performance.
Second, the selective-sharing mechanism starts with a significant improvement in comparison to relying on the agent"s own observations, and then this improvement gradually decreases until finally its performance curve coincides with the self-learning method"s curve. The selective-sharing mechanism performs better or worse, depending on the type, because the Wilcoxon test cannot guarantee an exact identification of similarity; different combinations of distribution function can result in an inability to exactly identify the similar users for some of the specific types. For example, for type I agents, the selective-sharing mechanism actually performs worse than self-learning in the short term (in the long run the two methods" performance converge). Nevertheless, for the other types in our example, the selective-sharing mechanism is the most efficient one, and outperforms the other two methods overall.
Third, it is notable that for agents that have a unique type (e.g., agent III), the selective-sharing mechanism quickly converges towards relying on self-collected data. This behavior guarantees that even in scenarios in which users are completely different, the method exhibits a graceful initial degradation but manages, within a few time steps, to adopt the proper behavior of counting exclusively on self-generated data.
Last, despite the difference in their overall distribution function, agents of type IV and V exhibit similar performance because the relevant portion of their distribution functions (i.e., the effective parts that affect the RV calculation as explained in Figure 1) is identical. Thus, the selective-sharing mechanism enables the agent of type V, despite its unique distribution function, to adopt relevant information collected by agents of types IV which improves its estimation of the expected interruption cost.
To evaluate selective-sharing, we ran a series of simulations in which the environment was randomly generated. These experiments focused on the CAs" estimations of the probability that the user would have the required information if interrupted. They used a multi-rectangular probability distribution function to represent The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 207 the amount of communication the user is engaged in with its environment. We models the growth of the probability the user has the required information as a function of the amount of communication using the logistic function,5 G(x) = 1 + e −x 12 1 + 60e −x 12 . (7) The expected (mean) value of the parameter representing the probability the user has the required information is thus μ = Z ∞ y=0 G(y)f(y)dy = kX i=1 hx + 708ln(60 + e x 12 )pi 60(xi − xi−1) ixi xi−1 (8) where k is the number of rectangles used. We ran 10000 simulation runs. For each simulation, a new 20-agent environment was automatically generated by the system, and the agents were randomly divided into a random number of different types.6 For each type, a random 3-rectangle distribution function was generated. Each simulation ran 40 time steps. At each time step each one of the agents accumulated one additional observation. Each CA calculated an estimate of the probability its user had the necessary information according to the three methods, and the absolute error (difference from the theoretical value calculated according to Equation 8) was recorded. The following table summarizes the average performance of the three mechanisms along different time horizons (measured at 5, 15 and 40 time steps): Iterations Self-Learning Averaging-All Selective-Sharing % Improvement 5 0.176 0.099 0.103 41.4% 15 0.115 0.088 0.087 23.9% 40 0.075 0.082 0.065 13.6% Table 2: Average absolute error along time steps As can be seen in the table above, the proposed selective-sharing method outperforms the two other methods over any execution in which more than 15 observations are collected by each of the agents.
As in the sample environment, the average-all method performs well in the initial few time steps, but does not exhibit further improvement. Thus, the more data collected, the greater the difference between this latter method and the two other methods. The average difference between selective-sharing and self-learning decreases as more data is collected.
Finally, we measured the effect of the number of types in the environment. For this purpose, we used the same self-generation method, but controlled the number of types generated for each run.
The number of types is a good indication for the level of heterogeneity in the environment. For each number of types, we ran 10000 simulations. Figure 6 depicts the performance of the different methods (for a 40-observation collection period for each agent).
Since all simulation runs used for generating Figure 6 are based on the same seed, the performance of the self-learning mechanism is constant regardless of the number of types in the environment. As expected, the average-all mechanism performs best when all agents are of the same type; however its performance deteriorates as the number of types increases. Similarly, the selective-sharing mechanism exhibits good results when all agents are of the same type, and as the number of types increases, its performance deteriorates.
However, the performance decrease is significantly more modest in comparison to the one experienced in the average-all mechanism. 5 The specific coefficients used guarantee an S-like curve of growth, along the interval (0, 100), where the initial stage of growth is approximately exponential, followed by asymptotically slowing growth. 6 In this suggested environment-generation scheme there is no guarantee that every agent will have a potential similar agent to share information with. In those non-rare scenarios where the CA is the only one of its type, it will rapidly need to stop relying on others. 0
1 2 3 4 5 Self Learning Average All Selective Sharing number of types averageabsoluteerror Figure 6: Average absolute deviation from actual value in 20 agent scenarios as a function of the agents" heterogeneity level Overall, the selective-sharing mechanism outperforms both other methods for any number of types greater than one.
In addition to the interruption management literature reviewed in Section 2, several other areas of prior work are relevant to the selective-sharing mechanism described in this paper.
Collaborative filtering, which makes predictions (filtering) about the interests of a user [7], operates similarly to selective-sharing.
However, collaborative filtering systems exhibit poor performance when there is not sufficient information about the users and when there is not sufficient information about a new user whose taste the system attempts to predict [7].
Selective-sharing relies on the ability to find similarity between specific parts of the probability distribution function associated with a characteristic of different users. This capability is closely related to clustering and classification, an area widely studied in machine learning. Given space considerations, our review of this area is restricted to some representative approaches for clustering. In spite of the richness of available clustering algorithms (such as the famous K-means clustering algorithm [11], hierarchical methods, Bayesian classifiers [6], and maximum entropy), various characteristics of fast-paced domains do not align well with the features of attributesbased clustering mechanisms, suggesting these mechanisms would not perform well in such domains. Of particular importance is that the CA needs to find similarity between functions, defined over a continuous interval, with no distinct pre-defined attributes. An additional difficulty is defining the distance measure.
Many clustering techniques have been used in data mining [2], with particular focus on incremental updates of the clustering, due to the very large size of the databases [3]. However the applicability of these to fast-paced domains is quite limited because they rely on a large set of existing data. Similarly, clustering algorithms designed for the task of class identification in spatial databases (e.g., relying on a density-based notion [4]) are not useful for our case, because our data has no spatial attributes.
The most relevant method for our purposes is the Kullback-Leibler relative entropy index that is used in probability theory and information theory [12]. This measure, which can also be applied on continuous random variables, relies on a natural distance measure from a true probability distribution (either observation-based or calculated) to an arbitrary probability distribution. However, the method will perform poorly in scenarios in which the functions alternate between different levels while keeping the general structure and moments. For example, consider the two functions f(x) = ( x mod2)/100 and g(x) = ( x mod2)/100 defined over the interval (0, 200). While these two functions are associated with almost identical reservation values (for any sampling cost) and mean, the Kullback-Leibler method will assign a poor correlation between 208 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) them, while our Wilcoxon-based approach will give them the highest rank in terms of similarity.
While the Wilcoxon test is a widely used statistical procedure [22, 14], it is usually used for comparing two sets of single-variate data. To our knowledge, no attempt has been made yet to extend its properties as an infrastructure for determining with whom and to what extent information should be shared, as presented in this paper. Typical use of this non-parametric tool includes detection of rare events in time series (e.g., a hard drive failure prediction [17]) and bioinformatics applications (e.g., finding informative genes from microarray data). In these applications, it is used primarily as an identification tool and ranking criterion.
The selective-sharing mechanism presented in this paper does not make any assumptions about the format of the data used or about the structure of the distribution function of the parameter to be estimated. It is computationally lightweight and very simple to execute. Selective-sharing allows an agent to benefit from other agents" observations in scenarios in which data sources of the same type are available. It also guarantees, as a fallback, performance equivalent to that of a self-learner when the information source is unique. Furthermore, selective-sharing does not require any prior knowledge about the types of information sources available in the environment or of the number of agents associated with each type.
The results of our simulations demonstrate the selective-sharing mechanism"s effectiveness in improving the estimation produced for probabilistic parameters based on a limited set of observations.
Furthermore, most of the improvement is achieved in initial interactions, which is of great importance for agents operating in fast-paced environments. Although we tested the selective-sharing mechanism in the context of the Coordinators project, it is applicable in any MAS environment having the characteristics of a fast-paced environment (e.g., rescue environments). Evidence for its general effectiveness is given in the general evaluation section, where environments were continuously randomly generated.
The Wilcoxon statistic used as described in this paper to provide a classifier for similarity between users provides high flexibility with low computational costs and is applicable for any characteristic being learned. Its use provides a good measure of similarity which an agent can use to decide how much external information to adopt for its assessments.
The research reported in this paper was supported in part by contract number 55-000720, a subcontract to SRI International"s DARPA Contract No. FA8750-05-C-0033. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the U.S. Government. We are grateful to an anonymous AAMAS reviewer for an exceptionally comprehensive review of this paper.
[1] P. Adamczyk, S. Iqbal, and B. Bailey. A method, system, and tools for intelligent interruption management. In TAMODIA "05, pages 123-126, New York, NY, USA, 2005. ACM Press. [2] P. Berkhin. Survey of clustering data mining techniques.
Technical report, Accrue Software, San Jose, CA, 2002. [3] M. Ester, H. Kriegel, J. Sander, M. Wimmer, and X. Xu.
Incremental clustering for mining in a data warehousing environment. In Proc. 24th Int. Conf. Very Large Data Bases,
VLDB, pages 323-333, 24-27 1998. [4] M. Ester, H. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD-96, pages 226-231, 1996. [5] M. Fleming and R. Cohen. A decision procedure for autonomous agents to reason about interaction with humans.
In AAAI Spring Symp. on Interaction between Humans and Autonomous Systems over Extended Operation, 2004. [6] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine Learning, 29:131-163, 1997. [7] N. Good, J. Ben Schafer, J. Konstan, A. Borchers, B. Sarwar,
J. Herlocker, and J. Riedl. Combining collaborative filtering with personal agents for better recommendations. In AAAI/IAAI, pages 439-446, 1999. [8] K. Hinckley, J. Pierce, M. Sinclair, and E. Horvitz. Sensing techniques for mobile interaction. In UIST "00, pages 91-100, New York, NY, USA, 2000. ACM Press. [9] E. Horvitz, C. Kadie, T. Paek, and D. Hovel. Models of attention in computing and communication: from principles to applications. Commun. ACM, 46(3):52-59, 2003. [10] B. Hui and C. Boutilier. Who"s asking for help?: a bayesian approach to intelligent assistance. In IUI "06, 2006. [11] J. Jang, C. Sun, and E. Mizutani. Neuro-Fuzzy and Soft Computing A Computational Approach to Learning and Machine Intelligence. Prentice Hall, 1997. [12] S. Kullback and R. Leibler. On information and sufficiency.
Ann. Math. Statist., 22:79-86, 1951. [13] P. Maglio, T. Matlock, C. Campbell, S. Zhai, and B. Smith.
Gaze and speech in attentive user interfaces. In ICMI, pages 1-7, 2000. [14] H. Mann and D. Whitney. On a test of whether one of 2 random variables is stochastically larger than the other.
Annals of Mathematical Statistics, 18:50-60, 1947. [15] W. McClure. Technology and command: Implications for military operations in the twenty-first century. Maxwell Air Force Base, Center for Strategy and Technology, 2000. [16] J. McMillan and M. Rothschild. Search. In Robert J. Aumann and Amsterdam Sergiu Hart, editors, Handbook of Game Theory with Economic Applications, pages 905-927. 1994. [17] J. Murray, G. Hughes, and K. Kreutz-Delgado. Machine learning methods for predicting failures in hard drives: A multiple-instance application. J. Mach. Learn. Res., 6:783-816, 2005. [18] D. Sarne and B. J. Grosz. Estimating information value in collaborative multi-agent planning systems. In AAMAS"07, page (to appear), 2007. [19] D. Sarne and B. J. Grosz. Timing interruptions for better human-computer coordinated planning. In AAAI Spring Symp. on Distributed Plan and Schedule Management, 2006. [20] R. Vertegaal. The GAZE groupware system: Mediating joint attention in multiparty communication and collaboration. In CHI, pages 294-301, 1999. [21] T. Wagner, J. Phelps, V. Guralnik, and R. VanRiper. An application view of coordinators: Coordination managers for first responders. In AAAI, pages 908-915, 2004. [22] F Wilcoxon. Individual comparisons by ranking methods.
Biometrics, 1:80-83, 1945. [23] D. Zeng and K. Sycara. Bayesian learning in negotiation. In AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, pages 99-104, 1996. [24] Y. Zhang, K. Biggers, L. He, S. Reddy, D. Sepulvado, J. Yen, and T. Ioerger. A distributed intelligent agent architecture for simulating aggregate-level behavior and interactions on the battlefield. In SCI-2001, pages 58-63, 2001.

Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15]. In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents. In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15]. While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors. First of all, similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes. Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms.
In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms. Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions. Particularly, agents maintain estimates, namely expected utility, on the downstream agents" capabilities of providing relevant documents for specific types of incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. This process is conducted in an iterative manner.
The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents. This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.
Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the contentsimilarity measure among agents" document collections in a bottom-up fashion. In the past work, we have shown that this organization improves search performance significantly.
However, this organizational structure does not take into account the arrival patterns of queries, including their frequency, types, and where they enter the system, nor the available communication bandwidth of the network and processing capabilities of individual agents. The intention of the reinforcement learning is to adapt the agents" routing decisions to the dynamic network situations and learn from past search sessions. Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.
To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.
The remainder of this paper is organized as follows: Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.
Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results. Section 5 discusses related studies and Section 6 concludes the paper.
P2P IR SYSTEMS This section briefly reviews our basic approaches to hierarchical P2P IR systems. In a hierarchical P2P IR system illustrated in Fig.1, agents are connected to each other through three types of links: upward links, downward links, and lateral links. In the following sections, we denote the set of agents that are directly connected to agent Ai as DirectConn(Ai), which is defined as DirectConn(Ai) = NEI(Ai) ∪ PAR(Ai) ∪ CHL(Ai) , where NEI(Ai) is the set of neighboring agents connected to Ai through lateral links; PAR(Ai) is the set of agents whom agent Ai is connected to through upward links and CHL(Ai) is the set of agents that agent Ai connects to through downward links. These links are established through a bottom-up content-similarity based distributed clustering process[15]. These links are then used by agents to locate other agents that contain documents relevant to the given queries.
A typical agent Ai in our system uses two queues: a local search queue, LSi, and a message forwarding queue MFi.
The states of the two queues constitute the internal states of an agent. The local search queue LSi stores search sessions that are scheduled for local processing. It is a priority queue and agent Ai always selects the most promising queries to process in order to maximize the global utility. MFi consists of a set of queries to forward on and is processed in a FIFO (first in first out) fashion. For the first query in MFi, agent Ai determines which subset of its neighboring agents to forward it to based on the agent"s routing policy πi. These routing decisions determine how the search process is conducted in the network. In this paper, we call Ai as Aj"s upstream agent and Aj as Ai"s downstream agent if A4 A5 A6 A7 A2 A3 A9 NEI(A2)={A3} PAR(A2)={A1} CHL(A2)={A4,A5} A1 A8 Figure 1: A fraction of a hierarchical P2PIR system an agent Ai routes a query to agent Aj.
The distributed search protocol of our hierarchical agent organization is composed of two steps. In the first step, upon receipt of a query qk at time tl from a user, agent Ai initiates a search session si by probing its neighboring agents Aj ∈ NEI(Ai) with the message PROBE for the similarity value Sim(qk, Aj) between qk and Aj. Here, Ai is defined as the query initiator of search session si. In the second step,
Ai selects a group of the most promising agents to start the actual search process with the message SEARCH. These SEARCH messages contain a TTL (Time To Live) parameter in addition to the query. The TTL value decreases by 1 after each hop. In the search process, agents discard those queries that either have been previously processed or whose TTL drops to 0, which prevents queries from looping in the system forever. The search session ends when all the agents that receive the query drop it or TTL decreases to 0. Upon receipt of SEARCH messages for qk, agents schedule local activities including local searching, forwarding qk to their neighbors, and returning search results to the query initiator. This process and related algorithms are detailed in [15, 14].
BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity comparison between incoming queries and Ai"s neighboring agents in order to forward those queries to relevant agents without flooding the network with unnecessary query messages.
However, this heuristic is myopic because a relevant direct neighbor is not necessarily connected to other relevant agents. In this section, we propose a more general approach by framing this problem as a reinforcement learning task.
In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode. In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15]. On the other hand, in the learning mode, in parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.
Note that in the learning protocol, the learning process does not interfere with the distributed search process. Agents can choose to initiate and stop learning processes without affecting the system performance. In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra communication costs incurred by the learning algorithm.
The section is structured as follows, Section 3.1 describes 232 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model. Section 3.2 describes a protocol to deploy the learning algorithm in the network.
Section 3.3 discusses the convergence of the learning algorithm.
An agent"s routing policy takes the state of a search session as input and output the routing actions for that query.
In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query. QL is an attribute of qk that indicates which type of queries qk most likely belong to. The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set. The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network. Future work includes exploring how learning can be accomplished when this assumption does not hold. Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].
The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }. An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai). The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj, denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai) under the policy πi(QSj), denoted as πi(QSj, αik ).
Therefore, equation (2) means that the probability of forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on. Under this stochastic policy, the routing action is nondeterministic. The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.
The expected utility, Un i (QSj), is used to estimate the potential utility gain of routing query type QSj to agent Ai under policy πn i . The superscript n indicates the value at the nth iteration in an iterative learning process. The expected utility provides routing guidance for future search sessions.
In the search process, each agent Ai maintains partial observations of its neighbors" states, as shown in Fig. 2. The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load information, Lm. These observations are updated periodically by the neighbors. The estimated utility information will be used to update Ai"s expected utility for its routing policy.
Load Information Expected Utility For Different Query Types Neighboring Agents ...
A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ......
Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ...
QS0 QS1 ...
Figure 2: Agent Ai"s Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Am"s message-forward queue. Therefore Lm characterizes the utilization of an agent"s communication channel, and thus provide non-local information for Am"s neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents. Note that based on the characteristics of the queries entering the system and agents" capabilities, the loading of agents may not be uniform. After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing probability in its routing policy.
Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead.
An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy. In this iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors. Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.
In particular, at time n, given a set of expected utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj). The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents. The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%. Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.
Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded. The dn+1 i value is updated based on dn i , the previous and current observations on the traffic situation in the neighborhood. Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ai"s neighborhood, i.e Li, and its previous value. (2) For each agent Aik in the PDn(Ai, QSj), the probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).
In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set. The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored. β is determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the possibilities. In order to balance between exploitation and exploration, a λ-Greedy approach is taken. In the λ-Greedy approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3). Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.
In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).
The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj). πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai). Note that the exploration rate λ is not a constant and it decreases overtime. The λ is determined according to the following equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit.
Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents. Under the assumption that after a query is forwarded to Ai"s neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa. Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session. P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.
Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation. In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value.
After a search session stops when its TTL values expires, all search results are returned back to the user and are compared against the relevance judgment. Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise. where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results. This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned. This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.
The details of the actual routing protocol will be introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems.
This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.
In the single-phase search algorithm, search sessions start from the initiators of the queries. In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2. Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.
Before learning starts, each agent initializes the expected utility value for all possible states as 0. Thereafter, upon receipt of a query, in addition to the normal operations described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its downstream agents. Once the timer expires or it has received response from all its downstream agents, Ai merges and forwards the search results accrued from its downstream agents to its upstream agent. Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results. Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its downstream agents can be evaluated. The duration of the timer is related to the TTL value. In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.
The search results will eventually be returned to the search session initiator A0. They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections). The reward will be calculated and propagated backward to the agents along the way that search results were passed. This is a reverse process of the search results propagation. In the process of propagating reward backward, agents update estimates of their own potential utility value, generate an upto-dated policy and pass their updated results to the neighboring agents based on the algorithm described in Section 3.
Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state. This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai. Each neighboring agent, Aj, reacts to this kind of update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value. Once they complete the update, the agents would again in turn inform related neighbors to update their values. This process goes on until the TTL value in the update message increases to the TTL limit.
To speed up the learning process, while updating the expected utility values of an agent Ai"s neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence. This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t.
In formalizing the content routing system as a learning task, many assumptions are made. In real systems, these assumptions may not hold, and thus the learning algorithm may not converge. Two problems are of particular note, (1) This content routing problem does not have Markov properties. In contrast to IP-level based packet routing, the routing decision of each agent for a particular search session sj depends on the routing history of sj. Therefore, the assumption that all subsequent search sessions are independent does not hold in reality. This may lead to double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1. However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure. With the absense of the cycles, the estimates inside the tree would be close to the accurate value.
Secondly, the stochastic nature of the routing policy partly remedies this problem.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations. In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].
This paper explores several approaches to speed up the learning process. Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their neighbors whenever its expected utility is updated. Thus a faster convergence speed can be achieved. This strategy contrasts to the Lazy update, where agents only echo their neighboring agents with their expected utility change when they exchange information. The trade off between the two approaches is the network load versus learning speed.
The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged. Instead, agent just have to determine the classification of the query properly and follow the learned policies. The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes. There are many potential extensions for this learning model. For example, a single measure is currently used to indicate the traffic load for an agent"s neighborhood. A simple extension would be to keep track of individual load for each neighbor of the agent.
The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and TREC123-100. The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results.
TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed. TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data dissemination framework for large scale distributed agent network based organizations. TRANO supports importation and exportation of agent organization profiles including topological connections and other features. Each TRANO agent is composed of an agent view structure and a control unit. In simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents .
In our experiment, we use two standard datasets,
TRECVLC-921 and TREC-123-100 datasets, to simulate the collections hosted on agents. The TREC-VLC-921 and TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.
In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections. It is observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13]. Hence, TREC-VLC-921 is much closer to real document distributions in P2P environments. Furthermore, TREC-123-100 is split into two sets of 0
0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search session) versus the number of search sessions for 1phase search in TREC-VLC-921 0
0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source. The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively. The documents in each subcollection in dataset TREC-123-100-Source are more coherent than those in TREC-123-100-Random. The two different sets of partitions allow us to observe how the distributed learning algorithm is affected by the homogeneity of the collections.
The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15]. During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al. [9] with parameters α = 0.5 and β = 0.6. In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.
Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n! e−λ 236 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent. In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit. The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units respectively. In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50.
Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921. It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time. The average reward for SSLA approach starts at the same level with the SSNA algorithm. But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.
Figure 4 shows the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data collection TREC-VLC-921. The TSNA approach has a relatively consistent performance with the average reward ranges from
learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system. The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.
Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively. It illustrates that the cumulative utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms accelerate when more queries enter the system. These experimental results demonstrate that learning-based approaches consistently perform better than non-learning based routing algorithm. Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total number of the relevant documents in that area. Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster. On the contrary, the two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents. The TSLA combines the merits of both approaches and outperforms them.
Table 1 lists the cumulative utility for datasets TREC123-100-Random and TREC-123-100-Source with hierarchical organizations. The five columns show the results for four different approaches. In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach. The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach. There are two numbers in each cell in the column TSLA-Random. The first number is the actual cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach. Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively. Table 1 shows that the performance improvement for TREC-123-100-Random is not as significant as the other datasets. This is because that the documents in the sub-collection of TREC-123-100-Random are selected randomly which makes the collection model, the signature of the collection, less meaningful. Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.
Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably enhance the system performance for both data collections.
However, it remains as future work to discover the correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the heterogeneity between the sub-collections.
The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks. In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process. IP-level Routing problems have been attacked from the reinforcement learning perspective[2, 5, 11, 12]. These studies have explored fully distributed algorithms that are able, without central coordination to disseminate knowledge about the network, to find the shortest paths robustly and efficiently in the face of changing network topologies and changing link costs. There are two major classes of adaptive, distributed packet routing algorithms in the literature: distance-vector algorithms and link-state algorithms. While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks. In this domain, the destination of a packet is deterministic and unique. Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors. A variant of Q-Learning techniques is deployed The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 237 Table 1: Cumulative Utility for Datasets TREC-123-100-Random and TREC-123-100-Source with Hierarchical Organization; The percentage numbers in the columns TSLA-Random and TSLA-Source demonstrate the performance gain over the algorithm without learning Query number TSNA-Random TSLA-Random TSNA-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to update the estimations to converge to the real distances.
It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies[3]. In P2P based content sharing systems, this property is exemplified by the phenomenon that users tend to send queries that represent only a limited number of topics and conversely, users in the same neighborhood are likely to share common interests and send similar queries [10]. The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property. This is because the users" traffic and query patterns can reduce the state space and speed up the learning process. Related work in taking advantage of this property include [7], where the authors attempted to address this problem by user modeling techniques.
In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms. Particularly, agents maintain estimates, namely expected utility, on the downstream agents" ability to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents modify their routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time.

Considering to be a decentralized control problem, information searching and sharing in large-scale systems of cooperative agents is a hard problem in the general case: The computation of an optimal policy, when each agent possesses an approximate partial view of the state of the environment and when agents" observations and activities are interdependent (i.e. one agent"s actions affect the observations and the state of an other) [3], is hard. This fact, has resulted to efforts that either require agents to have a global view of the system [15], to heuristics [4], to precomputation of agents" information needs and information provision capabilities for proactive communication [17], to localized reasoning processes built on incoming information [12,13,14], and to mathematical frameworks for coordination whose optimal policies can be approximated [11] for small (sub-) networks of associated agents.
On the other hand, there is a lot of research on semantic peer to peer search networks and social networks [1,5,6,8,9,10,16,18,19] many of which deal with tuning a network of peers for effective information searching and sharing. They do it mostly by imposing logical and semantic overlay structures. However, as far as we know there is no work that demonstrates the effectiveness of a gradual tuning process in large-scale dynamic networks that studies the impact of the information gathered by agents as more and more queries are issued and served in concurrent sessions in the network.
The main issue in this paper concerns ‘tuning" a network of agents, each with a specific expertise, for efficient and effective information searching and sharing, without altering the topology or imposing an overlay structure via clustering, introduction of shortcut indices, or re-wiring. ‘ " is the task of sharing and gathering the knowledge for agents to propagate requests to the acquaintances, minimizing the searching effort, increasing the efficiency and the benefit of the system.
Specifically, this paper proposes a method for information searching and sharing in dynamic and large scale networks, which combines routing indices with token-based methods for information sharing in large-scale multi-agent systems.
This paper is structured as follows: Section 2 presents related work and motivates the proposed method. Section 3 states the problem and section 4 presents in detail the individual techniques and the overall proposed method. Section 5 presents the experimental setup and results, and section 6 concludes the paper, sketching future work.
Information provision and sharing can be considered to be a decentralized partially-observable Markov decision process [3,4,11,14]. In the general case, decentralized control of largescale dynamic systems of cooperative agents is a hard problem.
Optimal solutions can only be approximated by means of heuristics, by relaxations of the original problem or by centralized solutions. The computation of an optimal control policy is simple given that global states can be factored, that the probability of transitions and observations are independent, the observations combined determine the global state of the system and the reward function can be easily defined as the sum of local reward functions [3].
However, in a large-scale dynamic system with decentralized control it is very hard for agents to possess accurate partial views of the environment, and it is even more hard for agents to possess a global view of the environment. Furthermore, agents" observations can not be assumed independent, as one agent"s actions can affect the observations of others: For instance, when one agent joins/leaves the system, then this may affect other agents" assessment of neighbours" information provision abilities.
Furthermore, the probabilities of transitions can be dependent too; something that increases the complexity of the problem: For example, when an agent sends a query to another agent, then this may affect the state of the latter, as far as the assessed interests of the former are concerned.
Considering independent activities and observations, authors in [4] propose a decision-theoretic solution treating standard action and information exchange as explicit choices that the decision maker must make. They approximate the solution using a myopic algorithm. Their work differs in the one reported here in the following aspects: First, it aims at optimizing communication, while the goal here is to tune the network for effective information sharing, reducing communication and increasing system"s benefit. Second, the solution is approximated using a myopic algorithm, but authors do not demonstrate how suboptimal are the solutions computed (something we neither do), given their interest to the optimal solution. Third, they consider that transitions and observations made by agents are independent, which, as already discussed, is not true in the general case. Last, in contrast to their approach where agents broadcast messages, here agents decide not only when to communicate, but to whom to send a message too.
Token based approaches are promising for scaling coordination and therefore information provision and sharing to large-scale systems effectively. In [11] authors provide a mathematical framework for routing tokens, providing also an approximation to solving the original problem in case of independent agents" activities. The proposed method requires a high volume of computations that authors aim to reduce by restricting its application to static logical teams of associated agents. In accordance to this approach, in [12,13,14], information sharing is considered only for static networks and self-tuning of networks is not demonstrated. As it will be shown in section 5, our experiments show that although these approaches can handle information sharing in dynamic networks, they require a larger amount of messages in comparison to the approach proposed here and can not tune the network for efficient information sharing.
Proactive communication has been proposed in [17] as a result of a dynamic decision theoretic determination of communication strategies. This approach is based on the specification of agents as providers and needers: This is done by a plan-based precomputation of information needs and provision abilities of agents. However, this approach can not scale to large and dynamic networks, as it would be highly inefficient for each agent to compute and determine its potential needs and information provision abilities given its potential interaction with 100s of other agents.
Viewing information retrieval in peer-to-peer systems from a multi-agent system perspective, the approach proposed in [18] is based on a language model of agents" documents collection.
Exploiting the models of other agents in the network, agents construct their view of the network which is being used for forming routing decisions. Initially, agents build their views using the models of their neighbours. Then, the system reorganizes by forming clusters of agents with similar content. Clusters are being exploited during information retrieval using a kNN approach and a gradient search scheme. Although this work aims at tuning a network for efficient information provision (through reorganization), it does not demonstrate the effectiveness of the approach with respect to this issue. Moreover, although during reorganization and retrieval they measure the similarity of content between agents, a more fine grained approach is needed that would allow agents to measure similarities of information items or sub-collections of information items. However, it is expected that this will complicate re-organization. Based on their work on peer-to-peer systems, H.Zhand and V.Lesser in [19] study concurrent search sessions. Dealing with static networks, they focus on minimizing processing and communication bottlenecks: Although we deal with concurrent search sessions, their work is orthogonal to ours, which may be further extended towards incorporating such features in the future.
Considering research in semantic peer-to-peer systems1 , most of the approaches exploit what can be loosely stated a routing index. A major question concerning information searching is what information has to be shared between peers, when, and what adjustments have to be made so as queries to be routed to trustworthy information sources in the most effective and efficient way.
REMINDIN" [10] peers gather information concerning the queries that have been answered successfully by other peers, so as to subsequently select peers to forward requests to: This is a lazy learning approach that does not involve advertisement of peer information provision abilities. This results in a tuning process where the overall recall increases over time, while the number of messages per query remains about the same. Here, agents actively advertise their information provision abilities based on the assessed interests of their peers: This results in a much lower number of messages per query than those reported in REMINDIN".
In [5,6] peers, using a common ontology, advertise their expertise, which is being exploited for the formation of a semantic overlay network: Queries are propagated in this network depending on their similarity with peers" expertise. It is on the receiver"s side to decide whether it shall accept or not an advertisement, based on the similarity between expertise descriptions. According to our approach, agents advertise selectively their information provision abilities about specific topics to their neighbours with similar information interests (and only to these). However, this is done as time passes and while agents" receive requests from their peers.
The gradual creation of overlay networks via re-wiring, shortcuts creation [1,8,16] or clustering of peers [17,9] are tuning approaches that differ fundamentally from the one proposed here: Through local interactions, we aim at tuning the network for efficient information provision by gathering routing information gradually, as queries are being propagated in the network and 1 General research in peer-to-peer systems concentrates either on network topologies or on distribution of documents: Approaches do not aim to optimize advertising, and search mostly requires common keys for nodes and their contents.
They generate a substantial overhead in highly dynamic settings, where nodes join/leave the system. 248 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) agents advertise their information provision abilities given the interests of their neighbours. Given the success of this method, we shall study how the addition of logical paths and gradual evolution of the network topology can further increase the effectiveness of the proposed method.
Let { } be the set of agents in the system. The network of agents is modelled as a graph =( , ), where is the set of agents and is a set of bidirectional edges denoted as nonordered pairs ( , ). The neighbourhood of an agent includes all the one-hop away agents (i.e. its acquaintance agents) such that ( ) The set of acquaintances of is denoted by Each agent maintains (a) an ontology that represents categories of information, (b) indices of information pieces available to its local database and to other agents, and (c) a profile model for some of its acquaintances. Indices and profile models are described in detail in section 4.
Ontology concepts represent categories that classify the information pieces available. It is assumed that agents in the network share the same ontology, but each agent has a set of information items in its local repository, which are classified under the concepts of its expertise. The set of concepts is denoted by It is assumed that the sets of items in agents" local repositories are non-overlapping.
Finally, it is assumed that there is a set of queries .
Each query is represented by a tuple where is the unique identity of the query is a non-negative integer representing the maximum number of information pieces requested, is the specific category to which these pieces must belong, is a path in the network of agents through which the query has been propagated (initially it contains the originator of the query and each agent appends its id in the before propagating the query), and is a positive integer that specifies the maximum number of hops that the query can reach. In case this limit is exceeded and the corresponding number of information pieces have not been found, then the query is considered unfulfilled However, even in this case, a (possibly high) percentage of the requested pieces of information may have been found.
The problem that this article deals with is as follows: Given a network of agents and a set of queries , agents must retrieve the pieces of information requested by queries, in concurrent search sessions, and further ‘tune" the network so as to answer future similar queries in the more effective and efficient way, increasing the benefit of the system and reducing the communication messages required. The of the system is the ratio of information pieces retrieved to the number of information pieces requested. The of the system is measured by the number of messages needed for searching and updating the indexes and profiles maintained. ‘Tuning" the network requires agents to acquire the necessary information about acquaintances" interests and information provision abilities (i.e. the routing and profiling tuples detailed in section 4), so as to route queries and further share information in the most efficient way. This must be done seamlessly to searching: I.e. agents in the network must share/acquire the necessary information while searching, increasing the benefit and efficiency gradually, as more queries are posed.
SHARING
Given a network =( , ) of agents and a set of queries , each agent maintains indices for routing queries to the right agents, as well as acquaintances" profiles for advertising its information provision abilities to those interested.
To capture information about pieces of information accessible by the agents, each agent maintains a routing index that is realized as a set of tuples of the form < , , >. Each such tuple specifies the number of information items in category that can be reached by , such that ( ) { }. This specifies the of to with respect to the information category . As it can be noticed, each tuple corresponds either to the agent itself (specifying the pieces of information classified in available to its local repository) or to an acquaintance of the agent (recording the pieces of information in category available to the acquaintance agent and to agents that can be reached through this acquaintance). The routing index is exploited for the propagation of queries to the right agents: Those that are either more likely to provide answers or that know someone that can provide the requested pieces of information.
Considering an agent , the profile model of some of its acquaintances , denoted by is a set of tuples < , >, maintained by . Such a tuple specifies the probability that the acquaintance is interested to pieces of information in category subsequently, such a probability is also denoted by ).
Formally, the profile model of an acquaintance of is { , >| ( ) and }. Profile models are exploited by the agents to decide where to ‘advertise" their information provision abilities.
Given two acquaintances and in , the information searching and sharing process proceeds as it is depicted in Figure 1: Initially, each agent has no knowledge about the information provision abilities of its acquaintances and also, it possesses no information about their interests. When that the query is sent to from the agent , then has to update the profile of concerning the category increasing the probability that is interested to information in When this probability is greater than a threshold value (due to the queries about that has sent to ), then assesses that it is highly Figure 1. Typical pattern for information sharing between two acquaintances (numbers show the sequence of tasks) Aj Ai The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 249 probable for to be interested about information in category .
This leads to inform about its information provision abilities as far as the category is concerned. This information is being used by to update its index about . This index is being exploited by to further propagate queries, and it is further propagated to those interested in . Moreover, the profile of maintained by guides to propagate changes concerning its information provision abilities to .
The above method has the following features: (a) It combines routing indices and token-based information sharing techniques for efficient information searching and sharing, without imposing an overlay network structure. (b) It can be used by agents to adapt safely and effectively to dynamic networks. (c) It supports the acquisition and exploitation of different types of locally available information for the ‘tuning" process. (d) It extends the tokenbased method for information sharing (as it was originally proposed in [12,13]) in two respects: First, to deal with categories of information represented by means of ontology concepts and not with specific pieces of information, and second, to guide agents to advertise information that is semantically similar to the information requested, by using a semantic similarity measure between information categories. Therefore, it paves the way for the use of token-based methods for semantic peer-to-peer systems. This is further described in section 4.3. (d) It provides a more sophisticated way for agents to update routing indices than that originally proposed in [2]. This is done by gathering and exploiting acquaintances" profiles for effective information sharing, avoiding unnecessary and cyclic updates that may result to misleading information about agents" information provision abilities. This is further described in the next sub-section.
As already specified, given a network of agents and the set of agent"s acquaintances, the routing index (RI) of (denoted by ) is a collection of at most | | indexing tuples < , >. The key idea is that given such an index and a request concerning , will forward this request to if the resources available (i.e. the information abilities of to ) can best serve this request. To compute the information abilities of to , all tuples < , > concerning all agents in ( )-{ } must be aggregated. Crespo and Garcia-Molina [2] examine various types of aggregations. In this paper, given some tuples < >,< , …> maintained by the agent , their aggregation is the tuple < , >. This gives information concerning the pieces of information that can be provided through , but it does not distinguish what each of "s acquaintances can provide: This is an inherent feature of routing indices. Without considering the interests of its acquaintances, may compute aggregations concerning agents in ( ) { }-{ } and advertise/share its information provision abilities to each agent in ( ).
For instance, given the network configuration depicted in Figure 2 and a category , agent sends the aggregation of the tuples concerning agents in ( ) { }-{ } (denoted as ( , )) to agent , which records the tuple < >. Similarly the aggregation of the tuples concerning the agents in ( ) { }-{ } (denoted as ( )) is sent to the agent , which also records the tuple < >. It must be noticed that and record the information provision abilities of each from its own point of view. Every time the tuple that models the information provision abilities of an agent changes, the aggregation has to re-compute and send the new aggregation to the appropriate neighbors in the way described above. Then, its neighbors have to propagate these updates to their acquaintances, and so on.
Figure 2.Aggregating and sharing information provision indices.
Routing indices may be misleading and lead to inefficiency in arbitrary graphs containing cycles. The exploitation of acquaintances" profiles can provide solutions to these deficiencies. Each agent propagates its information provision abilities concerning a category only to these acquaintances that have high interest in this category. As it has been mentioned, an agent expresses its interest in a category by propagating queries about it. Therefore, indices concerning a category are propagated in the inverse direction in the paths to which queries about are propagated. Indices are propagated as long as agents in the path have a high interest in . Queries can not be propagated in a cyclic fashion since an agent serves and propagates queries that have not been served by it in a previous time point. Therefore, due to their relation to queries, indices are not propagated in a cyclic fashion, as well. However, there is still a specific case where cycles can not be avoided. Such a case is shown in Figure 3: Figure 3. Cyclic pattern for the sharing of indices.
While the propagation of the query causes the propagation of information provision abilities of agents in a non cyclic way (since the agent A recognizes that has been served), the query causes the propagation of information abilities of A to other agents in the network, causing, in conjunction to the propagation of indices due to a cyclic update of indices.
The key assumption behind the exploitation of acquaintances" profiles, as it was originally proposed in [12,13], is that for an agent to pass a specific information item, this agent has a high interest on it or to related information. As already said, in our case, acquaintances" profiles are created based on received queries and specify the interests of acquaintances to specific information categories. Given the query sent from to , has to record not only the interest of to , but Ak A2 A Notation Acquaintance relation Flow of query Flow of indices due to Flow of query Flow of indices due to 250 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the interest of to all the related classes, given their semantic similarity to To measure the similarity between two ontology classes we use the similarity function [0,1] [7]: = otherwise cc
1 cofsubconceptaiscif ji ji where is the length of the shortest path between and in the graph spanned by the sub concept relation and the minimal level in the hierarchy of either or . and are parameters scaling the contribution of shortest path length and , respectively.
Based on previous works we choose =0.2 and =0.6 as optimal values. It must be noticed that we measure similarity between sub-concepts, assigning a very low similarity value between concepts that are not related by the sub-concept relation. This is due to that, each query about information in category can be answered by information in any sub-category of close enough to Given a threshold value 0.3, 0.3 indicates that an agent interested in is also interested in , while <0.3 indicates that an agent interested in is unlikely to be interested in . This threshold value was chosen after some empirical experiments with ontologies.
The update of "s assessment on pc based on an incoming query from is computed by leveraging Bayes Rule as follows [12,13]: , and ( ) If is the last in the || 1 || 2 ),( If is not the last in the Then probabilities must be normalized to ensure that , 1 )( ,
According to the first case of the equation, the probability that the agent that has propagated a query about to be interested about information in , is updated based on the similarity between and . The second case updates the interests of agents other than the requesting one, in a way that ensures that normalization works. It must be noticed that in contrast to [12,13], the computation has been changed in favour to the agent that passed the query.
The profiles of acquaintances enable an agent to decide where and which advertisements to be sent. Specifically, for each and for which is greater than a threshold value (currently set to 0.5), the agent aggregates the vectors ( ) of each agent ( ) { }-{ }and sends the tuple ( , ) to . Also, given a high , when a change to an index concerning occurs (e.g. due to a change in "s local repository, or due to that the set of its acquaintances changed), sends the updated aggregated index entry to . Doing so, the agent which is highly interested to pieces of information in category updates its index so as to become aware of the information provision abilities of as far as the category is concerned.
Tuning is performed seamlessly to searching: As agents propagate queries to be served, their profiles are getting updated by their acquaintances. As their profiles are getting updated, agents receive the aggregated indices of their acquaintances, becoming aware of their information provision abilities on information categories to which they are probably interested. Given these indices, agents further propagate queries to acquaintances that are more likely to serve queries, and so on. Concerning the routing index and the profiles maintained by an agent , it must be pointed that does not need to record all possible tuples, i.e. | | | { }|: It records only those that are of particular interest for searching and sharing information, depending on the expertise and interests of its own and its acquaintances.
Initially, agents do not possess profiles of their acquaintances. For indices there are two alternatives: Either agents do not initially possess any information about acquaintances" local repositories (this is the   case), or they do (this is the   case). Given a query, agents propagate this query to those acquaintances that have the highest information provision abilities. In the no initialization of indices case where an agent does not initially possess information about its acquaintances" abilities, it may initially propagate a query to all of them, resulting to a pure flooding approach; or it may propagate the query randomly to a percentage of them. In the initialization of indices case, where an agent initially possesses information about its acquaintances" local repository, it can propagate queries to all or to a percentage of those that can best serve the request. We considered both cases in our experiments.
Given a static setting where agents do not shift their expertise, and the distribution of information pieces does not change, the network will eventually reach a state where no information concerning agents" information abilities will need to be propagated and no agents" profiles will need to be updated: Queries shall be propagated only to those agents that will lead to a near-to-the-maximum benefit of the system in a very efficient way. In a dynamic setting, agents may shift their expertise, their interests, they may leave the network at will, or welcome new agents that join the network and bring new information provision abilities, new interests and new types of queries. In this paper we study settings where agents may leave or join the network. This requires agents to adapt safely and effectively. Towards this goal, in case an agent does not receive a reply from one of its acquaintances within a time interval, then it retracts all the indices and the profile concerning the missing acquaintance and repropagates the queries that have been sent to the missing agent since the last successful handshake, to other agents. In case a new agent joins the network, then its acquaintances that are getting aware of its presence propagate all the queries that have processed by them in the last time points (currently is set to 6) to the newcomer. This is done so as to inform the newcomer about their interests and initiate information sharing.
To validate the proposed approach we have built a prototype that simulates large networks. To test the scalability of our approach The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 251 we have run several experiments with various types of networks.
Here we present results from 3 network types with | |=100, | |=500 and | |=1000 that provide representative cases. Networks are constructed by distributing randomly | | agents in an area, each with a visibility ratio equal to The acquaintances of an agent are those that are visible to the agent and those from which the agent is visible (since edges in the network are bidirectional). Details about networks are given in Table 1. The column avg(|N(A)|) shows the average number of acquaintances per agent in the network and the column |T| shows the number of queries per network type. It must be noticed that the TypeA network is more dense than the others, which are much larger than this.
Each experiment ran 40 times. In each run the network is provided with a new set of randomly generated queries that are originated from randomly chosen agents. The agents search and gather knowledge that they further use and enrich, tuning the network gradually, run by run. Each run lasts a number of rounds that depends on the of queries and on the parameters that determine the dynamics of the network: To end a run, all queries must have either been served (i.e. 100% of the information items requested must have been found), or they must have been unfulfilled (i.e. have exceeded their ). It must be noticed that in case of a dynamic setting, this ending criterion causes some of the queries to be lost. This is the case when some queries are the only active remained and the agents to whom they have been propagated left the network without their acquaintances to be aware of it.
Table1: Network types |N| R N avg(|N(A)|) |T| TypeA 100 10 25 50 363 TypeB 500 10 125 20 1690 TypeC 1000 10 250 10 3330 Information used in the experiments is synthetic and is being classified in 15 distinct categories: Each agent"s expertise comprises a unique information category. For the category in its expertise each agent holds at most 1000 information pieces, the exact number of which is determined randomly.
At each run a constant number of queries are being generated, depending on the type of network used (last column in Table 1).
At each run, each query is randomly assigned to an originator agent and is set to request a random number of information items, classified in a sub-category of the query-originator agent"s expertise. This sub-category is chosen in a random way and the requested items are less than 6000. The for any query is set to be equal to 6. In such a setting, the demand for information items is much higher than the agents" information provision abilities, given the of queries: The maximum benefit in any experimental case is much less than 60% (this has been done so as to challenge the ‘tuning" task in settings where queries can not be served in the first hop or after 2-3 hops).
Given that agents are initially not aware of acquaintances" local repository (  case), we have run several evaluation experiments for each network type depending on the percentage of acquaintances to which a query can be propagated by an agent. These types of experiments are denoted by TypeX-Y, where X denotes the type of network and Y the percentage of acquaintances: Here we present results for Y equal to 10, 20 or 50. For instance, TypeA-10 denotes a setting with a 0 50 100 150 200 250 300 TypeA-10 Type B-20 (no initialization) TypeB-20 (initialization) Type C-50 4000 14000 24000 34000 44000 54000 40 42 44 46 48 50 52 54 56 58 0
TypeA-10 TypeB-20 (no initialization) Type B-20 (initialization) TypeC-50 TypeB-20 without RIs Figure 4. Results for static networks as agents gather information about acquaintances" abilities and interests network of TypeA where each query is being propagated to at most 10% of an agent"s acquaintances. The exact number of acquaintances is randomly chosen per agent and queries are being propagated only to those acquaintances that are likely to best i-messages per run q-messages per run benefit per run message gain per run 252 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) serve the request. Figures 4 and 5 show experiments for static and dynamic networks of TypeA-10 (dense network with a low percentage of acquaintances), TypeB-20 (quite dense network with a low percentage of acquaintances), with initialization and without initialization, and TypeC-50 (not a so dense network with a quite high percentage of acquaintances). To demonstrate the advantages of our method we have considered networks without 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 10000 20000 30000 40000 50000 60000 70000 80000 42 43 44 45 46 47 48 49 50 0
TypeB-20 TypeB-20 without RIs TypeC-50 TypeC-50 without RIs TypeC-50 (static) Figure 5. Results for dynamic networks as agents gather information about acquaintances" abilities and interests routing indices for TypeC-50 and TypeB-20 networks: Agents in these networks, similarly to [12,13], share information concerning their local repository based on their assessments on acquaintances" interests.
Results computed in each experiment show the number of querypropagation messages ( ), the number of messages for the update of indices ( ), the of the system, i.e. the average ratio of information pieces provided to the number of pieces requested per query, and the i.e. the ratio of benefit to the total number of messages. The horizontal axis in each diagram corresponds to the runs.
As it is shown in Figure 4, as agents search and share information from run 1 to run 40, they manage to increase the benefit of the system, by drastically reducing the number of messages. Also (not shown here due to space reasons) the number of unfulfilled queries decrease, while the served queries increase gradually.
Experiments show: (a) An effective tuning of the networks as time passes and more queries are posed to the network, even if agents maintain the models of a small percentage of their acquaintances. (b) That ‘tuning" can greatly facilitate the scalability of the information searching and sharing tasks in networks.
To show whether initial knowledge about acquaintances local repository (the   case) affects the effective tuning of the network, we provide representative results from the TypeB-20 network. As it is shown in Figure 4, the tuning task in this case does not manage to achieve the benefit of the system reported for the   case. On the contrary, while the tuning affects the drastically; the are not affected in the same way: The in the  case are less than those in the TypeB-20 with   case. This is further shown in a more clear way in the message gain of both approaches: The message gain of the TypeB-20 with case is higher than the message gain for the TypeB-20 experiment with  .
Therefore, initial knowledge concerning local information of acquaintances can be used for guiding searching and tuning at the initial stages of the tuning task, only if we need to gain efficiency (i.e. decrease the number of required messages) to the cost of loosing effectiveness (i.e. have lower benefit): This is due to the fact that, as agents posses information about acquaintances" local repositories, the tuning process enables the further exchange of messages concerning agents" information provision abilities in cases where agents" profiles provide evidence for such a need.
However, initial information about acquaintances" local repositories may mislead the searching process, resulting in low benefit. In case we need to gain effectiveness to the cost of reducing efficiency, this type of local knowledge does not suffice.
Considering also the information sharing method without routing indices ( cases), we can see that for static networks it requires more without managing to tune the system, while the benefit is nearly the same to the one reported by our method. This is shown clearly in the message gain diagrams in Figure 4.
Figure 5 provides results for dynamic networks. These are results from a particular representative case of our experiments where more than 25% of (randomly chosen) nodes leave the network in each run during the experiment. After a random number of i-messages per run q-messages per run benefit per run message gain per run The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 253 rounds, a new node may replace the one left. This newcomer has no information about the network. Approximately 25% of the nodes that leave the network are not replaced for 50% of the experiment, and approximately 50% are not replaced for more than 35% of the experiment. In such a highly dynamic setting with very scarce information resources distributed in the network, as Figure 5 shows, the tuning approach has managed to keep the benefit to acceptable levels, while still reducing drastically the number of i-messages. However, as it can be expected, this reduction is not so drastic as it was in the corresponding static cases. Figure 5 shows that the message gain for the dynamic case is comparable to the message gain for the corresponding (TypeC50) static case, which proves the value of this approach for dynamic settings. The comparison to the case where no routing indices are exploited reveals the same results as in the static case, to the cost of a large number of messages.
Finally it must be pointed that the maximum number of messages per query required by the proposed method is nearly 12, which is less than that that reported by other efforts.
This paper presents a method for semantic query processing in large networks of agents that combines routing indices with information sharing methods. The presented method enables agents to keep records of acquaintances" interests, to advertise their information provision abilities to those that have a high interest on them, and to maintain indices for routing queries to those agents that have the requested information provision abilities. Specifically, the paper demonstrates through extensive performance experiments: (a) How networks of agents can be ‘tuned" so as to provide requested information effectively, increasing the benefit and the efficiency of the system. (b) How different types of local knowledge (number, local information repositories, percentage, interests and information provision abilities of acquaintances) can guide agents to effectively answer queries, balancing between efficiency and efficacy. (c) That the proposed tuning task manages to increase the efficiency of information searching and sharing in highly dynamic and large networks. (d) That the information gathered and maintained by agents supports efficient and effective information searching and sharing: Initial information about acquaintances information provision abilities is not necessary and a small percentage of acquaintances suffices.
Further work concerns experimenting with real data and ontologies, differences in ontologies between agents, shifts in expertise and the parallel construction of overlay structure.

Electronic displays are increasingly being used within public environments, such as airports, city centres and retail stores, in order to advertise commercial products, or to entertain and inform passersby. Recently, researchers have begun to investigate how the content of such displays may be varied dynamically over time in order to increase its variety, relevance and exposure [9]. Particular research attention has focused on the need to take into account the dynamic nature of the display"s audience, and to this end, a number of interactive public displays have been proposed. These displays have typically addressed the needs of a closed set of known users with pre-defined interests and requirements, and have facilitated communication with these users through the active use of handheld devices such as PDAs or phones [3, 7]. As such, these systems assume prior knowledge about the target audience, and require either that a single user has exclusive access to the display, or that users carry specific tracking devices so that their presence can be identified [6, 11]. However, these approaches fail to work in public spaces, where no prior knowledge regarding the users who may view the display exists, and where such displays need to react to the presence of several users simultaneously.
By contrast, Payne et al. have developed an intelligent public display system, named BluScreen, that detects and tracks users through the Bluetooth enabled devices that they carry with them everyday [8]. Within this system, a decentralised multi-agent auction mechanism is used to efficiently allocate advertising time on each public display. Each advert is represented by an individual advertising agent that maintains a history of users who have already been exposed to the advert. This agent then seeks to acquire advertising cycles (during which it can display its advert on the public displays) by submitting bids to a marketplace agent who implements a sealed bid auction. The value of these bids is based upon the number of users who are currently present in front of the screen, the history of these users, and an externally derived estimate of the value of exposing an advert to a user.
In this paper, we present an advanced bidding agent that significantly extends the sophistication of this approach. In particular, we consider the more general setting in which it is impossible to determine an a priori valuation for exposing an advert to a user.
This is likely to be the case for BluScreen installations within private organisations where the items being advertised are forthcoming events or news items of interest to employees and visitors, and thus have no direct monetary value (indeed in this case bidding is likely to be conducted in some virtual currency). In addition, it is also likely to be the case within new commercial installations where limited market experience makes estimating a valuation impossible. In both cases, it is more appropriate to assume that an advertising agents will be assigned a total advertising budget, and that it will have a limited period of time in which to spend this budget (particularly so where the adverts are for forthcoming events).
The advertising agent is then simply tasked with using this budget to maximum effect (i.e. to achieve the maximum possible advert exposure within this time period).
Now, in order to achieve this goal, the advertising agent must be capable of modelling the behaviour of the users in order to predict the number who will be present in any future advertising cycle. In addition, it must also understand the auction environment in which 263 978-81-904262-7-5 (RPS) c 2007 IFAAMAS it competes, in order that it may make best use of its limited budget.
Thus, in developing an advanced bidding agent that achieves this, we advance the state of the art in four key ways:
departure of users as independent Poisson processes, and to make maximum likelihood estimates of the rates of these processes based on their observations. We show how these agents can then calculate the expected number of users who will be present during any future advertising cycle.
advertising agents to model the probability of winning any given auction when a specific amount is bid. The cumulative form of the gamma distribution is used to represent this probability, and its parameters are fitted using observations of both the closing price of previous auctions, and the bids that that advertising agent itself submits.
agent derives no additional benefit by showing an advert to a single user more than once, causes the expected utility of each future advertising cycle to be dependent on the expected outcome of all the auctions that precede it. We thus present a stochastic optimisation algorithm based upon simulated annealing that enables the advertising agent to calculate the optimal sequence of bids that maximises its expected utility.
outperforms a simple strategy with none of these features (within an heterogenous population the advertising agents who use the advanced bidding strategy are able to expose their adverts to 25% more users than those using the simple bidding strategy), and we show that it performs within 7.5% of that of a centralised optimiser with perfect knowledge of the number of users who will arrival and depart in all future advertising cycles.
The remainder of this paper is organised as follows: Section 2 discusses related work where agents and auction-based marketplaces are used to allocated advertising space. Section 3 describes the prototype BluScreen system that motivates our work. In section 4 we present a detailed description of the auction allocation mechanism, and in section 5 we describe our advanced bidding strategy for the advertising agents. In section 6 we present an empirical validation of our approach, and finally, we conclude in section 7.
The commercial attractiveness of targeted advertising has been amply demonstrated on the internet, where recommendation systems and contextual banner adverts are the norm [1]. These systems typically select content based upon prior knowledge of the individual viewing the material, and such systems work well on personal devices where the owner"s preferences and interests can be gathered and cached locally, or within interactive environments which utilise some form of credential to identify the user (e.g. e-commerce sites such as Amazon.com).
Attempts to apply these approaches within the real world have been much more limited. Gerding et al. present a simulated system (CASy) whereby a Vickrey auction mechanism is used to sell advertising space within a modelled electronic shopping mall [2]. The auction is used to rank a set of possible advertisements provided by different retail outlets, and the top ranking advertisements are selected for presentation on public displays. Feedback is provided through subsequent sales information, allowing the model to build up a profile of a user"s preferences. However, unlike the BluScreen Figure 1: A deployed BluScreen prototype. system that we consider here, it is not suitable for advertising to many individuals simultaneously, as it requires explicit interaction with a single user to acquire the user"s preferences.
By contrast, McCarthy et al. have presented a prototype implementation of a system (GroupCast) that attempts to respond to a group of individuals by assuming a priori profiles of several members of the audience [7]. User identification is based on infrared badges and embedded sensors within an office environment. When several users pass by the display, a centralised system compares the user"s profiles to identify common areas of interest, and content that matches this common interest is shown.
Thus, whilst CASy is a simulated system that allows advertisers to compete for the attention of single user, GroupCast is a prototype system that detects the presence of groups of users and selects content to match their profiles. Despite their similarities, neither system addresses the settings that interests us here: how to allocate advertising space between competing advertisers who face an audience of multiple individuals about whom there is no a priori profile information. Thus, in the next section we describe the prototype BluScreen system that motivates our work.
BluScreen is based on the notion of a scalable, extendable, advertising framework whereby adverts can be efficiently displayed to as many relevant users as possible, within a knowledge-poor environment. To achieve these goals, several requirements have been identified:
possible, whilst minimising the number of times the advert is presented to any single user.
devices, so that future deployments within public arenas will not require uptake of new hardware.
appear on different displays at different times.
audience should be exploited to facilitate inference of user interests which can be exploited by the system.
To date, a prototype systems that addresses the first two goals has been demonstrated [8]. This system uses a 23 inch flat-screen display deployed within an office environment to advertise events and news items. Rather than requiring the deployment of specialised hardware, such as active badges (see [11] for details), BluScreen detects the presence of users in the vicinity of each display through the Bluetooth-enabled devices that they carry with them everyday1 . 1 Devices must be in discovery mode to detectable. 264 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Device Type Unique Samples Devices Occasional < 10 135 Frequent 10 − 1000 70 Persistent > 1000 6 Table 1: Number of Bluetooth devices observed at different frequencies over a six month sample period.
This approach is attractive since the Bluetooth wireless protocol is characterised by its relative maturity, market penetration, and emphasis on short-range communication. Table 1 summarises the number of devices detected by this prototype installation over a six month period. Of the 212 Bluetooth devices detected, approximately 70 were detected regularly, showing that Bluetooth is a suitable proxy for detecting individuals in front of the screen.
In order to achieve a scalable and extendable solution a multiagent systems design philosophy is adopted whereby a number of different agents types interact (see figure 2). The interactions of these agents are implemented through a web services protocol2 , and they constitute a decentralised marketplace that allocates advertising space in an efficient and timely manner. In more detail, the responsibilities of each agent types are: Bluetooth Device Detection Agent: This agent monitors the environment in the vicinity of a BluScreen display and determines the number and identity of any Bluetooth devices that are close by. It keeps historical records of the arrival and departure of Bluetooth devices, and makes this information available to advertising agents as requested.
Marketplace Agent: This agent facilitates the sale of advertising space to the advertising agents. A single marketplace agent represents each BluScreen display, and access to this screen is divided into discrete advertising cycles of fixed duration.
Before the start of each advertising cycle, the marketplace agent holds a sealed-bid auction (see section 4 for more details). The winner of this auction is allocated access to the display during the next cycle.
Advertising Agent: This agent represents a single advert and is responsible for submitting bids to the marketplace agent in order that it may be allocated advertising cycles, and thus, display its advert to users. It interacts with the device detection agent in order to collect information regarding the number and identity of users who are currently in front of the display. On the basis of this information, its past experiences, and its bidding strategy, it calculates the value of the bid that it should submit to the marketplace agent.
Thus, having described the prototype BluScreen system, we next go on to describe the details of the auction mechanism that we consider in this work, and then the advanced bidding agent that operates bids within this auction.
As described above, BluScreen is designed to efficiency allocate advertising cycles in a distributed and timely manner. Thus, oneshot sealed bid auctions are used for the market mechanism of the marketplace agent. In previous work, each advertising agent was assumed to have an externally derived estimate of the value of exposing an advert to a user. Under this assumption, a secondprice sealed-bid auction was shown to be effective, since advertis2 This is implemented on a distributed Mac OS X based system using the Bonjour networking protocol for service discovery.
Advert Advert Marketplace Agent Device ID Advert Advertising Agent Device ID Device ID Advertising Agent Advertising Agent BluetoothDevice DetectionAgent 2) Bids based on predicted future device presence 1) Device presence detected 3) Winning Agent displays advert on the screen Device ID Figure 2: The BluScreen agent architecture for a single display. ing agents have a simple strategy of truthfully bidding their valuation in each auction [8].
However, as described earlier, in this paper we consider the more general setting in which it is impossible to determine an a priori valuation for exposing an advert to a single user. This may be because the BluScreen installation is within a private organisation where what is being advertised (e.g. news items or forthcoming events) has no monetary value, or it may be a new commercial installation where limited market experience makes estimating such a valuation impossible. In the absence of such a valuation, the attractive economic properties of the second-price auction can not be achieved in practise, and thus, in our work there is no need to limit our attention to the second-price auction. Indeed, since these auctions are actually extremely rare within real world settings [10], in this work we consider the more widely adopted first-price auction since this increases the applicability of our results.
Thus, in more detail, we consider an instance of a BluScreen installation with a single display screen that is managed by a single marketplace agent3 . We consider that access to the display screen is divided into discrete advertising cycles, each of length tc, and a first-price sealed bid auction is held immediately prior to the start of each advertising cycle. The marketplace agent announces the start and deadline of the auction, and collects sealed bids from each advertising agent. At the closing time of the auction the marketplace agent announces to all participants and observers the amount of the winning bid, and informs the winning advertising agent that it was successful (the identity of the winning advertising agent is not announced to all observers). In the case that no bids are placed within any auction, a default advert is displayed.
Having described the market mechanism that the marketplace agent implements, we now go on to describe and evaluate an advanced bidding strategy for the advertising agents to adopt.
As described above, we consider the case that the advertising agents do not have an externally derived estimate of the value of exposing the advert to a single user. Rather, they have a constrained budget,
B, and a limited period of interest during which they wish to display their advert. Their goal is then to find the appropriate amount to bid within each auction in this period, in order to maximise the exposure of their advert.
In attempting to achieve this goal the advertising agent is faced with a high level of uncertainty about future events. It will be uncertain of the number of users who will be present during any advertising cycle since even if the number of users currently present 3 This assumption of having a single BluScreen instance is made to simplify our task of validating the correctness and the efficiency of the proposed mechanism and strategy, and generalising these results to the case of multiple screens is the aim of our future work.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 265 is known, some may leave before the advert commences, and others may arrive. Moreover, the amount that must be bid to ensure that an auction is won is uncertain since it depends on the number and behaviour of the competing advertising agents.
Thus, we enable the agent to use its observations of the arrival and departure of users to build a probabilistic model, based upon independent Poisson processes, that describes the number of users who are likely to be exposed to any advert. In addition, we enable the agent to observe the outcome of previous advertising cycle auctions, and use the observations of the closing price, and the success or otherwise of the bids that it itself submitted, to build a probabilistic model of the bid required to win the auction. The agent then uses these two models to calculate its expected utility in each advertising cycle, and in turn, determine the optimal sequence of bids that maximises this utility given its constrained budget. Having calculated this sequence of bids, then the first bid in the sequence is actually used in the auction for the next advertising cycle. However, at the close of this cycle, the process is repeated with a new optimal sequence of bids being calculated in order take to account of what actually happened in the preceding auction (i.e. whether the bid was successful or not, and how many users arrived or departed).
Thus, in the next three subsections we describe these two probabilistic models, and their application within the bidding strategy of the advertising agent.
In order to predict the number of users that will be present in any future advertising cycle, it is necessary to propose a probabilistic model for the behaviour of the users. Thus, our advanced bidding strategy assumes that their arrival and departures are determined by two independent Poisson processes4 with arrival rate, λa, and departure rate, λd. This represents a simple model that is commonly applied within queuing theory5 [5], yet is one that we believe well describes the case where BluScreen displays are placed in communal areas where people meet and congregate. Given the history of users" arrivals and departures obtained from the device detection agent, the advertising agent makes a maximum likelihood estimation of the values of λa and λd.
In more detail, if the advertising agent has observed n users arriving within a time period t, then the maximum likelihood estimation for the arrival rate λa is simply given by: λa = n t (1) Likewise, if an agent observes n users each with a duration of stay of t1, t2, . . . , tn time periods, then the maximum likelihood estimation for the departure rate λd is given by: 1 λd = 1 n n i=1 ti (2) 4 Given a Poisson distribution with rate parameter λ, the number of events, n, within an interval of time t is given by: P(n) = e−λt (λt)n n! In addition, the probability of having to wait a period of time, t, before the next event is determined by: P(t) = λeλt 5 Note however that in queuing theory it is typically the arrival rate and service times of customers that are modelled as Poisson processes. Our users are not actually modelled as a queue since the duration of their stay is independent of that of the other users.  0 t t + tc τ (i) n users ? (iii) λatc users ? (ii) λat users ?
Figure 3: Example showing how to predict the number of users who see an advert shown in an advertising cycle of length tc, commencing at time t in the future.
In environments where these rates are subject to change, the agent can use a limited time window over which observations are used to estimate these rates. Alternatively, in situations where cyclic changes in these rates are likely to occur (i.e. changing arrival and departure rates at different times of the day, as may be seen in areas where commuters pass through), the agent can estimate separate values over each hour long period.
Having estimated the arrival and departure rate of users, and knowing the number of users who are present at the current time, the advertising agent is then able to predict the number of users who are likely to be present in any future advertising cycle6 . Thus, we consider the problem of predicting this number for an advertising cycle of duration tc that starts at a time t in the future, given that n users are currently present (see figure 3). This number will be composed of three factors: (i) the fraction of the n users that are initially present who do not leave in the interval, 0 ≤ τ < t, before the advertising cycle commences, (ii) users that actually arrive in the interval, 0 ≤ τ < t, and are still present when the advertising cycle actually commences, and finally, (iii) users that arrive during the course of the advertising cycle, t ≤ τ < t + tc.
Now, considering case (i) above, the probability of one of the n users still being present when the advertising cycle starts is given by ∞ t λde−λdτ dτ = e−λdt . Thus we expect ne−λdt of these users to be present. In case (ii), we expect λat new users to arrive before the advertising cycle commences, and the probability that any of these will still be there when it actually does so is given by 1 t t 0 e−λd(t−τ) dτ = 1 λdt 1 − e−λdt . Thus we expect λa λd 1 − e−λdt of these users to be present. Finally, in case (iii) we expect λatc users to arrive during the course of the advertising cycle. Thus, the combination of these three factors gives an expression for the expected number of users who will be present within an advertising cycle of length tc, that commencing at time t in the future, given that there are n users currently present: Nn,t = ne−λdt + λa λd 1 − e−λdt + λatc (3) Note that as t increases the results become less dependent upon the initial number of users, n. The mean number of users present at any time is simply λa/λd, and the mean number of users exposed to an advert in any advertising cycle is given by λa tc + 1 λd .
In addition to estimating the number of users who will be present in any advertising cycle, an effective bidding agent must also be able to predict the probability of it winning an auction given that it submits any specified bid. This is a common problem within bidding agents, and approaches can generally be classified as game theoretic or decision theoretic. Since our advertising agents are unaware of the number or identity of the competing advertising 6 Note that we do not require a user to be present for the entire advertising cycle in order to be counted as present. 266 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) agents, the game theoretic approach is precluded. Thus, we take a decision theoretic approach similar to that adopted within continuous double auctions where bidding agents estimate the market price of goods by observing transaction prices [4].
Thus, our advertising agents uses a parameterised function to describe the probability of winning the auction given any submitted bid, P(b). This function must have support [0, ∞) since bids must be positive. In addition, we expect it to exhibit by an ‘s" shaped curve whereby the probability of winning an auction is small when the submitted bid is very low, the probability is close to one when the bid is very high, and there is a transition point that characterises the change from a losing to a wining bid. To this end, we use the cumulative form of the gamma distribution for this function: P(b) = γ (k, b/θ) Γ (k) (4) where Γ(k) is the standard gamma function, and γ (k, b/θ) is the incomplete gamma function. This function has the necessary properties described above, and has two parameters, k and θ. The transition point where P(b) = 0.5 is given by kθ and the sharpness of the transition is described by kθ2 . In figure 4 we show examples of this function for three different values of k and θ.
The advertising agent chooses the most appropriate values of k and θ by fitting the probability function to observations of previous auctions. An observation is a pair {bi, oi} consisting of the bid, bi, and an auction outcome, oi. Each auction generates at least one pair in which bi is equal to the closing price of the auction, and oi = 1. In addition, another pair is generated for each unsuccessful bid submitted by the advertising agent itself, and in this case oi =
, the agent finds the values of k and θ by evaluating: arg min k,θ N i=1 oi − γ (k, bi/θ) Γ (k) 2 (5) This expression can not be evaluated analytically, but can be simply found using a numerical gradient descent method whereby the values of k and θ are initially estimated using their relationship to the transition point described above. The gradient of this expression is then numerically evaluated at these points, and new estimates of k and θ calculated by making a fixed size move in the direction of maximum gradient. This process is repeated until k and θ have converged to an appropriate degree of accuracy.
The goal of the advertising agent is to gain the maximum exposure for its advert given its constrained budget. We define the utility of any advertising cycle as the expected number of users who will see the advert for the first time during that cycle, and hence, we explicitly assume that no additional utility is derived by showing the advert to any user more than once8 . Thus, we can use the results of the previous two sections to calculate the expected utility of each advertising cycle remaining within the advertising agent"s period of 7 In the case that no unsuccessful bids have been observed, there is no evidence of where the transition point between successful and unsuccessful bids is likely to occur. Thus, in this case, an additional pair with value {α min(b1 . . . bn), 0} is automatically created. Here α ∈ [0, 1] determines how far below the lowest successful bid the advertising agent believes the transition point to be. We have typically used α = 0.5 within our experiments. 8 As noted before, we assume that a user has seen the advert if they are present during any part of the advertising cycle, and we do not differentiate between users who see the entire advert, or users who see a fraction of it. 0 10 20 30 40 0
1 Probability of Winning Auction P(b) Bid (b) k = 5 k = 10 k = 20 Figure 4: Cumulative gamma distribution representing the probability of winning an auction (θ = 1 and k = 5, 10 & 20). interest. In the first advertising cycle this is simply determined by the probability of the advertising agent winning the auction, given that it submits a bid b1, and the number of users who are currently in front of the BluScreen display, but have not seen the advert before, is n. Thus, the expected utility of this advertising cycle is simply described by: u1 = P(b1)Nn,0 (6) Now, in the second advertising cycle, the expected utility will clearly depend on the outcome of the auction for the first. If the first auction was indeed won by the agent, then there will be no users who have yet to see the advert present at the start of the second advertising cycle. Thus, in this case, the expected number of new users who will see the advert in the second advertising cycle is described by N0,0 (i.e. only newly arriving users will contribute any utility).
By contrast, if the first auction was not won by the agent, then the expected number of users who have yet to see the advert is given by Nn,tc where tc is the length of the preceding adverting cycle (i.e. exactly the case described in section 5.1 where there are n users initially present and the advertising cycle starts at a time tc in the future). Thus, the expected utility of the second advertising cycle is given by: u2 = P(b2) [P(b1)N0,0 + (1 − P(b1))Nn,tc ] (7) We can generalise this result by noting that the number of users expected to be present within any future advertising cycle will depend on the number of cycles since an auction was last won (since at this point the number of users who are present but have not seen the advert must be equal to zero). Thus, we must sum over all possible ways in which this can occur, and weight each by its probability.
Hence, the general case for any advertising cycle is described by the rather complex expression: ui = P(bi) i−1 j=1 N0,(i−j−1)tc P(bj) i−1 m=j+1 (1 − P(bm)) + Nn,(i−1)tc i−1 m=1 (1 − P(bm)) (8) Thus, given this expression, the goal of the advertising agent is to calculate the sequence of bids over the c remaining auctions, such that the total expected utility is maximised, whilst ensuring that the remaining budget, B, is not exceeded: arg max b1...bc c i=1 ui such that c i=1 bi = B (9) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 267 0 0.25 0.5 0.75 1 0 1 2 3 4 5 6 Expected Utility (U) b 1 / B B = 5 B = 10 B = 20 B = 30 B = 40 Figure 5: Total expected utility of the advertising agent over a continuous range of values of b1 for a number of discrete values of budget, B, when there are just two auction cycles.
Having calculated this sequence, a bid of b1 is submitted in the next auction. Once the outcome of this auction is known, the process repeats with a new optimal sequence of bids being calculated for the remaining advertising cycles of the agent"s period of interest.
Solving for the optimal sequence of bids expressed in equation 9 can not be performed analytically. Instead we develop a numerical routine to perform this maximisation. However, it is informative to initially consider the simple case of just two auctions.
In this case the expected utility of the advertising agent is simply given by u1 + u2 (as described in equations 6 and 7), and the bidding sequence is solely dependent on b1 (since b2 = B−b1). Thus, we can plot the total expected utility against b1 and graphically determine the optimal value of b1 (and thus also b2).
To this end, figure 5 shows an example calculated using parameter values λa = 1/120, λd = 1/480 and tc = 120. In this case, we assume that k = 10 and θ = 1, and thus, given that kθ describes the midpoint of the cumulative gamma distribution, a bid of 10 represents a 50% chance of winning any auction (i.e. P(10) = 0.5).
In addition, we assume that n = λa/λd = 4, and thus the initial number of users present is equal to the mean number that we expect to find present at any time. The plot indicates that when the budget is small, then the maximum utility is achieved at the extreme values of b1. This corresponds to bidding in just one of the two auctions (i.e. b1 = 0 and b2 = B or b1 = B and b2 = 0). However, as the budget increases, the plot passes through a transition whereby the maximum utility occurs at the midpoint of the x-axis, corresponding to bidding equally in both auctions (i.e. b1 = b2 = B/2).
This is simply understood by the fact that continuing to allocate the budget to a single auction results in diminishing returns as the probability of actually winning this auction approaches one.
In this case, the plot is completely symmetrical since the number of users present at the start is equal to its expected value (i.e. n = λa/λd). If however, n < λa/λd the plot is skewed such that when the budget is small, it should be allocated to the second auction (since more users are expected to arrive before this advertising cycle commences). Conversely, when n > λa/λd the entire budget should be allocated to the first auction (since the users who are currently present are likely to depart in the near future). However, in both cases, a transition occurs whereby given sufficient budget it is preferable to allocate the budget evenly between both auctions9 . 9 In fact, one auction is still slightly preferred, but the difference in temp ← 1 rate ← 0.995 bold ← initial random allocation Uold ← Evaluate(bold ) WHILE temp > 0.0001 i, j ← random integer index within b t ← random real number between 0 and bi bnew ← bold bnew i ← bold i − t bnew j ← bold j + t Unew ← Evaluate(bnew ) IF rand < exp((Unew − Uold )/temp) THEN bold ← bnew Uold ← Unew ENDIF temp ← temp × rate ENDWHILE Figure 6: Stochastic optimisation algorithm to calculate the optimal sequence of bids in the general case of multiple auctions.
In general, the behaviour seen in the previous example characterises the optimal bidding behaviour of the advertising agent. If there is sufficient budget, bidding equally in all auctions results in the maximum expected utility. However, typically this is not possible and thus utility is maximised by concentrating what budget is available into a subset of the available auction. The choice of this subset is determined by a number of factors. If there are very few users currently present, it is optimal to allocate the budget to later auctions in the expectation that more users will arrive.
Conversely, if there are many users present, a significant proportion of the budget should be allocated to the first auction to ensure that it is indeed won, and these users see the advert. Finally, since no utility is derived by showing the advert to a single user more than once, the budget should be allocated such that there are intervals between showings of the advert, in order that new users may arrive.
Now, due to the complex form of the expression for the expected utility of the agent (shown in equation 8) it is not possible to analytically calculate the optimal sequence of bids. However, the inverse problem (that of calculating the expected utility for any given sequence of bids) is easy. Thus, we can use a stochastic optimisation routine based on simulated annealing to solve the maximisation problem. This algorithm starts by assuming some initial random allocation of bids (normalised such that the total of all the bids is equal to the budget B). It then makes small adjustments to this allocation by randomly transferring the budget from one auction to another. If this transfer results in an increase in expected utility, then it is accepted. If it results in a decrease in expected utility, it might still be accepted, but with a probability that is determined by a temperature parameter. This temperature parameter is annealed such that the probability of accepting such transfers decreases over time. In figure 6 we present this algorithm in pseudo-code.
In order to evaluate the effectiveness of the advanced bidding strategy developed within this paper we compare its performance to three alternative mechanisms. One of these mechanisms represents a simple alternative bidding strategy for the advertising agents, whilst the other two are centralised allocation mechanisms that represent expected utility between this and an even allocation is negliable. 268 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 20 30 40 50 60 0
1 Number Advertising Agents Mean Normalised Exposure Random Allocation Simple Bidding Strategy Advanced Bidding Strategy Optimal Allocation Figure 7: Comparison of four different allocation mechanisms for allocating advertising cycles to advertising agents. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. the upper and lower bounds to the overall performance of the system. In more detail, the four mechanisms that we compare are: Random Allocation: Rather than implementing the auction mechanism, the advertising cycle is randomly allocated to one of the advertising agents.
Simple Bidding Strategy: We implement the full auction mechanism but with a population of advertising agents that employ a simple bidding strategy. These advertising agents do not attempt to model the users or the auction environment in which they bid, but rather, they simply evenly allocate their remaining budget over the remaining advertising cycles.
Advanced Bidding Strategy: We implement the full auction mechanism with a population of advertising agents using the probabilistic models and the bidding strategy described here.
Optimal Allocation: Rather than implementing the auction mechanism, the advertising cycle is allocated to the advertising agent that will derive the maximum utility from it, given perfect knowledge of the number of users who will arrive and depart in all future advertising cycles.
Using these four alternative allocation mechanisms, we ran repeated simulations of two hours of operation of the entire BluScreen environment for a default set of parameters whereby the arrival and departure rate of the users are given by λa = 1/120s and λd = 1/480s, and the length of an advertising cycle is 120s. Each advertising agent is assigned an advert with a period of interest drawn from a Poisson distribution with a mean of 8 advertising cycles, and these agents are initially allocated a budget equal to 10 times their period of interest. For each simulation run, we measure the mean normalised exposure of each advert. That is, the fraction of users who were detected by the BluScreen display during the period of interest of the advertising agent who were actually exposed to the agent"s advert. Thus a mean normalised exposure of 1 indicates that the agent managed to expose its advert to all of the users who were present during its period of interest (and a mean normalised exposure of 0 means that no users were exposed to the advert).
Figure 7 shows the results of this experiments. We first observe the general result that as the number of advertising agents increases, and thus the competition between them increases, then the mean normalised exposure of all allocation mechanisms decreases. We then observe that in all cases, there is no statistically significant improvement in using the simple bidding strategy compared to random allocation (p > 0.25 in Student"s t-test). Since this simple bidding strategy does not take account of the number of users present, and in general, simply increases its bid price in each auction until it does in fact win one, this is not unexpected. However, in all cases the advanced bidding strategy does indeed significantly outperform the simple bidding agent (p < 0.0005 in Student"s t-test), and its performance is within 7.5% of that of the optimal allocation that has perfect knowledge of the number of users who will arrival and depart in all future advertising cycles.
In addition, we present results of experiments performed over a range of parameter values, and also with a mixed population of advertising agents using both the advanced and simple bidding strategies. This is an important scenario since advertisers may wish to supply their own bidding agents, and thus, a homogeneous population is not guaranteed. In each case, keeping all other parameters fixed, we varied one parameter, and these results are shown in figure 8. In general, we see the similar trends as before. Increasing the departure rate causes an decrease in the mean normalised exposure since advertising agents have less opportunities to expose users to their adverts. Increasing the period of interest of each agent decreases the mean normalised exposure, since more advertising agents are now competing for the same users. Finally, increasing the arrival rate of the users causes the results of the simple and advanced bidding strategies to approach one another, since the variance in the number of users who are present during any advertising cycle decreases, and thus, modelling their behaviour provides less gain. However, in all cases, the advanced bidding strategy significantly outperforms the simple one (p < 0.0005 in Student"s t-test). On average, we observe that advertising agents who use the advanced bidding strategy are able to expose their adverts to 25% more users than those using the simple bidding strategy.
Finally, we show that a rational advertising agent, who has a choice of bidding strategy, would always opt to use the advanced bidding strategy over the simple bidding strategy, regardless of the composition of the population that it finds itself in. Figure 9 shows the average normalised exposure of the advertising agents when the population is composed of different fractions of the two bidding strategies. In each case, the advanced bidding strategy shows a significant gain in performance compared to the simple bidding strategy (p < 0.0005 in Student"s t-test), and thus, gains improved exposure over all population compositions.
In this paper, we presented an advanced bidding strategy for use by advertising agents within the BluScreen advertising system. This bidding strategy enabled advertising agents to model and predict the arrival and departure of users, and also to model their success within a first-price sealed bid auction by observing both the bids that they themselves submitted and the winning bid. The exThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 269 1/600 1/480 1/360 0
Departure Rate (λ )d Mean Normalised Exposure Simple Bidding Strategy Advanced Bidding Strategy 6 8 10 0
Mean Period of Interest (Cycles) Mean Normalised Exposure Simple Bidding Strategy Advanced Bidding Strategy 1/240 1/120 1/80 0
Arrival Rate (λ )a Mean Normalised Exposure Simple Bidding Strategy Advanced Bidding Strategy (a) (b) (c) Figure 8: Comparison of an evenly mixed population of advertising agents using simple and advanced bidding strategies over a range of parameter settings. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. 1/39 5/35 10/30 20/20 30/10 5/35 1/39 0
Number of Advertising Agents Mean Normalised Exposure Simple Bidding Strategy Advanced Bidding Strategy Figure 9: Comparison of an unevenly mixed population of advertising agents using simple and advanced bidding strategies. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. pected utility, measured as the number of users who the advertising agent exposes its advert to, was shown to depend on these factors, and resulted in a complex expression where the expected utility of each auction depended on the success or otherwise of earlier auctions. We presented an algorithm based upon simulated annealing to solve for the optimal bidding strategy, and in simulation, this bidding strategy was shown to significantly outperform a simple bidding strategy that had none of these features. Its performance closely approached that of a central optimal allocation, with perfect knowledge of the arrival and departure of users, despite the uncertain environment in which the strategy must operate.
Our future work in this area consists of extending this bidding strategy to richer environments where there are multiple interrelated display screens, where maintaining profiles of users allows a richer matching of user to advert, and where alternative auction mechanisms are applied (we a particularly interesting in introducing a ‘pay per user" auction setting similar to the ‘pay per click" auctions employed by internet search websites). This work will continue to be done in conjunction with the deployment of more BluScreen prototypes in order to gain further real world experience.
The authors would like to thank Heather Packer and Matthew Sharifi (supported by the ALADDIN project - www.aladdinproject.org) for their help in developing the deployed prototype.
[1] A. Amiri and S. Menon. Efficient scheduling of internet banner advertisements. ACM Transactions on Internet Technology, 3(4):334-346, 2003. [2] S. M. Bohte, E. Gerding, and H. L. Poutre. Market-based recommendation: Agents that compete for consumer attention. ACM Transactions on Internet Technology, 4(4):420-448, 2004. [3] K. Cheverst, A. Dix, D. Fitton, C. Kray, M. Rouncefield, C. Sas,
G. Saslis-Lagoudakis, and J. G. Sheridan. Exploring bluetooth based mobile phone interaction with the hermes photo display. In Proc. of the 7th Int. Conf. on Human Computer Interaction with Mobile Devices & Services, pages 47-54, Salzburg, Austria, 2005. [4] S. Gjerstad and J. Dickhaut. Price formation in double auctions.
Games and Economic Behavior, (22):1-29, 1998. [5] D. Gross and C. M. Harris. Fundamentals of Queueing Theory.

Much research has been undertaken to increase satellite autonomy such as enabling them to solve by themselves problems that may occur during a mission, adapting their behaviour to new events and transferring planning on-board ; even if the development cost of such a satellite is increased, there is an increase in performance and mission possibilities [34]. Moreover, the use of satellite swarms - sets of satellites flying in formation or in constellation around the Earthmakes it possible to consider joint activities, to distribute skills and to ensure robustness.
Multi-agent architectures have been developed for satellite swarms [36, 38, 42] but strong assumptions on deliberation and communication capabilities are made in order to build a collective plan.
Mono-agent planning [4, 18, 28] and task allocation [20] are widely studied. In a multi-agent context, agents that build a collective plan must be able to change their goals, reallocate resources and react to environment changes and to the others" choices. A coordination step must be added to the planning step [40, 30, 11]. However, this step needs high communication and computation capabilities. For instance, coalition-based [37], contract-based [35] and all negotiationbased [25] mechanisms need these capabilities, especially in dynamic environments.
In order to relax communication constraints, coordination based on norms and conventions [16] or strategies [17] are considered. Norms constraint agents in their decisions in such a way that the possibilities of conflicts are reduced.
Strategies are private decision rules that allow an agent to draw benefit from the knowledgeable world without communication. However, communication is still needed in order to share information and build collective conjectures and plans.
Communication can be achieved through a stigmergic approach (via the environment) or through message exchange and a protocol. A protocol defines interactions between agents and cannot be uncoupled from its goal, e.g. exchanging information, finding a trade-off, allocating tasks and so on. Protocols can be viewed as an abstraction of an interaction [9]. They may be represented in a variety of ways, e.g. AUML [32] or Petri-nets [23]. As protocols are originally designed for a single goal, some works aim at endowing them with flexibility [8, 26]. However, an agent cannot always communicate with another agent or the communication possibilites are restricted to short time intervals.
The objective of this work is to use intersatellite connections, called InterSatellite Links or ISL, in an Earth observation constellation inspired from the Fuego mission [13, 19], in order to increase the system reactivity and to improve the mission global return through a hybrid agent approach. At the individual level, agents are deliberative in order to create a local plan but at the collective level, they use normative decision rules in order to coordinate with one another. We will present the features of our problem, a communication protocol, a method for request allocation and finally, collaboration strategies. 287 978-81-904262-7-5 (RPS) c 2007 IFAAMAS
An observation satellite constellation is a set of satellites in various orbits whose mission is to take pictures of various areas on the Earth surface, for example hot points corresponding to volcanos or forest fires. The ground sends the constellation observation requests characterized by their geographical positions, priorities specifying if the requests are urgent or not, the desired dates of observation and the desired dates for data downloading.
The satellites are equipped with a single observation instrument whose mirror can roll to shift the line of sight. A minimum duration is necessary to move the mirror, so requests that are too close together cannot be realized by the same satellite. The satellites are also equipped with a detection instrument pointed forward that detects hot points and generates observation requests on-board.
The constellations that we consider are such as the orbits of the various satellites meet around the poles. A judicious positioning of the satellites in their orbits makes it possible to consider that two (or more) satellites meet in the polar areas, and thus can communicate without the ground intervention. Intuitively, intersatellite communication increases the reactivity of the constellation since each satellite is within direct view of a ground station (and thus can communicate with it) only 10 % of the time.
The features of the problem are the following: - 3 to 20 satellites in the constellation; - pair communication around the poles; - no ground intervention during the planning process; - asynchronous requests with various priorities.
As each satellite is a single entity that is a piece of the global swarm, a multi-agent system fits to model satellite constellations [39]. This approach has been developped through the ObjectAgent architecture [38], TeamAgent [31], DIPS [14] or Prospecting ANTS [12].
An observation satellite swarm1 is a multi-agent system where the requests do not have to be carried out in a fixed order and the agents (the satellites) do not have any physical interaction. Carrying out a request cannot prevent another agent from carrying out another one, even the same one. At most, there will be a waste of resources. Formally, a swarm is defined as follows: Definition 1 (Swarm). A satellite swarm E is a triplet < S, T, Vicinity >: - S is a set of n agents {s1 . . . sn}; - T ⊆ R+ or N+ is a set of dates with a total order <; - Vicinity : S × T → 2S .
In the sequel, we will assume that the agents share a common clock.
For a given agent and a given time, the vicinity relation returns the set of agents with whom it can communicate at that time. As we have seen previously, this relation exists when the agents meet. 1 This term will designate a satellite constellation with InterSatellite Links.
Requests are the observation tasks that the satellite swarm must achieve. As we have seen previously, the requests are generated both on the ground and on board. Each agent is allocated a set of initial requests. During the mission, new requests are sent to the agents by the ground or agents can generate new requests by themselves. Formally, a request is defined as follows: Definition 2 (Request). A request R is defined as a tuple < idR, pos(R), prio(R), tbeg(R),bR >: - idR is an identifier; - pos(R) is the geographic position of R; - prio(R) ∈ R is the request priority; - tbeg(R) ∈ T is the desired date of observation; - bR ∈ {true, false} specifies if R has been realized.
The priority prio(R) of a request represents how much it is important for the user, namely the request sender, that the request should be carried out. Thus a request with a high priority must be realized at all costs. In our application, priorities are comprised between 1 and 5 (the highest).
In the sequel, we will note Rt si the set of the requests that are known by agent si at time t ∈ T.
For each request R in Rt si , there is a cost value, noted costsi (R) ∈ R, representing how far from the desired date of observation tbeg(R) an agent si can realize R. So, the more an agent can carry out a request in the vicinity of the desired date of observation, the lower the cost value.
An agent may have several intentions about a request, i.e. for a request R, an agent si may: - propose to carry out R : si may realize R; - commit to carry out R : si will realize R; - not propose to carry out R : si may not realize R; - refuse to carry out R : si will not realize R.
We can notice that these four propositions are modalities of proposition C: si realizes R: - 3C means that si proposes to carry out R; - 2C means that si commits to carry out R; - ¬3C means that si does not propose to carry out R; - ¬2C means that si refuses to carry out R.
More formally: Definition 3 (Candidacy). A candidacy C is a tuple < idC , modC, sC , RC , obsC, dnlC >: - idC is an identifier; - modC ∈ {3, 2, ¬3, ¬2} is a modality; - sC ∈ S is the candidate agent; - RC ∈ Rt sC is the request on which sC candidates; - obsC ∈ T is the realization date proposed by sC ; - dnlC ∈ T is the download date.
Then, our problem is the following: we would like each agent to build request allocations (i.e a plan) dynamically such as if these requests are carried out their number is the highest possible or the global cost is minimal. More formally,
Definition 4 (Problem). Let E be a swarm. Agents si in E must build a set {At s1 . . . At sn } where At si ⊆ Rt si such 288 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) as: - | S si∈S At si | is maximal;  P si∈S P R∈At si prio(R) is maximal.  P si∈S P R∈At si costsi (R) is minimal.
Let us notice that these criteria are not necessarily compatible.
As the choices of an agent will be influenced by the choices of the others, it is necessary that the agents should reason on a common knowledge about the requests. It is thus necessary to set up an effective communication protocol.
Communication is commonly associated with cooperation.
Deliberative agents need communication to cooperate, whereas it is not necessarily the case for reactive agents [2, 41].
Gossip protocols [22, 24], or epidemic protocols, are used to share knowledge with multicast. Each agent selects a set of agents at a given time in order to share information. The speed of information transmission is contingent upon the length of the discussion round.
The suggested protocol is inspired from what we name the corridor metaphor, which represents well the satellite swarm problem. Various agents go to and fro in a corridor where objects to collect appear from time to time. Two objects that are too close to each other cannot be collected by the same agent because the action takes some time and an agent cannot stop its movement. In order to optimize the collection, the agents can communicate when they meet.
S 2 S ABel A 1 A 3S Figure 1: Time t 1 S 2S Bel non A 3S Figure 2: Time t Example 1. Let us suppose three agents, s1, s2, s3 and an object A to be collected. At time t, s1 did not collect A and s2 does not know that A exists. When s1 meets s2, it communicates the list of the objects it knows, that is to say A. s2 now believes that A exists and prepares to collect it.
It is not certain that A is still there because another agent may have passed before s2, but it can take it into account in its plan.
At time t , s3 collects A. In the vicinity of s2, s3 communicates its list of objects and A is not in the list. As both agents meet in a place where it is possible for s3 to have collected A, the object would have been in the list if it had not been collected. s2 can thus believe that A does not exist anymore and can withdraw it from its plan.
In order to build up their plans, agents need to know the current requests and the others agents" intentions. For each agent two kinds of knowledge to maintain are defined: - requests (Definition 2); - candidacies (Definition 3).
Definition 5 (Knowledge). Knowledge K is a tuple < data(K), SK , tK >: - data(K) is a request R or a candidacy C; - SK ⊆ S is the set of agents knowing K; - tK ∈ T is a temporal timestamp.
In the sequel, we will note Kt si the knowledge of agent si at time t ∈ T.
From the corridor metaphor, we can define a communication protocol that benefits from all the communication opportunities. An agent notifies any change within its knowledge and each agent must propagate these changes to its vicinity who update their knowledge bases and reiterate the process. This protocol is a variant of epidemic protocols [22] inspired from the work on overhearing [27].
Protocol 1 (Communication). Let si be an agent in S. ∀t ∈ T: - ∀ sj ∈ Vicinity(si, t), si executes:
si such as sj ∈ SK : a. si communicates K to sj b. if sj acknowledges receipt of K, SK ← SK ∪ {sj}. - ∀ K ∈ Kt si received by sj at time t:
sj with K
Two kinds of updates exist for an agent: - an internal update from a knowledge modification by the agent itself; - an external update from received knowledge.
For an internal update, updating K depends on data(K): a candidacy C is modified when its modality changes and a request R is modified when an agent realizes it. When K is updated, the timestamp is updated too.
Protocol 2 (Internal update). Let si ∈ S be an agent. An internal update from si at time t ∈ T is performed: - when knowledge K is created; - when data(K) is modified.
In both cases:
For an external update, only the most recent knowledge K is taken into account because timestamps change only when data(K) is modified. If K is already known, it is updated if the content or the set of agents knowing it have been modified. If K is unknown, it is simply added to the agent"s knowledge.
Protocol 3 (External update). Let si be an agent and K the knowledge transmitted by agent sj. ∀ K ∈ K, the external update at time t ∈ T is defined as follows:
si such as iddata(K) = iddata(K ) then a. if tK ≥ tK then i. if tK > tK then SK ← SK ∪ {si} ii. if tK = tK then SK ← SK ∪ SK iii. Kt si ← (Kt si \{K }) ∪ {K}
a. Kt si ← Kt si ∪ {K} b. SK ← SK ∪ {si} The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 289 If the incoming information has a more recent timestamp, it means that the receiver agent has obsolete information.
Consequently, it replaces the old information by the new one and adds itself to the set of agents knowing K (1.a.i).
If both timestamps are the same, both pieces of information are the same. Only the set of the agents knowing K may have changed because agents si and sj may have already transmitted the information to other agents.
Consequently, the sets of agents knowing K are unified (1.a.ii).
Communication between two agents when they meet is made of the conjunction of Protocol 1 and Protocol 3. In the sequel, we call this conjunction a communication occurrence.
The structure of the transmitted information and the internal update mechanism (Protocol 2) allow the process to converge. Indeed, a request R can only be in two states (realized or not) given by the boolean bR. Once an internal update is made - i.e. R is realized - R cannot go back to its former state. Consequently, an internal update can only be performed once.
As far as candidacies are concerned, updates only modify the modalities, which may change many times and go back to previous states. Then it seems that livelocks2 would be likely to appear. However, a candidacy C is associated to a request and a realization date (the deadline given by obsC ). After the deadline, the candidacy becomes meaningless. Thus for each candidacy, there exists a date t ∈ T when changes will propagate no more.
It has been shown that in a set of N agents where a single one has a new piece of information, an epidemic protocol takes O(logN) steps to broadcast the information [33].
During one step, each agent has a communication occurrence.
As agents do not have much time to communicate, such a communication occurrence must not have a too big temporal complexity, which we can prove formally: Proposition 1. The temporal complexity of a communication occurrence at time t ∈ T between two agents si and sj is, for agent si,
O(|Rt si |.|Rt sj |.|S|2 ) Proof 1. For the worst case, each agent sk sends |Rt sk | pieces of information on requests and |Rt sk |.|S| pieces of informations on candidacies (one candidacy for each request and for each agent of the swarm). Let si and sj two agents meeting at time t ∈ T. For agent si, the complexity of Protocol 1 is O(|Rt si | + |Rt si |.|S| | {z } emission + |Rt sj | + |Rt sj |.|S| | {z } reception ) For each received piece of information, agent si uses Protocol 3 and searches through its knowledge bases: |Rt si | pieces of information for each received request and |Rt si |.|S| pieces of 2 Communicating endlessly without converging. information for each received candidacy. Consequently, the complexity of Protocol 3 is O(|Rt sj |.|Rt si | + |Rt sj |.|Rt si |.|S|2 ) Thus, the temporal complexity of a communication occurrence is: O(|Rt si | + |Rt si |.|S| + |Rt sj |.|Rt si | + |Rt sj |.|Rt si |.|S|2 )) Then: O(|Rt si |.|Rt sj |.|S|2 )
In space contexts, [5, 21, 6] present multi-agent architectures for on-board planning. However, they assume high communication and computation capabilities [10]. [13] relax these constraints by cleaving planning modules: on the first hand, satellites have a planner that builds plans on a large horizon and on the second hand, they have a decision module that enables them to choose to realize or not a planned observation.
In an uncertain environment such as the one of satellite swarms, it may be advantageous to delay the decision until the last moment (i.e. the realization date), especially if there are several possibilities for a given request. The main idea in contingency planning [15, 29] is to determine the nodes in the initial plan where the risks of failures are most important and to incrementally build contingency branches for these situations.
Inspired from both approaches, we propose to build allocations made up of a set of unquestionable requests and a set of uncertain disjunctive requests on which a decision will be made at the end of the decision horizon. This horizon corresponds to the request realization date. Proposing such partial allocations allows conflicts to be solved locally without propagating them through the whole plan.
In order to build the agents" initial plans, let us assume that each agent is equipped with an on-board planner. A plan is defined as follows: Definition 6 (Plan). Let si be an agent, Rt si a set of requests and Ct si a set of candidacies. Let us define three sets: - the set of potential requests: Rp = {R ∈ Rt si |bR = false} - the set of mandatory requests: Rm = {R ∈ Rp |∃C ∈ Ct si : modC = 2, sC = si, RC = R} - the set of given-up requests: Rg = {R ∈ Rp |∃C ∈ Ct si : modC = ¬2, sC = si, RC = R} A plan At si generated at time t ∈ T is a set of requests such as Rm ⊆ At si ⊆ Rp and ∃ R ∈ Rg such as R ∈ At si .
Building a plan generates candidacies.
Definition 7 (Generating candidacies). Let si be an agent and At1 si a (possibly empty) plan at time t1. Let At2 si be the plan generated at time t2 with t2 > t1. - ∀ R ∈ At1 si such as R ∈ At2 si , a candidacy C such as mod(C) = ¬3, sC = si and RC = R is generated; - ∀ R ∈ At2 si such as R ∈ At1 si , a candidacy C such as mod(C) = 3, sC = si and RC = R is generated; - Protocol 2 is used to update Kt1 si in Kt2 si . 290 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
When two agents compare their respective plans some conflicts may appear. It is a matter of redundancies between allocations on a given request, i.e.: several agents stand as candidates to carry out this request. Whereas such redundancies may sometimes be useful to ensure the realization of a request (the realization may fail, e.g. because of clouds), it may also lead to a loss of opportunity. Consequently, conflict has to be defined: Definition 8 (Conflict). Let si and sj be two agents with, at time t, candidacies Csi and Csj respectively (sCsi = si and sCsj = sj). si and sj are in conflict if and only if: - RCsi = RCsj - modCsi and modCsj ∈ {2, 3} Let us notice that the agents have the means to know whether they are in conflict with another one during the communication process. Indeed, they exchange information not only concerning their own plan but also concerning what they know about the other agents" plans.
All the conflicts do not have the same strength, meaning that they can be solved with more or less difficulty according to the agents" communication capacities. A conflict is soft when the concerned agents can communicate before one or the other carries out the request in question. A conflict is hard when the agents cannot communicate before the realization of the request.
Definition 9 (Soft/Hard conflict). Let si and sj (i < j) two agents in conflict with, at time t, candidacies Csi and Csj respectively (sCsi = si and sCsj = sj). If ∃ V ⊆ S such as V = {si . . . sj} and if ∃ T ∈ T such as T = {ti−1 . . . tj−1} (ti−1 = t) where: ∀ i ≤ k <j, sk+1 ∈ Vicinity(sk, tk) with tk < obsCsi , tk < obsCsj and tk ≥ tk−1 then the conflict is soft else it is hard.
A conflict is soft if it exists a chain of agents between the two agents in conflict such as information can propagate before both agents realize the request. If this chain does not exist, it means that the agents in conflict cannot communicate directly or not. Consequently, the conflict is hard.
In satellite swarms, the geographical positions of the requests are known as well as the satellite orbits. So each agent is able to determine if a conflict is soft or hard.
We can define the conflict cardinality: Definition 10 (Conflict cardinality). Let si be an agent and R a request in conflict. The conflict cardinality is cardc(R) = |{C ∈ Ct si |modC ∈ {2, 3}, CR = R}|.
The conflict cardinality corresponds to the number of agents that are candidates or committed to the same request. Thus, a conflict has at least a cardinality of 2.
In space contexts, communication time and agents" computing capacities are limited. When they are in conflict, the agents must find a local agreement (instead of an expensive global agreement) by using the conflict in order to increase the number of realized requests, to decrease the time of mission return, to increase the quality of the pictures taken or to make sure that a request is carried out.
Example 2. Let us suppose a conflict on request R between agents si and sj. We would like that the most expert agent, i.e. the agent that can carry out the request under the best conditions, does it. Let us suppose si is the expert. si must allocate R to itself. It remains to determine what sj must do: sj can either select a substitute for R in order to increase the number of requests potentially realized, or do nothing in order to preserve resources, or allocate R to itself to ensure redundancy.
Consequently, we can define collaboration strategies dedicated to conflict solving. A strategy is a private (namely intrinsic to an agent) decision process that allows an agent to make a decision on a given object. In our application, strategies specify what to do with redundancies.
In our application, cost is linked to the realization dates.
Carrying out a request consumes the agents" resources (e.g.: on-board energy, memory). Consequently, an observation has a cost for each agent which depends on when it is realized: the closer the realization date to the desired date of observation, the lower the cost.
Definition 11 (Cost). Let si be an agent. The cost costsi (RC ) ∈ R to carry out a request RC according to a candidacy C is defined as: costsi (RC ) = |obsC − tbeg(RC)|.
From this cost notion, we can formally define an expert notion between two agents. The expertise for an agent means it can realize the request at the lower cost.
Definition 12 (Expertise). Let si and sj ∈ S be two agents and R a request. Agent si is an expert for R if and only if costsi (R) ≤ costsj (R).
Three strategies are proposed to solve a conflict. The expert strategy means that the expert agent maintains its candidacy whereas the other one gives up. The altruist strategy means that the agent that can download first3 , provided the cost increase is negligible, maintains its candidacy whereas the other one gives up. The insurance strategy means that both agents maintain their candidacies in order to ensure redundancy.
Strategy 1 (Expert). Let si and sj be two agents in conflict on their respective candidacies Csi and Csj such as si is the expert agent. The expert strategy is: modCsi = 2 and modCsj = ¬2.
Strategy 2 (Altruist). Let si and sj be two agents in conflict on their respective candidacies Csi and Csj such as si is the expert agent. Let ∈ R+ be a threshold on the cost increase. The altruist strategy is : if dnlCsi > dnlCsj and |costsi (R) − costsj (R)| < then modCsi = ¬2 and modCsj = 2.
Strategy 3 (Insurance). Let si and sj be two agents in conflict on their respective candidacies Csi and Csj such as si is the expert agent. Let α ∈ R be a priority threshold.
The insurance strategy is : if prio(R) cardc(R)−1 > α then modCsi = 3 and modCsj = 3. 3 i.e. the agent using memory resources during a shorter time.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 291 In the insurance strategy, redundancy triggering is adjusted by the conflict cardinality cardc(R). The reason is the following: the more redundancies on a given request, the less a new redundancy on this request is needed.
The three strategies are implemented in a negotiation protocol dedicated to soft conflicts. The protocol is based on a subsumption architecture [7] on strategies: the insurance strategy (1) is the major strategy because it ensures redundancy for which the swarm is implemented. Then the altruist strategy comes (2) in order to allocate the resources so as to enhance the mission return. Finally, the expert strategy that does not have preconditions (3) enhances the cost of the plan.
Protocol 4 (Soft conflict solving). Let R be a request in a soft conflict between two agents, si and sj.
These agents have Csi and Csj for respective candidacies.
Let si be the expert agent. Agents apply strategies as follows:
The choice of parameters α and allows to adjust the protocol results. For example, if = 0, the altruist strategy is never used.
In case of a hard conflict, the agent that is not aware will necessarily realize the request (with success or not).
Consequently, a redundancy is useful only if the other agent is more expert or if the priority of the request is high enough to need redundancy. Therefore, we will use the insurance strategy (refer to Section 6.2) and define a competitive strategy.
The latter is defined for two agents, si and sj, in a hard conflict on a request R. Let si be the agent that is aware of the conflict4 .
Strategy 4 (Competitive). Let λ ∈ R+ be an cost threshold. The competitive strategy is: if costsi (R) < costsj (R) − λ then modCsi = 3.
Protocol 5 (Hard conflict solving). Let si be an agent in a hard conflict with an agent sj on a request R. si applies strategies as follows:
= ¬2
Although agents use pair communication, they may have information about several agents and conflict cardinality may be more than 2. Therefore, we define a k-conflict as a conflict with a cardinality of k on a set of agents proposing or committing to realize the same request. Formally,
Definition 13 (k-conflict). Let S = {s1 . . . sk} be a set of agents with respective candidacies Cs1 . . . Csk at time t. The set S is in a k-conflict if and only if: - ∀1 ≤ i ≤ k, sCsi = si; - !∃R such as ∀1 ≤ i ≤ k, RCsi = R; 4 i.e. the agent that must make a decision on R. - ∀1 ≤ i ≤ k, modCsi ∈ {2, 3}. - S is maximal (⊆) among the sets that satisfy these properties.
As previously, a k-conflict can be soft or hard. A k-conflict is soft if each pair conflict in the k-conflict is a soft conflict with respect to Definition 9.
As conflicts bear on sets of agents, expertise is a total order on agents. We define rank-i-expertise where the concerned agent is the ith expert.
In case of a soft k-conflict, the rank-i-expert agent makes its decision with respect to the rank-(i + 1)-expert agent according to Protocol 4. The protocol is applied recursively and α and parameters are updated at each step in order to avoid cost explosion5 .
In case of a hard conflict, the set S of agents in conflict can be splitted in SS (the subset of agents in a soft conflict) and SH (the subset of unaware agents). Only agents in SS can take a decision and must adapt themselves to agents in SH . The rank-i-expert agent in SS uses Protocol 5 on the whole set SH and the rank-(i − 1)-expert agent in SS . If an agent in SS applies the competitive strategy all the others withdraws.
Satellite swarm simulations have been implemented in JAVA with the JADE platform [3]. The on-board planner is implemented with linear programming using ILOG CPLEX [1]. The simulation scenario implements 3 satellites on 6hour orbits. Two scenarios have been considered: the first one with a set of 40 requests with low mutual exclusion and conflict rate and the second one with a set of 74 requests with high mutual exclusion and conflict rate.
For each scenario, six simulations have been performed: one with centralized planning (all requests are planned by the ground station before the simulation), one where agents are isolated (they cannot communicate nor coordinate with one another), one informed simulation (agents only communicate requests) and three other simulations implementing the instanciated collaboration strategies (politics): - neutral politics: α, and λ are set to average values; - drastic politics: α and λ are set to higher values, i.e. agents will ensure redundancy only if the priorities are high and, in case of a hard conflict, if the cost payoff is much higher; - lax politics: α is set to a lower value, i.e. redundancies are more frequent.
In the case of low mutual exclusion and conflict rate (Table 1), centralized and isolated simulations lead to the same number of observations, with the same average priorities.
Isolation leading to a lower cost is due to the high number of redundancies: many agents carry out the same request at different costs. The informed simulation reduces the number of redundancies but sligthly increases the average cost for the same reason. We can notice that the use of 5 For instance, the rank-1-expert agent withdraws due to the altruist strategy and the cost increases by in the worst case, then rank-2-expert agent withdraws due to the altruist strategy and the cost increases by in the worst case. So the cost has increased by 2 in the worst case. 292 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Simulation Observations Redundancies Messages Average priority Average cost Centralized 34 0 0 2.76 176.06 Isolated 34 21 0 2.76 160.88 Informed 34 6 457 2.65 165.21 Neutral politics 31 4 1056 2.71 191.16 Drastic politics 24 1 1025 2.71 177.42 Lax politics 33 5 1092 2.7 172.88 Table 1: Scenario 1 - the 40-request simulation results Simulation Observations Redundancies Messages Average priority Average cost Centralized 59 0 0 2.95 162.88 Isolated 37 37 0 3.05 141.62 Informed 55 27 836 2.93 160.56 Neutral politics 48 25 1926 3.13 149.75 Drastic politics 43 21 1908 3.19 139.7 Lax politics 53 28 1960 3 154.02 Table 2: Scenario 2 - the 74-request simulation results collaboration strategies allows the number of redundancies to be much more reduced but the number of observations decreases owing to the constraint created by commitments.
Furthermore, the average cost is increased too. Nevertheless each avoided redundancy corresponds to saved resources to realize on-board generated requests during the simulation.
In the case of high mutual exclusion and conflict rate (Table 2), noteworthy differences exist between the centralized and isolated simulations. We can notice that all informed simulations (with or without strategies) allow to perform more observations than isolated agents do with less redundancies. Likewise, we can notice that all politics reduce the average cost contrary to the first scenario. The drastic politics is interesting because not only does it allow to perform more observations than isolated agents do but it allows to highly reduce the average cost with the lowest number of redundancies.
As far as the number of exchanged messages is concerned, there are 12 meetings between 2 agents during the simulations. In the worst case, at each meeting each agent sends N pieces of information on the requests plus 3N pieces of information on the agents" intentions plus 1 message for the end of communication, where N is the total number of requests. Consequently, 3864 messages are exchanged in the worst case for the 40-request simulations and 7128 messages for the 74-request simulations. These numbers are much higher than the number of messages that are actually exchanged. We can notice that the informed simulations, that communicate only requests, allow a higher reduction.
In the general case, using communication and strategies allows to reduce redundancies and saves resources but increases the average cost: if a request is realized, agents that know it do not plan it even if its cost can be reduce afterwards. It is not the case with isolated agents. Using strategies on little constrained problems such as scenario 1 constrains the agents too much and causes an additional cost increase. Strategies are more useful on highly constrained problems such as scenario 2. Although agents constrain themselves on the number of observations, the average cost is widely reduce.
An observation satellite swarm is a cooperative multiagent system with strong constraints in terms of communication and computation capabilities. In order to increase the global mission outcome, we propose an hybrid approach: deliberative for individual planning and reactive for collaboration.
Agents reason both on requests to carry out and on the other agents" intentions (candidacies). An epidemic communication protocol uses all communication opportunities to update this information. Reactive decision rules (strategies) are proposed to solve conflicts that may arise between agents. Through the tuning of the strategies (α, and λ) and their plastic interlacing within the protocol, it is possible to coordinate agents without additional communication: the number of exchanged messages remains nearly the same between informed simulations and simulations implementing strategies.
Some simulations have been made to experimentally validate these protocols and the first results are promising but raise many questions. What is the trade-off between the constraint rate of the problem and the need of strategies?
To what extent are the number of redundancies and the average cost affected by the tuning of the strategies?
Future works will focus on new strategies to solve new conflicts, specially those arising when relaxing the independence assumption between the requests. A second point is to take into account the complexity of the initial planning problem. Indeed, the chosen planning approach results in a combinatory explosion with big sets of requests: an anytime or a fully reactive approach has to be considered for more complex problems.
Acknowledgements We would like to thank Marie-Claire Charmeau (CNES6 ),
Serge Rainjonneau and Pierre Dago (Alcatel Space Alenia) for their relevant comments on this work. 6 The French Space Agency The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 293
[1] ILOG inc. CPLEX. http://www.ilog.com/products/cplex. [2] T. Balch and R. Arkin. Communication in reactive multiagent robotic systems. Autonomous Robots, pages 27-52, 1994. [3] F. Bellifemine, A. Poggi, and G. Rimassa. JADE - a FIPA-compliant agent framework. In Proceedings of PAAM"99, pages 97-108, 1999. [4] A. Blum and M. Furst. Fast planning through planning graph analysis. Artificial Intelligence, Vol. 90:281-300, 1997. [5] E. Bornschlegl, C. Guettier, G. L. Lann, and J.-C.
Poncet. Constraint-based layered planning and distributed control for an autonomous spacecraft formation flying. In Proceedings of the 1st ESA Workshop on Space Autonomy, 2001. [6] E. Bornschlegl, C. Guettier, and J.-C. Poncet.
Automatic planning for autonomous spacecraft constellation. In Proceedings of the 2nd NASA Intl.
Workshop on Planning and Scheduling for Space, 2000. [7] R. Brooks. A robust layered control system for a mobile robot. MIT AI Lab Memo, Vol. 864, 1985. [8] A. Chopra and M. Singh. Nonmonotonic commitment machines. Lecture Notes in Computer Science: Advances in Agent Communication, Vol. 2922:183-200, 2004. [9] A. Chopra and M. Singh. Contextualizing commitment protocols. In Proceedings of the 5th AAMAS, 2006. [10] B. Clement and A. Barrett. Continual coordination through shared activites. In Proceedings of the 2nd AAMAS, pages 57-64, 2003. [11] J. Cox and E. Durfee. Efficient mechanisms for multiagent plan merging. In Proceedings of the 3rd AAMAS, 2004. [12] S. Curtis, M. Rilee, P. Clark, and G. Marr. Use of swarm intelligence in spacecraft constellations for the resource exploration of the asteroid belt. In Proceedings of the Third International Workshop on Satellite Constellations and Formation Flying, pages 24-26, 2003. [13] S. Damiani, G. Verfaillie, and M.-C. Charmeau. An Earth watching satellite constellation : How to manage a team of watching agents with limited communications. In Proceedings of the 4th AAMAS, pages 455-462, 2005. [14] S. Das, P. Gonzales, R. Krikorian, and W. Truszkowski. Multi-agent planning and scheduling environment for enhanced spacecraft autonomy. In Proceedings of the 5th ISAIRAS, 1999. [15] R. Dearden, N. Meuleau, S. Ramakrishnan, D. Smith, and R. Wahington. Incremental contingency planning.
In Proceedings of ICAPS"03 Workshop on Planning under Uncertainty and Incomplete Information, pages 1-10, 2003. [16] F. Dignum. Autonomous agents with norms. Artificial Intelligence and Law, Vol. 7:69-79, 1999. [17] E. Durfee. Scaling up agent coordination strategies.
IEEE Computer, Vol. 34(7):39-46, 2001. [18] K. Erol, J. Hendler, and D. Nau. HTN planning : Complexity and expressivity. In Proceedings of the 12th AAAI, pages 1123-1128, 1994. [19] D. Escorial, I. F. Tourne, and F. J. Reina. Fuego : a dedicated constellation of small satellites to detect and monitor forest fires. Acta Astronautica,
Vol.52(9-12):765-775, 2003. [20] B. Gerkey and M. Matarić. A formal analysis and taxonomy of task allocation in multi-robot systems.
Journal of Robotics Research, Vol. 23(9):939-954,
[21] C. Guettier and J.-C. Poncet. Multi-level planning for spacecraft autonomy. In Proceedings of the 6th ISAIRAS, pages 18-21, 2001. [22] I. Gupta, A.-M. Kermarrec, and A. Ganesh. Efficient epidemic-style protocols for reliable and scalable multicast. In Proceedings of the 21st IEEE Symposium on Reliable Distributed Systems, pages 180-189, 2002. [23] G. Gutnik and G. Kaminka. Representing conversations for scalable overhearing. Journal of Artificial Intelligence Research, Vol. 25:349-387, 2006. [24] K. Jenkins, K. Hopkinson, and K. Birman. A gossip protocol for subgroup multicast. In Proceedings of the 21st International Conference on Distributed Computing Systems Workshops, pages 25-30, 2001. [25] N. Jennings, S. Parsons, P. Norriega, and C. Sierra.
On augumentation-based negotiation. In Proceedings of the International Workshop on Multi-Agent Systems, pages 1-7, 1998. [26] J.-L. Koning and M.-P. Huget. A semi-formal specification language dedicated to interaction protocols. Information Modeling and Knowledge Bases XII: Frontiers in Artificial Intelligence and Applications, pages 375-392, 2001. [27] F. Legras and C. Tessier. LOTTO: group formation by overhearing in large teams. In Proceedings of 2nd AAMAS, 2003. [28] D. McAllester, D. Rosenblitt, P. Norriega, and C. Sierra. Systematic nonlinear planning. In Proceedings of the 9th AAAI, pages 634-639, 1991. [29] N. Meuleau and D. Smith. Optimal limited contingency planning. In Proceedings of the 19th AAAI, pages 417-426, 2003. [30] P. Modi and M. Veloso. Bumping strategies for the multiagent agreement problem. In Proceedings of the 4th AAMAS, pages 390-396, 2005. [31] J. B. Mueller, D. M. Surka, and B. Udrea.
Agent-based control of multiple satellite formation flying. In Proceedings of the 6th ISAIRAS, 2001. [32] J. Odell, H. Parunak, and B. Bauer. Extending UML for agents. In Proceedings of the Agent-Oriented Information Systems Workshop at the 17th AAAI,
[33] B. Pittel. On spreading a rumor. SIAM Journal of Applied Mathematics, Vol. 47:213-223, 1987. [34] B. Polle. Autonomy requirement and technologies for future constellation. Astrium Summary Report, 2002. [35] T. Sandholm. Contract types for satisficing task allocation. In Proceedings of the AAAI Spring Symposium: Satisficing Models, pages 23-25, 1998. [36] T. Schetter, M. Campbell, and D. M. Surka. Multiple agent-based autonomy for satellite constellation.
Artificial Intelligence, Vol. 145:147-180, 2003. [37] O. Shehory and S. Kraus. Methods for task allocation via agent coalition formation. Artificial Intelligence,
Vol. 101(1-2):165-200, 1998. [38] D. M. Surka. ObjectAgent for robust autonomous control. In Proceedings of the AAAI Spring Symposium, 2001. [39] W. Truszkowski, D. Zoch, and D. Smith. Autonomy for constellations. In Proceedings of the SpaceOps Conference, 2000. [40] R. VanDerKrogt and M. deWeerdt. Plan repair as an extension of planning. In Proceedings of the 15th ICAPS, pages 161-170, 2005. [41] B. Werger. Cooperation without deliberation : A minimal behavior-based approach to multi-robot teams. Artificial Intelligence, Vol. 110:293-320, 1999. [42] P. Zetocha. Satellite cluster command and control.

The recent surge of interest in online auctions has resulted in an increasing number of auctions offering very similar or even identical goods and services [9, 10]. In eBay alone, for example, there are often hundreds or sometimes even thousands of concurrent auctions running worldwide selling such substitutable items1 . Against this background, it is essential to develop bidding strategies that autonomous agents can use to operate effectively across a wide number of auctions. To this end, in this paper we devise and analyse optimal bidding strategies for an important yet barely studied setting - namely, an agent that participates in multiple, concurrent (i.e., simultaneous) second-price auctions for goods that are perfect substitutes. As we will show, however, this analysis is also relevant to a wider context where auctions are conducted sequentially, as well as concurrently.
To date, much of the existing literature on multiple auctions focuses either on sequential auctions [6] or on simultaneous auctions for complementary goods, where the value of items together is greater than the sum of the individual items (see Section 2 for related research on simultaneous auctions).
In contrast, here we consider bidding strategies for markets with multiple concurrent auctions and perfect substitutes.
In particular, our focus is on Vickrey or second-price sealed bid auctions. We choose these because they require little communication and are well known for their capacity to induce truthful bidding, which makes them suitable for many multi-agent system settings. However, our results generalise to settings with English auctions since these are strategically equivalent to second-price auctions. Within this setting, we are able to characterise, for the first time, a bidder"s utilitymaximising strategy for bidding simultaneously in any number of such auctions and for any type of bidder valuation distribution. In more detail, we first consider a market where a single bidder, called the global bidder, can bid in any number of auctions, whereas the other bidders, called the local bidders, are assumed to bid only in a single auction. For this case, we find the following results: • Whereas in the case of a single second-price auction a bidder"s best strategy is to bid its true value, the best strategy for a global bidder is to bid below it. • We are able to prove that, even if a global bidder requires only one item, the expected utility is maximised by participating in all the auctions that are selling the desired item. • Finding the optimal bid for each auction can be an arduous task when considering all possible combinations.
However, for most common bidder valuation distributions, we are able to significantly reduce this search space and thus the computation required. • Empirically, we find that a bidder"s expected utility is maximised by bidding relatively high in one of the auctions, and equal or lower in all other auctions.
We then go on to consider markets with more than one global bidder. Due to the complexity of the problem, we combine analytical results with a discrete simulation in order to numerically derive the optimal bidding strategy. By so doing, we find that, in a market with only global bidders, the dynamics of the best response do not converge to a pure strategy. In fact it fluctuates between two states. If the market consists of both local and global bidders, however, the global bidders" strategy quickly reaches a stable solution and we approximate a symmetric Nash equilibrium.
The remainder of the paper is structured as follows.
Section 2 discusses related work. In Section 3 we describe the bidders and the auctions in more detail. In Section 4 we investigate the case with a single global bidder and characterise the optimal bidding behaviour for it. Section 5 considers the case with multiple global bidders and in Section 6 we address the market efficiency. Finally, Section 7 concludes.
Research in the area of simultaneous auctions can be segmented along two broad lines. On the one hand, there is the game-theoretic and decision-theoretic analysis of simultaneous auctions which concentrates on studying the equilibrium strategy of rational agents [3, 7, 8, 9, 12, 11]. Such analyses are typically used when the auction format employed in the concurrent auctions is the same (e.g. there are M Vickrey auctions or M first-price auctions). On the other hand, heuristic strategies have been developed for more complex settings when the sellers offer different types of auctions or the buyers need to buy bundles of goods over distributed auctions [1, 13, 5]. This paper adopts the former approach in studying a market of M simultaneous Vickrey auctions since this approach yields provably optimal bidding strategies.
In this case, the seminal paper by Engelbrecht-Wiggans and Weber provides one of the starting points for the gametheoretic analysis of distributed markets where buyers have substitutable goods. Their work analyses a market consisting of couples having equal valuations that want to bid for a dresser. Thus, the couple"s bid space can at most contain two bids since the husband and wife can be at most at two geographically distributed auctions simultaneously. They derive a mixed strategy Nash equilibrium for the special case where the number of buyers is large. Our analysis differs from theirs in that we study concurrent auctions in which bidders have different valuations and the global bidder can bid in all the auctions concurrently (which is entirely possible given autonomous agents).
Following this, [7] then studied the case of simultaneous auctions with complementary goods. They analyse the case of both local and global bidders and characterise the bidding of the buyers and resultant market efficiency. The setting provided in [7] is further extended to the case of common values in [9]. However, neither of these works extend easily to the case of substitutable goods which we consider. This case is studied in [12], but the scenario considered is restricted to three sellers and two global bidders and with each bidder having the same value (and thereby knowing the value of other bidders). The space of symmetric mixed equilibrium strategies is derived for this special case, but again our result is more general. Finally, [11] considers the case of concurrent English auctions, in which he develops bidding algorithms for buyers with different risk attitudes. However, he forces the bids to be the same across auctions, which we show in this paper not always to be optimal.
The model consists of M sellers, each of whom acts as an auctioneer. Each seller auctions one item; these items are complete substitutes (i.e., they are equal in terms of value and a bidder obtains no additional benefit from winning more than one item). The M auctions are executed concurrently; that is, they end simultaneously and no information about the outcome of any of the auctions becomes available until the bids are placed2 . However, we briefly address markets with both sequential and concurrent auctions in Section 4.4.
We also assume that all the auctions are equivalent (i.e., a bidder does not prefer one auction over another). Finally, we assume free disposal (i.e., a winner of multiple items incurs no additional costs by discarding unwanted ones) and risk neutral bidders.
The seller"s auction is implemented as a Vickrey auction, where the highest bidder wins but pays the second-highest price. This format has several advantages for an agent-based setting. Firstly, it is communication efficient. Secondly, for the single-auction case (i.e., where a bidder places a bid in at most one auction), the optimal strategy is to bid the true value and thus requires no computation (once the valuation of the item is known). This strategy is also weakly dominant (i.e., it is independent of the other bidders" decisions), and therefore it requires no information about the preferences of other agents (such as the distribution of their valuations).
We distinguish between global and local bidders. The former can bid in any number of auctions, whereas the latter only bid in a single one. Local bidders are assumed to bid according to the weakly dominant strategy and bid their true valuation3 .
We consider two ways of modelling local bidders: static and dynamic. In the first model, the number of local bidders is assumed to be known and equal to N for each auction.
In the latter model, on the other hand, the average number of bidders is equal to N, but the exact number is unknown and may vary for each auction. This uncertainty is modelled using a Poisson distribution (more details are provided in Section 4.1).
As we will later show, a global bidder who bids optimally has a higher expected utility compared to a local bidder, even though the items are complete substitutes and a bidder only requires one of them. However, we can identify a number of compelling reasons why not all bidders would choose to bid globally. Firstly, participation costs such as entry fees and time to set up an account may encourage occasional users to 2 Although this paper focuses on sealed-bid auctions, where this is the case, the conditions are similar for last-minute bidding in English auctions such as eBay [10]. 3 Note that, since bidding the true value is optimal for local bidders irrespective of what others are bidding, their strategy is not affected by the presence of global bidders. 280 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) participate in auctions that they are already familiar with.
Secondly, bidders may simply not be aware of other auctions selling the same type of item. Even if this is known, however, additional information such as the distribution of the valuations of other bidders and the number of participating bidders is required for bidding optimally across multiple auctions. This lack of expert information often drives a novice to bid locally. Thirdly, an optimal global strategy is harder to compute than a local one. An agent with bounded rationality may therefore not have the resources to compute such a strategy. Lastly, even though a global bidder profits on average, such a bidder may incur a loss when inadvertently winning multiple auctions. This deters bidders who are either risk averse or have budget constraints from participating in multiple auction. As a result, in most market places we expect a combination of global and local bidders.
In view of the above considerations, human buyers are more likely to bid locally. The global strategy, however, can be effectively executed by autonomous agents since they can gather data from many auctions and perform the required calculations within the desired time frame.
In this section, we provide a theoretical analysis of the optimal bidding strategy for a global bidder, given that all other bidders are local and simply bid their true valuation.
After we describe the global bidder"s expected utility in Section 4.1, we show in Section 4.2 that it is always optimal for a global bidder to participate in the maximum number of auctions available. In Section 4.3 we discuss how to significantly reduce the complexity of finding the optimal bids for the multi-auction problem, and we then apply these methods to find optimal strategies for specific examples. Finally, in Section 4.4 we extend our analysis to sequential auctions.
In what follows, the number of sellers (auctions) is M ≥ 2 and the number of local bidders is N ≥ 1. A bidder"s valuation v ∈ [0, vmax] is randomly drawn from a cumulative distribution F with probability density f, where f is continuous, strictly positive and has support [0, vmax]. F is assumed to be equal and common knowledge for all bidders. A global bid B is a set containing a bid bi ∈ [0, vmax] for each auction 1 ≤ i ≤ M (the bids may be different for different auctions).
For ease of exposition, we introduce the cumulative distribution function for the first-order statistics G(b) = F(b)N ∈ [0, 1], denoting the probability of winning a specific auction conditional on placing bid b in this auction, and its probability density g(b) = dG(b)/db = NF(b)N−1 f(b). Now, the expected utility U for a global bidder with global bid B and valuation v is given by: U(B, v) = v ⎡ ⎣1 − bi∈B (1 − G(bi)) ⎤ ⎦ − bi∈B bi 0 yg(y)dy (1) Here, the left part of the equation is the valuation multiplied by the probability that the global bidder wins at least one of the M auctions and thus corresponds to the expected benefit. In more detail, note that 1 − G(bi) is the probability of not winning auction i when bidding bi, bi∈B(1 − G(bi)) is the probability of not winning any auction, and thus 1 − bi∈B(1 − G(bi)) is the probability of winning at least one auction. The right part of equation 1 corresponds to the total expected costs or payments. To see the latter, note that the expected payment of a single secondprice auction when bidding b equals b 0 yg(y)dy (see [6]) and is independent of the expected payments for other auctions.
Clearly, equation 1 applies to the model with static local bidders, i.e., where the number of bidders is known and equal for each auction (see Section 3.2). However, we can use the same equation to model dynamic local bidders in the following way: Lemma 1 By replacing the first-order statistic G(y) with ˆG(y) = eN(F (y)−1) , (2) and the corresponding density function g(y) with ˆg(y) = d ˆG(y)/dy = N f(y)eN(F (y)−1) , equation 1 becomes the expected utility where the number of local bidders in each auction is described by a Poisson distribution with average N (i.e., where the probability that n local bidders participate is given by P(n) = Nn e−N /n!).
Proof To prove this, we first show that G(·) and F(·) can be modified such that the number of bidders per auction is given by a binomial distribution (where a bidder"s decision to participate is given by a Bernoulli trial) as follows: G (y) = F (y)N = (1 − p + p F (y))N , (3) where p is the probability that a bidder participates in the auction, and N is the total number of bidders. To see this, note that not participating is equivalent to bidding zero. As a result, F (0) = 1 − p since there is a 1 − p probability that a bidder bids zero at a specific auction, and F (y) = F (0) + p F(y) since there is a probability p that a bidder bids according to the original distribution F(y). Now, the average number of participating bidders is given by N = p N.
By replacing p with N/N, equation 3 becomes G (y) = (1 − N/N + (N/N)F(y))N . Note that a Poisson distribution is given by the limit of a binomial distribution. By keeping N constant and taking the limit N → ∞, we then obtain G (y) = eN(F (y)−1) = ˆG(y). This concludes our proof.
The results that follow apply to both the static and dynamic model unless stated otherwise.
We now show that, for any valuation 0 < v < vmax, a utilitymaximising global bidder should always place non-zero bids in all available auctions. To prove this, we show that the expected utility increases when placing an arbitrarily small bid compared to not participating in an auction. More formally,
Theorem 1 Consider a global bidder with valuation 0 < v < vmax and global bid B, where bi ≤ v for all bi ∈ B.
Suppose B contains no bid for auction j ∈ {1, 2, . . . , M}, then there exists a bj > 0 such that U(B∪{bj }, v) > U(B, v).
Proof Using equation 1, the marginal expected utility for participating in an additional auction can be written as: U(B ∪ {bj }, v) − U(B, v) = vG(bj ) bi∈B (1 − G(bi)) − bj 0 yg(y)dy Now, using integration by parts, we have bj 0 yg(y) = bjG(bj)− bj 0 G(y)dy and the above equation can be rewritten as: U(B ∪ {bj }, v) − U(B, v) = G(bj ) ⎡ ⎣v bi∈B (1 − G(bi)) − bj ⎤ ⎦ + bj 0 G(y)dy (4) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 281 Let bj = , where is an arbitrarily small strictly positive value. Clearly, G(bj) and bj 0 G(y)dy are then both strictly positive (since f(y) > 0). Moreover, given that bi ≤ v < vmax for bi ∈ B and that v > 0, it follows that v bi∈B(1 − G(bi)) > 0. Now, suppose bj = 1 2 v bi∈B(1 − G(bi)), then U(B ∪ {bj }, v) − U(B, v) = G(bj ) 1 2 v bi∈B(1 − G(bi)) + bj 0 G(y)dy > 0 and thus U(B ∪ {bj }, v) > U(B, v). This completes our proof.
A general solution to the optimal global bid requires the maximisation of equation 1 in M dimensions, an arduous task, even when applying numerical methods. In this section, however, we show how to reduce the entire bid space to two dimensions in most cases (one continuous, and one discrete), thereby significantly simplifying the problem at hand. First, however, in order to find the optimal solutions to equation 1, we set the partial derivatives to zero: ∂U ∂bi = g(bi) ⎡ ⎣v bj ∈B\{bi} (1 − G(bj)) − bi ⎤ ⎦ = 0 (5) Now, equality 5 holds either when g(bi) = 0 or when bj ∈B\{bi}(1 − G(bj ))v − bi = 0. In the dynamic model, g(bi) is always greater than zero, and can therefore be ignored (since g(0) = Nf(0)e−N and we assume f(y) > 0).
In the static model, g(bi) = 0 only when bi = 0.
However, theorem 1 shows that the optimal bid is non-zero for 0 < v < vmax. Therefore, we can ignore the first part, and the second part yields: bi = v bj ∈B\{bi} (1 − G(bj)) (6) In other words, the optimal bid in auction i is equal to the bidder"s valuation multiplied by the probability of not winning any of the other auctions. It is straightforward to show that the second partial derivative is negative, confirming that the solution is indeed a maximum when keeping all other bids constant. Thus, equation 6 provides a means to derive the optimal bid for auction i, given the bids in all other auctions.
In what follows, we show that, for non-decreasing probability density functions (such as the uniform and logarithmic distributions), the optimal global bid consists of at most two different values for any M ≥ 2. That is, the search space for finding the optimal bid can then be reduced to two continuous values. Let these values be bhigh and blow, where bhigh ≥ blow. More formally: Theorem 2 Suppose the probability density function f is non-decreasing within the range [0, vmax], then the following proposition holds: given v > 0, for any bi ∈ B, either bi = bhigh, bi = blow, or bi = bhigh = blow.
Proof Using equation 6, we can produce M equations, one for each auction, with M unknowns. Now, by combining these equations, we obtain the following relationship: b1(1 − G(b1)) = b2(1 − G(b2)) = . . . = bm(1 − G(bm)). By defining H(b) = b(1 − G(b)) we can rewrite the equation to: H(b1) = H(b2) = . . . = H(bm) = v bj ∈B (1 − G(bj )) (7) In order to prove that there exist at most two different bids, it is sufficient to show that b = H−1 (y) has at most two solutions that satisfy 0 ≤ b ≤ vmax for any y. To see this, suppose H−1 (y) has two solutions but there exists a third bid bj = blow = bhigh. From equation 7 it then follows that there exists a y such that H(bj) = H(blow) = H(bhigh) = y.
Therefore, H−1 (y) must have at least three solutions, which is a contradiction.
Now, note that, in order to prove that H−1 (y) has at most two solutions, it is necessary and sufficient to show that H(b) has at most one local maximum for 0 ≤ b ≤ vmax. A sufficient conditions, however, is for H(b) to be strictly concave4 .
The function H is strictly concave if and only if the following condition holds: H (b) = d db (1 − b · g(b) − G(b)) = − b dg db + 2g(b) < 0 (8) where H (b) = d2 H/db2 . By performing standard calculations, we obtain the following condition for the static model: b (N − 1) f(b) F(b) + N f (b) f(b) > −2 for 0 ≤ b ≤ vmax, (9) and similarly for the dynamic model we have: b N f(b) + f (b) f(b) > −2 for 0 ≤ b ≤ vmax, (10) where f (b) = df/db. Since both f and F are positive, conditions 9 and 10 clearly hold for f (b) ≥ 0. In other words, conditions 9 and 10 show that H(b) is strictly concave when the probability density function is non-decreasing for 0 ≤ b ≤ vmax, completing our proof.
Note from conditions 9 and 10 that the requirement of non-decreasing density functions is sufficient, but far from necessary. Moreover, condition 8 requiring H(b) to be strictly concave is also stronger than necessary to guarantee only two solutions. As a result, in practice we find that the reduction of the search space applies to most cases.
Given there are at most 2 possible bids, blow and bhigh, we can further reduce the search space by expressing one bid in terms of the other. Suppose the buyer places a bid of blow in Mlow auctions and bhigh for the remaining Mhigh = M−Mlow auctions, equation 6 then becomes: blow = v(1 − G(blow))Mlow−1 (1 − G(bhigh))Mhigh , and can be rearranged to give: bhigh = G−1 1 − blow v(1 − G(blow))Mlow−1 1 Mhigh (11) Here, the inverse function G−1 (·) can usually be obtained quite easily. Furthermore, note that, if Mlow = 1 or Mhigh = 1, equation 6 can be used directly to find the desired value.
Using the above, we are able to reduce the bid search space to a single continuous dimension, given Mlow or Mhigh.
However, we do not know the number of auctions in which to bid blow and bhigh, and thus we need to search M different combinations to find the optimal global bid. Moreover, for each 4 More precisely, H(b) can be either strictly convex or strictly concave. However, it is easy to see that H is not convex since H(0) = H(vmax) = 0, and H(b) ≥ 0 for 0 < b < vmax. 282 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 0.5 1 0
1 valuation (v) bidfraction(x) 0 0.5 1 0
local M=2 M=4 M=6 valuation (v) expectedutility Figure 1: The optimal bid fractions x = b/v and corresponding expected utility for a single global bidder with N = 5 static local bidders and varying number of auctions (M). In addition, for comparison, the dark solid line in the right figure depicts the expected utility when bidding locally in a randomly selected auction, given there are no global bidders (note that, in case of local bidders only, the expected utility is not affected by M). combination, the optimal blow and bhigh can vary.
Therefore, in order to find the optimal bid for a bidder with valuation v, it is sufficient to search along one continuous variable blow ∈ [0, v], and a discrete variable Mlow = M − Mhigh ∈ {1, 2, . . . , M}.
In this section, we present results from an empirical study and characterise the optimal global bid for specific cases.
Furthermore, we measure the actual utility improvement that can be obtained when using the global strategy. The results presented here are based on a uniform distribution of the valuations with vmax = 1, and the static local bidder model, but they generalise to the dynamic model and other distributions (not shown due to space limitations). Figure 1 illustrates the optimal global bids and the corresponding expected utility for various M and N = 5, but again the bid curves for different values of M and N follow a very similar pattern.
Here, the bid is normalised by the valuation v to give the bid fraction x = b/v. Note that, when x = 1, a bidder bids its true value.
As shown in Figure 1, for bidders with a relatively low valuation, the optimal strategy is to submit M equal bids at, or very close to, the true value. The optimal bid fraction then gradually decreases for higher valuations. Interestingly, in most cases, placing equal bids is no longer the optimal strategy after the valuation reaches a certain point. A socalled pitchfork bifurcation is then observed and the optimal bids split into two values: a single high bid and M − 1 low ones. This transition is smooth for M = 2, but exhibits an abrupt jump for M ≥ 3. In all experiments, however, we consistently observe that the optimal strategy is always to place a high bid in one auction, and an equal or lower bid in all others. In case of a bifurcation and when the valuation approaches vmax, the optimal high bid goes to the true value and the low bids go to zero.
As illustrated in Figure 1, the utility of a global bidder becomes progressively higher with more auctions. In absolute terms, the improvement is especially high for bidders that have an above average valuation, but not too close to vmax.
The bidders in this range thus benefit most from bidding globally. This is because bidders with very low valuations have a very small chance of winning any auction, whereas bidders with a very high valuation have a high probability of winning a single auction and benefit less from participating in more auctions. In contrast, if we consider the utility relative to bidding in a single auction, this is much higher for bidders with relatively low valuations (this effect cannot be seen clearly in Figure 1 due to the scale). In particular, we notice that a global bidder with a low valuation can improve its utility by up to M times the expected utility of bidding locally. Intuitively, this is because the chance of winning one of the auctions increases by up to a factor M, whereas the increase in the expected cost is negligible. For high valuation buyers, however, the benefit is not that obvious because the chances of winning are relatively high even in case of a single auction.
In this section we extend our analysis of the optimal bidding strategy to sequential auctions. Specifically, the auction process consists of R rounds, and in each round any number of auctions are running simultaneously. Such a combination of sequential and concurrent auctions is very common in practice, especially online5 . It turns out that the analysis for the case of simultaneous auctions is quite general and can be easily extended to include sequential auctions. In the following, the number of simultaneous auctions in round r is denoted by Mr, and the set of bids in that round by Br. As before, the analysis assumes that all other bidders are local and bid in a single auction. Furthermore, we assume that the global bidders have complete knowledge about the number of rounds and the number of auctions in each round.
The expected utility in round r, denoted by Ur, is similar to before (equation 1 in Section 4.1) except that now additional benefit can be obtained from future auctions if the desired item is not won in one of the current set of simultaneous auctions. For convenience, Ur(Br, Mr) is abbreviated to Ur in the following. The expected utility thus becomes: Ur = v · Pr(Br) − bri∈Br bri 0 yg(y)dy + Ur+1 · (1 − Pr(Br)) = Ur+1 + (v − Ur+1)Pr(Br) − bri∈Br bri 0 yg(y)dy, (12) where Pr(Br) = 1 − bri∈Br (1 − G(bri)) is the probability of winning at least one auction in round r. Now, we take the partial derivative of equation 12 in order to find the optimal bid brj for auction j in round r: ∂Us ∂brj = g(brj) ⎡ ⎣(v − Us+1) bri∈Br\{brj } (1 − G(bri)) − brj ⎤ ⎦ (13) 5 Rather than being purely sequential in nature, online auctions also often overlap (i.e., new auctions can start while others are still ongoing). In that case, however, it is optimal to wait and bid in the new auctions only after the outcome of the earlier auctions is known, thereby reducing the chance of unwittingly winning multiple items. Using this strategy, overlapping auctions effectively become sequential and can thus be analysed using the results in this section.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 283 Note that equation 13 is almost identical to equation 5 in Section 4.3, except that the valuation v is now replaced by v−Ur+1. The optimal bidding strategy can thus be found by backward induction (where UR+1 = 0) using the procedure outlined in Section 4.3.
As argued in section 3.2, we expect a real-world market to exhibit a mix of global and local bidders. Whereas so far we assumed a single global bidder, in this section we consider a setting where multiple global bidders interact with one another and with local bidders as well. The analysis of this problem is complex, however, as the optimal bidding strategy of a global bidder depends on the strategy of other global bidders. A typical analytical approach is to find the symmetric Nash equilibrium solution [9, 12], which occurs when all global bidders use the same strategy to produce their bids, and no (global) bidder has any incentive to unilaterally deviate from the chosen strategy. Due to the complexity of the problem, however, here we combine a computational simulation approach with analytical results. The simulation works by iteratively finding the best response to the optimal bidding strategies in the previous iteration. If this should result in a stable outcome (i.e., when the optimal bidding strategies remains unchanged for two subsequent iterations), the solution is by definition a (symmetric) Nash equilibrium.
In order to find a global bidder"s best response, we first need to calculate the expected utility given the global bid B and the strategies of both the other global bidders as well as the local bidders. In the following, let Ng denote the number of other global bidders. Furthermore, let the strategies of the other global bidders be represented by the set of functions βk(v), 1 ≤ k ≤ M, producing a bid for each auction given a bidder"s valuation v. Note that all other global bidders use the same set of functions since we consider symmetric equilibria. However, we assume that the assignment of functions to auctions by each global bidder occurs in a random fashion without replacement (i.e., each function is assigned exactly once by each global bidder). Let Ω denote the set of all possible assignments. Each such assignment ω ∈ Ω is a (M, Ng) matrix, where each entry ωi,j identifies the function used by global bidder j in auction i. Note that the cardinality of Ω, denoted by |Ω|, is equal to M!Ng . Now, the expected utility is the average expected utility over all possible assignments and is given by: U(B, v) = 1 |Ω| ω∈Ω v ⎛ ⎝1 − bi∈B (1 − ˜Gωi (bi)) ⎞ ⎠ − 1 |Ω| ω∈Ω bi∈B bi 0 y˜gωi (y)dy, (14) where ˜Gωi (b) = G(b) · Ng j=1 b 0 βωi,j (y)f(y)dy denotes the probability of winning auction i, given that each global bidder 1 ≤ j ≤ Ng bids according to the function βωi,j , and ˜gωi (y) = d ˜Gωi (y)/dy. Here, G(b) is the probability of winning an auction with only local bidders as described in Section 4.1, and f(y) is the probability density of the bidder valuations as before.
The simulation works by discretising the space of possible valuations and bids and then finding a best response to an initial set of bidding functions. The best response is found by maximising equation 14 for each discrete valuation, which, in turn, results in a new set of bidding functions. These functions then affect the probabilities of winning in the next iteration for which the new best response strategy is calculated.
This process is then repeated for a fixed number of iterations or until a stable solution has been found6 .
Clearly, due to the large search space, finding the utilitymaximising global bid quickly becomes infeasible as the number of auctions and global bidders increases. Therefore, we reduce the search space by limiting the global bid to two dimensions where a global bidder bids high in one of the auctions and low in all the others7 . This simplification is justified by the results in Section 4.3.1 which show that, for a large number of commonly used distributions, the optimal global bid consist of at most two different values.
The results reported here are based on the following settings.8 In order to emphasize that the valuations are discrete, we use integer values ranging from 1 to 1000. Each valuation occurs with equal probability, equivalent to a uniform valuation distribution in the continuous case. A bidder can select between 300 different equally-spaced bid levels. Thus, a bidder with valuation v can place bids b ∈ {0, v/300, 2v/300, . . . , v}. The local bidders are static and bid their valuation as before. The initial set of functions can play an important role in the experiments. Therefore, to ensure our results are robust, experiments are repeated with different random initial functions.
First, we describe the results with no local bidders. For this case, we find that the simulation does not converge to a stable state. That is, when there is at least one other global bidder, the best response strategy keeps fluctuating, irrespective of the number of iterations and of the initial state. The fluctuations, however, show a distinct pattern and alternate between two states. Figure 2 depicts these two states for NG = 10 and M = 5. The two states vary most when there are at least as many auctions as there are global bidders. In that case, one of the best response states is to bid truthfully in one auction and zero in all others. The best response to that, however, is to bid an equal positive amount close to zero in all auctions; this strategy guarantees at least one object at a very low payment. The best response is then again to bid truthfully in a single auction since this appropriates the object in that particular auction. As a result, there exists no stable solution. The same result is observed when the number of global bidders is less than the number of auctions. This oc6 This approach is similar to an alternating-move bestresponse process with pure strategies [4], although here we consider symmetric strategies within a setting where an opponent"s best response depends on its valuation. 7 Note that the number of possible allocations still increases with the number of auctions and global bids. However, by merging all utility-equivalent permutations, we significantly increase computation speed, allowing experiments with relatively large numbers of auctions and bidders to be performed (e.g., a single iteration with 50 auctions and 10 global bidders takes roughly 30 seconds on a 3.00 Ghz PC). 8 We also performed experiments with different precision, other valuation distributions, and dynamic local bidders. We find that the prinicipal conclusions generalise to these different settings, and therefore we omit the results to avoid repetitiveness. 284 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 200 400 600 800 1000 200 400 600 800 1000 valuation (v) bid(b) state 1 state 2 Figure 2: The two states of the best response strategy for M = 5 and Ng = 10 without local bidders. 5 10 15 0 1 2 3 4 x 10 4 number of static local bidders variance Ng = 5 Ng = 10 Ng = 15 Figure 3: The variance of the best response strategy over 10 iterations and 10 experiments with different initial settings and M = 5. The errorbars show the (small) standard deviations. curs since global bidders randomise over auctions, and thus they cannot coordinate and choose to bid high in different auctions.
As shown in Figure 2, a similar fluctuation is observed when the number of global bidders increases relative to the number of auctions. However, the bids in the equal-bid state (state 2 in Figure 2), as well as the low bids of the other state, increase. Moreover, if the number of global bidders is increased even further, a bifurcation occurs in the equal-bid state similar to the case without local bidders.
We now consider the best response strategies when both local and global bidders participate and each auction contains the same number of local bidders. To this end, Figure 3 shows the average variance of the best response strategies.
This is measured as the variance of an actual best-response bid over different iterations, and then taking the average over the discrete bidder valuations. Here, the variance is a gauge for the amount of fluctuation and thus the instability of the strategy. As can be seen from this figure, local bidders have a large stabilising effect on the global bidder strategies. As a result, the best response strategy approximates a pure symmetric Nash equilibrium. We note that the results converge after only a few iterations.
The results show that the principal conclusions in the case of a single global bidder carry over to the case of multiple global bidders. That is, the optimal strategy is to bid positive in all auctions (as long as there are at least as many bidders as auctions). Furthermore, a similar bifurcation point is observed. These results are very robust to changes to the auction settings and the parameters of the simulation.
To conclude, even though a theoretical analysis proves difficult in case of several global bidders, we can approximate a (symmetric) Nash equilibrium for specific settings using a discrete simulation in case the system consists of both local and global bidders. Thus, our simulation can be used as a tool to predict the market equilibrium and to find the optimal bidding strategy for practical settings where we expect a combination of local and global bidders.
Efficiency is an important system-wide property since it characterises to what extent the market maximises social welfare (i.e. the sum of utilities of all agents in the market). To this end, in this section we study the efficiency of markets with either static or dynamic local bidders, and the impact that a global bidder has on the efficiency in these markets.
Specifically, efficiency in this context is maximised when the bidders with the M highest valuations in the entire market obtain a single item each. More formally, we define the efficiency of an allocation as: Definition 1 Efficiency of Allocation. The efficiency ηK of an allocation K is the obtained social welfare proportional to the maximum social welfare that can be achieved in the market and is given by: ηK = NT i=1 vi(K) NT i=1 vi(K∗) , (15) where K∗ = arg maxK∈K NT i=1 vi(K) is an efficient allocation, K is the set of all possible allocations, vi(K) is bidder i"s utility for the allocation K ∈ K, and NT is the total number of bidders participating across all auctions (including any global bidders).
Now, in order to measure the efficiency of the market and the impact of a global bidder, we run simulations for the markets with the different types of local bidders. The experiments are carried out as follows. Each bidder"s valuation is drawn from a uniform distribution with support [0, 1]. The local bidders bid their true valuations, whereas the global bidder bids optimally in each auction as described in Section 4.3. The experiments are repeated 5000 times for each run to obtain an accurate mean value, and the final average results and standard deviations are taken over 10 runs in order to get statistically significant results.
The results of these experiments are shown in Figure 4.
Note that a degree of inefficiency is inherent to a multiauction market with only local bidders [2].9 For example, if there are two auctions selling one item each, and the two bidders with the highest valuations both bid locally in the same auction, then the bidder with the second-highest value does not obtain the good. Thus, the allocation of items to bidders is inefficient. As can be observed from Figure 4, however, the efficiency increases when N becomes larger. This is because the differences between the bidders with the highest valuations become smaller, thereby decreasing the loss of efficiency.
Furthermore, Figure 4 shows that the presence of a global bidder has a slightly positive effect on the efficiency in case the local bidders are static. In the case of dynamic bidders, however, the effect of a global bidder depends on the number of sellers. If M is low (i.e., for M = 2), a global bidder significantly increases the efficiency, especially for low values of 9 Trivial exceptions are when either M = 1 or N = 1 and bidders are static, since the market is then completely efficient without a global bidder.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 285 2 4 6 8 10 12
1 1 3 4 5 6 7 8 2 M Local Bidders Global Bidder 2 Dynamic No 2 Dynamic Yes 2 Static No 2 Static Yes 6 Dynamic No 6 Dynamic Yes 6 Static No 6 Static Yes 2 1 3 4 5 6 7 8 (average) number of local bidders (N) efficiency(ηK) Figure 4: Average efficiency for different market settings as shown in the legend. The error-bars indicate the standard deviation over the 10 runs.
N. For M = 6, on the other hand, the presence of a global bidder has a negative effect on the efficiency (this effect becomes even more pronounced for higher values of M). This result is explained as follows. The introduction of a global bidder potentially leads to a decrease of efficiency since this bidder can unwittingly win more than one item. However, as the number of local bidders increase, this is less likely to happen. Rather, since the global bidder increases the number of bidders, its presence makes an overall positive (albeit small) contribution in case of static bidders. In a market with dynamic bidders, however, the market efficiency depends on two other factors. On the one hand, the efficiency increases since items no longer remain unsold (this situation can occur in the dynamic model when no bidder turns up at an auction). On the other hand, as a result of the uncertainty concerning the actual number of bidders, a global bidder is more likely to win multiple items (we confirmed this analytically). As M increases, the first effect becomes negligible whereas the second one becomes more prominent, reducing the efficiency on average.
To conclude, the impact of a global bidder on the efficiency clearly depends on the information that is available. In case of static local bidders, the number of bidders is known and the global bidder can bid more accurately. In case of uncertainty, however, the global bidder is more likely to win more than one item, decreasing the overall efficiency.
In this paper, we derive utility-maximising strategies for bidding in multiple, simultaneous second-price auctions. We first analyse the case where a single global bidder bids in all auctions, whereas all other bidders are local and bid in a single auction. For this setting, we find the counter-intuitive result that it is optimal to place non-zero bids in all auctions that sell the desired item, even when a bidder only requires a single item and derives no additional benefit from having more. Thus, a potential buyer can achieve considerable benefit by participating in multiple auctions and employing an optimal bidding strategy. For a number of common valuation distributions, we show analytically that the problem of finding optimal bids reduces to two dimensions. This considerably simplifies the original optimisation problem and can thus be used in practice to compute the optimal bids for any number of auctions.
Furthermore, we investigate a setting with multiple global bidders by combining analytical solutions with a simulation approach. We find that a global bidder"s strategy does not stabilise when only global bidders are present in the market, but only converges when there are local bidders as well. We argue, however, that real-world markets are likely to contain both local and global bidders. The converged results are then very similar to the setting with a single global bidder, and we find that a bidder benefits by bidding optimally in multiple auctions. For the more complex setting with multiple global bidders, the simulation can thus be used to find these bids for specific cases.
Finally, we compare the efficiency of a market with multiple concurrent auctions with and without a global bidder. We show that, if the bidder can accurately predict the number of local bidders in each auction, the efficiency slightly increases.
In contrast, if there is much uncertainty, the efficiency significantly diminishes as the number of auctions increases due to the increased probability that a global bidder wins more than two items. These results show that the way in which the efficiency, and thus social welfare, is affected by a global bidder depends on the information that is available to that global bidder.
In future work, we intend to extend the results to imperfect substitutes (i.e., when a global bidder gains from winning additional items), and to settings where the auctions are no longer identical. The latter arises, for example, when the number of (average) local bidders differs per auction or the auctions have different settings for parameters such as the reserve price.

Social choice theory can serve as an appropriate foundation upon which to build multiagent applications. There is a rich literature on the subject of voting1 from political science, mathematics, and economics, with important theoretical results, and builders of automated agents can benefit from this work as they engineer systems that reach group consensus.
Interest in the theory of economics and social choice has in fact become widespread throughout computer science, because it is recognized as having direct implications on the building of systems comprised of multiple automated agents [16, 4, 22, 17, 14, 8, 15].
What distinguishes computer science work in these areas is its concern for computational issues: how are results arrived at (e.g., equilibrium points)? What is the complexity of the process? Can complexity be used to guard against unwanted phenomena? Does complexity of computation prevent realistic implementation of a technique?
The practical applications of voting among automated agents are already widespread. Ghosh et al. [6] built a movie recommendation system; a user"s preferences were represented as agents, and movies to be suggested were selected through agent voting.
Candidates in virtual elections have also been beliefs, joint plans [5], and schedules [7]. In fact, to see the generality of the (automated) voting scenario, consider modern web searching. One of the most massive preference aggregation schemes in existence is Google"s PageRank algorithm, which can be viewed as a vote among indexed web pages on candidates determined by a user-input search string; winners are ranked (Tennenholtz and Altman [21] consider the axiomatic foundations of ranking systems such as this).
In this paper, we consider a topic that has been less studied in the context of automated agent voting, namely power indices. A power index is a measure of the power that a subgroup, or equivalently a voter in a weighted voting environment, has over decisions of a larger group. The Banzhaf power index is one of the most popular measures of voting power, and although it has been used primarily for measuring power in weighted voting games, it is well-defined for any simple coalitional game.
We look at some computational aspects of the Banzhaf power index in a specific environment, namely a network flow game. In this game, a coalition of agents wins if it can send a flow of size k from a source vertex s to a target vertex t, with the relative power of each edge reflecting its significance in allowing such a flow. We show that calculating the Banzhaf power index of each agent in this general network flow domain is #P-complete. We also show that for some restricted network flow domains (specifically, of con1 We use the term in its intuitive sense here, but in the social choice literature, preference aggregation and voting are basically synonymous. 335 978-81-904262-7-5 (RPS) c 2007 IFAAMAS nectivity games on bounded layer graphs), there does exist a polynomial algorithm to calculate the Banzhaf power index of an agent.
There are implications in this scenario to real-world networks; for example, the power index might be used to allocate maintenance resources (a more powerful edge being more critical), in order to maintain a given flow of data between two points.
The paper proceeds as follows. In Section 2 we give some background concerning coalitional games and the Banzhaf power index, and in Section 3 we introduce our specific network flow game.
In Section 4 we discuss the Banzhaf power index in network flow games, presenting our complexity result in the general case. In Section 5 we consider a restricted case of the network flow game, and present results. In Section 6 we discuss related work, and we conclude in Section 7.
A coalitional game is composed of a set of n agents, I, and a function mapping any subset (coalition) of the agents to a real value v : 2I → R. In a simple coalitional game, v only gets values of 0 or 1 (v : 2I → {0, 1}). We say a coalition C ⊂ I wins if v(C) = 1, and say it loses if v(C) = 0. We denote the set of all winning coalitions as W(v) = {C ⊂ 2I |v(C) = 1}.
An agent i is a swinger (or pivot) in a winning coalition C if the agent"s removal from that coalition would make it a losing coalition: v(C) = 1, v(C \ {i}) = 0. A swing is a pair < i, S > such that agent i is a swinger in coalition S.
A question that arises in this context is that of measuring the influence a given agent has on the outcome of a simple game. One approach to measuring the power of individual agents in simple coalitional games is the Banzhaf index.
A common interpretation of the power an agent possesses is that of its a priori probability of having a significant role in the game.
Different assumptions about the formation of coalitions, and different definitions of having a significant role, have caused researchers to define different power indices, one of the most prominent of which is the Banzhaf index [1]. This index has been widely used, though primarily for the purpose of measuring individual power in a weighted voting system. However, it can also easily be applied to any simple coalitional game.
The Banzhaf index depends on the number of coalitions in which an agent is a swinger, out of all possible coalitions.2 The Banzhaf index is given by β(v) = (β1(v), ..., βn(v)) where βi(v) = 1 2n−1 S⊂N|i∈S [v(S) − v(S \ {i})].
Different probabilistic models on the way a coalition is formed yield different appropriate power indices [20]. The Banzhaf power index reflects the assumption that the agents are independent in their choices.
Consider a communication network, where it is crucial to be able to send a certain amount of information between two sites. Given limited resources to maintain network links, which edges should get those resources? 2 Banzhaf actually considered the percentage of such coalitions out of all winning coalitions. This is called the normalized Banzhaf index.
We model this problem by considering a network flow game. The game consists of agents in a network flow graph, with a certain source vertex s and target vertex t. Each agent controls one of the graph"s edges, and a coalition of agents controls all the edges its members control. A coalition of agents wins the game if it manages to send a flow of at least k from source s to target t, and loses otherwise.
To ensure that the network is capable of maintaining the desired flow between s and t, we may choose to allocate our limited maintenance resources to the edges according to their impact on allowing this flow. In other words, resources could be devoted to the links whose failure is most likely to cause us to lose the ability to send the required amount of information between the source and target.
Under a reasonable probabilistic model, the Banzhaf index provides us with a measure of the impact each edge has on enabling this amount of information to be sent between the sites, and thus provides a reasonable basis for allocation of scarce maintenance resources.
Formally, a network flow game is defined as follows. The game consists of a network flow graph G =< V, E >, with capacities on the edges c : E → R, a source vertex s, a target vertex t, and a set I of agents, where agent i controls the edge ei. Given a coalition C, which controls the edges EC = {ei|i ∈ C}, we can check whether the coalition allows a flow of k from s to t. We define the simple coalitional game of network flow as the game where the coalition wins if it allows such a flow, and loses otherwise: v(C) = 1 if EC allows a flow of k from s to t; 0 otherwise; A simplified version of the network flow game is the connectivity game; in a connectivity game, a coalition wants to have some path from source to target. More precisely, a connectivity game is a network flow game where each of the edges has identical capacity, c(e) = 1, and the target flow value is k = 1. In such a scenario, the goal of a coalition is to have at least one path from s to t: v(C) = 1 if EC contains a path from s to t; 0 otherwise; Given a network flow game (or a connectivity game), we can compute the power indices of the game. When a coalition of edges is chosen at random, and each coalition is equiprobable, the appropriate index is the Banzhaf index.3 We can use the Banzhaf value of an agent i ∈ I (or the edge it controls, ei), βei (v) = βi(v), to measure its impact on allowing a given flow between s and t.
FLOW GAMES We now define the problem of calculating the Banzhaf index in the network flow game.
DEFINITION 1. NETWORK-FLOW-BANZHAF: We are given a network flow graph G =< V, E > with a source vertex s and a target vertex t, a capacity function c : E → R, and a target flow value k. We consider the network flow game, as defined above in Section 3. We are given an agent i, controlling the edge ei, and are asked to calculate the Banzhaf index for that agent. In the network 3 When each ordering of edges is equiprobable, the appropriate index is the Shapley-Shubik index. 336 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) flow game, let Cei be the set of all subsets of E that contain ei: Cei = {C ⊂ E|ei ∈ C}. In this game, the Banzhaf index of ei is: βi(v) = 1 2|E|−1 E ⊂Cei [v(E ) − v(E \ {ei})].
Let W(Cei ) be the set of winning subsets of edges in Cei , i.e., the subsets E ∈ Cei where a flow of at least k can be sent from s to t using only the edges in E . The Banzhaf index of ei is the proportion of subsets in W(Cei ) where ei is crucial to maintaining the k-flow. All the edge subsets in W(Cei ) contain ei and are winning, but only for some of them, E ∈ W(Cei ), do we have that v(E \ {ei}) = 0 (i.e., E is no longer winning if we remove ei). The Banzhaf index of ei is the proportion of such subsets.
Index in the Network Flow Game We now show that the general case of NETWORK-FLOW-BANZHAF is #P-complete, by a reduction from #MATCHING.
First, we note that NETWORK-FLOW-BANZHAF is in #P. There are several polynomial algorithms to calculate the maximal network flow, so it is easy to check if a certain subset of edges E ⊂ E contains ei and allows a flow of at least k from s to t. It is also easy to check if a flow of at least k is no longer possible when we remove ei from E (again, by running a polynomial algorithm for calculating the maximal flow). The Banzhaf index of ei is exactly the number of such subsets E ⊂ E, so NETWORK-FLOWBANZHAF is in #P. To show that NETWORK-FLOW-BANZHAF is #P-complete, we reduce a #MATCHING problem4 to a NETWORKFLOW-BANZHAF problem.
DEFINITION 2. #MATCHING: We are given a bipartite graph G =< U, V, E >, such that |U| = |V | = n, and are asked to count the number of perfect matchings possible in G.
The reduction is done as follows. From the #MATCHING input, G =< U, V, E >, we build two inputs for the NETWORKFLOW-BANZHAF problem. The difference between the answers obtained from the NETWORK-FLOW-BANZHAF runs is the answer to the #MATCHING problem. Both runs of the NETWORKFLOW-BANZHAF problem are constructed with the same graph G =< V , E >, with the same source vertex s and target vertex t, and with the same edge ef for which to compute the Banzhaf index. They differ only in the target flow value. The first run is with a target flow of k, and the second run is with a target flow of k + .
A choice of subset Ec ⊂ E reflects a possible matching in the original graph. G is a subgraph of the constructed G . We identify an edge in G , e ∈ E , with the same edge in G. This edge indicates a particular match between some vertex u ∈ U and another vertex v ∈ V . Thus, if Ec ⊂ E is a subset of edges in G which contains only edges in the subgraph of G, we identify it with a subset of edges in G, or with some candidate of a matching.
We say Ec ⊂ E matches some vertex v ∈ V , if Ec contains some edge that connects to v, i.e., for some u ∈ U we have (u, v) ∈ Ec. Ec is a possible matching if it does not match a vertex v ∈ V with more than one vertex in U, i.e., there are not two vertices u1 = u2 in U that both (u1, v) ∈ Ec and (u2, v) ∈ Ec. A perfect matching matches all the vertices in V .
If Ec fails to match a vertex in V (the right side of the partition), the maximal possible flow that Ec allows in G is less than k. If it matches all the vertices in V , a flow of k is possible. If it matches 4 This is one of the most well-known #P-complete problems. all the vertices in V , but matches some vertex in V more than once (which means this is not a true matching), a flow of k+ is possible. is chosen so that if a single vertex v ∈ V is unmatched, the maximal possible flow would be less than |V |, even if all the other vertices are matched more than once. In other words, is chosen so that matching several vertices in V more than once can never compensate for not matching some vertex in V , in terms of the maximal possible flow.
Thus, when we check the Banzhaf index of ef when the required flow is at least k, we get the number of subsets E ⊂ E that match all the vertices in V at least once. When we check the Banzhaf index of ef with a required flow of at least k+ , we get the number of subsets E ⊂ E that match all the vertices in V at least once, and match at least one vertex v ∈ V more than once. The difference between the two is exactly the number of perfect matchings in G.
Therefore, if there existed a polynomial algorithm for NETWORKFLOW-BANZHAF, we could use it to build a polynomial algorithm for #MATCHING, so NETWORK-FLOW-BANZHAF is #Pcomplete.
The reduction takes the #MATCHING input, the bipartite graph G =< U, V, E >, where |U| = |V | = k. It then generates a network flow graph G as follows. The graph G is kept as a subgraph of G , and each edge in G is given a capacity of 1. A new source vertex s is added, along with a new vertex t and a new target vertex t. Let = 1 k+1 so that · k < 1. The source s is connected to each of the vertices in U, the left partition of G, with an edge of capacity 1 + . Each of the vertices in V is connected to t with an edge of capacity 1 + . t is connected to t with an edge ef of capacity 1 + .
As mentioned above, we perform two runs of NETWORK-FLOWBANZHAF, both checking the Banzhaf index of the edge ef in the flow network G . We denote the network flow game defined on G with target flow k as v(G ,k). The first run is performed on the game with a target flow of k, v(G ,k), returning the index βef (v(G ,k)).
The second run is performed on the game with a target flow of k + , v(G ,k+ ), returning the index βef (v(G ,k+ )). The number of perfect matchings in G is the difference between the answers in the two runs, βef (v(G ,k)) − βef (v(G ,k+ )). This is proven in Theorem 5.
Figure 1 shows an example of constructing G from G. On the left is the original graph G, and on the right is the constructed network flow graph G .
We now prove that the reduction above is correct. In all of this section, we take the input to the #MATCHING problem to be G =< U, V, E > with |U| = |V | = k, the network flow graph constructed in the reduction to be G =< V , E > with capacities c : E → R as defined in Section 4.3, the edge for which to calculate the Banzhaf index to be ef , and target flow values of k and k + .
PROPOSITION 1. Let Ec ⊂ E be a subset of edges that lacks one or more edges of the following:
We call such a subset a missing subset. The maximal flow between s and t using only the edges in the missing subset Ec is less than k.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 337 Figure 1: Reducing #MATCHING to NETWORK-FLOW-BANZHAF PROOF. The graph is a layer graph, with s being the vertex in the first layer, U the vertices in the second layer, V the vertices in the third, t the vertex in the fourth, and t in the fifth. Edges in G only go between consecutive layers. The maximal flow in a layer graph is limited by the total capacity of the edges between every two consecutive layers. If any of the edges between s and U is missing, the flow is limited by (|V | − 1)(1 + ) < k. If any of the edges between V and t is missing, the flow is also limited by (|V | − 1)(1 + ) < k. If the edge ef is missing, there are no edges going to the last layer, and the maximal flow is 0.
Since such missing subsets of edges do not affect the Banzhaf index of ef (they add 0 to the sum), from now on we will consider only non-missing subsets. As explained in Section 4.2, we identify the edges in G that were copied from G (the edges between U and V in G ) with their counterparts in G. Each such edge (u, v) ∈ E represents a match between u and v in G. Ec is a perfect matching if it matches every vertex u to a single vertex v and vice versa.
PROPOSITION 2. Let Ec ⊂ E be a subset of edges that fails to match some vertex v ∈ V . The maximal flow between s and t using only the edges in the missing subset Ec is less than k. We call such a set sub-matching, and it is not a perfect matching.
PROOF. If Ec fails to match some vertex v ∈ V , the maximal flow that can reach the vertices in the V layer is (1+ )(k−1) < k, so this is also the maximal flow that can reach t.
PROPOSITION 3. Let Ec ⊂ E be a subset of edges that is a perfect matching in G. Then the maximal flow between s and t using only the edges in Ec is exactly k.
PROOF. A flow of k is possible. We send a flow of 1 from s to each of the vertices in U, send a flow of 1 from each vertex u ∈ U to its match v ∈ V , and send a flow of 1 from each v ∈ V to t . t gets a total flow of exactly k, and sends it to t. A flow of more than k is not possible since there are exactly k edges of capacity 1 between the U layer and the V layer, and the maximal flow is limited by the total capacity of the edges between these two consecutive layers.
PROPOSITION 4. Let Ec ⊂ E be a subset of edges that contains a perfect matching M ⊂ E in G and at least one more edge ex between some vertex ua ∈ U and va ∈ V . Then the maximal flow between s and t using only the edges in Ec is at least k+ . We call such a set a super-matching, and it is not a perfect matching.
PROOF. A flow of k is possible, by using the edges of the perfect match as in Proposition 3. We send a flow of 1 from s to each of the vertices in U, send a flow of 1 from each vertex u ∈ U to its match v ∈ V , and send a flow of 1 from each v ∈ V to t . t gets a total flow of exactly k, and sends it to t. After using the edges of the perfect matching, we send a flow of from s to ua (this is possible since the capacity of the edge (s, ua) is 1 + and we have only used up 1). We then send a flow of from ua to va. This is possible since we have not used this edge at all-it is the edge which is not a part of the perfect matching. We then send a flow of from va to t . Again, this is possible since we have used 1 out of the total capacity of 1 + which that edge has. Now t gets a total flow of k + , and sends it all to t, so we have achieved a total flow of k + . Thus, the maximal possible flow is at least k + .
THEOREM 5. Consider a #MATCHING instance G =< U, V, E > reduced to a BANZHAF-NETWORK-FLOW instance G as explained in Section 4.3. Let v(G ,k) be the network flow game defined on G with target flow k, and v(G ,k+ ) be the game defined with a target flow of k+ . Let the resulting index of the first run be βef (v(G ,k)), and βef (v(G ,k+ )) be the resulting index of the second run. Then the number of perfect matchings in G is the difference between the answers in the two runs, βef (v(G ,k)) − βef (v(G ,k+ )).
PROOF. Consider the game v(G ,k). According to Proposition 1, in this game, the Banzhaf index of Ef does not count missing subsets Ec ∈ E , since they are losing in this game. According to Proposition 2, it does not count subsets Ec ∈ E that are submatchings, since they are also losing. According to Proposition 3, it adds 1 to the count for each perfect matching, since such subsets allow a flow of k and are winning. According to Proposition 3, it adds 1 to the count for each super-matching, since such subsets allow a flow of k (and more than k) and are winning.
Consider the game v(G ,k+ ). Again, according to Proposition 1, in this game the Banzhaf index of Ef does not count missing subsets Ec ∈ E , since they are losing in this game. According to Proposition 2, it does not count subsets Ec ∈ E that are submatchings, since they are also losing. According to Proposition 3, it adds 0 to the count for each perfect matching, since such subsets allow a flow of k but not k + , and are thus losing. According to Proposition 3, it adds 1 to the count for each super-matching, since such subsets allow a flow of k + and are winning.
Thus the difference between the two indices, βef (v(G ,k)) − βef (v(G ,k+ )), is exactly the number of perfect matchings in G.
We have reduced a #MATCHING problem to a NETWORKFLOW-BANZHAF problem. This means that given a polynomial 338 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) algorithm to calculate the Banzhaf index of an agent in a general network flow game, we can build an algorithm to solve the #MATCHING problem. Thus, the problem of calculating the Banzhaf index of agents in general network flow games is also #P-complete.
IN BOUNDED LAYER GRAPH CONNECTIVITY GAMES We here present a polynomial algorithm to calculate the Banzhaf index of an edge in a connectivity game, where the network is a bounded layer graph. This positive result indicates that for some restricted domains of network flow games, it is possible to calculate the Banzhaf index in a reasonable amount of time.
DEFINITION 3. A layer graph is a graph G =< V, E >, with source vertex s and target vertex t, where the vertices of the graph are partitioned into n + 1 layers, L0 = {s}, L1, ..., Ln = {t}.
The edges run only between consecutive layers.
DEFINITION 4. A c-bounded layer graph is a layer graph where the number of vertices in each layer is bounded by some constant number c.
Although there is no limit on the number of layers in a bounded layer graph, the structure of such graphs makes it possible to calculate the Banzhaf index of edges in connectivity games on such graphs. The algorithm provided below is indeed polynomial in the number of vertices given that the network is a c-bounded layer graph. However, there is a constant factor to the running time, which is exponential in c. Therefore, this method is only tractable for graphs where the bound c is small. Bounded layer graphs may occur in networks when the nodes are located in several ordered segments, where nodes can be connected only between consecutive segments.
Let v be a vertex in layer Li. We say an edge e occurs before v if it connects two vertices in v"s layer or a previous layer: e = (u, w) connects vertex u ∈ Lj to vertex w ∈ Lj+1 and j + 1 ≤ i. Let Predv ⊂ E be the subset of edges that occur before v. Consider a subset of these edges, E ⊂ Predv. E may contain a path from s to v, or it may not. We define Pv as the number of subsets E ⊂ Predv that contain a path from s to v.
Similarly, let Vi ∈ V be the subset of all the vertices in the same layer Li. Let PredVi ⊂ E be the subset of edges that occur before Vi (all the vertices in Vi are in the same layer, so any edge that occurs before some v ∈ Vi occurs before any other vertex w ∈ Vi).
Consider a subset of these edges, E ⊂ PredV . Let Vi(E ) be the subset of vertices in Vi that are reachable from s using only the edges in E : Vi(E ) = {v ∈ Vi|E contains a path from s to v}.
We say E ∈ PredV connects exactly the vertices in Si ⊂ Vi if all the vertices in Si are reachable from s using the edges in E but no other vertices in Vi are reachable from s using E , so Vi(E ) = Si.
Let V ⊂ Vi be a subset of the vertices in layer Li. We define PV as the number of subsets E ⊂ PredV that connect exactly the vertices in V : PV = |{E ⊂ PredV |Vi(E ) = V }|.
LEMMA 1. Let S1, S2 ⊂ Vi where S1 = S2 be two different subsets of vertices in the same layer. Let E , E ⊂ PredVi be two sets of edge subsets, so that E connects exactly the vertices in S1 and E connects exactly the vertices in S2: Vi(E ) = S1 and Vi(E ) = S2. Then E and E do not contain the same edges: E = E .
PROOF. If E = E then both sets of edges allow the same paths from s, so Vi(E ) = Vi(E ).
Let Si ⊂ Vi be a subset of vertices in layer Li. Let Ei ⊂ E be the set of edges between the vertices in layer Li and layer Li+1. Let E ⊂ Ei be some subset of these edges. We denote by Dests(Si, E) the set of vertices in layer Li+1 that are connected to some vertex in Si by an edge in E: Dests(Si, E) = {v ∈ Vi+1|there exists some w ∈ Si and some e ∈ E that e = (w, v)}.
Let Si ⊂ Vi be a subset of vertices in Li and E ⊂ Ei be some subset of the edges between layer Li and layer Li+1. PSi counts the number of edge subsets in PredVi that connect exactly the vertices in Si. Consider such a subset E counted in PSi . E ∪ E is a subset of edges in PredVi+1 that connects exactly to Dest(Si, E).
According to Lemma 1, if we iterate over the different Si"s in layer Li, the PSi "s count different subsets of edges, and thus every expansion using the edges in E is also different.
Algorithm 1 calculates Pt. It iterates through the layers, and updates the data for the next layer given the data for the current layer. For each layer Li and every subset of edges in that layer Si ⊂ Vi, it calculates PSi . It does so using the values calculated in the previous layer. The algorithm considers every subset of possible vertices in the current layer, and every possible subset of expanding edges to the next layer, and updates the value of the appropriate subset in the next layer.
Algorithm 1 1: procedure CONNECTING-EXACTLY-SUBSETS(G, v) 2: P{s} ← 1 Initialization 3: for all other subsets of vertices S do Initialization 4: PS ← 0 5: end for 6: for i ← 0 to n − 1 do Iterate through layers 7: for all vertex subsets Si in Li do 8: for all edge subsets E between Li, Li+1 do 9: D ← Dests(Si, E) subset in Li+1 10: PD ← PD + PSi 11: end for 12: end for 13: end for 14: end procedure A c-bounded layer graph contains at most c vertices in each layer, so for each layer there are at most 2c different subsets of vertices in that layer. There are also at most c2 edges between 2 consecutive layers, and thus at most 2(c2 ) edge subsets between two layers.
If the graph contains k layers, the running time of the algorithm is bounded by k·2c ·2(c2 ) . Since c is a constant, this is a polynomial algorithm.
Consider the connectivity game on a layer graph G, with a single source vertex s and target vertex t. The Banzhaf index of the edge e is the number of subsets of edges that allow a path between s and t, but do not allow such a path when e is removed (divided by a constant). We can calculate P{t} = P{t}(G) for G using the algorithm to count the number of subsets of edges that allow a path from s to t. We can then remove e from G to obtain the graph G =< V, E \ {e} >, and calculate P{t} = P{t}(G ). The difference P{t}(G) − P{t}(G ) is the number of subsets of edges that contain a path from s to t but no longer contain such a path when e is removed. The Banzhaf index for e is P{t}(G)−P{t}(G ) 2|E|−1 .
Thus, this algorithm allows us to calculate the Banzhaf index on an edge in the connectivity games on bounded layer graphs.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 339
Measuring the power of individual players in coalitional games has been studied for many years. The most popular indices suggested for such measurement are the Banzhaf index [1] and the Shapley-Shubik index [19].
In his seminal paper, Shapley [18] considered coalitional games and the fair allocation of the utility gained by the grand coalition (the coalition of all agents) to its members. The Shapley-Shubik index [19] is the direct application of the Shapley value to simple coalitional games.
The Banzhaf index emerged directly from the study of voting in decision-making bodies. The normalized Banzhaf index measures the proportion of coalitions in which a player is a swinger, out of all winning coalitions. This index is similar to the Banzhaf index discussed in Section 1, and is defined as: βi = βi(v) k∈N βk .
The Banzhaf index was mathematically analyzed in [3], where it was shown that this normalization lacks certain desirable properties, and the more natural Banzhaf index is introduced.
Both the Shapley-Shubik and the Banzhaf indices have been widely studied, and Straffin [20] has shown that each index reflects specific conditions in a voting body. [11] considers these two indices along with several others, and describes the axioms that characterize the different indices.
The naive implementation of an algorithm for calculating the Banzhaf index of an agent i enumerates all coalitions containing i. There are 2n−1 such coalitions, so the performance is exponential in the number of agents. [12] contains a survey of algorithms for calculating power indices of weighted majority games. Deng and Papadimitriou [2] show that computing the Shapley value in weighted majority games is #P-complete, using a reduction from KNAPSACK. Since the Shapley value of any simple game has the same value as its Shapley-Shubik index, this shows that calculating the Shapley-Shubik index in weighted majority games is #Pcomplete.
Matsui and Matsui [13] have shown that calculating both the Banzhaf and Shapley-Shubik indices in weighted voting games is NP-complete.
The problem of computing power indices in simple games depends on the chosen representation of the game. Since the number of possible coalitions is exponential in the number of agents, calculating power indices in time polynomial in the number of agents can only be achieved in specific domains.
In this paper, we have considered the network flow domain, where a coalition of agents must achieve a flow beyond a certain value.
The network flow game we have defined is a simple game. [10, 9] have considered a similar network flow domain, where each agent controls an edge of a network flow graph. However, they introduced a non-simple game, where the value a coalition of agents achieves is the maximal total flow. They have shown that certain families of network flow games and similar games have nonempty cores.
DIRECTIONS We have considered network flow games, where a coalition of agents wins if it manages to send a flow of more than some value k between two vertices. We have assessed the relative power of each agent in this scenario using the Banzhaf index. This power index may be used to decide how to allocate maintenance resources in real-world networks, in order to maximize our ability to maintain a certain flow of information between two sites.
Although the Banzhaf index theoretically allows us to measure the power of the agents in the network flow game, we have shown that the problem of calculating the Banzhaf index in this domain in #P-complete. Despite this discouraging result for the general network flow domain, we have also provided a more encouraging result for a restricted domain. In the case of connectivity games (where it is only required for a coalition to contain a path from the source to the destination) played on bounded layer graphs, it is possible to calculate the Banzhaf index of an agent in polynomial time.
It remains an open problem to find ways to tractably approximate the Banzhaf index in the general network flow domain. It might also be possible to find other useful restricted domains where it is possible to exactly calculate the Banzhaf index. We have only considered the complexity of calculating the Banzhaf index; it remains an open problem to find the complexity of calculating the Shapley-Shubik or other indices in the network flow domain.
Finally, we believe that there are many additional interesting domains other than weighted voting games and network flow games, and it would be worthwhile to investigate the complexity of calculating the Banzhaf index or other power indices in such domains.
This work was partially supported by grant #898/05 from the Israel Science Foundation.
[1] J. F. Banzhaf. Weighted voting doesn"t work: a mathematical analysis. Rutgers Law Review, 19:317-343, 1965. [2] X. Deng and C. H. Papadimitriou. On the complexity of cooperative solution concepts. Math. Oper. Res., 19(2):257-266, 1994. [3] P. Dubey and L. Shapley. Mathematical properties of the Banzhaf power index. Mathematics of Operations Research, 4(2):99-131, 1979. [4] E. Ephrati and J. S. Rosenschein. The Clarke Tax as a consensus mechanism among automated agents. In Proceedings of the Ninth National Conference on Artificial Intelligence, pages 173-178, Anaheim, California, July
[5] E. Ephrati and J. S. Rosenschein. A heuristic technique for multiagent planning. Annals of Mathematics and Artificial Intelligence, 20:13-67, Spring 1997. [6] S. Ghosh, M. Mundhe, K. Hernandez, and S. Sen. Voting for movies: the anatomy of a recommender system. In Proceedings of the Third Annual Conference on Autonomous Agents, pages 434-435, 1999. [7] T. Haynes, S. Sen, N. Arora, and R. Nadella. An automated meeting scheduling system that utilizes user preferences. In Proceedings of the First International Conference on Autonomous Agents, pages 308-315, 1997. [8] E. Hemaspaandra, L. Hemaspaandra, and J. Rothe. Anyone but him: The complexity of precluding an alternative. In Proceedings of the 20th National Conference on Artificial Intelligence, Pittsburgh, July 2005. [9] E. Kalai and E. Zemel. On totally balanced games and games of flow. Discussion Papers 413, Northwestern University,
Center for Mathematical Studies in Economics and Management Science, Jan. 1980. available at http://ideas.repec.org/p/nwu/cmsems/413.html. 340 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) [10] E. Kalai and E. Zemel. Generalized network problems yielding totally balanced games. Operations Research, 30:998-1008, September 1982. [11] A. Laruelle. On the choice of a power index. Papers 99-10,
Valencia - Instituto de Investigaciones Economicas, 1999. [12] Y. Matsui and T. Matsui. A survey of algorithms for calculating power indices of weighted majority games.
Journal of the Operations Research Society of Japan, 43,
[13] Y. Matsui and T. Matsui. NP-completeness for calculating power indices of weighted majority games. Theoretical Computer Science, 263(1-2):305-310, 2001. [14] N. Nisan and A. Ronen. Algorithmic mechanism design.
Games and Economic Behavior, 35:166-196, 2001. [15] A. D. Procaccia and J. S. Rosenschein. Junta distributions and the average-case complexity of manipulating elections.
In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 497-504, Hakodate,
Japan, May 2006. [16] J. S. Rosenschein and M. R. Genesereth. Deals among rational agents. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, pages 91-99, Los Angeles, California, August 1985. [17] T. Sandholm and V. Lesser. Issues in automated negotiation and electronic commerce: Extending the contract net framework. In Proceedings of the First International Conference on Multiagent Systems (ICMAS-95), pages 328-335, San Francisco, 1995. [18] L. S. Shapley. A value for n-person games. Contributions to the Theory of Games, pages 31-40, 1953. [19] L. S. Shapley and M. Shubik. A method for evaluating the distribution of power in a committee system. American Political Science Review, 48:787-792, 1954. [20] P. Straffin. Homogeneity, independence and power indices.

Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product [14] and the availability of more attractive alternatives [17]. However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves (in the guise of formal or informal contracts), or other so-called ‘switching costs" or market barriers [12, 5].
The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market.
While the level of satisfaction with a currently-used product will largely be a function of one"s own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier.
In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. These are complex questions, and our main interest in this paper is to address the simpler problems of investigating (a) the extent to which network structure influences the ways in which information is disseminated across a network of decision makers, (b) the extent to which market barriers affect this dissemination, and (c) the consequent implications for overall system performance, in terms of the proportion of agents who are satisfied, and the speed with which the system moves towards equilibrium, which we term stability.
An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 387 978-81-904262-7-5 (RPS) c 2007 IFAAMAS product-level, or at the level of the entire system; we are particularly interested in the implications of market barriers for the latter two. The model presented here allows for an investigation into the effects of market barriers to be carried out in a complex environment where at every time period each agent in a population must decide which one of a set of products to purchase. These decisions are based on multiattribute information gathered by personal product trials as well as from the referrals of agents. Agents use this gathered information to search for a product that exceeds their satisfaction thresholds on all attributes - so that the agents may be said to be satisficing rather than optimising (e.g. [15]).
Market barriers may act to influence an agent to continue to use a product that is no longer offering satisfactory performance. We allow agents to hold different opinions about the performance of a product, so that as a result a referral from another agent may not lead to a satisfying experience.
Agents therefore adjust their evaluations of the validity of other agents" referrals according to the success of past referrals, and use these evaluations to judge whether or not to make use of any further referrals. The level of satisfaction provided to an agent by a product is itself inherently dynamic, being subject to random fluctuations in product performance as well as a tendency for an agent to discount the performance of a product they have used for a long time - a process akin to habituation.
Much of the work done on word-of-mouth communication in the context of social psychology and marketing research has focused on its forms and determinants, suggesting that word-of-mouth arises in three possible ways: it may be induced by a particular transaction or product experience [11], particularly when that transaction has been an especially good or bad one [1]; it may be solicited from others [10], usually when the task involved is difficult, ambiguous, or new [7]; and it may come about when talk of products and brands arise in the course of informal conversation, particularly when a ‘passion for the subject" is present [4].
Wordof-mouth becomes more influential when the source of the communication is credible, with credibility decisions based largely on one or a combination of evaluations of professional qualification, informal training, social distance [7], and similarity of views and experiences [3].
The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. The analytical work in [8] investigates the conditions under which word-of-mouth leads to conformity in behaviour and the adoption of socially efficient outcomes (e.g. choosing an alternative that is on average better than another), finding that conformity of behaviour arises when agents are exposed to word-of-mouth communication from only a small number of other agents, but that this conformity may result in socially inefficient outcomes where the tendency toward conformity is so strong that it overwhelms the influence of the superior payoffs provided by the socially efficient outcome. Simulation-based investigations of wordof-mouth [6, 13] have focused on developing strategies for ensuring that a system reaches an equilibrium level where all agents are satisfied, largely by learning about the effectiveness of others" referrals or by varying the degree of inertia in individual behaviour. These studies have found that, given a sufficient number of service providers, honest referrals lead to faster convergence to satisfactory distributions than deceitful ones, and that both forms of word-of-mouth provide better performance than none at all. The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in [6] in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects.
The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. While the formalisation of the idea that satisfaction drives purchase behaviour can be traced back to the work of Fishbein and Ajzen [9] on reasoned choice, nearly all writers, including Fishbein and Ajzen, recognise that this relationship can be thwarted by circumstances (e.g. [17]).
A useful typology of market barriers distinguishes ‘transactional" barriers associated with the monetary cost of changing (e.g. in financial services), ‘learning" barriers associated with deciding to replace well-known existing products, and ‘contractual" barriers imposing legal constraints for the term of the contract [12]. A different typology [5] introduces the additional aspect of ‘relational" barriers arising from personal relationships that may be interwoven with the use of a particular product.
There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. Burnham et al. [5] find that perceived market barriers account for nearly twice the variance in intention to stay with a product than that explained by satisfaction with the product (30% and 16% respectively), and that so-called relational barriers are considerably more influential than either transactional or learning barriers. Further, they find that switching costs are perceived by consumers to exist even in markets which are fluid and where barriers would seem to be weak. Simply put, market barriers appear to play a greater role in what people do than satisfaction; and their presence may be more pervasive than is generally thought.
We use a problem representation in which, at each time period, every agent must decide which one of a set of products to choose. Let A = {ak}k=1...p be the set of agents,
B = {bi}i=1...n be the set of products, and C = {cj }j=1...m be the set of attributes on which the choice decision is to be based i.e. the decision to be made is a multiattribute choice one. Let fj : B → [0, 1] be an increasing function providing the intrinsic performance of a product on attribute j (so that 0 and 1 are the worst- and best-possible performances respectively), and Sij : A × [0, 1] → [0, 1] be a subjective opinion function of agents. The intrinsic performance of 388 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) product i on attribute j is given by fj (bi). However, the subjective opinion of the level of performance (of product i on attribute j) given by agent k is given by sij(ak, fj (bi)).
All subsequent modelling is based on these subjective performance ratings. For the purposes of this paper, each agent belongs to one of three equally-sized groups, with each group possessing its own subjective performance ratings.
We assume that the subjective performance ratings are not known a priori by the agents, and it is their task to discover these ratings by a combination of personal exploration and referral gathering. In order to model this process we introduce the notion of perceived performance ratings at time t, denoted by pij(ak, fj (bi), t). Initially, all perceived performance ratings are set to zero, so that the initial selection of a product is done randomly. Subsequent variation in product performance over time is modelled using two quantities: a random perturbation jkt applied at each purchase occasion ensures that the experience of a particular product can vary over purchase occasions for the same agent, and a habituation discounting factor Hikt tends to decrease the perceived performance of a product over time as boredom creeps in with repeated usage. Our habituation mechanism supposes that habituation builds up with repeated use of a product, and is used to discount the performance of the product.
In most cases i.e. unless the habituation factor is one or extremely close to one, this habituation-based discounting eventually leads to defection, after which the level of habituation dissipates as time passes without the product being used. More formally, once a product i∗ has been chosen by agent k, the subjective level of performance is perceived and pi∗j(ak, fj (b∗ i ), t) is set equal to si∗j(ak, fj (b∗ i ))Hi∗kt + jkt, where jkt is distributed as N(0, σ) and Hi∗kt is an decreasing function of the number of time periods that agent k has been exposed to i∗ .
In evaluating the performance of a product, agents make use of a satisficing framework by comparing the perceived performance of the chosen product with their satisfaction thresholds Γk = {g1k, . . . , gmk}, with 0 ≤ gik ≤ 1. Agent k will be satisfied with a product i∗ selected in time t if pi∗j(ak, fj (b∗ i ), t) ≥ gjk, ∀j.
In designing the mechanism by which agents make their choice decisions, we allow for the possibility that satisfied agents defect from the products that are currently satisfying them. Satisfied agents stay with their current product with probability Pr(stay), with a strategy prohibiting satisfied agents from moving (e.g. [6]) obtained as a special case when Pr(stay) = 1.
A defecting satisfied agent decides on which product to choose by considering all other products for which it has information, either by previous personal exploration or by referrals from other agents. The selection of a new product begins by the agent identifying those products from which he or she expects to gain a satisfactory performance on all attributes i.e. those products for which δik < 0, where δik = maxj [gjk − pij(ak, fj(bi), t)], and selecting a product from this set with selection probabilities proportional to −δik. If no satisfactory product exists (or at least the agent is unaware of any such product) the agent identifies those products that offer at least a minimum level of ‘acceptable" performance γ− k . The minimum level of acceptability is defined as the maximum deviation from his or her aspirations across all attributes that the agent is willing to accept i.e. a product is minimally acceptable if and only if δik < γ− k .
Agents then select a product at random from the set of minimally acceptable products. If the set of minimally acceptable products is empty, agents select a product from the full set of products B at random.
The decision process followed by unsatisfied agents is largely similar to that of defecting satisfied agents, with the exception that at the outset of the decision process agents will chose to explore a new product, chosen at random from the set of remaining products, with probability α. With probability 1 − α, they will use a decision process like the one outlined above for satisfied agents.
In some circumstances market barriers may exist that make switching between products more difficult, particularly where some switching costs are incurred as a result of changing one"s choice of product. When barriers are present, agents do not switch when they become unsatisfied, but rather only when the performance evaluation drops below some critical level i.e. when δik > β, where β > 0 measures the strength of the market barriers. Although in this paper market barriers do not vary over products or time, it is straightforward to allow this to occur by allowing barriers take the general form β = max(β∗ +Δtuse, β∗ ), where β∗ is a barrier to defection that is applied when the product is purchased for the first time (e.g. a contractual agreement), Δ is the increase in barriers that are incurred for every additional time period the product is used for, and β∗ is the maximum possible barrier, and all three quantities are allowed to vary over products i.e. be a function of i.
Each agent is assumed to be connected to qk < p agents i.e. to give and receive information from qk other agents.
The network over which word-of-mouth communication travels is governed by the small-world effect [18], by which networks simultaneously exhibit a high degree of clustering of agents into ‘communities" and a short average path length between any two agents in the network, and preferential attachment [2], by which agents with greater numbers of existing connections are more likely to receive new ones.
This is easily achieved by building a one-dimensional lattice with connections between all agent pairs separated by κ or fewer lattice spacings, and creating a small-world network by choosing at random a small fraction r of the connections in the network and moving one end of each to a new agent, with that new agent chosen with probability proportional to its number of existing connections. This results in a distribution of the number of connections possessed by each agent i.e. a distribution of qk, that is strongly skewed to the right.
In fact, if the construction of the network is slightly modified so that new connections are added with preferential attachment (but no connections are removed), the distribution of qk follows a power-law distribution, but a distribution with a non-zero probability of an agent having less than the modal number of connections seems more realistic in the context of word-of-mouth communication in marketing systems.
When an agent purchases a product, they inform each of the other agents in their circle with probability equal to Pr(spr)k∗ + |δik∗ |, where Pr(spr)k∗ is the basic propensity of agent k∗ to spread word of mouth and δik∗ captures the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 389 extent to which the agent"s most recent experience was satisfying or dissatisfying. Agents are thus more likely to spread word-of-mouth about products that they have just experienced as either very good or very bad. If an agent receives information on the same product from more than one agent, he or she selects the referral of only one of these agents, with selection probabilities proportional to Tt(k∗ , k), the degree to which previous referrals from k∗ to k were successful i.e. resulted in satisfying experiences for agent k. Thus agents have the capacity to learn about the quality of other agents" referrals and use this information to accept or block future referrals. In this paper, we employ a learning condition in which Tt(k∗ , k) is multiplied by a factor of 0.1 following an unsatisfying referral and a factor of 3 following a satisfying referral. The asymmetry in the weighting is similar to that employed in [16], and is motivated by the fact that an unsatisfying referral is likely to be more reliable evidence that a referring agent k∗ does not possess the same subjective preferences as agent k than a positive referral is of indicating the converse.
Other referral process are certainly possible, for example one integrating multiple sources of word-of-mouth rather than choosing only the most-trusted source: our main reason for employing the process described above is simplicity. Integrating different sources considerably complicates the process of learning about the trustworthiness of others, and raises further questions about the precise nature of the integration.
After determining who contacts whom, the actual referral is modelled as a transmittance of information about the perceived level of performance of an experience of product i∗ from the referring agent k∗ to the accepting agent k i.e. pi∗j(ak, fj (bi), t) takes on the value pi∗j(ak∗ , fj(bi), t−1), ∀j, provided that agent k is not currently using i∗ . Information about other products is not transmitted, and an agent will ignore any word-of-mouth about the product he or she is currently using. In effect, the referral creates an expected level of performance in the mind of an accepting agent for the product referred to, which that agent may then use when making choice decision in subsequent time periods using the decision processes outlined in the previous section. Once an agent has personally experienced a product, any expected performance levels suggested by previous referrals are replaced by the experienced (subjective) performance levels sij(ak, fj(bi)) + jkt and Tt(k∗ , k) is adjusted depending on whether the experience was a satisfying one or not.
We examine the behaviour of a system of 200 agents consisting of three groups of 67, 67, and 66 agents respectively.
Agents in each of the three groups have homogeneous subjective opinion functions Sij. Simulations were run for 500 time periods, and twenty repetitions of each condition were used in order to generate aggregate results.
We begin by examining the effect of task difficulty on the ability of various network configurations to converge to a state in which an acceptable proportion of the population are satisfied. In the ‘easy" choice condition, there are 50 products to choose from in the market, evaluated over 4 attributes with all satisfaction thresholds set to 0.5 for all groups. There are therefore on average approximately 3 products that can be expected to satisfy any particular agent. In the ‘hard" choice condition, there are 500 products to choose from in the market, still evaluated over 4 attributes but with all satisfaction thresholds now set to
4 products that can be expected to satisfy any particular agent. Locating a satisfactory product is therefore far more difficult under the ‘hard" condition. The effect of task difficulty is evaluated on three network structures corresponding to r = 1 (random network), r = 0.05 (small-world network), and r = 0 (tight ‘communities" of agents), with results shown in Figure 1 for the case of κ = 3. 0 100 200 300 400 500
Time Proportionsatisfied Easy task r = 1 r = 0.05 r = 0 (a) Proportion of agents satisfied 0 100 200 300 400 500
Time Shareofmarket Easy task r = 1 r = 0.05 r = 0 (b) Market share for leading product Figure 1: Moderating effect of task difficulty on relationship between network structure (r) and system behaviour Given a relatively easy task, the system very quickly i.e. in little over 50 time periods, converges to a state in which just less than 60% of agents are satisfied at any one time.
Furthermore, different network structures have very little influence on results, so that only a single (smoothed) series is given for comparison with the ‘hard" condition. Clearly, there are enough agents independently solving the task i.e. 390 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) finding a satisfying brand, to make the dissemination of information relatively independent of the ways in which connections are made. However, when it is more difficult to locate a satisfying product, the structure of the network becomes integral to the speed at which the system converges to a stable state. Importantly, the overall satisfaction level to which the system converges remains just below 60% regardless of which network structure is used, but convergence is considerably speeded by the random rewiring of even a small proportion of connections. Thus while the random network (r = 1) converges quickest, the small-world network (r = 0.05) also shows a substantial improvement over the tight communities represented by the one-dimensional ring lattice. This effect of the rewiring parameter r is much less pronounced for more highly-connected networks (e.g. κ = 9), which suggests that the degree distribution of a network is a more important determinant of system behaviour than the way in which agents are connected to one another.
Similar results are observed when looking at the market share achieved by the market leading product under each level of choice difficulty: market share is essentially independent of network structure for the easy task, with average share converging quickly to around 35%. Set a more difficult task, the convergence of market share to an approximate long-run equilibrium is in fact fastest for the smallworld network, with the random and tight community networks taking different paths but similar times to reach their equilibrium levels. Also interesting is the finding that equilibrium market shares for the market leader appear to be slightly (of the order of 5-10%) higher when network connections are non-random - the random network seems to suffer more from the effects of habituatation than the other networks as a result of the rapid early adoption of the market leading product.
In the remainder of this paper we focus on the effect of various forms of market barriers on the ability of a system of agents to reach a state of acceptable satisfaction. For simplicity, we concentrate on the smaller set of 50 products i.e. the ‘easy" choice task discussed above, but vary the number of connections that each agent begins with in order to simultaneously investigate the effect of degree distribution on system behaviour. Tables 1 and 2 show the effect of different degree distributions on the equilibrium proportion of agents that are satisfied at any one time, and the equilibrium proportion of agents switching products (moving) in any one time period, under various levels of market barriers constraining their behaviour. In these two tables, equilibrium results have been calculated by averaging over time periods 450 to 500, when the system is in equilibrium or extremely close to equilibrium (Table 3 and 4 make use of all time periods).
No WoM κ = 1 κ = 3 κ = 9 β = 0 0.27 0.44 0.56 0.58 β = 0.05 0.26 0.39 0.50 0.52 β = 0.2 0.14 0.27 0.32 0.34 β = 0.4 0.07 0.17 0.22 0.25 Table 1: Effect of degree distribution and market barriers on proportion of market satisfied No WoM κ = 1 κ = 3 κ = 9 β = 0 0.74 0.51 0.45 0.45 β = 0.05 0.66 0.43 0.38 0.37 β = 0.2 0.41 0.21 0.21 0.21 β = 0.4 0.17 0.09 0.09 0.09 Table 2: Effect of degree distribution and market barriers on proportion of market moving Three aspects are worth noting. Firstly, there is a strong diminishing marginal return of additional connections beyond a small number. The first few connections one makes increases the probability of finding a satisfying product 60% from 0.27 to 0.44 (for the first two contacts), followed by a further increase of roughly 25% to 0.56 for the next four.
In contrast, adding a further 12 contacts improves relative satisfaction levels by less than 4%. Secondly, word-of-mouth communication continues to play an important role in improving the performance of the system even when market barriers are high. In fact, the role may even be more important in constrained conditions, since the relative gains obtained from word-of-mouth are greater the higher market barriers are - just having two contacts more than doubles the aggregate satisfaction level under the most extreme barriers (β = 0.4). Finally, it is clear that the mechanism by which barriers reduce satisfaction is by restricting movement (reflected in the lower proportion of agents moving in any particular column of Tables 1 and 2), but that increases in degree distribution act to increase satisfaction by precisely the same mechanism of reducing movement - this time by reducing the average amount of time required to find a satisfying brand.
Positive referrals Negative referrals κ = 1 κ = 3 κ = 9 κ = 1 κ = 3 κ = 9 β = 0 0.21 0.93 3.27 0.00 0.00 0.00 β = 0.05 0.19 0.85 2.96 0.06 0.19 0.60 β = 0.2 0.13 0.57 2.04 0.30 0.92 2.83 β = 0.4 0.08 0.40 1.49 0.60 1.81 5.44 Table 3: Median number of positive and negative referrals made per agent per time period Perhaps the most interesting effects exerted by market barriers are those exerted over the market shares of leading products. Figure 2 shows the cumulative market share captured by the top three products in the market over time, for all types of market barriers using different degree distributions. Again, two comments can be made. Firstly, in the absence of market barriers, a greater proportion of the market is captured by the market leading products when markets are highly-connected relative to when they are poorlyconnected. This difference can amount to as much as 15%, and is explained by positive feedback within the more highlyconnected networks that serves to increase the probability that, once a set of satisfying products have emerged, one is kept informed about these leading products because at least one of one"s contacts is using it. Secondly, the relatively higher market share enjoyed by market leaders in highly-connected networks is eroded by market barriers. In moving from β = 0 to β = 0.2 to β = 0.4, market leaders collectively lose an absolute share of 15% and 10% under the larger degree distributions κ = 9 and κ = 3 respectively.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 391 0 100 200 300 400 500
Time Shareofmarket κ = 9 κ = 3 κ = 1 No WoM (a) β = 0 0 100 200 300 400 500
Time Shareofmarket κ = 9 κ = 3 κ = 1 No WoM (b) β = 0.05 0 100 200 300 400 500
Time Shareofmarket κ = 9 κ = 3 κ = 1 No WoM (c) β = 0.2 0 100 200 300 400 500
Time Shareofmarket κ = 9 κ = 3 κ = 1 No WoM (d) β = 0.4 Figure 2: Effect of market barriers on the share of the market captured by the leading 3 products In contrast, no change in collective market share is observed when κ = 1, although convergence to equilibrium conditions is slower. It seems reasonable to suggest that increases in negative word-of-mouth, which occurs when an unsatisfied agent is prevented from switching to another product, are particularly damaging to leading products when agents are well-connected, and that under moderate to strong market barriers these effects more than offset any gains achieved by the spread of positive word-of-mouth through the network.
Table 3 displays the number of attempted referrals, both positive and negative, as a function of degree distribution and extent of market barriers, and shows that stronger market barriers act to simultaneously depress positive word-ofmouth communication and increase negative communication from those trapped in unsatisfying product relationships, and that this effect is particularly pronounced for more highly-connected networks. The reduction in the number of positive referrals as market barriers impose increasingly severe constraints is also reflected in Table 4, which shows the median number of product trials each agent makes per time period based on a referral from another agent.
Whereas under few or no barriers agents in a highly-connected network make substantially more reference-based product trials than agents in poorly-connected networks, when barriers are severe both types of network carry only very little positive referral information. This clearly has a relatively greater impact on the highly-connected network, which relies on the spread of positive referral information to achieve higher satisfaction levels. Moreover, this result might be even stronger in reality if agents in poorly-connected networks attempt to compensate for the relative sparcity of connections by making more forceful or persuasive referrals where they do occur. κ = 1 κ = 3 κ = 9 β = 0 0.13 0.27 0.35 β = 0.05 0.11 0.22 0.28 β = 0.2 0.05 0.10 0.14 β = 0.4 0.02 0.05 0.06 Table 4: Median number of referrals leading to a product trial received per agent per time period 392 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components: networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world.
Using the key notions of a limited communication network, a simple learning process, and a satisficing heuristic that may be subject to external constraints, we showed (1) the importance of word-of-mouth communication to both system effectiveness and stability, (2) that the degree distribution of a network is more influential than the way in which agents are connected, but that both are important in more complex environments, (3) that rewiring even a small number of connections to create a small-world network can have dramatic results for the speed of convergence to satisficing distributions and market share allocations, (4) that word-of-mouth continues to be effective when movements between products are constrained by market barriers, and (5) that increases in negative word-of-mouth incurred as a result of market barriers can reduce the market share collectively captured by leading brands, but that this is dependent on the existence of a suitably well-connected network structure.
It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving. In poorly-connected networks, the effect of barriers on market shares is slight. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share. Products with small market share (which, in the context of our simulations, is generally due to the product offering poor performance) are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case.
Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two ‘modules" comprising network structure and local behaviour. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions.
[1] E. Anderson. Customer satisfaction and word-of-mouth. Journal of Service Research, 1(Aug):5-17, 1998. [2] A. Barab´asi. Emergence of scaling in random networks. Science, 286:509-512, 1999. [3] J. Brown and P. Reingen. Social ties and word-of-mouth referral behaviour. Journal of Consumer Research, 14(Dec):350-362, 1987. [4] T. Brown, T. Berry, P. Dacin, and R. Gunst.
Spreading the word: investigating positive word-of-mouth intentions and behaviours in a retailing context. Journal of the Academy of Marketing Sciences, 33(2):123-139, 2005. [5] T. Burnham, J. Frels, and V. Mahajan. Consumer switching costs: A typology, antecedents, and consequences. Journal of the Academy of Marketing Sciences, 31(2):109-126, 2003. [6] T. Candale and S. Sen. Effect of referrals on convergence to satisficing distributions. In Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 347-354. ACM Press, New York, 2005. [7] D. Duhan, S. Johnson, J. Wilcox, and G. Harrell.
Influences on consumer use of word-of-mouth recommendation sources. Journal of the Academy of Marketing Sciences, 25(4):283-295, 1997. [8] G. Ellison and D. Fudenberg. Word-of-mouth communication and social learning. Quarterly Journal of Economics, 110(1):93-125, 1995. [9] M. Fishbein and I. Ajzen. Belief, Attitude, Intention, and Behaviour: An Introduction to the Theory and Research. Addison-Wesley, Reading, 1975. [10] R. Fisher and L. Price. An investigation into the social context of early adoption behaviour. Journal of Consumer Research, 19(Dec):477-486, 1992. [11] S. Keaveney. Customer switching behaviour in service industries: an exploratory study. Journal of Marketing Research, 59(Apr):71-82, 1995. [12] P. Klemperer. Markets with consumer switching costs.
Quarterly Journal of Economics, 102:375-394, 1979. [13] D. McDonald. Recommending collaboration with social networks: a comparative evaluation. In CHI "03: Proceedings of the SIGCHI conference on human factors in computing systems, pages 593-600. ACM Press, New York, 2003. [14] R. Oliver. A cognitive model of the antecedents and consequences of satisfaction decisions. Journal of Marketing, 17:460-469, 1980. [15] H. Simon. Administrative Behaviour. The Free Press,
New York, 1976.

The entire movement of agent paradigm was spawned, at least in part, by the perceived importance of fostering human-like adjustable autonomy. Human-centered multiagent teamwork has thus attracted increasing attentions in multi-agent systems field [2, 10, 4]. Humans and autonomous systems (agents) are generally thought to be complementary: while humans are limited by their cognitive capacity in information processing, they are superior in spatial, heuristic, and analogical reasoning; autonomous systems can continuously learn expertise and tacit problem-solving knowledge from humans to improve system performance. In short, humans and agents can team together to achieve better performance, given that they could establish certain mutual awareness to coordinate their mixed-initiative activities.
However, the foundation of human-agent collaboration keeps being challenged because of nonrealistic modeling of mutual awareness of the state of affairs. In particular, few researchers look beyond to assess the principles of modeling shared mental constructs between a human and his/her assisting agent. Moreover, human-agent relationships can go beyond partners to teams. Many informational processing limitations of individuals can be alleviated by having a group perform tasks. Although groups also can create additional costs centered on communication, resolution of conflict, and social acceptance, it is suggested that such limitations can be overcome if people have shared cognitive structures for interpreting task and social requirements [8]. Therefore, there is a clear demand for investigations to broaden and deepen our understanding on the principles of shared mental modeling among members of a mixed human-agent team.
There are lines of research on multi-agent teamwork, both theoretically and empirically. For instance, Joint Intention [3] and SharedPlans [5] are two theoretical frameworks for specifying agent collaborations. One of the drawbacks is that, although both have a deep philosophical and cognitive root, they do not accommodate the modeling of human team members. Cognitive studies suggested that teams which have shared mental models are expected to have common expectations of the task and team, which allow them to predict the behavior and resource needs of team members more accurately [14, 6]. Cannon-Bowers et al. [14] explicitly argue that team members should hold compatible models that lead to common expectations. We agree on this and believe that the establishment of shared expectations among human and agent team members is a critical step to advance human-centered teamwork research.
It has to be noted that the concept of shared expectation can broadly include role assignment and its dynamics, teamwork schemas and progresses, communication patterns and intentions, etc. While the long-term goal of our research is to understand how shared cognitive structures can enhance human-agent team performance, the specific objective of the work reported here is to develop a computational cognitive 395 978-81-904262-7-5 (RPS) c 2007 IFAAMAS capacity model to facilitate the establishment of shared expectations. In particular, we argue that to favor humanagent collaboration, an agent system should be designed to allow the estimation and prediction of human teammates" (relative) cognitive loads, and use that to offer improvised, unintrusive help. Ideally, being able to predict the cognitive/processing capacity curves of teammates could allow a team member to help the right party at the right time, avoiding unbalanced work/cognitive loads among the team.
The last point is on the modeling itself. Although an agent"s cognitive model of its human peer is not necessarily to be descriptively accurate, having at least a realistic model can be beneficial in offering unintrusive help, bias reduction, as well as trustable and self-adjustable autonomy. For example, although humans" use of cognitive simplification mechanisms (e.g., heuristics) does not always lead to errors in judgment, it can lead to predictable biases in responses [8]. It is feasible to develop agents as cognitive aids to alleviate humans" biases, as long as an agent can be trained to obtain a model of a human"s cognitive inclination. With a realistic human cognitive model, an agent can also better adjust its automation level. When its human peer is becoming overloaded, an agent can take over resource-consuming tasks, shifting the human"s limited cognitive resources to tasks where a human"s role is indispensable. When its human peer is underloaded, an agent can take the chance to observe the human"s operations to refine its cognitive model of the human. Many studies have documented that human choices and behaviors do not agree with predictions from rational models. If agents could make recommendations in ways that humans appreciate, it would be easier to establish trust relationships between agents and humans; this in turn, will encourage humans" automation uses.
The rest of the paper is organized as follows. In Section 2 we review cognitive load theories and measurements. A HMM-based cognitive load model is given in Section 3 to support resource-bounded teamwork among human-agentpairs. Section 4 describes the key concept shared belief map as implemented in SMMall, and Section 5 reports the experiments for evaluating the cognitive models and their impacts on the evolving of shared mental models.
People are information processors. Most cognitive scientists [8] believe that human information-processing system consists of an executive component and three main information stores: (a) sensory store, which receives and retains information for one second or so; (b) working (or shortterm) memory, which refers to the limited capacity to hold (approximately seven elements at any one time [9]), retain (for several seconds), and manipulate (two or three information elements simultaneously) information; and (c) longterm memory, which has virtually unlimited capacity [1] and contains a huge amount of accumulated knowledge organized as schemata. Cognitive load studies are, by and large, concerned about working memory capacity and how to circumvent its limitations in human problem-solving activities such as learning and decision making.
According to the cognitive load theory [11], cognitive load is defined as a multidimensional construct representing the load that a particular task imposes on the performer. It has a causal dimension including causal factors that can be characteristics of the subject (e.g. expertise level), the task (e.g. task complexity, time pressure), the environment (e.g. noise), and their mutual relations. It also has an assessment dimension reflecting the measurable concepts of mental load (imposed exclusively by the task and environmental demands), mental effort (the cognitive capacity actually allocated to the task), and performance.
Lang"s information-processing model [7] consists of three major processes: encoding, storage, and retrieval. The encoding process selectively maps messages in sensory stores that are relevant to a person"s goals into working memory; the storage process consolidates the newly encoded information into chunks, and form associations and schema to facilitate subsequent recalls; the retrieval process searches the associated memory network for a specific element/schema and reactivates it into working memory. The model suggests that processing resources (cognitive capacity) are independently allocated to the three processes. In addition, working memory is used both for holding and for processing information [1]. Due to limited capacity, when greater effort is required to process information, less capacity remains for the storage of information. Hence, the allocation of the limited cognitive resources has to be balanced in order to enhance human performance. This comes to the issue of measuring cognitive load, which has proven difficult for cognitive scientists.
Cognitive load can be assessed by measuring mental load, mental effort, and performance using rating scales, psychophysiological (e.g. measures of heart activity, brain activity, eye activity), and secondary task techniques [12].
Selfratings may appear questionable and restricted, especially when instantaneous load needs to be measured over time.
Although physiological measures are sometimes highly sensitive for tracking fluctuating levels of cognitive load, costs and work place conditions often favor task- and performancebased techniques, which involve the measure of a secondary task as well as the primary task under consideration.
Secondary task techniques are based on the assumption that performance on a secondary task reflects the level of cognitive load imposed by a primary task [15]. From the resource allocation perspective, assuming a fixed cognitive capacity, any increase in cognitive resources required by the primary task must inevitably decrease resources available for the secondary task [7]. Consequently, performance in a secondary task deteriorates as the difficulty or priority of the primary task increases. The level of cognitive load can thus be manifested by the secondary task performance: the subject is getting overloaded if the secondary task performance drops.
A secondary task can be as simple as detecting a visual or auditory signal but requires sustained attention. Its performance can be measured in terms of reaction time, accuracy, and error rate. However, one important drawback of secondary task performance, as noted by Paas [12], is that it can interfere considerably with the primary task (competing for limited capacity), especially when the primary task is complex. To better understand and measure cognitive load,
Xie and Salvendy [16] introduced a conceptual framework, which distinguishes instantaneous load, peak load, accumulated load, average load, and overall load. It seems that the notation of instantaneous load, which represents the dynamics of cognitive load over time, is especially useful for monitoring the fluctuation trend so that free capacity can be exploited at the most appropriate time to enhance the overall performance in human-agent collaborations. 396 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Agent n Human Human-Agent Pair n Agent 1 Human Human-Agent Pair 1 Teammates Agent Processing Model Agent Comm Model Human Partner HAI Agent Processing Model Agent Comm Model Human Partner HAI Teammates Figure 1: Human-centered teamwork model.
People are limited information processors, and so are intelligent agent systems; this is especially true when they act under hard or soft timing constraints imposed by the domain problems. In respect to our goal to build realistic expectations among teammates, we take two important steps.
First, agents are resource-bounded; their processing capacity is limited by computing resources, inference knowledge, concurrent tasking capability, etc. We withdraw the assumption that an agent knows all the information/intentions communicated from other teammates. Instead, we contend that due to limited processing capacity, an agent may only have opportunities to process (make sense of) a portion of the incoming information, with the rest ignored. Taking this approach will largely change the way in which an agent views (models) the involvement and cooperativeness of its teammates in a team activity. In other words, the establishment of shared mental models regarding team members" beliefs, intentions, and responsibilities can no longer rely on inter-agent communication only. This being said, we are not dropping the assumption that teammates are trustable.
We still stick to this, only that team members cannot overtrust each other; an agent has to consider the possibility that its information being shared with others might not be as effective as expected due to the recipients" limited processing capacities. Second, human teammates are bounded by their cognitive capacities. As far as we know, the research reported here is the first attempt in the area of humancentered multi-agent teamwork that really considers building and using human"s cognitive load model to facilitate teamwork involving both humans and agents.
We use Hi, Ai to denote Human-Agent-Pair (HAP) i.
An intelligent agent being a cognitive aid, it is desirable that the model of its human partner implemented within the agent is cognitively-acceptable, if not descriptively accurate. Of course, building a cognitive load model that is cognitively-acceptable is not trivial; there exist a variety of cognitive load theories and different measuring techniques.
We here choose to focus on the performance variables of secondary tasks, given the ample evidence supporting secondary task performance as a highly sensitive and reliable technique for measuring human"s cognitive load [12]. It"s worth noting that just for the purpose of estimating a human subject"s cognitive load, any artificial task (e.g, pressing a button in response to unpredictable stimuli) can be used as a secondary task to force the subject to go through.
However, in a realistic application, we have to make sure that the selected secondary task interacts with the primary task in meaningful ways, which is not easy and often depends on the domain problem at hand. For example, in the experiment below, we used the number of newly available information correctly recalled as the secondary task, and the effective0 1 2 3 4 negligibly slightly fairly heavily overly
0 1 2 3 4 5 6 7 8 ≥ 9 B = 0 1 2 3 4 ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ 0 0 0 0 0 0.02 0.03 0.05 0.1 0.8 0 0 0 0 0 0.05 0.05 0.1 0.7 0.1 0 0 0 0 0.01 0.02 0.45 0.4 0.1 0.02
⎤ ⎥ ⎥ ⎥ ⎥ ⎦ Figure 2: A HMM Cognitive Load Model. ness of information sharing as the primary task. This is realistic to intelligence workers because in time stress situations they have to know what information to share in order to effectively establish an awareness of the global picture.
In the following, we adopt the Hidden Markov Model (HMM) approach [13] to model human"s cognitive capacity. It is thus assumed that at each time step the secondary task performance of a human subject in a team composed of human-agent-pairs (HAP) is observable to all the team members. Human team members" secondary task performance is used for estimating their hidden cognitive loads.
A HMM is denoted by λ = N, V, A, B, π , where N is a set of hidden states, V is a set of observation symbols, A is a set of state transition probability distributions, B is a set of observation symbol probability distributions (one for each hidden state), and π is the initial state distribution.
We consider a 5-state HMM model of human cognitive load as follows (Figure 2). The hidden states are 0 (negligiblyloaded), 1 (slightly-loaded), 2 (fairly-loaded), 3 (heavilyloaded), and 4 (overly loaded). The observable states are tied with secondary task performance, which, in this study, is measured in terms of the number of items correctly recalled. According to Miller"s 7±2 rule, the observable states take integer values from 0 to 9 ( the state is 9 when the number of items correctly recalled is no less than 9). For the example B Matrix given in Fig. 2, it is very likely that the cognitive load of the subject is negligibly when the number of items correctly recalled is larger than 9.
However, to determine the current hidden load status of a human partner is not trivial. The model might be oversensitive if we only consider the last-step secondary task performance to locate the most likely hidden state. There is ample evidence suggesting that human cognitive load is a continuous function over time and does not manifest sudden shifts unless there is a fundamental changes in tasking demands. To address this issue, we place a constraint on the state transition coefficients: no jumps of more than 2 states are allowed. In addition, we take the position that, a human subject is very likely overloaded if his secondary task performance is mostly low in recent time steps, while he is very likely not overloaded if his secondary task performance is mostly high recently. This leads to the following Windowed-HMM approach.
Given a pre-trained HMM λ of human cognitive load and the recent observation sequence Ot of length w, let parameter w be the effective window size, ελ t be the estimated hidden state at time step t. First apply the HMM to the observation sequence to find the optimal sequence of hidden states Sλ t = s1s2 · · · sw (Viterbi algorithm). Then, compute the estimated hidden state ελ t for the current time step, viewing it as a function of Sλ t . We consider all the hidden states in Sλ t , weighted by their respective distance to ελ t−1 (the estimated state of the last step): the closer of a state in Sλ t The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 397 to ελ t−1, the higher probability of the state being ελ t . ελ t is set to be the state with the highest probability (note that a state may have multiple appearances in Sλ t ). More formally, the probability of state s ∈ S being ελ t is given by: pλ(s, t) = s=sj ∈Sλ t η(sj)e−|sj −ελ t−1| , (1) where η(sj) = ej / w k=1 ek is the weight of sj ∈ Sλ t (the most recent hidden state has the most significant influence in predicting the next state). The estimated state for the current step is the state with maximum likelihood: ελ t = argmax s∈Sλ t pλ(s, t) (2)
According to schema theory [11], multiple elements of information can be chunked as single elements in cognitive schemas. A schema can hold a huge amount of information, yet is processed as a single unit. We adapt this idea and assume that agent i"s estimation of agent j"s processing load at time step t is a function of two factors: the number of chunks cj(t) and the total number sj(t) of information being considered by agent j. If cj(t) and sj(t) are observable to agent i, agent i can employ a Windowed-HMM approach as described in Section 3.1 to model and estimate agent j"s instantaneous processing load.
In the study reported below, we also used 5-state HMM models for agent processing load. With the 5 hidden states similar to the HMM models adopted for human cognitive load, we employed multivariate Gaussian observation probability distributions for the hidden states.
As discussed above, a Human-Agent-Pair (HAP) is viewed as a unit when teaming up with other HAPs. The processing load of a HAP can thus be modeled as the co-effect of the processing load of the agent and the cognitive load of the human partner as captured by the agent.
Suppose agent Ai has models for its processing load and its human partner Hi"s cognitive load. Denote the agent processing load and human cognitive load of HAP Hi, Ai at time step t by μi t and νi t, respectively. Agent Ai can use μi t and νi t to estimate the load of Hi, Ai as a whole. Similarly, if μj t and νj t are observable to agent Ai, it can estimate the load of Hj, Aj . For model simplicity, we still used 5-state HMM models for HAP processing load, with the estimated hidden states of the corresponding agent processing load and human cognitive load as the input observation vectors.
Building a load estimation model is the means. The goal is to use the model to enhance information sharing performance so that a team can form better shared mental models (e.g., to develop inter-agent role expectations in their collaboration), which is the key to high team performance.
Each agent has to adopt a certain strategy to process the incoming information. As far as resource-bounded agents are concerned, it is of no use for an agent to share information with teammates who are already overloaded; they simply do not have the capacity to process the information.
Consider the incoming information processing strategy as shown in Table 1. Agent Ai (of HAPi) ignores all the incoming information when it is overloaded, and processes all the incoming information when it is negligibly loaded. When it Table 1: Incoming information processing strategy HAPi Load Strategy Overly Ignore all shared info Heavily Consider every teammate A ∈ [1, 1 q |Q| ], randomly process half amount of info from A; Ignore info from any teammate B ∈ ( 1 q |Q|, |Q|] Fairly Process half of shared info from any teammate Slightly Process all info from any A ∈ [1, 1 q |Q| ]; For any teammate B ∈ ( 1 q |Q|, |Q|] randomly process half amount of info from B Negligibly Process all shared info HAPj Process all info from HAPj if it is overloaded *Q is a FIFO queue of agents from whom this HAP has received information at the current step; q is a constant known to all. is heavily loaded, Ai randomly processes half of the messages from those agents which are the first 1/q teammates appeared in its communication queue; when it is fairly loaded,
Ai randomly processes half of the messages from any teammates; when it is slightly loaded, Ai processes all the messages from those agents which are the first 1/q teammates appeared in its communication queue, and randomly processes half of the messages from other teammates.
To further encourage sharing information at the right time, the last row of Table 1 says that HAPi , if having not sent information to HAPj who is currently overloaded, will process all the information from HAPj . This can be justified from resource allocation perspective: an agent can reallocate its computing resource reserved for communication to enhance its capacity of processing information. This strategy favors never sending information to an overloaded teammate, and it suggests that estimating and exploiting others" loads can be critical to enable an agent to share the right information with the right party at the right time.
SMMall (Shared Mental Models for all) is a cognitive agent architecture developed for supporting human-centric collaborative computing. It stresses human"s role in team activities by means of novel collaborative concepts and multiple representations of context woven through all aspects of team work. Here we describe two components pertinent to the experiment reported in Section 5: multi-party communication and shared mental maps (a complete description of the SMMall system is beyond the scope of this paper).
Multi-party communication refers to conversations involving more than two parties. Aside from the speaker, the listeners involved in a conversation can be classified into various roles such as addressees (the direct listeners), auditors (the intended listeners), overhearers (the unintended but anticipated listeners), and eavesdroppers (the unanticipated listeners). Multi-party communication is one of the characteristics of human teams. SMMall agents, which can form Human-Agent-Pairs with human partners, support multiparty communication with the following features.
performatives such as MInform (multi-party inform), MAnnounce (multi-party announce), and MAsk (multi-party ask). The listeners of a multi-party performative can be addressees, auditors, and overhearers, which correspond to ‘to", ‘cc", and ‘bcc" in e-mail terms, respectively.
398 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) are three built-in channels: agentTalk channel (inter-agent activity-specific communication), control channel (meta communication for team coordination), and world channel (communication with the external world). An agent can fully tune to a channel to collect messages sent (or cc, bcc) to it. An agent can also partially tune to a channel to get statistic information about the messages communicated over the channel. This is particularly useful if an agent wants to know the communication load imposed on a teammate.
A concept shared belief map has been proposed and implemented into SMMall; this responds to the need to seek innovative perspectives or concepts that allow group members to effectively represent and reason about shared mental models at different levels of abstraction. As described in Section 5, humans and agents interacted through shared belief maps in the evaluation of HMM-based load models.
A shared belief map is a table with color-coded info-cellscells associated with information. Each row captures the belief model of one team member, and each column corresponds to a specific information type (all columns together define the boundary of the information space being considered). Thus, info-cell Cij of a map encodes all the beliefs (instances) of information type j held by agent i. Color coding applies to each info-cell to indicate the number of information instances held by the corresponding agent.
The concept of shared belief map helps maintain and present a human partner with a synergy view of the shared mental models evolving within a team. Briefly, SMMall has implemented the concept with the following features:
to view and share the associated information instances. It allows selective (selected subset) or holistic info-sharing.
partners can initiate a multi-party conversation. It also allows third-party info-sharing, say, A shares the information held by B with C.
by inference rules) can be closely organized. Hence, nearby info-cells can form meaningful plateaus (or contour lines) of similar colors. Colored plateaus indicate those sections of a shared mental model that bear high overlapping degrees.
a shared belief map indicates the information difference among team members, and hence visually represents the potential information needs of each team member (See Figure 3).
SMMall has also implemented the HMM-based models (Section 3) to allow an agent to estimate its human partner"s and other team members" cognitive/processing loads.
As shown in Fig. 3, below the shared belief map there is a load display for each team member. There are 2 curves in a display: the blue (dark) one plots human"s instantaneous cognitive loads and the red one plots the processing loads of a HAP as a whole. If there are n team members, each agent needs to maintain 2n HMM-based models to support the load displays. The human partner of a HAP can adjust her cognitive load at runtime, as well as monitor another HAP"s agent processing load and its probability of processing the information she sends at the current time step. Thus, the more closely a HAP can estimate the actual processing loads of other HAPs, the better information sharing performance the HAP can achieve.
Figure 3: Shared Mental Map Display In sum, shared belief maps allow the inference of who needs what, and load displays allow the judgment of when to share information. Together they allow us to investigate the impact of sharing the right info. with the right party at the right time on the evolving of shared mental models.
We here describe how we measure team performance in our experiment. We use mental model overlapping percentage (MMOP) as the base to measure shared mental models. MMOP of a group is defined as the intersection of all the individual mental states relative to the union of individual mental states of the group. Formally, given a group of k agents G = {Ai|1 ≤ i ≤ k}, let Bi = {Iim|1 ≤ m ≤ n} be the beliefs (information) held by agent Ai, where each Iim is a set of information of the same type, and n (the size of information space) is fixed for the agents in G, then MMOP(G) = 100 n 1≤m≤n ( | ∩1≤i≤k Iim| | ∪1≤i≤k Iim| ). (3) First, a shared mental model can be measured in terms of the distance of averaged subgroup MMOPs to the MMOP of the whole group. Without losing generality, we define paired SMM distance (subgroups of size 2) D2 as: D2 (G) = 1≤i<j≤k (MMOP({Ai, Aj}) − MMOP(G))2 . (4) The MMOP of a subgroup is always larger than the MMOP of the whole group. Intuitively, the larger distance from the MMOP of a subgroup to that of the whole group, the more overlapping mental models the subgroup shares. This notion can be used to measure the tightness of an emerging subgroup or guide the process of team coalition.
Second, due to communication or information processing limits, each individual"s subjective measure of the group"s MMOP can be very different from the group"s MMOP measured objectively from external. A shared mental model can thus be measured in terms of the closeness of individuals" measure of the group"s MMOP to the objective measure. Let MMOP(G) and M MOPi(G) be the objective measure and agent Ai"s subjective measure of the group"s shared mental models, respectively. We define SMM deviation D⊥ as: D⊥(G) = 1≤i≤k (MMOPi(G) − MMOP(G))2 . (5) Obviously, D⊥ measures the coherency of the whole group: the smaller the better.
Third, shared mental models evolve over time. A shared mental model can be measured in terms of the stableness of the instantaneous measures of MMOP, D , or D⊥ The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 399 over time. High performing teams can often maintain their shared mental models such that the MMOP is stable at an acceptable level as activities proceed, while the MMOP measure of the shared mental models of a low-performing teams can fluctuate or decrease notably over time.
In this section we describe the experiments conducted to evaluate the load estimation models and the shared belief map concept for developing team shared mental models.
The members of a HAP team (i.e., a team composed of Human-Agent-Pairs) are situated in a dynamic environment. Due to their different (maybe overlapping) observability, at each time step they may get different situational information. The goal of a HAP team is to selectively share information in a timely manner to develop global situation awareness (say, for making critical decisions).
Each run of the experiment has 45 time steps; each time step lasts 15 seconds. A time step starts with certain infocells of the shared belief map being flashed quickly (for 2 s). The flashed cells are exactly those with newly available information that should be shared at that time step. An info-cell is frozen at the end of a time step: the associated information is no longer sharable. This requires that the newly available information be shared in time, not later.
The human and agent of a HAP assume different roles.
An agent governs group communication and processes messages to update the shared belief map on its display. An agent, with a pre-trained HMM-based cognitive load model for its human partner and a processing load model for each of the other HAPs in the team, also estimates and displays their instantaneous loads. Human subjects need to perform a primary task and a secondary task. The secondary task of a human subject is to remember and mark the cells being flashed (not necessarily in the exact order) by left mouse clicks. Secondary task performance at step t is thus measured as the number of cells marked (remembered) correctly at t, which is taken as the observable state of the HMMbased cognitive load model of that human subject. The primary task of a human subject is to build a shared mental model of the dynamic situation by sharing the right information with the right party at the right time. To share the information associated with an info-cell, a human subject needs to click the right mouse button on the cell, and select the receiving teammate(s) from the popup menu.
There are costs associated with information sharing.
Communications among HAPs is done by the corresponding SMMall agents, which have both limited capacity nin for processing incoming information and limited outgoing communication capacity nout. Thus, depending on the current HAP load, an agent may randomly ignore part or all of the incoming information (having no effect on the establishment of shared mental models). On the other hand, each time step at most nout number of information-sharing commands can be effective; more than that contribute nothing to the establishment of shared mental models. Sending information to an overloaded teammate will waste the capacity that otherwise can be used to share information with a less loaded teammate. This means that at each time step a human subject has to carefully go through three cognitive decisions: whether the information under consideration needs to be shared (i.e., whether it is associated with an info-cell just flashed), whether a team member is the right party to share the information with (i.e., whether it really needs the information), and whether this is the right time to share (i.e., whether the team member is currently overloaded).
The above description applies to HAP teams. For teams composed of SMMall agents only, the agents will take all the roles played by an agent or a human partner in HAP teams.
To investigate the impacts of the HMM-based load models on the evolution of shared mental models (SMM), we conducted experiments for both Agent teams and HAP teams, where agent teams involve agent processing load models only, HAP teams involve models of HAP processing load (i.e., the co-effect of agent processing load and human cognitive load). To get insights on how load predictions and multi-party communication may affect the performance of forming SMM, we designed 3 Agent teams (TA1, TA2, TA3) and 3 HAP teams (TH1, TH2, TH3), where all agents adopt the strategy in Table 1 to send and process information.
When sharing information, teams of type 1 (TA1, TH1) ignore load predictions; teams of type 2 (TA2, TH2) consider load predictions; teams of type 3 (TA3, TH3) follow load predictions more strictly in the sense that the agents will further group the receivers of a multi-party message (MInform) by their loads and split the message into multiple messages with their receivers having the same load.
For example, given that agents A1, A2, A3 have load 1, 2, 1, respectively. An agent A0 in a team of TA2 may send one multi-party message, while an agent A0 in a team of TA3 will send two messages (one to A2, one to A1 and A3).
In addition, we controlled agents" outgoing communication capacities by varying from 6, 8, to 10.
Due to constraints on communication capacity and processing capacity, an agent can be inaccurate when tracking other teammates" mental models. In order to measure the actual shared mental models, a special SMMall agent named ‘OmniAll" was added to each team to monitor inter-agent communications and record the actual effects of information sharing on each agent"s mental model. This realizes a way, as suggested by Klimoski [6], to measuring the degree of overlap in immediate, intermediate, and long-term situation awareness zones held by group members.
We also recorded instantaneous information sharing utility, which is defined as follows. At each time step, let T = T0, T1, T2, T3, T4 be a sequence of sets, where T0, T1, T2, T3, and T4 are sets of teammates whose current load states are negligibly, slightly, fairly, heavily, and overly respectively. Let S be the set of information-sharing commands issued by a human partner at the current step. Let Mi = {Tk ∈ T|k ≤ i, Tk = ∅}. Instantaneous info-sharing utility is defined as c∈S s value(c)/|S|, where s value(c) = ⎧ ⎪⎨ ⎪⎩ 0 receiver(c) ∈ T4 0 c is known to receiver(c) 1/|Mi| receiver(c) ∈ Ti, i = 4 (6) In sum, this study involved 18 types of teams, each team had 4 members, and each team type was tested by 10 domain scenarios. 30 human subjects were recruited for HAP teams.
The experiment results are plotted in Figures 4, 6, and 7.
We next present our findings in this study.
400 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 35 40 45 50 55 60 65 Evolution of Shared Mental Models of Agent Teams time step PercentageofSMM(%) TA1−6 TA1−8 TA1−10 TA2−6 TA2−8 TA2−10 TA3−6 TA3−8 TA3−10 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 14 16 18 20 22 24 26 28 30 time step PercentageofSMM(%) Evolution of Shared Mental Models of HAP Teams TH1−6 TH1−8 TH1−10 TH2−6 TH2−8 TH2−10 TH3−6 TH3−8 TH3−10 NoSharing Figure 4: Evolution of SMMs.
Consider teams of type 1 (TA1, TH1) and teams of type 2 (TA2, TH2). First look at the performance of HAP teams in Fig. 4, we have: (1) For each team type, the performance (percentage of SMM overtime) averaged over 10 teams increased as communication capacity increased (TH1-6<TH18<TH1-10, TH2-6<TH2-8<TH2-10). (2) The averaged performance of TH2 teams performed consistently better than the TH1 teams for each capacity setting (TH2-6>TH1-6,
TH2-8>TH1-8, TH2-10>TH1-10), and the performance difference of TH1 and TH2 teams increased as communication capacity increased. This indicates that, other things being equal, the benefit of exploiting load estimation when sharing information becomes more significant when communication capacity is larger. From Fig. 4 the same findings can be derived for the performance of agent teams.
In addition, the results also show that the SMMs of each team type were maintained steadily at a certain level after about 20 time steps. However, to maintain a SMM steadily at a certain level is a non-trivial team task. The performance of teams who did not share any information (the ‘NoSharing" curve in Fig. 4) decreased constantly as time proceeded.
We now compare teams of type 2 and type 3 (which splits multi-party messages by receivers" loads). As plotted in Fig. 4, for HAP teams, the performance of team type 2 for each fixed communication capacity was consistently better than team type 3 (TH3-6≤TH2-6, TH3-8<TH2-8,
TH3-10<TH210); the difference became more significant as communication capacity increased. These also hold for the Agent teams (upper one in Fig. 4). Actually, the performance of type 3 (a) (b) A B C MInform I A B C Inform I Inform I Inform I Figure 5: Multi-party messages. agent teams was even worse (for each fixed capacity) than the performance of type 1 agent teams.
This can be explained by the difference of two-party and multi-party communications. In SMMall, in order to enable team-level inference, each agent maintains an internal model of every team member"s mental model (beliefs). According to the semantics of MInform (multi-party inform), when A MInforms I to others, assuming all the receivers and overhearers will accept I, A will update its internal model of their beliefs; each of the receivers (overhearers), upon getting the message, will update its own beliefs as well as its model of the sender"s and all the other receivers" beliefs.
Compared to two-party performatives, multi-party performatives are preferable for forming shared mental models.
As illustrated in Fig. 5(a), agent A only needs to perform MInform once (with B and C being receivers) to achieve the common knowledge of the shared belief about I (It consumes 2 of A"s communication resources, one for each receiver). However, to achieve the same effects using Inform (Fig. 5(b)), it is hard to form team-level SMM especially when the team size is big (missing one Inform will nullify all others" efforts). Moreover, although agent A still consumes 2, the whole team needs more resources (3 in this case).
However, splitting multi-party messages by receivers" loads does enhance subgroup SMMs. In Fig. 6 we plotted the other two measures of SMMs (distance and closeness as defined in Sec. 4.3). For HAP teams, TH3>TH2>TH1 holds in Fig. 6(c) (larger distances indicate better subgroup SMMs), and TH3<TH2<TH1 holds in Fig. 6(d) (smaller deviations indicate higher coherency of the whole team). Thus, HAP teams of type 3 achieved better subgroup SMMs, and their team members had higher coherent view of group SMMs than teams of other types. For agent teams,
TH3>TH1>TH2 holds in Fig. 6(a), and TH2<TH3<TH1 holds in Fig. 6(b). Thus, agent teams of type 3 achieved better subgroup SMMs, and their team members had much higher coherent view of group SMMs than teams of type 1 (although slightly worse than team type 2).
Hence, generally, multi-party communication encourages the forming/evolving of team SMMs. When a group of agents can be partitioned into subteams, splitting messages by their loads can be more effective for subteam SMMs.
To validate the HMM-based cognitive load model is extremely difficult because detecting the real, noise-resistant cognitive load of human beings is far beyond the current technology. As an indirect judgment, we plotted the regression fitted lines for the means of information sharing utility of HAP teams with and without load displays. For the HAP teams with load displays there is a strong negative linear relationship between info-sharing utility and cognitive load levels (Pearson correlation coefficient is − √
with P-Value = 0.014). Because the info-sharing utility measure and the cognitive load measure are indicators of primary task performance and secondary task performance, respectively, their linear relationship complies with cognitive studies that secondary task performance can be used The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 401 Agent Teams: Distance from Group SMM to Paired SMM (averaged) 80 90 100 110 120 130 140 150 160 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step Distance Agent Teams: Summed Deviation of Group SMM from Individual Perspective 0 5 10 15 20 25 30 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step Deviation TA1 TA2 TA3 HAP Teams: Distance from Group SMM to Paired SMM (averaged) 130 140 150 160 170 180 190 200 210 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step Distance HAP Teams: Summed Deviation of Group SMM from Individual Perspective 0 5 10 15 20 25 30 35 40 45 50 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45time step Deviation TH1 TH2 TH3 (a) (b) (c) (d) Figure 6: The distance of subgroup SMMs and the closeness of individual views of team SMMs. to explain primary task performance. However, for teams without load displays there is a strong quadratic rather than linear relationship (correlation coefficient is √
with P-Value = 0.015). This indicates that the information sharing performance of a HAP team can be significantly affected by both the human subject"s own cognitive load, and the awareness of other team members" load. Knowing of others" load (by estimation) will reduce the quadratic relation to a linear relation.
Figure 7: Info-sharing utility by cognitive loads.
Recent research attention on human-centered teamwork highly demands the design of agent systems as cognitive aids that can model and exploit human partners" cognitive capacities to offer help unintrusively. In this paper, we investigated several factors surrounding the challenging problem of evolving shared mental models of teams composed of human-agent-pairs. The major contribution of this research includes (1) HMM-based load models were proposed for an agent to estimate its human partner"s cognitive load and other HAP teammates" processing loads; (2) The shared belief map concept was introduced and implemented. It allows group members to effectively represent and reason about shared mental models; (3) Experiments were conducted to evaluate the HMM-based cognitive/processing load models and the impacts of multi-party communication on the evolving of team SMMs. The usefulness of shared belief maps was also demonstrated during the experiments.

A two-sided economic search is a distributed mechanism for forming agents" pairwise partnerships [5].1 On every stage of the process, each of the agents is randomly matched with another agent 1 Notice that the concept of search here is very different from the classical definition of search in AI. While AI search is an active process in which an agent finds a sequence of actions that will bring it from the initial state to a goal state, economic search refers to the identification of the best agent to commit to a partnership with. and the two interact bilaterally in order to learn the benefit encapsulated in a partnership between them. The interaction does not involve bargaining thus each agent merely needs to choose between accepting or rejecting the partnership with the other agent.
A typical market where this kind of two-sided search takes place is the marriage market [22]. Recent literature suggests various software agent-based applications where a two-sided distributed (i.e., with no centralized matching mechanisms) search takes place.
An important class of such applications includes secondary markets for exchanging unexploited resources. An exchange mechanism is used in those cases where selling these resources is not the core business of the organization or when the overhead for selling them makes it non-beneficial. For example, through a twosided search, agents, representing different service providers, can exchange unused bandwidth [21] and communication satellites can transfer communication with a greater geographical coverage.
Twosided agents-based search can also be found in applications of buyers and sellers in eMarkets and peer-to-peer applications. The twosided nature of the search suggests that a partnership between a pair of agents is formed only if it is mutually accepted. By forming a partnership the agents gain an immediate utility and terminate their search. When resuming the search, on the other hand, a more suitable partner might be found however some resources will need to be consumed for maintaining the search process.
In this paper we focus on a specific class of two-sided search matching problems, in which the performance of the partnership applies to both parties, i.e., both gain an equal utility [13]. The equal utility scenario is usually applicable in domains where the partners gain from the synergy between them. For example, consider tennis players that seek partners when playing doubles (or a canoe"s paddler looking for a partner to practice with). Here the players are being rewarded completely based on the team"s (rather than the individual) performance. Other examples are the scenario where students need to form pairs for working together on an assignment, for which both partners share the same grade, and the scenario where two buyer agents interested in similar or interchangeable products join forces to buy a product together, taking advantage of discount for quantity (i.e. each of them enjoys the same reduced price). In all these applications, any two agents can form a partnership and the performance of any given partnership depends on the skills or the characteristics of its members.
Furthermore, the equal utility scenario can also hold whenever there is an option for side-payments and the partnership"s overall utility is equally split among the two agents forming it [22].
While the two-sided search literature offers comprehensive equilibrium analysis for various models, it assumes that the agents" search is conducted in a purely sequential manner: each agent locates and interacts with one other agent in its environment at a time 450 978-81-904262-7-5 (RPS) c 2007 IFAAMAS [5, 22]. Nevertheless, when the search is assigned to autonomous software agents a better search strategy can be used. Here an agent can take advantage of its unique inherent filtering and information processing capabilities and its ability to efficiently (in comparison to people) maintain concurrent interactions with several other agents at each stage of its search. Such use of parallel interactions in search is favorable whenever the average cost2 per interaction with another agent, when interacting in parallel with a batch of other agents, is smaller than the cost of maintaining one interaction at a time (i.e., advantage to size). For example, the analysis of the costs associated with evaluating potential partnerships between service providers reveals both fixed and variable components when using the parallel search, thus the average cost per interaction decreases as the number of parallel interactions increases [21].
Despite the advantages identified for parallel interactions in adjacent domains (e.g., in one-sided economic search [7, 16]), a first attempt for modeling a repeated pairwise matching process in which agents are capable of maintaining interaction with several other agents at a time was introduced only recently [21]. However, the agents in that seminal model are required to synchronize their decision making process. Thus each agent, upon reviewing the opportunities available in a specific search stage, has to notify all other agents of its decision whether to commit to a partnership (at most with one of them) or reject the partnership (with the rest of them). This inherent restriction imposes a significant limitation on the agents" strategic behavior.
In our model, the agents are free to notify the other agents of their decisions in an asynchronous manner. The asynchronous approach allows the agents to re-evaluate their strategy, based on each new response they receive from the agents they interact with. This leads to a sequential decision making process by which each agent, upon sending a commit message to one of the other agents, delays its decision concerning a commitment or rejection of all other potential partnerships until receiving a response from that agent (i.e., the agent still maintains parallel interactions in each search stage, except that its decision making process at the end of the stage is sequential rather than instantaneous). The new model is a much more realistic pairwise model and, as we show in the analysis section, is always preferred by any single agents participating in the process.
In the absence of other economic two-sided parallel search models, we use the model that relies on an instantaneous (synchronous) decision making process [21] (denoted I-DM throughout the rest of the paper) as a benchmark for evaluating the usefulness of our proposed sequential (asynchronous) decision making strategy (denoted S-DM).
The main contributions of this paper are threefold: First, we formally model and analyze a two-sided search process in which the agents have no temporal decision making constraints concerning the rejection of or commitment to potential partnerships they encounter in parallel (the S-DM model). This model is a general search model which can be applied in various (not necessarily software agents-based) domains. Second, we prove that the agents" SDM strategy weakly dominates the I-DM strategy, thus every agent has an incentive to deviate to the S-DM strategy when all other agents are using the I-DM strategy. Finally, by using an innovative recursive presentation of the acceptance probabilities of different potential partnerships, we identify unique characteristics of the equilibrium strategies in the new model. These are used for supplying an appropriate computational means that facilitates the calculation of the agents" equilibrium strategy. This latter contribution is 2 The term costs refers to resources the agent needs to consume for maintaining its search, such as: self advertisement, locating other agents, communicating with them and processing their offers. of special importance since the transition to the asynchronous mode adds inherent complexity to the model (mainly because now each agent needs to evaluate the probabilities of having each other agent being rejected or accepted by each of the other agents it interacts with, in a multi-stage sequential process). We manage to extract the agents" new equilibrium strategies without increasing the computational complexity in comparison to the I-DM model. Throughout the paper we demonstrate the different properties of the new model and compare it with the I-DM model using an artificial synthetic environment.
In the following section we formally present the S-DM model.
An equilibrium analysis and computational means for finding the equilibrium strategy are provided in Section 3. In Section 4 we review related MAS and economic search theory literature. We conclude with a discussion and suggest directions for future research in Section 5.
We consider an environment populated with an infinite number of self-interested fully rational agents of different types3 . Any agent Ai can form a partnership with any other agent Aj in the environment, associated with an immediate perceived utility U(Ai, Aj) for both agents. As in many other partnership formation models (see [5, 21]) we assume that the value of U(x, y) (where x and y are any two agents in the environment) is randomly drawn from a continuous population characterized with a probability distribution function (p.d.f.) f(U) and a cumulative distribution function (c.d.f.) F(U), (0 ≤ U < ∞). The agents are assumed to be acquainted with the utility distribution function f(x), however they cannot tell a-priori what utility can be gained by a partnership with any specific agent in their environment. Therefore, the only way by which an agent Ai can learn the value of a partnership with another agent Aj, U(Ai, Aj), is by interacting with agent Aj. Since each agent in two-sided search models has no prior information concerning any of the other agents in its environment, it initiates interactions (i.e., search) with other agents randomly. The nature of the two-sided search application suggests that the agents are satisfied with having a single partner, thus once a partnership is formed the two agents forming it terminate their search process and leave the environment.
The agents are not limited to interacting with a single potential partner agent at a time, but rather can select to interact with several other agents in parallel. We define a search round/stage as the interval in which the agent interacts with several agents in parallel and learns the utility of forming a partnership with each of them. Based on the learned values, the agent needs to decide whether to commit or reject each of the potential partnerships available to it.
Commitment is achieved by sending a commit message to the appropriate agent and an agent cannot commit to more than one potential partnership simultaneously. Declining a partnership is achieved by sending a reject message. The communication between the agents is assumed to be asynchronous and each agent can delay its decision, concerning any given potential partnership, as necessary.4 If two agents Ai and Aj mutually commit to a partnership between 3 The infinite number of agents assumption is common in two-sided search models (see [5, 22, 21]). In many domains (e.g., eCommerce) this derives from the high entrance and leave rates, thus the probability of running into the same agent in a random match is negligible. 4 Notice that the asynchronous procedure does not eliminate the inherent structure of the search. The search is still based on stages/rounds where on each search round the agent interacts with several other agents, except that now the agent can delay its decision making process (within each search round) as necessary.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 451 them, then the partnership is formed and both agents gain the immediate utility U(Ai, Aj) associated with it. If an agent does not form a partnership in a given search stage, it continues to its next search stage and interacts with more agents in a similar manner.
Given the option for asynchronous decision making, each individual agent, Ai, follows the following procedure: 1: loop 2: Set N (number of parallel interactions for next search round) 3: Locate randomly a set A = {A1, . . . , AN } of agents to interact with 4: Evaluate the set of utilities {U(Ai, A1), . . . , U(Ai, AN )} 5: Set A∗ ={Aj|Aj ∈A and U(Ai, Aj)>U(resume)} 6: Send a reject message to each agent in the set {A \ A∗ } 7: while (A∗ = ∅) do 8: Send a commit message to Aj = argmaxAl∈A∗ U(Ai, Al) 9: Remove Aj from A∗ 10: Wait for Aj"s decision 11: if (Aj responded commit) then 12: Send reject messages to the remaining agents in A∗ 13: Terminate search 14: end if 15: end while 16: end loop where U(resume) denotes the expected utility of continuing the search (in the following paragraphs we show that U(resume) is fixed throughout the search and derives from the agent"s strategy).
In the above algorithm, any agent Ai first identifies the set A∗ of other agents it is willing to accept out of those reviewed in the current search stage and sends a reject message to the rest. Then it sends a commit message to the agent Aj ∈ A∗ that is associated with the partnership yielding the highest utility. If a reject message was received from agent Aj then this agent is removed from A∗ and a new commit message is sent according to the same criteria.
The process continues until either: (a) the set A∗ becomes empty, in which case the agent initiates another search stage; or (b) a dual commitment is obtained, in which case the agent sends reject messages to the remaining agents in A∗ . The method differs from the one used in the I-DM model in the way it handles the commitment messages: in the I-DM model, after evaluating the set of utilities (step 4), the agent merely sends instantaneously a commit message to the agent associated with the greatest utility and a reject message to all the other agents it interacted with (as a replacement to steps 5-15 in the above procedure). Our proposed S-DM model is much more intuitive as it allows an agent to hold and possibly exploit relatively beneficial opportunities even if its first priority partnership is rejected by the other agent. In the I-DM model, on the other hand, since reject messages are sent alongside the commit message, simultaneously, a reject message from the agent associated with the best partnership enforces a new search round.
Notice that the two-sided search mechanism above aligns with most other two-sided search mechanisms in a sense that it is based on random matching (i.e., in each search round the agent encounters a random sample of agents). While the maintenance of the random matching infrastructure is an interesting research question, it is beyond the scope of this paper. Notwithstanding, we do wish to emphasize that given the large number of agents in the environment and the fact that in MAS the turnover rate is quite substantial due to the open nature of the environment (and the interoperability between environments). Therefore, the probability of ending up interacting with the same agent more than once, when initiating a random interaction, is practically negligible.
THEOREM 1. The S-DM agent"s decision making process: (a) is the optimal one (maximizes the utility) for any individual agent in the environment; and (b) guarantees a zero deadlock probability for any given agent in the environment.
Proof: (a) The method is optimal since it cannot be changed in a way that produces a better utility for the agent. Since bargaining is not applicable here (benefits are non-divisible) then the agent"s strategy is limited to accepting or rejecting offers. The decision of rejecting a partnership in step 6 is based only on the immediate utility that can be gained from this partnership in comparison to the expected utility of resuming the search (i.e., moving on to the next search stage) and is not affected by the willingness of the other agents to commit or reject a partnership with Ai. As for partnerships that yield a utility greater than the expected utility of resuming the search (i.e., the partnerships with agents from the set A∗ ), the agent always prefers to delay its decision concerning partnerships of this type until receiving all notifications concerning potential partnerships that are associated with a greater immediate utility. The delay never results with a loss of opportunity since the other agent"s decision concerning this opportunity is not affected by agent Ai"s willingness to commit or reject this opportunity (but rather by the other agent"s estimation of its expected utility if resuming the search and the rejection messages it receives for more beneficial potential partnerships). Finally, the agent cannot benefit from delaying a commit message to the agent associated with the highest utility in A∗ , thus will always send it a commit message. (b) We first prove the following lemma that states that the probability of having two partnering opportunities associated with an identical utility is zero.
LEMMA 2.1. When f is a continuous distribution function, then lim y→x »Z y z=x f(z)dz -2 ! = 0.
Proof: since f is continuous and the interval between x and y is finite, by the intermediate value theorem (found in most calculus texts) there exists a c between x and y thatZ y z=x f(z)dz = f(c)(y − x) (intuitively, a rectangle with the base from z = x to z = y and height = f(c) has the same area as the integral on the left hand side.). Therefore »Z y z=x f(z)dz -2 = |f(c)|2 |y − x|2 When y → x, f(c) stays bounded due to continuity of f, moreover limy→x f(c) = f(x), hence lim y→x »Z y z=x f(z)dz -2 ! = f(x)2 lim y→x |y − x|2 = 0. .
An immediate derivative from the above lemma is that no tiebreaking procedures are required and an agent in a waiting state is always waiting for a reply from the single agent that is associated with the highest utility among the agents in the set A∗ (i.e., no other agent in the set A∗ is associated with an equal utility). A deadlock can be formed only if we can create a cyclic sequence of agents in which any agent is waiting for a reply from the subsequent agent in the sequence. However, in our method any agent Ai will be waiting for a reply from another agent Aj, to which it sent a commit message, only if: (1) any agent Ak ∈ A, associated with a utility U(Ai, Ak) > U(Ai, Aj), has already rejected the partnership with agent Ai; and (2) agent Aj itself is waiting for a reply from agent Al where U(Al, Aj) > U(Aj, Ai). Therefore, if we have a sequence of waiting agents then the utility associated with partnerships between any two subsequent agents in the sequence must increase along the sequence. If the sequence is cyclic, then we have a 452 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) pattern of the form: U(Ai, Al) > U(Al, Aj) > U(Aj, Ai). Since U(Ai, Al) > U(Aj, Ai), agent Ai can be waiting for agent Aj only if it has already been rejected by Al (see (1) above). However, if agent Al has rejected agent Ai then it has also rejected agent Aj. Therefore, agent Aj cannot be waiting for agent Al to make a decision. The same logic can be applied to any longer sequence. 2 The search activity is assumed to be costly [11, 1, 16] in a way that any agent needs to consume some of its resources in order to locate other agents to interact with, and for maintaining the interactions themselves. We assume utilities and costs are additive and that the agents are trying to maximize their overall utility, defined as the utility from the partnership formed minus the aggregated search costs along the search process. The agent"s cost of interacting with N other agents (in parallel) is given by the function c(N). The search cost structure is principally a parameter of the environment and thus shared by all agents.
An agent"s strategy S(A ) → {commit Aj ∈ A , reject A ⊂ A , N} defines for any given set of partnership opportunities, A , what is the subset of opportunities that should be immediately declined, to which agent to send a commit message (if no pending notification from another agent is expected) or the number of new interactions to initiate (N). Since the search process is two-sided, our goal is to find an equilibrium set of strategies for the agents.
Recall that each agent declines partnerships based on (a) the partnerships" immediate utility in comparison to the agent"s expected utility from resuming search; and (b) achieving a mutual commitment (thus declining pending partnerships that were not rejected in (a)). Therefore an agent"s strategy can be represented by a pair (Nt , xt ) where Nt is the number of agents with whom it chooses to interact in search stage t and xt is its reservation value5 (a threshold) for accepting/rejecting the resulting N potential partnerships.
The subset A∗ , thus, will include all partnership opportunities of search stage t that are associated with a utility equal to or greater than xt . The reservation value xt is actually the expected utility for resuming the search at time t (i.e., U(resume)). The agent will always prefer committing to an opportunity greater than the expected utility of resuming the search and will always prefer to resume the search otherwise.
Since the agents are not limited by a decision horizon, and their search process does not imply any new information about the market structure (e.g., about the utility distribution of future partnership opportunities), their strategy is stationary - an agent will not accept an opportunity it has rejected beforehand (i.e., x1 = x2 = ... = x) and will use the same sample size, N1 = N2 = ... = N, along its search.
The transition from instantaneous decision making process to a sequential one introduces several new difficulties in extracting the agents" strategies. Now, in order to estimate the probability of being accepted by any of the other agents, the agent needs to recursively model, while setting its strategy, the probabilities of rejections other agents might face from other agents they interact with.
In the following paragraphs we introduce several complementary definitions and notations, facilitating the formal introduction of the acceptance probabilities. Consider an agent Ai, using a strategy (N, xN ) while operating in an environment where all other agents 5 Notice the reservation value used here is different from a reservation price concept (that is usually used as buyers" private evaluation). The use of reservation-value based strategies is common in economic search models [21, 17]. are using a strategy (k, xk). The probability that agent Ai will receive a commitment message from agent Aj it interacted with depends on the utility associated with the potential partnership between them, x. This probability, denoted by Gk(x) can be calculated as:6 Gk(x) = 8 >< >: „ 1 − Z ∞ y=x f(y)Gk(y)dy «k−1 if x ≥ xk 0 otherwise. (1) The case where x < xk above is trivial: none of the other agents will accept agent Ai if the utility in such a partnership is smaller than their reservation value xk. However even when the partnership"s utility is greater or equal to xk, commitment is not guaranteed. In the latter scenario, a commitment message from agent Aj will be received only if agent Aj has been rejected by all other agents in its set A∗ that were associated with a utility greater than the utility of a partnership with agent Ai.
The unique solution to the recursive Equation 1 is: Gk(x) = 8 >>>>>< >>>>>:  1+(k−2) R ∞ y=xf(y)dy 1−k k−2 , k>2, x≥xk, exp(− R ∞ y=x f(y)dy), k=2, x≥xk, 1, k=1, x≥xk 0, x < xk. (2) Notice that as expected, a partnership opportunity that yields the maximum mutual utility is necessarily accepted by both agents, i.e., limx→∞ Gk(x) = 1. On the other hand, when the utility associated with a potential partnership opportunity is zero (x = 0) the acceptance probability is non-negligible: lim x→0 Gk(x) = (k − 1) 1−k k−2 (3) This non-intuitive result derives from the fact that there is still a non-negligible probability that the other agent is rejected by all other agents it interacts with.
Using the function Gk(x), we can now formulate and explore the agents" expected utility when using their search strategies.
Consider again an agent Ai that is using a sample of size N while all other agents are using a strategy (k, xk). We denote by RN (x) the probability that the maximum utility that agent Ai can be guaranteed when interacting with N agents (i.e., the highest utility to which a commit message will be received) is at most x. This can be calculated as the probability that none of N agents send agent Ai a commit message for a partnership associated with a utility greater than x: RN (x) =  1 − Z ∞ max(x,xk) f(y)Gk(y)dy N (4) Notice that RN (x) is in fact a cumulative distribution function, satisfying: limx→∞ RN (x) = 1 and dRN (x)/dx > 0 (the function never gets a zero value simply because there is always a positive probability that none of the agents commit at all to a partnership with agent Ai). Therefore, the derivative of the function RN (x), denoted rN (x), is in fact the probability distribution function of the maximum utility that can be guaranteed for agent Ai when sampling N other agents: rN (x) = dRN (x) dx = 8 < : Nf(x)Gk(x) N+k−2 k−1 , x ≥ xk 0, x < xk (5) 6 The use of the recursive Equation 1 is enabled since we assume that the number of agents is infinite (thus the probability of having an overlap between the interacting agents and the affect of such overlap on the probabilities we calculate become insignificant).
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 453 This function rN (x) is essential for calculating VN (xN ), the expected utility of agent Ai when using a strategy (N, xN ), given the strategy (k, xk) used by the other agents: VN (xN )= Z ∞ y=max(xN ,xk) yrN (y)dy+  1− Z ∞ y=max(xN ,xk) rN (y)dy  VN (xN ) − c(N) (6) The right hand side of the above equation represents the expected utility of agent Ai from taking an additional search stage. The first term represents the expected utility from mutual commitment scenarios, whereas the second term is the expected utility associated with resuming the search (which equals VN (xN ) since nothing has changed for the agent). Using simple mathematical manipulations and substituting rN (x), Equation 6 transforms into: VN (x) = R ∞ y=max(x,xk) yNf(y)Gk(y) N+k−2 k−1 dy − c(N) R ∞ y=max(x,xk) Nf(y)Gk(y) N+k−2 k−1 dy (7) and further simplified into: VN (x) = max(x, xk) + Z ∞ max(x,xk) (1 − Gk(y) N k−1 )dy − c(N) 1 − Gk(max(x, xk)) N k−1 (8) Equation 8, allows us to prove some important characteristics of the model as summarized in the following Theorem 2.
THEOREM 2. When other agents use strategy (k, xk): (a) An agent"s expected utility function, VN (xN ), when using a strategy (N, x), is quasi concave in x with a unique maximum, obtained for the value xN satisfying: VN (xN ) = xN (9) (b) The value xN satisfies: c(N) = ` max(xN , xk) − xN ´` 1 − Gk(xk) N k−1 ´ + + Z ∞ max(xN ,xk) (1 − Gk(y) N k−1 )dy (10) The proof is obtained by deriving VN (xN ) in Equation 8 and setting it to zero. After applying further mathematical manipulations we obtain (9) and (10).
Both parts of Theorem 2 can be used as an efficient means for extracting the optimal reservation value xN of an agent, given the strategies of the other agents in the environment and the number of parallel interactions it uses. Furthermore, in the case of complex distribution functions where extracting xN from Equation 10 is not immediate, a simple algorithm (principally based on binary search) can be constructed for calculating the agent"s optimal reservation value (which equals its expected utility, according to 9), with a complexity O(log( ˆx ρ )), where ρ is the required precision level for xN and ˆx is the solution to: R ∞ y=ˆx yNf(y)F(y)N−1 dy = c(N).
Having the ability to calculate xN , we can now prove the following Proposition 2.1.
PROPOSITION 2.1. An agent operating in an environment where all agents are using a strategy according to the instantaneous parallel search equilibrium (i.e., according to the I-DM model [21]) can only benefit from deviating to the proposed S-DM strategy.
Sketch of proof: For the I-DM model the following holds [21]: c(N) = N 2N − 1 Z ∞ y=xI−DM N (1 − F(y)2N−1 )dy (11) We apply the methodology used above in this subsection for constructing the expected utility of the agent using the S-DM strategy as a function of its reservation value, assuming all other agents are using the I-DM search strategy. This results with an optimal reservation value for the agent using S-DM, satisfying: c(N) = Z ∞ y=xS−DM N (1 − (1 − 1 N + F(y)N N )N )dy (12) Finally, we prove that the integrand in Equation 11 is smaller than the integrand in Equation 12. Given the fact that both terms equal c(N), we obtain xS−DM N > xI−DM N and consequently (according to Theorem 2) a similar relationship in terms of expected utilities.
Figure 1 illustrates the superiority of the proposed search strategy S-DM, as well as the expected utility function"s characteristics (as reflected in Theorem 2). For comparative reasons we use the same synthetic environment that was used for the I-DM model [21]. Here the utilities are assumed to be drawn from a uniform distribution function and the cost function was taken to be c(N) =
are using k = 25 and xk = 0.2. The different curves depict the expected utility of the agent as a function of the reservation value, x, that it uses, when: (a) all agents are using the I-DM strategy (marked as I-DM); (b) the agent is using the S-DM strategy while the other agents are using the I-DM strategy (marked as I-DM/SDM); and (c) all agents are using the S-DM strategy (marked as S-DM). As expected, according to Equation 8 and Theorem 2, the agent"s expected utility remains constant until its reservation value exceeds xk. Then, it reaches a global maximum when the reservation value satisfies VN (x) = x. From the graph we can see that the agent always has an incentive to deviate from the I-DM strategy to S-DM strategy (as was proven in Proposition 2.1).
reservation value (x) expected utility VN(x) S-D M I-D M I-D M / S-D M Figure 1: The expected utility as a function of the reservation value used by the agent
Since all agents are subject to similar search costs, and their perceived utilities are drawn from the same distribution function, they all share the same strategy in equilibrium. A multi-equilibria scenario may occur, however as we discuss in the following paragraphs since all agents share the same preferences/priorities (unlike, for example, in the famous battle of the sexes scenario) we can always identify which equilibrium strategy will be used.
Notice that if all agents are using the same sample size, N, then the value xN resulting from solving Equation 10 by substituting k = N and xk = xN is a stable reservation value (i.e., none of the agents can benefit from changing just the value of xN ).
An equilibrium strategy (N, xN ) can be found by identifying an N value for which no single agent has an incentive to use a different number of parallel interactions, k (and the new optimal reservation 454 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) value that is associated with k according to Equation 10). While this implies an infinite solution space, we can always bound it using Equations 8 and 10. Within the framework of this paper, we demonstrate such a bounding methodology for the common case were c(N) is linear7 or convex, by using the following Theorem 3.
THEOREM 3. When c(N) is linear (or convex), then: (a) When all other agents sample k potential partners over a search round, if an agent"s expected utility of sampling k + 1 potential partners,
Vk+1(xk+1), is smaller than Vk(xk), then the expected utility when sampling N potential partners, VN (xN ), where N > k+1, is also smaller than Vk(xk). (b) Similarly, when all other agents sample k potential partners over a search round, if an agent"s expected utility of using k − 1 potential partners, Vk−1(xk−1), is smaller than the expected utility when using k potential partners, Vk(xk), then the expected utility when using N potential partners, where N < k − 1, is also smaller than Vk(xk).
Proof: Let us use the notation ci for c(i). Since Vk(xk) = xk ∀k (according to Equation 9), the claims are: (a) if xk+1 < xk then xN < xk for all N ≥ k + 1, and (b) if xk−1 < xk then xN < xk for all N ≤ k − 1. (a) We start by proving that if xk+1 < xk then xk+2 < xk.
Assume otherwise, i.e., xk+1 < xk and xk+2 > xk. Therefore, according to Equation 10, the following holds: 0 < ck+2 − 2ck+1 + ck < Z ∞ xk+2 (1 − Gk(y) k+2 k−1 )dy − 2 Z ∞ xk (1 − Gk(y) k+1 k−1 )dy + Z ∞ xk (1 − Gk(y) k k−1 )dy where the transition to inequality is valid since c(i) is convex. Since the assumption in this proof is that xk+2 > xk then the above can be transformed into: Z ∞ xk  2Gk(y) k+1 k−1 − Gk(y) k+2 k−1 − Gk(y) k k−1  dy > 0 (13) Now notice that the integrated term is actually −Gk(y) k k−1 ` 1− Gk(y) 1 k−1 ´2 which is obviously negative, contradicting the initial assumption, thus if xk+1 < xk then necessarily xk+2 < xk.
Now we need to prove the same for any xk+j. We will prove this in two steps: first, if xk+i < xk then xk+2i < xk. Second, if xk+i < xk and xk+i+1 < xk, then xk+2i+1 < xk. Together these constitute the necessary induction arguments to prove the case (a).
We start with the even case, using a similar methodology: Assume otherwise, i.e., xk+l < xk ∀l = 1, ..., j − 1 and xk+2i > xk.
According to Equation 10, and the fact that c(i) is convex, the following holds: Z ∞ xk  2Gk(y) k+i k−1 − Gk(y) k+2i k−1 − Gk(y) k k−1  dy > 0 (14) And again the integrand is actually −Gk(y) k k−1 ` 1−Gk(y) i k−1 ´2 which is obviously negative, contradicting the initial assumption, thus xk+2i < xk.
As for the odd case, we use Equation 10 once for k + i + 1 parallel interactions and once for k + 2i + 1. From the convexity of ci, we obtain: ck+2i+1 − ck+i − ck+i+1 + ck > 0, thus: Z ∞ xk ` Gk(y) k+i k−1 +Gk(y) k+i+1 k−1 −Gk(y) k+2i+1 k−1 −Gk(y) k k−1 ´ dy>0 (15) 7 A linear cost function is mostly common in agent-based two-sided search applications, since often the cost function can be divided into fixed costs (e.g. operating the agent per time unit) and variable costs (i.e., cost of processing a single interaction"s data).
This time the integrated term in Equation 15 can be re-written as Gk(y) k k−1 (1 − Gk(y) i k−1 )(Gk(y) i+1 k−1 − 1) which is obviously negative, contradicting the initial assumption, thus xk+i+1 < xk.
Now using induction one can prove that if xk+1 < xk then xk+i < xk. This concludes part (a) of the proof.
The proof for part (b) of the theorem is obtained in a similar manner. In this case: ck − 2ck−i + ck−2i > 0 and ck − ck−i−1 − ck−i + ck−2i−1 > 0.
The above theorem supplies us with a powerful tool for eliminating non-equilibrium N values. It suggests that we can check the stability of a sample size N and the appropriate reservation value xN simply by calculating the optimal reservation values of a single agent when deviating towards using samples of sizes N − 1 and N + 1 (keeping the other agents with strategy (N, xN )). If both the appropriate reservation values associated with the two latter sample sizes are smaller than xN then according to Theorems 3 the same holds when deviating to any other sample size k. The above process can be further simplified by using VN+1(xN ) > xN and VN−1(xN ) > xN as the two elimination rules. This derives from Theorem 3 and the properties of the function VN (x) found in Theorem 2.
Notice that a multi-equilibria scenario may occur, however can easily be resolved. If several strategies satisfy the stability condition defined above, then the agents will always prefer the one associated with the highest expected utility. Therefore an algorithm that goes over the different N values and checks them according to the rules above can be applied, assuming that we can bound the interval for searching the equilibrium N. The following Theorem 4 suggests such an upper bound.
THEOREM 4. An upper bound for the equilibrium number of partners to be considered over a search round is the solution of the equation: A(N) = c(N) (16) provided A(N − 1) > c(N − 1), where we denote,
A(N) := Z ∞ y=0 yNf(y)Gk(y) N+k−2 k−1 dy.
Proof: We denote: A(N, x) = Z ∞ y=x yNf(y)Gk(y) N+k−2 k−1 dy so that A(N) = A(N, 0). From Equation 7: VN (x) = A(N, x) − c(N) N R ∞ x f(y)Gk(y)bdy = A(N, x) − c(N) positive ,
Clearly A(N) ≥ A(N, x)∀x since the integrand is positive. Hence if A(N) − c(N) < 0, then A(N, x) − c(N) < 0∀x and VN (x) < 0 ∀x.
Next we prove that if A(N)−c(N) gets negative, it stays negative.
Recalling that for any g(y): d dN (g(y)b(N) ) = g(y)b(N) log(g(y)) db dN we get: A (N) = −1 (k − 1)2 Z ∞ 0 Gk(y) N k−1 (log Gk(y))2 dy which is always negative, since the integrand is nonnegative.
Therefore A(N) is concave. Since c(N) is convex, −c(N) is concave, and a sum of concave functions is concave, we obtain that The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 455 A(N) − c(N) is concave. This guarantees that once the concave expression A(N) − c(N) shifts from a positive value to a negative one (with the increase in N), it cannot become positive again. Therefore, having N∗ such that A(N∗ ) = c(N∗ ), and A(N∗∗ ) > c(N∗∗ ) for some N∗∗ < N∗ , is an upper bound for N, i.e., VN (x) < 0 ∀N ≥ N∗ . The condition we specify for N∗∗ is merely for ensuring that VN is switching from a positive value to a negative one (and not vice versa) and is trivial to implement.
Given the existence of the upper bound, we can design an algorithm for finding the equilibrium strategy (if one exists). The algorithm extracts the upper bound, ˆN, for the equilibrium number of parallel interactions according to Theorem 4. Out of the set of values satisfying the stability condition defined above, the algorithm chooses the one associated with the highest reservation value according to Equation 10. This is the equilibrium associated with the highest expected utility to all agents according to Theorem 2.
2 3 4 5 6 7 8 9 10 11 12 13 expected utility VN(x) num ber ofparallelinteractions (N) VN+ 1 ( XN) VN( XN) VN-1 ( XN) enlarged Figure 2: The incentive to deviate from strategy (N, xN ) The process is illustrated in Figure 2 for an artificial environment where partnerships" utilities are associated with a uniform distribution. The cost function used is c(N) = 0.2 + 0.02N. The graph depicts a single agent"s expected utility when all other agents are using N parallel interactions (on the horizontal axis) and the appropriate reservation value xN (calculated according to Equation 10).
The different curves depict the expected utility of the agent when it uses a strategy: (a) (N, xN ) similar to the other agents (marked as VN (xN )); (b) (N + 1, xN ) (marked as VN+1(xN )); and (c) (N − 1, xN ) (marked as VN−1(xN )). According to the discussion following Theorem 3, a stable equilibrium satisfies: VN (xN ) > max{VN+1(xN ), VN−1(xN )}. The strategy satisfying the latter condition in our example is (9, 0.437).
The two-sided economic search for partnerships in AI literature is a sub-domain of coalition formation8 . While coalition formation models usually consider general coalition-sizes [24], the partnership formation model (often referred as matchmaking) considers environments where agents have a benefit only when forming a partnership and this benefit can not be improved by extending the partnership to more than two agents [12, 23] (e.g., in the case of buyers and sellers or peer-to-peer applications). As in the general 8 The use of the term partnership in this context refers to the agreement between two individual agents to cooperate in a pre-defined manner. For example, in the buyer-seller application a partnership is defined as an agreed transaction between the two-parties [9]. coalition formation case, agents have the incentive to form partnerships when they are incapable of executing a task by their own or when the partnership can improve their individual utilities [14].
Various centralized matching mechanisms can be found in the literature [6, 2, 8]. However, in many MAS environments, in the absence of any reliable central matching mechanism, the matching process is completely distributed.
While the search in agent-based environments is well recognized to be costly [11, 21, 1], most of the proposed coalition formation mechanisms assume that an agent can scan as many partnership opportunities in its environment as needed or have access to central matchers or middle agents [6]. The incorporation of costly search in this context is quite rare [21] and to the best of our knowledge, a distributed two-sided search for partners model similar to the S-DM model has not been studied to date.
Classical economic search theory ([15, 17], and references therein) widely addresses the problem of a searcher operating in a costly environment, seeking to maximize his long term utility. In these models, classified as one-sided search, the focus is on establishing the optimal strategies for the searcher, assuming no mutual search activities (i.e., no influence on the environment). Here the sequential search procedure is often applied, allowing the searcher to investigate a single [15] or multiple [7, 19] opportunities at a time. While the latter method is proven to be beneficial for the searcher, it was never used in the two-sided search models that followed (where dual search activities are modeled) [22, 5, 18]. Therefore, in these models, the equilibrium strategies are always developed based on the assumption that the agents interact with others sequentially (i.e., with one agent at a time). A first attempt to integrate the parallel search into a two-sided search model is given in [21], as detailed in the introduction section.
Several of the two-sided search essences can be found in the strategic theory of bargaining [3] - both coalition formation and matching can be represented as a sequential bargaining game [4] in which payoffs are defined as a function of the coalition structure and can be divided according to a fixed or negotiated division rule.
Nevertheless, in the sequential bargaining literature, most emphasis is put on specifying the details of the sequential negotiating process over the division of the utility (or cost) jointly owned by parties or the strategy the coalition needs to adopt [20, 4]. The models presented in this area do not associate the coalition formation process with search costs, which is the essence of the analysis that economic search theory aims to supply. Furthermore, even in repeated pairwise bargaining [10] models the agents are always limited to initiating a single bargaining interaction at a time.
The phenomenal growth evidenced in recent years in the number of software agent-based applications, alongside the continuous improvement in agents" processing and communication capabilities, suggest various incentives for agents to improve their search performance by applying advanced search strategies such as parallel search. The multiple-interactions technique is known to be beneficial for agents both in one-sided and two-sided economic search [7, 16, 21], since it allows the agents to decrease their average cost of learning about potential partnerships and their values. In this paper we propose a new parallel two-sided search mechanism that differs from the existing one in a sense that it allows the agents to delay their decision making process concerning the acceptance and rejection of potential partnerships as necessary. This, in comparison to the existing instantaneous model [21] which force each agent to make a simultaneous decision concerning each of the potential partnerships revealed to it during the current search stage. 456 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) As discussed throughout the paper, the new method is much more intuitive to the agent than the existing model - an agent will always prefer to keep all options available. Furthermore, as we prove in the former sections, an agent"s transition to the new search method always results with a better utility.
As we prove in Section 2, in spite of the transition to a sequential decision making, deadlocks never occur in the proposed method as long as all agents use the proposed strategies. Since our analysis is equilibrium-based, a deviation from the proposed strategies is not beneficial. Similarly, we show that a deviation of a single agent (back) to the instantaneous decision making strategy is not beneficial. The only problem that may arise in the transition from an instantaneous to sequential decision making is when an agent fails (technically) to function (endlessly delaying the notification to the agents it interacted with). While equilibrium analysis normally do not consider malfunction as a legitimate strategy, we do wish to emphasize that the malfunctioning agent problem can be resolved by using a simple timeout for receiving responses and skipping this agent in the sequential decision process if the timeout is exceeded.
Our analysis covers all aspects of the new two-sided search technique, from individual strategy construction throughout the dynamics that lead to stability (equilibrium). The difficulty in the extraction of the agents" equilibrium strategies in the new model derives from the need to recursively model, while setting an agent"s strategy, the rejection other agents might face from other agents they interact with. This complexity (that does not exist in former models) is resolved by the introduction of the recursive function Gk(x) in Section 2. Using the different theorems and propositions we prove, we proffer efficient tools for calculating the agents" equilibrium strategies. Our capabilities to produce an upper bound for the number of parallel interactions used in equilibrium (Theorem 4) and to quickly identify (and eliminate) non-equilibrium strategies (Theorem 3) resolves the problem of the computational complexity associated with having to deal with a theoretically infinite strategy space.
While the analysis we present is given in the context of software agents, the model we suggest is general, and can be applied to any two-sided economic search environment where the searchers can search in parallel. In particular, in addition to weakly dominating the instantaneous decision making model (as we prove in the analysis section) the proposed method weakly dominates the purely sequential two-sided search model (where each agent interacts with only one other agent at a time) [5]. This derives from the fact that the proposed method is a generalization of the latter (i.e., in the worst case scenario, the agent can interact with one other agent at a time in parallel).
Naturally the attempt to integrate search theory techniques into day-to-day applications brings up the applicability question.
Justification and legitimacy considerations for this integration were discussed in the wide literature we refer to throughout the paper. The current paper is not focused on re-arguing applicability, but rather on the improvement of the the core two-sided search model. We see great importance in future research that will combine bargaining as part of the interaction process. We believe such research can result in many rich variants of our two-sided search model.
[1] Y. Bakos. Reducing buyer search costs: Implications for electronic marketplaces. Management Science, 42(12):1676-1692, June 1997. [2] G. Becker. A theory of marriage. Journal of Political Economy, 81:813-846, 1973. [3] K. Binmore, M. Osborne, and A. Rubinstein.
Non-cooperative models of bargaining. In Handbook of Game Theory with Economic Applications, pages 180-220.
Elsevier, New York, 1992. [4] F. Bloch. Sequential formation of coalitions in games with externalities and fixed payoff division. Games and Economic Behavior, 14(1):90-123, 1996. [5] K. Burdett and R. Wright. Two-sided search with nontransferable utility. Review of Economic Dynamics, 1:220-245, 1998. [6] K. Decker, K. Sycara, and M. Williamson. Middle-agents for the internet. In Proc. of IJCAI, pages 578-583, 1997. [7] S. Gal, M. Landsberger, and B. Levykson. A compound strategy for search in the labor market. Int. Economic Review, 22(3):597-608, 1981. [8] D. Gale and L. Shapley. College admissions and the stability of marriage. American Math. Monthly, 69:9-15, 1962. [9] M. Hadad and S. Kraus. Sharedplans in electronic commerce. In M. Klusch, editor, Intelligent Information Agents, pages 204-231. Springer Publisher, 1999. [10] M. Jackson and T. Palfrey. Efficiency and voluntary implementation in markets with repeated pairwise bargaining. Econometrica, 66(6):1353-1388, 1998. [11] J. Kephart and A. Greenwald. Shopbot economics. JAAMAS, 5(3):255-287, 2002. [12] M. Klusch. Agent-mediated trading: Intelligent agents and e-business. J. on Data and Knowledge Engineering, 36(3),
[13] S. Kraus, O. Shehory, and G. Taase. Coalition formation with uncertain heterogeneous information. In Proc. of AAMAS "03, pages 1-8, 2003. [14] K. Lermann and O. Shehory. Coalition formation for large scale electronic markets. In Proc. of ICMAS"2000, pages 216-222, Boston, 2000. [15] S. A. Lippman and J. J. McCall. The economics of job search: A survey. Economic Inquiry, 14:155-189, 1976. [16] E. Manisterski, D. Sarne, and S. Kraus. Integrating parallel interactions into cooperative search. In AAMAS, pages 257-264, 2006. [17] J. McMillan and M. Rothschild. Search. In R. Aumann and S. Hart, editors, Handbook of Game Theory with Economic Applications, pages 905-927. 1994. [18] J. M. McNamara and E. J. Collins. The job search problem as an employer-candidate game. Journal of Applied Probability, 27(4):815-827, 1990. [19] P. Morgan. Search and optimal sample size. Review of Economic Studies, 50(4):659-675, 1983. [20] A. Rubinstein. Perfect equilibrium in a bargaining model.
Econometrica, 50(1):97-109, 1982. [21] D. Sarne and S. Kraus. Agents strategies for the dual parallel search in partnership formation applications. In Proc. of AMEC2004, LNCS 3435, pages 158 - 172, 2004. [22] R. Shimer and L. Smith. Assortative matching and search.
Econometrica, 68(2):343-370, 2000. [23] K. Sycara, S. Widoff, M. Klusch, and J. Lu. Larks: Dynamic matchmaking among heterogeneous software agents in cyberspace. JAAMAS, 5:173-203, 2002. [24] N. Tsvetovat, K. Sycara, Y. Chen, and J. Ying. Customer coalitions in electronic markets. In Proc. of AMEC2000, pages 121-138, 2000.

The practical constraints of many application environments require distributed management of executing plans and schedules. Such factors as geographical separation of executing agents, limitations on communication bandwidth, constraints relating to chain of command and the high tempo of execution dynamics may all preclude any single agent from obtaining a complete global view of the problem, and hence necessitate collaborative yet localized planning and scheduling decisions. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency (fall-back) options.
Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework.
In a flexible-times representation of an agent"s schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. This representation allows the explicit use of slack as a hedge against simple forms of executional uncertainty (e.g., activity durations), and its underlying implementation as a Simple Temporal Network (STN) model provides efficient updating and consistency enforcement mechanisms. The advantages of flexible times frameworks have been demonstrated in various centralized planning and scheduling contexts (e.g., [12, 8, 9, 10, 11]). However their use in distributed problem solving settings has been quite sparse ([7] is one exception), and prior approaches to multi-agent scheduling (e.g., [6, 13, 5]) have generally operated with fixed-times representations of agent schedules.
We define an agent architecture centered around incremental management of a flexible times schedule. The underlying STN-based representation is used (1) to loosen the coupling between executor and scheduler threads, (2) to retain a basic ability to absorb unexpected executional delays (or speedups), and (3) to provide a basic criterion for detecting the need for schedule change. Local change is ac484 978-81-904262-7-5 (RPS) c 2007 IFAAMAS Figure 1: A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change.
To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process (similar in some respects to [5]), aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. This latter process uses analysis of detected conflicts in the STN as a basis for generating options.
The remainder of the paper is organized as follows. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans.
As indicated above the distributed schedule management problem that we address in this paper is that put forth by the DARPA Coordinators program. The Coordinators problem is concerned generally with the collaborative execution of a joint mission by a team of agents in a highly dynamic environment. A mission is formulated as a network of tasks, which are distributed among the agents by the MASS simulator such that no agent has a complete, objective view of the whole problem. Instead, each agent receives only a subjective view containing just the portion of the task network that relates to ground tasks that it is responsible for and any remote tasks that have interdependencies with these local tasks. A pre-computed initial schedule is also distributed to the agents, and each agent"s schedule indicates which of its local tasks should be executed and when. Each task has an associated quality value which accrues if it is successfully executed within its constraints, and the overall goal is to maximize the quality obtained during execution.
Figure 2: Subjective view for Agent 2.
As execution proceeds, agents must react to unexpected results (e.g., task delays, failures) and changes to the mission (e.g., new tasks, deadline changes) generated by the simulator, recognize when scheduled tasks are no longer feasible or desirable, and coordinate with each other to take corrective, quality-maximizing rescheduling actions that keep execution of the overall mission moving forward.
Problems are formally specified using a version of the TAEMS language (Task Analysis, Environment Modeling and Simulation) [4] called C TAEMS [1]. Within C TAEMS, tasks are represented hierarchically, as shown in the example in Figure 1. At the highest, most abstract level, the root of the tree is a special task called the task group.
On successive levels, tasks constitute aggregate activities, which can be decomposed into sets of subtasks and/or primitive activities, termed methods. Methods appear at the leaf level of C TAEMS task structures and are those that are directly executable in the world. Each declared method m can only be executed by a specified agent (denoted by ag : AgentN in Figure 1) and each agent can be executing at most one method at any given time (i.e. agents are unit-capacity resources). Method durations and quality are typically specified as discrete probability distributions, and hence known with certainty only after they have been executed.1 It is also possible for a method to fail unexpectedly in execution, in which case the reported quality is zero.
For each task, a quality accumulation function qaf is defined, which specifies when and how a task accumulates quality as its subtasks (methods) are executed. For example, a task with a min qaf will accrue the quality of its child with lowest quality if all its children execute and accumulate positive quality. Tasks with sum or max qafs acquire quality as soon as one child executes with positive quality; as their qaf names suggest, their respective values ultimately will be the total or maximum quality of all children that executed. A sync-sum task will accrue quality only for those children that commence execution concurrently with the first child that executes, while an exactly-one task accrues quality only if precisely one of its children executes.
Inter-dependencies between tasks/methods in the problem are modeled via non-local effects (nles). Two types of nles can be specified: hard and soft. Hard nles express 1 For simplicity, Figures 1 and 2 show only fixed values for method quality and duration.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 485 causal preconditions: for example, the enables nle in Figure 1 stipulates that the target method M5 can not be executed until the source M4 accumulates quality. Soft nles, which include facilitates and hinders, are not required constraints; however, when they are in play, they amplify (or dampen) the quality and duration of the target task.
Any given task or method a can also be constrained by an earliest start time and a deadline, specifying the window in which a can be feasibly executed. a may also inherit these constraints from ancestor tasks at any higher level in the task structure, and its effective execution window will be defined by the tightest of these constraints.
Figure 1 shows the complete objective view of a simple 2 agent problem. Figure 2 shows the subjective view available to agent 2 for the same problem. In what follows, we will sometimes use the term activity to refer generically to both task and method nodes.
Our solution framework combines two basic principles for coping with the problem of managing multi-agent schedules in an uncertain and time stressed execution environment.
First is the use of a STN-based flexible times representation of solution constraints, which allows execution to be driven by a set of schedules rather than a single point solution. This provides a basic hedge against temporal uncertainty and can be used to modulate the need for solution revision. The second principle is to first respond locally to exceptional events, and then, as time permits, explore nonlocal options (i.e., options involving change by 2 or more agents) for global solution improvement. This provides a means for keeping pace with execution, and for tying the amount of effort spent in more global multi-agent solution improvement to the time available. Both local and non-local problem solving time is further minimized by the use of a core incremental scheduling procedure.
Figure 3: Agent Architecture.
Our solution framework is made concrete in the agent architecture depicted in Figure 3. In its most basic form, an agent comprises four principal components - an Executor, a Scheduler, a Distributed State Manager (DSM), and an Options Manager - all of which share a common model of the current problem and solution state that couples a domainlevel representation of the subjective c taems task structure to an underlying STN. At any point during operation, the currently installed schedule dictates the timing and sequence of domain-level activities that will be initiated by the agent.
The Executor, running in its own thread, continually monitors the enabling conditions of various pending activities, and activates the next pending activity as soon as all of its causal and temporal constraints are satisfied.
When execution results are received back from the environment (MASS) and/or changes to assumed external constraints are received from other agents, the agent"s model of current state is updated. In cases where this update leads to inconsistency in the STN or it is otherwise recognized that the current local schedule might now be improved, the Scheduler, running on a separate thread, is invoked to revise the current solution and install a new schedule. Whenever local schedule constraints change either in response to a current state update or through manipulation by the Scheduler, the DSM is invoked to communicate these changes to interested agents (i.e., those agents that share dependencies and have overlapping subjective views).
After responding locally to a given state update and communicating consequences, the agent will use any remaining computation time to explore possibilities for improvement through joint change. The Option Manager utilizes the Scheduler (in this case in hypothetical mode) to generate one or more non-local options, i.e., identifying changes to the schedule of one or more other agents that will enable the local agent to raise the quality of its schedule. These options are formulated and communicated as queries to the appropriate remote agents, who in turn hypothetically evaluate the impact of proposed changes from their local perspective. In those cases where global improvement is verified, joint changes are committed to.
In the following sections we consider the mechanics of these components in more detail.
As indicated above, our agent scheduler operates incrementally. Incremental scheduling frameworks are ideally suited for domains requiring tight scheduler-execution coupling: rather than recomputing a new schedule in response to every change, they respond quickly to execution events by localizing changes and making adjustments to the current schedule to accommodate the event. There is an inherent bias toward schedule stability which provides better support for the continuity in execution. This latter property is also advantageous in multi-agent settings, since solution stability tends to minimize the ripple across different agents" schedules.
The coupling of incremental scheduling with flexible times scheduling adds additional leverage in an uncertain, multiagent execution environment. As mentioned earlier, slack can be used as a hedge against uncertain method execution times. It also provides a basis for softening the impact of inter-dependencies across agents.
In this section, we summarize the core scheduler that we have developed to solve the Coordinators problem. In subsequent sections we discuss its use in managing execution and coordinating with other agents.
To maintain the range of admissible values for the start and end times of various methods in a given agent"s sched486 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ule, all problem and scheduling constraints impacting these times are encoded in an underlying Simple Temporal Network (STN)[3]. An STN represents temporal constraints as a graph G < N, E >, where nodes in N represent the set of time points of interest, and edges in E are distances between pairs of time points in N. A special time point, called calendar zero grounds the network and has the value
duration) and relationships between activities (e.g. parentchild relation, enables) are uniformly represented as temporal constraints (i.e., edges) between relevant start and finish time points. An agent"s schedule is designated as a total ordering of selected methods by posting precedence constraints between the end and start points of each ordered pair. As new methods are inserted into a schedule or external state updates require adjustments to existing constraints (e.g., substitution of an actual duration constraint, tightening of a deadline), the network propagates constraints and maintains lower and upper bounds on all time points in the network. This is accomplished efficiently via the use of a standard all-pairs shortest path algorithm; in our implementation, we take advantage of an incremental procedure based on [2]. As bounds are updated, a consistency check is made for the presence of negative cycles, and the absence of any such cycle ensures the continued temporal feasibility of the network (and hence the schedule). Otherwise a conflict has been detected, and some amount of constraint retraction is necessary to restore feasibility.
The scheduler consists of two basic components: a quality propagator and an activity allocator that work in a tightly integrated loop. The quality propagator analyzes the activity hierarchy and collects a set of methods that (if scheduled) would maximize the quality of the agent"s local problem.
The methods are collected without regard for resource contention; in essence, the quality propagator optimally solves a relaxed problem where agents are capable of performing an infinite number of activities at once. The allocator selects methods from this list and attempts to install them in the agent"s schedule. Failure to do so reinvokes the quality propagator with the problematic activity excluded.
The Quality Propagator - The quality propagator performs the following actions on the C TAEMS task structure: • Computes the quality of all activities in the task structure: The expected quality qual(m) of a method m is computed from the probability distribution of the execution outcomes. The quality qual(t) of a task t is computed by applying its qaf to the assessed quality of its children. • Generates a list of contributors for each task: methods that, if scheduled, will maximize the quality obtained by the task. • Generates a list of activators for each task: methods that, if scheduled, are sufficient to qualify the task as scheduled. Methods in the activators list are chosen to minimize demands on the agent"s timeline without regard to quality.
The first time the quality propagator is invoked, the qualities of all tasks and methods are calculated and the initial lists of contributors and activators are determined.
Subsequent calls to the propagator occur as the allocator installs methods on the agent"s timeline: failure of the allocator to install a method causes the propagator to recompute a new list of contributors and activators.
The Activity Allocator - The activity allocator seeks to install the contributors of the taskgroup identified by the quality propagator onto the agent"s timeline. Any currently scheduled methods that do not appear in the contributors list are first unscheduled and removed from the timeline. The contributors are then preprocessed using a quality-centric heuristic to create an agenda sorted in decreasing quality order. In addition, methods associated with a and task (i.e., min, sumand) are grouped consecutively within the agenda. Since an and task accumulates quality only if all its children are scheduled, this biases the scheduling process towards failing early (and regenerating contributors) when the methods chosen for the and cannot together be allocated.
The allocator iteratively pops the first method mnew from the agenda and attempts to install it. This entails first checking that all activities that enable mnew have been scheduled, while attempting to install any enabler that is not. If any of the enabler activities fails to install, the allocation pass fails. When successful, the enables constraints linking the enabler activities to mnew are activated. The STN rejects an infeasible enabler constraint by returning a conflict.
In this event any enabler activities it has scheduled are uninstalled and the allocator returns failure. Once scheduling of enablers is ensured, a feasible slot on the agent"s timeline within mnew"s time window is sought and the allocator attempts to insert mnew between two currently scheduled methods. At the STN level, mnew"s insertion breaks the sequencing constraint between the two extant timeline methods and attempts to insert two new sequencing constraints that chain mnew to these methods. If these insertions succeed, the routine returns success, otherwise the two extant timeline methods are relinked and allocation attempts the next possible slot for mnew insertion.
Maintaining a flexible-times schedule enables us to use a conflict-driven approach to schedule repair: Rather than reacting to every event in the execution that may impact the existing schedule by computing an updated solution, the STN can absorb any change that does not cause a conflict.
Consequently, computation (producing a new schedule) and communication costs (informing other agents of changes that affect them) are minimized.
One basic mechanism needed to model execution in the STN is a dynamic model for current time. We employ a model proposed by [7] that establishes a ‘current-time" time point and includes a link between it and the calendar-zero time point. As each method is scheduled, a simple precedence constraint between the current-time time point and the method is established. When the scheduler receives a current time update, the link between calendar-zero and current-time is modified to reflect this new time, and the constraint propagates to all scheduled methods.
A second issue concerns synchronization between the executor and the scheduler, as producer and consumer of the schedule running on different threads within a given agent.
This coordination must be robust despite the fact that the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 487 executor needs to start methods for execution in real-time even while the scheduler may be reassessing the schedule to maximize quality, and/or transmitting a revised schedule.
If the executor, for example, slates a method for execution based on current time while the scheduler is instantiating a revised schedule in which that method is no longer nextto-be-executed, an inconsistent state may arise within the agent architecture. This is addressed in part by introducing a freeze window; a specified short (and adjustable) time period beyond current time within which any activity slated as eligible to start in the current schedule cannot be rescheduled by the scheduler.
The scheduler is triggered in response to various environmental messages. There are two types of environmental message classes that we discuss here as execution dynamics: 1) feedback as a result of method execution - both the agent"s own and that of other agents, and 2) changes in the C TAEMS model corresponding to a set of simulatordirected evolutions of the problem and environment. Such messages are termed updates and are treated by the scheduler as directives to permanently modify parameters in its model. We discuss these update types in turn here and defer until later the discussion of queries to the scheduler, a "what-if" mode initiated by a remote agent that is pursuing higher global quality.
Whether it is invoked via an update or a query, the scheduler"s response is an option; essentially a complete schedule of activities the agent can execute along with associated quality metrics. We define a local option as a valid schedule for an agent"s activities, which does not require change to any other agent"s schedule. The overarching design for handling execution dynamics aims at anytime scheduling behavior in which a local option maximizing the local view of quality is returned quickly, possibly followed by globally higher quality schedules that entail inter-agent coordination if available scheduler cycles permit. As such, the default scheduling mode for updates is to seek the highest quality local option according to the scheduler"s search strategy, instantiate the option as its current schedule, and notify the executor of the revision.
As suggested earlier, a committed schedule consists of a sequence of methods, each with a designated [est, lst] start time window (as provided by the underlying STN representation). The executor is free to execute a method any time within its start time window, once any additional enabling conditions have been confirmed. These scheduled start time windows are established using the expected duration of each scheduled method (derived from associated method duration distributions during schedule construction). Of course as execution unfolds, actual method durations may deviate from these expectations. In these cases, the flexibility retained in the schedule can be used to absorb some of this unpredictability and modulate invocation of a schedule revision process.
Consider the case of a method completion message, one of the environmental messages that could be communicated to the scheduler as an execution state update. If the completion time is coincident with the expected duration (i.e., it completes exactly as expected), then the scheduler"s response is to simply mark it as ‘completed" and the agent can proceed to communicate the time at which it has accumulated quality to any remote agents linked to this method.
However if the method completes with a duration shorter than expected a rescheduling action might be warranted.
The posting of the actual duration in the STN introduces no potential for conflict in this case, either with the latest start times (lsts) of local or remote methods that depend on this method as an enabler, or to successively scheduled methods on the agent"s timeline. However, it may present a possibility for exploiting the unanticipated scheduling slack.
The flexible times representation afforded by the STN provides a quick means of assessing whether the next method on the timeline can begin immediate execution instead of waiting for its previously established earliest start time (est).
If indeed the est of the next scheduled method can spring back to current-time once the actual duration constraint is substituted for the expected duration constraint, then the schedule can be left intact and simply communicated back to the executor. If alternatively, other problem constraints prevent this relaxation of the est, then there is forced idle time that may be exploited by revising the schedule, and the scheduler is invoked (always respecting the freeze period).
If the method completes later than expected, then there is no need for rescheduling under flexible times scheduling unless 1) the method finishes later than the lst of the subsequent scheduled activity, or 2) it finishes later than its deadline. Thus we only invoke the scheduler if, upon posting the late finish in the STN, a constraint violation occurs.
In the latter case no quality is accrued and rescheduling is mandated even if there are no conflicts with subsequent scheduled activities.
Other execution status updates the agent may receive include: • method start - If a method sent for execution is started within its [est, lst] window, the response is to mark it as "executing". A method cannot start earlier than when it is transmitted by the executor but it is possible for it to start later than requested. If the posted start time causes an inconsistency in the STN (e.g. because the expected method duration can no longer be accommodated) the duration constraint in the STN is shortened based on the known distribution until either consistency is restored or rescheduling is mandated. • method failure - Any method under execution may fail unexpectedly, garnering no quality for the agent. At this point rescheduling is mandated as the method may enable other activities or significantly impact quality in the absence of local repair. Again, the executor will proceed with execution of the next method if its start time arrives before the revised schedule is committed, and the scheduler accommodates this by respecting the freeze window. • current time advances An update on "current time" may arrive either alone or as part of any of the previously discussed updates. If, when updating the currenttime link in the STN (as described above), a conflict results, the execution state is inconsistent with the schedule. In this case, the scheduler proceeds as if execution were consistent with its expectations, subject to possible later updates. 488 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
The agent can also dynamically receive changes to the agent"s underlying C TAEMS model. Dynamic revisions in the outcome distributions for methods already in an agent"s subjective view may impact the assessed quality and/or duration values that shaped the current schedule. Similarly, dynamic revisions in the designated release times and deadlines for methods and tasks already in an agent"s subjective view can invalidate an extant schedule or present opportunities to boost quality. It is also possible during execution to receive updates in which new methods and possibly entire task structures are given to the agent for inclusion in its subjective view. Model changes that involve temporal constraints are handled in much the same fashion as described for method starts and completions, i.e, rescheduling is required only when the posting of the revised constraints leads to an STN conflict. In the case of non-temporal model changes, rescheduling action is currently always initiated.
Having responded locally to an unexpected execution result or model change, it is necessary to communicate the consequences to agents with inter-dependent activities so that they can align their decisions accordingly. Responses that look good locally may have a sub-optimal global effect once alignments are made, and hence agents must have the ability to seek mutually beneficial joint schedule changes.
In this section we summarize the coordination mechanisms provided in the agent architecture to address these issues.
A basic means of coordination with other agents is provided by the Distributed State Mechanism (DSM), which is responsible for communicating changes made to the model or schedule of a given agent to other interested agents.
More specifically, the DSM of a given agent acts to push any changes made to the time bounds, quality, or status of a local task/method to all the other agents that have that same task/method as a remote node in their subjective views. A recipient agent treats any communicated changes as additional forms of updates, in this case an update that modifies the current constraints associated with non-local (but inter-dependent) tasks or methods. These changes are handled identically to updates reflecting schedule execution results, potentially triggering the local scheduler if the need to reschedule is detected.
As mentioned in the previous section, the agent"s first response to any given query or update (either from execution or from another agent) is to generate one or more local options. Such options represent local schedule changes that are consistent with all currently known constraints originating from other agents" schedules, and hence can be implemented without interaction with other agents. In many cases, however, a larger-scoped change to the schedules of two or more agents can produce a higher-quality response.
Exploration of opportunities for such coordinated action by two or more agents is the responsibility of the Options Manager. Running in lower priority mode than the Executor and Scheduler, the Options Manager initiates a non-local option generation and evaluation process in response to any local schedule change made by the agent if computation time constraints permits. Generally speaking, a non-local option identifies certain relaxations (to one or more constraints imposed by methods that are scheduled by one or more remote agents) that enable the generation of a higher quality local schedule. When found, a non-local option is used by a coordinating agent to formulate queries to any other involved agents in order to determine the impact of such constraint relaxations on their local schedules. If the combined quality change reported back from a set of one or more relevant queries is a net gain, then the issuing agent signals to the other involved agents to commit to this joint set of schedule changes. The Option Manager currently employs two basic search strategies for generating non-local options, each exploiting the local scheduler in hypothetical mode.
Optimistic Synchronization - Optimistic synchronization is a non-local option generation strategy where search is used to explore the impact on quality if optimistic assumptions are made about currently unscheduled remote enablers. More specifically, the strategy looks for would be contributor methods that are currently unscheduled due to the fact that one or more remote enabling (source) tasks or methods are not currently scheduled. For each such local method, the set of remote enablers are hypothetically activated, and the scheduler attempts to construct a new local schedule under these optimistic assumptions. If successful, a non-local option is generated, specifying the value of the new, higher quality local schedule, the temporal constraints on the local target activity, and the set of must-schedule enabler activities that must be scheduled by remote agents in order to achieve this local quality. The needed queries requesting the quality impact of scheduling these activities are then formulated and sent to the relevant remote agents.
To illustrate, consider again the example in Figure 1. The maximum quality that Agent1 can contribute to the task group is 15 (by scheduling M1, M2 and M3). Assume that this is Agent1"s current schedule. Given this state, the maximum quality that Agent2 can contribute to the task group is 10, and the total task group quality would then be 15 + 10 = 25. Using optimistic synchronization, Agent2 will generate a non-local option that indicates that if M5 becomes enabled, both M5 and M6 would be scheduled, and the quality contributed by Agent2 to the task group would become 30. Agent2 sends a must schedule M4 query to Agent1. Because of the time window constraints, Agent1 must remove M3 from its schedule to get M4 on, resulting in a new lower quality schedule of 5. However, when Agent2 receives this option response from Agent1, it determines that the total quality accumulated for the task group would be 5 + 30 = 35, a net gain of 10. Hence, Agent 2 signals to Agent1 to commit to this non-local option.
Conflict-Driven Relaxation - A second strategy for generating non-local options, referred to as Conflict-Directed Relaxation, utilizes analysis of STN conflicts to identify and prioritize external constraints to relax in the event that a particular method that would increase local quality is found to be unschedulable. Recall that if a method cannot be feasibly inserted into the schedule, an attempt to do so will generate a negative cycle. Given this cycle, the mechanism proceeds in three steps. First, the constraints involved in the cycle are collected. Second, by virtue of the connections in the STN to the domain-level C TAEMS model, this set is filtered to identify the subset associated with remote nodes.
Third, constraints in this subset are selectively retracted to The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 489 Figure 4: A high quality task is added to the task structure of Agent2.
Figure 5: If M4, M5 and M7 are scheduled, a conflict is detected by the STN. determine if STN consistency is restored. If successful, a non-local option is generated indicating which remote constraint(s) must be relaxed and by how much to allow installation of the new, higher quality local schedule.
To illustrate this strategy, consider Figure 5 where Agent1 has M1, M2 and M4 on its timeline, and therefore est(M4) =
31 (M6 could be scheduled before or after M5). Suppose that Agent2 receives a new task M7 with deadline 55 (see Figure 4). If Agent2 could schedule M7, the quality contributed by Agent2 to the task group would be 70.
However, an attempt to schedule M7 together with M5 and M6 leads to a conflict, since the est(M7) = 46, dur(M7) = 10 and lft(M7) = 55 (see Figure 5). Conflict-directed relaxation by Agent 2 suggests relaxing the lft(M4) by 1 tick to 30, and this query is communicated to Agent 1. In fact, by retracting either method M1 or M2 from the schedule this relaxation can be accommodated with no quality loss to Agent1 (due to the min qaf). Upon communication of this fact Agent 2 signals to commit.
An initial version of the agent described in this paper was developed in collaboration with SRI International and subjected to the independently conducted Coordinators programmatic evaluation. This evaluation involved over 2000 problem instances randomly generated by a scenario generator that was configured to produce scenarios of varying Problem Class Description Agent Class Quality OD ‘Only Dynamics". No NLEs. 97.9% (390 probs) Actual task duration & quality vary according to distribution.
INT ‘Interdependent". Frequent & 100% (360 probs) random (esp. facilitates) CHAINS Activities chained together 99.5% (360 probs) via sequences of enables NLEs (1-4 chains/prob) TT ‘Temporal Tightness". Release - 94.9% (360 probs) Deadline windows preclude preferred high quality (longest duration) tasks from all being scheduled.
SYNC Problems contain range of 97.1% (360 probs) different Sync sum tasks NTA ‘New Task Arrival". cTaems 99.0% (360 probs) model is augmented with new tasks dynamically during run.
OVERALL Avg: 98.1% (2190 probs) Std dev: 6.96 Table 1: Performance of year 1 agent over Coordinators evaluation. ‘Agent Quality" is % of ‘optimal" durations within six experiment classes. These classes, summarized in Table 1, were designed to evaluate key aspects of a set of Coordinators distributed scheduling agents, such as their ability to handle unexpected execution results, chains of nle"s involving multiple agents, and effective scheduling of new activities that arise unexpectedly at some point during the problem run. Year 1 evaluation problems were constrained to be small enough (3 -10 agents, 50 - 100 methods) such that comparison against an optimal centralized solver was feasible. The evaluation team employed an MDP-based solver capable of unrolling the entire search space for these problems, choosing for an agent at each execution decision point the activity most likely to produce maximum global quality. This established a challenging benchmark for the distributed agent systems to compare against. The hardware configuration used by the evaluators instantiated and ran one agent per machine, dedicating a separate machine to the MASS simulator.
As reported in Table 1, the year 1 prototype agent clearly compares favorably to the benchmark on all classes, coming within 2% of the MDP optimal averaged over the entire set of 2190 problems. These results are particularly notable given that each agent"s STN-based scheduler does very little reasoning over the success probability of the activity sequences it selects to execute. Only simple tactics were adopted to explicitly address such uncertainty, such as the use of expected durations and quality for activities and a policy of excluding from consideration those activities with failure likelihood of >75%. The very respectable agent performance can be at least partially credited to the fact that the flexible times representation employed by the scheduler affords it an important buffer against the uncertainty of execution and exogenous events.
The agent turns in its lowest performance on the TT (Temporal Tightness) experiment classes, and an examination of the agent trace logs reveals possible reasons. In about half of the TT problems the year 1 agent under-performs on, the specified time windows within which an agent"s ac490 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) tivities must be scheduled are so tight that any scheduled activity which executes with a longer duration than the expected value, causes a deadline failure. This constitutes a case where more sophisticated reasoning over success probability would benefit this agent. The other half of underperforming TT problems involve activities that depend on facilitation relationships in order to fit in their time windows (recall that facilitation increases quality and decreases duration). The limited facilitates reasoning performed by the year 1 scheduler sometimes causes failures to install a heavily facilitated initial schedule. Even when such activities are successfully installed they tend to be prone to deadline failures -If a source-side activity(s) either fails or exceeds its expected duration the resulting longer duration of the target activity can violate its time window deadline.
Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods. This scale places much higher computational demands on all of the agent"s components. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems.
To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms.
To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling [7] to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules.
The Year 1 agent architecture was developed in collaboration with Andrew Agno, Roger Mailler and Regis Vincent of SRI International. This paper is based on work supported by the Department of Defense Advance Research Projects Agency (DARPA) under Contract # FA8750-05-C0033. Any opinions findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA.
[1] M. Boddy, B. Horling, J. Phelps, R. Goldman,
R. Vincent, A. Long, and B. Kohout. C taems language specification v. 1.06, October 2005. [2] A. Cesta and A. Oddi. Gaining efficiency and flexibility in the simple temporal problem. In Proc. 3rd Int. Workshop on Temporal Representation and Reasoning, Key West FL, May 1996. [3] R. Dechter, I. Meiri, and J. Pearl. Temporal constraint networks. Artificial Intelligence, 49:61-95, May 1991. [4] K. Decker. TÆMS: A framework for environment centered analysis & design of coordination mechanisms. In G. O"Hare and N. Jennings, editors,
Foundations of Distributed Artificial Intelligence, chapter 16, pages 429-448. Wiley Inter-Science, 1996. [5] K. Decker and V. Lesser. Designing a family of coordination algorithms. In Proc. 1st. Int. Conference on Multi-Agent Systems, San Francisco, 1995. [6] A. J. Garvey. Design-To-Time Real-Time Scheduling.
PhD thesis, Univ. of Massachusetts, Feb. 1996. [7] L. Hunsberger. Algorithms for a temporal decoupling problem in multi-agent planning. In Proc. 18th National Conference on AI, 2002. [8] S. Lemai and F. Ingrand. Interleaving temporal planning and execution in robotics domains. In Proc. 19th National Conference on AI, 2004. [9] N. Muscettola, P. P. Nayak, B. Pell, and B. C.
Williams. Remote agent: To boldly go where no AI system has gone before. Artificial Intelligence, 103(1-2):5-47, 1998. [10] W. Ruml, M. B. Do, and M. Fromherz. On-line planning and scheduling of high-speed manufacturing.
In Proc. ICAPS-05, Monterey, 2005. [11] I. Shu, R. Effinger, and B. Williams. Enabling fast flexible planning through incremental temporal reasoning with conflict extraction. In Proce.
ICAPS-05, Monterey, 2005. [12] S. Smith and C. Cheng. Slack-based heuristics for constraint satisfaction scheduling. In Proc. 12th National Conference on AI, Wash DC, July 1993. [13] T. Wagner, A. Garvey, and V. Lesser. Criteria-directed heuristic task scheduling. International Journal of Approximate Reasoning, 19(1):91-118, 1998.

Recent years have seen a lot of work on task and resource allocation methods, which can potentially be applied to many real-world applications. However, some interesting applications where relations between agents play a role require a slightly more general model. Such situations appear very frequently in real-world scenarios, and recent technological developments are bringing more of them within the range of task allocation methods. Especially in business applications, preferential partner selection and interaction is very common, and this aspect becomes more important for task allocation research, to the extent that technological developments need to be able to support it.
For example, the development of semantic web and grid technologies leads to increased and renewed attention for the potential of the web to support business processes [7, 15]. As an example, virtual organizations (VOs) are being re-invented in the context of the grid, where they are composed of a number of autonomous entities (representing different individuals, departments and organizations), each of which has a range of problem-solving capabilities and resources at its disposal [15, p. 237]. The question is how VOs are to be dynamically composed and re-composed from individual agents, when different tasks and subtasks need to be performed. This would be done by allocating them to different agents who may each be capable of performing different subsets of those tasks. Similarly, supply chain formation (SCF) is concerned with the, possibly ad-hoc, allocation of services to providers in the supply chain, in such a way that overall profit is optimized [6, 21].
Traditionally, such allocation decisions have been analyzed using transaction cost economics (TCE) [4], which takes the transaction between consecutive stages of development as its basic unit of analysis, and considers the firm and the market as alternative structural forms for organizing transactions. (Transaction cost) economics has traditionally built on analysis of comparative statics: the central problem of economic organization is considered to be the adaptation of organizational forms to the characteristics of transactions. More recently, TCE"s founding father, Ronald Coase, acknowledged that this is too simplistic an approach [5, p. 245]: The analysis cannot be confined to what happens within a single firm. (. . . ) What we are dealing with is a complex interrelated structure.
In this paper, we study the problem of task allocation from the perspective of such a complex interrelated structure. In particular, ‘the market" cannot be considered as an organizational form without considering specific partners to interact with on the market [11]. Specifically, therefore, we consider agents to be connected to each other in a social network. Furthermore, this network is not fully connected: as informed by the business literature, firms typically have established working relations with limited numbers of preferred partners [10]; these are the ones they consider when new tasks arrive and they have to form supply chains to allocate those tasks [19]. Other than modeling the interrelated 500 978-81-904262-7-5 (RPS) c 2007 IFAAMAS structure between business partners, the social network introduced in this paper can also be used to represent other types of connections or constraints among autonomous entities that arise from other application domains.
The next section gives a formal description of the task allocation problem on social networks. In Section 3, we prove that the complexity of this problem remains NP-hard. We then proceed to develop a distributed algorithm in Section 4, and perform a series of experiments with this algorithm, as described in Section 5. Section 6 discusses related work, and Section 7 concludes.
We formulate the social task allocation problem in this section. There is a set A of agents: A = {a1, . . . , am}.
Agents need resources to complete tasks. Let R = {r1, . . . , rk} denote the collection of the resource types available to the agents A. Each agent a ∈ A controls a fixed amount of resources for each resource type in R, which is defined by a resource function: rsc : A × R → N. Moreover, we assume agents are connected by a social network.
Definition 1 (Social network). An agent social network SN = (A, AE) is an undirected graph, where vertices A are agents, and each edge (ai, aj) ∈ AE indicates the existence of a social connection between agents ai and aj.
Suppose a set of tasks T = {t1, t2, . . . , tn} arrives at such an agent social network. Each task t ∈ T is then defined by a tuple u(t), rsc(t), loc(t) , where u(t) is the utility gained if task t is accomplished, and the resource function rsc : T ×R → N specifies the amount of resources required for the accomplishment of task t. Furthermore, a location function loc : T → A defines the locations (i.e., agents) at which the tasks arrive in the social network. An agent a that is the location of a task t, i.e. loc(t) = a, is called the manager of this task.
Each task t ∈ T needs some specific resources from the agents in order to complete the task. The exact assignment of tasks to agents is defined by a task allocation.
Definition 2 (Task allocation). Given a set of tasks T = {t1, . . . , tn} and a set of agents A = {a1, . . . , am} in a social network SN, a task allocation is a mapping φ : T × A × R → N. A valid task allocation in SN must satisfy the following constrains: • A task allocation must be correct. Each agent a ∈ A cannot use more than its available resources, i.e. for each r ∈ R,
P t∈T φ(t, a, r) ≤ rsc(a, r). • A task allocation must be complete. For each task t ∈ T , either all allocated agents" resources are sufficient, i.e. for each r ∈ R,
P a∈A φ(t, a, r) ≥ rsc(t, r), or t is not allocated, i.e. φ(t, ·, ·) = 0. • A task allocation must obey the social relationships.
Each task t ∈ T can only be allocated to agents that are (direct) neighbors of agent loc(t) in the social network SN. Each such agent that can contribute to a task is called a contractor.
We write Tφ to represent the tasks that are fully allocated in φ. The utility of φ is then the summation of the utilities of each task in Tφ, i.e., Uφ = P t∈Tφ u(t). Using this notation, we define the efficient task allocation below.
Definition 3 (Efficient task allocation). We say a task allocation φ is efficient if it is valid and Uφ is maximized, i.e., Uφ = max( P t∈Tφ u(t)).
We are now ready to define the task allocation problem in social network that we study in this paper.
Definition 4 (Social task allocation problem).
Given a set of agents A connected by a social network SN = (A, AE), and a finite set of tasks T , the social task allocation problem (or STAP for short) is the problem of finding the efficient task allocation φ, such that φ is valid and the social welfare Uφ is maximized.
The traditional task allocation problem, TAP (without the condition of the social network SN), is NP-complete [18], and the complexity comes from the fact that we need to evaluate the exponential number of subsets of the task set.
Although we may consider the TAP as a special case of the STAP by assuming agents are fully connected, we cannot directly use the complexity results from the TAP, since we study the STAP in an arbitrary social network, which, as we argued in the introduction, should be partially connected.
We now show that the TAP with an arbitrary social network is also NP-complete, even when the utility of each task is 1, and the quantity of all required and available resources is 1.
Theorem 1. Given the social task allocation problem with an arbitrary social network, as defined in Definition 4, the problem of deciding whether a task allocation φ with utility more than k exists is NP-complete.
Proof. We first show that the problem is in NP. Given an instance of the problem and an integer k, we can verify in polynomial time whether an allocation φ is a valid allocation and whether the utility of φ is greater than k.
We now prove that the STAP is NP-hard by showing that MAXIMUM INDEPENDENT SET ≤P STAP. Given an undirected graph G = (V, E) and an integer k, we construct a network G = (V , E ) which has an efficient task allocation with k tasks of utility 1 allocated if and only if G has an independent set (IS) of size k. av1 av3 ae3 rsc(ae1 ) = {e1} rsc(ae4 ) = {e4} av4 ae2 av2 ae4 ae1 rsc(ae2 ) = {e2}{e3} rsc(av3 ) = {v3} rsc(av4 ) = {v4} t1 = {v1, e1, e3} t2 = {v2, e1, e2} rsc(ae3 ) = rsc(av1 ) = {v1} rsc(av2 ) = {v2} t3 = {v3, e3, e4} t4 = {v4, e2, e4} e1 e2 e4 e3 v1 v2 v4v3 Figure 1: The MIS problem can be reduced to the STAP. The left figure is an undirected graph G, which has the optimal solution {v1, v4} or {v2, v3}; the right figure is the constructed instance of the STAP, where the optimal allocation is {t1, t4} or {t2, t3}.
An instance of the following construction is shown in Figure 1. For each node v ∈ V and each edge e ∈ E in the graph G, we create a vertex agent av and an edge agent ae in G .
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 501 When v was incident to e in G we correspondingly add an edge e in G between av and ae. We assign each agent in G one resource, which is related to the node or the edge in the graph G, i.e., for each v ∈ V , rsc(av) = {v} (here we write rsc(a) and rsc(t) to represent the set of resources available to/required by a and t), and for each e ∈ E, rsc(ae) = {e}.
Each vertex agent avi in G has a task ti that requires a set of neighboring resources ti = {vi} ∪ {e|e = (u, vi) ∈ E}.
There is no task on the edge agents in G . We define utility 1 for each task, and the quantity of all required and available resources to be 1.
Taken an instance of the IS problem, suppose there is a solution of size k, i.e., a subset N ⊆ V such that no two vertices in N are joined by an edge in E and |N| = k.
N specifies a set of vertex agents AN in the corresponding graph G . Given two agents a1, a2 ∈ AN we now know that there is no edge agent ae connected to both a1 and a2. Thus, for each agent a ∈ AN , a assigns its task to the edge agents which are connected to a. All other vertex agents a /∈ AN are not able to assign their tasks, since the required resources of the edge agents are already used by the agents a ∈ AN .
The set of tasks of the agents AN (|AN | = k) is thus the maximum set of tasks that can be allocated. The utility of this allocation is k.
Similarly, if there is a solution for the STAP with the utility value k, and the allocated task set is N, then for the IS problem, there exists a maximum independent set N of size k in G. An example can be found in Figure 1.
We just proved that the STAP is NP-hard for an arbitrary graph. In our proof, the complexity comes from the introduction of a social network. One may expect that the complexity of this problem can be reduced for some networks where the number of neighbors of the agents is bounded by a fixed constant. We now give a complexity result on this class of networks as follows.
Theorem 2. Let the number of neighbors of each agent in the social network SN be bounded by Δ for Δ ≥ 3.
Computing the efficient task allocation given such a network is NP-complete. In addition, it is not approximable within Δε for some ε > 0.
Proof. It has been shown in [2] that the maximum independent set problem in the case of the degree bounded by Δ for Δ ≥ 3 is NP-complete and is not approximable within Δε for some ε > 0. Using the similar reduction from the proof of Theorem 1, this result also holds for the STAP.
Since our problem is as hard as MIS as shown in Theorem 1, it is not possible to give a worst case bound better than Δε for any polynomial time algorithm, unless P = NP.
To deal with the problem of allocating tasks in a social network, we present a distributed algorithm. We introduce this algorithm by describing the protocol for the agents.
After that we give the optimal, centralized algorithm and an upper bound algorithm, which we use in Section 5 to benchmark the quality of our distributed algorithm.
We can summarize the description of the task allocation problem in social networks from Section 2 as follows. We Algorithm 1 Greedy distributed allocation protocol (GDAP).
Each manager a calculates the efficiency e(t) for each of their tasks t ∈ Ta, and then while Ta = ∅:
such that for each task t ∈ Ta: e(t ) ≤ e(t).
neighbors (of a) by informing these neighbors of the efficiency e(t) and the required resources for t.
offer all relevant resources to the manager for the task with the highest efficiency.
allocate their tasks, and inform each contractor which part of the offer is accepted. When a task is allocated, or when a manager has received offers from all neighbors, but still cannot satisfy its task, the task is removed from the task list Ta.
have a (social) network of agents. Each agent has a set of resources of different types at its disposal. We also have a set of tasks. Each task requires some resources, has a fixed benefit, and is located at a certain agent. This agent is called a manager. We only allow neighboring agents to help with a task. These agents are called contractors. Agents can fulfill the role of manager as well as contractor. The problem is to find out which tasks to execute, and which resources of which contractors to use for these tasks.
The idea of the protocol is as follows. All manager agents a ∈ A try to find neighboring contractors to help them with their task(s) Ta = {ti ∈ T | loc(ti) = a}. They start with offering the task that is most efficient in terms of the ratio between benefit and required resources. Out of all tasks offered, contractors select the task with the highest efficiency, and send a bid to the related manager. A bid consists of all the resources the agent is able to supply for this task. If sufficient resources have been offered, the manager selects the required resources and informs all contractors of its choice.
The efficiency of a task is defined as follows: Definition 5. The efficiency e of a task t ∈ T is defined by the utility of this task divided by the sum of all required resources: e(t) = u(t)P r∈R rsc(t,r) .
A more detailed description of this protocol can be found in Algorithm 1. Here it is also defined how to determine when a task should not be offered anymore, because it is impossible to fulfill locally. Obviously, a task is also not offered anymore when it has been allocated. This protocol is such that, when no two tasks have exactly the same efficiency, in every iteration at least one task is removed from a task list.1 From this the computation and communication property of the algorithm follows.
Proposition 1. For a STAP with n tasks and m agents, the run time of the distributed algorithm is O(nm), and the number of communication messages is O(n2 m). 1 Even when some tasks have the same efficiency, it is straightforward to make this result work. For example, the implementation can ensure that the contractors choose the task with the lowest task-id. 502 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 2 Optimal social task allocation (OPT).
Repeat the following for each combination of tasks:
any previous combination, test if this combination is feasible as follows:
r ∈ R (separately) as follows: (a) Create a source s and a sink s . (b) For each agent a ∈ A create an agent node and an edge from s to this node with capacity equal to the amount of resources of type r agent a has. (c) For each task t ∈ T create a task node and an edge from this node to s with capacity equal to the amount of resources of type r task T requires. (d) For each agent a connect the agent node to all task nodes of neighboring tasks, i.e., t ∈ {t ∈ T | (a, loc(t)) ∈ AE}. Give this connection unlimited capacity.
networks. If the maximum flow in each network is equal to the total required resources of that type, the current combination of tasks is feasible. In that case, this is the current best combination of tasks.
Proof. In the worst case, in each iteration exactly one task is removed from a task list, so there are n iterations.
In each iteration in the worst case (i.e., a fully connected network), for each of the O(n) managers, O(m) messages are sent. Next the task with the highest efficiency can be selected by each contractor in O(n). Assigning an allocation can be done in O(m). This leads to a total of O(n + m) operations for each iteration, and thus O(n2 + nm) operations in total. The number of messages sent is O(n(nm + nm + nm)) = O(n2 m).
We establish the quality of this protocol experimentally (in Section 5). Preferably, we compare the results to the optimal solution.
The optimal task allocation algorithm should deal with the restrictions posed by the social network. For this NPcomplete problem we used an exponential brute-force algorithm to consider relevant combinations of tasks to execute.
For each combination we use a maximum-flow algorithm to check whether the resources are sufficient for the selected subset of tasks. The flow network describes which resources can be used for which tasks, depending on the social network. If the maximum flow is equal to the sum of all resources required by the subset of tasks, we know that a feasible solution exists (see Algorithm 2). Clearly, we cannot expect this optimal algorithm to be able to find solutions for larger problem sizes. To establish the quality of our protocol for large instances, we use the following method to determine an upper bound.
Given a social task allocation problem, if the number of resource types for every task t ∈ T is bounded by 1, the Algorithm 3 An upper bound for social task allocation (UB).
Create a network flow problem with costs as follows:
create an agent-resource node ai, and an edge from s to this node with capacity equal to the amount of resources of type r agent a has available and with costs
create a task-resource node ti, and an edge from this node to s with capacity equal to the amount of resources of type r task t requires and costs −e(t).
connect the agent-resource node ai to all task-resource nodes ti for neighboring tasks t ∈ {t ∈ T | (a, loc(t)) ∈ AE or a = loc(t)}. Give this connection unlimited capacity and zero costs.
capacity and zero costs.
Solve the minimum cost flow network problem for this network. The costs of the resulting network is an upper bound for the social task allocation problem. problem is polynomially solvable by transforming it to a flow network problem. Our method for efficiently calculating an upper bound for STAP makes use of this special case by converting any given STAP instance P into a new problem P where each task only has one resource type.
More specifically, for every task t ∈ T with utility u(t), we do the following. Let m be the number of resource types {r1, . . . , rm} required by t. We then split t into a set of m tasks T = {t1, . . . , tm} where each task ti only has one unique resource type (of {r1, . . . , rm}) and each task has a fair share of the utility, i.e., the efficiency of t from Definition 5 times the amount of this resource type rsc(ti, ri).
After polynomially performing this conversion for every task in T , the original problem P becomes the special case P .
Note that the set of valid allocations in P is only a subset of the set of valid allocations in P , because it is now possible to partially allocate a task. From this it is easy to see that the solution of P gives an upper bound of the solution of the original problem P.
To compute the optimal solution for P , we transform it to a minimum cost flow problem. We model the cost in the flow network by the negation of the new task"s utility. A polynomial-time implementation of a scaling minimum cost flow algorithm [9] is used for the computation. The resulting minimum cost flow represents a maximum allocation of the tasks for P . The detailed modeling is described in Algorithm 3. In the next section, we use this upper bound to estimate the quality of the GDAP for large-scale instances.
We implemented the greedy distributed allocation protocol (GDAP), the optimal allocation algorithm (OPT), and the upper bound algorithm (UB) in Java, and tested them on a Linux PC. The purpose of these experiments is to study the performance of the distributed algorithm in different problem settings using different social networks. The perThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 503
1
Rewardrelativetooptimal Resource ratio small-world - upper bound random - upper bound scale-free - upper bound small-world - GDAP random - GDAP scale-free - GDAP Figure 2: The solution qualities of the GDAP and the upper bound depend on the resource ratio. 0 5 10 15 20 25 30 0 2 4 6 8 10 12 14 16 18 Numberofagents Degree small-world random scale-free Figure 3: The histogram of the degrees. formance measurements are the solution quality and computation time, where the solution quality (SQ) is computed as follows. When the number of tasks is small, we compare the output of the distributed algorithm with the optimal solution, i.e., SQ = GDAP OP T , but if it is not feasible to compute the optimal solution, we use the value returned by the upper bound algorithm for evaluation, i.e., SQ = GDAP UB .
To see whether the latter is a good measure, we also compare the quality of the upper bound to the optimal solution for smaller problems. In the following, we describe the setup of all experiments, and present the results.
We consider several experimental environments. In all environments the agents are connected by a social network. In the experiments, three different networks are used to simulate the social relationships among agents in potential realworld problems.
Small-world networks are networks where most neighbors of an agent are also connected to each other. For the experiments we use a method for generating random small-world networks proposed by Watts et al. [22], with a fixed rewiring probability p = 0.05.
Scale-free networks have the property that a couple of nodes have many connections, and many nodes have only a small number of connections. To generate these we use the implementation in the JUNG library of the generator proposed by Barab´asi and Albert [3].
We also generate random networks as follows. First we connect each agent to another agent such that all agents are connected. Next, we randomly add connections until the desired average degree has been reached.
We now describe the different settings used in our experiments with both small and large-scale problems.
Setting 1. The number of agents is 40, and the number of tasks is 20. The number of different resource types is bounded by 5, and the average number of resources required by a task is 30. Consequently, the total number of resources required by the tasks is fixed. However, the resources available to the agents are varied. We define the resource ratio to refer to the ratio between the total number of available resources and the total number of required resources. Resources are allocated uniformly to the agents. The average degrees of the networks may also change. In this setting the task benefits are distributed normally around the number of resources required.
Setting 2. This setting is similar to Setting 1, but here we let the benefits of the tasks vary dramatically-40% of the tasks have around 10 times higher benefit than the other 60% of the tasks.
Setting 3. This setting is for large-scale problems. The ratio between the number of agents and the number of tasks is set to 5/3, and the number of agents varies from 100 to 2000. We also fix the resource ratio to 1.2 and the average degree to 6. The number of different resource types is 20, and the average resource requirement of a tasks is 100. The task benefits are again normally distributed.
The experiments are done with the three different settings in the three different networks mentioned before, where each recorded data is the average over 20 random instances.
Experimental setting 1 is used for this set of experiments.
We would like to see how the GDAP behaves in the different networks when the number of resources available to the agents is changing. We also study the behavior of our upper bound algorithm. For this experiment we fix the average number of neighbors (degree) in each network type to six.
In Figure 2 we see how the quality of both the upper bound and the GDAP algorithm depends on the resource ratio. Remarkably, for lower resource ratios our GDAP is much closer to the optimal allocation than the upper bound.
When the resource ratio grows above 1.5, the graphs of the upper bound and the GDAP converge, meaning that both are really close to the optimal solution. This can be explained by the fact that when plenty of resources are available, all tasks can be allocated without any conflicts.
However, when resources are very scarce, the upper bound is much too optimistic, because it is based on the allocation of sub-tasks per resource type, and does not reason about how many of the tasks can actually be allocated completely. We also notice from the graph that the solution quality of the GDAP on all three networks is quite high (over 0.8) when the available resource is very limited (0.3). It drops below
plenty of resources available (resource ratio 0.9). Clearly, if 504 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
1
2 4 6 8 10 12 14 16 Rewardrelativetooptimal Degree small-world - upper bound random - upper bound scale-free - upper bound small-world - GDAP random - GDAP scale-free - GDAP Figure 4: The quality of the GDAP and the upper bound depend on the network degree. resources are really scarce, only a few tasks can be successfully allocated even by the optimal algorithm. Therefore, the GDAP is able to give quite a good allocation.
Although the differences are minor, it can also be seen that the results for the small-world network are consistently slightly better than those of random networks, which in turn outperform scale-free networks. This can be understood by looking at the distribution of the agents" degree, as shown in Figure 3. In this experiment, in the small-world network almost every manager has a degree of six. In random networks, the degree varies between one and about ten.
However, in the scale-free network, most nodes have only three or four connections, and only a very few have up to twenty connections. As we will see in the next experiment, having more connections means getting better results.
For the next experiment we fix the resource ratio to 1.0 and study the quality of both the upper bound and the GDAP algorithm related to the degree of the social network. The result can be found in Figure 4. In this figure we can see that a high average degree also leads to convergence of the upper bound and the GDAP. Obviously, when managers have many connections, it becomes easier to allocate tasks. An exception is, similar to what we have seen in Figure 2, that the solution of the GDAP is also very good if the connections are extremely limited (degree 2), due to the fact that the number of possibly allocated tasks is very small. Again we see that the upper bound is not that good for problems where resources are hard to reach, i.e. in social networks with a low average degree.2 Since the solution quality clearly depends on the resource ratio as well as on the degree of the social network, we study the effect of changing both, to see whether they influence each other. Figure 5 shows how the solution quality depends on both the resource ratio and the network degree.
This graph confirms the results that the GDAP performs better for problems with higher degree and higher resource ratio. However, it is now also more clear that it performs better for very low degree and resource availability. For this experiment with 40 agents and 20 tasks, the worst performance is met for instances with degree six and resource ratio
But even for those instances, the performance lies above 0.7. 2 The consistent standard deviation of about 15% over the 20 problem instances is not displayed as error-bars in these first graphs, because it would obfuscate the interesting correlations that can now be seen. 4 6 8 10 12 14 16 0.4 0.6 0.8 1 1.2 1.4 1.6
1 Relative reward small-world Average degree Resource ratio Relative reward Figure 5: The quality of the GDAP depends on both the resource ratio and the network degree.
To study the robustness of the GDAP against different problem settings, we generate instances where the task benefit distribution is different: 40% of the tasks gets a 10 times higher benefit (as described in Setting 2). The effect of this different distribution can be seen in Figure 6. These two graphs show that the results for the skewed task benefit distribution are slightly better on average, both when varying the resource ratio, and when varying the average degree of the network. We argue that this can be explained by the greedy nature of GDAP, which causes the tasks with high efficiency to be allocated first, and makes the algorithm perform better in this heterogeneous setting.
The purpose of this final experiment is to test whether the algorithm can be scaled to large problems, like applications running on the internet. We therefore generate instances where the number of agents varies from 100 to 2000, and simultaneously increase the number of tasks from 166 to 3333 (Setting 3). Figure 7 shows the run time for these instances on a Linux machine with an AMD Opteron 2.4 GHz processor. These graphs confirm the theoretical analysis from the previous section, saying that both the upper bound and the GDAP are polynomial. In fact, the graphs show that the GDAP almost behaves linearly. Here we see that the locality of the GDAP really helps in reducing the computation time. Also note that the GDAP requires even less computation time than the upper bound.
The quality of the GDAP for these large instances cannot be compared to the optimal solution. Therefore, in Figure 8 the upper bound is used instead. This result shows that the GDAP behaves stably and consistently well with the increasing problem size. It also shows once more that the GDAP performs better in a small-world network.
Task allocation in multiagent systems has been investigated by many researchers in recent years with different assumptions and emphases. However, most of the research to date on task allocation does not consider social connections among agents, and studies the problem in a centralized The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 505
1
Rewardrelativetooptimal Resource ratio skewed small-world skewed random skewed scale-free uniform small-world uniform random uniform scale-free
1 2 4 6 8 10 12 14 16 Rewardrelativetooptimal Degree skewed small-world skewed random skewed scale-free uniform small-world uniform random uniform scale-free Figure 6: The quality of the GDAP algorithm for a uniform and a skewed task benefit distribution related to the resource ratio (the first graph), and the network degree (the second graph). setting. For example, Kraus et al. [12] develop an auction protocol that enables agents to form coalitions with time constraints. It assumes each agent knows the capabilities of all others. The proposed protocol is centralized, where one manager is responsible for allocating the tasks to all coalitions. Manisterski at al. [14] discuss the possibilities of achieving efficient allocations in both cooperative and noncooperative settings. They propose a centralized algorithm to find the optimal solution. In contrast to this work, we introduce also an efficient completely distributed protocol that takes the social network into account.
Task allocation has also been studied in distributed settings by for example Shehory and Kraus [18] and by Lerman and Shehory [13]. They propose distributed algorithms with low communication complexity for forming coalitions in large-scale multiagent systems. However, they do not assume the existence of any agent network. The work of Sander et al. [16] introduces computational geometry-based algorithms for distributed task allocation in geographical domains. Agents are then allowed to move and actively search for tasks, and the capability of agents to perform tasks is homogeneous. In order to apply their approach, agents need to have some knowledge about the geographical positions of tasks and some other agents. Other work [17] proposes a location mechanism for open multiagent systems to allocate tasks to unknown agents. In this approach each agent caches a list of agents they know. The analysis of the communication complexity of this method is based on lattice-like graphs, while we investigate how to efficiently solve task allocation in a social network, whose topology can be arbitrary.
Networks have been employed in the context of task allocation in some other works as well, for example to limit the 0 1000 2000 3000 4000 5000 6000 7000 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Time(ms) Agents upper bound - small-world upper bound - random upper bound - scale-free GDAP - small-world GDAP - random GDAP - scale-free Figure 7: The run time of the GDAP algorithm.
1 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Rewardrelativetoupperbound Agents small-world random scale-free Figure 8: The quality of the GDAP algorithm compared to the upper bound. interactions between agents and mediators [1]. Mediators in this context are agents who receive the task and have connections to other agents. They break up the task into subtasks, and negotiate with other agents to obtain commitments to execute these subtasks. Their focus is on modeling the decision process of just a single mediator. Another approach is to partition the network into cliques of nodes, representing coalitions which the agents involved may use as a coordination mechanism [20]. The focus of that work is distributed coalition formation among agents, but in our approach, we do not need agents to form groups before allocating tasks.
Easwaran and Pitt [6] study ‘complex tasks" that require ‘services" for their accomplishment. The problem concerns the allocation of subtasks to service providers in a supply chain. Another study of task allocation in supply chains is [21], where it is argued that the defining characteristic of Supply Chain Formation is hierarchical subtask decomposition (HSD). HSD is implemented using task dependency networks (TDN), with agents and goods as nodes, and I/O relations between them as edges. Here, the network is given, and the problem is to select a subgraph, for which the authors propose a market-based algorithm, in particular, a series of auctions. Compared to these works, our approach is more general in the sense that we are able to model different types of connections or constraints among agents for different problem domains in addition to supply chain formation.
Finally, social networks have been used in the context of team formation. Previous work has shown how to learn which relations are more beneficial in the long run [8], and adapt the social network accordingly. We believe these results can be transferred to the domain of task allocation as well, leaving this as a topic for further study. 506 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
In this paper we studied the task allocation problem in a social network (STAP), which can be seen as a new, more general, variant of the TAP. We believe it has a great amount of potential for realistic problems. We provided complexity results on computing the efficient solution for the STAP, as well as a bound on possible approximation algorithms. Next, we presented a distributed protocol, related to the contractnet protocol. We also introduced an exponential algorithm to compute the optimal solution, as well as a fast upperbound algorithm. Finally, we used the optimal solution and the upper-bound (for larger instances) to conduct an extensive set of experiments to assess the solution quality and the computational efficiency of the proposed distributed algorithm in different types of networks, namely, small-world networks, random networks, and scale-free networks.
The results presented in this paper show that the distributed algorithm performs well in small-world, scale-free, and random networks, and for many different settings. Also other experiments were done (e.g. on grid networks) and these results held up over a wider range of scenarios.
Furthermore, we showed that it scales well to large networks, both in terms of quality and of required computation time.
The results also suggest that small-world networks are slightly better suited for local task allocation, because there are no nodes with very few neighbors.
There are many interesting extensions to our current work.
In this paper, we focus on the computational aspect in the design of the distributed algorithm. In our future work, we would also like to address some of the related issues in game theory, such as strategic agents, and show desirable properties of a distributed protocol in such a context.
In the current algorithm we assume that agents can only contact their neighbors to request resources, which may explain why our algorithm performs not as good in the scalefree networks as in the small-world networks. Our future work may allow agents to reallocate (sub)tasks. We are interested in seeing how such interactions will affect the performance of task allocation in different social networks.
A third interesting topic for further work is the addition of reputation information among the agents. This may help to model changing business relations and incentivize agents to follow the protocol.
Finally, it would be interesting to study real-life instances of the social task allocation problem, and see how they relate to the randomly generated networks of different types studied in this paper.
Acknowledgments. This work is supported by the Technology Foundation STW, applied science division of NWO, and the Ministry of Economic Affairs.
[1] S. Abdallah and V. Lesser. Modeling Task Allocation Using a Decision Theoretic Model. In Proc. AAMAS, pages 719-726. ACM, 2005. [2] N. Alon, U. Feige, A. Wigderson, and D. Zuckerman.
Derandomized Graph Products. Computational Complexity, 5(1):60-75, 1995. [3] A.-L. Barab´asi and R. Albert. Emergence of scaling in random networks. Science, 286(5439):509-512, 1999. [4] R. H. Coase. The Nature of the Firm. Economica NS, 4(16):386-405, 1937. [5] R. H. Coase. My Evolution as an Economist. In W. Breit and R. W. Spencer, editors, Lives of the Laureates, pages 227-249. MIT Press, 1995. [6] A. M. Easwaran and J. Pitt. Supply Chain Formation in Open, Market-Based Multi-Agent Systems.
International J. of Computational Intelligence and Applications, 2(3):349-363, 2002. [7] I. Foster, N. R. Jennings, and C. Kesselman. Brain Meets Brawn: Why Grid and Agents Need Each Other. In Proc. AAMAS, pages 8-15, Washington,
DC, USA, 2004. IEEE Computer Society. [8] M. E. Gaston and M. desJardins. Agent-organized networks for dynamic team formation. In Proc.
AAMAS, pages 230-237, New York, NY, USA, 2005.
ACM Press. [9] A. Goldberg. An Efficient Implementation of a Scaling Minimum-Cost Flow Algorithm. J. of Algorithms, 22:1-29, 1997. [10] R. Gulati. Does Familiarity Breed Trust? The Implications of Repeated Ties for Contractual Choice in Alliances. Academy of Management Journal, 38(1):85-112, 1995. [11] T. Klos and B. Nooteboom. Agent-based Computational Transaction Cost Economics.
Economic Dynamics and Control, 25(3-4):503-526, 01. [12] S. Kraus, O. Shehory, and G. Taase. Coalition formation with uncertain heterogeneous information.
In Proc. AAMAS, pages 1-8. ACM, 2003. [13] K. Lerman and O. Shehory. Coalition formation for large-scale electronic markets. In Proc. ICMAS, pages 167-174. IEEE Computer Society, 2000. [14] E. Manisterski, E. David, S. Kraus, and N. Jennings.
Forming Efficient Agent Groups for Completing Complex Tasks. In Proc. AAMAS, pages 257-264.
ACM, 2006. [15] J. Patel et al. Agent-Based Virtual Organizations for the Grid. Multi-Agent and Grid Systems, 1(4):237-249, 2005. [16] P. V. Sander, D. Peleshchuk, and B. J. Grosz. A scalable, distributed algorithm for efficient task allocation. In Proc. AAMAS, pages 1191-1198, New York, NY, USA, 2002. ACM Press. [17] O. Shehory. A scalable agent location mechanism. In Proc. ATAL, volume 1757 of LNCS, pages 162-172.
Springer, 2000. [18] O. Shehory and S. Kraus. Methods for Task Allocation via Agent Coalition Formation. Artificial Intelligence, 101(1-2):165-200, 1998. [19] R. M. Sreenath and M. P. Singh. Agent-based service selection. Web Semantics, 1(3):261-279, 2004. [20] P. T. Toˇsi´c and G. A. Agha. Maximal Clique Based Distributed Coalition Formation for Task Allocation in Large-Scale Multi-Agent Systems. In Proc. MMAS, volume 3446 of LNAI, pages 104-120. Springer, 2005. [21] W. E. Walsh and M. P. Wellman. Modeling Supply Chain Formation in Multiagent Systems. In Proc.
AMEC II, volume 1788 of LNAI, pages 94-101.
Springer, 2000. [22] D. J. Watts and S. H. Strogatz. Collective dynamics of ‘small world" networks. Nature, 393:440-442, 1998.

In this paper, we are interested in knowledge representation formalisms for systems in which agents need to aggregate their preferences, judgments, beliefs, etc. For example, an agent may need to reason about majority voting in a group he is a member of.
Preference aggregation - combining individuals" preference relations over some set of alternatives into a preference relation which represents the joint preferences of the group by so-called social welfare functions - has been extensively studied in social choice theory [2]. The recently emerging field of judgment aggregation studies aggregation from a logical perspective, and discusses how, given a consistent set of logical formulae for each agent, representing the agent"s beliefs or judgments, we can aggregate these to a single consistent set of formulae. A variety of judgment aggregation rules have been developed to this end. As a special case, judgment aggregation can be seen to subsume preference aggregation [5].
In this paper we present a logic, called Judgment Aggregation Logic (jal), for reasoning about judgment aggregation. The formulae of the logic are interpreted as statements about judgment aggregation rules, and we give a sound and complete axiomatisation of all such rules. The axiomatisation is parameterised in such a way that we can instantiate it to get a range of different judgment aggregation logics. For example, one instance is an axiomatisation, in our language, of all social welfare functions - thus we get a logic of classical preference aggregation as well. And this is one of the main contributions of this paper: we identify the logical properties of judgment aggregation, and we can compare the logical properties of different classes of judgment aggregation - and of general judgment aggregation and preference aggregation in particular.
Of course, a logic is only interesting as long as it is expressive. One of the goals of this paper is to investigate the representational and logical capabilities an agent needs for judgment and preference aggregation; that is, what kind of logical language might be used to represent and reason about judgment aggregation? An agent"s knowledge representation language should be able to express: common aggregation rules such as majority voting; commonly discussed properties of judgment aggregation rules and social welfare functions such as independence; paradoxes commonly used to illustrate judgment aggregation and preference aggregation, viz. the discursive paradox and Condorcet"s paradox respectively; and other important properties such as Arrow"s theorem. In order to illustrate in more detail what such a language would need to be able to express, take the example of a potential property of social welfare functions (SWFs) called independence of irrelevant alternatives (IIA): given two preference profiles (each consisting of one preference relation for each agent) and two alternatives, if for each agent the two alternatives have the same order in the two preference profiles, then the two alternatives must have the same order in the two preference relations resulting from applying the SWF to the two preference profiles, respectively. From this example it seems that a formal language for SWFs should be able to express: 566 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Quantification on several levels: over alternatives; over preference profiles, i.e., over relations over alternatives (secondorder quantification); and over agents. • Properties of preference relations for different agents, and properties of several different preference relations for the same agent in the same formula. • Comparison of different preference relations. • The preference relation resulting from applying a SWF to other preference relations.
From these points it might seem that such a language would be rather complex (in particular, these requirements seem to rule out a standard propositional modal logic). Perhaps surprisingly, the language of jal is syntactically and semantically rather simple; and yet the language is, nevertheless, expressive enough to give elegant and succinct expressions of, e.g., IIA, majority voting, the discursive dilemma, Condorcet"s paradox and Arrow"s theorem. This means, for example, that Arrow"s theorem is a formal theorem of jal, i.e., a derivable formula; we thus have a formal proof theory for social choice.
The structure of the rest of the paper is as follows. In the next section we review the basics of judgment aggregation as well as preference aggregation, and mention some commonly discussed properties of judgment aggregation rules and social welfare functions. In Section 3 we introduce the syntax and semantics of jal, and study the complexity of the model checking problem.
Formulae of jal are interpreted directly by, and thus represent properties of, judgment aggregation rules. In Section 4 we demonstrate that the logic can express commonly discussed properties of judgment aggregation rules, such as the discursive paradox. We give a sound and complete axiomatisation of the logic in Section 5, under the assumption that the agenda the agents make judgments over is finite.
As mentioned above, preference aggregation can be seen as a special case of judgment aggregation, and in Section 6 we introduce an alternative interpretation of jal formulae directly in social welfare functions. We obtain a sound and complete axiomatisation of the logic for preference aggregation as well. Sections 7 and 8 discusses related work and concludes.
AGGREGATION Judgment aggregation is concerned with judgment aggregation rules aggregating sets of logical formulae; preference aggregation is concerned with social welfare functions aggregating preferences over some set of alternatives. Let n be a number of agents; we write Σ for the set {1, . . . , n}.
Let L be a logic with language L(L). We require that the language has negation and material implication, with the usual semantics. We will sometimes refer to L as the underlying logic. An agenda over L is a non-empty set A ⊆ L(L), where for every formula φ that does not start with a negation, φ ∈ A iff ¬φ ∈ A. We sometimes call a member of A an agenda item. A subset A ⊆ A is consistent unless A entails both ¬φ and φ in L for some φ ∈ L(L); A is complete if either φ ∈ A or ¬φ ∈ A for every φ ∈ A which does not start with negation. An (admissible) individual judgment set is a complete and consistent subset Ai ⊆ A of the agenda. The idea here is that a judgment set Ai represents the choices from A made by agent i. Two rationality criteria demand that an agents" choices at least be internally consistent, and that each agent makes a decision between every item and its negation. An (admissible) judgment profile is an n-tuple A1, . . . , An , where Ai is the individual judgment set of agent i. J(A, L) denotes the set of all individual (complete and L-consistent) judgment sets over A, and J(A, L)n the set of all judgment profiles over A. When γ ∈ J(A, L)n , we use γi to denote the ith element of γ, i.e., agent i"s individual judgment set in judgment profile γ.
A judgment aggregation rule (JAR) is a function f that maps each judgment profile A1, . . . , An to a complete and consistent collective judgment set f(A1, . . . , An) ∈ J(A, L). Such a rule hence is a recipe to enforce a rational group decision, given an tuple of rational choices by the individual agents. Of course, such a rule should to a certain extent be ‘fair". Some possible properties of a judgment aggregation rule f over an agenda A: Non-dictatorship (ND1) There is no agent i such that for every judgment profile A1, . . . , An , f(A1, . . . , An) = Ai Independence (IND) For any p ∈ A and judgment profiles A1, . . . , An and B1, . . . , Bn , if for all agents i (p ∈ Ai iff p ∈ Bi), then p ∈ f(A1, . . . , An) iff p ∈ f(B1, . . . , Bn) Unanimity (UNA) For any judgment profile A1, . . . , An and any p ∈ A, if p ∈ Ai for all agents i, then p ∈ f(A1, . . . , An)
Social welfare functions (SWFs) are usually defined in terms of ordinal preference structures, rather than cardinal structures such as utility functions. An SWF takes a preference relation, a binary relation over some set of alternatives, for each agent, and outputs another preference relation representing the aggregated preferences.
The most well known result about SWFs is Arrow"s theorem [1].
Many variants of the theorem appear in the literature, differing in assumptions about the preference relations. In this paper, we take the assumption that all preference relations are linear orders, i.e., that neither agents nor the aggregated preference can be indifferent between distinct alternatives. This gives one of the simplest formulations of Arrow"s theorem (Theorem 1 below). Cf., e.g., [2] for a discussion and more general formulations.
Formally, let K be a set of alternatives. We henceforth implicitly assume that there are always at least two alternatives. A preference relation (over K) is, here, a total (linear) order on K, i.e., a relation R over K which is antisymmetric (i.e., (a, b) ∈ R and (b, a) ∈ R implies that a = b), transitive (i.e., (a, b) ∈ R and (b, c) ∈ R implies that (a, c) ∈ R), and total (i.e., either (a, b) ∈ R or (b, a) ∈ R). We sometimes use the infix notation aRb for (a, b) ∈ R. The set of preference relations over alternatives K is denoted L(K). Alternatively, we can view L(K) as the set of all permutations of K. Thus, we shall sometimes use a permutation of K to denote a member of L(K). For example, when K = {a, b, c}, we will sometimes use the expression acb to denote the relation {(a, c), (a, b), (c, b), (a, a), (b, b), (c, c)}. aRb means that b is preferred over a if a and b are different. Rs denotes the irreflexive version of R, i.e., Rs = R \ {(a, a) : a ∈ K}. aRs b means that b is preferred over a and that a b.
A preference profile for Σ over alternatives K is a tuple (R1, . . . , Rn) ∈ L(K)n , consisting of one preference relation Ri for each agent i. A social welfare function (SWF) is a function F : L(K)n → L(K) mapping each preference profile to an aggregated preference relation. The class of all SWFs over alternatives K is denoted F (K).
Properties of SWFs F corresponding to the judgment aggregation rule properties discussed in Section 2.1 are: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 567 Non-dictatorship (ND2) ¬∃i∈Σ∀(R1, . . . , Rn) ∈ L(K)n F(R1, . . . , Rn) = Ri (corresponds to ND1) Independence of irrelevant alternatives (IIA) ∀(R1, . . . , Rn) ∈ L(K)n ∀(S1, . . . , Sn) ∈ L(K)n ∀a ∈ K∀b ∈ K((∀i ∈ Σ(aRib ⇔ aSib)) ⇒ (aF(R1, . . . , Rn)b ⇔ aF(S1, . . . , Sn)b)) (corresponds to IND) Pareto Optimality (PO) ∀(R1, . . . , Rn) ∈ L(K)n ∀a ∈ K∀b ∈ K ((∀i ∈ ΣaRs i b) ⇒aF(R1, . . . , Rn)s b) (corresponds to UNA) Arrow"s theorem says that the three properties above are inconsistent if there are more than two alternatives.
Theorem 1 (Arrow). If there are more than two alternatives, no SWF has all the properties PO, ND2 and IIA.
SYNTAX AND SEMANTICS The language of Judgment Aggregation Logic (jal) is parameterised by a set of agents Σ = {1, 2, . . . , n} (we will assume that there are at least two agents) and an agenda A. The following atomic propositions are used: Π = {i, σ, hp | p ∈ A, i ∈ Σ} The language L(Σ, A) of jal is defined by the following grammar: φ ::= α | φ | φ | φ ∧ φ | ¬φ where α ∈ Π. This language will be formally interpreted in structures consisting of an agenda item, a judgment profile and a judgment aggregation function; informally, i means that the agenda item is in agent i"s judgment set in the current judgment profile; σ means that the agenda item is in the aggregated judgment set of the current judgment profile; hp means that the agenda item is p; φ means that φ is true in every judgment profile; φ means that φ is true in every agenda item.
We define ψ = ¬ ¬ψ, intuitively meaning ψ is true for some judgment profile, and ψ = ¬ ¬ψ, intuitively meaning ψ is true for some agenda item, as usual, in addition to the usual derived propositional connectives.
We now define the formal semantics of L(Σ, A). A model wrt.
L(Σ, A) and underlying logic L is a judgment aggregation rule f over A. Recall that J(A, L)n denotes the set of complete and Lconsistent judgment profiles over A. A table is a tuple T = f, γ, p such that f is a model, γ ∈ J(A, L)n and p ∈ A. A formula is interpreted on a table as follows. f, γ, p |=L hq ⇔ p = q f, γ, p |=L i ⇔ p ∈ γi f, γ, p |=L σ ⇔ p ∈ f(γ) f, γ, p |=L ψ ⇔ ∀γ ∈ J(A, L)n f, γ , p |=L ψ f, γ, p |=L ψ ⇔ ∀p ∈ A f, γ, p |=L ψ f, γ, p |=L φ ∧ ψ ⇔ f, γ, p |=L φ and f, γ, p |=L ψ f, γ, p |=L ¬φ ⇔ f, γ, p |=L φ So, e.g., we have that f, γ, p |=L i∈Σ i if everybody chooses p in γ.
Example 1. A committee of three agents are voting on the following three propositions: the candidate is qualified (p), if the candidate is qualified he will get an offer (p → q), and the candidate will get an offer (q). One possible voting scenario is illustrated in the left part of Table 1. In the table, the results of proposition-wise majority voting, i.e., the JAR fmaj accepting a proposition iff it is accepted by a majority of the agents, are also p p → q q 1 yes yes yes 2 no yes yes 3 yes no no fmaj yes yes yes 1 mdc 2 mcd 3 cmd Fmaj mcd Table 1: Examples shown. This example can be modelled by taking the agenda to be A = {p, p → q, q, ¬p, ¬(p → q), ¬q} (recall that agendas are closed under single negation) and L to be propositional logic. The agents" votes can be modelled by the following judgment profile: γ = γ1, γ2, γ3 , where γ1 = {p, p → q, q}, γ2 = {¬p, p → q, q}, γ3 = {p, ¬(p → q), ¬q}. We then have that: • fmaj, γ, p |=L 1 ∧ ¬2 ∧ 3 (agents 1 and 3 judges p to be true in the profile γ, while agent 2 does not) • fmaj, γ, p |=L σ (majority voting on p given the preference profile γ leads to acceptance of p) • fmaj, γ, p |=L (1 ∧ 2) (agents 1 and 2 agree on some agenda item, under the judgment profile γ. Note that this formula does not depend on which agenda item is on the table.) • fmaj, γ, p |=L ((1 ↔ 2) ∧ (2 ↔ 3) ∧ (1 ↔ 3)) (there is some judgment profile on which all agents agree on p. Note that this formula does not depend on which judgment profile is on the table.) • fmaj, γ, p |=L ((1 ↔ 2) ∧ (2 ↔ 3) ∧ (1 ↔ 3)) (there is some judgment profile on which all agents agree on all agenda items. Note that this formula does not depend on any of the elements on the table.) • fmaj, γ, p |=L σ ↔ G⊆{1,2,3},|G|≥2 i∈G i (the JAR fmaj implements majority voting) We write f |=L φ iff f, γ, p |=L φ for every γ over A and p ∈ A; |=L φ iff f |=L φ for all models f. Given a possible property of a JAR, such as, e.g., independence, we say that a formula expresses the property if the formula is true in an aggregation rule f iff f has the property.
Note that when we are given a formula φ ∈ L(Σ, A), validity, i.e., |=L φ, is defined with respect to models of the particular language L(Σ, A) defined over the particular agenda A (and similar for validity with respect to a JAR, i.e., f |=L φ). The agenda, like the set of agents Σ, is given when we define the language, and is thus implicit in the interpretation of the language1 .
Let an outcome o be a maximal conjunction of literals (¬)1, . . . , (¬)n. The set O is the set of all possible outcomes. Note that the decision of the society is not incorporated here: an outcome only collects votes of agents from Σ.
Model checking is currently one of the most active areas of research with respect to reasoning in modal logics [4], and it is natural to investigate the complexity of this problem for judgment aggregation logic. Intuitively, the model checking problem for judgment aggregation logic is as follows: Given f, γ, p and formula φ of jal, is it the case that f, γ, p |= φ or not? 1 Likewise, in classical modal logic the language is parameterised with a set of primitive propositions, and validity is defined with respect to all models with valuations over that particular set. 568 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) While this problem is easy to understand mathematically, it presents some difficulties if we want to analyse it from a computational point of view. Specifically, the problem lies in the representation of the judgment aggregation rule, f. Recall that this function maps judgment profiles to complete and consistent judgment sets. A JAR must be defined for all judgment profiles over some agenda, i.e., it must produce an output for all these possible inputs. But how are we to represent such a rule? The simplest representation of a function f : X → Y is as the set of ordered pairs {(x, y) | x ∈ X & y = f(x)}. However, this is not a feasible representation for JARs, as there will be exponentially many judgment profiles in the size of the agenda, and so the representation would be unfeasibly large in practice. If we did assume this representation for JARs, then it is not hard to see that model checking for our logic would be decidable in polynomial time: the naive algorithm, derivable from semantics, serves this purpose.
However, we emphasise that this result is of no practical significance, since it assumes an unreasonable representation for models - a representation that simply could not be used in practice for examples of anything other than trivial size.
So, what is a more realistic representation for JARs? Let us say a representation Rf of a JAR f is reasonable if: (i) the size of Rf is polynomial in the size of the agenda; and (ii) there is a polynomial time algorithm A, which takes as input a representation Rf and a judgment profile γ, and produces as output f(γ). There are, of course, many such representations Rf for JARs f. Here, we will look at one very general one: where the JAR is represented as a polynomially bounded two-tape Turing machine Tf , which takes on its first tape a judgment profile, and writes on its second tape the resulting judgment set. The requirement that the Turing machine should be polynomially bounded roughly corresponds to the requirement that a JAR is reasonable to compute; if there is some JAR that cannot be represented by such a machine, then it is arguably of little value, since it could not be used in practice2 . With such a representation, we can investigate the complexity of our model checking problem.
In modal logics, the usual source of complexity, over and above the classical logic connectives, is the modal operators. With respect to judgment aggregation logic, the operator quantifies over all judgment profiles, and hence over all consistent subsets of the agenda. It follows that this is a rather powerful operator: as we will see, it can be used as an np oracle [9, p.339]. In contrast, the operator quantifies over members of the agenda, and is hence much weaker, from a computational perspective (we can think of it as a conjunction over elements of the agenda).
The power of the quantifier suggests that the complexity of model checking judgment aggregation logic over relatively succinct representations of JAR is going to be relatively high; we now prove that the complexity of model checking judgment aggregation logic is as hard as solving a polynomial number of np-hard problems [9, pp.424-429].
Theorem 2. The model checking problem for judgment aggregation logic, assuming the representation of JARs described above, is Δp 2-hard; it is np-hard even if the formula to be checked is of the form ψ, where ψ contains no further or operators.
Proof. For Δp 2-hardness, we reduce snsat (sequentially nested 2 Of course, we have no general way of checking whether any given Turing machine is guaranteed to terminate in polynomial time; the problem is undecidable. As a consequence, we cannot always check whether a particular Turing machine representation of a JAR meets our requirements. However, this does not prevent specific JARs being so represented, with corresponding proofs that they terminate in polynomial time. satisfiability). An instance is given by a series of equations of the form z1 = ∃X1.φ1(X1) z2 = ∃X2.φ2(X2, z1) z3 = ∃X3.φ3(X3, z1, z2) . . . zk = ∃Xk.φk(Xk, z1, . . . , zk−1) where X1, . . . , Xk are disjoint sets of variables, and each φi(Y) is a propositional logic formula over the variables Y; the idea is we first check whether φ1(X1) is satisfiable, and if it is, we assign z1 the value true, otherwise assign it false; we then check whether φ2 is satisfiable under the assumption that z1 takes the value just derived, and so on. Thus the result of each equation depends on the value of the previous one. The goal is to determine whether zk is true.
To reduce this problem to judgment aggregation logic model checking, we first fix the JAR: this rule simply copies whatever agent 1"s judgment set is. (Clearly this can be implemented by a polynomially bounded Turing machine.) The agenda is assumed to contain the variables X1 ∪ · · · ∪ Xk ∪ {z1, . . . , zk} and their negations.
We fix the initial judgment profile γ to be X1 ∪· · ·∪Xk ∪{z1, . . . , zk}, and fix p = x1. Given a variable xi, define x∗ i to be (hxi ∧1). If φi is one of the formulae φ1, . . . , φk, define φ∗ i to be the formula obtained from φi by systematically substituting x∗ i for each variable xi and z∗ i similarly.
Now, we define the function ξi for natural numbers i > 0 as: ξk = z∗ 1 ↔ (φ∗ 1) if i = 1 z∗ i ↔ (φ∗ i ∧i−1 j=1 ξj) otherwise.
And we define the formula to be model checked as: φ∗ k ∧k−1 j=1 ξj It is now straightforward from construction that this formula is true under the interpretation iff zk is true in the snsat instance. The proof of the latter half of the theorem is immediate from the special case where k = 1.
We have thus defined a language which can be used to express properties of judgment aggregation rules. An interesting question is then: what are the universal properties of aggregation rules expressible in the language; which formulae are valid? Here, in order to illustrate the logic, we discuss some of these logical properties.
In Section 5 we give a complete axiomatisation of all of them.
Recall that we defined the set O of outcomes as the set of all conjunctions with exactly one, possibly negated, atom from Σ. Let P = {o ∧ σ, o ∧ ¬σ : o ∈ O}; p ∈ P completely describes the decisions of the agents and the aggregation function. Let denote exclusive or.
We have that: |=L p∈Pp - any agent and the JAR always have to make a decision |=L (i ∧ ¬j) → ¬i - if some agent can think differently about an item than i does, then also i can change his mind about it. In fact this principle can be strengthened to |=L ( i ∧ ¬j) → (¬i ∧ j) |=L x - for any x ∈ {i, ¬i, σ, ¬σ : i ∈ Σ} - both the individual agents and the JAR will always judge some agenda item to be true, and conversely, some agenda item to be false |=L (i ∧ j) - there exist admissible judgment sets such that agents i and j agree on some judgment. |=L (i ↔ j) - there exist admissible judgment sets such that agents i and j always agree.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 569 The interpretation of formulae depends on the agenda A and the underlying logic L, in the quantification over the set J(A, L)n of admissible, e.g., complete and L-consistent, judgment profiles. Note that this means that some jal formula might be valid under one underlying logic, while not under another. For example, if the agenda contains some formula which is inconsistent in the underlying logic (and, by implication, some tautology), then the following hold: |=L (i ∧ σ) - for every judgment profile, there is some agenda item (take a tautology) which both agent i and the JAR judges to be true But this property does not hold when every agenda item is consistent with respect to the underlying logic. One such agenda and underlying logic will be discussed in Section 6.
Non-dictatorship can be expressed as follows: ND = i∈Σ ¬(σ ↔ i) (1) Lemma 1. f |=L ND iff f has the property ND1.
Independence can be expressed as follows: IND = o∈O ((o ∧ σ) → (o → σ)) (2) Lemma 2. f |=L IND iff f has the property IND.
Unanimity can be expressed as follows: UNA = ((1 ∧ · · · ∧ n) → σ) (3) Lemma 3. f |=L UNA iff f has the property UNA.
As illustrated in Example 1, the following formula expresses proposition-wise majority voting over some proposition p MV = σ ↔ G⊆Σ,|G|> n 2 i∈G i (4) i.e., the following property of a JAR f and admissible profile A1, . . . , An : p ∈ f(A1, . . . , An) ⇔ |{i : p ∈ Ai}| > |{i : p Ai}| f |= MV exactly iff f has the above property for all judgment profiles and propositions.
However, we have the following in our logic. Assume that the agenda contains at least two distinct formulae and their material implication (i.e., A contains p, q, p → q for some p, q ∈ L(L)).
Proposition 1 (Discursive Paradox). |=L (( MV) → ⊥) when there are at least three agents and the agenda contains at least two distinct formulae and their material implication.
Proof. Assume the opposite, e.g., that A = {p, p → q, q, ¬p, ¬(p → q), ¬q, . . .} and there exists an aggregation rule f over A such that f |=L (σ ↔ G⊆Σ,|G|> n 2 i∈G i). Let γ be the judgment profile γ = A1, A2, A3 where A1 = {p, p → q, q, . . .}, A2 = {p, ¬(p → q), ¬q, . . .} and A3 = {¬p, p → q, ¬q, . . .}. We have that f, γ, p |=L (σ ↔ G⊆Σ,|G|> n 2 i∈G i) for any p , so f, γ, p |=L σ ↔ G⊆Σ,|G|> n 2 i∈G i. Because f, γ, p |=L 1 ∧ 2, it follows that f, γ, p |=L σ. In a similar manner it follows that f, γ, p → q |=L σ and f, γ, q |=L ¬σ. In other words, p ∈ f(γ), p → q ∈ f(γ) and q f(γ). Since f(γ) is complete, ¬q ∈ f(γ). But that contradicts the fact that f(γ) is required to be consistent.
Proposition 1 is a logical statement of a variant of the well-known discursive dilemma: if three agents are voting on propositions p, q and p → q, proposition-wise majority voting might not yield a consistent result.
Given an underlying logic L, a finite agenda A over L, and a set of agents Σ, Judgment Aggregation Logic (jal(L), or just jal when L is understood) for the language L(Σ, A), is defined in Table 2. ¬(hp ∧ hq) if p q Atmost p∈A hp Atleast hp p ∈ A Agenda (hp ∧ ϕ) → (hp → ϕ) Once (hp ∧ x) ∨ (hp ∧ x) CpJS all instantiations of propositional tautologies taut (ψ1 → ψ2) → ( ψ1 → ψ2) K ψ → ψ T ψ → ψ 4 ¬ ψ → ¬ ψ 5 ( i ∧ ¬j) → o∈O o C ψ ↔ ψ (COMM) From p1, . . . pn L q infer (hp1 ∧ x) ∧ · · · ∧ (hpn ∧ x) → (hq → x) ∧ (hq → ¬x) Closure From ϕ → ψ and ϕ infer ψ MP From ψ infer ψ Nec Table 2: The logic jal(L) for the language L(Σ, A). p, pi, q range over the agenda A; φ,ψ,ψi over L(Σ, A); x over {σ, i : i ∈ Σ}; over { , }; i, j over Σ; o over the set of outcomes O. hp means hq when p = ¬q for some q, otherwise it means h¬p. L is the underlying logic.
The first 5 axioms represent properties of a table and of judgment sets. Axiom Atmost says that there is at most one item on the table at a time, and Atleast says that we always have an item on the table.
Axiom Agenda says that every agenda item will appear on the table, whereas Once says that every item of the agenda only appears on the table once. Note that a conjunction hp ∧ x reads: item p is on the agenda, and x is in favour of it, or x judges it true. Axiom CpJS corresponds to the requirement that judgment sets are complete.
Note that from Agenda, CsJS and CpJS we derive the scheme x ∧ ¬x, which says that everybody should at least express one opinion in favour of something, and against something.
The axioms taut − 5 are well familiar from modal logic: they directly reflect the unrestricted quantification in the truth definition of and . Axiom C says that for any agenda item for which it is possible to have opposing opinions, every possible outcome for that item should be achievable. COMM says that everything that is true for an arbitrary profile and item, is also true for an arbitrary item and profile. Closure guarantees that agents behave consistently with respect to consequence in the logic L. MP and Nec are standard. We use JAL(L) to denote derivability in jal(L).
Theorem 3. If the agenda is finite, we have that for any formula ψ ∈ L(Σ, A), JAL(L) ψ iff |=L ψ.
Proof. Soundness is straightforward. For completeness (we focus on the main idea here and leave out trivial details), we build a 570 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) jal table for a consistent formula ψ as follows. In fact, our axiomatisation completely determines a table, except for the behaviour of f. To be more precise, let a table description be a conjunction of the form hp ∧ o ∧ (¬)σ. It is easy to see that table descriptions are mutually exclusive, and, moreover, we can derive τ∈T τ, where T is the set of all table descriptions. Let D be the set of all maximal consistent sets Δ. We don"t want all of those: it might well be that ψ requires σ to be in a certain way, which is incompatible with some Δ"s. We define two accessibility relations in the standard way: R Δ1Δ2 iff for all ψ: ψ ∈ Δ1 ⇒ ψ ∈ Δ2. Similarly for R with respect to . Both relations are equivalences (due to taut-5), and moreover, when R Δ1Δ2 and R Δ2Δ3 then for some Δ2, also R Δ1Δ2 and R Δ2Δ3 (because of axiom COMM).
Let Δ0 be a MCS containing ψ. We now define the set Tables = {Δ0} ∪ {Δ1, Δ2 | (R Δ0Δ1 and R Δ1Δ2) or (R Δ0Δ1 and R Δ1Δ2)} Every Δ ∈ Tables can be conceived as a pair γ, p, since every Δ contains a unique (hq ∧ o ∧ (¬)σ) for every hq and a unique hp.
It is then easy to verify that, for every Δ ∈ Tables, and every formula ϕ, Δ |= ϕ iff ϕ ∈ Δ, where |= here means truth in the ordinary modal logic sense when the set of states is taken to be Tables. Now, we extract an aggregation function f and pairs γ, p as follows: For every Δ ∈ Tables, find a conjunction hp ∧ o ∧ (¬)σ. There will be exactly one such p. This defines the p we are looking for.
Furthermore, the γ is obtained, for every agent i, by finding all q for which (hq ∧ i) is currently true. Finally, the function f is a table of all tuples hp, o(p), σ for which (hp ∧ o(o) ∧ σ) is contained in some set in Tables.
We point out that jal has all the axioms taut, K, T, 4, 5 and the rules MP and Nec of the modal logic S5. However, uniform substitution, a principle of all normal modal logics (cf., e.g., [3]), does not hold. A counter example is the fact that the following is valid: σ (5) - no matter what preferences the agents have, the JAR will always make some judgment - while this is not valid: (σ ∧ i) (6) - the JAR will not necessarily make the same judgments as agent i.
So, for example, we have that the discursive paradox is provable in jal(L): JAL(L) (( MV) → ⊥). An example of a derivation of the less complicated (valid) property (i ∧ j) is shown in Table 3.
Recently, Dietrich and List [5] showed that preference aggregation can be embedded in judgment aggregation. In this section we show that our judgment aggregation logic also can be used to reason about preference aggregation.
Given a set K of alternatives, [5] defines a simple predicate logic LK with language L(LK ) as follows: • L(LK ) has one constant a for each alternative a ∈ K, variables v1, v2, . . ., a binary identity predicate =, a binary predicate P for strict preference, and the usual propositional and first order connectives • Z is the collection of the following axioms: - ∀v1 ∀v2 (v1Pv2 → ¬v2Pv1) - ∀v1 ∀v2 ∀v3 ((v1Pv2 ∧ v2Pv3) → v1Pv3) - ∀v1 ∀v2 (¬v1 = v2 → (v1Pv2 ∨ v2Pv1)) • When Γ ⊆ L(LK ) and φ is a formula, Γ |= φ is defined to hold iff Γ ∪ Z entails φ in the standard sense of predicate logic 1 (hp ∧ i) ∨ (hp ∧ i) CpJS(i) 2 (hp ∧ j) ∨ (hp ∧ j) CpJS(j) 3 Call 1 A ∨ B and 2 C ∨ D abbreviation, 1, 2 4 (A ∧ C) ∨ (A ∧ D) ∨ (B ∧ C) ∨ (B ∧ D) taut, 3 5 derive (i ∧ j) from every disjunct of 4 strategy is ∨ elim 6 (hp ∧ i) ∧ (hp ∧ j) assume A ∧ C 7 (hp → (i ∧ j)) Once, 6, K( ) 8 (i ∧ j) 7, Agenda 9 (i ∧ j) 8, T( ) 10 (hp ∧ i) ∧ (hp ∧ j) assume A ∧ D 11 (hp ∧ x) ↔ (hp ∧ ¬x) Agenda, Closure 12 (hp ∧ i) ∧ (hp ∧ ¬j) 10, 11 13 (hp ∧ i ∧ ¬j) 12, Once, K( ) 14 (i ∧ ¬j) 13, taut 15 (i ∧ ¬j) 14, K( ) 16 (i ∧ ¬j) 15, COMM 17 ( i ∧ D¬j) 16, K( ) 18 (i ∧ j) 17, C 19 (hp ∧ i) ∧ (hp ∧ j) assume B ∧ D 20 goes as 6-9 21 (hp ∧ i) ∧ (hp ∧ j) assume B ∧ C 22 goes as 10 - 18 23 (i ∧ j) ∨-elim, 1, 2, 9, 18, 20, 22 Table 3: jar derivation of (i ∧ j) It is easy to see that there is an one-to-one correspondence between the set of preference relations (total linear orders) over K and the set of LK -consistent and complete judgment sets over the preference agenda AK = {aPb, ¬aPb : a, b ∈ K, a b} Given a SWF F over K, the corresponding JAR fF over the preference agenda AK is defined as follows fF (A1, . . . , An) = A, where A is the consistent and complete judgment set corresponding to F(L1, . . . , Ln) where Li is the preference relation corresponding to the consistent and complete judgment set Ai.
Thus we can use jal to reason about preference aggregation as follows. Take the logical language L(Σ, AK ), for some set of agents Σ, and take the underlying logic to be LK . We can then interpret our formulae in an SWF F over K, a preference profile L ∈ L(K) and a pair (a, b) ⊆ K × K, a b, as follows: F, L, (a, b) |=swf φ ⇔ fF , γL , aPb |=LK φ where γL is the judgment profile corresponding to the preference profile L.
While in the general judgment aggregation case a formula is interpreted in the context of an agenda item, in the preference aggregation case a formula is thus interpreted in the context of a pair of alternatives.
Example 2. Three agents must decide between going to dinner (d), a movie (m) or a concert (c). Their individual preferences are illustrated on the right in Table 1 in Section 3, along with the result of a SWF Fmaj implementing pair-wise majority voting. Let L = mdc, mcd, cmd be the preference profile corresponding to the preferences in the example. We have the following: • Fmaj, L, (m, d) |=swf 1 ∧ 2 ∧ 3 (all agents agree, under the individual rankings L, on the relative ranking of m and dthey agree that d is better than m) • Fmaj, L, (m, d) |=swf ¬(1 ↔ 2) (under the individual rankings L, there is some pair of alternatives on which agents 1 and 2 disagree) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 571 • Fmaj, L, (m, d) |=swf (1 ∧ 2) (agents 1 and 2 can choose their preferences such that they will agree on some pair of alternatives) • Fmaj, L, (m, d) |=swf σ ↔ G⊆{1,2,3},|G|≥2 i∈G i (the SWF Fmaj implements pair-wise majority voting) As usual, we write F |=swf φ when F, L, (a, b) |=swf φ for any L and (a, b), and so on. Thus, our formulae can be seen as expressing properties of social welfare functions.
Example 3. Take the formula (i ↔ σ). When this formula is interpreted as a statement about a social welfare function, it says that there exists a preference profile such that for all pairs (a, b) of alternatives, b is preferred over a in the aggregation (by the SWF) of the preference profile if and only if agent i prefers b over a.
We make precise the claim in Section 2.2 that the three mentioned SWF properties correspond to the three mentioned JAR properties, respectively. Recall the formulae defined in Section 4.
Proposition 2.
F |=swf ND iff F has the property ND2 F |=swf IND iff F has the property IIA F |=swf UNA iff F has the property PO The properties expressed above are properties of SWFs. Let us now look at properties of the set of alternatives K we can express.
Properties involving cardinality is often of interest, for example in Arrow"s theorem. Let: MT2 = ( (1 ∧ 2) ∧ (1 ∧ ¬2)) Proposition 3. Let F ∈ F (K). |K| > 2 iff F |=swf MT2.
Proof. For the direction to the left, let F |=swf MT2. Thus, there is a γ such that there exists (a1 , b1 ), (a2 , b2 ) ∈ K × K, where a1 b1 , and a2 b2 , such that (i) a1 Pb1 ∈ γ1, (ii) a1 Pb1 ∈ γ2, (iii) a2 Pb2 ∈ γ1 and (iv) a2 Pb2 γ2. From (ii) and (iv) we get that (a1 , b1 ) (a2 , b2 ), and from that and (i) and (iii) it follows that γ1 contains two different pairs a1 Pb1 and a2 Pb2 each having two different elements. But that is not possible if |K| = 2, because if K = {a, b} then AK = {aPb, ¬aPb, bPa, ¬bPa} and thus it is impossible that γ1 ⊆ AK since we cannot have aPb, bPa ∈ γ1.
For the direction to the right, let |K| > 2; let a, b, c be three distinct elements of K. Let γ1 be the judgment set corresponding to the ranking abc and γ2 the judgment set corresponding to acb.
Now, for any aggregation rule f, f, γ, aPb |= 1 ∧ 2 and f, γ, bPc |= 1 ∧ ¬2. Thus, F |=swf MT2, for any SWF F.
We now have everything we need to express Arrow"s statement as a formula. It follows from his theorem that the formula is valid on the class of all social welfare functions.
Theorem 4. |=swf MT2 → ¬(PO ∧ ND ∧ IIA) Proof. Note that MT2, PO, ND and IIA are true SWF properties, their truth value wrt. a table is determined solely by the SWF. For example, F, L, (a, b) |=swf MT2 iff F |= MT2, for any F, L, a, b.
Let F ∈ F (K), and F, L, (a, b) |=swf MT2 for some L and a, b. By Proposition 3, K has more than two alternatives. By Arrow"s theorem, F cannot have all the properties PO, ND2 and IIA. W.l.o.g assume that F does not have the PO property. By Proposition 2,
F |=swf PO. Since PO is a SWF property, this means that F, L, (a, b) |=swf PO (satisfaction of PO is independent of L, a, b), and thus that F, L, (a, b) |=swf ¬PO ∨ ¬ND ∨ ¬IIA.
Note that the formula in Theorem 4 does not mention any agenda items (i.e., pairs of alternatives) such as haPb directly in an expression. This means that the formula is a member of L(Σ, AK ) for any set of alternatives K, and is valid no matter which set of alternatives we assume.
The formula MV which in the general judgment aggregation case expresses proposition-wise majority voting, expresses in the preference aggregation case pair-wise majority voting, as illustrated in Example 2. The preference aggregation correspondent to the discursive paradox of judgment aggregation is the well known Condorcet"s voting paradox, stating that pair-wise majority voting can lead to aggregated preferences which are cyclic (even if the individual preferences are not). We can express Condorcet"s paradox as follows, again as a universally valid logical property of SWFs.
Proposition 4. |=swf MT2 → ¬MV, when there are at least three agents.
Proof. The proof is similar to the proof of the discursive paradox. Let fF , γ, aPb |=LK MT2; there are thus three distinct elements a, b, c ∈ K. Assume that fF , γ, aPb |=LK MV. Let γ be the judgment profile corresponding to the preference profile X = (abc, cab, bca). We have that fF , γ , aPb |=LK 1 ∧ 2 and, since fF , γ , aPb |=LK MV, we have that fF , γ , aPb |=LK σ and thus that aPb ∈ fF (γ ) and (a, b) ∈ F(X). In a similar manner we get that (c, a) ∈ F(X) and (b, c) ∈ F(X). But that is impossible, since by transitivity we would also have that (a, c) ∈ F(X) which contradicts the fact that F(X) is antisymmetric. Thus, it follows that fF , γ, aPb |=LK MV.
We immediately get, from Theorem 3, a sound and complete axiomatisation of preference aggregation over a finite set of alternatives.
Corollary 1. If the set of alternatives K is finite, we have that for any formula ψ ∈ L(Σ, AK ), JAL(LK ) ψ iff |=swf ψ.
Proof. Follows immediately from Theorem 3 and the fact that for any JAR f, there is a SWF F such that f = fF .
So, for example, Arrow"s theorem is provable in jal(LK ): JAL(LK ) MT2 → ¬(PO ∧ ND ∧ IIA).
Every formula which is valid with respect to judgment aggregation rules is also valid with respect to social welfare functions, so all general logical properties of JARs are also properties of SWFs.
Depending on the agenda, SWFs may have additional properties, induced by the logic LK , which are not always shared by JARs with other underlying logics. One such property is i. While we have |=swf i, for other agendas there are underlying logics L such that |=L i To see the latter, take an agenda with a formula p which is inconsistent in the underlying logic L - p can never be included in a judgment set. To see the former, take an arbitrary pair of alternatives (a, b). There exists some preference profile in which agent i prefers b over a.
Technically speaking, the formula i holds in SWFs because the agenda AK does not contain a formula which (alone) is inconsistent wrt. the underlying logic LK . By the same reason, the following properties also hold in SWFs but not in JARs in general. |=swf o∈O o 572 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) - for any pair of alternatives (a, b), any possible combination of the relative ranking of a and b among the agents is possible. |=swf i → ¬i - given an alternative b which is preferred over some other alternative a by agent i, there is some other pair of alternatives c and d such that d is not preferred over c - namely (c, d) = (b, a). |=swf ( (i ∨ j) → (i ∧ ¬j)) - if, given preferences of agents and a SWF, for any two alternatives it is always the case that either agent i or agent j prefers the second alternative over the first, then there must exist a pair of alternatives for which the two agents disagree. A justification is that no single agent can prefer the second alternative over the first for every pair of alternatives, so in this case if i prefers b over a then j must prefer a over b. Again, this property does not necessarily hold for other agendas, because the agenda might contain an inconsistency the agents could not possibly disagree upon.
Proof theoretically, these additional properties of SWFs are derived using the Closure rule.
Formal logics related to social choice have focused mostly on the logical representation of preferences when the set of alternatives is large and on the computation properties of computing aggregated preferences for a given representation [6, 7, 8].
A notable and recent exception is a logical framework for judgment aggregation developed by Marc Pauly in [10], in order to be able to characterise the logical relationships between different judgment aggregation rules. While the motivation is similar to the work in this paper, the approaches are fundamentally different: in [10], the possible results from applying a rule to some judgment profile are taken as primary and described axiomatically; in our approach the aggregation rule and its possible inputs, i.e., judgment profiles, are taken as primary and described axiomatically. The two approaches do not seem to be directly related to each other in the sense that one can be embedded in the other.
The modal logic arrow logic [11] is designed to reason about any object that can be graphically represented as an arrow, and has various modal operators for expressing properties of and relationships between these arrows. In the preference aggregation logic jal(LK ) we interpreted formulae in pairs of alternatives - which can be seen as arrows. Thus, (at least) the preference aggregation variant of our logic is related to arrow logic. However, while the modal operators of arrow logic can express properties of preference relations such as transitivity, they cannot directly express most of the properties we have discussed in this paper. Nevertheless, the relationship to arrow logic could be investigated further in future work. In particular, arrow logics are usually proven complete wrt. an algebra.
This could mean that it might be possible to use such algebras as the underlying structure to represent individual and collective preferences. Then, changing the preference profile takes us from one algebra to another, and a SWF determines the collective preference, in each of the algebras.
We have presented a sound and complete logic jal for representing and reasoning about judgment aggregation. jal is expressive: it can express judgment aggregation rules such as majority voting; complicated properties such as independence; and important results such as the discursive paradox, Arrow"s theorem and Condorcet"s paradox. We argue that these results show exactly which logical capabilities an agent needs in order to be able to reason about judgment aggregation. It is perhaps surprising that a relatively simple language provides these capabilities. jal provides a proof theory, in which results such as those mentioned above can be derived3 .
The axiomatisation describes the logical principles of judgment aggregation, and can also be instantiated to reason about specific instances of judgment aggregation, such as classical Arrovian preference aggregation. Thus our framework sheds light on the differences between the logical principles behind general judgment aggregation on the one hand and classical preference aggregation on the other.
In future work it would be interesting to relax the completeness and consistency requirements of judgment sets, and try to characterise these in the logical language, as properties of general judgment sets, instead.
We thank the anonymous reviewers for their helpful remarks.
Thomas Ågotnes" work on this paper was supported by grants 166525/V30 and 176853/S10 from the Research Council of Norway.
[1] K. J. Arrow. Social Choice and Individual Values. Wiley,
[2] K. J. Arrow, A. K. Sen, and K. Suzumura, eds. Handbook of Social Choice and Welfare, volume 1. North-Holland, 2002. [3] P. Blackburn, M. de Rijke, and Y. Venema. Modal Logic.
Cambridge University Press, 2001. [4] E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking. The MIT Press: Cambridge, MA, 2000. [5] F. Dietrich and C. List. Arrow"s theorem in judgment aggregation. Social Choice and Welfare, 2006. Forthcoming. [6] C. Lafage and J. Lang. Logical representation of preferences for group decision making. In Proceedings of the Conference on Principles of Knowledge Representation and Reasoning (KR-00), pages 457-470. Morgan Kaufman, 2000. [7] J. Lang. From preference representation to combinatorial vote. Proceedings of the Eighth International Conference on Principles and Knowledge Representation and Reasoning (KR-02), pages 277-290. Morgan Kaufmann, 2002. [8] J. Lang. Logical preference representation and combinatorial vote. Ann. Math. Artif. Intell, 42(1-3):37-71, 2004. [9] C. H. Papadimitriou. Computational Complexity.
Addison-Wesley: Reading, MA, 1994. [10] M. Pauly. Axiomatizing collective judgment sets in a minimal logical language, 2006. Manuscript. [11] Y. Venema. A crash course in arrow logic. In M. Marx,
M. Masuch, and L. Polos, editors, Arrow Logic and Multi-Modal Logic, pages 3-34. CSLI Publications,
Stanford, 1996. 3 Dietrich and List [5] prove a general version of Arrow"s theorem for JARs: for a strongly connected agenda, a JAR has the IND and UNA properties iff it does not have the ND1 property, where strong connectedness is an algebraic and logical condition on agendas. Thus, if we assume that the agenda is strongly connected then (ND ∧ UNA) ↔ ¬ND1 is valid, and derivable in jar. An interesting possibility for future work is to try to characterise conditions such as strong connectedness directly as a logical formula.

Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.
MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries.
Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]). However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.
Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere. In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents. In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].
In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.
We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players" behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files. In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.
The paper proceeds as follows. Section 2 presents the model"s formalization. Section 3 presents the empirical analysis and its results. We discuss related work in Section 4, and conclude and present future directions in Section 5.
The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment. We focus here on specific types of adversarial environments, specified as follows:
of all agents sum to zero;
adversarial agents;
We will work on both bilateral and multilateral instantiations of zero-sum and simple environments. In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed. Examples of such environments range from board games (e.g., Chess,
Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good).
Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE. The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE:
will be completed;
adversaries are pursuing full conflicting goals (defined below)there can be only one winner;
has an intention to complete its own full conflicting goal;
of its adversaries.
Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it. This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding. In such cases, it might not consider itself to even be in an adversarial environment.
Item 4 states that the agent should hold some belief about the profiles of its adversaries. The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more. It can be given explicitly or can be learned from observations of past encounters.
We use Grosz and Kraus"s definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4]. We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ai"s intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ai"s intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds. The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf . MB(A, f, Tf ) represents mutual belief for a group of agents A.
A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states. At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries. For example, in a Texas Hold"em poker game, an agent"s local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).
A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective. We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world. The implementation of the utility function is dependent on the domain in question.
The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization:
predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai).
predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal).
Aj Ai is the profile object agent Ai holds about agent Aj.
are derived from the environment"s constraints. CAi ⊆ CA is the set of agent Ai"s possible actions.
time interval Tα in world w.
Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi .
Ai ) is true when agent Ai holds an object profile for agent Aj.
Definition 1. Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.
FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2. Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn. The higher the value, the more knowledge agent Ai has.
AdvKnow : P Aj Ai × Tn → Definition 3. Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w.
Eval : A × CA × w → Definition 4. TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value. An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.
The Eval value is an estimation and not the real utility function, which is usually unknown. Using the real utility value for a rational agent would easily yield the best outcome for that agent. However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.
There are two important properties that should hold for the evaluation function: Property 1. The evaluation function should state that the most desirable world state is one in which the goal is achieved.
Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2. The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.
Definition 5. SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ai"s belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agent"s knowledge about the profile of its adversary.
Property 3. As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions. Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.
AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1)
The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).
Satisfaction of these axioms means that the agent is situated in such an environment. It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w.
AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w)
(∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE)
pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn)
Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn)
adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions. Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].
The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.
Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments. The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action. This reasoning will lead to the adoption of an Int.To(...) (see [4]).
A1. Goal Achieving Axiom. The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom. In any situation, when the agent is an action away from completing the goal, it should complete the action.
Any fair Eval function would naturally classify α as the maximal value action (property 1). However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.
A2. Preventive Act Axiom. Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversary"s plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag . Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH). Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment. For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move. Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.
A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent. Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.
Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.
The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.
However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.
A3. Suboptimal Tactical Move Axiom. In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversary"s response) a future possibility for a highly beneficial action.
This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.
Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain. For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponent"s moves to avoiding the check, to which the first player might react with another check, and so on. The agent might also believe in a chain of events based on its knowledge of its adversary"s profile, which allows it to foresee the adversary"s movements with high accuracy.
A4. Profile Detection Axiom. The agent can adjust its adversary"s profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary). However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it. Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversary"s profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent. We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.
A5. Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter). In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance. Such an alliance is an agreement that constrains its members" behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.
As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game. However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.
An alliance"s terms defines the way its members should act. It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance. For example, the set Terms in the Risk scenario, could contain the following predicates:
X, Y and Z;
attacking adversary Ao;
time Tk or until adversary"s Ao army is smaller than Q.
The set Terms specifies inter-group constraints on each of the alliance member"s (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C.
Definition 6. Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7. Al TrH - is a number representing an Al val The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.
The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].
After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance. The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.
AL(Aal , Cal , w, Tn)
has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn)
has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn)
has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members" profiles are a crucial part of successful alliances.
We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments. Such agents will be able to predict when a member is about to breach the alliance"s contract (item 2 in the above model), and take counter measures (when item 3 will falsify). The robustness of the alliance is in part a function of its members" trustfulness measure, objective position estimation, and other profile properties. We should note that an agent can simultaneously be part of more than one alliance.
Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal). The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.
When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment model"s definitions are almost identical (see SA"s definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agent"s behavior. The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.
This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.
Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.
A6. Evaluation Maximization Axiom. In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents. The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).
Theorem 1. Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary. Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α. The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ag"s goal. It will obtain the highest utility by Min-Max for Au ag. The Ae ag agent will select α or another action with the same utility value via A1. If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag. Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1). In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable. Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ag"s actions can increase its knowledge. That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag. Given that Eval = Utility, the same α that was selected by Au ag will be selected.
The main purpose of our experimental analysis is to evaluate the model"s behavior and performance in a real adversarial environment. This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γ"s existence will cause it to classify β as a low utility action. 554 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms.
To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment. Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.
Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion). The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color. On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.
The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.
Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environment"s definition as given above (which the behavioral axioms are based on). First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).
Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie). In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).
Of course, not all Connect-Four encounters are adversarial. For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him). However, the parent"s point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning. In such an educational environment, a new set of behavioral axioms might be more beneficial to the parent"s goals than our suggested adversarial behavioral axioms.
After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players" behaviors during the game and check whether behaving according to our model does improve performance. To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet. Our collected log file data came from Play by eMail (PBeM) sites. These are web sites that host email games, where each move is taken by an email exchange between the server and the players. Many such sites" archives contain real competitive interactions, and also maintain a ranking system for their members. Most of the data we used can be found in [6].
As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player). We will concentrate in our analysis on the second player"s moves (to be called Black). The White player, being the first to act, has the so-called initiative advantage. Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats. A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk. An open threat is a threat that can be realized in the opponent"s next move. In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player. We will explore Black players" behavior and their conformance to our axioms.
To do so, we built an application that reads log files and analyzes the Black player"s moves. The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats. The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically). The threat detector"s job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponent"s next move).
The heuristic function used by Min-Max to evaluate the player"s utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8. Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal. Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values). Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.
We now use our estimated evaluation function to evaluate the Black player"s actions during the Connect-Four adversarial interaction. Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected. A total of 123 games were analyzed (57 with White winning, and 66 with Black winning). A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves). In addition, a single tie game was also removed. The simulator was run to a search depth of 3 moves. We now proceed to analyze the games with respect to each behavioral axiom.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg" minh -17.62 -12.02 Avg" 3 lowest h moves (min3 h) -13.20 -8.70
The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.
Table 1 shows results and insights from the games" heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3). The table"s heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).
The first row presents the difference values of the action that had the maximal difference value among all the Black player"s actions in a given game, as averaged over all Black"s winning and losing games (see respective columns).
In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02. The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them. In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins. Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.
After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.
To do so we took the subset of games in which the minimum heuristic difference value for Black"s actions was 11.5. As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won. The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.
The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins. Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black. However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results. The last row sums up the main insights from the analysis; most of Black"s wins (83%) came when its min3 h was in the range of -11.5 to -4. A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.
White continues to build its threats, while usually disregarding Black"s last move, which in turn uses the isolated disc as an anchor for a future winning threat.
The results show that it was beneficial for the Black player Table 2: Black"s winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary). As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black player"s winning chances by a large margin.
In the task of showing the importance of monitoring one"s adversaries" profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players" knowledge about their adversaries. However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).
In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions. They apply simple learning strategies by analyzing examples from past interactions in a specific domain. They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversary"s profile.
Following the presentation of their theoretical model, they describe an extensive empirical study and check the agent"s performance after learning the weakness model with past examples. One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain). Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values. The search depth for the players was 3 (as in our analysis). Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods. The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22). Their conclusions, showing improved performance when holding and using the adversary"s model, justify the effort to monitor the adversary profile for continuous and repeated interactions.
An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agent"s decision strategy. Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future. The agent"s behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification. However, even with respect to those axioms, a few interesting insights came up in the log analysis.
The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player. In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move. We can blame those faults on the human"s lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.
A typical Connect-Four game revolves around generating threats and blocking them. In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon). We found that in 83% of the total games there was at least one preventive action taken by the Black player. It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning. It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats. If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White.
Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4]. However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents" behavior in it.
The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents" strategies were modeled as finite automata. Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.
The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agent"s advantage. However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification.
We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.
The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.
The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research.
This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05.

The opportunity of a technology transfer from the field of organizational and social theory to distributed AI and multiagent systems (MASs) has long been advocated ([8]). In MASs the application of the organizational and institutional metaphors to system design has proven to be useful for the development of methodologies and tools. In many cases, however, the application of these conceptual apparatuses amounts to mere heuristics guiding the high level design of the systems. It is our thesis that the application of those apparatuses can be pushed further once their key concepts are treated formally, that is, once notions such as norm, role, structure, etc. obtain a formal semantics. This has been the case for agent programming languages after the relevant concepts borrowed from folk psychology (belief, intention, desire, knowledge, etc.) have been addressed in comprehensive formal logical theories such as, for instance, BDICTL ([22]) and KARO ([17]). As a matter of fact, those theories have fostered the production of architectures and programming languages.
What is lacking at the moment for the design and development of open MASs is, in our opinion, something that can play the role that BDI-like formalisms have played for the design and development of single-agent architectures. Aim of the present paper is to fill this gap with respect to the notion of institution providing formal foundations for the application of the institutional metaphor and for its relation to the organizational one. The main result of the paper consists in showing how abstract constraints (institutions) can be step by step refined to concrete structural descriptions (organizational structures) of the to-be-implemented system, bridging thus the gap between abstract norms and concrete system specifications.
Concretely, in Section 2, a logical framework is presented which provides a formal semantics for the notions of institution, norm, role, and which supports the account of key features of institutions such as the translation of abstract norms into concrete and implementable ones, the institutional empowerment of agents, and some aspects of the design of norm enforcement. In Section 3 the framework is extended to deal with the notion of the infrastructure of an institution. The extended framework is then studied in relation to the formalism for representing organizational structures presented in [11]. In Section 4 some conclusions follow.
Social theory usually thinks of institutions as the rules of the game ([18, 23]). From an agent perspective institutions are, to paraphrase this quote, the rules of the various games agents can play in order to interact with one another. To assume an institutional perspective on MASs means therefore to think of MASs in normative terms: [. . . ] law, computer systems, and many other kinds of organizational structure may be viewed as instances of normative systems. We use the term to refer to any set of interacting agents whose behavior can usefully be regarded as governed by norms ([15], p.276).
The normative system perspective on institutions is, as such, nothing original and it is already a quite acknowledged position within the community working on electronic institutions, or eInstitutions ([26]). What has not been sufficiently investigated and understood with formal methods is, in our view, the question: what does it 628 978-81-904262-7-5 (RPS) c 2007 IFAAMAS amount to, for a MAS, to be put under a set of norms? Or in other words: what does it mean for a designer of an eInstitution to state a set of norms? We advance a precise thesis on this issue, which is also inspired by work in social theory: Now, as the original manner of producing physical entities is creation, there is hardly a better way to describe the production of moral entities than by the word ‘imposition" [impositio]. For moral entities do not arise from the intrinsic substantial principles of things but are superadded to things already existent and physically complete ([21], pp. 100-101).
By ignoring for a second the philosophical jargon of the Seventeenth century we can easily extract an illuminating message from the excerpt: what institutions do is to impose properties on already existing entities. That is to say, institutions provide descriptions of entities by making use of conceptualizations that are not proper of the common descriptions of those entities. For example, that cars have wheels is a common factual property, whereas the fact that cars count as vehicles in some technical legal sense is a property that law imposes on the concept car. To say it with [25], the fact that cars have wheels is a brute fact, while the fact that cars are vehicles is an institutional fact. Institutions build structured descriptions of institutional properties upon brute descriptions of a given domain.
At this point, the step toward eInstitutions is natural. eInstitutions impose properties on the possible states of a MAS: they specify what are the states in which an agent i enacts a role r; what are the states in which a certain agent is violating the norms of the institution, etc. They do this via linking some institutional properties of the possible states and transitions of the system (e.g., agent i enacts role r) to some brute properties of those states and transitions (e.g., agent i performs protocol No.56). An institutional property is therefore a property of system states or system transitions (i.e., a state type or a transition type) that does not belong to a merely technical, or factual, description of the system.
To sum up, institution are viewed as sets of norms (normative system perspective), and norms are thought of as the imposition of an institutional description of the system upon its description in terms of brute properties. In a nutshell, institutions are impositions of institutional terminologies upon brute ones. The following sections provide a formal analysis of this thesis and show its explanatory power in delivering a rigorous understanding of key features of institutions. Because of its suitability for representing complex domain descriptions, the formal framework we will make use of is the one of Description Logics (DL). The use of such formalism will also stress the idea of viewing institutions as the impositions of domain descriptions.
The description logic language enabling the necessary expressivity expands the standard description logic language ALC ([3]) with relational operators ( ,◦,¬,id) to express complex transition types, and relational hierarchies (H) to express inclusion between transition types. Following a notational convention common within DL we denote this language with ALCH( ,◦,¬,id) .
DEFINITION 1. (Syntax of ALCH( ,◦,¬,id) ) transition types and state type constructs are defined by the following BNF: α := a | α ◦ α | α α | ¬α | id(γ) γ := c | ⊥ | ¬γ | γ γ | ∀α.γ where a and c are atomic transition types and, respectively, atomic state types.
It is worth providing the intuitive reading of a couple of the operators and the constructs just introduced. In particular ∀α.γ has to be read as: after all executions of transitions of type α, states of type γ are reached. The operator ◦ denotes the concatenation of transition types. The operator id applies to a state description γ and yields a transition description, namely, the transition ending in γ states. It is the description logic variant of the test operator in Dynamic Logic ([5]). Notice that we use the same symbols and ¬ for denoting the boolean operators of disjunction and negation of both state and transition types. Atomic state types c are often indexed by an agent identifier i in order to express agent properties (e.g., dutch(i)), and atomic transition types a are often indexed by a pair of agent identifiers (i, j) (e.g., PAY(i, j)) denoting the actor and, respectively, the recipient of the transition. By removing the agent identifiers from state types and transition types we obtain state type forms (e.g., dutch or rea(r)) and transition type form (e.g., PAY).
A terminological box (henceforth TBox) T = Γ, A consists of a finite set Γ of state type inclusion assertions (γ1 γ2), and of a finite set A of transition type inclusion assertions (α1 α2).
The semantics of ALCH( ,◦,¬,id) is model theoretical and it is given in terms of interpreted transition systems. As usual, state types are interpreted as sets of states and transition types as sets of state pairs.
DEFINITION 2. (Semantics of ALCH( ,◦,¬,id) ) An interpreted transition system m for ALCH( ,◦,¬,id) is a structure S, I where S is a non-empty set of states and I is a function such that: I(c) ⊆ S I(a) ⊆ S × S I(⊥) = ∅ I(¬γ) = Δm\ I(γ) I(γ1 γ2) = I(γ1) ∩ I(γ2) I(∀α.γ) = {s ∈ S | ∀t, (s, t) ∈ I(α) ⇒ t ∈ I(γ)} I(α1 α2) = I(α1) ∪ I(α2) I(¬α) = S × S \ I(α) I(α1 ◦ α2) = {(s, s ) | ∃s , (s, s ) ∈ I(α1) & (s , s ) ∈ I(α2)} I(id(γ)) = {(s, s) | s ∈ I(γ)} An interpreted transition system m is a model of a state type inclusion assertion γ1 γ2 if I(γ1) ⊆ I(γ2). It is a model of a transition type inclusion assertion α1 α2 if I(α1) ⊆ I(α2). An interpreted transition system m is a model of a TBox T = Γ, A if m is a model of each inclusion assertion in Γ and A.
REMARK 1. (Derived constructs) The correspondence between description logic and dynamic logic is well-known ([3]). In fact, the language presented in Definitions 1 and 2 is a notational variant of the language of Dynamic Logic ([5]) without the iteration operator of transition types. As a consequence, some key constructs are still definable in ALCH( ,◦,¬,id) . In particular we will make use of the following definition of the if-then-else transition type: if γ then α1else α2 = (id(γ) ◦ α1) (id(¬γ) ◦ α2).
Boolean operators are defined as usual.
We will come back to some complexity features of this logic in Section 2.5.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 629
We have upheld that institutions impose new system descriptions which are formulated in terms of sets of norms. The step toward a formal grounding of this view of institutions is now short: norms can be thought of as terminological axioms, and institutions as sets of terminological axioms, i.e., terminological boxes.
An institution can be specified as a terminological box Ins = Γins, Ains , where each inclusion statement in Γins and Ains models a norm of the institution. Obviously, not every TBox can be considered to be an institution specification. In particular, an institution specification Ins must have some precise linguistic relationship with the ‘brute" descriptions upon which the institution is specified. We denote by Lins the non-logical alphabet containing only institutional state and transition types, and by Lbrute the nonlogical alphabet containing those types taken to talk about, instead, ‘brute" states and transitions1 .
DEFINITION 3. (Institutions as TBoxes) A TBox Ins = Γins, Ains is an institution specification if:
elements of both Lins and Lbrute. In symbols: L(Ins) ⊆ Lins ∪ Lbrute.
and Abridge ⊆ Ains such that either the left-hand side of these axioms is always a description expressed in Lbrute and the right-hand side a description expressed in Lins, or those axioms are definitions. In symbols: if γ1 γ2 ∈ Γbridge then either γ1 ∈ Lbrute and γ2 ∈ Lins or it is the case that also γ2 γ1 ∈ Γbridge. The clause for Abridge is analogous.
and Ains\Abridge are all expressed in Lins. In symbols: L(Γins\Γbridge) ⊆ Lins and L(Ains\Abridge) ⊆ Lins.
The definition states that an institution specification needs to be expressed on a language including institutional as well as brute terms (1); that a part of the specification concerns a description of mere institutional terms (3); and that there needs to be a part of the specification which connects institutional terms to brute ones (2).
Terminological axioms in Γbridge and Abridge formalize in DL the Searlean notion of counts-as conditional ([25]), that is, rules stating what kind of meaning an institution gives to certain brute facts and transitions (e.g., checking box No.4 in form No.2 counts as accepting your personal data to be used for research purposes). A formal theory of counts-as statements has been thoroughly developed in a series of papers among which [10, 13]. The technical content of the present paper heavily capitalizes on that work.
Notice also that given the semantics presented in Definition 2, if institutions can be specified via TBoxes then the meaning of such specifications is a set of interpreted transition systems, i.e., the models of those TBoxes. These transitions systems can be in turn thought of as all the possible MASs which model the specified institution.
REMARK 2. (Lbrute from a designer"s perspective) From a design perspective language Lbrute has to be thought of as the language on which a designer would specify a system instantiating a given institution2 . Definition 3 shows that for such a design task 1 Symbols from Lins and Lbrute will be indexed (especially with agent identifiers) to add some syntactic sugar. 2 To make a concrete example, the AMELI middleware [7] can be viewed as a specification tool at a Lbrute level. it is needed to formally specify an explicit bridge between the concepts used in the description of the actual system and the institutional ‘abstract" concepts. We will come back to this issue in Section 3.
To illustrate Definition 3, and show its explanatory power, an example follows which depicts an essential phenomenon of institutions.
EXAMPLE 1. (From abstract to concrete norms) Consider an institution supposed to regulate access to a set of public web services. It may contain the following norm: it is forbidden to discriminate access on the basis of citizenship. Suppose now a system has to be built which complies with this norm. The first question is: what does it mean, in concrete, to discriminate on the basis of citizenship? The system designer should make some concrete choices for interpreting the norm and these choices should be kept track of in order to explicitly link the abstract norm to its concrete interpretation. The problem can be represented as follows. The abstract norm is formalized by Formula 1 by making use of a standard reduction technique of deontic notions (see [16]): the statement it is forbidden to discriminate on the basis of citizenship amounts to the statement after every execution of a transition of type DISCR(i, j) the system always ends up in a violation state.
Together with the norm also some intuitive background knowledge about the discrimination action needs to be formalized. Here, as well as in the rest of the examples in the paper, we provide just that part of the formalization which is strictly functional to show how the formalism works in practice. Formulae 2 and 3 express two effect laws: if the requester j is Dutch then after all executions of transitions of type DISCR(i, j) j is accepted by i, whereas if it is not all the executions of the transitions of the same type have as an effect that it is not accepted. All formulae have to be read as schemata determining a finite number of subsumption expressions depending on the number of agents i, j considered. ∀DISCR(i, j).viol ≡ (1) dutch(j) ∀DISCR(i, j).accepted(j) (2) ¬dutch(j) ∀DISCR(i, j).¬accepted(j) (3) The rest of the axioms concern the translation of the abstract type DISCR(i, j) to concrete transition types. Formula 4 refines it by making explicit that a precise if-then-else procedure counts as a discriminatory act of agent i. Formulae 5 and 6 specify which messages of i to j count as acceptance and rejection. If the designer uses transition types SEND(msg33, i, j) and SEND(msg38, i, j) for the concrete system specification, then Formulae 5 and 6 can be thought of as bridge axioms connecting notions belonging to the institutional alphabet (to accept, and to reject) to concrete ones (to send specific messages). Finally, Formulae 7 and 8 state two intuitive effect laws concerning the ACCEPT(i, j) and REJECT(i, j) types. if dutch(j)then ACCEPT(i, j) else REJECT(i, j) DISCR(i, j) (4) SEND(msg33, i, j) ACCEPT(i, j) (5) SEND(msg38, i, j) REJECT(i, j) (6) ∀ACCEPT(i, j).accepted(j) ≡ (7) ∀REJECT(i, j).¬accepted(j) ≡ (8) It is easy to see, on the grounds of the semantics exposed in Definition 2, that the following concrete inclusion statement holds w.r.t. 630 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the specified institution: if dutch(j) then SEND(msg33, i, j) else SEND(msg38, i, j) DISCR(i, j) (9) This scenario exemplifies a pervasive feature of human institutions which, as extensively argued in [10], should be incorporated by electronic ones. Current formal approaches to institutions, such as ISLANDER [6], do not allow for the formal specification of explicit translations of abstract norms into concrete ones, and focus only on norms that can be specified at the concrete system specification level. What Example 1 shows is that the problem of the abstractness of norms in institutions can be formally addressed and can be given a precise formal semantics.
The scenario suggests that, just by modifying an appropriate set of terminological axioms, it is possible for the designer to obtain a different institution by just modifying the sets of bridge axioms without touching the terminological axioms expressed only in the institutional language Lins. In fact, it is the case that a same set of abstract norms can be translated to different and even incompatible sets of concrete norms. This translation can nevertheless not be arbitrary ([1]).
EXAMPLE 2. (Acceptable and unacceptable translations of abstract norms) Reconsider again the scenario sketched in Example
complex procedure composed by concrete transition types. Would any translation do? Consider an alternative institution specification Ins containing Formulae 1-3 and the following translation rule: PAY(j, i, e10) DISCR(i, j) (10) Would this formula be an acceptable translation of the abstract norm expressed in Formula 1? The axiom states that transitions where i receives e10 from j count as transitions of type DISCR(i, j).
Needless to say this is not intuitive, because the abstract transition type DISCR(i, j) obeys some intuitive conceptual constraints (Formulae 2 and 3) that all its translations should also obey. In fact, the following inclusions would then hold in Ins : dutch(j) ∀PAY(j, i, e10).accepted(j) (11) ¬dutch(j) ∀PAY(j, i, e10).¬accepted(j) (12) In fact, there properties of the transition type PAY(j, i, e10) look at least awkward: if an agent is Dutch than by paying e10 it would be accepted, while if it was not Dutch the same action would make it not accepted. The problem is that the meaning of ‘paying" is not intuitively subsumed by the meaning of ‘discriminating". In other words, a transition type PAY(j, i, e10) does not intuitively yield the effects that a sub-type of DISCR(i, j) yields. It is on the contrary perfectly intuitive that Formula 9 obeys the constraints in Formulae 2 and 3, which it does, as it can be easily checked on the grounds of the semantics.
It is worth stressing that without providing a model-theoretic semantics for the translation rules linking the institutional notions to the brute ones, it would not be so straightforward to model the logical constraints to which the translations are subjected (Example 2).
This is precisely the advantage of viewing translation rules as specific terminological axioms, i.e., Γbridge and Abridge, working as a bridge between two languages (Definition 3). In [12], we have thoroughly compared this approach with approaches such as [9] which conceive of translation rules as inference rules.
The two examples have shown how our approach can account for some essential features of institutions. In the next section the same framework is applied to provide a formal analysis of the notion of role.
Viewing institutions as the impositions of institutional descriptions on systems" states and transitions allows for analyzing the normative system perspective itself (i.e., institutions are sets of norms) at a finer granularity. We have seen that the terminological axioms specifying an institution concern complex descriptions of new institutional notions. Some of the institutional state types occurring in the institution specification play a key role in structuring the specification of the institution itself. The paradigmatic example in this sense ([25]) are facts such as agent i enacts role r which will be denoted by state types rea(i, r). By stating how an agent can enact and ‘deact" a role r, and what normative consequences follow from the enactment of r, an institution describes expected forms of agents" behavior while at the same time abstracting from the concrete agents taking part to the system.
The sets of norms specifying an institution can be clustered on the grounds of the rea state types. For each relevant institutional state type (e.g., rea(i, r)), the terminological axioms which define an institution, i.e., its norms, can be clustered in (possibly overlapping) sets of three different types: the axioms specifying how states of that institutional type can be reached (e.g., how an agent i can enact the role r); how states of that type can be left (e.g., how an agent i can ‘deact" the a role r); and what kind of institutional consequences do those states bear (e.g., what rights and power does agent i acquire by enacting role r). Borrowing the terminology from work in legal and institutional theory ([23, 25]), these clusters of norms can be called, respectively, institutive, terminative and status modules.
Status modules We call status modules those sets of terminological axioms which specify the institutional consequences of the occurrence of a given institutional state-of-affairs, for instance, the fact that agent i enacts role r.
EXAMPLE 3. (A status module for roles) Enacting a role within an institution bears some institutional consequences that are grouped under the notion of status: by playing a role an agent acquires a specific status. Some of these consequences are deontic and concern the obligations, rights, permissions under which the agent puts itself once it enacts the role. An example which pertains to the normative description of the status of both a buyer and a seller roles is the following: rea(i, buyer) rea(j, seller) win bid(i, j, b) ∀¬PAY(i, j, b).viol(i) (13) If agent i enacts the buyer role and j the seller role and i wins bid b then if i does not perform a transition of type PAY (i, j, b), i.e., does not pay to j the price corresponding to bid b, then the system ends up in a state that the institution classifies as a violation state with i being the violator. Notice that Formula 13 formalizes at the same time an obligation pertaining to the role buyer and a right pertaining to the role seller. Of particular interest are then those consequences that attribute powers to agents enacting specific roles: rea(i, buyer) rea(j, seller) ∀BID(i, j, b).bid(i, j, b) (14) SEND(i, j, msg49) BID(i, j, b) (15) If agent i enacts the buyer role and j the seller role, every time agent i bids b to j this action results in an institutional state testifying that the corresponding bid has been placed by i (Formula 14). Formula 15 states how the bidding action can be executed by sending a specific message to j (SEND(i, j, msg49)).
Some observations are in order. As readers acquainted with deontic logic have probably already noticed, our treatment of the notion The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 631 of obligation (Formula 13) makes again use of a standard reduction approach ([16]). More interesting is instead how the notion of institutional power is modeled. Essentially, the empowerment phenomenon is analyzed in term of two rules: one specifying the institutional effects of an institutional action (Formula 14), and one translating the institutional transition type in a brute one (Formula 15). Systems of rules of this type empower the agents enacting some relevant role by establishing a connection between the brute actions of the agents and some institutional effect. Whether the agents are actually able to execute the required ‘brute" actions is a different issue, since agent i can be in some states (or even all states) unable to effectuate a SEND(i, j, msg49) transition. This is the case also in human societies: priests are empowered to give rise to marriages but if a priest is not in state of performing the required speech acts he is actually unable to marry anybody. There is a difference between being entitled to make a bid and being in state of making a bid ([4]). In other words, Formulae 14 and 15 express only that agents playing the buyer role are entitled to make bids. The actual possibility of performing the required ‘brute" actions is not an institutional issue, but rather an issue concerning the implementation of an institution in a concrete system. We address this issue extensively in Section 33 .
Institutive modules We call institutive modules those sets of terminological axioms of an institution specification describing how states with certain institutional properties can be reached, for instance, how an agent i can reach a state in which it enacts role r.
They can be seen as procedures that the institution define in order for the agents to bring institutional states of affairs about.
EXAMPLE 4. (An institutive module for roles) The fact that an agent i enacts a role r (rea(i, r)) is the effect of a corresponding enactment action ENACT(i, r) performed under certain circumstances (Formula 16), namely that the agent does not already enact the role, and that the agent satisfies given conditions (cond(i, r)), which might for instance pertain the computational capabilities required for an agent to play the chosen role, or its capability to interact with some specific system"s infrastructures. Formula 17 specifies instead the procedure counting as an action of type ENACT(i, r). Such a procedure is performed through a system infrastructure s, which notifies to i that it has been registered as enacting role r after sending the necessary piece of data d (SEND(i, s, d)), e.g., a valid credit card number. ¬rea(i, r) cond(i, r) ENACT(i, r).rea(i, r) (16) SEND(i, s, d) ◦ NOTIFY(s, i) ENACT(i, r) (17) Terminative modules Analogously, we call terminative modules those sets of terminological axioms stating how a state with certain institutional properties can be left. Rules of this kind state for instance how an agent can stop enacting a certain role. They can be thus thought of as procedures that the institution defines in order for the agent to see to it that certain institutional states stop holding.
EXAMPLE 5. (A terminative module for roles) Terminative modules for roles specify, for instance, how a transition type DEACT(i, r) can be executed which has as consequence the reaching of a state of type ¬rea(i, r): rea(i, r) DEACT(i, r).¬rea(i, r) (18) SEND(i, s, msg9) DEACT(i, r) (19) That is to say, i deacting a role r always leads to a state where 3 See in particular Example 6 and Definition 5 i does not enact role r; and i sending message No.9 to a specific interface infrastructure s count as i deacting role r.
Examples 3-5 have shown how roles can be formalized in our framework thereby getting a formal semantics: roles are also sets of terminological axioms concerning state types of the sort rea(i, r).
It is worth noticing that this modeling option is aligned with work on social theory addressing the concept of role such as [20].
In the previous sections we fully deployed the expressivity of the language introduced in Section 2.1 and used its semantics to provide a formal understanding of many essential aspects of institutions in terms of transition systems. This section spends a few words about the viability of performing automated reasoning in the logic presented. The satisfiability problem4 in logic ALCH( ,◦,¬,id) is undecidable since transition type inclusion axioms correspond to a version of what in Description Logic are known as role-value maps and logics extending ALC with role-value maps are known to be undecidable ([3]).
Tractable (i.e., polynomial time decidable) fragments of logic ALCH( ,◦,¬,id) can however be isolated which still exhibit some key expressive features. One of them is logic ELH(◦) . It is obtained from description logic EL, which contains only state types intersection , existential restriction ∃ and 5 , but extended with the ⊥ state type and with transition type inclusion axioms of a complex form: a1 ◦. . .◦an a (with n finite number). Logic ELH(◦) is also a fragment of the well investigated description logic EL++ whose satisfiability problem has been shown in [2] to be decidable in polynomial time. Despite the very limited expressivity of this fragment, some rudimentary institutional specifications can still be successfully represented. Specifically, institutive and terminative modules can be represented which contain transition types inclusion axioms. Restricted versions of status modules can also be represented enabling two essential deontic notions: it is possible (respectively, impossible) to reach a violation state by performing a transition of a certain type, and it is possible (respectively, impossible) to reach a legal state by performing a transition of a certain type. To this aim language Lins would need to be expanded with a set of state types {legal(i)}0≤i≤n whose intuitive meaning is to denote legal states as opposed to states of type viol(i).
Fragments like ELH(◦) could be used as target logics within theory approximation approaches ([24]) by aiming at compiling TBoxes expressed in ALCH( ,◦,¬,id) into approximations in those fragments.
In discussing Example 3 we observed how being entitled to make a bid does not imply being in state of making a bid. In other words, an institution can empower agents by means of appropriate rules but this empowerment can remain dead letter. Similar 4 This problem amounts to check whether a state description γ is satisfiable w.r.t. a given TBox T, i.e., to check if there exists a model m of T such that ∅ ⊂ I(γ). Notice that language ALCH( ,◦,¬,id) contains negation and intersection of arbitrary state types. It is well-known that if these operators are available then all most typical reasoning tasks at the TBox level can be reduced to the satisfiability problem. 5 Notice therefore that EL is a seriously restricted fragment of ALC since it does not contain the negation operator for state types (operators and ∀ remain thus undefinable). 632 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) observations apply also to deontic notions: agents might be allowed to perform certain transactions under some relevant conditions but they might be unable to do so under those same conditions. We refer to this kind of problems as infrastructural. The implementation of an institution in a concrete system calls therefore for the design of appropriate infrastructures or artifacts ([19]). The formal specification of an infrastructure amounts to the formal specification of interaction requirements, that is to say, the specification of which relevant transition types are executable and under what conditions.
DEFINITION 4. (Infrastructures as TBoxes) An infrastructure Inf = Γinf , Ainf for institution Ins is a TBox on Lbrute such that for all a ∈ L(Abridge) there exist terminological axioms in Γinf of the following form: γ ≡ ∃a. (a is executable exactly in γ states) and γ ≡ ∃¬a. (the negation of a is executable exactly in γ states).
In other words, an infrastructure specification states all and only the conditions under which an atomic brute transition type and its negation are executable, which occur in the brute alphabet of the bridge axioms of Ins. It states what can be in concrete done and under what conditions.
EXAMPLE 6. (Infrastructure specification) Consider the institution specified in Example 1. A simple infrastructure Inf for that institution could contain for instance the following terminological axioms for any pair of different agents i, j and message type msg: ∃SEND(msg33, i, j). (20) The formula states that it is always in the possibilities of agent i to send message No. 33 to agent j. It then follows on the grounds of Example 1 that agent i can always accept agent j. ∃ACCEPT(i, j). (21) Notice that the executability condition is just .
We call a concrete institution specification CIns an institution specification Ins coupled with an infrastructure specification Inf.
DEFINITION 5. (Concrete institution) A concrete institution obtained by joining the institution Ins = Γins, Ains and the infrastructure Inf = Γinf , Ainf is a TBox CIns = Γ, A such that Γ = Γins ∪ Γinf and A = Ains ∪ Ainf .
Obviously, different infrastructures can be devised for a same institution giving rise to different concrete institutions which makes precise implementation choices explicit. Of particular relevance are the implementation choices concerning abstract norms like the one represented in Formula 13. A designer can choose to regiment such norm ([15]), i.e., make violation states unreachable, via an appropriate infrastructure.
EXAMPLE 7. (Regimentation via infrastructure specification) Consider Example 3 and suppose the following translation rule to be also part of the institution: BNK(i, j, b) CC(i, j, b) ≡ PAY(i, j, b) (22) condition pay(i, j, b) ≡ rea(i, buyer) rea(j, seller) win bid(i, j, b) (23) The first formula states how the payment can be concretely carried out (via bank transfer or credit card) and the second just provides a concrete label grouping the institutional state types relevant for the norm. In order to specify a regimentation at the infrastructural level it is enough to state that: condition pay(i, j, b) ≡ ∃(BNK(i, j, b) CC(i, j, b)). (24) ¬condition pay(i, j, b) ≡ ∃¬(BNK(i, j, b) CC(i, j, b)). (25) In other words, in states of type condition pay(i, j, b) the only executable brute actions are BANK(i, j, b) or CC(i, j, b) and, therefore, PAY(i, j, b) would necessarily be executed. As a result, the following inclusion does not hold with respect to the corresponding concrete institution: condition pay(i, j, b) ∃¬PAY(i).viol(i).
This section briefly summarizes and adapts the perspective and results on organizational structures presented in [14, 11]. We refer to that work for a more comprehensive exposition.
Organizational structures typically concern the way agents interact within organizations. These interactions can be depicted as the links of a graph defined on the set of roles of the organization. Such links are then to be labeled on the basis of the type of interaction they stand for. First of all, it should be clear whether a link denotes that a certain interaction between two roles can, or ought to, or may etc. take place. Secondly, links should be labeled according to the transition type α they refer to and the conditions γ in which that transition can, ought to, may etc. take place. Links in a formal specification of an organizational structure stand therefore for statements of the kind: role r can (ought to, may) execute α w.r.t. role s if γ is the case. For the sake of simplicity, the following definition will consider only the can and ought-to interaction modalities. State and transition types in Lins ∪Lbrute will be used to label the links of the structure. Interaction modalities can therefore be of an institutional kind or of a brute kind.
DEFINITION 6. (Organizational structure) An organizational structure is a multi-graph: OS = Roles, {Cp}p∈Mod, {Op}p∈Mod where: • Mod denotes a set of pairs p = γ : α, that is, a set of state type (condition) and transition type (action) pairs of Lins∪Lbrute with α being an atomic transition-type indexed with a pair (i, j) denoting placeholders for the actor and the recipient of the transition; • C (can) denotes links to be interpreted in terms of the executability of the related α in γ, whereas O (ought) denotes links to be interpreted in terms of the obligation to execute the related α in γ.
By the expressions (r, s) ∈ Cγ:α and (r, s) ∈ Oγ:α we mean therefore: agents enacting role r can and, respectively, ought to interact with agents enacting role s by performing α in states of type γ.
As shown in [11] such formal representations of organizational structures are of use for investigating the structural properties (robustness, flexibility, etc.) that a given organization exhibits.
At this point all the formal means are put in place which allow us to formally represent institutions as well as organizational structures. The next and final step of the work consists in providing a formal relation between the two frameworks. This formal relation will make explicit how institutions are related to organizational structures and vice versa. In particular, it will become clear how a normative conception of the notion of role relates to a structural one, that is, how the view of roles as a sets of norms (specifying how an agent can enact and deact the role, and what social status it obtains by doing that) relates to the view of roles as positions within social structures.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 633 To translate a given concrete institution into a corresponding organizational structure we need a function t assigning pairs of roles to axioms. Let us denote with Sub the set of all state type inclusion statements γ1 γ2 that can be expressed on Lins ∪ Lbrute.
Function t is a partial function Sub Roles × Roles such that, for any x ∈ Sub if x = rea(i, r) rea(j, s) γ ∃α. (executability) or x = rea(i, r) rea(j, s) γ ∀¬α.viol(i) (obligation) then t(x) = (r, s), where α is an atomic transition-type indexed with a pair (i, j). That is to say, executability and obligation laws containing the enactment configuration rea(i, r) rea(j, s) as a premise and concerning transition of types α, with i actor and j recipient of the α transition, are translated into role pairs (r, s).
DEFINITION 7. (Correspondence of specifications) A concrete institution CIns = Γ, A is said to correspond to an organizational structure OS (and vice versa) if, for every x ∈ Γ: • x = rea(i, r) rea(j, s) γ ∃α. iff t(x) ∈ Cγ:α • x = rea(i, r) rea(j, s) γ ∀¬α.viol(i) iff t(x) ∈ Oγ:α Intuitively, function t takes axioms from Γ (i.e., the set of state type terminological axioms of CIns) and yields pairs of roles.
Definition 7 labels the yielded pairs accordingly to the syntactic form of the translated axioms. More concretely, axioms of the form rea(i, r) rea(j, s) γ ∃α. (executability laws) are translated into the pair (r, s) belonging to the executability dimension (i.e., C) of the organizational structure w.r.t. the execution of α under circumstances γ. Analogously, axioms of the form rea(i, r) rea(j, s) γ ∀¬α.viol(i) (obligation laws) are translated into the pair (r, s) belonging to the obligation dimension (i.e., O) of the organizational structure w.r.t. the execution of α under circumstances γ. Leaving technicalities aside, function t distills thus the terminological and infrastructural constraints of CIns into structural ones. The institutive, terminative and status modules of roles are translated into definitions of positions within a OS.
From a design perspective the interpretation of Definition 7 is twofold. On the one hand (from left to right), it can make explicit what the structural consequences are of a given institution supported by a given infrastructure. On the other hand (from right to left), it can make explicit what kind of institution is actually implemented by a given organizational structure. Let us see this in some more details.
Given a concrete institution CIns, Definition 7 allows a designer to be aware of the impact that specific terminological choices (in particular, the choice of certain bridge axioms) and infrastructural ones have at a structural level. Notice that Definition 7 supports the inference of links in a structure. By checking whether a given inclusion statement of the relevant syntactic form follows from CIns (i.e., the so-called subsumption problem of DL) it is possible, via t, to add new links to the corresponding organizational structure.
This can be recursively done by just adding any new inferred inclusion x to the previous set of axioms Γ, thus obtaining an updated institutional specification containing Γ ∪ {x}. This process can be thought of as the inference of structural links from institutional specifications. In other words, it is possible to use institution specifications as inference tools for structural specifications. For instance, the infrastructural choice formalized in Example 7 implies that for the pair of roles (buyer, seller), it is always the case that (buyer, seller) ∈ C :PAY(i,j,b). This link follows from links (buyer, seller) ∈ C :BNK(i,j,b) and (buyer, seller) ∈ C :CC(i,j,b) on the grounds of the bridge axioms of the institution (Formula 22).
Suppose now a designer to be interested in a system which, besides implementing an institution, also incorporates an organizational structure enjoying desirable structural properties such as flexibility, or robustness6 . By relating structural links to state type inclusions it is therefore possible to check whether adding a link in OS results in a stronger institutional specification, that is, if the corresponding inclusion statement is not already implied by Ins.
To draw a parallelism with what just said in the previous paragraph, this process can be thought of as the inference of norms and infrastructural constraints from the specification of organizational structures. To give a simple example consider again Example 6 but from a reversed perspective. Suppose a designer wants a fully connected graph in the dimension C :SEND(i,j) of the organizational structure. Exploiting Definition 7, we would obtain a number of executability laws in the fashion of Formula 20 for all roles in Roles (thus 2|Roles| axioms).
Definition 7 establishes a correspondence between two essentially different perspectives on the design of open systems allowing for feedbacks between the two to be formally analyzed. One last observation is in order. While given a concrete institution an organizational structure can be in principle fully specified (by checking for all -finitely many- relevant inclusion statements whether they are implied or not by the institution), it is not possible to obtain a full terminological specification from an organizational structure. This lies on the fact that in Definition 6 the strictly terminological information contained in the specification of an institution (eminently, the set of transition type axioms A and therefore the bridge axioms) is lost while moving to a structural description. This shows, in turn, that the added value of the specification of institutions lies precisely in the terminological link they establish between institutional and brute, i.e., system level notions.
The paper aimed at providing a comprehensive formal analysis of the institutional metaphor and its relation to the organizational one. The predominant formal tool has been description logic.
TBoxes has been used to represent the specifications of institutions (Definition 3) and their infrastructures (Definition 6), providing therefore a transition system semantics for a number of institutional notions (Examples 1-7). Multi-graphs has then been used to represent the specification of organizational structures (Definition 6). The last result presented concerned the definition of a formal correspondence between institution and organization specifications (Definition 7), which provides a formal way for switching between the two paradigms. All in all, these results deliver a way for relating abstract system specifications (i.e., institutions as sets of norms) to specifications that are closer to an implemented system (i.e., organizational structures).
[1] G. Azzoni. Il cavallo di caligola. In Ontologia sociale potere deontico e regole costitutive, pages 45-54. Quodlibet,
Macerata, Italy, 2003. [2] F. Baader, S. Brandt, and C. Lutz. Pushing the EL envelope.
In Proceedings of IJCAI"05, Edinburgh, UK, 2005.
Morgan-Kaufmann Publishers. [3] F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. Patel-Schneider. The Description Logic Handbook.
Cambridge Univ. Press, Cambridge, 2002. [4] C. Castelfranchi. The micro-macro constitution of power.
ProtoSociology, 18:208-268, 2003. 6 In [11] it is shown how these and analogous properties can be precisely measured within the type of structures presented in Definition 6. 634 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) [5] D. D. Harel amd Kozen and J. Tiuryn. Dynamic logic. In D. Gabbay and F. Guenthner, editors, Handbook of Philosophical Logic: Volume II, pages 497-604. Reidel,
Dordrecht, 1984. [6] M. Esteva, D. de la Cruz, and C. Sierra. ISLANDER: an electronic institutions editor. In Proceedings of AAMAS"02, pages 1045-1052, New York, NY, USA, 2002. ACM Press. [7] M. Esteva, J. Rodr´ıguez-Aguilar, B. Rosell, and J. Arcos.
Ameli: An agent-based middleware for electronic institutions. In Proceedings of AAMAS"04, New York, US,
July 2004. [8] M. S. Fox. An organizational view of distributed systems.
IEEE Trans. Syst. Man Cyber, 11(1)70 - 80, 1981. [9] C. Ghidini and F. Giunchiglia. A semantics for abstraction.
In R. de M´antaras and L. Saitta, editors, Proceedings of ECAI"04, pages 343-347, 2004. [10] D. Grossi, H. Aldewereld, J. V´azquez-Salceda, and F. Dignum. Ontological aspects of the implementation of norms in agent-based electronic institutions. Computational & Mathematical Organization Theory, 12(2-3):251-275,
April 2006. [11] D. Grossi, F. Dignum, V. Dignum, M. Dastani, and L. Royakkers. Structural evaluation of agent organizations.
In Proceedings of AAMAS"06, pages 1110 - 1112, Hakodate,
Japan, May 2006. ACM Press. [12] D. Grossi, F. Dignum, and J.-J. C. Meyer. Context in categorization. In L. Serafini and P. Bouquet, editors,
Proceedings of CRR"05, volume 136 of CEUR Workshp Proceedings, Paris, June 2005. [13] D. Grossi, J.-J. Meyer, and F. Dignum. Classificatory aspects of counts-as: An analysis in modal logic. Journal of Logic and Computation, October 2006. doi:
[14] J. F. H¨ubner, J. S. Sichman, and O. Boissier. Moise+: Towards a structural functional and deontic model for mas organization. In Proceedings of AAMAS"02, Bologna, Italy,
July 2002. ACM Press. [15] A. J. I. Jones and M. Sergot. On the characterization of law and computer systems: The normative systems perspective.
Deontic Logic in Computer Science, pages 275-307, 1993. [16] J. Krabbendam and J.-J. C. Meyer. Contextual deontic logics.
In P. McNamara and H. Prakken, editors, Norms, Logics and Information Systems, pages 347-362, Amsterdam, 2003. IOS Press. [17] J.-J. Meyer, F. de Boer, R. M. van Eijk, K. V. Hindriks, and W. van der Hoek. On programming karo agents. Logic Journal of the IGPL, 9(2), 2001. [18] D. C. North. Institutions, Institutional Change and Economic Performance. Cambridge University Press, Cambridge, 1990. [19] A. Omicini, A. Ricci, A. Viroli, C. Castelfranchi, and L. Tummolini. Coordination artifacts: Environment-based coordination for intelligent agents. In Proceedings of AAMAS"04, 2004. [20] I. P¨orn. Action theory and social science. Some formal models. Reidel Publishing Company, Dordrecht, The Netherlands, 1977. [21] S. Pufendorf. De Jure Naturae et Gentium. Amsterdam,
[22] A. S. Rao and M. P. Georgeff. Modeling rational agents within a BDI-architecture. In J. Allen, R. Fikes, and E. Sandewall, editors, Proceedings of KR"91), pages 473-484. Morgan Kaufmann: San Mateo, CA, USA, 1991. [23] D. W. P. Ruiter. A basic classification of legal institutions.
Ratio Juris, 10:357-371, 1997. [24] M. Schaerf and M. Cadoli. Tractable reasoning via approximation. Artificial Intelligence, 74(2):249-310, 1995. [25] J. Searle. The Construction of Social Reality. Free Press,
[26] J. V´azquez-Salceda. The role of Norms and Electronic Institutions in Multi-Agent Systems. Birkhuser Verlag AG,

Virtual organizations (VOs) facilitate coordinated resource sharing and problem solving involving various parties geographically remote [9]. VOs define and regulate interactions (thus facilitating coordination) among software and/or human agents that communicate to achieve individual and global goals [16]. VOs are realised as multi-agent systems and a most desirable feature of such systems is openness whereby new components designed by other parties are seamlessly accommodated. The use of norms, that is, prohibitions, permissions and obligations, in the specification and operation of multi-agent systems (MASs) is a promising approach to achieving openness [2, 4, 5, 6]. Norms regulate the observable behaviour of self-interested, heterogeneous software agents, designed by various parties who may not entirely trust each other [3, 24].
However, norm-regulated VOs may experience problems when norms assigned to their agents are in conflict (i.e., an action is simultaneously prohibited and permitted) or inconsistent (i.e., an action is simultaneously prohibited and obliged).
We propose a means to automatically detect and solve conflict and inconsistency in norm-regulated VOs. We make use of firstorder term unification [8] to find out if and how norms overlap in their influence (i.e., the agents and values of parameters in agents" actions that norms may affect). This allows for a fine-grained solution whereby the influence of conflicting or inconsistent norms is curtailed for particular sets of values. For instance, norms agent x is permitted to send bid(ag1, 20) and agent ag2 is prohibited from doing send bid(y, z) (where x, y, z are variables and ag1, ag2, 20 are constants) are in conflict because their agents, actions and terms (within the actions) unify. We solve the conflict by annotating norms with sets of values their variables cannot have, thus curtailing their influence. In our example, the conflict is avoided if we require that variable y cannot be ag1 and that z cannot be 20.
This paper is organized as follows. In the next section we provide a minimalistic definition for norm-regulated VOs. In section 3 we formally define norm conflicts, and explain how they are detected and resolved. In section 4 we describe how the machinery of the previous section can be adapted to detect and resolve norm inconsistencies. In section 5 we describe how our curtailed norms are used in norm-aware agent societies. In section 6 we explain how our machinery can be used to detect and solve indirect conflicts/inconsistencies, that is, those caused via relationships among actions; we extend and adapt the machinery to accommodate the delegation of norms. In section 7 we illustrate our approach with an example of norm-regulated software agents serving the Grid. In section 8 we survey related work and in section 9 we discuss our contributions and give directions for future work. 644 978-81-904262-7-5 (RPS) c 2007 IFAAMAS
Virtual organizations [17] allow various parties to come together to share resources and engage in problem solving. This paradigm has found strong applications in Web-service orchestration [14], e-Science [16] and the Grid [9]. VOs, in their most generic formulation, can be seen as coordination artifacts, allowing software and human agents to engage in sophisticated forms of interaction.
We formally represent our VOs as finite-state machines in which the actions of individual agents label the edges between discrete states. This provides us with a lowest common denominator: there are much more sophisticated, convenient and expressive ways to represent interactions among agents (e.g., AUML [19] and electronic institutions [20], to name a few), but for the sake of generalising our approach, we shall assume any higher-level formalism can be mapped onto a finite-state machine (possibly with some loss of expressiveness). We show in Figure 1 a simple VO graphically represented as a finite-state machine1 . The labels on the edges con//?>=<89:;0 p(X)  q(Y,Z) //?>=<89:;1 s(A,B) //?>=<89:;/.-,()*+2 Figure 1: Sample VO as a Finite-State Machine necting the states are first-order atomic formulae, denoted generically as ϕ; they stand for actions performed by individual agents.
We define our virtual organizations as follows: DEF. 1. A virtual organization I is the triple S, s0, E, T where S = {s1, . . . , sn} is a finite and non-empty set of states, s0 ∈ S is the initial state, E is a finite set of edges (s, s , ϕ), s, s ∈ S connecting s to s with a first-order atomic formula ϕ as a label, and T ⊆ S is the set of terminal states.
Notice that edges are directed, so (s, t, ϕ) = (t, s, ϕ). The sample VO of Figure 1 is formally represented as I = {0, 1, 2}, 0, {(0, 0, p(X)), (0, 1, q(Y, Z)), (1, 2, s(A, B)}, {2} . We assume an implicit existential quantification on any variables in ϕ, so that, for instance, s(A, B) stands for ∃A, B s(A, B).
VOs should allow for two kinds of non-determinism corresponding to choices autonomous agents can make, viz., i) the one arising when there is more than one edge leaving a state; and ii) the one arising from variables in the formulae ϕ labelling an edge, for which the agent carrying out the action instantiates. These kinds of non-determinism are desirable as they help define generic and flexible coordination mechanisms.
Another important concept we use is the roles of agents in VOs.
Roles, as exploited in, for instance, [18] and [20], help us abstract from individual agents and define a pattern of behaviour to which any agent that adopts a role ought to conform. Moreover, all agents with the same role are guaranteed the same rights, duties and opportunities. We shall make use of two finite, non-empty sets,
Agents = {ag1, . . . , agn} and Roles = {r1, . . . , rm}, representing, respectively, the sets of agent identifiers and role labels. We refer generically to first-order terms, i.e., constants, variables, and (nested) functions as τ.
The specification of a VO as a finite-state machine gives rise to a possibly infinite set of histories of computational behaviours, in which the actions labelling the paths from the initial state to a final state are recorded. Although the actions comprising a VO are carried out distributedly, we propose an explicit global account of all events. In practice, this can be achieved if we require individual 1 We adopt Prolog"s convention [1] and use strings starting with a capital letter to represent variables and strings starting with a small letter to represent constants. agents to declare/inform whatever actions they have carried out; this assumes trustworthy agents, naturally2 .
In order to record the authorship of the action, we annotate the formulae with the agents" unique identification. Our explicit global account of all events is a set of ground atomic formulae ϕ, that is, we only allow constants to appear as terms of formulae. Each formula is a truthful record of an action specified in the VO. Notice, however, that in the VO specification we do not restrict the syntax of the formulae: variables may appear in them, and when an agent performs an actual action then any variables of the specified action must be assigned values. We thus define: DEF. 2. A global execution state of a VO, denoted as Ξ, is a finite, possibly empty, set of tuples a : r, ¯ϕ, t where a ∈ Agents is an agent identifier, r ∈ Roles is a role label, ¯ϕ is a ground first-order atomic formula, and t ∈ IN is a time stamp.
For instance, ag1:buyer, p(a, 34), 20 states that agent ag1 adopting role buyer performed action p(a, 34) at instant 20. Given a VO I = S, s0, E, T , an execution state Ξ and a state s ∈ S, we can define a function which obtains a possible next execution state, viz., h(I, Ξ, s) = Ξ ∪ { a:r, ¯ϕ, t }, for one (s, s , ϕ) ∈ E. Such function h must address the two kinds of non-determinism above, as well as the choice on the potential agents that can carry out the action and their adopted roles. We also define a function to compute the set of all possible execution states, h∗ (I, Ξ, s) = {Ξ ∪ { a: r, ¯ϕ, t }|(s, s , ϕ) ∈ E}.
We advocate a separation of concerns whereby the virtual organization is complemented with an explicit and separate set of norms that further regulates the behaviour of agents as they take part in the enactment of an organization. The freedom of choice given to agents (captured via the non-determinism of VOs, explained above) must be curtailed in some circumstances. For instance, we might need to describe that whoever carried out ϕ is obliged to carry out ϕ , so that if there is a choice point in which ϕ appears as a label of an edge, then that edge should be followed.
Rather than embedding such normative aspects into the agents" design (say, by explicitly encoding normative aspects in the agents" behaviour) or into the VO itself (say, by addressing exceptions and deviant behaviour in the mechanism itself), we adopt the view that a VO should be supplemented with a separate set of norms that further regulates the behaviour of agents as they take part in the enactment of the organization. This separation of concerns should facilitate the design of MASs; however, the different components (VOs and norms) must come together at some point in the design process. Our norms are defined as below: DEF. 3. A norm, generically referred to as ν, is any construct of the form Oτ:τ ϕ, Pτ:τ ϕ, or Fτ:τ ϕ, where τ, τ are either variables or constants and ϕ is a first-order atomic formula.
We adopt the notation of [18]: Oτ:τ ϕ represents an obligation on agent τ taking up role τ to bring about ϕ; we recall that τ, τ are variables, constants and functions applied to (nested) terms. Pτ:τ ϕ and Fτ:τ ϕ stand for, respectively, a permission and a prohibition on agent τ, playing role τ to bring about ϕ. We shall assume that sorts are used to properly manipulate variables for agent identifiers and role labels.
We propose to formally represent the normative positions of all agents enacting a VO. By normative position we mean the social burden associated to individuals [12], that is, their obligations, permissions and prohibitions: 2 Non-trustworthy agents can be accommodated in this proposal, if we associate to each of them a governor agent which supervises the actions of the external agent and reports on them. This approach was introduced in [12] and is explained in section 5.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 645 DEF. 4. A global normative state Ω is a finite and possibly empty set of tuples ω = ν, td, ta, te where ν is a norm as above and td, ta, te ∈ IN are, respectively, the time when ν was declared (introduced), when ν becomes active and when ν expires, td ≤ ta < te.
It is worth noticing that we do not require the atomic formulae of norms to be ground: there could be variables in them. We assume an implicit universal quantification on the variables A, R of norms XA:Rϕ (for the deontic modalities X ∈ {O, P, F}), so that, for instance, PA:Rp(X, b, c) stands for ∀A ∈ Agents.∀R ∈ Roles.∃X.PA:Rp(X, b, c). We also refer to the tuples in Ω as norms.
Global normative states complement the execution states of VOs with information on the normative positions of individual agents.
We can relate them via a function to obtain a norm-regulated next execution state of a VOs, that is, g(I, Ξ, s, Ω, t) = Ξ , t standing for the time of the update. For instance, we might want all prohibited actions to be excluded from the next execution state, that is, g(I, Ξ, s, Ω, t) = Ξ ∪ { a :r, ¯ϕ, t }, (s, s , ϕ) ∈ E and Fa:rϕ, td, ta, te ∈ Ω, ta ≤ t ≤ te. We might equally wish that only permitted actions be chosen for the next execution state. We do not legislate, or indeed recommend, any particular way to regulate VOs. We do, however, offer simple underpinnings to allow arbitrary policies to be put in place.
In the same way that a normative state is useful to obtain the next execution state of a VO, we can use an execution state to update a normative state. For instance, we might want to remove any obligation specific to an agent and role, which has been carried out by that specific agent and role, that is, f(Ξ, Ω) = Ω − Obls,
Obls = { Oa:rϕ, td, ta, te ∈ Ω| a:r, ¯ϕ, t ∈ Ξ}.
The management (i.e., creation and updating) of global normative states is an interesting area of research. A simple but useful approach is reported in [11]: production rules generically depict how norms should be updated to reflect what agents have done and which norms currently hold. In this paper our focus is not to propose how Ω"s should be managed; we assume some mechanism which does that.
We now define means to detect and resolve norm conflicts and inconsistencies. We make use of the concept of unification [1, 8] of first-order terms τ, i.e., constants, variables or (nested) functions with terms as parameters. Initially we define substitutions: DEF. 5. A substitution σ is a finite and possibly empty set of pairs x/τ, where x is a variable and τ is a term.
We define the application of a substitution as:
(τ0, . . . , τn) · σ = pn (τ0 · σ, . . . , τn · σ).
Where X generically refers to any of the deontic modalities O, P, F.
Unification between two terms τ, τ consists of finding a substitution σ (also called, in this context, the unifier of τ and τ ) such that τ · σ = τ · σ. Many algorithms have been proposed to solve the unification problem, a fundamental issue in automated theorem proving [8], and more recent work provides very efficient ways to obtain unifiers. We shall make use of the following definition: DEF. 6. Relationship unify(τ, τ , σ) holds iff there is a possibly empty σ such that τ · σ = τ · σ.
We also define the unification of atomic formulae as unify(pn (τ0, . . . , τn), pn (τ0, . . . , τn), σ) which holds iff τi · σ = τi · σ, 0 ≤ i ≤ n. The unify relationship checks if a substitution σ is indeed a unifier for τ, τ but it can also be used to find such σ. We assume that unify is a suitable implementation of a unification algorithm which i) always terminates (possibly failing, if a unifier cannot be found); ii) is correct; and iii) has a linear computational complexity.
A norm conflict arises when an atomic formula labelling an edge in the VO, i.e. an action, is simultaneously permitted and prohibited [13]. In this case, both norms are in conflict with regard to their agents, roles and parameters (terms) of specific actions. We propose to use unification to detect when a prohibition and a permission overlap and to employ the unifier to resolve the conflict.
For instance, PA:Rp(c, X) and Fa:bp(Y, Z) are in conflict as they unify under σ = {A/a, R/b, Y/c, X/d}). If, however, the variables in Fa:bp(Y, Z) do not get the values in σ then there will be no conflicts. We thus propose to annotate the prohibitions in Ω with unifiers, called here conflict sets, and use these annotations to determine what the variables of the prohibition cannot be in future unifications in order to avoid a conflict. Each prohibition is henceforth regarded as having such an annotation, denoted as (Fτ1:τ2 ϕ) Σc, td, ta, te . Initially, this annotation is empty.
We propose to curtail the influence of prohibitions, thus giving agents more choices in the actions they may perform. A similar approach could be taken whereby permissions are curtailed, thus limiting the available agents" actions. Each of these policies is possible: we do not legislate over any of them nor do we give preference over any. In this paper we are interested in formalising such policies within a simple mathematical framework. A prohibition can be in conflict with various permissions in Ω. We, therefore, have to find the maximal set of conflicting pairs of permissions and prohibitions in Ω, by performing a pairwise inspection. This requires identifying the substitution between two pairs of norms that characterises a conflict. This is formally captured by the following definition: DEF. 7. A conflict arises between two tuples ω, ω ∈ Ω under a substitution σ, denoted as cflct(ω, ω , σ), iff the following conditions hold:
ϕ , td, ta, te
That is, a prohibition and a permission conflict (condition 1) if, and only if, the agents and roles they apply to and their actions, respectively, unify under σ (condition 2) and their activation periods overlap (condition 3). Substitution σ, the conflict set, unifies the agents, roles and atomic formulae of a permission and a prohibition. The annotation Σc does not play any role when detecting conflicts, but, as we show below, we have to update the annotation to reflect new curtailments to solve conflicts. For instance, cflct( (Fa:bp(Y, d)) ∅, 1, 3, 5 , PA:Rp(c, X), 2, 3, 4 , {A/a, R/b, Y/c, Z/X}) holds. We define below how we obtain the set of conflicting norms of a normative state Ω: DEF. 8. The finite, possibly empty set of conflicting norms of a normative state Ω, denoted as CFLS(Ω), is defined as CFLS(Ω) = { ω, ω , σ |ω, ω ∈ Ω, cflct(ω, ω , σ)}
A fine-grained way of resolving conflict can be done via unification. We detect the overlapping of the norms" influences, i.e. how they affect the behaviours of agents in the VO, and we curtail the 646 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) influence of the prohibition. We illustrate with Venn diagrams in Figure 2 the overlap of norm influences (left) which characterises a conflict and the curtailment necessary to resolve the conflict (right).
The illustration shows the space of possible values for p(X, Y ) and p(X, Y ) PA:Rp(c, X) Fa:bp(Y, Z) p(X, Y ) Fa:bp(Y, Z) PA:Rp(c, X) Figure 2: Overlap of Influence (Left) and Curtailment (Right) two portions of this space defining the scope of influence of norms PA:Rp(c, X) and Fa:bp(Y, Z). The scope of these norms overlap, illustrated by the intersection of boxes on the left, in actions with values, for instance, a, b, p(c, 2) , . . . , a, b, p(c, n) . The curtailment of the prohibition eliminates the intersection: it moves the scope of the norm influence to outside the influence of the permission. If there were multiple overlaps among one prohibition and various permissions, which is likely to happen, then the prohibition will be multiply curtailed to move the scope of the norm to avoid all intersections.
The algorithm shown in Figure 3 depicts how we obtain a conflictfree set of norms. It maps an existing set Ω possibly with conflictalgorithm conflictResolution(Ω, Ω ) input Ω output Ω begin Ω := Ω for each ω ∈ Ω s.t. ω = (Fa:r ¯ϕ) Σc, td, ta, te do if ω, ω , σ ∈ CFLS(Ω) then Ω := Ω − {ω} end for for each ω ∈ Ω s.t. ω = (Fτ1:τ2 ϕ) Σc, td, ta, te do ΣMAX c := [ ω,ω ,σc ∈CFLS(Ω ) {σc} Ω := (Ω − {ω}) ∪ { (Fτ1:τ2 ϕ) (Σc ∪ ΣMAX c ), td, ta, te } end for end Figure 3: Algorithm to Resolve Conflicts in a Set of Norms ing norms onto a new set Ω in which the conflicts (if any) are resolved. The algorithm forms Ω as a set that is conflict-freethis means that prohibitions are annotated with a conflict set that indicates which bindings for variables have to be avoided.
Initially, Ω is set to be Ω. The algorithm operates in two stages.
In the first stage (first for each loop), we remove all conflicting prohibitions ω = (Fa:r ¯ϕ) Σc, td, ta, te with ground agent/role pairs a : r and ground formulae ¯ϕ: the only way to resolve conflicts arising from such prohibitions is to remove them altogether, as we cannot curtail a fully ground norm. In the second stage (second for each loop), the remaining prohibitions in Ω are examined: the set CFLS(Ω ) contains all conflicts between permissions and the remaining prohibitions in Ω represented as tuples ω, ω , σc , with σc representing the conflict set. As a prohibition may have conflicts with various permissions, the set CFLS(Ω ) may contain more than one tuple for each prohibition. In order to provide an Ω that reflects all these conflicts for a specific prohibition, we have to form ΣMAX c containing all conflict sets σc for a given prohibition ω. The maximal set is used to update the annotation of the prohibition.
It is important to explain the need for updating the conflict set of prohibitions. Normative states change as a result of agents" actions [11]: existing permissions, prohibitions and obligations are revoked and/or new ones are put in place as a result of agents" interactions with the environment and other agents. Whenever new norms are added we must check for new conflicts and inconsistencies. If we only apply our algorithm to a pair consisting of an old and a new norm, then some re-processing of pairs of old norms (which were dealt with before) can be saved. The removal of norms from the set Ω is dealt with efficiently: each permission to be removed must be checked first for conflicts with any existing prohibition (re-processing can be avoided if we record the conflict, instead of detecting it again). If there is a conflict, then the conflict set will have been recorded in the prohibition"s annotation; this conflict set is thus removed from the prohibition"s annotation. The removal of obligations follows a similar process. Prohibitions are removed without the need to consider their relationships with other norms.
Our algorithm is correct in that it provides, for a given Ω, a new Ω in which i) all ground prohibitions which conflict with permissions have been removed; and ii) all remaining annotated prohibitions (Fτ:τ ¯ϕ) Σc, td, ta, te will not unify with any of the permissions in Ω , provided the unifier does not appear in Σc. The first requirement is addressed by the first for each loop, which does precisely this: it removes all ground prohibitions which unify with an obligation. The second requirement is addressed by the second for each loop: each prohibition has its annotation Σc added with ΣMAX c , thus accommodating the unifiers from all permissions that unify with the prohibition. It is easy to see that the algorithm always terminates: each of its two loops go through a finite set, processing one element at a time. The set CFLS(Ω) is computed in a finite number of steps as are the set operations performed within each loop. The algorithm has, however, exponential complexity3 , as the computation of CFLS(Ω) requires a pairwise comparison of all elements in Ω.
We illustrate our algorithm with the following example. Let there be the following global normative state Ω: j (FA:Rp(X, Y )) {}, 2, 2, 9 , Pa:rp(a, b), 3, 4, 8 , (Fa:rp(a, b)) {}, 2, 4, 12 Pa:rp(d, e), 3, 4, 9 , ff The first loop removes the ground prohibition, thus obtaining the following Ω : j (FA:Rp(X, Y )) {}, 2, 2, 9 , Pa:bp(c, d), 3, 4, 8 ,
Pe:f p(g, h), 3, 4, 9 ff We then have the following set of conflicting norms CFLS(Ω ): 8 < : * (FA:Rp(X, Y )) {}, 2, 2, 9 ,
Pa:bp(c, d), 3, 4, 8 , {A/a, R/b, X/c, Y/d} + , * (FA:Rp(X, Y )) {}, 2, 2, 9 ,
Pe:f p(g, h), 3, 4, 9 , {A/e, R/f, X/g, Y/h} +9 = ; For each prohibition ω ∈ Ω we retrieve all elements from w, w , σ ∈ CFLS(Ω ) and collect their σ"s in ΣMAX c . The final Ω is thus: 8 < : (FA:Rp(X, Y )) j {A/a, R/b, X/c, Y/d} {A/e, R/f, X/g, Y/h} ff , 2, 2, 9 ,
Pa:rp(a, b), 3, 4, 8 , Pa:rp(d, e), 3, 4, 9 , 9 = ; The annotated set of conflict sets should be understood as a record of past unifications, which informs how prohibitions should be used in the future in order to avoid any conflicts with permissions. We show in Section 5.1 how annotations are used by norm-aware agents.
If a substitution σ can be found that unifies an obligation and a prohibition, then a situation of norm inconsistency occurs [13].
The obligation demands that an agent performs an action that is forbidden. We can reuse the machinery, introduced above for resolving conflicts between permissions and prohibitions, in order to a) detect and b) resolve such inconsistencies. With Definition 7, we 3 The combinatorial effort is not necessary anymore if instead we maintain a set of norms conflict-free: each time a new norm is to be introduced then we compare it with the existing ones, thus making the maintenance process of linear complexity.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 647 express the nature of a conflict between a prohibition and permission. Similarly, a situation of inconsistency can be defined reusing this definition and replacing the P deontic modality with O. We can reuse the machinery for conflict resolution, developed previously, for resolving inconsistency. The conflict resolution algorithm can be applied without change to accumulate a maximal conflict set ΣMAX c for each prohibition in Ω that unifies with obligations.
We now describe how our norm-regulated VOs give rise to normaware agent societies. We address open and heterogeneous MASs: we accommodate external agents by providing each of them with a corresponding governor agent [12]. This is a kind of chaperon that interacts with an external agent, and observes and reports on its behaviour. We show our architecture in Figure 4 below: a number External Governor Agents Agents Tuple Space ag1 £ ¢   ¡ gov1 ⇐⇒ . . . . . . . . . . . . I, s, Ξ, Ω I, s , Ξ , Ω · · · agn £ ¢   ¡ govn ⇐⇒ Figure 4: Architecture for Norm-Aware Agent Societies of external agents interact (denoted by the  ) with their corresponding governor agents. The governor agents have access to the VO description I, the current state s of the VO enactment, the global execution state Ξ and the global normative state Ω.
Governor agents are able to write to and read from (denoted by the ⇐⇒) a shared memory space (e.g., a blackboard-like solution implemented as a tuple space), updating the global configuration (denoted by the  ) to reflect the dynamics of the VO enactment.
Governor agents are necessary because we cannot anticipate or legislate over the design or behaviour of external agents. We depict below how the pairs of governor/external agents work together: any non-deterministic choices on the VO are decided by the external agent; any normative aspects are considered by the governor agent.
The governor agent represents the external agent within the VO.
As such, it has the unique identifier of the external agent. The governor agent keeps an account of all roles the external agent is currently playing: in our VOs, it is possible for agents to take up more than one role simultaneously. We define in Figure 5 how governor agents work - we use a logic program for this purpose. We show 1 main(Id, Roles) ← 2 get tuple( I, s, Ξ, Ω )∧ 3 terminate(Id, Roles, I, Ξ, Ω) 4 main(Id, Roles) ← 5 get tuple( I, s, Ξ, Ω )∧ 6 filter norms(Id, Roles, Ω, ΩId )∧ 7 discuss norms(Id, Roles, I, s, Ξ, ΩId , Actions)∧ 8 update tuple(Roles, Actions, NewRoles)∧ 9 main(Id, NewRoles) Figure 5: Governor Agent as a Logic Program the lines of our clauses numbered 1-9. The first clause (lines 1-3) depicts the termination condition: get tuple/1 (line 2) retrieves I, s, Ξ, Ω from the shared tuple space and terminate/4 checks if the current VO enactment (recorded in Ξ) has come to an end.
The team of governor agents synchronise their access to the tuple space [12], thus ensuring each has a chance to function.
The second clause (lines 4-9) depicts a generic loop when the termination condition of the first clause does not hold. In this case, the tuple is again retrieved (line 5) and the governor agent proceeds (line 6) to analyse the current global normative state Ω with a view to obtaining the subset ΩId ⊆ Ω of norms referring to agent Id under roles Roles. Predicate filter norms/4 collects the norms which apply to agent Id (the governor agent"s external agent). In line 7 the governor agent, in possession of the applicable norms as well as other relevant information, interacts with the external agent to decide on a set of Actions which are norm-compliant - these actions will be used to update (line 8) the global execution state Ξ.
In the process of updating the state of execution, a new set of roles must be assigned to the external agent, represented as NewRoles.
The governor agent keeps looping (line 9) using the identifier for the external agent and its new set of roles.
We now explain how annotated norms are used by norm-aware agents. We do so via the definition of predicate check/2, which holds if its first argument, a candidate action (in the format of the elements of Ξ of Def. 2), is within the influence of an annotated prohibition ω, its second parameter. The definition, as a logic program, is shown in Figure 6. It checks (line 4) if the agent identifier 1 check(Action, ω) ← 2 Action = a:r, ¯ϕ, t ∧ 3 ω = (Fτ1:τ2 ϕ) Σc, td, ta, te ∧ 4 unify(a, τ1, σ) ∧ unify(r, τ2, σ) ∧ unify( ¯ϕ, ϕ, σ)∧ 5 forall(σ , (σc ∈ Σc, unify(σc, σ, σ )), MGUs)∧ 6 MGUs = ∅∧ 7 ta ≤ t ≤ te Figure 6: Check if Action is within Influence of Curtailed Norm and role of the action unify with the appropriate terms τ1, τ2 of ω and that the actions ¯ϕ, ϕ themselves unify, all under the same unifier σ. It then verifies (lines 5-6) that σ does not unify with any of the conflict sets in Σc. Finally, in line 7 it checks if the time of the action is within the norm temporal influence.
The verification of non-unification of σ with any element of Σc deserves an explanation. The elements of Σc are unifiers stating what values the variables of the norm cannot have, that is, they represent gaps in the original scope of the norm"s influence. The test thus equates to asking if the action is outside such gaps, that is, the action is within the curtailed scope of influence of the norm.
In our previous discussion, norm conflict and inconsistency were detected via a direct comparison of the atomic formulae representing the action. However, conflicts and inconsistencies may also arise indirectly via relationships among actions. For instance, if p(X) amounts to q(X, X), then norms PA:Rp(X) and FA:Rq(X,
X) are in conflict since PA:Rp(X) can be rewritten as PA:Rq(X,
X) and we thus have both PA:Rq(X, X) and FA:Rq(X, X). In the discussion below we concentrate on norm conflict, but norm inconsistency can be dealt with similarly, if we change the deontic modalities P for O.
Relationships among actions are domain-dependent. Different domains have distinct ways of relating their actions; engineers build ontologies to represent such relationships. We propose a simple means to account for such relationships and show how these can be connected to the mechanisms introduced above. Rather than making use of sophisticated formalisms for ontology construction, we employ a set of domain axioms, defined below: DEF. 9. The domain axioms, denoted as Δ, are a finite and possibly empty set of formulae ϕ → (ϕ1 ∧ · · · ∧ ϕn) where ϕ, ϕi, 1 ≤ i ≤ n, are atomic first-order formulae.

A fundamental feature of open, regulated multi-agent systems in which autonomous agents interact, is that participating agents are meant to comply with the conventions of the system. Norms can be used to model such conventions and hence as a means to regulate the observable behaviour of agents [6, 29]. There are many contributions on the subject of norms from sociologists, philosophers and logicians (e.g., [15, 28]). However, there are very few proposals for computational realisations of normative models - the way norms can be integrated in the design and execution of MASs. The few that exist (e.g. [10, 13, 24]), operate in a centralised manner which creates bottlenecks and single points-of-failure. To our knowledge, no proposal truly supports the distributed enactment of normative environments.
In our paper we approach that problem and propose means to handle conflicting commitments in open, regulated, multiagent systems in a distributed manner. The type of regulated MAS we envisage consists of multiple, concurrent, related activities where agents interact. Each agent may concurrently participate in several activities, and change from one activity to another. An agent"s actions within an activity may have consequences in the form of normative positions (i.e. obligations, permissions, and prohibitions) [26] that may constrain its future behaviour. For instance, a buyer agent who runs out of credit may be forbidden to make further offers, or a seller agent is obliged to deliver after closing a deal. We assume that agents may choose not to fulfill all their obligations and hence may be sanctioned by the MAS. Notice that, when activities are distributed, normative positions must flow from the activities in which they are generated to those in which they take effect. For instance, the seller"s obligation above must flow (or be propagated) from a negotiation activity to a delivery activity.
Since in an open, regulated MAS one cannot embed normative aspects into the agents" design, we adopt the view that the MAS should be supplemented with a separate set of norms that further regulates the behaviour of participating agents. In order to model the separation of concerns between the coordination level (agents" interactions) and the normative level (propagation of normative positions), we propose an artifact called the Normative Structure (NS).
Within a NS conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents" actions. For instance, an agent may be obliged and prohibited to do the 636 978-81-904262-7-5 (RPS) c 2007 IFAAMAS very same action in an activity. Since the regulation of a MAS entails that participating agents need to be aware of the validity of those actions that take place within it, such conflicts ought to be identified and possibly resolved if a claim of validity is needed for an agent to engage in an action or be sanctioned. However, ensuring conflict-freedom of a NS at design time is computationally intractable. We show this by formalising the notion of conflict, providing a mapping of NSs into Coloured Petri Nets (CPNs) and borrowing well-known theoretical results from the field of CPNs.
We believe that online conflict detection and resolution is required. Hence, we present a tractable algorithm for conflict resolution. This algorithm is paramount for the distributed enactment of a NS.
The paper is organised as follows. In Section 2 we detail a scenario to serve as an example throughout the paper. Next, in Section 3 we formally define the normative structure artifact. Further on, in Section 4 we formalise the notion of conflict to subsequently analyse the complexity of conflict detection in terms of CPNs in Section 5. Section 6 describes the computational management of NSs by describing their enactment and presenting an algorithm for conflict resolution. Finally, we comment on related work, draw conclusions and report on future work in Section 7.
We use a supply-chain scenario in which companies and individuals come together at an online marketplace to conduct business. The overall transaction procedure may be organised as six distributed activities, represented as nodes in the diagram in Figure 1. They involve different participants whose behaviour is coordinated through protocols.
In this scenario agents can play one of four roles: marExit Registration Payment Delivery Negotiation Coordination Model Contract Figure 1: Activity Structure of the Scenario ketplace accountant (acc), client, supplier (supp) and warehouse managers (wm). The arrows connecting the activities represent how agents can move from one activity to another.
After registering at the marketplace, clients and suppliers get together in an activity where they negotiate the terms of their transaction, i.e. prices, amounts of goods to be delivered, deadlines and other details. In the contract activity, the order becomes established and an invoice is prepared.
The client will then participate in a payment activity, verifying his credit-worthiness and instructing his bank to transfer the correct amount of money. The supplier in the meantime will arrange for the goods to be delivered (e.g. via a warehouse manager) in the delivery activity. Finally, agents can leave the marketplace conforming to a predetermined exit protocol. The marketplace accountant participates in most of the activities as a trusted provider of auditing tools.
In the rest of the paper we shall build on this scenario to exemplify the notion of normative structure and to illustrate our approach to conflict detection and resolution in a distributed setting.
In MASs agents interact according to protocols which naturally are distributed. We advocate that actions in one such protocol may have an effect on the enactment of other protocols. Certain actions can become prohibited or obligatory, for example. We take normative positions to be obligations, prohibitions and permissions akin to work described in [26].
The intention of adding or removing a normative position we call normative command. Occurrences of normative positions in one protocol may also have consequences for other protocols1 .
In order to define our norm language and specify how normative positions are propagated, we have been inspired by multi-context systems [14]. These systems allow the structuring of knowledge into distinct formal theories and the definition of relationships between them. The relationships are expressed as bridge rules - deducibility of formulae in some contexts leads to the deduction of other formulae in other contexts. Recently, these systems have been successfully used to define agent architectures [11, 23]. The metaphor translates to our current work as follows: the utterance of illocutions and/or the existence of normative positions in some normative scenes leads to the deduction of normative positions in other normative scenes. We are concerned with the propagation and distribution of normative positions within a network of distributed, normative scenes as a consequence of agents" actions. We take normative scenes to be sets of normative positions and utterances that are associated with an underlying interaction protocol corresponding to an activity.
In this section, we first present a simple language capturing these aspects and formally introduce the notions of normative scene, normative transition rule and normative structure. We give the intended semantics of these rules and show how to control a MAS via norms in an example.
The building blocks of our language are terms and atomic formulae: Def. 1. A term, denoted as t, is (i) any constant expressed using lowercase (with or without subscripts), e.g. a, b0, c or (ii) any variable expressed using uppercase (with or without subscripts), e.g. X, Y, Zb or (iii) any function f(t1, . . . , tn), where f is an n-ary function symbol and t1, .., tn are terms.
Some examples of terms and functions are Credit, price or offer(bible, 30) being respectively a variable, a constant and a function. We will be making use of identifiers throughout the paper, which are constant terms and also need the following definition: Def. 2. An atomic formula is any construct p(t1, . . . , tn), where p is an n-ary predicate symbol and t1, . . . , tn are terms.
The set of all atomic formulae is denoted as Δ.
We focus on an expressive class of MASs in which interaction is carried out by means of illocutionary speech acts exchanged among participating agents: Def. 3. Illocutions I are ground atomic formulae which have the form p(ag, r, ag , r , δ, t) where p is an element of 1 Here, we abstract from protocols and refer to them generically as activities.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 637 a set of illocutionary particles (e.g. inform, request, offer); ag, ag are agent identifiers; r, r are role identifiers; δ, an arbitrary ground term, is the content of the message, built from a shared content language; t ∈ N is a time stamp.
The intuitive meaning of p(ag, r, ag , r , m, t) is that agent ag playing role r sent message m to agent ag playing role r at time t. An example of an illocution is inform(ag4, supp, ag3, client, offer(wire, 12), 10). Sometimes it is useful to refer to illocutions that are not fully grounded, that is, those that may contain uninstantiated (free) variables. In the description of a protocol, for instance, the precise values of the message exchanged can be left unspecified. During the enactment of the protocol agents will produce the actual values which will give rise to a ground illocution. We can thus define illocution schemata: Def. 4. An illocution schema ¯I is any atomic formula p(ag, r, ag , r , δ, t) in which some of the terms may either be variables or may contain variables.
We first define normative scenes as follows: Def. 5. A normative scene is a tuple s = ids, Δs where ids is a scene identifier and Δs is the set of atomic formulae δ (i.e. utterances and normative positions) that hold in s.
We will also refer to Δs as the state of normative scene s.
For instance, a snapshot of the state of the delivery normative scene of our scenario could be represented as: Δs = 8 < : utt(request(sean, client, kev, wm, receive(wire, 200), 20)), utt(accept(kev, wm, sean, client, receive(wire, 200), 30)), obl(inform(kev, wm, sean, client, delivered(wire, 200), 30)) 9 = ; That is, agent Sean taking up the client role has requested agent Kev (taking up the warehouse manager role wm) to receive 200kg of wire, and agent Kev is obliged to deliver 200kg of wire to Sean since he accepted the request. Note that the state of a normative scene Δs evolves over time.
These normative scenes are connected to one another via normative transitions that specify how utterances and normative positions in one scene affect other normative scenes.
As mentioned above, activities are not independent since illocutions uttered in some of them may have an effect on other ones. Normative transition rules define the conditions under which a normative command is generated. These conditions are either utterances or normative positions associated with a given protocol (denoted e.g. activity : utterance) which yield a normative command, i.e. the addition or removal of another normative position, possibly related to a different activity. Our transition rules are thus defined: Def. 6. A normative transition rule R is of the form: R ::= V C V ::= ids : D | V, V D ::= N | utt(¯I) N ::= per(¯I) | prh(¯I) | obl(¯I) C ::= add(ids : N) | remove(ids : N) where ¯I is an illocution schema, N is a normative position (i.e. permission, prohibition or obligation), ids is an identifier for activity s and C is a normative command.
We endow our language with the usual semantics of rulebased languages [19]. Rules map an existing normative structure to a new normative structure where only the state of the normative scenes change. In the definitions below we rely on the standard concept of substitution [9].
Def. 7. A normative transition is a tuple b = idb, rb where idb is an identifier and rb is a normative transition rule.
We are proposing to extend the notion of MAS, regulated by protocols, with an extra layer consisting of normative scenes and normative transitions. This layer is represented as a bi-partite graph that we term normative structure. A normative structure relates normative scenes and normative transitions specifying which normative positions are to be generated or removed in which normative scenes.
Def. 8. A normative structure is a labelled bi-partite graph NS = Nodes, Edges, Lin , Lout . Nodes is a set S∪B where S is a set of normative scenes and B is a set of normative transitions. Edges is a set Ain ∪ Aout where Ain ⊆ S × B is a set of input arcs labelled with an atomic formula using the labelling function Lin : Ain → D; and Aout ⊆ B × S is a set of output arcs labelled with a normative position using the labelling function Lout : Aout → N. The following must hold:
rb must be of the form (ids : D) where s ∈ S and D ∈ Δ and ∃ain ∈ Ain such that ain = (s, b) and Lin (ain ) = D.
must be of the form add(ids : N) or remove(ids : N) where s ∈ S and ∃aout ∈ Aout such that aout = (b, s) and Lout (aout ) = N.
such that a = (s, b) and b = idb, rb and Lin (a) = D then (ids:D) must occur in the LHS of rb.
such that a = (b, s) and b = idb, rb and Lout (a) = N then add(ids : N) or remove(ids : N) must occur in the RHS of rb.
The first two points ensure that every atomic formulae on the LHS of a normative transition rule labels an arc entering the appropriate normative transition in the normative structure, and that the atomic formula on the RHS labels the corresponding outgoing arc. Points three and four ensure that labels from all incoming arcs are used in the LHS of the normative transition rule that these arcs enter into, and that the labels from all outgoing arcs are used in the RHS of the normative transition rule that these arcs leave.
The formal semantics will be defined via a mapping to Coloured Petri Nets in Section 5.1. Here we start defining the intended semantics of normative transition rules by describing how a rule changes a normative scene of an existing normative structure yielding a new normative structure.
Each rule is triggered once for each substitution that unifies the left-hand side V of the rule with the state of the corresponding normative scenes. An atomic formula (i.e. an utterance or a normative position) holds iff it is unifiable with an utterance or normative position that belongs to the state of the corresponding normative scene. Every time a rule is triggered, the normative command specified on the right-hand side of that rule is carried out, intending to add or remove a normative position from the state of the corresponding normative scene. However, addition is not unconditional as conflicts may arise. This topic will be treated in Sections 4 and 6.1. 638 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
In our running example we have the following exemplary normative transition rule: „ payment : obl(inform(X, client, Y, acc, pay(Z, P, Q), T )), payment : utt(inform(X, client, Y, acc, pay(Z, P, Q), T )) « delivery : add(obl(inform(Y, wm, X, client, delivered(Z, Q), T ))) That is, during the payment activity, an obligation on client X to inform accountant Y about the payment P of item Z at time T and the corresponding utterance which fulfills this obligation allows the flow of a norm to the delivery activity.
The norm is an obligation on agent Y (this time taking up the role of the warehouse manager wm) to send a message to client X that item Z has been delivered. We show in Figure 2 a diagrammatic representation of how activities and a normative structure relate: Payment Delivery Contract Normative Level Exit Registration Payment Delivery Negotiation Coordination Level Contract nt Figure 2: Activities and Normative Structure As illocutions are uttered during activities, normative positions arise. Utterances and normative positions are combined in transition rules, causing the flow of normative positions between normative scenes. The connection between the two levels is described in Section 6.2.
The terms deontic conflict and deontic inconsistency have been used interchangeably in the literature. However, in this paper we adopt the view of [7] in which the authors suggest that a deontic inconsistency arises when an action is simultaneously permitted and prohibited - since a permission may not be acted upon, no real conflict occurs. The situations when an action is simultaneously obliged and prohibited are, however, deontic conflicts, as both obligations and prohibitions influence behaviours in a conflicting fashion. The content of normative positions in this paper are illocutions. Therefore, a normative conflict arises when an illocution is simultaneously obliged and prohibited.
We propose to use the standard notion of unification [9] to detect when a prohibition and a permission overlap. For instance, an obligation obl(inform(A1, R1, A2, R2, p(c, X), T)) and a prohibition prh(inform(a1, r1, a2, r2, p(Y, d), T )) are in conflict as they unify under σ = {A1/a1, R1/r1, A2/a2,
R2/r2, Y/c, X/d, T/T }). We formally capture this notion: Def. 9. A (deontic) conflict arises between two normative positions N and N under a substitution σ, denoted as conflict(N, N , σ), if and only if N = prh(¯I), N = obl(¯I ) and unify(¯I,¯I , σ).
That is, a prohibition and an obligation are in conflict if, and only if, their illocutions unify under σ. The substitution σ, called here the conflict set, unifies the agents, roles and atomic formulae. We assume that unify is a suitable implementation of a unification algorithm which i) always terminates (possibly failing, if a unifier cannot be found); ii) is correct; and iii) has linear computational complexity.
Inconsistencies caused by the same illocution being simultaneously permitted and prohibited can be formalised similarly. In this paper we focus on prohibition/obligation conflicts, but the computational machinery introduced in Section 6.1 can equally be used to detect prohibition/permission inconsistencies, if we substitute modality obl for per.
In this section we introduce some background knowledge on CPNs assuming a basic understanding of ordinary Petri Nets. For technical details we refer the reader to [16]. We then map NSs to CPNs and analyse their properties.
CPNs combine the strength of Petri nets with the strength of functional programming languages. On the one hand,
Petri nets provide the primitives for the description of the synchronisation of concurrent processes. As noticed in [16],
CPNs have a semantics which builds upon true concurrency, instead of interleaving. In our opinion, a true-concurrency semantics is easier to work with because it is the way we envisage the connection between the coordination level and the normative level of a multi-agent system to be. On the other hand, the functional programming languages used by CPNs provide the primitives for the definition of data types and the manipulation of their data values. Thus, we can readily translate expressions of a normative structure. Last but not least, CPNs have a well-defined semantics which unambiguously defines the behaviour of each CPN.
Furthermore, CPNs have a large number of formal analysis methods and tools by which properties of CPNs can be proved.
Summing up, CPNs provide us with all the necessary features to formally reason about normative structures given that an adequate mapping is provided.
In accordance with Petri nets, the states of a CPN are represented by means of places. But unlike Petri Nets, each place has an associated data type determining the kind of data which the place may contain. A state of a CPN is called a marking. It consists of a number of tokens positioned on the individual places. Each token carries a data value which has the type of the corresponding place. In general, a place may contain two or more tokens with the same data value.
Thus, a marking of a CPN is a function which maps each place into a multi-set2 of tokens of the correct type. One often refers to the token values as token colours and one also refers to the data types as colour sets. The types of a CPN can be arbitrarily complex.
Actions in a CPN are represented by means of transitions.
An incoming arc into a transition from a place indicates that the transition may remove tokens from the corresponding place while an outgoing arc indicates that the transition may add tokens. The exact number of tokens and their data values are determined by the arc expressions, which are encoded using the programming language chosen for the CPN. A transition is enabled in a CPN if and only if all the 2 A multi-set (or bag) is an extension to the notion of set, allowing the possibility of multiple appearances of the same element.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 639 variables in the expressions of its incoming arcs are bound to some value(s) (each one of these bindings is referred to as a binding element). If so, the transition may occur by removing tokens from its input places and adding tokens to its output places. In addition to the arc expressions, it is possible to attach a boolean guard expression (with variables) to each transition. Putting all the elements above together we obtain a formal definition of CPN that shall be employed further ahead for mapping purposes.
Def. 10. A CPN is a tuple Σ, P, T, A, N, C, G, E, I where: (i) Σ is a finite set of non-empty types, also called colour sets; (ii) P is a finite set of places; (iii) T is a finite set of transitions; (iv) A is a finite set of arcs; (v) N is a node function defined from A into P × T ∪ T × P; (vi) C is a colour function from P into Σ; (vii) G is a guard function from T into expressions; (viii) E is an arc expression function from A into expressions; (ix) I is an initialisation function from P into closed expressions; Notice that the informal explanation of the enabling and occurrence rules given above provides the foundations to understand the behaviour of a CPN. In accordance with ordinary Petri nets, the concurrent behaviour of a CPN is based on the notion of step. Formally, a step is a non-empty and finite multi-set over the set of all binding elements. Let step S be enabled in a marking M. Then, S may occur, changing the marking M to M . Moreover, we say that marking M is directly reachable from marking M by the occurrence of step S, and we denote it by M[S > M .
A finite occurrence sequence is a finite sequence of steps and markings: M1[S1 > M2 . . . Mn[Sn > Mn+1 such that n ∈ N and Mi[Si > Mi+1 ∀i ∈ {1, . . . , n}. The set of all possible markings reachable for a net Net from a marking M is called its reachability set, and is denoted as R(Net, M).
Our normative structure is a labelled bi-partite graph.
The same is true for a Coloured Petri Net. We are presenting a mapping f from one to the other, in order to provide semantics for the normative structure and prove properties about it by using well-known theoretical results from work on CPNs. The mapping f makes use of correspondences between normative scenes and CPN places, normative transitions and CPN transitions and finally, between arc labels and CPN arc expressions.
S → P B → T Lin ∪ Lout → E The set of types is the singleton set containing the colour NP (i.e. Σ = {NP}). This complex type is structured as follows (we use CPN-ML [4] syntax): color NPT = with Obl | Per | Prh | NoMod color IP = with inform | declare | offer color UTT = record illp : IP ag1, role1, ag2, role2 : string content: string time : int color NP = record mode : NPT illoc : UTT Modelling illocutions as norms without modality (NoMod) is a formal trick we use to ensure that sub-nets can be combined as explained below. Arcs are mapped almost directly.
A is a finite set of arcs and N is a node function, such that ∀a ∈ A ∃a ∈ Ain ∪Aout . N(a) = a . The initialisation function I is defined as I(p) = Δs (∀s ∈ S where p is obtained from s using the mapping; remember that s = ids, Δs ).
Finally, the colour function C assigns the colour NP to every place: C(p) = NP (∀p ∈ P). We are not making use of the guard function G. In future work, this function can be used to model constraints when we extend the expressiveness of our norm language.
Having defined the mapping from normative structures to Coloured Petri Nets, we now look at properties of CPNs that help us understand the complexity of conflict detection.
One question we would like to answer is, whether at a given point in time, a given normative structure is conflict-free.
Such a snapshot of a normative structure corresponds to a marking in the mapped CPN.
Def. 11. Given a marking Mi, this marking is conflictfree if ¬∃p ∈ P. ∃np1, np2 ∈ Mi(p) such that np1.mode = Obl and np2.mode = Prh and np1.illoc and np2.illoc unify under a valid substitution.
Another interesting question would be, whether a conflict will occur from such a snapshot of the system by propagating the normative positions. In order to answer this question, we first translate the snapshot of the normative structure to the corresponding CPN and then execute the finite occurence sequence of markings and steps, verifying the conflict-freedom of each marking as we go along.
Def. 12. Given a marking Mi, a finite occurrence sequence Si, Si+1, ..., Sn is called conflict-free, if and only if Mi[Si > Mi+1 . . . Mn[Sn > Mn+1 and Mk is conflict-free for all k such that i ≤ k ≤ n + 1.
However, the main question we would like to investigate, is whether or not a given normative structure is conflictresistant, that is, whether or not the agents enacting the MAS are able to bring about conflicts through their actions.
As soon as one includes the possibility of actions (or utterances) from autonomous agents, one looses determinism.
Having mapped the normative structure to a CPN, we now add CPN models of the agents" interactions. Each form of agent interaction (i.e. each activity) can be modelled using CPNs along the lines of Cost et al. [5]. These nondeterministic CPNs feed tokens into the CPN that models the normative structure. This leads to the introduction of non-determinism into the combined CPN.
The lower half of figure 3 shows part of a CPN model of an agent protocol where the arc denoted with ‘1" represents some utterance of an illocution by an agent. The target transition of this arc, not only moves a token on to the next state of this CPN, but also places a token in the place corresponding to the appropriate normative scene in the CPN model of the normative structure (via arc ‘2"). Transition ‘3" finally could propagate that token in form of an obligation, for example. Thus, from a given marking, many different occurrence sequences are possible depending on the agents" actions. We make use of the reachability set R to define a situation in which agents cannot cause conflicts. 640 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Constructing the combined CPN Def. 13. Given a net N, a marking M is conflict-resistant if and only if all markings in R(N,M) are conflict-free.
Checking conflict-freedom of a marking can be done in polynomial time by checking all places of the CPN for conflicting tokens. Conflict-freedom of an occurrence sequence in the CPN that represents the normative structure can also be done in polynomial time since this sequence is deterministic given a snapshot.
Whether or not a normative structure is designed safely corresponds to checking the conflict-resistance of the initial marking M0. Now, verifying conflict-resistance of a marking becomes a very difficult task. It corresponds to the reachability problem in a CPN: can a state be reached or a marking achieved, that contains a conflict?. This reachability problem is known to be NP-complete for ordinary Petri Nets [22] and since CPNs are functionally identical, we cannot hope to verify conflict-resistance of a normative structure off-line in a reasonable amount of time. Therefore, distributed, run-time mechanisms are needed to ensure that a normative structure maintains consistency. We present one such mechanism in the following section.
Once a conflict (as defined in Section 4) has been detected, we propose to employ the unifier to resolve the conflict.
In our example, if the variables in prh(inform(a1, r1, a2, r2, p(Y, d), T )) do not get the values specified in substitution σ then there will not be a conflict. However, rather than computing the complement set of a substitution (which can be an infinite set) we propose to annotate the prohibition with the unifier itself and use it to determine what the variables of that prohibition cannot be in future unifications in order to avoid a conflict. We therefore denote annotated prohibitions as prh(¯I) Σ, where Σ = {σ1, . . . , σn}, is a set of unifiers. Annotated norms3 are interpreted as deontic constructs with curtailed influences, that is, their effect (on agents, roles and illocutions) has been limited by the set Σ of unifiers. A prohibition may be in conflict with various obligations in a given normative scene s = id, Δ and we need to record (and possibly avoid) all these conflicts. We define below an algorithm which ensures that a normative position will be added to a normative scene in such a way that it will not cause any conflicts. 3 Although we propose to curtail prohibitions, the same machinery can be used to define the curtailment of obligations instead. These different policies are dependent on the intended deontic semantics and requirements of the systems addressed. For instance, some MASs may require that their agents should not act in the presence of conflicts, that is, the obligation should be curtailed.
We propose a fine-grained way of resolving normative conflicts via unification. We detect the overlapping of the influences of norms , i.e. how they affect the behaviour of the concerned agents, and we curtail the influence of the normative position, by appropriately using the annotations when checking if the norm applies to illocutions. The algorithm shown in Figure 4 depicts how we maintain a conflict-free set of norms. It adds a given norm N to an existing, conflictfree normative state Δ, obtaining a resulting new normative state Δ which is conflict-free, that is, its prohibitions are annotated with a set of conflict sets indicating which bindings for variables have to be avoided for conflicts not to take place. algorithm addNorm(N, Δ) begin 1 timestamp(N) 2 case N of 3 per(¯I): Δ := Δ ∪ {N} 4 prh(I): if N ∈ Δ s.t. conflict(N, N , σ) then Δ := Δ 5 else Δ := Δ ∪ {N} 6 prh(¯I): 7 begin 8 Σ := ∅ 9 for each N ∈ Δ do 10 if conflict(N, N , σ) then Σ := Σ ∪ {σ} 11 Δ := Δ ∪ {N Σ} 12 end 13 obl(¯I): 14 begin 15 Δ1 := ∅; Δ2 := ∅ 16 for each (N Σ) ∈ Δ do 17 if N = prh(I) then 18 if conflict(N , N, σ) then Δ1 := Δ1 ∪ {N Σ} 19 else nil 20 else 21 if conflict(N , N, σ) then 22 begin 23 Δ1 := Δ1 ∪ {N Σ} 24 Δ2 := Δ2 ∪ {N (Σ ∪ {σ})} 25 end 26 Δ := (Δ − Δ1) ∪ Δ2 ∪ {N} 27 end 28 end case 29 return Δ end Figure 4: Algorithm to Preserve Conflict-Freedom The algorithm uses a case of structure to differentiate the different possibilities for a given norm N. Line 3 addresses the case when the given norm is a permission: N is simply added to Δ. Lines 4-5 address the case when we attempt to add a ground prohibition to a normative state: if it conflicts with any obligation, then it is discarded; otherwise it is added to the normative state. Lines 6-12 describe the situation when the normative position to be added is a nonground prohibition. In this case, the algorithm initialises Σ to an empty set and loops (line 9-10) through the norms N in the old normative state Δ. Upon finding one that conflicts with N, the algorithm updates Σ by adding the newly found conflict set σ to it (line 10). By looping through Δ, we are able to check any conflicts between the new prohibition and the existing obligations, adequately building the annotation Σ to be used when adding N to Δ in line 11.
Lines 13-27 describe how a new obligation is accommodated to an existing normative state. We make use of two initially empty, temporary sets, Δ1, Δ2. The algorithm loops through Δ (lines 16-25) picking up those annotated prohibitions N Σ which conflict with the new obligation. There are, however, two cases to deal with: the one when a ground The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 641 prohibition is found (line 17), and its exception, covering non-ground prohibitions (line 20). In both cases, the old prohibition is stored in Δ1 (lines 18 and 23) to be later removed from Δ (line 26). However, in the case of a nonground prohibition, the algorithm updates its annotation of conflict sets (line 24). The loop guarantees that an exhaustive (linear) search through a normative state takes place, checking if the new obligation is in conflict with any existing prohibitions, possibly updating the annotations of these conflicting prohibitions. In line 26 the algorithm builds the new updated Δ by removing the old prohibitions stored in Δ1 and adding the updated prohibitions stored in Δ2 (if any), as well as the new obligation N.
Our proposed algorithm is correct in that, for a given normative position N and a normative state Δ, it provides a new normative state Δ in which all prohibitions have annotations recording how they unify with existing obligations.
The annotations can be empty, though: this is the case when we have a ground prohibition or a prohibition which does not unify/conflict with any obligation. Permissions do not affect our algorithm and they are appropriately dealt with (line 3). Any attempt to insert a ground prohibition which conflicts, yields the same normative state (line 4). When a new obligation is being added then the algorithm guarantees that all prohibitions are considered (lines 14-27), leading to the removal of conflicting ground prohibitions or the update of annotations of non-ground prohibitions. The algorithm always terminates: the loops are over a finite set Δ and the conflict checks and set operations always terminate. The complexity of the algorithm is linear: the set Δ is only examined once for each possible case of norm to be added.
When managing normative states we may also need to remove normative positions. This is straightforward: permissions can be removed without any problems; annotated prohibitions can also be removed without further considerations; obligations, however, require some housekeeping.
When an obligation is to be removed, we must check it against all annotated prohibitions in order to update their annotations. We apply the conflict check and obtain a unifier, then remove this unifier from the prohibition"s annotation. We invoke the removal algorithm as removeNorm(N, Δ): it returns a new normative state Δ in which N has been removed, with possible alterations to other normative positions as explained.
The enactment of a normative structure amounts to the parallel, distributed execution of normative scenes and normative transitions. For illustrative purposes, hereafter we shall describe the interplay between the payment and delivery normative scenes and the normative transition nt linking them in the upper half of figure 2. With this aim, consider for instance that obl(inform(jules, client, rod, acc, pay(copper, 400, 350), T) ∈ Δpayment and that Δdelivery holds prh(inform(rod,wm, jules, client, delivered(Z, Q), T)).
Such states indicate that client Jules is obliged to pay £400 for 350kg of copper to accountant Rod according to the payment normative scene, whereas Rod, taking up the role of warehouse manager this time, is prohibited to deliver anything to client Jules according to the delivery normative scene.
For each normative scene, the enactment process goes as follows. Firstly, it processes its incoming message queue that contains three types of messages: utterances from the activity it is linked to; and normative commands either to add or remove normative positions. For instance, in our example, the payment normative scene collects the illocution I = utt((inform(jules, client, rod, acc, pay(copper, 400, 350), 35)) standing for client Jules" pending payment for copper (via arrow A in figure 2). Utterances are timestamped and subsequently added to the normative state.
We would have Δpayment = Δpayment ∪ {I}, in our example. Upon receiving normative commands to either add or remove a normative position, the normative scene invokes the corresponding addition or removal algorithm described in Section 6.1. Secondly, the normative scene acknowledges its state change by sending a trigger message to every outgoing normative transition it is connected to. In our example, the payment normative scene would be signalling its state change to normative transition nt.
For normative transitions, the process works differently.
Because each normative transition controls the operation of a single rule, upon receiving a trigger message, it polls every incoming normative scene for substitutions for the relevant illocution schemata on the LHS of its rule. In our example, nt (being responsible for the rule described in Section 3.4), would poll the payment normative scene (via arrow B) for substitutions. Upon receiving replies from them (in the form of sets of substitutions together with time-stamps), it has to unify substitutions from each of these normative scenes. For each unification it finds, the rule is fired, and hence the corresponding normative command is sent along to the output normative scene. The normative transition then keeps track of the firing message it sent on and of the time-stamps of the normative positions that triggered the firing. This is done to ensure that the very same normative positions in the LHS of a rule only trigger its firing once.
In our example, nt would be receiving σ = {X/jules,
Y/rod, Z/copper, Q/350} from the payment normative scene.
Since the substitions in σ unify with nt"s rule, the rule is fired, and the normative command add(delivery : obl(rod, wm, jules, client, delivered(copper, 350), T)) is sent along to the delivery normative scene to oblige Rod to deliver to client Jules 350kg of copper. After that, the delivery normative scene would invoke the addNorm algorithm from figure 4 with Δdelivery and N = obl(rod, wm, jules, client, delivered(copper, 350)) as arguments.
Our contributions in this paper are three-fold. Firstly, we introduce an approach for the management of and reasoning about norms in a distributed manner.
To our knowledge, there is little work published in this direction. In [8, 21], two languages are presented for the distributed enforcement of norms in MAS. However, in both works, each agent has a local message interface that forwards legal messages according to a set of norms. Since these interfaces are local to each agent, norms can only be expressed in terms of actions of that agent. This is a serious disadvantage, e.g. when one needs to activate an obligation to one agent due to a certain message of another one.
The second contribution is the proposal of a normative structure. The notion is fruitful because it allows the separation of normative and procedural concerns. The normative structure we propose makes evident the similarity between the propagation of normative positions and the propagation 642 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) of tokens in Coloured Petri Nets. That similarity readily suggests a mapping between the two, and gives grounds to a convenient analytical treatment of the normative structure, in general, and the complexity of conflict detection, in particular. The idea of modelling interactions (in the form of conversations) via Petri Nets has been investigated in [18], where the interaction medium and individual agents are modelled as CPN sub-nets that are subsequently combined for analysis. In [5], conversations are first designed and analysed at the level of CPNs and thereafter translated into protocols. Lin et al. [20] map conversation schemata to CPNs. To our knowledge, the use of this representation in the support of conflict detection in regulated MAS has not been reported elsewhere.
Finally, we present a distributed mechanism to resolve normative conflicts. Sartor [25] treats normative conflicts from the point of view of legal theory and suggests a way to order the norms involved. His idea is implemented in [12] but requires a central resource for norm maintenance. The approach to conflict detection and resolution is an adaptation and extension of the work on instantiation graphs reported in [17] and a related algorithm in [27]. The algorithm presented in the current paper can be used to manage normative states distributedly: normative scenes that happen in parallel have an associated normative state Δ to which the algorithm is independently applied each time a new norm is to be introduced.
These three contributions we present in this paper open many possibilities for future work. We should mention first, that as a broad strategy we are working on a generalisation of the notion of normative structure to make it operate with different coordination models, with richer deontic content and on top of different computational realisations of regulated MAS. As a first step in this direction we are taking advantage of the de-coupling between interaction protocols and declarative normative guidance that the normative structure makes available, to provide a normative layer for electronic institutions (as defined in [1]). We expect such coupling will endow electronic institutions with a more flexible -and more expressive- normative environment.
Furthermore, we want to extend our model along several directions: (1) to handle negation and constraints as part of the norm language, and in particular the notion of time; (2) to accommodate multiple, hierarchical norm authorities based on roles, along the lines of Cholvy and Cuppens [3] and power relationships as suggested by Carabelea et al. [2]; (3) to capture in the conflict resolution algorithm different semantics relating the deontic notions by supporting different axiomations (e.g., relative strength of prohibition versus obligation, default deontic notions, deontic inconsistencies).
On the theoretical side, we intend to use analysis techniques of CPNs in order to characterise classes of CPNs (e.g., acyclic, symmetric, etc.) corresponding to families of Normative Structures that are susceptible to tractable offline conflict detection. The combination of these techniques along with our online conflict resolution mechanisms is intended to endow MAS designers with the ability to incorporate norms into their systems in a principled way.
[1] J. L. Arcos, M. Esteva, P. Noriega, J. A. Rodr´ıguez, and C. Sierra. Engineering open environments with electronic institutions. Journal on Engineering Applications of Artificial Intelligence, 18(2):191-204, 2005. [2] C. Carabelea, O. Boissier, and C. Castelfranchi. Using social power to enable agents to reason about being part of a group.
In 5th Internat. Workshop, ESAW 2004, pages 166-177, 2004. [3] L. Cholvy and F. Cuppens. Solving normative conflicts by merging roles. In Fifth International Conference on Artificial Intelligence and Law, Washington, USA, 1995. [4] S. Christensen and T. B. Haagh. Design CPN - overview of CPN ML syntax. Technical report, University of Aarhus, 1996. [5] R. S. Cost, Y. Chen, T. W. Finin, Y. Labrou, and Y. Peng.
Using colored petri nets for conversation modeling. In Issues in Agent Communication, pages 178-192, London, UK, 2000. [6] F. Dignum. Autonomous Agents with Norms. Artificial Intelligence and Law, 7(1):69-79, 1999. [7] A. Elhag, J. Breuker, and P. Brouwer. On the Formal Analysis of Normative Conflicts. Information & Comms. Techn. Law, 9(3):207-217, Oct. 2000. [8] M. Esteva, W. Vasconcelos, C. Sierra, and J. A.
Rodr´ıguez-Aguilar. Norm consistency in electronic institutions. volume 3171 (LNAI), pages 494-505. Springer-Verlag, 2004. [9] M. Fitting. First-Order Logic and Automated Theorem Proving. Springer-Verlag, New York, U.S.A., 1990. [10] N. Fornara, F. Vigan`o, and M. Colombetti. An Event Driven Approach to Norms in Artificial Institutions. In AAMAS05 Workshop: Agents, Norms and Institutions for Regulated Multiagent Systems (ANI@REM), Utrecht, 2005. [11] D. Gaertner, P. Noriega, and C. Sierra. Extending the BDI architecture with commitments. In Proceedings of the 9th International Conference of the Catalan Association of Artificial Intelligence, 2006. [12] A. Garc´ıa-Camino, P. Noriega, and J.-A. Rodr´ıguez-Aguilar.
An Algorithm for Conflict Resolution in Regulated Compound Activities. In 7th Int.Workshop - ESAW "06, 2006. [13] A. Garc´ıa-Camino, J.-A. Rodr´ıguez-Aguilar, C. Sierra, and W. Vasconcelos. A Distributed Architecture for Norm-Aware Agent Societies. In DALT III, volume 3904 (LNAI), pages 89-105. Springer, 2006. [14] F. Giunchiglia and L. Serafini. Multi-language hierarchical logics or: How we can do without modal logics. Artificial Intelligence, 65(1):29-70, 1994. [15] J. Habermas. The Theory of Communication Action, Volume One, Reason and the Rationalization of Society. Beacon Press, 1984. [16] K. Jensen. Coloured Petri Nets: Basic Concepts, Analysis Methods and Practical Uses (Volume 1). Springer, 1997. [17] M. Kollingbaum and T. Norman. Strategies for resolving norm conflict in practical reasoning. In ECAI Workshop Coordination in Emergent Agent Societies 2004, 2004. [18] J.-L. Koning, G. Francois, and Y. Demazeau. Formalization and pre-validation for interaction protocols in a multi agent systems. In ECAI, pages 298-307, 1998. [19] B. Kramer and J. Mylopoulos. Knowledge Representation. In S. C. Shapiro, editor, Encyclopedia of Artificial Intelligence, volume 1, pages 743-759. John Wiley & Sons, 1992. [20] F. Lin, D. H. Norrie, W. Shen, and R. Kremer. A schema-based approach to specifying conversation policies. In Issues in Agent Communication, pages 193-204, 2000. [21] N. Minsky. Law Governed Interaction (LGI): A Distributed Coordination and Control Mechanism (An Introduction, and a Reference Manual). Technical report, Rutgers University, 2005. [22] T. Murata. Petri nets: Properties, analysis and applications.
Proceedings of the IEEE, 77(4):541-580, 1989. [23] S. Parsons, C. Sierra, and N. Jennings. Agents that reason and negotiate by arguing. Journal of Logic and Computation, 8(3):261-292, 1998. [24] A. Ricci and M. Viroli. Coordination Artifacts: A Unifying Abstraction for Engineering Environment-Mediated Coordination in MAS. Informatica, 29:433-443, 2005. [25] G. Sartor. Normative conflicts in legal reasoning. Artificial Intelligence and Law, 1(2-3):209-235, June 1992. [26] M. Sergot. A Computational Theory of Normative Positions.
ACM Trans. Comput. Logic, 2(4):581-622, 2001. [27] W. W. Vasconcelos, M. Kollingbaum, and T. Norman.
Resolving Conflict and Inconsistency in Norm-Regulated Virtual Organisations. In Proceedings of AAMAS "07, Hawai"i,
USA, 2007. IFAAMAS. [28] G. H. von Wright. Norm and Action: A Logical Inquiry.
Routledge and Kegan Paul, London, 1963. [29] M. Wooldridge. An Introduction to Multiagent Systems. John Wiley & Sons, Chichester, UK, Feb. 2002.

In the areas of machine learning and data mining (cf. [14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance. Various techniques have been suggested in this respect, ranging from the low-level integration of independently derived learning hypotheses (e.g. combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high-level combination of learning results obtained by heterogeneous learning agents using meta-learning (e.g. [3, 10, 21]).
All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and/or agent objectives (all agents are trying to cooperatively solve a single, global learning problem). Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems. In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others" learning results, etc.).
Examples for applications of this kind abound. Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g. exchange of patient data in distributed brain tumour diagnosis [2]) - however, they may permit the exchange of local learning hypotheses among different learners. In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g. in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]). Furthermore, agents might have a vested interest in negatively affecting other agents" learning performance. An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud.
Viewing learners as autonomous, self-directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non-agent) distribution and parallelisation in principle.
Despite the autonomy and self-directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to
own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed,
about their own learning processes and utilise information provided by other learners, and
improve their own learning performance.
Our model is based on the simple idea that autonomous learners should maintain meta-descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e. with the overall objective of improving their own learning results). Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents.
To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real-world domain (section 3). We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4).
Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6).
Our framework is based on providing formal (meta-level) descriptions of learning processes, i.e. representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process.
To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance. 1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space.
This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise. For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, . . . , n}.
For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e. a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, . . . , k}} in the case of clustering (i.e. a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k). For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined).
Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework.
For this, we assume that each learner is presented with a finite set of data D = d1, . . . dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training/update function f : H × D∗ → H which updates h given a series of samples d1, . . . , dk. In other words, one learning step always consists of applying the update function to all samples in D exactly once. We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H.
The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt. Such a learning step is equivalent to the following steps of computation:
calculate ft(ht−1, Dt) = ht,
gt(ht).
We denote the set of all possible learning steps by L. For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively. The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H).
A learning process is a finite, non-empty sequence l = l1 → l2 → . . . → ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e. the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step. We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g). The performance trace associated with a learning process l is the sequence q1, . . . , qn ∈ Qn where qi = g(li)(h(li)), i.e. the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses.
Such specifications allow agents to provide a selfdescription of their learning process. However, in communication among learning agents, it is often useful to provide only partial information about one"s internal learning process rather than its full details, e.g. when advertising this information in order to enter information exchange negotiations with others. For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process.
This allows us to describe properties of a learning process without specifying its details exhaustively. As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary). Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment). Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below.
In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents. For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)).
In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co-inhabited by other learning agents. This means that it is not only capable of meta-level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment. Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self-interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others" learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agent"s learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action. In the most general sense, our learning agent"s internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g. by appending the new learning action to one"s own learning process and leaving all information about others" learning processes untouched). Note that in λ({l1, . . . ln}) = (l, {l1, . . . ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment.
The λ-function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed. More formally, ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e. whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re-train on previously used data).
The goal of the agent is to output an optimal learning step in each iteration given the information that it has. One possibility of specifying this is to require that ∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e. the learning goal itself is not affected by agents" behaviour in the environment.
i j Dj Hj fj gj hj Di pD→D
. .. pD→D kD→D (Di, Dj) . . . . . . n/a . . .
Hi . .. ... n/a fi . .. ... n/a gi . .. n/a pg→h
. .. pg→h kg→h (gi, hj) hi . .. n/a ...
Table 1: Matrix of integration functions for messages sent from learner i to j to simply use g(l )(h(l )) as a running performance measure to evaluate how well the agent is performing.
This is too abstract and unspecific for our purposes: While it describes what agents should do (transform the settings for the next learning step in an optimal way), it doesn"t specify how this can be achieved in practice.
To specify how an agent"s learning process can be affected by integrating information received from others, we need to flesh out the details of how the learning steps it will perform can be modified using incoming information about learning processes described by other agents (this includes the acquisition of new learning data from the environment as a special case). In the most general case, we can specify this in terms of the potential modifications to the existing information about learning histories that can be performed using new information. For ease of presentation, we will assume that agents are stationary learning processes that can only record the previously executed learning step and only exchange information about this one individual learning step (our model can be easily extended to cater for more complex settings).
Let lj = Dj, Hj, fj, gj, hj be the current state of agent j when receiving a learning process description li = Di, Hi, fi, gi, hi from agent i (for the time being, we assume that this is a specific learning step and not a more vague, disjunctive description of properties of the learning step of i). Considering all possible interactions at an abstract level, we basically obtain a matrix of possibilities for modifications of j"s learning step specification as shown in Table 1. In this matrix, each entry specifies a family of integration functions pc→c
kc→c where c, c ∈ {D, H, f, g, h} and which define how agent j"s component cj will be modified using the information ci provided about (the same or a different component of) i"s learning step by applying pc→c r (ci, cj) for some r ∈ {1, . . . , kc→c }.
To put it more simply, the collections of p-functions an agent j uses specifies how it will modify its own learning behaviour using information obtained from i.
For the diagonal of this matrix, which contains the most common ways of integrating new information in one"s own learning model, obvious ways of modifying one"s own learning process include replacing cj by ci or ignoring ci altogether. More complex/subtle forms of learning process integration include: • Modification of Dj: append Di to Dj; filter out all elements from Dj which also appear in Di; append Di to Dj discarding all elements with attributes outside ranges which affect gj, or those elements already correctly classified by hj; • Modification of Hi: use the union/intersection of Hi and Hj; alternatively, discard elements of Hj that are inconsistent with Dj in the process of intersection or union, or filter out elements that cannot be obtained using fj (unless fj is modified at the same time) • Modification of fj: modify parameters or background knowledge of fj using information about fi; assess their relevance by simulating previous learning steps on Dj using gj and discard those that do not help improve own performance • Modification of hj: combine hj with hi using (say) logical or mathematical operators; make the use of hi contingent on a pre-integration assessment of its quality using own data Dj and gj While this list does not include fully fledged, concrete integration operations for learning processes, it is indicative of the broad range of interactions between individual agents" learning processes that our framework enables.
Note that the list does not include any modifications to gj. This is because we do not allow modifications to the agent"s own quality measure as this would render the model of rational (learning) action useless (if the quality measure is relative and volatile, we cannot objectively judge learning performance). Also note that some of the above examples require consulting other elements of lj than those appearing as arguments of the p-operations; we omit these for ease of notation, but emphasise that information-rich operations will involve consulting many different aspects of lj.
Apart from operations along the diagonal of the matrix, more exotic integration operations are conceivable that combine information about different components. In theory we could fill most of the matrix with entries for them, but for lack of space we list only a few examples: • Modification of Dj using fi: pre-process samples in fi, e.g. to achieve intermediate representations that fj can be applied to • Modification of Dj using hi: filter out samples from Dj that are covered by hi and build hj using fj only on remaining samples • Modification of Hj using fi: filter out hypotheses from Hj that are not realisable using fi • Modification of hj using gi: if hj is composed of several sub-components, filter out those sub-components that do not perform well according to gi • . . .
Finally, many messages received from others describing properties of their learning processes will contain information about several elements of a learning step, giving rise to yet more complex operations that depend on which kinds of information are available.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681 Figure 2: Screenshot of our simulation system, displaying online vessel tracking data for the North Sea region
As an illustration of our framework, we present an agentbased data mining system for clustering-based surveillance using AIS (Automatic Identification System [1]) data. In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts.
This data contains the ship"s identity, type, position, course, speed, navigational status and other safety-related information. Figure 2 shows a screenshot of our simulation system.
It is the task of AIS agencies to detect anomalous behaviour so as to alarm police/coastguard units to further investigate unusual, potentially suspicious behaviour. Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account. While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist/military attacks) it is obviously useful to pre-process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts.
To support this automated pre-processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user. However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results. Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems.
design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system:
individual agents,
descriptions of learning processes, and
decisions.
Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions. Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, . . . , n} including things such as width, length, etc. with real-valued domains ([Ai] = R for all i).
In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k-means and k-medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles). This means that the hypothesis space can be defined as H = { c1, . . . , ck |ci ∈ R|A| } i.e. the set of all possible sets of k cluster centroids in |A|-dimensional Euclidean space. For each hypothesis h = c1, . . . , ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, . . . , ck , d) = arg min 1≤j≤k |d − cj| i.e. d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance.
For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi. Then, we generate a set of fake vessels Fi such that |Fi| = |Vi|. These two sets assess the agent"s ability to detect suspicious vessels. For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid. Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r-value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi. With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi.
As concerns 2., we use a simple Contract-Net Protocol (CNP) [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls-For-Proposals (CfPs), advertising their own numerical model quality. In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis/model. We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP.
Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model. If the
bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues.
To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process. Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be one"s own model quality and g that advertised by the CfP (or highest bid, respectively). If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g /g) and ignore (reject) it else. If two agents make a deal, they exchange their learning hypotheses (models). In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided).
As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receiver"s own model and hi is the provider"s model): • ph→h (hi, hj) : - m-join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj. - m-select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model. (Unlike m-join this method does not prefer own clusters over others".) • ph→D (hi, Dj) : - m-filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj.
Whenever m is large enough to encompass all clusters, we simply write join or filter for them. In section 4 we analyse the performance of each of these two classes for different choices of m.
It is noteworthy that this agent-based distributed data mining system is one of the simplest conceivable instances of our abstract architecture. While we have previously applied it also to a more complex market-based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues.
Figure 3 shows results obtained from simulations with three learning agents in the above system using the k-means and k-medoids clustering methods respectively. We partition the total dataset of 300 ships into three disjoint sets of
agent. The Single Agent is learning from the whole dataset.
The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies-Bouldin index [9].
For m-select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k-means (top) and k-medoids (bottom) methods model size. For m-join and m-filter we assume m = 3 to limit the extent to which models increase over time.
During each experiment the learning agents receive ship descriptions in batches of 10 samples. Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary. Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties. The validation set contains 100 real and 100 randomly generated fake ships. To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships).
In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self-interested learning agents could improve agent performance compared to a setting of isolated learners. Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms.
As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases. This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents).
We can see that the quality of these operations is very close to the Single Agent that has access to all training data.
For the restricted (m < k) m-join, m-filter and m-select methods we can also observe an interesting distinction,
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k-means (top) and k-medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies. This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents. This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance.
Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m-filter operation, for example, decreases the number of learning samples and thus can speed up the learning process. The relative number of filtered examples measured in our experiments is shown in the following table. k-means k-medoids filtering 30-40 % 10-20 % m-filtering 20-30 % 5-15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners.
We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems.
Very often, approaches that are allegedly agent-based completely disregard agent autonomy and prescribe local decision-making procedures a priori. A typical example for this type of system is the one suggested by Caragea et al. [6] which is based on a distributed support-vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm. A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour.
The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents.
Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case-based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method). An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al. [13] which is also specifically designed for a particular learning algorithm. The same is true of [22, 23] which both present market-based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners.
A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al. [11] is a step in the right direction in terms of revealing only partial information about one"s learning process as it deals with limited information sharing in distributed clustering.
Papyrus [3] is a system that provides a markup language for meta-description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks.
The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses. Agents were able to critique each others" hypotheses until agreement was reached.
However, all agents in this system were identical and the system was strictly cooperative.
The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system.
As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work.
In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining. This framework constitutes, to our knowledge, the first attempt
to capture complex forms of interaction between heterogeneous and/or self-interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity.
To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market-based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour.
Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation.
Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free. This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results. Secondly, we have not experimented with agents using completely different learning algorithms (e.g. symbolic and numerical). In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents" learning processes. Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be.
These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject.
Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558-03-0819 and Office for Naval Research project N00014-06-1-0232.

Both the research and practice of combinatorial auctions have grown rapidly in the past ten years. In a combinatorial auction bidders can place bids on combinations of items, called packages or bidsets, rather than just individual items. Once the bidders place their bids, it is necessary to find the allocation of items to bidders that maximizes the auctioneer"s revenue. This problem, known as the winner determination problem, is a combinatorial optimization problem and is NP-Hard [10]. Nevertheless, several algorithms that have a satisfactory performance for problem sizes and structures occurring in practice have been developed. The practical applications of combinatorial auctions include: allocation of airport takeoff and landing time slots, procurement of freight transportation services, procurement of public transport services, and industrial procurement [2].
Because of their wide applicability, one cannot hope for a general-purpose winner determination algorithm that can efficiently solve every instance of the problem. Thus, several approaches and algorithms have been proposed to address the winner determination problem. However, most of the existing winner determination algorithms for combinatorial auctions are centralized, meaning that they require all agents to send their bids to a centralized auctioneer who then determines the winners. Examples of these algorithms are CASS [3], Bidtree [11] and CABOB [12]. We believe that distributed solutions to the winner determination problem should be studied as they offer a better fit for some applications as when, for example, agents do not want to reveal their valuations to the auctioneer.
The PAUSE (Progressive Adaptive User Selection Environment) auction [4, 5] is one of a few efforts to distribute the problem of winner determination amongst the bidders.
PAUSE establishes the rules the participants have to adhere to so that the work is distributed amongst them. However, it is not concerned with how the bidders determine what they should bid.
In this paper we present two algorithms, pausebid and cachedpausebid, which enable agents in a PAUSE auction to find the bidset that maximizes their utility. Our algorithms implement a myopic utility maximizing strategy and are guaranteed to find the bidset that maximizes the agent"s utility given the outstanding best bids at a given time. pausebid performs a branch and bound search completely from scratch every time that it is called. cachedpausebid is a caching-based algorithm which explores fewer nodes, since it caches some solutions. 694 978-81-904262-7-5 (RPS) c 2007 IFAAMAS
A PAUSE auction for m items has m stages. Stage 1 consists of having simultaneous ascending price open-cry auctions and during this stage the bidders can only place bids on individual items. At the end of this state we will know what the highest bid for each individual item is and who placed that bid. Each successive stage k = 2, 3, . . . , m consists of an ascending price auction where the bidders must submit bidsets that cover all items but each one of the bids must be for k items or less. The bidders are allowed to use bids that other agents have placed in previous rounds when building their bidsets, thus allowing them to find better solutions.
Also, any new bidset has to have a sum of bid prices which is bigger than that of the currently winning bidset. At the end of each stage k all agents know the best bid for every subset of size k or less. Also, at any point in time after stage
monotonically as new bidsets are submitted. Since in the final round all agents consider all possible bidsets, we know that the final winning bidset will be one such that no agent can propose a better bidset. Note, however, that this bidset is not guaranteed to be the one that maximizes revenue since we are using an ascending price auction so the winning bid for each set will be only slightly bigger than the second highest bid for the particular set of items. That is, the final prices will not be the same as the prices in a traditional combinatorial auction where all the bidders bid their true valuation. However, there remains the open question of whether the final distribution of items to bidders found in a PAUSE auction is the same as the revenue maximizing solution. Our test results provide an answer to this question.
The PAUSE auction makes the job of the auctioneer very easy. All it has to do is to make sure that each new bidset has a revenue bigger than the current winning bidset, as well as make sure that every bid in an agent"s bidset that is not his does indeed correspond to some other agents" previous bid. The computational problem shifts from one of winner determination to one of bid generation. Each agent must search over the space of all bidsets which contain at least one of its bids. The search is made easier by the fact that the agent needs to consider only the current best bids and only wants bidsets where its own utility is higher than in the current winning bidset. Each agent also has a clear incentive for performing this computation, namely, its utility only increases with each bidset it proposes (of course, it might decrease with the bidsets that others propose).
Finally, the PAUSE auction has been shown to be envy-free in that at the conclusion of the auction no bidder would prefer to exchange his allocation with that of any other bidder [2].
We can even envision completely eliminating the auctioneer and, instead, have every agent perform the task of the auctioneer. That is, all bids are broadcast and when an agent receives a bid from another agent it updates the set of best bids and determines if the new bid is indeed better than the current winning bid. The agents would have an incentive to perform their computation as it will increase their expected utility. Also, any lies about other agents" bids are easily found out by keeping track of the bids sent out by every agent (the set of best bids). Namely, the only one that can increase an agent"s bid value is the agent itself.
Anyone claiming a higher value for some other agent is lying.
The only thing missing is an algorithm that calculates the utility-maximizing bidset for each agent.
A bid b is composed of three elements bitems (the set of items the bid is over), bagent (the agent that placed the bid), and bvalue (the value or price of the bid). The agents maintain a set B of the current best bids, one for each set of items of size ≤ k, where k is the current stage. At any point in the auction, after the first round, there will also be a set W ⊆ B of currently winning bids. This is the set of bids that covers all the items and currently maximizes the revenue, where the revenue of W is given by r(W) = b∈W bvalue . (1) Agent i"s value function is given by vi(S) ∈ where S is a set of items. Given an agent"s value function and the current winning bidset W we can calculate the agent"s utility from W as ui(W) = b∈W | bagent=i vi(bitems ) − bvalue . (2) That is, the agent"s utility for a bidset W is the value it receives for the items it wins in W minus the price it must pay for those items. If the agent is not winning any items then its utility is zero.
The goal of the bidding agents in the PAUSE auction is to maximize their utility, subject to the constraint that their next set of bids must have a total revenue that is at least bigger than the current revenue, where is the smallest increment allowed in the auction. Formally, given that W is the current winning bidset, agent i must find a g∗ i such that r(g∗ i ) ≥ r(W) + and g∗ i = arg max g⊆2B ui(g), (3) where each g is a set of bids that covers all items and ∀b∈g (b ∈ B) or (bagent = i and bvalue > B(bitems ) and size(bitems ) ≤ k), and where B(items) is the value of the bid in B for the set items (if there is no bid for those items it returns zero). That is, each bid b in g must satisfy at least one of the two following conditions. 1) b is already in B, 2) b is a bid of size ≤ k in which the agent i bids higher than the price for the same items in B.
According to the PAUSE auction, during the first stage we have only several English auctions, with the bidders submitting bids on individual items. In this case, an agent"s dominant strategy is to bid higher than the current winning bid until it reaches its valuation for that particular item. Our algorithms focus on the subsequent stages: k > 1. When k > 1, agents have to find g∗ i . This can be done by performing a complete search on B. However, this approach is computationally expensive since it produces a large search tree. Our algorithms represent alternative approaches to overcome this expensive search.
In the pausebid algorithm (shown in Figure 1) we implement some heuristics to prune the search tree. Given that bidders want to maximize their utility and that at any given point there are likely only a few bids within B which The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 695 pausebid(i, k)
= i or vi(bitems ) > bvalue
+new Bid(bitems , i, vi(bitems ))
vi(S) > 0 and ¬∃b∈Bbitems = S
← ∅ £ Global variable
← ui(W)£ Global variable
− B(bitems )
) − u∗
| bagent = i
← B(bitems )
← B(bitems ) + my-payment ·bvalue −B(bitems ) surplus
Figure 1: The pausebid algorithm which implements a branch and bound search. i is the agent and k is the current stage of the auction, for k ≥ 2. the agent can dominate, we start by defining my-bids to be the list of bids for which the agent"s valuation is higher than the current best bid, as given in B. We set the value of these bids to be the agent"s true valuation (but we won"t necessarily be bidding true valuation, as we explain later).
Similarly, we set their-bids to be the rest of the bids from B.
Finally, the agent"s search list is simply the concatenation of my-bids and their-bids. Note that the agent"s own bids are placed first on the search list as this will enable us to do more pruning (pausebid lines 3 to 9). The agent can now perform a branch and bound search on the branch-on-bids tree produced by these bids. This branch and bound search is implemented by pbsearch (Figure 2). Our algorithm not only implements the standard bound but it also implements other pruning techniques in order to further reduce the size of the search tree.
The bound we use is the maximum utility that the agent can expect to receive from a given set of bids. We call it u∗ .
Initially, u∗ is set to ui(W) (pausebid line 11) since that is the utility the agent currently receives and any solution he proposes should give him more utility. If pbsearch ever comes across a partial solution where the maximum utility the agent can expect to receive is less than u∗ then that subtree is pruned (pbsearch line 21). Note that we can determine the maximum utility only after the algorithm has searched over all of the agent"s own bids (which are first on the list) because after that we know that the solution will not include any more bids where the agent is the winner thus the agent"s utility will no longer increase. For example, pbsearch(bids, g)
b∈g | bagent=i B(bitems ))
← g
← max-utility
= i
−(r(g) − ri(g)) − h(¯Ig)
xitems ∩ bitems = ∅}, g) £ b is In
xitems ∩ bitems = ∅}, g) £ b is In
Figure 2: The pbsearch recursive procedure where bids is the set of available bids and g is the current partial solution. if an agent has only one bid in my-bids then the maximum utility he can expect is equal to his value for the items in that bid minus the minimum possible payment we can make for those items and still come up with a set of bids that has revenue greater than r(W). The calculation of the minimum payment is shown in line 19 for the partial solution case and line 9 for the case where we have a complete solution in pbsearch. Note that in order to calculate the min-payment for the partial solution case we need an upper bound on the payments that we must make for each item. This upper bound is provided by h(S) = s∈S max b∈B | s∈bitems bvalue size(bitems) . (4) This function produces a bound identical to the one used by the Bidtree algorithm-it merely assigns to each individual item in S a value equal to the maximum bid in B divided by the number of items in that bid.
To prune the branches that cannot lead to a solution with revenue greater than the current W, the algorithm considers both the values of the bids in B and the valuations of the
agent. Similarly to (4) we define hi(S, k) = s∈S max S | size(S )≤k and s∈S and vi(S )>0 vi(S ) size(S ) (5) which assigns to each individual item s in S the maximum value produced by the valuation of S divided by the size of S , where S is a set for which the agent has a valuation greater than zero, contains s, and its size is less or equal than k. The algorithm uses the heuristics h and hi (lines 15 and 19 of pbsearch), to prune the just mentioned branches in the same way an A∗ algorithm uses its heuristic. A final pruning technique implemented by the algorithm is ignoring any branches where the agent has no bids in the current answer g and no more of the agent"s bids are in the list (pbsearch lines 6 and 7).
The resulting g∗ found by pbsearch is thus the set of bids that has revenue bigger than r(W) and maximizes agent i"s utility. However, agent i"s bids in g∗ are still set to his own valuation and not to the lowest possible price. Lines 17 to 20 in pausebid are responsible for setting the agent"s payments so that it can achieve its maximum utility u∗ . If the agent has only one bid in g∗ then it is simply a matter of reducing the payment of that bid by u∗ from the current maximum of the agent"s true valuation. However, if the agent has more than one bid then we face the problem of how to distribute the agent"s payments among these bids. There are many ways of distributing the payments and there does not appear to be a dominant strategy for performing this distribution.
We have chosen to distribute the payments in proportion to the agent"s true valuation for each set of items. pausebid assumes that the set of best bids B and the current best winning bidset W remains constant during its execution, and it returns the agent"s myopic utility-maximizing bidset (if there is one) using a branch and bound search.
However it repeats the whole search at every stage. We can minimize this problem by caching the result of previous searches.
The cachedpausebid algorithm (shown in Figure 3) is our second approach to solve the bidding problem in the PAUSE auction. It is based in a cache table called C-Table where we store some solutions to avoid doing a complete search every time. The problem is the same; the agent i has to find g∗ i . We note that g∗ i is a bidset that contains at least one bid of the agent i. Let S be a set of items for which the agent i has a valuation such that vi(S) ≥ B(S) > 0, let gS i be a bidset over S such that r(gS i ) ≥ r(W) + and gS i = arg max g⊆2B ui(g), (6) where each g is a set of bids that covers all items and ∀b∈g (b ∈ B) or (bagent = i and bvalue > B(bitems )) and (∃b∈gbitems = S and bagent = i). That is, gS i is i"s best bidset for all items which includes a bid from i for all S items.
In the PAUSE auction we cannot bid for sets of items with size greater than k. So, if we have for each set of items S for which vi(S) > 0 and size(S) ≤ k its corresponding gS i then g∗ i is the gS i that maximizes the agent"s utility. That is g∗ i = arg max {S | vi(S)>0∧size(S)≤k} ui(gS i ). (7) Each agent i implements a hash table C-Table such that C-Table[S] = gS for all S which vi(S) ≥ B(S) > 0. We can cachedpausebid(i, k, k-changed)
← ∅
← ui(W)
← C-Table[S] £ Global variable
))
← r(gS ) − min-payment £ Global variable
or (∃b∈B bitems ⊆ ¯S and bagent = i)
⊆ ¯S}
+{b ∈ B|bitems ⊆ ¯S and b /∈ B }
) > bvalue
← i
← vi(bitems )
← 0
> u∗ and r(gS ) ≥ r(W) +
b∈gS | bagent=i bvalue − B(bitems )
) − ui(gS )
| bagent = i
← B(bitems )
← B(bitems )+ my-payment ·bvalue −B(bitems ) surplus
← ui(gS )
← gS
≤ 0 and vi(S) < B(S)
Figure 3: The cachedpausebid algorithm that implements a caching based search to find a bidset that maximizes the utility for the agent i. k is the current stage of the auction (for k ≥ 2), and k-changed is a boolean that is true right after the auction moved to the next stage.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 697 cpbsearch(bids, g, n)
b∈g | bagent=i B(bitems ))
← g
← max-utility
= i
−(r(g) − ri(g)) − h(¯Ig)
xitems ∩ bitems = ∅}, g, n + 1) £ b is In
xitems ∩ bitems = ∅}, g, n + 1) £ b is In
Figure 4: The cpbsearch recursive procedure where bids is the set of available bids, g is the current partial solution and n is a value that indicates how deep in the list bids the algorithm has to search. then find g∗ by searching for the gS , stored in C-Table[S], that maximizes the agent"s utility, considering only the set of items S with size(S) ≤ k. The problem remains in maintaining the C-Table updated and avoiding to search every gS every time. cachedpausebid deals with this and other details.
Let B be the set of bids that contains the new best bids, that is, B contains the bids recently added to B and the bids that have changed price (always higher), bidder, or both and were already in B. Let ¯S = Items − S be the complement of S (the set of items not included in S). cachedpausebid takes three parameters: i the agent, k the current stage of the auction, and k-changed a boolean that is true right after the auction moved to the next stage. Initially C-Table has one row or entry for each set S for which vi(S) > 0. We start by eliminating the entries corresponding to each set S for which vi(S) < B(S) from C-Table (line 3). Then, in the case that k-changed is true, for each set S with size(S) = k, we add to B a bid for that set with value equal to vi(S) and bidder agent i (line 5); this a bid that the agent is now allowed to consider. We then search for g∗ amongst the gS stored in C-Table, for this we only need to consider the sets with size(S) ≤ k (line 8). But how do we know that the gS in C-Table[S] is still the best solution for S? There are only two cases when we are not sure about that and we need to do a search to update C-Table[S]. These cases are: i) When k-changed is true and size(S) ≤ k, since there was no gS stored in C-Table for this S. ii) When there exists at least one bid in B for the set of items ¯S or a subset of it submitted by an agent different than i, since it is probable that this new bid can produce a solution better than the one stored in C-Table[S].
We handle the two cases mentioned above in lines 13 to 26 of cachedpausebid. In both of these cases, since gS must contain a bid for S we need to find a bidset that cover the missing items, that is ¯S. Thus, our search space consists of all the bids on B for the set of items ¯S or for a subset of it. We build the list bids that contains only those bids.
However, we put the bids from B at the beginning of bids (line 14) since they are the ones that have changed. Then, we replace the bids in bids that have a price lower than the valuation the agent i has for those same items with a bid from agent i for those items and value equal to the agent"s valuation (lines 16-19).
The recursive procedure cpbsearch, called in line 25 of cachedpausebid and shown in Figure 4, is the one that finds the new gS . cpbsearch is a slightly modified version of our branch and bound search implemented in pbsearch.
The first modification is that it has a third parameter n that indicates how deep on the list bids we want to search, since it stops searching when n less or equal to zero and not only when the list bids is empty (line 1). Each time that there is a recursive call of cpbsearch n is decreased by one when a bid from bids is discarded or out (lines 12, 15, 21, and 24) and n remains the same otherwise (lines 20 and 23). We set the value of n before calling cpbsearch, to be the size of the list bids (cachedpausebid line 21) in case i), since we want cpbsearch to search over all bids; and we set n to be the number of bids from B included in bids (cachedpausebid line 23) in case ii), since we know that only the those first n bids in bids changed and can affect our current gS .
Another difference with pbsearch is that the bound in cpbsearch is uS which we set to be 0 (cachedpausebid line 22) when in case i) and r(gS )−min-payment (cachedpausebid line 12) when in case ii). We call cpbsearch with g already containing a bid for S. After cpbsearch is executed we are sure that we have the right gS , so we store it in the corresponding C-Table[S] (cachedpausebid line 26).
When we reach line 27 in cachedpausebid, we are sure that we have the right gS . However, agent i"s bids in gS are still set to his own valuation and not to the lowest possible price. If uS is greater than the current u∗ , lines 31 to 34 in cachedpausebid are responsible for setting the agent"s payments so that it can achieve its maximum utility uS .
As in pausebid, we have chosen to distribute the payments in proportion to the agent"s true valuation for each set of items. In the case that uS less than or equal to zero and the valuation that the agent i has for the set of items S is lower than the current value of the bid in B for the same set of items, we remove the corresponding C-Table[S] since we know that is not worthwhile to keep it in the cache table (cachedpausebid line 38).
The cachedpausebid function is called when k > 1 and returns the agent"s myopic utility-maximizing bidset, if there is one. It assumes that W and B remains constant during its execution.
generatevalues(i, items)
Figure 5: Algorithm for the generation of random value functions. expd(x) returns a random number taken from an exponential distribution with mean 1/x. 0 20 40 60 80 100
Number of Items CachedPauseBid
3 3 PauseBid + + + + + + + + + + Figure 6: Average percentage of convergence (y-axis), which is the percentage of times that our algorithms converge to the revenue-maximizing solution, as function of the number of items in the auction.
We have implemented both algorithms and performed a series of experiments in order to determine how their solution compares to the revenue-maximizing solution and how their times compare with each other. In order to do our tests we had to generate value functions for the agents1 .
The algorithm we used is shown in Figure 5. The type of valuations it generates correspond to domains where a set of agents must perform a set of tasks but there are cost savings for particular agents if they can bundle together certain subsets of tasks. For example, imagine a set of robots which must pick up and deliver items to different locations. Since each robot is at a different location and has different abilities, each one will have different preferences over how to bundle. Their costs for the item bundles are subadditive, which means that their preferences are superadditive. The first experiment we performed simply ensured the proper 1 Note that we could not use CATS [6] because it generates sets of bids for an indeterminate number of agents. It is as if you were told the set of bids placed in a combinatorial auction but not who placed each bid or even how many people placed bids, and then asked to determine the value function of every participant in the auction. 0 20 40 60 80 100
Number of Items CachedPauseBid 3 3
3 PauseBid + + + + + + + + + + Figure 7: Average percentage of revenue from our algorithms relative to maximum revenue (y-axis) as function of the number of items in the auction. functioning of our algorithms. We then compared the solutions found by both of them to the revenue-maximizing solution as found by CASS when given a set of bids that corresponds to the agents" true valuation. That is, for each agent i and each set of items S for which vi(S) > 0 we generated a bid. This set of bids was fed to CASS which implements a centralized winner determination algorithm to find the solution which maximizes revenue. Note, however, that the revenue from the PAUSE auction on all the auctions is always smaller than the revenue of the revenue-maximizing solution when the agents bid their true valuations. Since PAUSE uses English auctions the final prices (roughly) represent the second-highest valuation, plus , for that set of items.
We fixed the number of agents to be 5 and we experimented with different number of items, namely from 2 to
combination. When we compared the solutions of our algorithms to the revenue-maximizing solution, we realized that they do not always find the same distribution of items as the revenue-maximizing solution (as shown in Figure 6). The cases where our algorithms failed to arrive at the distribution of the revenue-maximizing solution are those where there was a large gap between the first and second valuation for a set (or sets) of items. If the revenue-maximizing solution contains the bid (or bids) using these higher valuation then it is impossible for the PAUSE auction to find this solution because that bid (those bids) is never placed. For example, if agent i has vi(1) = 1000 and the second highest valuation for (1) is only 10 then i only needs to place a bid of 11 in order to win that item. If the revenue-maximizing solution requires that 1 be sold for 1000 then that solution will never be found because that bid will never be placed.
We also found that average percentage of times that our algorithms converges to the revenue-maximizing solution decreases as the number of items increases. For 2 items is almost 100% but decreases a little bit less than 1 percent as the items increase, so that this average percentage of convergence is around 90% for 10 items. In a few instances our algorithms find different solutions this is due to the different The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 699 1 10 100 1000 10000
Number of Items CachedPauseBid 3 3 3 3 3 3 3 3 3 PauseBid + + + + + + + + + + Figure 8: Average number of expanded nodes (y-axis) as function of items in the auction. ordering of the bids in the bids list which makes them search in different order.
We know that the revenue generated by the PAUSE auction is generally lower than the revenue of the revenuemaximizing solution, but how much lower? To answer this question we calculated percentage representing the proportion of the revenue given by our algorithms relative to the revenue given by CASS. We found that the percentage of revenue of our algorithms increases in average 2.7% as the number of items increases, as shown in Figure 7. However, we found that cachedpausebid generates a higher revenue than pausebid (4.3% higher in average) except for auctions with 2 items where both have about the same percentage.
Again, this difference is produced by the order of the search.
In the case of 2 items both algorithms produce in average a revenue proportion of 67.4%, while in the other extreme (10 items), cachedpausebid produced in average a revenue proportion of 91.5% while pausebid produced in average a revenue proportion of 87.7%.
The scalability of our algorithms can be determined by counting the number of nodes expanded in the search tree.
For this we count the number of times that pbsearch gets invoked for each time that pausebid is called and the number of times that fastpausebidsearch gets invoked for each time that cachedpausebid, respectively for each of our algorithms. As expected since this is an NP-Hard problem, the number of expanded nodes does grow exponentially with the number of items (as shown in Figure 8). However, we found that cachedpausebid outperforms pausebid, since it expands in average less than half the number of nodes.
For example, the average number of nodes expanded when
2; and in the other extreme (10 items) cachedpausebid expands in average only 633 nodes while pausebid expands in average 1672 nodes, a difference of more than 1000 nodes.
Although the number of nodes expanded by our algorithms increases as function of the number of items, the actual number of nodes is a much smaller than the worst-case scenario of nn where n is the number of items. For example, for 10 items we expand slightly more than 103 nodes for the case of pausebid and less than that for the case of cachedpause0.1 1 10 100 1000
Number of Items CachedPauseBid 3 3 3 3 3 3 3 3 3 3 PauseBid + + + + + + + + + + Figure 9: Average time in seconds that takes to finish an auction (y-axis) as function of the number of items in the auction. bid which are much smaller numbers than 1010 . Notice also that our value generation algorithm (Figure 5) generates a number of bids that is exponential on the number of items, as might be expected in many situations. As such, these results do not support the conclusion that time grows exponentially with the number of items when the number of bids is independent of the number of items. We expect that both algorithms will grow exponentially as a function the number of bids, but stay roughly constant as the number of items grows.
We wanted to make sure that less expanded nodes does indeed correspond to faster execution, especially since our algorithms execute different operations. We thus ran the same experiment with all the agents in the same machine, an Intel Centrino 2.0 GHz laptop PC with 1 GB of RAM and a 7200 RMP 60 GB hard drive, and calculated the average time that takes to finish an auction for each algorithm. As shown in Figure 9, cachedpausebid is faster than pausebid, the difference in execution speed is even more clear as the number of items increases.
A lot of research has been done on various aspects of combinatorial auctions. We recommend [2] for a good review.
However, the study of distributed winner determination algorithms for combinatorial auctions is still relatively new.
One approach is given by the algorithms for distributing the winner determination problem in combinatorial auctions presented in [7], but these algorithms assume the computational entities are the items being sold and thus end up with a different type of distribution. The VSA algorithm [3] is another way of performing distributed winner determination in combinatorial auction but it assumes the bids themselves perform the computation. This algorithm also fails to converge to a solution for most cases. In [9] the authors present a distributed mechanism for calculating VCG payments in a mechanism design problem. Their mechanism roughly amounts to having each agent calculate the payments for two other agents and give these to a secure
central server which then checks to make sure results from all pairs agree, otherwise a re-calculation is ordered. This general idea, which they call the redundancy principle, could also be applied to our problem but it requires the existence of a secure center agent that everyone trusts. Another interesting approach is given in [8] where the bidding agents prioritize their bids, thus reducing the set of bids that the centralized winner determination algorithm must consider, making that problem easier. Finally, in the computation procuring clock auction [1] the agents are given an everincreasing percentage of the surplus achieved by their proposed solution over the current best. As such, it assumes the agents are impartial computational entities, not the set of possible buyers as assumed by the PAUSE auction.
We believe that distributed solutions to the winner determination problem should be studied as they offer a better fit for some applications as when, for example, agents do not want to reveal their valuations to the auctioneer or when we wish to distribute the computational load among the bidders. The PAUSE auction is one of a few approaches to decentralize the winner determination problem in combinatorial auctions. With this auction, we can even envision completely eliminating the auctioneer and, instead, have every agent performe the task of the auctioneer. However, while PAUSE establishes the rules the bidders must obey, it does not tell us how the bidders should calculate their bids.
We have presented two algorithms, pausebid and cachedpausebid, that bidder agents can use to engage in a PAUSE auction. Both algorithms implement a myopic utility maximizing strategy that is guaranteed to find the bidset that maximizes the agent"s utility given the set of outstanding best bids at any given time, without considering possible future bids. Both algorithms find, most of the time, the same distribution of items as the revenue-maximizing solution. The cases where our algorithms failed to arrive at that distribution are those where there was a large gap between the first and second valuation for a set (or sets) of items.
As it is an NP-Hard problem, the running time of our algorithms remains exponential but it is significantly better than a full search. pausebid performs a branch and bound search completely from scratch each time it is invoked. cachedpausebid caches partial solutions and performs a branch and bound search only on the few portions affected by the changes on the bids between consecutive times. cachedpausebid has a better performance since it explores fewer nodes (less than half) and it is faster. As expected the revenue generated by a PAUSE auction is lower than the revenue of a revenue-maximizing solution found by a centralized winner determination algorithm, however we found that cachedpausebid generates in average 4.7% higher revenue than pausebid. We also found that the revenue generated by our algorithms increases as function of the number of items in the auction.
Our algorithms have shown that it is feasible to implement the complex coordination constraints supported by combinatorial auctions without having to resort to a centralized winner determination algorithm. Moreover, because of the design of the PAUSE auction, the agents in the auction also have an incentive to perform the required computation. Our bidding algorithms can be used by any multiagent system that would use combinatorial auctions for coordination but would rather not implement a centralized auctioneer.
[1] P. J. Brewer. Decentralized computation procurement and computational robustness in a smart market.
Economic Theory, 13(1):41-92, January 1999. [2] P. Cramton, Y. Shoham, and R. Steinberg, editors.
Combinatorial Auctions. MIT Press, 2006. [3] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.
Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 548-553. Morgan Kaufmann Publishers Inc., 1999. [4] F. Kelly and R. Stenberg. A combinatorial auction with multiple winners for universal service.
Management Science, 46(4):586-596, 2000. [5] A. Land, S. Powell, and R. Steinberg. PAUSE: A computationally tractable combinatorial auction. In Cramton et al. [2], chapter 6, pages 139-157. [6] K. Leyton-Brown, M. Pearson, and Y. Shoham.
Towards a universal test suite for combinatorial auction algorithms. In Proceedings of the 2nd ACM conference on Electronic commerce, pages 66-76.
ACM Press, 2000. http://cats.stanford.edu. [7] M. V. Narumanchi and J. M. Vidal. Algorithms for distributed winner determination in combinatorial auctions. In LNAI volume of AMEC/TADA. Springer,
[8] S. Park and M. H. Rothkopf. Auctions with endogenously determined allowable combinations.
Technical report, Rutgets Center for Operations Research, January 2001. RRR 3-2001. [9] D. C. Parkes and J. Shneidman. Distributed implementations of vickrey-clarke-groves auctions. In Proceedings of the Third International Joint Conference on Autonomous Agents and MultiAgent Systems, pages 261-268. ACM, 2004. [10] M. H. Rothkopf, A. Pekec, and R. M. Harstad.
Computationally manageable combinational auctions.
Management Science, 44(8):1131-1147, 1998. [11] T. Sandholm. An algorithm for winner determination in combinatorial auctions. Artificial Intelligence, 135(1-2):1-54, February 2002. [12] T. Sandholm, S. Suri, A. Gilpin, and D. Levine.
CABOB: a fast optimal algorithm for winner determination in combinatorial auctions. Management Science, 51(3):374-391, 2005.

Sophisticated negotiation for task and resource allocation is crucial for the next generation of multi-agent systems (MAS) applications. Groups of agents need to efficiently negotiate over multiple related issues concurrently in a complex, distributed setting where there are deadlines by which the negotiations must be completed.
This is an important research area where there has been very little work done.
This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility; however, the performance of each individual agent is tightly related to other agent"s cooperation and the system"s overall performance. There is no single global goal in such systems, either because each agent represents a different organization/user, or because it is difficult/impossible to design one single global goal.
This issue arises due to multiple concurrent tasks, resource constrains and uncertainties, and thus no agent has sufficient knowledge or computational resources to determine what is best for the whole system [11]. An example of such a system would be a virtual organization [12] (i.e. a supply chain) dynamically formed in an electronic marketplace such as the one developed by the CONOISE project [5]. To accomplish tasks continuously arriving in the virtual organization, cooperation and sub-task relocation are needed and preferred. There is no single global goal since each agent may be involved in multiple virtual organizations. Meanwhile, the performance of each individual agent is tightly related to other agents" cooperation and the virtual organization"s overall performance. The negotiation in such systems is not a zero-sum game, a deal that increases both agents" utilities can be found through efficient negotiation. Additionally, there are multiple encounters among agents since new tasks are arriving all the time. In such negotiations, price may or may not be important, since it can be fixed resulting from a long-term contract. Other factors like quality and delivery time are important too. Reputation mechanisms in the system makes cheating not attractive from a long term viewpoint due to multiple encounters among agents. In such systems, agents are self-interested because they primarily focus on their own goals; but they are also semi-cooperative, meaning they are willing to be truthful and collaborate with other agents to find solutions that are beneficial to all participants, including itself; though it won"t voluntarily scarify its own utility in exchange of others" benefits.
Another major difference between this work and other work on negotiation is that negotiation, here, is not viewed as a stand-alone process. Rather it is one part of the agent"s activity which is tightly interleaved with the planning, scheduling and executing of the agent"s activities, which also may relate to other negotiations. Based on this recognition, this work on negotiation is concerned more about the meta-level decision-making process in negotiation rather than the basic protocols or languages. The goal of this research is to develop a set of macro-strategies that allow the agents to effectively manage multiple related negotiations, including, but not limited to the following issues: how much time should be spent on each negotiation, how much flexibility (see formal definition in Formula 3) should be allocated for each negotiation, and in what order should 50 978-81-904262-7-5 (RPS) c 2007 IFAAMAS the negotiations be performed. These macro-strategies are different from those micro-strategies that direct the individual negotiation thread, such as whether the agent should concede and how much the agent should concede, etc[3].
In this paper we extend a multi-linked negotiation model [10] from a single-agent perspective to a multi-agent perspective, so that a group of agents involved in chains of interrelated negotiations can find nearly-optimal macro negotiation strategies for pursuing their negotiations. The remainder of this paper is structured in the following manner. Section 2 describes the basic negotiation process and briefly reviews a single agent"s model of multi-linked negotiation. Section 3 introduces a complex supply-chain scenario.
Section 4 details how to solve those problems arising in the negotiation chain. Section 5 reports on the experimental work. Section 6 discusses related work and Section 7 presents conclusions and areas of future work.
NEGOTIATION In this work, the negotiation process between any pair of agents is based on an extended version of the contract net [6]: the initiator agent announces the proposal including multiple features; the responding agent evaluates it and responds with either a yes/no answer or a counter proposal with some features modified. This process can go back and forth until an agreement is reached or the agents decide to stop. If an agreement is reached and one agent cannot fulfill the commitment, it needs to pay the other party a decommitment penalty as specified in the commitment. A negotiation starts with a proposal, which announces that a task (t) needs to be performed includes the following attributes:
t cannot be started before time est.
to be finished before the deadline dl.
finished with a quality achievement no less than minq.
requested, the contractor agent will get reward r.
the task earlier than dl, it will get the extra early finish reward proportional to this rate.
cannot perform the task as it promised in the contract or if the contractee agent needs to cancel the contract after it has been confirmed, it also needs to pay a decommitment penalty (p∗r) to the other agent.
The above attributes are also called attribute-in-negotiation which are the features of the subject (issue) to be negotiated, and they are domain-dependent. Another type of attribute 1 is the attribute-ofnegotiation, which describes the negotiation process itself and is domain-independent, such as: 1 These attributes are similar to those used in project management; however, the multi-linked negotiation problem cannot be reduced to a project management problem or a scheduling problem. The multi-linked negotiation problem has two dimensions: the negotiations, and the subjects of negotiations. The negotiations are interrelated and the subjects are interrelated; the attributes of negotiations and the attributes of the subjects are interrelated as well. This two-dimensional complexity of interrelationships distinguishes it from the classic project management problem or scheduling problem, where all tasks to be scheduled are local tasks and no negotiation is needed.
negotiation v to complete, either reaching an agreed upon proposal (success) or no agreement (failure).
α(v) is an attribute that needs to be decided by the agent.
finished before this deadline (v). The negotiation is no longer valid after time (v), which is the same as a failure outcome of this negotiation.
successful. It depends on a set of attributes, including both attributes-in-negotiation (i.e. reward, flexibility, etc.) and attributes-of-negotiation (i.e. negotiation start time, negotiation deadline, etc.).
An agent involved in multiple related negotiation processes needs to reason on how to manage these negotiations in terms of ordering them and choosing the appropriate values for features. This is the multi-linked negotiation problem [10] : DEFINITION 2.1. A multi-linked negotiation problem is defined as an undirected graph (more specifically, a forest as a set of rooted trees): M = (V, E), where V = {v} is a finite set of negotiations, and E = {(u, v)} is a set of binary relations on V . (u, v) ∈ E denotes that negotiation u and negotiation v are directly-linked. The relationships among the negotiations are described by a forest, a set of rooted trees {Ti}. There is a relation operator associated with every non-leaf negotiation v (denoted as ρ(v)), which describes the relationship between negotiation v and its children. This relation operator has two possible values: AND and OR. The AND relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires all its children nodes have successful accomplishments. The OR relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires at least one child node have successful accomplishment, where the multiple children nodes represent alternatives to accomplish the same goal.
Multi-linked negotiation problem is a local optimization problem. To solve a multi-linked negotiation problem is to find a negotiation solution (φ, ϕ) with optimized expected utility EU(φ, ϕ), which is defined as: EU(φ, ϕ) = 2n X i=1 P(χi, ϕ) ∗ (R(χi, ϕ) − C(χi, φ, ϕ)) (1) A negotiation ordering φ defines a partial order of all negotiation issues. A feature assignment ϕ is a mapping function that assigns a value to each attribute that needs to be decided in the negotiation. A negotiation outcome χ for a set of negotiations {vj}, (j = 1, ..., n) specifies the result for each negotiation, either success or failure. There are a total of 2n different outcomes for n negotiations: {chii}, (i = 1, ..., 2n ). P(χi, ϕ) denotes the probability of the outcome χi given the feature assignment ϕ, which is calculated based on the success probability of each negotiation.
R(χi, ϕ) denotes the agent"s utility increase given the outcome χi and the feature assignment ϕ, and C(χi, φ, ϕ) is the sum of the decommitment penalties of those negotiations, which are successful, but need to be abandoned because the failure of other directly related negotiations; these directly related negotiations are performed concurrently with this negotiation or after this negotiation according to the negotiation ordering φ.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 51 Computer Producer CPU Other Tasks Distribution Center Memory Producer Transporter Deliver Hardware Order Memory (2) Other Tasks Other Tasks Order Chips PC Manufacturer Order Store Order Memory (1) Other Tasks Purchase Memory Customer Deliver Computer Hardware Computer Order Purchase Figure 1: A Complex Negotiation Chain Scenario A heuristic search algorithm [10] has been developed to solve the single agent"s multi-linked negotiation problem that produces nearly-optimal solutions. This algorithm is used as the core of the decision-making for each individual agent in the negotiation chain scenario. In the rest of the paper, we present our work on how to improve the local solution of a single agent in the global negotiation chain context.
Negotiation chain problem occurs in a multi-agent system, where each agent represents an individual, a company, or an organization, and there is no absolute authority in the system. Each agent has its own utility function for defining the implications of achieving its goals. The agent is designed to optimize its expected utility given its limited information, computational and communication resources. Dynamic tasks arrive to individual agents, most tasks requiring the coordination of multiple agents. Each agent has the scheduling and planning ability to manage its local activities, some of these activities are related to other agents" activities. Negotiation is used to coordinate the scheduling of these mutual related activities. The negotiation is tightly connected with the agent"s local scheduling/planning processes and is also related to other negotiations. An agent may be involved in multiple related negotiations with multiple other agents, and each of the other agents may be involved in related negotiations with others too.
Figure 1 describes a complex negotiation chain scenario. The Store, the PC manufacturer, the Memory Producer and the Distribution Center are all involved in multi-linked negotiation problems. Figure 2 shows a distributed model of part of the negotiation chain described in Figure 1. Each agent has a local optimization problem - the multi-linked negotiation problem (represented as an and-or tree), which can be solved using the model and procedures described in Section 2. However, the local optimal solution may not be optimal in the global context given the local model is neither complete or accurate. The dash line in Figure 2 represents the connection of these local optimization problem though the common negotiation subject.
Negotiation chain problem O is a group of tightly-coupled local optimization problems: O = {O1, O2, ....On}, Oi denotes the local optimization problem (multi-linked negotiation problem) of agent Ai Agent Ai"s local optimal solution Slo i maximizes the expected local utility based on an incomplete information and assumptions about other agents" local strategies - we defined such incomplete information and imperfect assumptions of agent i as Ii): Uexp i (Slo i , Ii) ≥ Uexp i (Sx i , Ii) for all x = lo.
However, the combination of these local optimal solutions {Slo i } : < Slo 1 , Slo 2 , ....Slo n > can be sub-optimal to a set of better local optimal solutions {Sblo i } : < Sblo 1 , Sblo 2 , ....Sblo n > if the global utility can be improved without any agent"s local utility being decreased by using {Sblo i }. In other words, {Slo i } is dominated by {Sblo i } ({Slo i } ≺ {Sblo i }) iff: Ui(< Slo 1 , Slo 2 , ....Slo n >) ≤ Ui(< Sblo 1 , Sblo 2 , ....Sblo n >) for i = 1, ...n and Pn i=1 Ui(< Slo 1 , Slo 2 , ....Slo n >) < Pn i=1 Ui(< Sblo 1 , Sblo 2 , ....Sblo n >) There are multiple sets of better local optimal solutions: {Sblo1 i }, {Sblo2 i }, ... {Sblom i }. Some of them may be dominated by others.
A set of better local optimal solutions {S blog i } that is not dominated by any others is called best local optimal. If a set of best local optimal solutions {S blog i } dominates all others, {S blog i } is called globally local optimal. However, sometimes the globally local optimal set does not exist, instead, there exist multiple sets of best local optimal solutions. Even if the globally local optimal solution does exist in theory, finding it may not be realistic given the agents are making decision concurrently, to construct the perfect local information and assumptions about other agents (Ii) in this dynamic environment is a very difficult and sometimes even impossible task.
The goal of this work is to improve each agent"s local model about other agents (Ii) through meta-level coordination. As Ii become more accurate, the agent"s local optimal solution to its local multi-linked negotiation problem become a better local optimal solution in the context of the global negotiation chain problem. We are not arguing that this statement is a universal valid statement that holds in all situations, but our experimental work shows that the sum of the agents" utilities in the system has been improved by 95% on average when meta-level coordination is used to improve each agent"s local model Ii. In this work, we focus on improving the agent"s local model through two directions. One direction is to build a better function to describe the relationship between the success probability of the negotiation and the flexibility allocated to the negotiation. The other direction is to find how to allocate time more efficiently for each negotiation in the negotiation chain context.
COORDINATION In order for an agent to get a better local model about other agents in the negotiation chain context, we introduce a pre-negotiation phase into the local negotiation process. During the pre-negotiation phase, agents communicate with other agents who have tasks contracting relationships with them, they transfer meta-level information before they decide on how and when to do the negotiations.
Each agent tells other agents what types of tasks it will ask them to perform, and the probability distributions of some parameters of those tasks, i.e. the earliest start times and the deadlines, etc. When these probability distributions are not available directly, agents can learn such information from their past experience. In our experiment described later, such distributed information is learned rather than being directly told by other agents. Specifically, each agent provides the following information to other related agents: • Whether additional negotiation is needed in order to make a decision on the contracting task; if so, how many more negotiations are needed. negCount represents the total number of additional negotiations needed for a task, including additional negotiations needed for its subtasks that happen among other agents. In a negotiation chain situation, this information is being propagated and updated through the chain until 52 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) E: Order Hardware F: Deliver Computer H: Get Memory I: Deliver Hardware I: Deliver Hardware F: Deliver Computer G: Get CPU E: Get Hardware and TransporterDistribution Center A: Purchase Computer B: Purchase Memory C: Order Computer D: Order Memory Store Agent PC Manufacturer and C: Order Computer Figure 2: Distributed Model of Negotiation Chains every agent has accurate information. Let subNeg(T) be a set of subtasks of task T that require additional negotiations, then we have: negCount(T) = |subNeg(T)| + X t∈subNeg(T ) (negCount(t)) (2) For example, in the scenario described in Figure 1, for the distribution center, task Order Hardware consists of three subtasks that need additional negotiations with other agents: Order Chips, Order Memory and Deliver Hardware.
However, no further negotiations are needed for other agents to make decision on these subtasks, hence the negCount for these subtasks are 0. The following information is sent to the PC manufacturer by the distribution center: negCount(Order Hardware) = 3 For the PC manufacturer task Order Computer contains two subtasks that requires additional negotiations: Deliver Computer and Order Hardware. When the PC manufacturer receives the message from the Distribution Center, it updates its local information: negCount(Order Computer) = 2+ negCount(Deliver Computer)(0)+ negCount(Order Hardware)(3) = 5 and sends the updated information to the Store Agent. • Whether there are other tasks competing with this task and what is the likelihood of conflict. Conflict means that given all constrains, the agent cannot accomplish all tasks on time, it needs to reject some tasks. The likelihood of conflict Pcij between a task of type i and another task of type j is calculated based on the statistical model of each task"s parameters, including earliest start time (est), deadline (dl), task duration (dur) and slack time (sl), using a formula [7]: Pcij = P(dli − estj ≤ duri + durj ∧ dlj − esti ≤ duri + durj) When there are more than two types of tasks, the likelihood of no conflict between task i and the rest of the tasks, is calculated as: PnoConflict(i) = Qn j=1,j=i(1 − Pcij) For example, the Memory Producer tells the Distribution Center about the task Order Memory. Its local decision does not involve additional negotiation with other agents (negCount = 0), however, there is another task from the Store Agent that competes with this task, thus the likelihood of no conflict is 0.5 (PnoConflict =
Center about the task Order Chips: its local decision does not involve additional negotiation with other agents, and there are no other tasks competing with this task (PnoConflict = 1.0) given the current environment setting. Based on the above information, the Distribution Center knows that task Order Memory needs more flexibility than task Order Chips in order to be successful in negotiation. Meanwhile, the Distribution Center would tell the PC Manufacturer that task Order Hardware involves further negotiation with other agents (negCount = 3), and that its local decision depends on other agents" decisions. This piece of information helps the PC Manufacturer allocate appropriate flexibility for task Order Hardware in negotiation. In this work, we introduce a short period and Produce_Computer Get_Software Install_Software Deliver_Computer Memory ProducerHardware Producer Transporter Consumer Agent Order_Computer Order_Memory Order_Hardware Order_Hardware process−time: 3 Distribution Center PC Manufacturer Order_Chips Deliver_HardwareGet_Parts process−time: 11 enables enables process−time: 4 process−time: 3 and and enables process−time: 4 and enables process−time: 3 process−time: 2 Figure 3: Task Structures of PC Manufacturer and Distribution Center for agents to learn the characteristics of those incoming tasks, including est, dl, dur and sl, which are used to calculate Pcij and PnoConflict for the meta-level coordination. During system performance, agents are continually monitoring these characteristics.
An updated message will be send to related agents when there is significant change of the meta-level information.
Next we will describe how the agent uses the meta-level information transferred during the pre-negotiation phase. This information will be used to improve the agent"s local model, more specifically, they are used in the agent"s local decision-making process by affecting the values of some features. Especially, we will be concerned with two features that have strong implications for the agent"s macro strategy for the multi-linked negotiations, and hence also affect the performance of a negotiation chain significantly. The first is the amount of flexibility specified in the negotiation parameter. The second feature we will explore is the time allocated for the negotiation process to complete. The time allocated for each negotiation affects the possible ordering of those negotiations, and it also affects the negotiation outcome. Details are discussed in the following sections.
Agents not only need to deal with complex negotiation problems, they also need to handle their own local scheduling and planning process that are interleaved with the negotiation process. Figure 3 shows the local task structures of the PC Manufacturer and the Distribution Center. Some of these tasks can be performed locally by the PC manufacturer, such as Get Software and Install Software, while other tasks (non-local tasks) such as Order Hardware and Deliver Computer need to be performed by other agents.The PC Manufacturer needs to negotiate with the Distribution Center and the Transporter about whether they can perform these tasks, and if so, when and how they will perform them.
When the PC Manufacturer negotiates with other agents about the non-local task, it needs to have the other agents" arrangement fit into its local schedule. Since the PC Manufacturer is dealing with multiple non-local tasks simultaneously, it also needs to ensure the commitments on these non-local tasks are consistent with each other. For example, the deadline of task Order Hardware cannot be later than the start time of task Deliver Computer. Figure 4 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 53 Order_Hardware Deliver_Computer [34, 40] process time: 4 process time: 3 [11, 28] [11, 28] process time: 11 Get_Software Install_Software [28, 34] process time: 2 Order_Computer starts at time 11 and finishes by 40 Figure 4: A Sample Local Schedule of the PC Manufacturer shows a sample local schedule of the PC Manufacturer. According to this schedule, as long as task Order Hardware is performed during time [11, 28] and task Deliver Computer is performed during time [34, 40], there exists a feasible schedule for all tasks and task Order Computer can be finished by time 40, which is the deadline promised to the Customer. These time ranges allocated for task Order Hardware and task Deliver Computer are called consistent ranges; the negotiations on these tasks can be performed independently within these ranges without worrying about conflict.
Notice that each task should be allocated with a time range that is large enough to accommodate the estimated task process time. The larger the range is, the more likely the negotiation will succeed, because it is easier for the other agent to find a local schedule for this task. Then the question is, how big should this time range be? We defined a quantitative measure called flexibility: Given a task t, suppose the allocated time range for t is [est, dl], est is the earliest start time and dl stands for the deadline, flexibility(t) = dl − est − process time(t) process time(t) (3) Flexibility is an important attribute because it directly affects the possible outcome of the negotiation. The success probability of a negotiation can be described as a function of the flexibility. In this work, we adopt the following formula for the success probability function based on the flexibility of the negotiation issue: ps(v) = pbs(v) ∗ (2/π) ∗ (arctan(f(v) + c))) (4) This function describes a phenomenon where initially the likelihood of a successful negotiation increases significantly as the flexibility grows, and then levels off afterward, which mirrors our experience from previous experiments. pbs is the basic success probability of this negotiation v when the flexibility f(v) is very large. c is a parameter used to adjust the relationship. Different function patterns can result from different parameter values, as shown in Figure 5. This function describes the agent"s assumption about how the other agent involved in this negotiation would response to this particular negotiation request, when it has flexibility f(v). This function is part of the agent"s local model about other agents. To improve the accuracy of this function and make it closer to the reality, the agent adjusts these two values according to the meta-level information transferred during pre-negotiation phase. The values of c depends on whether there is further negotiation involved and whether there are other tasks competing with this task for common resources. If so, more flexibility is needed for this issue and hence c should be assigned a smaller value. In our implementation, the following procedure is used to calculate c based on the meta-level information negCount and PnoConflict: if(PnoConflict > 0.99) // no other competing task c = Clarge − negCount else // competing task exists c = Csmall This procedure works as follows: when there is no other competing Figure 5: Different Success Probability Functions task, c depends on the number of additional negotiations needed.
The more additional negotiations that are needed, the smaller value c has, hence more flexibility will be assigned to this issue to ensure the negotiation success. If no more negotiation is needed, c is assigned to a large number Clarge, meaning that less flexibility is needed for this issue. When there are other competing tasks, c is assigned to a small number Csmall, meaning that more flexibility is needed for this issue. In our experimental work, we have Clarge as 5 and Csmall as 1. These values are selected according to our experience; however, a more practical approach is to have agents learn and dynamically adjust these values. This is also part of our future work. pbs is calculated based on PnoConflict, f(v) (the flexibility of v in previous negotiation), and c, using the reverse format of equation
pbs(v) = min(1.0, PnoConflict(v)∗(π/2)/(arctan(f(v)+c))) (5) For example, based on the scenario described above, the agents have the following values for c and pbs based on the meta-level information transferred: • PC Manufacturer, Order Hardware: pbs = 1.0, c = 2; • Distribution Center, Order Chips: pbs = 1.0, c = 5; • Store Agent, Order Memory: pbs = 0.79, c = 1; Figure 5 shows the different patterns of the success probability function given different parameter values. Based on such patterns, the Store Agent would allocate more flexibility to the task Order Memory to increase the likelihood of success in negotiation. In the agent"s further negotiation process, formula 4 with different parameter values is used in reasoning on how much flexibility should be allocated to a certain issue.
The pre-negotiation communication occurs before negotiation, but not before every negotiation session. Agents only need to communicate when the environment changes, for example, new types of tasks are generated, the characteristics of tasks changes, the negotiation partner changes, etc. If no major change happens, the agent can just use the current knowledge from previous communications. The communication and computation overhead of this prenegotiation mechanism is very small, given the simple information collection procedure and the short message to be transferred. We will discuss the effect of this mechanism in Section 5.
In the agent"s local model, there are two attributes that describe how soon the agent expects the other agent would reply to the negotiation v: negotiation duration δ(v) and negotiation deadline (v) 54 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Examples of negotiations (δ(v): negotiation duration, s.p.: success probability) index task-name δ(v) reward s.p. penalty 1 Order Hardware 4 6 0.99 3 2 Order Chips 4 1 0.99 0.5 3 Order Memory 4 1 0.80 0.5 4 Deliver Hardware 4 1 0.70 0.5 . These two important attributes that affect the negotiation solution. Part of the negotiation solution is a negotiation ordering φ which specifies in what order the multiple negotiations should be performed. In order to control the negotiation process, every negotiation should be finished before its negotiation deadline, and the negotiation duration is the time allocated for this negotiation. If a negotiation cannot be finished during the allocated time, the agent has to stop this negotiation and consider it as a failure. The decision about the negotiation order depends on the success probability, reward, and decommitment penalty of each negotiation. A good negotiation order should reduce the risk of decommitment and hence reduce the decommitment penalty. A search algorithm has been developed to find such negotiation order described in [10].
For example, Table 1 shows some of the negotiations for the Distribution Center and their related attributes. Given enough time (negotiation deadline is greater than 16), the best negotiation order is: 4 → 3 → 2 → 1. The most uncertain negotiation (4: Deliver Hardware) is performed first. The negotiation with highest penalty (1: Order hardware) is performed after all related negotiations (2, 3, and 4) have been completed so as to reduce the risk of decommitment. If the negotiation deadline is less than 12 and greater than 8, the following negotiation order is preferred: (4, 3, 2) → 1, which means negotiation 4, 3, 2 can be performed in parallel, and 1 needs to be performed after them. If the negotiation deadline is less than 8, then all negotiations have to be performed in parallel, because there is no time for sequencing negotiations.
In the original model for single agent [10], the negotiation deadline (v) is assumed to be given by the agent who initiates the contract. The negotiation duration δ(v) is an estimation of how long the negotiation takes based on experience. However, the situation is not that simple in a negotiation chain problem. Considering the following scenario. When the customer posts a contract for task Purchase Computer, it could require the Store Agent to reply by time 20. Time 20 can be considered as the negotiation deadline for Purchase Computer. When the Store Agent negotiates with the PC Manufacturer about Order Computer, what negotiation deadline should it specify? How long the negotiation on Order Computer takes depends on how the PC Manufacturer handles its local multiple negotiations: whether it replies to the Store Agent first or waits until all other related negotiations have been settled.
However, the ordering of negotiations depends on the negotiation deadline on Order Computer, which should be provided by the Store Agent. The negotiation deadline of Order Computer for the PC Manufacturer is actually decided based on the negotiation duration of Order Computer for the Store Agent. How much time the Store Agent would like to spend on the negotiation Order Computer is its duration, and also determines the negotiation deadline for the PC Manufacturer.
Now the question arises: how should an agent decide how much time it should spend on each negotiation, which actually affects the other agents" negotiation decisions. The original model does not handle this question since it assumes the negotiation duration δ(v) is known. Here we propose three different approaches to handle this issue.
all related negotiations, which means allocate all available time to all negotiations: δ(v) = total available time For example if the negotiation deadline for Purchase Computer is 20, the Store Agent will tell the PC Manufacturer to reply by 20 for Order Computer (ignoring the communication delay). This strategy allows every negotiation to have the largest possible duration, however it also eliminates the possibility of performing negotiations in sequence - all negotiations need to be performed in parallel because the total available time is the same as the duration of each negotiation.
negotiation according to the meta-level information transferred in the pre-negotiation phase. A more complicated negotiation, which involves further negotiations, should be allocated additional time. For example, the PC Manufacturer allocates a duration of 12 for the negotiation Order Hardware, and a duration of 4 for Deliver Computer. The reason is that the negotiation with the Distribution Center about Order Hardware is more complicated because it involves further negotiations between the Distribution Center and other agents. In our implementation, we use the following procedure to decide the negotiation duration δ(v): if(negCount(v) >= 3) // more additional negotiation needed δ(v) = (negCount(v)−1)∗basic neg cycle else if(negCount(v) > 0) // one or two additional negotiations needed δ(v) = 2 ∗ basic neg cycle else //no additional negotiation δ(v) = basic neg cycle + 1 basic neg cycle represents the minimum time needed for a negotiation cycle (proposal-think-reply), which is 3 in our system setting including communication delay. One additional time unit is allocated for the simplest negotiation because it allows the agent to perform a more complicated reasoning process in thinking. Again, the structure of this procedure is selected according to experience, and it can be learned and adjusted by agents dynamically.
time among the n related negotiations: δ(v) = total available time/n For example, if the current time is 0, and the negotiation deadline for Order Computer is 21, given two other related negotiations, Order Hardware and Deliver Computer, each negotiation is allocated with a duration of 7.
Intuitively we feel the strategy 1 may not be a good one, because performing all negotiations in parallel would increase the risk of decommitment and hence also decommitment penalties. However, it is not very clear how strategy 2 and 3 perform, and we will discuss some experimental results in Section 5.
To verify and evaluate the mechanisms presented for the negotiation chain problem, we implemented the scenario described in Figure 1 . New tasks were randomly generated with decommitment penalty rate p ∈ [0, 1], early finish reward rate e ∈ [0, 0.3], and deadline dl ∈ [10, 60] (this range allows different flexibilities available for those sub-contracted tasks), and arrived at the store agent periodically. We performed two sets of experiments to study The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 55 Table 2: Parameter Values Without/With Meta-level Information fixed-flex meta-info-flex negotiation pbs pbs c Order Computer 0.95 1.0 0 Order Memory (1) 0.95 0.79 1 Order Hardware 0.95 1.0 2 Deliver Computer 0.95 1.0 1 Deliver Hardware 0.95 1.0 5 Order Chips 0.95 1.0 1 Order Memory (2) 0.95 0.76 1 Figure 6: Different Flexibility Policies how the success probability functions and negotiation deadlines affect the negotiation outcome, the agents" utilities and the system"s overall utility. In this experiment, agents need to make decision on negotiation ordering and feature assignment for multiple attributes including: earliest start time, deadline, promised finish time, and those attributes-of-negotiation. To focus on the study of flexibility, in this experiment, the regular rewards for each type of tasks are fixed and not under negotiation. Here we only describe how agents handle the negotiation duration and negotiation deadlines because they are the attributes affected by the pre-negotiation phase. All other attributes involved in negotiation are handled according to how they affect the feasibility of local schedule (time-related attributes) and how they affect the negotiation success probability (time and cost related attributes) and how they affect the expect utility. A search algorithm [10] and a set of partial order scheduling algorithms are used to handle these attributes.
We tried two different flexibility policies.
success probability (ps(v) = pbs(v)), according to its local knowledge and estimation.
pbs(v) ∗ (2/π) ∗ (arctan(f(v) + c))) to model the success probability. It also adjusts those parameters (pbs(v) and c) according to the meta-level information obtained in prenegotiation phase as described in Section 4. Table 2 shows the values of those parameters for some negotiations.
Figure 6 shows the results of this experiment. This set of experiments includes 10 system runs, and each run is for 1000 simulating time units. In the first 200 time units, agents are learning about the task characteristics, which will be used to calculate the conflict probabilities Pcij. At time 200, agents perform meta-level information communication, and in the next 800 time units, agents use the meta-level information in their local reasoning process. The data was collected over the 800 time units after the pre-negotiation Figure 7: Different Negotiation Deadline Policies phase 2 . One Purchase Computer task is generated every 20 time units, and two Purchase Memory tasks are generated every 20 time units. The deadline for task Purchase Computer is randomly generated in the range of [30, 60], the deadline for task Purchase Memory is in the range of [10, 30]. The decommitment penalty rate is randomly generated in the range of [0, 1]. This setting creates multiple concurrent negotiation chain situations; there is one long chain: Customer - Store - PC Manufacturer - Distribution Center - Producers - Transporter and two short chains: Customer - Store - Memory Producer This demonstrates that this mechanism is capable of handling multiple concurrent negotiation chains.
All agents perform better in this example (gain more utility) when they are using the meta-level information to adjust their local control through the parameters in the success probability function (meta-info-flex policy). Especially for those agents in the middle of the negotiation chain, such as the PC Manufacturer and the Distribution Center, the flexibility policy makes a significant difference.
When the agent has a better understanding of the global negotiation scenario, it is able to allocate more flexibility for those tasks that involve complicated negotiations and resource contentions.
Therefore, the success probability increases and fewer tasks are rejected or canceled (90% of the tasks have been successfully negotiated over when using meta-level information, compared to 39% when no pre-negotiation is used), resulting in both the agent and the system achieving better performance.
In the second set of experiments studies, we compare three negotiation deadline policies described in Section 4.2 when using the meta-info flexibility policy described above. The initial result shows that the same-deadline policy and the meta-info-deadline policy perform almost the same when the amount of system workload level is moderate, tasks can be accommodated given sufficient flexibility. In this situation, with either of the policies, most negotiations are successful, and there are few decommitment occurrences, so the ordering of negotiations does not make too much difference. Therefore, in this second set of experiments, we increase the number of new tasks generated to raise the average workload in the system. One Purchase Computer task is generated every 15 time units, three Purchase Memory tasks are generated every 2 We only measure the utility collected after the learning phase because that the learning phase is relatively short comparing to the evaluation phase, also during the learning phase, no meta-level information is used, so some of the policies are invalid. 56 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 15 time units, and one task Deliver Gift (directly from the customer to the Transporter) is generated every 10 time units. This setup generates a higher level of system workload, which results in some tasks not being completed no matter what negotiation ordering is used. In this situation, we found the meta-info-deadline policy performs much better than same-deadline policy (See Figure 7). When an agent uses the same-deadline policy, all negotiations have to be performed in parallel. In the case that one negotiation fails, all related tasks have to be canceled, and the agent needs to pay multiple decommitment penalties. When the agent uses the meta-info-deadline policy, complicated negotiations are allocated more time and, correspondingly, simpler negotiations are allocated less time. This also has the effect of allowing some negotiations to be performed in sequence. The consequence of sequencing negotiation is that, if there is failure, an agent can simply cancel the other related negotiations that have not been started. In this way, the agent does not have to pay decommitment penalty for those canceled negotiations because no commitment has been established yet. The evenly-divided-deadline policy performs much worse than the meta-info-deadline policy. In the evenly-divideddeadline policy, the agent allocates negotiation time evenly among the related negotiations, hence the complicated negotiation does not get enough time to complete.
The above experiment results show that the meta-level information transferred among agents during the pre-negotiation phase is critical in building a more accurate model of the negotiation problem. The reasoning process based on this more accurate model produces an efficient negotiation solution, which improves the agent"s and the system"s overall utility significantly. This conclusion holds for those environments where the system is facing moderate heavy load and tasks have relatively tight time deadline (our experiment setup is to produce such environment); the efficient negotiation is especially important in such environments.
Fatima, Wooldridge and Jennings [1] studied the multiple issues in negotiation in terms of the agenda and negotiation procedure.
However, this work is limited since it only involves a single agent"s perspective without any understanding that the agent may be part of a negotiation chain. Mailler and Lesser [4] have presented an approach to a distributed resource allocation problem where the negotiation chain scenario occurs. It models the negotiation problem as a distributed constraint optimization problem (DCOP) and a cooperative mediation mechanism is used to centralize relevant portions of the DCOP. In our work, the negotiation involves more complicated issues such as reward, penalty and utility; also, we adopt a distribution approach where no centralized control is needed. A mediator-based partial centralized approach has been applied to the coordination and scheduling of complex task network [8], which is different from our work since the system is a complete cooperative system and individual utility of single agent is not concerned at all.
A combinatorial auction [2, 9] could be another approach to solving the negotiation chain problem. However, in a combinatorial auction, the agent does not reason about the ordering of negotiations. This would lead to a problem similar to those we discussed when the same-deadline policy is used.
In this paper, we have solved negotiation chain problems by extending our multi-linked negotiation model from the perspective of a single agent to multiple agents. Instead of solving the negotiation chain problem in a centralized approach, we adopt a distributed approach where each agent has an extended local model and decisionmaking process. We have introduced a pre-negotiation phase that allows agents to transfer meta-level information on related negotiation issues. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing the appropriate negotiation solution. The experimental data shows that these mechanisms improve the agent"s and the system"s overall performance significantly. In future extension of this work, we would like to develop mechanisms to verify how reliable the agents are. We also recognize that the current approach of applying the meta-level information is mainly heuristic, so we would like to develop a learning mechanism that enables the agent to learn how to use such information to adjust its local model from previous experience. To further verify this distributed approach, we would like to develop a centralized approach, so we can evaluate how good the solution from this distributed approach is compared to the optimal solution found by the centralized approach.
[1] S. S. Fatima, M. Wooldridge, and N. R. Jennings. Optimal negotiation strategies for agents with incomplete information. In Revised Papers from the 8th International Workshop on Intelligent Agents VIII, pages 377-392. Springer-Verlag, 2002. [2] L. Hunsberger and B. J. Grosz. A combinatorial auction for collaborative planning. In Proceedings of the Fourth International Conference on Multi-Agent Systems (ICMAS-2000), 2000. [3] N. R. Jennings, P. Faratin, T. J. Norman, P. O"Brien, B. Odgers, and J. L. Alty. Implementing a business process management system using adept: A real-world case study. Int. Journal of Applied Artificial Intelligence, 2000. [4] R. Mailler and V. Lesser. A Cooperative Mediation-Based Protocol for Dynamic, Distributed Resource Allocation. IEEE Transaction on Systems, Man, and Cybernetics, Part C, Special Issue on Game-theoretic Analysis and Stochastic Simulation of Negotiation Agents, 2004. [5] T. J. Norman, A. Preece, S. Chalmers, N. R. Jennings, M. Luck, V. D.
Dang, T. D. Nguyen, V. Deora, J. Shao, A. Gray, and N. Fiddian.
Agent-based formation of virtual organisations. Int. J. Knowledge Based Systems, 17(2-4):103-111, 2004. [6] T. Sandholm and V. Lesser. Issues in automated negotiation and electronic commerce: Extending the contract net framework. In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS95), pages 328-335, 1995. [7] J. Shen, X. Zhang, and V. Lesser. Degree of Local Cooperation and its Implication on Global Utility. Proceedings of Third International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS 2004), July 2004. [8] M. Sims, H. Mostafa, B. Horling, H. Zhang, V. Lesser, and D. Corkill. Lateral and Hierarchical Partial Centralization for Distributed Coordination and Scheduling of Complex Hierarchical Task Networks. AAAI 2006 Spring Symposium on Distributed Plan and Schedule Management, 2006. [9] W. Walsh, M. Wellman, and F. Ygge. Combinatorial auctions for supply chain formation. In Second ACM Conference on Electronic Commerce, 2000. [10] X. Zhang, V. Lesser, and S. Abdallah. Efficient management of multi-linked negotiation based on a formalized model. Autonomous Agents and MultiAgent Systems, 10(2):165-205, 2005. [11] X. Zhang, V. Lesser, and T. Wagner. Integrative negotiation among agents situated in organizations. IEEE Transactions on System, Man, and Cybernetics: Part C, Special Issue on Game-theoretic Analysis and Stochastic Simulation of Negotiation Agents, 36(1):19-30,
January 2006. [12] Q. Zheng and X. Zhang. Automatic formation and analysis of multi-agent virtual organization. Journal of the Brazilian Computer Society: Special Issue on Agents Organizations, 11(1):74-89, July

Many historical problems in the AI community can be transformed into Constraint Satisfaction Problems (CSP). With the advent of distributed AI, multi-agent systems became a popular way to model the complex interactions and coordination required to solve distributed problems. CSPs were originally extended to distributed agent environments in [9]. Early domains for distributed constraint satisfaction problems (DisCSP) included job shop scheduling [1] and resource allocation [2]. Many domains for agent systems, especially teamwork coordination, distributed scheduling, and sensor networks, involve overly constrained problems that are difficult or impossible to satisfy for every constraint.
Recent approaches to solving problems in these domains rely on optimization techniques that map constraints into multi-valued utility functions. Instead of finding an assignment that satisfies all constraints, these approaches find an assignment that produces a high level of global utility. This extension to the original DisCSP approach has become popular in multi-agent systems, and has been labeled the Distributed Constraint Optimization Problem (DCOP) [1].
Current algorithms that solve complete DCOPs use two main approaches: search and dynamic programming. Search based algorithms that originated from DisCSP typically use some form of backtracking [10] or bounds propagation, as in ADOPT [3].
Dynamic programming based algorithms include DPOP and its extensions [5, 6, 7]. To date, both categories of algorithms arrange agents into a traditional pseudotree to solve the problem.
It has been shown in [6] that any constraint graph can be mapped into a traditional pseudotree. However, it was also shown that finding the optimal pseudotree was NP-Hard. We began to investigate the performance of traditional pseudotrees generated by current edge-traversal heuristics. We found that these heuristics often produced little parallelism as the pseudotrees tended to have high depth and low branching factors. We suspected that there could be other ways to arrange the pseudotrees that would provide increased parallelism and smaller message sizes. After exploring these other arrangements we found that cross-edged pseudotrees provide shorter depths and higher branching factors than the traditional pseudotrees. Our hypothesis was that these crossedged pseudotrees would outperform traditional pseudotrees for some problem types.
In this paper we introduce an extension to the DPOP algorithm that handles an extended set of pseudotree arrangements which include cross-edged pseudotrees. We begin with a definition of 741 978-81-904262-7-5 (RPS) c 2007 IFAAMAS DCOP, traditional pseudotrees, and cross-edged pseudotrees. We then provide a summary of the original DPOP algorithm and introduce our DCPOP algorithm. We discuss the complexity of our algorithm as well as the impact of pseudotree generation heuristics. We then show that our Distributed Cross-edged Pseudotree Optimization Procedure (DCPOP) performs significantly better in practice than the original DPOP algorithm for some problem instances. We conclude with a selection of ideas for future work and extensions for DCPOP.
DCOP has been formalized in slightly different ways in recent literature, so we will adopt the definition as presented in [6]. A Distributed Constraint Optimization Problem with n nodes and m constraints consists of the tuple < X, D, U > where: • X = {x1,..,xn} is a set of variables, each one assigned to a unique agent • D = {d1,..,dn} is a set of finite domains for each variable • U = {u1,..,um} is a set of utility functions such that each function involves a subset of variables in X and defines a utility for each combination of values among these variables An optimal solution to a DCOP instance consists of an assignment of values in D to X such that the sum of utilities in U is maximal.
Problem domains that require minimum cost instead of maximum utility can map costs into negative utilities. The utility functions represent soft constraints but can also represent hard constraints by using arbitrarily large negative values. For this paper we only consider binary utility functions involving two variables. Higher order utility functions can be modeled with minor changes to the algorithm, but they also substantially increase the complexity.
Pseudotrees are a common structure used in search procedures to allow parallel processing of independent branches. As defined in [6], a pseudotree is an arrangement of a graph G into a rooted tree T such that vertices in G that share an edge are in the same branch in T. A back-edge is an edge between a node X and any node which lies on the path from X to the root (excluding X"s parent). Figure 1 shows a pseudotree with four nodes, three edges (A-B, B-C,
BD), and one back-edge (A-C). Also defined in [6] are four types of relationships between nodes exist in a pseudotree: • P(X) - the parent of a node X: the single node higher in the pseudotree that is connected to X directly through a tree edge • C(X) - the children of a node X: the set of nodes lower in the pseudotree that are connected to X directly through tree edges • PP(X) - the pseudo-parents of a node X: the set of nodes higher in the pseudotree that are connected to X directly through back-edges (In Figure 1, A = PP(C)) • PC(X) - the pseudo-children of a node X: the set of nodes lower in the pseudotree that are connected to X directly through back-edges (In Figure 1, C = PC(A)) Figure 1: A traditional pseudotree. Solid line edges represent parent-child relationships and the dashed line represents a pseudo-parent-pseudo-child relationship.
Figure 2: A cross-edged pseudotree. Solid line edges represent parent-child relationships, the dashed line represents a pseudoparent-pseudo-child relationship, and the dotted line represents a branch-parent-branch-child relationship. The bolded node, B, is the merge point for node E.
We define a cross-edge as an edge from node X to a node Y that is above X but not in the path from X to the root. A cross-edged pseudotree is a traditional pseudotree with the addition of cross-edges.
Figure 2 shows a cross-edged pseudotree with a cross-edge (D-E).
In a cross-edged pseudotree we designate certain edges as primary.
The set of primary edges defines a spanning tree of the nodes. The parent, child, pseudo-parent, and pseudo-child relationships from the traditional pseudotree are now defined in the context of this primary edge spanning tree. This definition also yields two additional types of relationships that may exist between nodes: • BP(X) - the branch-parents of a node X: the set of nodes higher in the pseudotree that are connected to X but are not in the primary path from X to the root (In Figure 2, D = BP(E)) • BC(X) - the branch-children of a node X: the set of nodes lower in the pseudotree that are connected to X but are not in any primary path from X to any leaf node (In Figure 2, E = BC(D))
Current algorithms usually have a pre-execution phase to generate a traditional pseudotree from a general DCOP instance. Our DCPOP algorithm generates a cross-edged pseudotree in the same fashion. First, the DCOP instance < X, D, U > translates directly into a graph with X as the set of vertices and an edge for each pair of variables represented in U. Next, various heuristics are used to arrange this graph into a pseudotree. One common heuristic is to perform a guided depth-first search (DFS) as the resulting traversal is a pseudotree, and a DFS can easily be performed in a distributed fashion. We define an edge-traversal based method as any method that produces a pseudotree in which all parent/child pairs share an edge in the original graph. This includes DFS, breadth-first search, and best-first search based traversals. Our heuristics that generate cross-edged pseudotrees use a distributed best-first search traversal.
The original DPOP algorithm operates in three main phases. The first phase generates a traditional pseudotree from the DCOP instance using a distributed algorithm. The second phase joins utility hypercubes from children and the local node and propagates them towards the root. The third phase chooses an assignment for each domain in a top down fashion beginning with the agent at the root node.
The complexity of DPOP depends on the size of the largest computation and utility message during phase two. It has been shown that this size directly corresponds to the induced width of the pseudotree generated in phase one [6]. DPOP uses polynomial time heuristics to generate the pseudotree since finding the minimum induced width pseudotree is NP-hard. Several distributed edgetraversal heuristics have been developed to find low width pseudotrees [8]. At the end of the first phase, each agent knows its parent, children, pseudo-parents, and pseudo-children.
Agents located at leaf nodes in the pseudotree begin the process by calculating a local utility hypercube. This hypercube at node X contains summed utilities for each combination of values in the domains for P(X) and PP(X). This hypercube has dimensional size equal to the number of pseudo-parents plus one. A message containing this hypercube is sent to P(X). Agents located at non-leaf nodes wait for all messages from children to arrive. Once the agent at node Y has all utility messages, it calculates its local utility hypercube which includes domains for P(Y), PP(Y), and Y. The local utility hypercube is then joined with all of the hypercubes from the child messages. At this point all utilities involving node Y are known, and the domain for Y may be safely eliminated from the joined hypercube. This elimination process chooses the best utility over the domain of Y for each combination of the remaining domains. A message containing this hypercube is now sent to P(Y).
The dimensional size of this hypercube depends on the number of overlapping domains in received messages and the local utility hypercube. This dynamic programming based propagation phase continues until the agent at the root node of the pseudotree has received all messages from its children.
Value propagation begins when the agent at the root node Z has received all messages from its children. Since Z has no parents or pseudo-parents, it simply combines the utility hypercubes received from its children. The combined hypercube contains only values for the domain for Z. At this point the agent at node Z simply chooses the assignment for its domain that has the best utility.
A value propagation message with this assignment is sent to each node in C(Z). Each other node then receives a value propagation message from its parent and chooses the assignment for its domain that has the best utility given the assignments received in the message. The node adds its domain assignment to the assignments it received and passes the set of assignments to its children. The algorithm is complete when all nodes have chosen an assignment for their domain.
Our extension to the original DPOP algorithm, shown in Algorithm 1, shares the same three phases. The first phase generates the cross-edged pseudotree for the DCOP instance. The second phase merges branches and propagates the utility hypercubes. The third phase chooses assignments for domains at branch merge points and in a top down fashion, beginning with the agent at the root node.
For the first phase we generate a pseudotree using several distributed heuristics and select the one with lowest overall complexity. The complexity of the computation and utility message size in DCPOP does not directly correspond to the induced width of the cross-edged pseudotree. Instead, we use a polynomial time method for calculating the maximum computation and utility message size for a given cross-edged pseudotree. A description of this method and the pseudotree selection process appears in Section 5. At the end of the first phase, each agent knows its parent, children, pseudo-parents, pseudo-children, branch-parents, and branch-children.
Propagation In the original DPOP algorithm a node X only had utility functions involving its parent and its pseudo-parents. In DCPOP, a node X is allowed to have a utility function involving a branch-parent.
The concept of a branch can be seen in Figure 2 with node E representing our node X. The two distinct paths from node E to node B are called branches of E. The single node where all branches of E meet is node B, which is called the merge point of E.
Agents with nodes that have branch-parents begin by sending a utility propagation message to each branch-parent. This message includes a two dimensional utility hypercube with domains for the node X and the branch-parent BP(X). It also includes a branch information structure which contains the origination node of the branch, X, the total number of branches originating from X, and the number of branches originating from X that are merged into a single representation by this branch information structure (this number starts at 1). Intuitively when the number of merged branches equals the total number of originating branches, the algorithm has reached the merge point for X. In Figure 2, node E sends a utility propagation message to its branch-parent, node D. This message has dimensions for the domains of E and D, and includes branch information with an origin of E, 2 total branches, and 1 merged branch.
As in the original DPOP utility propagation phase, an agent at leaf node X sends a utility propagation message to its parent. In DCPOP this message contains dimensions for the domains of P(X) and PP(X). If node X also has branch-parents, then the utility propagation message also contains a dimension for the domain of X, and will include a branch information structure. In Figure 2, node E sends a utility propagation message to its parent, node C. This message has dimensions for the domains of E and C, and includes branch information with an origin of E, 2 total branches, and 1 merged branch.
When a node Y receives utility propagation messages from all of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 743 its children and branch-children, it merges any branches with the same origination node X. The merged branch information structure accumulates the number of merged branches for X. If the cumulative total number of merged branches equals the total number of branches, then Y is the merge point for X. This means that the utility hypercubes present at Y contain all information about the valuations for utility functions involving node X. In addition to the typical elimination of the domain of Y from the utility hypercubes, we can now safely eliminate the domain of X from the utility hypercubes. To illustrate this process, we will examine what happens in the second phase for node B in Figure 2.
In the second phase Node B receives two utility propagation messages. The first comes from node C and includes dimensions for domains E, B, and A. It also has a branch information structure with origin of E, 2 total branches, and 1 merged branch. The second comes from node D and includes dimensions for domains E and B.
It also has a branch information structure with origin of E, 2 total branches, and 1 merged branch. Node B then merges the branch information structures from both messages because they have the same origination, node E. Since the number of merged branches originating from E is now 2 and the total branches originating from E is 2, node B now eliminates the dimensions for domain E. Node B also eliminates the dimension for its own domain, leaving only information about domain A. Node B then sends a utility propagation message to node A, containing only one dimension for the domain of A.
Although not possible in DPOP, this method of utility propagation and dimension elimination may produce hypercubes at node Y that do not share any domains. In DCPOP we do not join domain independent hypercubes, but instead may send multiple hypercubes in the utility propagation message sent to the parent of Y. This lazy approach to joins helps to reduce message sizes.
As in DPOP, value propagation begins when the agent at the root node Z has received all messages from its children. At this point the agent at node Z chooses the assignment for its domain that has the best utility. If Z is the merge point for the branches of some node X, Z will also choose the assignment for the domain of X.
Thus any node that is a merge point will choose assignments for a domain other than its own. These assignments are then passed down the primary edge hierarchy. If node X in the hierarchy has branch-parents, then the value assignment message from P(X) will contain an assignment for the domain of X. Every node in the hierarchy adds any assignments it has chosen to the ones it received and passes the set of assignments to its children. The algorithm is complete when all nodes have chosen or received an assignment for their domain.
We will prove the correctness of DCPOP by first noting that DCPOP fully extends DPOP and then examining the two cases for value assignment in DCPOP. Given a traditional pseudotree as input, the DCPOP algorithm execution is identical to DPOP. Using a traditional pseudotree arrangement no nodes have branch-parents or branch-children since all edges are either back-edges or tree edges. Thus the DCPOP algorithm using a traditional pseudotree sends only utility propagation messages that contain domains belonging to the parent or pseudo-parents of a node. Since no node has any branch-parents, no branches exist, and thus no node serves as a merge point for any other node. Thus all value propagation assignments are chosen at the node of the assignment domain.
For DCPOP execution with cross-edged pseudotrees, some nodes serve as merge points. We note that any node X that is not a merge point assigns its value exactly as in DPOP. The local utility hypercube at X contains domains for X, P(X), PP(X), and BC(X).
As in DPOP the value assignment message received at X includes the values assigned to P(X) and PP(X). Also, since X is not a merge point, all assignments to BC(X) must have been calculated at merge points higher in the tree and are in the value assignment message from P(X). Thus after eliminating domains for which assignments are known, only the domain of X is left. The agent at node X can now correctly choose the assignment with maximum utility for its own domain.
If node X is a merge point for some branch-child Y, we know that X must be a node along the path from Y to the root, and from P(Y) and all BP(Y) to the root. From the algorithm, we know that Y necessarily has all information from C(Y), PC(Y), and BC(Y) since it waits for their messages. Node X has information about all nodes below it in the tree, which would include Y, P(Y), BP(Y), and those PP(Y) that are below X in the tree. For any PP(Y) above X in the tree, X receives the assignment for the domain of PP(Y) in the value assignment message from P(X). Thus X has utility information about all of the utility functions of which Y is a part.
By eliminating domains included in the value assignment message, node X is left with a local utility hypercube with domains for X and Y. The agent at node X can now correctly choose the assignments with maximum utility for the domains of X and Y.
The first phase of DCPOP sends one message to each P(X),
PP(X), and BP(X). The second phase sends one value assignment message to each C(X). Thus, DCPOP produces a linear number of messages with respect to the number of edges (utility functions) in the cross-edged pseudotree and the original DCOP instance. The actual complexity of DCPOP depends on two additional measurements: message size and computation size.
Message size and computation size in DCPOP depend on the number of overlapping branches as well as the number of overlapping back-edges. It was shown in [6] that the number of overlapping back-edges is equal to the induced width of the pseudotree. In a poorly constructed cross-edged pseudotree, the number of overlapping branches at node X can be as large as the total number of descendants of X. Thus, the total message size in DCPOP in a poorly constructed instance can be space-exponential in the total number of nodes in the graph. However, in practice a well constructed cross-edged pseudotree can achieve much better results.
Later we address the issue of choosing well constructed crossedged pseudotrees from a set.
We introduce an additional measurement of the maximum sequential path cost through the algorithm. This measurement directly relates to the maximum amount of parallelism achievable by the algorithm. To take this measurement we first store the total computation size for each node during phase two and three. This computation size represents the number of individual accesses to a value in a hypercube at each node. For example, a join between two domains of size 4 costs 4 ∗ 4 = 16. Two directed acyclic graphs (DAG) can then be drawn; one with the utility propagation messages as edges and the phase two costs at nodes, and the other with value assignment messages and the phase three costs at nodes. The maximum sequential path cost is equal to the sum of the longest path on each DAG from the root to any leaf node.
In our assessment of complexity in DCPOP we focused on the worst case possibly produced by the algorithm. We acknowledge
Algorithm 1 DCPOP Algorithm 1: DCPOP(X; D; U) Each agent Xi executes: Phase 1: pseudotree creation 2: elect leader from all Xj ∈ X 3: elected leader initiates pseudotree creation 4: afterwards, Xi knows P(Xi), PP(Xi), BP(Xi), C(Xi), BC(Xi) and PC(Xi) Phase 2: UTIL message propagation 5: if |BP(Xi)| > 0 then 6: BRANCHXi ← |BP(Xi)| + 1 7: for all Xk ∈BP(Xi) do 8: UTILXi (Xk) ←Compute utils(Xi, Xk) 9: Send message(Xk,UTILXi (Xk),BRANCHXi ) 10: if |C(Xi)| = 0(i.e. Xi is a leaf node) then 11: UTILXi (P(Xi)) ← Compute utils(P(Xi),PP(Xi)) for all PP(Xi) 12: Send message(P(Xi),
UTILXi (P(Xi)),BRANCHXi ) 13: Send message(PP(Xi), empty UTIL, empty BRANCH) to all PP(Xi) 14: activate UTIL Message handler() Phase 3: VALUE message propagation 15: activate VALUE Message handler() END ALGORITHM UTIL Message handler(Xk,UTILXk (Xi),
BRANCHXk ) 16: store UTILXk (Xi),BRANCHXk (Xi) 17: if UTIL messages from all children and branch children arrived then 18: for all Bj ∈BRANCH(Xi) do 19: if Bj is merged then 20: join all hypercubes where Bj ∈UTIL(Xi) 21: eliminate Bj from the joined hypercube 22: if P(Xi) == null (that means Xi is the root) then 23: v ∗ i ← Choose optimal(null) 24: Send VALUE(Xi, v ∗ i) to all C(Xi) 25: else 26: UTILXi (P(Xi)) ← Compute utils(P(Xi),
PP(Xi)) 27: Send message(P(Xi),UTILXi (P(Xi)),
BRANCHXi (P(Xi))) VALUE Message handler(VALUEXi ,P(Xi)) 28: add all Xk ← v ∗ k ∈VALUEXi ,P(Xi) to agent view 29: Xi ← v ∗ i =Choose optimal(agent view) 30: Send VALUEXl , Xi to all Xl ∈C(Xi) that in real world problems the generation of the pseudotree has a significant impact on the actual performance. The problem of finding the best pseudotree for a given DCOP instance is NP-Hard.
Thus a heuristic is used for generation, and the performance of the algorithm depends on the pseudotree found by the heuristic. Some previous research focused on finding heuristics to generate good pseudotrees [8]. While we have developed some heuristics that generate good cross-edged pseudotrees for use with DCPOP, our focus has been to use multiple heuristics and then select the best pseudotree from the generated pseudotrees.
We consider only heuristics that run in polynomial time with respect to the number of nodes in the original DCOP instance. The actual DCPOP algorithm has worst case exponential complexity, but we can calculate the maximum message size, computation size, and sequential path cost for a given cross-edged pseudotree in linear space-time complexity. To do this, we simply run the algorithm without attempting to calculate any of the local utility hypercubes or optimal value assignments. Instead, messages include dimensional and branch information but no utility hypercubes.
After each heuristic completes its generation of a pseudotree, we execute the measurement procedure and propagate the measurement information up to the chosen root in that pseudotree. The root then broadcasts the total complexity for that heuristic to all nodes. After all heuristics have had a chance to complete, every node knows which heuristic produced the best pseudotree. Each node then proceeds to begin the DCPOP algorithm using its knowledge of the pseudotree generated by the best heuristic.
The heuristics used to generate traditional pseudotrees perform a distributed DFS traversal. The general distributed algorithm uses a token passing mechanism and a linear number of messages.
Improved DFS based heuristics use a special procedure to choose the root node, and also provide an ordering function over the neighbors of a node to determine the order of path recursion. The DFS based heuristics used in our experiments come from the work done in [4, 8].
heuristic The heuristics used to generate cross-edged pseudotrees perform a best-first traversal. A general distributed best-first algorithm for node expansion is presented in Algorithm 2. An evaluation function at each node provides the values that are used to determine the next best node to expand. Note that in this algorithm each node only exchanges its best value with its neighbors.
In our experiments we used several evaluation functions that took as arguments an ordered list of ancestors and a node, which contains a list of neighbors (with each neighbor"s placement depth in the tree if it was placed). From these we can calculate branchparents, branch-children, and unknown relationships for a potential node placement. The best overall function calculated the value as ancestors−(branchparents+branchchildren) with the number of unknown relationships being a tiebreak. After completion each node has knowledge of its parent and ancestors, so it can easily determine which connected nodes are pseudo-parents, branchparents, pseudo-children, and branch-children.
The complexity of the best-first traversal depends on the complexity of the evaluation function. Assuming a complexity of O(V ) for the evaluation function, which is the case for our best overall function, the best-first traversal is O(V · E) which is at worst O(n3 ). For each v ∈ V we perform a place operation, and find the next node to place using the getBestNeighbor operation. The place operation is at most O(V ) because of the sent messages.
Finding the next node uses recursion and traverses only already placed The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 745 Algorithm 2 Distributed Best-First Search Algorithm root ← electedleader next(root, ∅) place(node, parent) node.parent ← parent node.ancestors ← parent.ancestors ∪ parent send placement message (node, node.ancestors) to all neighbors of node next(current, previous) if current is not placed then place(current, previous) next(current, ∅) else best ← getBestNeighbor(current, previous) if best = ∅ then if previous = ∅ then terminate, all nodes are placed next(previous, ∅) else next(best, current) getBestNeighbor(current, previous) best ← ∅; score ← 0 for all n ∈ current.neighbors do if n! = previous then if n is placed then nscore ← getBestNeighbor(n, current) else nscore ← evaluate(current, n) if nscore > score then score ← nscore best ← n return best, score nodes, so it has O(V ) recursions. Each recursion performs a recursive getBestNeighbor operation that traverses all placed nodes and their neighbors. This operation is O(V · E), but results can be cached using only O(V ) space at each node. Thus we have O(V ·(V +V +V ·E)) = O(V 2 ·E). If we are smart about evaluating local changes when each node receives placement messages from its neighbors and cache the results the getBestNeighbor operation is only O(E). This increases the complexity of the place operation, but for all placements the total complexity is only O(V · E). Thus we have an overall complexity of O(V ·E+V ·(V +E)) = O(V ·E).
DPOP AND DCPOP We have already shown that given the same input, DCPOP performs the same as DPOP. We also have shown that we can accurately predict performance of a given pseudotree in linear spacetime complexity. If we use a constant number of heuristics to generate the set of pseudotrees, we can choose the best pseudotree in linear space-time complexity. We will now show that there exists a DCOP instance for which a cross-edged pseudotree outperforms all possible traditional pseudotrees (based on edge-traversal heuristics).
In Figure 3(a) we have a DCOP instance with six nodes. This is a bipartite graph with each partition fully connected to the other (a) (b) (c) Figure 3: (a) The DCOP instance (b) A traditional pseudotree arrangement for the DCOP instance (c) A cross-edged pseudotree arrangement for the DCOP instance partition. In Figure 3(b) we see a traditional pseudotree arrangement for this DCOP instance. It is easy to see that any edgetraversal based heuristic cannot expand two nodes from the same partition in succession. We also see that no node can have more than one child because any such arrangement would be an invalid pseudotree. Thus any traditional pseudotree arrangement for this DCOP instance must take the form of Figure 3(b). We can see that the back-edges F-B and F-A overlap node C. Node C also has a parent E, and a back-edge with D. Using the original DPOP algorithm (or DCPOP since they are identical in this case), we find that the computation at node C involves five domains: A, B, C, D, and E.
In contrast, the cross-edged pseudotree arrangement in Figure 3(c) requires only a maximum of four domains in any computation during DCPOP. Since node A is the merge point for branches from both B and C, we can see that each of the nodes D, E, and F have two overlapping branches. In addition each of these nodes has node A as its parent. Using the DCPOP algorithm we find that the computation at node D (or E or F) involves four domains: A, B, C, and D (or E or F).
Since no better traditional pseudotree arrangement can be created using an edge-traversal heuristic, we have shown that DCPOP can outperform DPOP even if we use the optimal pseudotree found through edge-traversal. We acknowledge that pseudotree arrangements that allow parent-child relationships without an actual constraint can solve the problem in Figure 3(a) with maximum computation size of four domains. However, current heuristics used with DPOP do not produce such pseudotrees, and such a heuristic would be difficult to distribute since each node would require information about nodes with which it has no constraint. Also, while we do not prove it here, cross-edged pseudotrees can produce smaller message sizes than such pseudotrees even if the computation size is similar. In practice, since finding the best pseudotree arrangement is NP-Hard, we find that heuristics that produce cross-edged pseudotrees often produce significantly smaller computation and message sizes.
Existing performance metrics for DCOP algorithms include the total number of messages, synchronous clock cycles, and message size. We have already shown that the total number of messages is linear with respect to the number of constraints in the DCOP instance. We also introduced the maximum sequential path cost (PC) as a measurement of the maximum amount of parallelism achievable by the algorithm. The maximum sequential path cost is equal to the sum of the computations performed on the longest path from the root to any leaf node. We also include as metrics the maximum computation size in number of dimensions (CD) and maximum message size in number of dimensions (MD). To analyze the relative complexity of a given DCOP instance, we find the minimum induced width (IW) of any traditional pseudotree produced by a heuristic for the original DPOP.
For our initial tests we randomly generated two sets of problems with 3000 cases in each. Each problem was generated by assigning a random number (picked from a range) of constraints to each variable. The generator then created binary constraints until each variable reached its maximum number of constraints. The first set uses 20 variables, and the best DPOP IW ranges from 1 to 16 with an average of 8.5. The second set uses 100 variables, and the best DPOP IW ranged from 2 to 68 with an average of 39.3. Since most of the problems in the second set were too complex to actually compute the solution, we took measurements of the metrics using the techniques described earlier in Section 5 without actually solving the problem. Results are shown for the first set in Table 1 and for the second set in Table 2.
For the two problem sets we split the cases into low density and high density categories. Low density cases consist of those problems that have a best DPOP IW less than or equal to half of the total number of nodes (e.g. IW ≤ 10 for the 20 node problems and IW ≤ 50 for the 100 node problems). High density problems consist of the remainder of the problem sets.
In both Table 1 and Table 2 we have listed performance metrics for the original DPOP algorithm, the DCPOP algorithm using only cross-edged pseudotrees (DCPOP-CE), and the DCPOP algorithm using traditional and cross-edged pseudotrees (DCPOP-All).
The pseudotrees used for DPOP were generated using 5 heuristics: DFS, DFS MCN, DFS CLIQUE MCN, DFS MCN DSTB, and DFS MCN BEC. These are all versions of the guided DFS traversal discussed in Section 5. The cross-edged pseudotrees used for DCPOP-CE were generated using 5 heuristics: MCN, LCN,
MCN A-B, LCN A-B, and LCSG A-B. These are all versions of the best-first traversal discussed in Section 5.
For both DPOP and DCPOP-CE we chose the best pseudotree produced by their respective 5 heuristics for each problem in the set. For DCPOP-All we chose the best pseudotree produced by all
metrics the value shown is the average number of dimensions. For the PC metric the value shown is the natural logarithm of the maximum sequential path cost (since the actual value grows exponentially with the complexity of the problem).
The final row in both tables is a measurement of improvement of DCPOP-All over DPOP. For the CD and MD metrics the value shown is a reduction in number of dimensions. For the PC metric the value shown is a percentage reduction in the maximum sequential path cost (% = DP OP −DCP OP DCP OP ∗ 100). Notice that DCPOPAll outperforms DPOP on all metrics. This logically follows from our earlier assertion that given the same input, DCPOP performs exactly the same as DPOP. Thus given the choice between the pseudotrees produced by all 10 heuristics, DCPOP-All will always outLow Density High Density Algorithm CD MD PC CD MD PC DPOP 7.81 6.81 3.78 13.34 12.34 5.34 DCPOP-CE 7.94 6.73 3.74 12.83 11.43 5.07 DCPOP-All 7.62 6.49 3.66 12.72 11.36 5.05 Improvement 0.18 0.32 13% 0.62 0.98 36% Table 1: 20 node problems Low Density High Density Algorithm CD MD PC CD MD PC DPOP 33.35 32.35 14.55 58.51 57.50 19.90 DCPOP-CE 33.49 29.17 15.22 57.11 50.03 20.01 DCPOP-All 32.35 29.57 14.10 56.33 51.17 18.84 Improvement 1.00 2.78 104% 2.18 6.33 256% Table 2: 100 node problems Figure 4: Computation Dimension Size Figure 5: Message Dimension Size The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 747 Figure 6: Path Cost DCPOP Improvement Ag Mtg Vars Const IW CD MD PC
Table 3: Meeting Scheduling Problems perform DPOP. Another trend we notice is that the improvement is greater for high density problems than low density problems. We show this trend in greater detail in Figures 4, 5, and 6. Notice how the improvement increases as the complexity of the problem increases.
In addition to our initial generic DCOP tests, we ran a series of tests on the Meeting Scheduling Problem (MSP) as described in [6]. The problem setup includes a number of people that are grouped into departments. Each person must attend a specified number of meetings. Meetings can be held within departments or among departments, and can be assigned to one of eight time slots.
The MSP maps to a DCOP instance where each variable represents the time slot that a specific person will attend a specific meeting.
All variables that belong to the same person have mutual exclusion constraints placed so that the person cannot attend more than one meeting during the same time slot. All variables that belong to the same meeting have equality constraints so that all of the participants choose the same time slot. Unary constraints are placed on each variable to account for a person"s valuation of each meeting and time slot.
For our tests we generated 100 sample problems for each combination of agents and meetings. Results are shown in Table 3. The values in the first five columns represent (in left to right order), the total number of agents, the total number of meetings, the total number of variables, the average total number of constraints, and the average minimum IW produced by a traditional pseudotree. The last three columns show the same metrics we used for the generic DCOP instances, except this time we only show the improvements of DCPOP-All over DPOP. Performance is better on average for all MSP instances, but again we see larger improvements for more complex problem instances.
We presented a complete, distributed algorithm that solves general DCOP instances using cross-edged pseudotree arrangements.
Our algorithm extends the DPOP algorithm by adding additional utility propagation messages, and introducing the concept of branch merging during the utility propagation phase. Our algorithm also allows value assignments to occur at higher level merge points for lower level nodes. We have shown that DCPOP fully extends DPOP by performing the same operations given the same input.
We have also shown through some examples and experimental data that DCPOP can achieve greater performance for some problem instances by extending the allowable input set to include cross-edged pseudotrees.
We placed particular emphasis on the role that edge-traversal heuristics play in the generation of pseudotrees. We have shown that the performance penalty is minimal to generate multiple heuristics, and that we can choose the best generated pseudotree in linear space-time complexity. Given the importance of a good pseudotree for performance, future work will include new heuristics to find better pseudotrees. Future work will also include adapting existing DPOP extensions [5, 7] that support different problem domains for use with DCPOP.
[1] J. Liu and K. P. Sycara. Exploiting problem structure for distributed constraint optimization. In V. Lesser, editor,
Proceedings of the First International Conference on Multi-Agent Systems, pages 246-254, San Francisco, CA,
[2] P. J. Modi, H. Jung, M. Tambe, W.-M. Shen, and S. Kulkarni.
A dynamic distributed constraint satisfaction approach to resource allocation. Lecture Notes in Computer Science, 2239:685-700, 2001. [3] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo. An asynchronous complete method for distributed constraint optimization. In AAMAS 03, 2003. [4] A. Petcu. Frodo: A framework for open/distributed constraint optimization. Technical Report No. 2006/001 2006/001, Swiss Federal Institute of Technology (EPFL),
Lausanne (Switzerland), 2006. http://liawww.epfl.ch/frodo/. [5] A. Petcu and B. Faltings. A-dpop: Approximations in distributed optimization. In poster in CP 2005, pages 802-806, Sitges, Spain, October 2005. [6] A. Petcu and B. Faltings. Dpop: A scalable method for multiagent constraint optimization. In IJCAI 05, pages 266-271, Edinburgh, Scotland, Aug 2005. [7] A. Petcu, B. Faltings, and D. Parkes. M-dpop: Faithful distributed implementation of efficient social choice problems. In AAMAS 06, pages 1397-1404, Hakodate,
Japan, May 2006. [8] G. Ushakov. Solving meeting scheduling problems using distributed pseudotree-optimization procedure. Master"s thesis, ´Ecole Polytechnique F´ed´erale de Lausanne, 2005. [9] M. Yokoo, E. H. Durfee, T. Ishida, and K. Kuwabara.
Distributed constraint satisfaction for formalizing distributed problem solving. In International Conference on Distributed Computing Systems, pages 614-621, 1992. [10] M. Yokoo, E. H. Durfee, T. Ishida, and K. Kuwabara. The distributed constraint satisfaction problem: Formalization and algorithms. Knowledge and Data Engineering, 10(5):673-685, 1998.

Planning and control constitutes a central research area in multiagent systems and artificial intelligence. In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments. In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility. While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].
We take an alternative view of planning in stochastic environments. We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics. The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria. We call this general planning framework Dynamics Based Control (DBC).
In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics. As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16]. Here, optimality is measured in terms of probability of deviation magnitudes.
In this paper, we present the structure of Dynamics Based Control. We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC. EMT is an efficient instantiation of DBC.
To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.
Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent"s position).
The paper is organized as follows. In Section 2 we motivate DBC using area-sweeping problems, and discuss related work. Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments. This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4. That section also discusses the limitations of EMT-based control relative to the general DBC framework. Experimental settings and results are then presented in Section 5. Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS
Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported. For example, security guards perform persistent sweeps of an area to detect any sign of intrusion. Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards" motion. It is thus advisable to make the guards" motion dynamics appear irregular and random.
Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs. The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels. Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).
The Game of Tag is another example of the applicability of the approach. It was introduced in the work by Pineau et al. [11]. There are two agents that can move about an area, which is divided into a grid. The grid may have blocked cells (holes) into which no agent can move. One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag). The quarry seeks to avoid the hunter agent, and is always aware of the hunter"s position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey. The hunter knows the quarry"s probabilistic law of motion, but does not know its current location. Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.
In [11], the hunter modeled the problem using a POMDP. A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time. Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.
In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics. In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics. Dynamics Based Control provides a natural approach to solving these problems.
The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment. For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe. The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation. In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.
As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time. To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.
Specific action selection then depends on system formalization.
One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1]. The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved. Notice that this manipulation is not direct, but via the environment. Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).
DBC levels can also have a back-flow of information (see Figure 1). For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior. Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.
UserEnv. Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm. For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior. In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update. In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9].
For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner. Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environment"s probabilistic transition function: T : S ×A → Π(S). That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations. This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).
That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}. Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.
There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1
|p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations. For instance,
KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p.
The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q. Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level.
It is important to note the complementary view that DBC and POMDPs take on the state space of the environment. POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.
This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.
DBC concentrates on the underlying principle of state sequencing, the system dynamics. DBC"s target dynamics specification can use the environment"s state space as a means to describe, discern, and preserve changes that occur within the system. As a result,
DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.
For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible. POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.
Alternatively, the state space could directly include the notion of speed. For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation. Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.
On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation. In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm.
Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework. Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15]. EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).
It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm. The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence. Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the system"s state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a
Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMT"s reaction. The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environment"s transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics. Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2).
At times, there may exist several behavioral preferences. For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity. On the other hand, no corner of the museum is to be left unchecked, which demands constant motion. Successful museum security would demand that the guards adhere to, and balance, both of these behaviors. For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them. A balancing mechanism can be applied to resolve this issue.
Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target. If these preference vectors are normalized, they can be combined into a single unified preference. This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .
Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves. This balancing method is also seamlessly integrated into the EMT-based control flow of operation.
EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure. It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection. This kind of combination, however, is common for on-line algorithms. Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.
There are two further, EMT-specific, limitations to EMT-based control that are evident at this point. Both already have partial solutions and are subjects of ongoing research.
The first limitation is the problem of negative preference. In the POMDP framework for example, this is captured simply, through The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure. For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution. Avoidance is thus unnatural in native EMT-based control.
The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received. Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.
Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference.
The Game of Tag was first introduced in [11]. It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems. An example domain is shown in Figure 2.
161514
2221 23
20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agent"s perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world. In the classical version of the game, co-location leads to a special observation, and the ‘Tag" action can be performed. We slightly modify this setting: the moment that both agents occupy the same cell, the game ends. As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West. These form a formal space of actions within a Markovian environment.
The state space of the formal Markovian environment is described by the cross-product of the agent and quarry"s positions. For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.
The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry. With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent. So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarry"s position is not. The only sensory information available to the agent is its own location.
We use EMT and directly specify the target dynamics. For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry. This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝
Tmobile(At+1 = si|Qt = so, At = sj) ∝
Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.
We ran several experiments to evaluate EMT performance in the Tag Game. Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability. In each setting, a set of 1000 runs was performed with a time limit of 100 steps. In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarry"s initial position was uniformly distributed over the entire domain cell space.
We also used two variations of the environment observability function. In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation. In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunter"s location. The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.
The results of these experiments are shown in Table 2.
Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.
Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach. In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments. For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours. That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11]. The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.
We also tested the behavior cell frequency entropy, empirical measures from trial data. As Figure 4 and Figure 5 show, empir794 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A
A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunter"s position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction. For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios. As the agent actively seeks the quarry, the entropy never reaches its maximum.
One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model. Near the maximum limit of trial length (100 steps), entropy suddenly dropped. Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior. Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells. It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics. This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them.
The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other. POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.
EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.
Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agent"s behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained. Experimental data shows that these targets need not be directly achievable via the agent"s actions. However, the ratio between EMT performance and achievability of target dynamics remains to be explored.
The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space. POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing. DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics. The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.
The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.
For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.
The complementary properties of POMDPs and EMT can be further exploited. There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself. DBC can be an effective partner in such a hybrid solution. For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation.
In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework. DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment. Optimality of DBC plans of action are measured The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795
1 Steps Entropy Dead−ends
1 Steps Entropy Arena
1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.
1 Steps Entropy Dead−ends
1 Steps Entropy Arena
1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunter"s position. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.
We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC. In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.
Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.
As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference. This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]). However, DBC in general has no such limitations, and readily enables the formulation of evasion games. In future work, we intend to proceed with the development of dynamics-based controllers for these problems.
The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israel"s Ministry of Science and Technology.
[1] R. C. Arkin. Behavior-Based Robotics. MIT Press, 1998. [2] J. A. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models. Technical Report TR-97-021,
Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J.
Wolverton. A survey of research in distributed, continual planning. AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis. Actor-Critic algorithms.
SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim. A rendezvous-evasion game on discrete locations with joint randomization. Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision problems. In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon. On the undecidability of probabilistic planning and related stochastic optimization problems. Artificial Intelligence Journal, 147(1-2):5-34,
July 2003. [9] R. M. Neal and G. E. Hinton. A view of the EM algorithm
that justifies incremental, sparse, and other variants. In M. I.
Jordan, editor, Learning in Graphical Models, pages 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for pomdps. In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman. Markov Decision Processes. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section. Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein. Extended Markov Tracking with an application to control. In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York,
July 2004. [14] Z. Rabinovich and J. S. Rosenschein. Multiagent coordination by Extended Markov Tracking. In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein. On the response of EMT-based control to interacting targets and models. In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan,
May 2006. [16] R. F. Stengel. Optimal Control and Estimation. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka,
R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce,

Agents are social, and agent interaction plays a vital role in multiagent systems. Consequently, design and implementation of agent interaction is an important research topic.
The standard approach for designing agent interactions is messagecentric: interactions are defined by interaction protocols that give the permissible sequences of messages, specified using notations such as finite state machines, Petri nets, or Agent UML.
It has been argued that this message-centric approach to interaction design is not a good match for intelligent agents. Intelligent agents should exhibit the ability to persist in achieving their goals in the face of failure (robustness) by trying different approaches (flexibility). On the other hand, when following an interaction protocol, an agent has limited flexibility and robustness: the ability to persistently try alternative means to achieving the interaction"s aim is limited to those options that the protocol"s designer provided, and in practice, message-centric design processes do not tend to lead to protocols that are flexible or robust.
Recognising these limitations of the traditional approach to designing agent interactions, a number of approaches have been proposed in recent years that move away from message-centric interaction protocols, and instead consider designing agent interactions using higher-level concepts such as social commitments [8, 10, 18] or interaction goals [2]. There has also been work on richer forms of interaction in specific settings, such as teams of cooperative agents [5, 11].
However, although there has been work on designing flexible and robust agent interactions, there has been virtually no work on providing programming language support for implementing such interactions. Current Agent Oriented Programming Languages (AOPLs) do not provide support for implementing flexible and robust agent interactions using higher-level concepts than messages.
Indeed, modern AOPLs [1], with virtually no exceptions, provide only simple message sending as the basis for implementing agent interaction.
This paper presents what, to the best of our knowledge, is the second AOPL to support high-level, flexible, and robust agent interaction implementation. The first such language, STAPLE, was proposed a few years ago [9], but is not described in detail, and is arguably impractical for use by non-specialists, due to its logical basis and heavy reliance on temporal and modal logic.
This paper presents a scheme for extending BDI-like AOPLs to support direct implementation of agent interactions that are designed using Yolum & Singh"s commitment machine (CM) framework [19]. In the remainder of this paper we briefly review commitment machines and present a simple abstraction of BDI AOPLs which lies in the common subset of languages such as Jason, 3APL, and CAN. We then present a scheme for translating commitment machines to this language, and indicate how the language needs to be extended to support this. We then extend our scheme to address a range of issues concerned with distribution, including turn tracking [7], and race conditions.
The aim of the commitment machine framework is to allow for the definition of interactions that are more flexible than traditional message-centric approaches. A Commitment Machine (CM) [19] specifies an interaction between entities (e.g. agents, services, processes) in terms of actions that change the interaction state. This interact state consists of fluents (predicates that change value over time), but also social commitments, both base-level and conditional.
A base-level social commitment is an undertaking by debtor A to creditor B to bring about condition p, denoted C(A, B, p). This is sometimes abbreviated to C(p), where it is not important to specify the identities of the entities in question. For example, a commitment by customer C to merchant M to make the fluent paid true would be written as C(C, M, paid).
A conditional social commitment is an undertaking by debtor A to creditor B that should condition q become true, A will then commit to bringing about condition p. This is denoted by CC(A, B, q, p), and, where the identity of the entities involved is unimportant (or obvious), is abbreviated to CC(q p) where the arrow is a reminder of the causal link between q becoming true and the creation of a commitment to make p true. For example, a commitment to make the fluent paid true once goods have been received would be written CC(goods paid).
The semantics of commitments (both base-level and conditional) is defined with rules that specify how commitments change over time. For example, the commitment C(p) (or CC(q p)) is discharged when p becomes true; and the commitment CC(q p) is replaced by C(p) when q becomes true. In this paper we use the more symmetric semantics proposed by [15] and subsequently reformalised by [14]. In brief, these semantics deal with a number of more complex cases, such as where commitments are created when conditions already hold: if p holds when CC(p q) is meant to be created, then C(q) is created instead of CC(p q).
An interaction is defined by specifying the entities involved, the possible contents of the interaction state (both fluents and commitments), and (most importantly) the actions that each entity can perform along with the preconditions and effects of each action, specified as add and delete lists.
A commitment machine (CM) defines a range of possible interactions that each start in some state1 , and perform actions until reaching a final state. A final state is one that has no base-level commitments. One way of visualising the interactions that are possible with a given commitment machine is to generate the finite state machine corresponding to the CM. For example, figure 1 gives the FSM2 corresponding to the NetBill [18] commitment machine: a simple CM where a customer (C) and merchant (M) attempt to trade using the following actions3 : 1 Unlike standard interaction protocols, or finite state machines, there is no designated initial state for the interaction. 2 The finite state machine is software-generated: the nodes and connections were computed by an implementation of the axioms (available from http://www.winikoff.net/CM) and were then laid out by graphviz (http://www.graphviz.org/). 3 We use the notation A(X) : P ⇒ E to indicate that action A is performed by entity X, has precondition P (with : P omitted if empty) and effect E. • sendRequest(C) ⇒ request • sendQuote(M) ⇒ offer where offer ≡ promiseGoods ∧ promiseReceipt and promiseGoods ≡ CC(M, C, accept, goods) and promiseReceipt ≡ CC(M, C, pay, receipt) • sendAccept(C) ⇒ accept where accept ≡ CC(C, M, goods, pay) • sendGoods(M) ⇒ promiseReceipt ∧ goods where promiseReceipt ≡ CC(M, C, pay, receipt) • sendEPO(C) : goods ⇒ pay • sendReceipt(M) : pay ⇒ receipt.
The commitment accept is the customer"s promise to pay once goods have been sent, promiseGoods is the merchant"s promise to send the goods once the customer accepts, and promiseReceipt is the merchant"s promise to send a receipt once payment has been made.
As seen in figure 1, commitment machines can support a range of interaction sequences.
Agent programming languages in the BDI tradition (e.g. dMARS,
JAM, PRS, UM-PRS, JACK, AgentSpeak(L), Jason, 3APL, CAN,
Jadex) define agent behaviour in terms of event-triggered plans, where each plan specifies what it is triggered by, under what situations it can be considered to be applicable (defined using a so-called context condition), and a plan body: a sequence of steps that can include posting events which in turn triggers further plans. Given a collection of plans and an event e that has been posted the agent first collects all plans types that are triggered by that event (the relevant plans), then evaluates the context conditions of these plans to obtain a set of applicable plan instances. One of these is chosen and is executed.
We now briefly define the formal syntax and semantics of a Simple Abstract (BDI) Agent Programming Language (SAAPL). This language is intended to be an abstraction that is in the common subset of such languages as Jason [1, Chapter 1], 3APL [1,
Chapter 2], and CAN [16]. Thus, it is intentionally incomplete in some areas, for instance it doesn"t commit to a particular mechanism for dealing with plan failure, since different mechanisms are used by different AOPLs.
An agent program (denoted by Π) consists of a collection of plan clauses of the form e : C ← P where e is an event, C is a context condition (a logical formula over the agent"s beliefs), and P is the plan body. The plan body is built up from the following constructs.
We have the empty step which always succeeds and does nothing, operations to add (+b) and delete (−b) beliefs, sending a message m to agent N (↑N m), and posting an event4 (e). These can be sequenced (P; P).
C ::= b | C ∧ C | C ∨ C | ¬C | ∃x.C P ::= | +b | −b | e | ↑N m | P; P Formal semantics for this language is given in figure 2. This semantics is based on the semantics for AgentSpeak given by [12], which in turn is based on the semantics for CAN [16]. The semantics is in the style of Plotkin"s Structural Operational Semantics, and assumes that operations exist that check whether a condition 4 We use ↓N m as short hand for the event corresponding to receiving message m from agent N.
Figure 1: Finite State Machine for NetBill (shaded = final states) follows from a belief set, that add a belief to a belief set, and that delete a belief from a belief set. In the case of beliefs being a set of ground atoms these operations are respectively consequence checking (B |= C), and set addition (B ∪ {b}) and deletion (B \ {b}).
More sophisticated belief management methods may be used, but are not considered here.
We define a basic configuration S = Q, N, B, P where Q is a (global) message queue (modelled as a sequence5 where messages are added at one end and removed from the other end), N is the name of the agent, B is the beliefs of the agent and P is the plan body being executed (i.e. the intention). We also define an agent configuration, where instead of a single plan body P there is a set of plan instances, Γ. Finally, a complete MAS is a pair Q, As of a global message queue Q and a set of agent configurations (without the queue, Q). The global message queue is a sequence of triplets of the form sender:recipient:message.
A transition S0 −→ S1 specifies that executing S0 a single step yields S1. We annotate the arrow with an indication of whether the configuration in question is basic, an agent configuration, or a MAS configuration. The transition relation is defined using rules of the form S −→ S or of the form S −→ Sr S −→ Sr ; the latter are conditional with the top (numerator) being the premise and the bottom (denominator) being the conclusion.
Note that there is non-determinism in SAAPL, e.g. the choice of plan to execute from a set of applicable plans. This is resolved by using selection functions: SO selects one of the applicable plan instances to handle a given event, SI selects which of the plan instances that can be executed should be executed next, and SA selects which agent should execute (a step) next.
INTERACTIONS In this section we present a mapping from a commitment machine to a collection of SAAPL programs (one for each role). We begin by considering the simple case of two interacting agents, and 5 The + operator is used to denote sequence concatenation. assume that the agents take turns to act. In section 4 we relax these assumptions.
Each action A(X) : P ⇒ E is mapped to a number of plans: there is a plan (for agent X) with context condition P that performs the action (i.e. applies the effects E to the agent"s beliefs) and sends a message to the other agent, and a plan (for the other agent) that updates its state when a message is received from X.
For example, given the action sendAccept(C) ⇒ accept we have the following plans, where each plan is preceded by M: or C: to indicate which agent that plan belongs to. Note that where the identify of the sender (respectively recipient) is obvious, i.e. the other agent, we abbreviate ↑N m to ↑m (resp. ↓N m to ↓m). Turn taking is captured through the event ı (short for interact): the agent that is active has an ı event that is being handled. Handling the event involves sending a message to the other agent, and then doing nothing until a response is received.
C: ı : true ← +accept; ↑sendAccept.
M: ↓sendAccept : true ← +accept; ı.
If the action has a non-trivial precondition then there are two plans in the recipient: one to perform the action (if possible), and another to report an error if the action"s precondition doesn"t hold (we return to this in section 4). For example, the action sendReceipt(M) : pay ⇒ receipt generates the following plans: M: ı : pay ← +receipt; ↑sendReceipt.
C: ↓sendReceipt : pay ← +receipt; ı.
C: ↓sendReceipt : ¬pay ← . . . report error . . . .
In addition to these plans, we also need plans to start and finish the interaction. An interaction can be completed whenever there are no base-level commitments, so both agents have the following plans: ı : ¬∃p.C(p) ← ↑done. ↓done : ¬∃p.C(p) ← . ↓done : ∃p.C(p) ← . . . report error . . . .
An interaction is started by setting up an agent"s initial beliefs, and then having it begin to interact. Exactly how to do this depends on the agent platform: e.g. the agent platform in question may offer a simple way to load beliefs from a file. A generic approach that is a little cumbersome, but is portable, is to send each of the agents involved in the interaction a sequence of init messages, each The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 875 Q, N, B, +b Basic −→ Q, N, B ∪ {b},
Q, N, B, −b Basic −→ Q, N, B \ {b},
Δ = {Piθ|(ti : ci ← Pi) ∈ Π ∧ tiθ = e ∧ B |= ciθ} Q, N, B, e Basic −→ Q, N, B, SO(Δ) Q, N, B, P1 Basic −→ Q , N, B , P Q, N, B, P1; P2 Basic −→ Q , N, B , P ; P2 Q, N, B, ; P Basic −→ Q, N, B, P Q, N, B, ↑NB m Basic −→ Q + N:NB:m, N, B,
Q = NA:N:m + Q Q, N, B, Γ Agent −→ Q , N, B, Γ ∪ {↓NA m} P = SI(Γ) Q, N, B, P Basic −→ Q , N, B , P Q, N, B, Γ Agent −→ Q , N, B , (Γ \ {P}) ∪ {P } P = SI(Γ) P = Q, N, B, Γ Agent −→ Q, N, B, (Γ \ {P}) N, B, Γ = SA(As) Q, N, B, Γ Agent −→ Q , N, B , Γ Q, As MAS −→ Q , (As ∪ { N, B , Γ }) \ { N, B, Γ } Figure 2: Operational Semantics for SAAPL containing a belief to be added; and then send one of the agents a start message which begins the interaction. Both agents thus have the following two plans: ↓init(B) : true ← +B. ↓start : true ← ı.
Figure 3 gives the SAAPL programs for both merchant and customer that implement the NetBill protocol. For conciseness the error reporting plans are omitted.
We now turn to refining the context conditions. There are three refinements that we consider. Firstly, we need to prevent performing actions that have no effect on the interaction state. Secondly, an agent may want to specify that certain actions that it is able to perform should not be performed unless additional conditions hold.
For example, the customer may not want to agree to the merchant"s offer unless the goods have a certain price or property. Thirdly, the context conditions of the plans that terminate the interaction need to be refined in order to avoid terminating the interaction prematurely.
For each plan of the form ı : P ← +E; ↑m we replace the context condition P with the enhanced condition P ∧ P ∧ ¬E where P is any additional conditions that the agent wishes to impose, and ¬E is the negation of the effects of the action. For example, the customer"s payment plan becomes (assuming no additional conditions, i.e. no P ): ı : goods ∧ ¬pay ← +pay; ↑sendEPO.
For each plan of the form ↓m : P ← +E; ı we could add ¬E to the precondition, but this is redundant, since it is already checked by the performer of the action, and if the action has no effect then Customer"s plans: ı : true ← +request; ↑sendRequest. ı : true ← +accept; ↑sendAccept. ı : goods ← +pay; ↑sendEPO. ↓sendQuote : true ← +promiseGoods; +promiseReceipt; ı. ↓sendGoods : true ← +promiseReceipt; +goods; ı. ↓sendReceipt : pay ← +receipt; ı.
Merchant"s plans: ı : true ← +promiseGoods; +promiseReceipt; ↑sendQuote. ı : true ← +promiseReceipt; +goods; ↑sendGoods. ı : pay ← +receipt; ↑sendReceipt. ↓sendRequest : true ← +request; ı. ↓sendAccept : true ← +accept; ı. ↓sendEPO : goods ← +pay; ı.
Shared plans (i.e. plans of both agents): ı : ¬∃p.C(p) ← ↑done. ↓done : ¬∃p.C(p) ← . ↓init(B) : true ← +B. ↓start : true ← ı.
Where accept ≡ CC(goods pay) promiseGoods ≡ CC(accept goods) promiseReceipt ≡ CC(pay receipt) offer ≡ promiseGoods ∧ promiseReceipt Figure 3: SAAPL Implementation of NetBill the sender won"t perform it and send the message (see also the discussion in section 4).
When specifying additional conditions (P ), some care needs to be taken to avoid situations where progress cannot be made because the only action(s) possible are prevented by additional conditions.
One way of indicating preference between actions (in many agent platforms) is to reorder the agent"s plans. This is clearly safe, since actions are not prevented, just considered in a different order.
The third refinement of context conditions concerns the plans that terminate the interaction. In the Commitment Machine framework any state that has no base-level commitment is final, in that the interaction may end there (or it may continue). However, only some of these final states are desirable final states. Which final states are considered to be desirable depends on the domain and the desired interaction outcome. In the NetBill example, the desirable final state is one where the goods have been sent and paid for, and a receipt issued (i.e. goods ∧ pay ∧ receipt). In order to prevent an agent from terminating the interaction too early we add this as a precondition to the termination plan: ı : goods ∧ pay ∧ receipt ∧ ¬∃p.C(p) ← ↑done.
Figure 4 shows the plans that are changed from figure 3.
In order to support the realisation of CMs, we need to change SAAPL in a number of ways. These changes, which are discussed below, can be applied to existing BDI languages to make them commitment machine supportive. We present the three changes, explain what they involve, and for each change explain how the change was implemented using the 3APL agent oriented programming language. The three changes are:
commitments;
Customer"s plans: ı : ¬request ← +request; ↑sendRequest. ı : ¬accept ← +accept; ↑sendAccept. ı : goods ∧ ¬pay ← +pay; ↑sendEPO.
Merchant"s plans: ı : ¬offer ← +promiseGoods; +promiseReceipt; ↑sendQuote. ı : ¬(promiseReceipt ∧ goods) ← +promiseReceipt; +goods; ↑sendGoods. ı : pay ∧ ¬receipt ← +receipt; ↑sendReceipt.
Where accept ≡ CC(goods pay) promiseGoods ≡ CC(accept goods) promiseReceipt ≡ CC(pay receipt) offer ≡ promiseGoods ∧ promiseReceipt Figure 4: SAAPL Implementation of NetBill with refined context conditions (changed plans only)
commitments; and
according to the rules of commitment dynamics.
Extending the notion of beliefs to encompass commitments in fact requires no change in agent platforms that are prolog-like and support terms as beliefs (e.g. Jason, 3APL, CAN). However, other agent platforms do require an extension. For example, JACK, which is an extension of Java, would require changes to support commitments that can be nested. In the case of 3APL no change is needed to support this.
Whenever a context condition contains commitments, determining whether the context condition is implied by the agent"s beliefs (B |= C) needs to take into account the notion of implied commitments [15]. In brief, a commitment can be considered to follow from a belief set B if the commitment is in the belief set (C ∈ B), but also under other conditions. For example, a commitment to pay C(pay) can be considered to be implied by a belief set containing pay because the commitment may have held and been discharged when pay was made true. Similar rules apply for conditional commitments. These rules, which were introduced in [15] were subsequently re-formalised in a simpler form by [14] resulting in the four inference rules in the bottom part of figure 5.
The change that needs to be made to SAAPL to support commitment machine implementations is to extend the definition of |= to include these four rules. For 3APL this was realised by having each agent include the following Prolog clauses: holds(X) :- clause(X,true). holds(c(P)) :- holds(P). holds(c(P)) :- clause(cc(Q,P),true), holds(Q). holds(cc(_,Q)) :- holds(Q). holds(cc(_,Q)) :- holds(c(Q)).
The first clause simply says that anything holds if it is in agent"s beliefs (clause(X,true) is true if X is a fact). The remaining four clauses correspond respectively to the inference rules C1,
C2, CC1 and CC2. To use these rules we then modify context conditions in our program so that instead of writing, for example, cc(m,c, pay, receipt) we write holds(cc(m,c, pay, receipt)).
B = norm(B ∪ {b}) Q, N, B, +b −→ Q, N, B , function norm(B) B ← B for each b ∈ B do if b = C(p) ∧ B |= p then B ← B \ {b} elseif b = CC(p q) then if B |= q then B ← B \ {b} elseif B |= p then B ← (B \ {b}) ∪ {C(q)} elseif B |= C(q) then B ← B \ {b} endif endif endfor return B end function B |= P B |= C(P) C1 CC(Q P) ∈ B B |= Q B |= P C2 B |= CC(P Q) B |= Q CC1 B |= C(Q) B |= CC(P Q) CC2 Figure 5: New Operational Semantics The final change is to update commitments when a belief is added. Formally, this is done by modifying the semantic rule for belief addition so that it applies an algorithm to update commitments. The modified rule and algorithm (which mirrors the definition of norm in [14]) can be found in the top part of figure 5.
For 3APL this final change was achieved by manually inserting update() after updating beliefs, and defining the following rules for update(): update() <- c(P) AND holds(P) | {Deletec(P) ; update()}, update() <- cc(P,Q) AND holds(Q) | {Deletecc(P,Q) ; update()}, update() <- cc(P,Q) AND holds(P) | {Deletecc(P,Q) ; Addc(Q) ; update()}, update() <- cc(P,Q) AND holds(c(Q)) | {Deletecc(P,Q) ; update()}, update() <- true | Skip where Deletec and Deletecc delete respectively a base-level and conditional commitment, and Addc adds a base-level commitment.
One aspect that doesn"t require a change is linking commitments and actions. This is because commitments don"t trigger actions directly: they may trigger actions indirectly, but in general their effect is to prevent completion of an interaction while there are outstanding (base level) commitments.
Figure 6 shows the message sequences from a number of runs of a 3APL implementation of the NetBill commitment machine6 . In order to illustrate the different possible interactions the code was modified so that each agent selected randomly from the actions that it could perform, and a number of runs were made with the customer as the initiator, and then with the merchant as the initiator. There are other possible sequences of messages, not shown, 6 Source code is available from http://www.winikoff.net/CM The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 877 Figure 6: Sample runs from 3APL implementation (alternating turns) including the obvious one: request, quote, accept, goods, payment, receipt, and then done.
One minor difference between the 3APL implementation and SAAPL concerns the semantics of messages. In the semantics of SAAPL (and of most AOPLs), receiving a message is treated as an event. However, in 3APL, receiving a message is modelled as the addition to the agent"s beliefs of a fact indicating that the message was received [6]. Thus in the 3APL implementation we have PG rules that are triggered by these beliefs, rather than by any event.
One issue with this approach is that the belief remains there, so we need to ensure that the belief in question is either deleted once handled, or that we modify preconditions of plans to avoid handling it more than once. In our implementation we delete these received beliefs when they are handled, to avoid duplicate handling of messages.
Generalising to more than two interaction participants requires revisiting how turn management is done, since it is no longer possible to assume alternating turns [7].
In fact, perhaps surprisingly, even in the two participant setting, an alternating turn setup is an unreasonable assumption! For example, consider the path (in figure 1) from state 1 to 15 (sendGoods) then to state 12 (sendAccept). The result, in an alternating turn setup, is a dead-end: there is only a single possible action in state 12, namely sendEPO, but this action is done by the customer, and it is the merchant"s turn to act! Figure 7 shows the FSM for NetBill with alternating initiative.
A solution to this problem that works in this example, but doesn"t generalise7 , is to weaken the alternating turn taking regime by allowing an agent to act twice in a row if its second action is driven by a commitment.
A general solution is to track whose turn it is to act. This can be done by working out which agents have actions that are able to be performed in the current state. If there is only a single active agent, then it is clearly that agent"s turn to act. However, if more than one agent is active then somehow the agents need to work out who should act next. Working this out by negotiation is not a particularly good solution for two reasons. Firstly, this negotiation has to be done at every step of the interaction where more than one agent is active (in the NetBill, this applies to seven out of sixteen states), so it is highly desirable to have a light-weight mechanism for doing this. Secondly, it is not clear how the negotiation can avoid an infinite regress situation (you go first, no, you go first, . ..) without imposing some arbitrary rule. It is also possible to resolve who should act by imposing an arbitrary rule, for example, that the customer always acts in preference to the merchant, or that each agent has a numerical priority (perhaps determined by the order in which they joined the interaction?) that determines who acts.
An alternative solution, which exploits the symmetrical properties of commitment machines, is to not try and manage turn taking. 7 Consider actions A1(C) ⇒ p, A2(C) ⇒ q, and A3(M) : p ∧ q ⇒ r.
Figure 7: NetBill with alternating initiative Instead of tracking and controlling whose turn it is, we simply allow the agents to act freely, and rely on the properties of the interaction space to ensure that things work out, a notion that we shall make precise, and prove, in the remainder of this section.
The issue with having multiple agents be active simultaneously is that instead of all agents agreeing on the current interaction state, agents can be in different states. This can be visualised as each agent having its own copy of the FSM that it navigates through where it is possible for agents to follow different paths through the FSM. The two specific issues that need to be addressed are:
it cannot perform an action corresponding to a received message?
We will show that, because actions commute under certain assumptions, agents cannot end up in different final states, and furthermore, that errors cannot occur (again, under certain assumptions).
By actions commute we mean that the state resulting from performing a sequence of actions A1 . . . An is the same, regardless of the order in which the actions are performed. This means that even if agents take different paths through the FSM, they still end up in the same resulting state, because once all messages have been processed, all agents will have performed the same set of actions. This addresses the issue of ending up in different final states. We return to the possibility of errors occurring shortly.
Definition 1 (Monotonicity) An action is monotonic if it does not delete8 any fluents or commitments. A Commitment Machine is 8 That is directly deletes, it is fine to discharge commitments by adding fluents/commitments.
monotonic if all of its actions are monotonic. (Adapted from [14,
Definition 6]) Theorem 1 If A1 and A2 are monotonic actions, then performing A1 followed by A2 has the same effect on the agent"s beliefs as performing A2 followed by A1. (Adapted from [14, Theorem 2]).
This assumes that both actions can be performed. However, it is possible for the performance of A1 to disable A2 from being done.
For example, if A1 has the effect +p, and A2 has precondition ¬p, then although both actions may be enabled in the initial state, they cannot be performed in either order. We can prevent this by ensuring that actions" preconditions do not contain negation (or implication), since a monotonic action cannot result in a precondition that is negation-free becoming false. Note that this restriction only applies to the original action precondition, P, not to any additional preconditions imposed by the agent (P ). This is because only P is used to determine whether another agent is able to perform the action.
Thus monotonic CMs with preconditions that do not contain negations have actions that commute. However, in fact, the restriction to monotonic CMs is unnecessarily strong: all that is needed is that whenever there is a choice of agent that can act, then the possible actions are monotonic. If there is only a single agent that can act, then no restriction is needed on the actions: they may or may not be monotonic.
Definition 2 (Locally Monotonic) A commitment machine is locally monotonic if for any state S either (a) only a single agent has actions that can be performed; or (b) all actions that can be performed in S are monotonic.
Theorem 2 In a locally monotonic CM, once all messages have been processed, all agents will be in the same state. Furthermore, no errors can occur.
Proof: Once all messages have been processed we have that all agents will have performed the same action set, perhaps in a different order. The essence of the proof is to argue that as long as agents haven"t yet converged to the same state, all actions must be monotonic, and hence that these actions commute, and cannot disable any other actions.
Consider the first point of divergence, where an agent performs action A and at the same time another agent (call it XB) performs action B. Clearly, this state has actions of more than one agent enabled, so, since the CM is locally monotonic, the relevant actions must be monotonic. Therefore, after doing A, the action B must still be enabled, and so the message to do B can be processed by updating the recipient agent"s beliefs with the effects of B.
Furthermore, because monotonic actions commute, the result of doing A before B is the same as doing B before A: S A −−−−−→ SA ? ? yB B ? ? y SB −−−−−→ A SAB However, what happens if the next action after A is not B, but C? Because B is enabled, and C is not done by agent XB (see below), we must have that C is also monotonic, and hence (a) the result of doing A and B and C is the same regardless of the order in which the three actions are done; and (b) C doesn"t disable B, so B can still be done after C.
S A −−−−−→ SA C −−−−−→ SAC ? ? yB B ? ? y B ? ? y SB −−−−−→ A SAB −−−−−→ C SABC The reason why C cannot be done by XB is that messages are processed in the order of their arrival9 . From the perspective of XB the action B was done before C, and therefore from any other agent"s perspective the message saying that B was done must be received (and processed) before a message saying that C is done.
This argument can be extended to show that once agents start taking different paths through the FSM all actions taken until the point where they converge on a single state must be monotonic, and hence it is always possible to converge (because actions aren"t disabled), so the interaction is error free; and the resulting state once convergence occurs is the same (because monotonic actions commute).
This theorem gives a strong theoretical guarantee that not doing turn management will not lead to disaster. This is analogous to proving that disabling all traffic lights would not lead to any accidents, and is only possible because the refined CM axioms are symmetrical.
Based on this theorem the generic transformation from CM to code should allow agents to act freely, which is achieved by simply changing ı : P ∧ P ∧ ¬E ← +E; ↑A to ı : P ∧ P ∧ ¬E ← +E; ↑A; ı For example, instead of ı : ¬request ← +request; ↑sendRequest we have ı : ¬request ← +request; ↑sendRequest; ı.
One consequence of the theorem is that it is not necessary to ensure that agents process messages before continuing to interact. However, in order to avoid unnecessary parallelism, which can make debugging harder, it may still be desirable to process messages before performing actions.
Figure 8 shows a number of runs from the 3APL implementation that has been modified to allow free, non-alternating, interaction.
We have presented a scheme for mapping commitment machines to BDI platforms (using SAAPL as an exemplar), identified three changes that needed to be made to SAAPL to support CM-based interaction, and shown that turn management can be avoided in CMbased interaction, provided the CM is locally monotonic. The three changes to SAAPL, and the translation scheme from commitment machine to BDI plans are both applicable to any BDI language.
As we have mentioned in section 1, there has been some work on designing flexible and robust agent interaction, but virtually no work on implementing flexible and robust interactions.
We have already discussed STAPLE [9, 10]. Another piece of work that is relevant is the work by Cheong and Winikoff on their Hermes methodology [2]. Although the main focus of their work is a pragmatic design methodology, they also provide guidelines for implementing Hermes designs using BDI platforms (specifically Jadex) [3]. However, since Hermes does not yield a design that is formal, it is only possible to generate skeleton code that then needs to be completed. Also, they do not address the turn taking issue: how to decide which agent acts when more than one agent is able to act. 9 We also assume that the communication medium does not deliver messages out of order, which is the case for (e.g.) TCP.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 879 Figure 8: Sample runs from 3APL implementation (non-alternating turns) The work of Kremer and Flores (e.g. [8]) also uses commitments, and deals with implementation. However, they provide infrastructure support (CASA) rather than a programming language, and do not appear to provide assistance to a programmer seeking to implement agents.
Although we have implemented the NetBill interaction using 3APL, the changes to the semantics were done by modifying our NetBill 3APL program, rather than by modifying the 3APL implementation itself. Clearly, it would be desirable to modify the semantics of 3APL (or of another language) directly, by changing the implementation. Also, although we have not done so, it should be clear that the translation from a CM to its implementation could easily be automated.
Another area for further work is to look at how the assumptions required to ensure that actions commute can be relaxed.
Finally, there is a need to perform empirical evaluation. There has already been some work on comparing Hermes with a conventional message-centric approach to designing interaction, and this has shown that using Hermes results in designs that are significantly more flexible and robust [4]. It would be interesting to compare commitment machines with Hermes, but, since commitment machines are a framework, not a design methodology, we need to compare Hermes with a methodology for designing interactions that results in commitment machines [13, 17].
[1] R. H. Bordini, M. Dastani, J. Dix, and A. E. F. Seghrouchni, editors. Multi-Agent Programming: Languages, Platforms and Applications. Springer, 2005. [2] C. Cheong and M. Winikoff. Hermes: Designing goal-oriented agent interactions. In Proceedings of the 6th International Workshop on Agent-Oriented Software Engineering (AOSE-2005), July 2005. [3] C. Cheong and M. Winikoff. Hermes: Implementing goal-oriented agent interactions. In Proceedings of the Third international Workshop on Programming Multi-Agent Systems (ProMAS), July 2005. [4] C. Cheong and M. Winikoff. Hermes versus prometheus: A comparative evaluation of two agent interaction design approaches. Submitted for publication, 2007. [5] P. R. Cohen and H. J. Levesque. Teamwork. Nous, 25(4):487-512, 1991. [6] M. Dastani, J. van der Ham, and F. Dignum. Communication for goal directed agents. In Proceedings of the Agent Communication Languages and Conversation Policies Workshop, 2002. [7] F. P. Dignum and G. A. Vreeswijk. Towards a testbed for multi-party dialogues. In Advances in Agent Communication, pages 212-230. Springer, LNCS 2922, 2004. [8] R. Kremer and R. Flores. Using a performative subsumption lattice to support commitment-based conversations. In F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. P. Singh, and M. Wooldridge, editors, Autonomous Agents and Multi-Agent Systems (AAMAS), pages 114-121. ACM Press, 2005. [9] S. Kumar and P. R. Cohen. STAPLE: An agent programming language based on the joint intention theory. In Proceedings of the Third International Joint Conference on Autonomous Agents & Multi-Agent Systems (AAMAS 2004), pages 1390-1391. ACM Press, July 2004. [10] S. Kumar, M. J. Huber, and P. R. Cohen. Representing and executing protocols as joint actions. In Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems, pages 543 - 550, Bologna, Italy,
[11] M. Tambe and W. Zhang. Towards flexible teamwork in persistent teams: Extended report. Journal of Autonomous Agents and Multi-agent Systems, 2000. Special issue on Best of ICMAS 98. [12] M. Winikoff. An AgentSpeak meta-interpreter and its applications. In Third International Workshop on Programming Multi-Agent Systems (ProMAS), pages 123-138. Springer, LNCS 3862 (post-proceedings, 2006),
[13] M. Winikoff. Designing commitment-based agent interactions. In Proceedings of the 2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT-06), 2006. [14] M. Winikoff. Implementing flexible and robust agent interactions using distributed commitment machines.
Multiagent and Grid Systems, 2(4), 2006. [15] M. Winikoff, W. Liu, and J. Harland. Enhancing commitment machines. In J. Leite, A. Omicini, P. Torroni, and P. Yolum, editors, Declarative Agent Languages and Technologies II, number 3476 in Lecture Notes in Artificial Intelligence (LNAI), pages 198-220. Springer, 2004. [16] M. Winikoff, L. Padgham, J. Harland, and J. Thangarajah.
Declarative & procedural goals in intelligent agent systems.
In Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), Toulouse, France, 2002. [17] P. Yolum. Towards design tools for protocol development. In F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. P. Singh, and M. Wooldridge, editors, Autonomous Agents and Multi-Agent Systems (AAMAS), pages 99-105. ACM Press, 2005. [18] P. Yolum and M. P. Singh. Flexible protocol specification and execution: Applying event calculus planning using commitments. In Proceedings of the 1st Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 527-534, 2002. [19] P. Yolum and M. P. Singh. Reasoning about commitments in the event calculus: An approach for specifying and executing protocols. Annals of Mathematics and Artificial Intelligence (AMAI), 2004.

The logical foundations of multi-agent systems have received much attention in recent years. Logic has been used to represent and reason about, e.g., knowledge [7], time [6], cooperation and strategic ability [3]. Lately, an increasing amount of research has focused on higher level representation languages for models of such logics, motivated mainly by the need for compact representations, and for representations that correspond more closely to the actual systems which are modeled. Multi-agent systems are open systems, in the sense that agents interact with an environment only partially known in advance. Thus, we need representations of models of multi-agent systems which are modular, in the sense that a component, such as an agent, can be replaced, removed, or added, without major changes to the representation of the whole model. However, as we argue in this paper, few existing representation languages are both modular, compact and computationally grounded on the one hand, and allow for representing properties of both knowledge and strategic ability, on the other.
In this paper we present a new class of representations for models of open multi-agent systems, which are modular, compact and come with an implicit methodology for modeling and designing actual systems.
The structure of the paper is as follows. First, in Section 2, we present the background of our work - that is, logics that combine time, knowledge, and strategies. More precisely: modal logics that combine branching time, knowledge, and strategies under incomplete information. We start with computation tree logic CTL, then we add knowledge (CTLK), and then we discuss two variants of alternating-time temporal logic (ATL): one for the perfect, and one for the imperfect information case. The semantics of logics like the ones presented in Section 2 are usually defined over explicit models (Kripke structures) that enumerate all possible (global) states of the system. However, enumerating these states is one of the things one mostly wants to avoid, because there are too many of them even for simple systems. Thus, we usually need representations that are more compact. Another reason for using a more specialized class of models is that general Kripke structures do not always give enough help in terms of methodology, both at the stage of design, nor at implementation. This calls for a semantics which is more grounded, in the sense that the correspondence between elements of the model, and the entities that are modeled, is more immediate. In Section 3, we present an overview of representations that have been used for modeling and model checking systems in which time, action (and possibly knowledge) are important; we mention especially representations used for theoretical analysis. We point out that the compact and/or grounded representations of temporal models do not play their role in a satisfactory way when agents" strategies are considered. Finally, in Section 4, we present our framework of modular interpreted systems (MIS), and show where it fits in the picture. We conclude with a somewhat surprising hypothesis, that model checking ability under imperfect information for MIS can be computationally cheaper than model checking perfect information.
Until now, almost all complexity results were distinctly in favor of perfect information strategies (and the others were indifferent).
STRATEGIC ABILITY First, we present the logics CTL, CTLK, ATL and ATLir that are the starting point of our study.
Computation tree logic CTL [6] includes operators for temporal properties of systems: i.e., path quantifier E (there is a path), together with temporal operators: f(in the next state), 2 (always from now on) and U (until).1 Every occurrence of a temporal operator is immediately preceded by exactly one path quantifier (this variant of the language is sometimes called vanilla CTL).
Let Π be a set of atomic propositions with a typical element p.
CTL formulae ϕ are defined as follows: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | E fϕ | E2ϕ | Eϕ U ϕ.
The semantics of CTL is based on Kripke models M = St, R, π , which include a nonempty set of states St, a state transition relation R ⊆ St × St, and a valuation of propositions π : Π → P(St).
A path λ in M refers to a possible behavior (or computation) of system M, and can be represented as an infinite sequence of states q0q1q2... such that qiRqi+1 for every i = 0, 1, 2, .... We denote the ith state in λ by λ[i]. A q-path is a path that starts in q.
Interpretation of a formula in a state q in model M is defined as follows: M, q |= p iff q ∈ π(p); M, q |= ¬ϕ iff M, q |= ϕ; M, q |= ϕ ∧ ψ iff M, q |= ϕ and M, q |= ψ; M, q |= E fϕ iff there is a q-path λ such that M, λ[1] |= ϕ; M, q |= E2ϕ iff there is a q-path λ such that M, λ[i] |= ϕ for every i ≥ 0; M, q |= Eϕ U ψ iff there is a q-path λ and i ≥ 0 such that M, λ[i] |= ψ and M, λ[j] |= ϕ for every 0 ≤ j < i.
CTLK [19] is a straightforward combination of CTL and standard epistemic logic [10, 7]. Let Agt = {1, ..., k} be a set of agents with a typical element a. Epistemic logic uses operators for representing agents" knowledge: Kaϕ is read as agent a knows that ϕ. Models of CTLK extend models of CTL with epistemic indistinguishability relations ∼a⊆ St × St (one per agent). We assume that all ∼a are equivalences. The semantics of epistemic operators is defined as follows: M, q |= Kaϕ iff M, q |= ϕ for every q such that q ∼a q .
Note that, when talking about agents" knowledge, we implicitly assume that agents may have imperfect information about the actual current state of the world (otherwise the notion of knowledge would be trivial). This does not have influence on the way we model evolution of a system as a single unit, but it will become important when particular agents and their strategies come to the fore.
Alternating-time temporal logic ATL [3] is a logic for reasoning about temporal and strategic properties of open computational systems (multi-agent systems in particular). The language of ATL consists of the following formulae: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | A fϕ | A 2ϕ | A ϕ U ϕ. where A ⊆ Agt. Informally, A ϕ says that agents A have a collective strategy to enforce ϕ. It should be noted that the CTL path quantifiers A, E can be expressed with ∅ , Agt respectively.
The semantics of ATL is defined in so called concurrent game structures (CGSs). A CGS is a tuple M = Agt, St, Act, d, o, Π, π , 1 Additional operators A (for every path) and 3 (sometime in the future) are defined in the usual way. consisting of: a set Agt = {1, . . . , k} of agents; set St of states; valuation of propositions π : Π → P(St); set Act of atomic actions. Function d : Agt × St → P(Act) indicates the actions available to agent a ∈ Agt in state q ∈ St. Finally, o is a deterministic transition function which maps a state q ∈ St and an action profile α1, . . . , αk ∈ Actk , αi ∈ d(i, q), to another state q = o(q, α1, . . . , αk).
DEFINITION 1. A (memoryless) strategy of agent a is a function sa : St → Act such that sa(q) ∈ d(a, q).2 A collective strategy SA for a team A ⊆ Agt specifies an individual strategy for each agent a ∈ A. Finally, the outcome of strategy SA in state q is defined as the set of all computations that may result from executing SA from q on: out(q, SA) = {λ = q0q1q2... | q0 = q and for every i = 1, 2, ... there exists αi−1
k such that αi−1 a = SA(a)(qi−1) for each a ∈ A, αi−1 a ∈ d(a, qi−1) for each a /∈ A, and o(qi−1, αi−1
k ) = qi}.
The semantics of cooperation modalities is as follows: M, q |= A fϕ iff there is a collective strategy SA such that, for every λ ∈ out(q, SA), we have M, λ[1] |= ϕ; M, q |= A 2ϕ iff there exists SA such that, for every λ ∈ out(q, SA), we have M, λ[i] for every i ≥ 0; M, q |= A ϕ U ψ iff there exists SA such that for every λ ∈ out(q, SA) there is a i ≥ 0, for which M, λ[i] |= ψ, and M, λ[j] |= ϕ for every 0 ≤ j < i.
As ATL does not include incomplete information in its scope, it can be seen as a logic for reasoning about agents who always have complete knowledge about the current state of the whole system.
ATLir [21] includes the same formulae as ATL, except that the cooperation modalities are presented with a subscript: A ir indicates that they address agents with imperfect information and imperfect recall. Formally, the recursive definition of ATLir formulae is: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | A ir fϕ | A ir2ϕ | A irϕ U ϕ Models of ATLir, concurrent epistemic game structures (CEGS), can be defined as tuples M = Agt, St, Act, d, o, ∼1, ..., ∼k, Π, π , where Agt, St, Act, d, o, Π, π is a CGS, and ∼1, ..., ∼k are epistemic (equivalence) relations. It is required that agents have the same choices in indistinguishable states: q ∼a q implies d(a, q) = d(a, q ). ATLir restricts the strategies that can be used by agents to uniform strategies, i.e. functions sa : St → Act, such that: (1) sa(q) ∈ d(a, q), and (2) if q ∼a q then sa(q) = sa(q ). A collective strategy is uniform if it contains only uniform individual strategies.
Again, the function out(q, SA) returns the set of all paths that may result from agents A executing collective strategy SA from state q.
The semantics of ATLir formulae can be defined as follows: M, q |= A ir fϕ iff there is a uniform collective strategy SA such that, for every a ∈ A, q such that q ∼a q , and λ ∈ out(SA, q ), we have M, λ[1] |= ϕ; 2 This is a deviation from the original semantics of ATL [3], where strategies assign agents" choices to sequences of states, which suggests that agents can by definition recall the whole history of each game. While the choice of one or another notion of strategy affects the semantics of the full ATL ∗ , and most ATL extensions (e.g. for games with imperfect information), it should be pointed out that both types of strategies yield equivalent semantics for pure ATL (cf. [21]).
M, q |= A ir2ϕ iff there exists SA such that, for every a ∈ A, q such that q ∼a q , and λ ∈ out(SA, q ), we have M, λ[i] for every i ≥ 0; M, q |= A irϕ U ψ iff there exist SA such that, for every a ∈ A, q such that q ∼a q , and λ ∈ out(SA, q ), there is i ≥ 0 for which M, λ[i] |= ψ, and M, λ[j] |= ϕ for every 0 ≤ j < i.
That is, A irϕ holds iff A have a uniform collective strategy, such that for every path that can possibly result from execution of the strategy according to at least one agent from A, ϕ is the case.
In this section, we present and discuss various (existing) representations of systems that can be used for modeling and model checking. We believe that the two most important points of reference are in this case: (1) the modeling formalism (i.e., the logic and the semantics we use), and (2) the phenomenon, or more generally, the domain we are going to model (to which we will often refer as the real world). Our aim is a representation which is reasonably close to the real world (i.e., it is sufficiently compact and grounded), and still not too far away from the formalism (so that it e.g. easily allows for theoretical analysis of computational problems). We begin with discussing the merits of explicit modelsin our case, these are transition systems, concurrent game structures and CEGSs, presented in the previous section.
Obviously, an advantage of explicit models is that they are very close to the semantics of our logics (simply because they are the semantics). On the other hand, they are in many ways difficult to use to describe an actual system: • Exponential size: temporal models usually have an exponential number of states with respect to any higher-level description (e.g. Boolean variables, n-ary attributes etc.). Also, their size is exponential in the number of processes (or agents) if the evolution of a system results from joint (synchronous or asynchronous) actions of several active entities [15]. For CGSs the situation is even worse: here, also the number of transitions is exponential, even if we fix the number of states.3 In practice, this means that such representations are very seldom scalable. • Explicit models include no modularity. States in a model refer to global states of the system; transitions in the model correspond to global transitions as well, i.e., they represent (in an atomic way) everything that may happen in one single step, regardless of who has done it, to whom, and in what way. • Logics like ATL are often advertised as frameworks for modeling and reasoning about open computational systems.
Ideally, one would like the elements of such a system to have as little interdependencies as possible, so that they can be plugged in and out without much hassle, for instance when we want to test various designs or implementations of the active component. In the case of a multi-agent system the 3 Another class of ATL models, alternating transition systems [2] represent transitions in a more succinct way. While we still have exponentially many states in an ATS, the number of transitions is simply quadratic wrt. to states (like for CTL models).
Unfortunately, ATS are even less modular and harder to design than concurrent game structures, and they cannot be easily extended to handle incomplete information (cf. [9]). need is perhaps even more obvious. We do not only need to re-plug various designs of a single agent in the overall architecture; we usually also need to change (e.g., increase) the number of agents acting in a given environment without necessarily changing the design of the whole system.
Unfortunately, ATL models are anything but open in this sense.
Theoretical complexity results for explicit models are as follows.
Model checking CTL and CTLK is P-complete, and can be done in time O(ml), where m is the number of transitions in the model, and l is the length of the formula [5]. Alternatively, it can be done in time O(n2 l), where n is the number of states. Model checking ATL is P-complete wrt. m, l and ΔP
the number of agents) [3, 12, 16]. Model checking ATLir is ΔP  2complete wrt. m, l and ΔP
Explicit representation of all states and transitions is inefficient in many ways. An alternative is to represent the state/transition space in a symbolic way [17, 18].
Such models offer some hope for feasible model checking properties of open/multi-agent systems, although it is well known that they are compact only in a fraction of all cases.4 For us, however, they are insufficient for another reason: they are merely optimized representations of explicit models. Thus, they are neither more open nor better grounded: they were meant to optimize implementation rather than facilitate design or modeling methodology.
Interpreted systems [11, 7] are held by many as a prime example of computationally grounded models of distributed systems. An interpreted system can be defined as a tuple IS = St1, ..., Stk, Stenv, R, π .
St1, ..., Stk are local state spaces of agents 1, ..., k, and Stenv is the set of states of the environment. The set of global states is defined as St = St1 × ... × Stk × Stenv; R ⊆ St × St is a transition relation, and π : Π → P(St). While the transition relation encapsulates the (possible) evolution of the system over time, the epistemic dimension is defined by the local components of each global state: q1, ..., qk, qenv ∼i q1, ..., qk, qenv iff qi = qi .
It is easy to see that such a representation is modular and compact as far as we are concerned with states. Moreover, it gives a natural (grounded) approach to knowledge, and suggests an intuitive methodology for modeling epistemic states. Unfortunately, the way transitions are represented in interpreted systems is neither compact, nor modular, nor grounded: the temporal aspect of the system is given by a joint transition function, exactly like in explicit models. This is not without a reason: if we separate activities of the agents too much, we cannot model interaction in the framework any more, and interaction is the most interesting thing here.
But the bottom line is that the temporal dimension of an interpreted system has exponential representation. And it is almost as difficult to plug components in and out of an interpreted system, as for an ordinary CTL or ATL model, since the local activity of an agent is completely merged with his interaction with the rest of the system.
The idea of concurrent programs has been long known in the literature on distributed systems. Here, we use the formulation from [15]. A concurrent program P is composed of k concurrent processes, each described by a labeled transition system Pi = Sti, Acti, Ri, Πi, πi , where Sti is the set of local states of process 4 Representation R of an explicit model M is compact if the size of R is logarithmic with respect to the size of M.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 899 i, Acti is the set of local actions, Ri ⊆ Sti ×Acti ×Sti is a transition relation, and Πi, πi are the set of local propositions and their valuation. The behavior of program P is given by the product automaton of P1, ..., Pk under the assumption that processes work asynchronously, actions are interleaved, and synchronization is obtained through common action names.
Concurrent programs have several advantages. First of all, they are modular and compact. They allow for local modeling of components - much more so than interpreted systems (not only states, but also actions are local here). Moreover, they allow for representing explicit interaction between local transitions of reactive processes, like willful communication, and synchronization. On the other hand, they do not allow for representing implicit, incidental, or not entirely benevolent interaction between processes. For example, if we want to represent the act of pushing somebody, the pushed object must explicitly execute an action of being pushed, which seems somewhat ridiculous. Side effects of actions are also not easy to model. Still, this is a minor complaint in the context of CTL, because for temporal logics we are only interested in the flow of transitions, and not in the underlying actions. For temporal reasoning about k asynchronous processes with no implicit interaction, concurrent programs seem just about perfect.
The situation is different when we talk about autonomous, proactive components (like agents), acting together (cooperatively or adversely) in a common environment - and we want to address their strategies and abilities. Now, particular actions are no less important than the resulting transitions. Actions may influence other agents" local states without their consent, they may have side effects on other agents" states etc. Passing messages and/or calling procedures is by no means the only way of interaction between agents. Moreover, the availability of actions (to an agent) should not depend on the actions that will be executed by other agents at the same time - these are the outcome states that may depend on these actions! Finally, we would often like to assume that agents act synchronously. In particular, all agents play simultaneously in concurrent game structures. But, assuming synchrony and autonomy of actions, synchronization can no longer be a means of coordination.
To sum up, we need a representation which is very much like concurrent programs, but allows for modeling agents that play synchronously, and which enables modeling more sophisticated interaction between agents" actions. The first postulate is easy to satisfy, as we show in the following section. The second will be addressed in Section 4.
We note that model checking CTL against concurrent programs is PSPACE-complete in the number of local states and the length of the formula [15].
Modules The semantics of ATL is based on synchronous models where availability of actions does not depend on the actions currently executed by the other players. A slightly different variant of concurrent programs can be defined via synchronous product of programs, so that all agents play simultaneously.5 Unfortunately, under such interpretation, no direct interaction between agents" actions can be modeled at all.
DEFINITION 2. A synchronous concurrent program consists of k concurrent processes Pi = Sti, Acti, Ri, Πi, πi with the follow5 The concept is not new, of course, and has already existed in folk knowledge, although we failed to find an explicit definition in the literature. ing unfolding to a CGS: Agt = {1, ..., k}, St = Qk i=1 Sti, Act = Sk i=1 Acti, d(i, q1, ..., qk ) = {αi | qi, αi, qi ∈ Ri for some qi ∈ Sti}, o( q1, ..., qk , α1, ..., αk) = q1, ..., qk such that qi, αi, qi ∈ Ri for every i; Π = Sk i=1 Πi, and π(p) = πi(p) for p ∈ Πi.
We note that the simple reactive modules (SRML) from [22] can be seen as a particular implementation of synchronous concurrent programs.
DEFINITION 3. A SRML system is a tuple Σ, Π, m1, . . . , mk , where Σ = {1, . . . , k} is a set of modules (or agents), Π is a set of Boolean variables, and, for each i ∈ Σ, we have mi = ctri, initi, updatei , where ctri ⊆ Π. Sets initi and updatei consist of guarded commands of the form φ ; v1 := ψ1; . . . ; vk := ψk, where every vj ∈ ctri, and φ, ψ1, . . . , ψk are propositional formulae over Π. It is required that ctr1, . . . ctrk partitions Π.
The idea is that agent i controls the variables ctri. The init guarded commands are used to initialize the controlled variables, while the update guarded commands can change their values in each round.
A guarded command is enabled if the guard φ is true in the current state of the system. In each round an enabled update guarded command is executed: each ψj is evaluated against the current state of the system, and its logical value is assigned to vj. Several guarded commands being enabled at the same time model non-deterministic choice. Model checking ATL for SRML has been proved EXPTIMEcomplete in the size of the model and the length of the formula [22].
Concurrent programs (both asynchronous and synchronous) can be used to encode epistemic relations too - exactly in the same way as interpreted systems do [20]. That is, when unfolding a concurrent program to a model of CTLK or ATLir, we define that q1, ..., qk ∼i q1, ..., qk iff qi = qi . Model checking CTLK against concurrent epistemic programs is PSPACE-complete [20].
SRML can be also interpreted in the same way; then, we would assume that every agent can see only the variables he controls.
Concurrent epistemic programs are modular and have a grounded semantics. They are usually compact (albeit not always: for example, an agent with perfect information will always blow up the size of such a program). Still, they inherit all the problems of concurrent programs with perfect information, discussed in Section 3.4: limited interaction between components, availability of local actions depending on the actual transition etc. The problems were already important for agents with perfect information, but they become even more crucial when agents have only limited knowledge of the current situation. One of the most important applications of logics that combine strategic and epistemic properties is verification of communication protocols (e.g., in the context of security).
Now, we may want to, e.g., check agents" ability to pass an information between them, without letting anybody else intercept the message. The point is that the action of intercepting is by definition enabled; we just look for a protocol in which the transition of successful interception is never carried out. So, availability of actions must be independent of the actions chosen by the other agents under incomplete information. On the other hand, interaction is arguably the most interesting feature of multi-agent systems, and it is really hard to imagine models of strategic-epistemic logics, in which it is not possible to represent communication.
Reactive modules [1] can be seen as a refinement of concurrent epistemic programs (primarily used by the MOCHA model checker [4]), but they are much more powerful, expressive and
grounded. We have already mentioned a very limited variant of RML (i.e., SRML). The vocabulary of RML is very close to implementations (in terms of general computational systems): the modules are essentially collections of variables, states are just valuations of variables; events/actions are variable updates. However, the sets of variables controlled by different agents can overlap, they can change over time etc. Moreover, reactive modules support incomplete information (through observability of variables), although it is not the main focus of RML. Again, the relationship between sets of observable variables (and to sets of controlled variables) is mostly left up to the designer of a system. Agents can act synchronously as well as asynchronously.
To sum up, RML define a powerful framework for modeling distributed systems with various kinds of synchrony and asynchrony.
However, we believe that there is still a need for a simpler and slightly more abstract class of representations. First, the framework of RML is technically complicated, involving a number auxiliary concepts and their definitions. Second, it is not always convenient to represent all that is going on in a multi-agent system as reading and/or writing from/to program variables. This view of a multi-agent system is arguably close to its computer implementation, but usually rather distant from the real world domainhence the need for a more abstract, and more conceptually flexible framework. Third, the separation of the local complexity, and the complexity of interaction is not straightforward. Our new proposal, more in the spirit of interpreted systems, takes these observations as the starting point. The proposed framework is presented in Section 4.
The idea behind distributed systems (multi-agent systems even more so) is that we deal with several loosely coupled components, where most of the processing goes on inside components (i.e., locally), and only a small fraction of the processing occurs between the components. Interaction is crucial (which makes concurrent programs an insufficient modeling tool), but it usually consumes much less of the agent"s resources than local computations (which makes the explicit transition tables of CGS, CEGS, and interpreted systems an overkill). Modular interpreted systems, proposed here, extrapolate the modeling idea behind interpreted systems in a way that allows for a tight control of the interaction complexity.
DEFINITION 4. A modular interpreted system (MIS) is defined as a tuple S = Agt, env, Act, In , where Agt = {a1, ..., ak} is a set of agents, env is the environment,
Act is a set of actions, and In is a set of symbols called interaction alphabet. Each agent has the following internal structure: ai = Sti, di, outi, ini, oi, Πi, πi , where: • Sti is a set of local states, • di : Sti → P(Act) defines local availability of actions; for convenience of the notation, we additionally define the set of situated actions as Di = { qi, α | qi ∈ Sti, α ∈ di(qi)}, • outi, ini are interaction functions; outi : Di → In refers to the influence that a given situated action (of agent ai) may possibly have on the external world, and ini : Sti ×Ink → In translates external manifestations of the other agents (and the environment) into the impression that they make on ai"s transition function depending on the local state of ai, • oi : Di × In → Sti is a (deterministic) local transition function, • Πi is a set of local propositions of agent ai where we require that Πi and Πj are disjunct when i = j, and • πi : Πi → P(Sti) is a valuation of these propositions.
The environment env = Stenv, outenv, inenv, oenv, Πenv, πenv has the same structure as an agent except that it does not perform actions, and that thus outenv : Stenv → In and oenv : Stenv × In → Stenv.
Within our framework, we assume that every action is executed by an actor, that is, an agent. As a consequence, every actor is explicitly represented in a MIS as an agent, just like in the case of CGS and CEGS. The environment, on the other hand, represents the (passive) context of agents" actions. In practice, it serves to capture the aspects of the global state that are not observable by any of the agents.
The input functions ini seem to be the fragile spots here: when given explicitly as tables, they have size exponential wrt. the number of agents (and linear wrt. the size of In). However, we can use, e.g., a construction similar to the one from [16] to represent interaction functions more compactly.
DEFINITION 5. Implicit input function for state q ∈ Sti is given by a sequence ϕ1, η1 , ..., ϕn, ηn , where each ηj ∈ In is an interaction symbol, and each ϕj is a boolean combination of propositions ˆηi , with η ∈ In; ˆηi stands for η is the symbol currently generated by agent i. The input function is now defined as follows: ini(q, 1, ..., k, env) = ηj iff j is the lowest index such that {ˆ1 1, ..., ˆk k, ˆenv env} |= ϕj. It is required that ϕn ≡ , so that the mapping is effective.
REMARK 1. Every ini can be encoded as an implicit input function, with each ϕj being of polynomial size with respect to the number of interaction symbols (cf. [16]).
Note that, for some domains, the MIS representation of a system requires exponentially many symbols in the interaction alphabet In.
In such a case, the problem is inherent to the domain, and ini will have size exponential wrt the number of agents.
Let Stg = ( Qk i=1 Sti)×Stenv be the set of all possible global states generated by a modular interpreted system S.
DEFINITION 6. The unfolding of a MIS S for initial states Q ⊆ Stg to a CEGS cegs(S, Q) = Agt , St , Π , π , Act , d , o , ∼1, ..., ∼k is defined as follows: • Agt = {1, ..., k} and Act = Act, • St is the set of global states from Stg which are reachable from some state in Q via the transition relation defined by o (below), • Π = Sk i=1 Πi ∪ Πenv, • For each q = q1, . . . , qk, qenv ∈ St and i = 1, ..., k, env, we define q ∈ π (p) iff p ∈ Πi and qi ∈ πi(p), • d (i, q) = di(qi) for global state q = q1, ..., qk, qenv , • The transition function is constructed as follows. Let q = q1, ..., qk, qenv ∈ St , and α = α1, ..., αk be an action profile s.t. αi ∈ d (i, q). We define inputi(q, α) = ini ` qi, out1(q1, α1), . . . , outi−1(qi−1, αi−1), outi+1(qi+1, αi+1), . . . , outk(qk, αk), outenv(qenv) ´ for each agent i = 1, . . . , k, and inputenv(q, α) = inenv ` qenv, out1(q1, α1), . . . , outk(qk, αk) ´ .
Then, o (q, α) = o1( q1, α1 , input1(q, α)), . . . , ok( qk, αk , inputk(q, α)), oenv(qenv, inputenv(q, α)) ; The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 901 • For each i = 1, ..., k: q1, ..., qk, qenv ∼i q1, ..., qk, qenv iff qi = qi .6 REMARK 2. Note that MISs can be used as representations of CGSs too. In that case, epistemic relations ∼i are simply omitted in the unfolding. We denote the unfolding of a MIS S for initial states Q into a CGS by cgs(S, Q).
Propositions 3 and 5 state that modular interpreted systems can be used as representations for explicit models of multi-agent systems. On the other hand, these representations are not always compact, as demonstrated by Propositions 7 and 8.
PROPOSITION 3. For every CEGS M, there is a MIS SM and a set of global states Q of SM such that cegs(SM , Q) is isomorphic to M.7 PROOF. Let M = {1, . . . , k}, St, Act, d, o, Π, π, ∼1, . . . , ∼k be a CEGS. We construct a MIS SM = {a1, . . . , ak}, env, Act, In with agents ai = Sti, di, outi, ini, oi, Πi, πi and environment env = Stenv, outenv, inenv, oenv, Πenv, πenv , plus a set Q ⊆ Stg of global states, as follows. • In = Act ∪ St ∪ (Actk−1 × St), • Sti = {[q]∼i | q ∈ St} for 1 ≤ i ≤ k (i.e., Sti is the set of i"s indistinguishability classes in M), • Stenv = St, • di([q]∼i ) = d(i, q) for 1 ≤ i ≤ k (this is well-defined since d(i, q) = d(i, q ) whenever q ∼i q ), • outi([q]∼i , αi) = αi for 1 ≤ i ≤ k; outenv(q) = q, • ini([q]∼i , α1, . . . , αi−1, αi+1, . . . , αk, qenv) = α1, . . . , αi−1, αi+1, . . . , αk, qenv for i ∈ {1, . . . , k}; inenv(q, α1 . . . , αk) = α1, . . . , αk ; ini(x) and inenv(x) are arbitrary for other arguments x, • oi( [q]∼i , αi , α1, . . . , αi−1, αi+1, . . . , αk, qenv ) = [o(qenv, α1, . . . , αk)]∼i for 1 ≤ i ≤ k and αi ∈ di([q]∼i ); oenv(q, α1, . . . , αk ) = o(q, α1, . . . , αk); oi and oenv are arbitrary for other arguments, • Πi = ∅ for 1 ≤ i ≤ k, and Πenv = Π, • πenv(p) = π(p) • Q = { [q]∼1 , . . . , [q]∼k , q : q ∈ St} Let M = cegs(SM , Q) = Agt , St , Act , d , o , Π , π , ∼1, . . . , ∼k .
We argue that M and M are isomorphic by establishing a oneto-one correspondence between the respective sets of states, and showing that the other parts of the structures agree on corresponding states.
First we show that, for any ˆq = [q ]∼1 , . . . , [q ]∼k , q ∈ Q and any α = α1, . . . , αk such that αi ∈ d (i, ˆq ), we have o (ˆq , α) = [q]∼1 , . . . , [q]∼k , q where q = o(q , α) (1) Let ˆq = o (ˆq , α). Now, for any i: inputi(ˆq , α) = ini([q ]∼i , out1([q ]∼1 , α1), ..., outi−1([q ]∼i−1 , αi−1), outi+1([q ]∼i+1 , αi+1), . . . , outk([q ]∼k , αk), outenv(q )) = ini([q ]∼i , α1, . . . , αi−1, αi+1, 6 This shows another difference between the environment and the agents: the environment does not possess knowledge. 7 We say that two CEGS are isomorphic if they only differ in the names of states and/or actions. . . . , αk, q ) = α1, . . . , αi−1, αi+1, . . . , αk, q . Similarly, we get that inputenv(ˆq , α) = α1, . . . , αk . Thus we get that o (ˆq , α) = o1( [q ]∼1 , α1 , input1(ˆq , α)), . . . , ok( [q ]∼k , αk , inputk(ˆq , α)), oenv(q , inputenv(ˆq , α)) = [o(q , α1, . . . , αk)]∼1 , . . . , [o(q , α1, . . . , αk)]∼k , o(q , α1, . . . , αk) . Thus, ˆq = [q]∼1 , . . . , [q]∼k , q for q = o(q , α1, . . . , αk), which completes the proof of (1).
We now argue that St = Q. Clearly, Q ⊆ St . Let ˆq ∈ St ; we must show that ˆq ∈ Q. The argument is on induction on the length of the least o path from Q to ˆq. The base case, ˆq ∈ Q, is immediate. For the inductive step, ˆq = o (ˆq , α) for some ˆq ∈ Q, and then we have that ˆq ∈ Q by (1). Thus, St = Q.
Now we have a one-to-one correspondence between St and St : r ∈ St corresponds to [r]∼1 , . . . , [r]∼k , r ∈ St . It remains to be shown that the other parts of the structures M and M agree on corresponding states: • Agt = Agt, • Act = Act, • Π = Sk i=1 Πi ∪ Πenv = Π, • For p ∈ Π = Π: [q ]∼1 , . . . , [q ]∼k , q ∈ π (p) iff q ∈ πenv(p) iff q ∈ π(p) (same valuations at corresponding states), • d (i, [q ]∼1 , . . . , [q ]∼k , q ) = di([q ]∼i ) = d(i, q), • It follows immediately from (1), and the fact that Q = St , that o ( [q ]∼1 , . . . , [q ]∼k , q , α) = [r ]∼1 , . . . , [r ]∼k , r iff o(q , α) = r (transitions on the same joint action in corresponding states lead to corresponding states), • [q ]∼1 , . . . , [q ]∼k , q ∼i [r ]∼1 , . . . , [r ]∼k , r iff [q ]∼i = [r ]∼i iff q ∼i r (the accessibility relations relate corresponding states), which completes the proof.
COROLLARY 4. For every CEGS M, there is an ATLir-equivalent MIS S with initial states Q, that is, for every state q in M there is a state q in cegs(S, Q) satisfying exactly the same ATLir formulae, and vice versa.
PROPOSITION 5. For every CGS M, there is a MIS SM and a set of global states Q of SM such that cgs(SM , Q) is isomorphic to M.
PROOF. Let M = Agt, St, Act, d, o, Π, π be given. Now, let ˆM = Agt, St, Act, d, o, Π, π, ∼1, . . . , ∼k for some arbitrary accessibility relations ∼i over St. By Proposition 3, there exists a MIS S ˆM with global states Q such that ˆM = cegs(S ˆM , Q) is isomorphic to ˆM. Let M be the CGS obtained by removing the accessibility relations from ˆM . Clearly, M is isomorphic to M.
COROLLARY 6. For every CGS M, there is an ATL-equivalent MIS S with initial states Q. That is, for every state q in M there is a state q in cgs(S, Q) satisfying exactly the same ATL formulae, and vice versa.
PROPOSITION 7. The local state spaces in a MIS are not always compact with respect to the underlying concurrent epistemic game structure.
PROOF. Take a CEGS M in which agent i has always perfect information about the current global state of the system. When constructing a modular interpreted system S such that M = cegs(S, Q), we have that Sti must be isomorphic with St.
The above property is a part of the interpreted systems heritage.
The next proposition stems from the fact that explicit models (and interpreted systems) allow for intensive interaction between agents.
PROPOSITION 8. The size of In in S is, in general, exponential with respect to the number of local states and local actions. This is the case even when epistemic relations are not relevant (i.e., when S is taken as a representation of an ordinary CGS).
PROOF. Consider a CGS M with agents Agt = {1, ..., k}, global states St = Qk i=1{qi 0, ..., qi i}, and actions Act = {0, 1}, all enabled everywhere. The transition function is defined as o( q1 j1 , ..., qk jk , α1, ..., αk) = q1 l1 , ..., qk lk , where li = (ji + α1 + ... + αk) mod i. Note that M can be represented as a modular interpreted system with succinct local state spaces Sti = {qi 0, ..., qi i}.
Still, the current actions of all agents are relevant to determine the resulting local transition of agent i.
We will call items In, outi, ini the interaction layer of a modular interpreted system S; the other elements of S constitute the local layer of the MIS. In this paper we are ultimately interested in model checking complexity with respect to the size of the local layer. To this end, we will assume that the size of interaction layer is polynomial in the number of local states and actions. Note that, by Propositions 7 and 8, not every explicit model submits to compact representation with a MIS. Still, as we declared at the beginning of Section 4, we are mainly interested in a modeling framework for systems of loosely coupled components, where interaction is essential, but most processing is done locally anyway. More importantly, the framework of MIS allows for separating the interaction of agents from their local structure to a larger extent. Moreover, we can control and measure the complexity of each layer in a finer way than before. First, we can try to abstract from the complexity of a layer (e.g. like in this paper, by assuming that the other layer is kept within certain complexity bounds). Second, we can also measure separately the interaction complexity of different agents.
Reactive Modules In this section we show that simple reactive modules are (as we already suggested) a specific (and somewhat limited) implementation of modular interpreted systems. First, we define our (quite strong) notion of equivalence of representations.
DEFINITION 7. Two representations are equivalent if they unfold to isomorphic concurrent epistemic game structures. They are CGS-equivalent if they unfold to the same CGS.
PROPOSITION 9. For any SRML there is a CGS-equivalent MIS.
PROOF. Consider an SRML R with k modules and n variables.
We construct S = Agt, Act, In with Agt = {a1, ..., ak}, Act = { 1, ..., n, ⊥1, ..., ⊥n}, and In = Sk i=1 Sti × Sti (the local state spaces Sti will be defined in a moment). Let us assume without loss of generality that ctri = {x1, ..., xr}. Also, we consider all guarded commands of i to be of type γi,ψ : ψ ; xi := , or γ⊥ i,ψ : ψ ; xi := ⊥. Now, agent ai in S has the following components: Sti = P(ctri) (i.e., local states of ai are valuations of variables controlled by i); di(qi) = { 1, ..., r, ⊥1, ..., ⊥r}; outi(qi, α) = qi, qi ; ini(qi, q1, q1 , ..., qi−1, qi−1 , qi+1, qi+1 , qk, qk ) = {xi ∈ ctri | q1, ..., qk |= W γi,ψ ψ}, {xi ∈ ctri | q1, ..., qk |= W γ⊥ i,ψ ψ} . To define local transitions, we consider three cases. If t = f = ∅ (no update is enabled), then oi(qi, α, t, f ) = qi for every action α. If t = ∅, we take any arbitrary ˆx ∈ t, and define oi(qi, j, t, f ) = qi ∪ {xj} if xj ∈ t, and qi ∪ {ˆx} otherwise; oi(qi, ⊥j, t, f ) = qi \ {xj} if xj ∈ f, and qi ∪ {ˆx} otherwise.
Moreover, if t = ∅ = f, we take any arbitrary ˆx ∈ f, and define oi(qi, j, t, f ) = qi ∪ {xj} if xj ∈ t, and qi \ {ˆx} otherwise; oi(qi, ⊥j, t, f ) = qi \{xj} if xj ∈ f, and qi \{ˆx} otherwise. Finally,
Πi = ctri, and qi ∈ πi(xj) iff xj ∈ qi.
The above construction shows that SRML have more compact representation of states than MIS: ri local variables of agent i give rise to 2ri local states. In a way, reactive modules (both simple and full) are two-level representations: first, the system is represented as a product of modules; next, each module can be seen as a product of its variables (together with their update operations).
Note, however, that specification of updates with respect to a single variable in an SRML may require guarded commands of total length O(2 Pk i=1 ri ). Thus, the representation of transitions in SRML is (in the worst case) no more compact than in MIS, despite the two-level structure of SRML. We observe finally that MIS are more general, because in SRML the current actions of other agents have no influence on the outcome of agent i"s current action (although the outcome can be influenced by other agents" current local states).
Systems One of our main aims was to study the complexity of symbolic model checking ATLir in a meaningful way. Following the reviewers" remarks, we state our complexity results only as conjectures.
Preliminary proofs can be found in [14].
CONJECTURE 10. Model checking ATL for modular interpreted systems is EXPTIME-complete.
CONJECTURE 11. Model checking ATLir for the class of modular interpreted systems is PSPACE-complete.
A summary of complexity results for model checking temporal and strategic logics (with and without epistemic component) is given in the table below. The table presents completeness results for various models and settings of input parameters. Symbols n, k, m stand for the number of states, agents and transitions in an explicit model; l is the length of the formula, and nlocal is the number of local states in a concurrent program or modular interpreted system. The new results, conjectured in this paper, are printed in italics. Note that the result for model checking ATL against modular interpreted systems is an extension of the result from [22]. m, l n, k, l nlocal, k, l CTL P [5] P [5] PSPACE [15] CTLK P [5, 8] P [5, 8] PSPACE [20] ATL P [3] ΔP
ATLir ΔP
If we are right, then the results for ATL and ATLir form an intriguing pattern. When we compare model checking agents with perfect vs. imperfect information, the first problem appears to be much easier against explicit models measured with the number of transitions; next, we get the same complexity class against explicit models measured with the number of states and agents; finally, model checking imperfect information turns out to be easier than model checking perfect information for modular interpreted systems. Why can it be so?
First, a MIS unfolds into CEGS and CGS in a different way. In the first case, the MIS is assumed to encode the epistemic relations explicitly (which makes it explode when we model agents with perfect, or almost perfect information). In the latter case, the epistemic The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 903 aspect is ignored, which gives some extra room for encoding the transition relation more efficiently. Another crucial factor is the number of available strategies (relative to the size of input parameters). The number of all strategies is exponential in the number of global states; for uniform strategies, there are usually much less of them but still exponentially many in general. Thus, the fact that perfect information strategies can be synthesized incrementally has a substantial impact on the complexity of the problem. However, measured in terms of local states and agents, the number of all strategies is doubly exponential, while there are only exponentially many uniform strategies - which settles the results in favor of imperfect information.
We have presented a new class of representations for open multiagent systems. Our representations, called modular interpreted systems, are: modular, in the sense that components can be changed, replaced, removed or added, with as little changes to the whole representation as possible; more compact than traditional explicit representations; and grounded, in the sense that the correspondences between the primitives of the model and the entities being modeled are more immediate, giving a methodology for designing and implementing systems. We also conjecture that the complexity of model checking strategic ability for our representations is higher if we assume perfect information than if we assume imperfect information.
The solutions, proposed in this paper, are not necessarily perfect (for example, the impression functions ini seem to be the main source of non-modularity in MIS, and can be perhaps improved), but we believe them to be a step in the right direction.
We also do not mean to claim that our representations should replace more elaborate modeling languages like Promela or reactive modules. We only suggest that there is a need for compact, modular and reasonably grounded models that are more expressive than concurrent (epistemic) programs, and still allow for easier theoretical analysis than reactive modules. We also suggest that MIS might be better suited for modeling simple multi-agent domains, especially for human-oriented (as opposed to computer-oriented) design.
We thank the anonymous reviewers and Andrzej Tarlecki for their helpful remarks. Thomas Ågotnes" work on this paper was supported by grants 166525/V30 and 176853/S10 from the Research Council of Norway.

The suitability of agent-based computing to manage the complex patterns of interactions naturally occurring in the development of large scale, open systems, has become one of its major assets over the last few years [26, 24, 15].
Particularly, the organizational or social stance advocated by institutional frameworks [2] and most multi-agent system (MAS) methodologies [26, 10], provides an excellent basis to deal with the complexity and dynamism of the interactions among system components. This approach has resulted in a wide spectrum of organizational and communicative abstractions, such as institutions, normative positions, power relationships, organizations, groups, scenes, dialogue games, communicative actions (CAs), etc., to effectively model the interaction space of MAS. This wealth of computational abstractions has found currency in several programming frameworks and software platforms (AMELI [9], MadKit [13],
INGENIAS toolkit [18], etc.), which leverage multi-agent middlewares built upon raw ACL-based interaction mechanism [14], and minimize the gap between organizational metamodels and target implementation languages.
Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions.
The main hypothesis motivating this paper is that the variety of multi-agent interaction mechanisms - both, organizational and communicative, share a common semantic core.
This paper thus focuses on the fundamental building blocks of multi-agent interactions: those which may be composed, extended or refined in order to define more complex organizational or communicative types of interactions.
Its first goal is to carry out a principled analysis of multiagent interactions, departing from general features commonly ascribed to agent-based computing: autonomy, situatedness and sociality [26]. To approach this issue, we draw on the notion of connector, put forward within the field of software architectures [1, 17]. The outcome of this analysis will be a connector-based model of multi-agent interactions between autonomous social and situated components, i.e. agents, attempting to identify their essential structure. Furthermore, the paper also provides this model with a formal execution semantics which describes the dynamics of multi-agent (or social) interactions. Structural Operational Semantics (SOS)[21], a common technique to specify the operational semantics of programming languages, is used for this purpose.
The paper is structured as follows: first, the major entities and relationships which constitute the structure of social interactions are introduced. Next, the dynamics of social interactions will show how these entities and relationships evolve. Last, relevant work in the literature is discussed 889 978-81-904262-7-5 (RPS) c 2007 IFAAMAS with respect to the proposal, limitations are addressed, and current and future work is described.
From an architectural point of view, interactions between software components are embodied in software connectors: first-class entities defined on the basis of the different roles played by software components and the protocols that regulate their behaviour [1]. The roles of a connector represent its participants, such as the caller and callee roles of an RPC connector, or the sender and receiver roles in a message passing connector. The attachment operation binds a component to the role of a given connector.
The analysis of social interactions introduced in this section gives rise to a new kind of social connector. It refines the generic model in several respects, attending to the features commonly ascribed to agent-based computing: • According to the autonomy feature, we may distinguish a first kind of participant (i.e. role) in a social interaction, so-called agents. Basically, agents are those software components which will be regarded as autonomous within the scope of the interaction1 . • A second group of participants, so-called environmental resources, may be identified from the situatedness feature. Unlike agents, resources represent those nonautonomous components whose state may be externally controlled by other components (agents or resources) within the interaction. Moreover, the participation of resources in an interaction is not mandatory. • Last, according to the sociality of agents, the specification of social connector protocols - the glue linking agents among themselves and with resources, will rely on normative concepts such as permissions, obligations and empowerments [23].
Besides agents, resources and social protocols, two other kinds of entities are of major relevance in our analysis of social interactions: actions, which represent the way in which agents alter the environmental and social state of the interaction; and events, which represent the changes in the interaction resulting from the performance of actions or the activity of environmental resources.
In the following, we describe the basic entities involved in social interactions. Each kind of entity T will be specified as a record type T l1 : T1, . . . ln : Tn , possibly followed by a number of invariants, definitions, and the actions affecting their state. Instances or values v of a record type T will be represented as v = v1, . . . , vn : T. The type SetT represents a collection of values drawn from type T. The type QueueT represents a queue of values v : T waiting to be processed. The value v in the expression [v| ] : Queue[T] represents the head of the queue. The type Enum {v1, . . . , vn} 1 Note that we think of the autonomy feature in a relative, rather than absolute, perspective. Basically, this means that software components counting as agents in a social interaction may behave non-autonomously in other contexts, e.g. in their interactions through human-user interfaces. This conceptualization of agenthood resembles the way in which objects are understood in CORBA: as any kind of software component (C, Prolog, Cobol, etc.) attached to an ORB. represents an enumeration type whose values are v1, . . . , vn. Given some value v : T, the term vl refers to the value of the field l of a record type T. Given some labels l1, l2, . . . , the expression vl1,l2,... is syntactic sugar for ((vl1 )l2 ) . . ..
The special term nil will be used to represent the absence of proper value for an optional field, so that vl = nil will be true in those cases and false otherwise. The formal model will be illustrated with several examples drawn from the design of a virtual organization to aid in the management of university courses.
Social interactions shall be considered as composite connectors [17], structured in terms of a tree of nested subinteractions. Let"s consider an interaction representing a university course (e.g. on data structures). On the one hand, this interaction is actually a complex one, made up of lower-level interactions. For instance, within the scope of the course agents will participate in programming assignment groups, lectures, tutoring meetings, examinations and so on. Assignment groups, in turn, may hold a number of assignment submissions and test requests interactions. A test request may also be regarded as a complex interaction, ultimately decomposed in the atomic, or bottom-level interactions represented by communicative actions (e.g. request, agree, refuse, . . . ). On the other hand, courses are run within the scope of a particular degree (e.g. computer science), a higher-level interaction. Traversing upwards from a degree to its ancestors, we find its faculty, the university and, finally, the multi-agent community or agent society.
The community is thus the top-level interaction which subsumes any other kind of multi-agent interaction2 .
The organizational and communicative interaction types identified above clearly differ in many ways. However, we may identify four major components in all of them: the participating agents, the resources that agents manipulate, the protocol regulating the agent activities and the subinteraction space. Accordingly, we may specify the type I of social interactions, ranged over by the meta-variable i, as follows: I state : SI, ini : A, mem : Set A, env : Set R, sub : Set I, prot : P, ch : CH def. : (1) icontext = i1 ⇔ i ∈ isub 1 inv. : (2) iini = nil ⇔ icontext = nil act. : setUp, join, create, destroy where the member and environment fields represent the agents (A) and local resources (R) participating in the interaction; the sub-interaction field, its set of inner interactions; and the protocol field the rules that govern the interaction (P). The event channel, to be described in the next section, allows the dispatching of local events to external interactions. The context of some interaction is defined as its super-interaction (def. 1), so that the context of the toplevel interaction is nil.
The type SI Enum {open, closing, closed} represents the possible execution states of the interaction. Any interaction, but the top-level one, is set up within the context of another interaction by an initiator agent. The initiator is 2 In the context of this application, a one-to-one mapping between human users and software components attached to the community as agents would be a right choice.
thus a mandatory feature for any interaction different to the community (inv. 2). The life-cycle of the interaction begins in the open state. Its sets of agent and resource participants, initially empty, vary as agents join and leave the interaction, and as they create and destroy resources from its local environment. Eventually, the interaction may come to an end (according to the protocol"s rules), or be explicitly closed by some agent, thus prematurely disabling the activity of its participants. The transient closing state will be described in the next section.
Components attach themselves as agents in social interactions with the purpose of achieving something. The purpose declared by some agent when it joins an interaction shall be regarded as the institutional goal that it purports to satisfy within that context3 . The types of agents participating in a given interaction are primarily identified from their purposes. For instance, students are those agents participating in a course who purport to obtain a certificate in the course"s subject. Other members of the course include lecturers and teaching assistants.
The type A of agents, ranged over by meta-variable a, is defined as follows: A state : SA, player : A, purp : F, att : Queue ACT , ev : Queue E, obl : Set O def. : (3) acontext = i ⇔ a ∈ imem (4) a1 ∈ aroles ⇔ aplayer
(5) i ∈ apartIn ⇔ a1 ∈ imem ∧ a1 ∈ aroles act. : see where the purpose is represented as a well-formed boolean formula, of a generic type F, which evaluates to true if the purpose is satisfied and false otherwise. The context of some agent is defined as the interaction in which it participates (def. 3).
The type SA Enum {playing, leaving, succ, unsuc} represents the execution state of the agent. Its life-cycle begins in the playing state when its player agent joins the interaction, or some software component is attached as an agent to the multi-agent system (in this latter case, the player value is nil). The derived roles and partIn features represent the roles played by the agent and the contexts in which these roles are played (def. 4, 5)4 . An agent may play roles at interactions within or outside the scope of its context. For instance, students of a course are played by student agents belonging to the (undergraduate) degree, whereas lecturers may be played by teachers of a given department and the assistant role may be played by students of a Ph.D degree (both, the department and the Ph.D. degrees, are modelled as sub-interactions of the faculty).
Components will normally attempt to perform different actions (e.g. to set up sub-interactions) in order to satisfy their purposes within some interaction. Moreover, components need to be aware of the current state of the interaction, so that they will also be capable of observing certain events from the interaction. Both, the visibility of the interaction 3 Thus, it may or may not correspond to actual internal goals or intentions of the component. 4 Free variables in the antecedents/consequents of implications shall be understood as universally/existentially quantified. and the attempts of members, are subject to the rules governing the interaction. The attempts and events fields of the agent structure represent the queues of attempts to execute some actions (ACT ), and the events (E) received by the agent which have not been observed yet. An agent may update its event queue by seeing the state of some entity of the community. The last field of the structure represents the obligations (O) of agents, to be described later.
Eventually, the participation of some agent in the interaction will be over. This may either happen when certain conditions are met (specified by the protocol rules), or when the agent takes the explicit decision of leaving the interaction. In either case, the final state of the agent will be successful if its purpose was satisfied; unsuccessful otherwise. The transient leaving state will be described in the next section.
Resources are software components which may represent different types of non-autonomous informational or computational entities. For instance, objectives, topics, assignments, grades and exams are different kinds of informational resources created by lecturers and assistants in the context of the course interaction. Students may also create programs to satisfy the requirements of some assignment. Other types of computational resources put at the disposal of students by teachers include compilers and interpreters.
The type R of resources, ranged over by meta-variable r, can be specified by the following record type: R cr : A, owners : Set A, op : Set OP def. : (6) rcontext = i ⇔ r ∈ ienv act. : take, share, give, invoke Essentially, resources can be regarded as objects deployed in a social setting. This means that resources are created, accessed and manipulated by agents in a social interaction context (def. 6), according to the rules specified by its protocol. The mandatory feature creator represents the agent who created this resource. Moreover, resources may have owners. The ownership relationship between members and resources is considered as a normative device aimed at the simplification of the protocol"s rules that govern the interaction of agents and the environment. Members may gain ownership of some resource by taking it, and grant ownership to other agents by giving or sharing their own properties. For instance, the ownership of programs may be shared by several students if the assignment can be performed by groups of two or more students.
The last operations feature represents the interface of the resource, consisting of a set of operations. A resource is structured around several public operations that participants may invoke, in accordance to the rules specified by the interaction"s protocol. The set of operations of a resource makes up its interface.
The protocol of any interaction is made up of the rules which govern its overall state and dynamics. The present specification abstracts away the particular formalism used to specify these rules, and focuses instead on several requirements concerning the structure and interface of protocols.
Accordingly, the type P of protocols, ranged over by metaThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 891 variable p, is defined as follows5 : P emp : A × ACT → Boolean, perm : A × ACT → Boolean, obl :→ Set (A × Set O × Set E), monitor : E → Set A, finish :→ Boolean, over : A → Boolean def. : (7) pcontext = i ⇔ p = iprot inv. : (8) pfinish() ∧ s ∈ pcontext,sub ⇒ sprot,finish() (9) pfinish() ∧ a ∈ pcontext,mem ⇒ pover(a) (10) pover(a) ∧ ai ∈ aroles ⇒ acontext,prot,over i (ai) (11) αadd ∪ {a} ⊆ pmonitor( a, α, ) act. : Close, Leave We demand from protocols four major kinds of functions.
Firstly, protocols shall include rules to identify the empowerments and permissions of any agent attempting to alter the state of the interaction (e.g. its members, the environment, etc.) through the execution of some action (e.g. join, create, etc.). Empowerments shall be regarded as the institutional capabilities which some agent possesses in order to satisfy its purpose. Corresponding rules, encapsulated by the empowered function field, shall allow to determine whether some agent is capable to perform a given action over the interaction. Empowerments may only be exercised under certain circumstances - that permissions specify.
Permission rules shall allow to determine whether the attempt of an empowered agent to perform some particular action is satisfied or not (cf. permitted field). For instance, the course"s protocol specifies that the agents empowered to join the interaction as students are those students of the degree who have payed the fee established for the course"s subject, and own the certificates corresponding to its prerequisite subjects. Permission rules, in turn, specify that those students may only join the course in the admission stage.
Hence, even if some student has paid the fee, the attempt to join the course will fail if the course has not entered the corresponding stage6 .
Secondly, protocols shall allow to determine the obligations of agents towards the interaction. Obligations represent a normative device of social enforcement, fully compatible with the autonomy of agents, used to bias their behaviour in a certain direction. These kinds of rules shall allow to determine whether some agent must perform an action of a given type, as well as if some obligation was fulfilled, violated or needs to be revoked. The function obligations of the protocol structure thus identifies the agents whose obligation set must be updated. Moreover, it returns for each agent a collection of events representing the changes in the obligation set. For instance, the course"s protocol establishes that members of departments must join the course as teachers whenever they are assigned to the course"s subject.
Thirdly, the protocol shall allow to specify monitoring rules for the different events originating within the interaction. Corresponding rules shall establish the set of agents that must be awared of some event. For instance, this func5 The formalization assumes that protocol"s functions implicitly recieve as input the interaction being regulated. 6 The hasPaidFee relationship between (degree) students and subject resources is represented by an additional, application-dependent field of the agent structure for this kind of roles. Similarly, the admission stage is an additional boolean field of the structure for school interactions. The generic types I, A, R and P are thus extendable. tionality is exploited by teachers in order to monitor the enrollment of students to the course.
Last, the protocol shall allow to control the state of the interaction as well as the states of its members. Corresponding rules identify the conditions under which some interaction will be automatically finished, and whether the participation of some member agent will be automatically over. Thus, the function field finish returns true if the regulated interaction must finish its execution. If so happens, a well-defined set of protocols must ensure that its sub-interactions and members are finished as well (inv. 8,9). Similarly, the function over returns true if the participation of the specified member must be over. Well-formed protocols must ensure the consistency between these functions across playing roles (inv. 10)7 . For instance, the course"s protocol establishes that the participation of students is over when they gain ownership of the course"s certificate or the chances to get it are exhausted.
It also establishes that the course must be finished when the admission stage has passed and all the students finished their participation.
The dynamics of the multi-agent community is influenced by the external actions executed by software components and the protocols governing their interactions. This section focuses on the dynamics resulting from a particular kind of external action: the attempt of some component, attached to the community as an agent, to execute a given (internal) action. The description of other external actions concerning agents (e.g. observe the events from its event queue, enter or exit from the community) and resources (e.g. a timer resource may signal the pass of time) will be skipped.
The processing of some attempt may give rise to changes in the scope of the target interaction, such as the instantiation of new participants (agents or resources) or the setting up of new sub-interactions. These resulting events may cause further changes in the state of other interactions (the target one included), namely, in its execution state as well as in the execution state, obligations and visibility of their members. This section will also describe the way in which these events are processed. The resulting dynamics described bellow allows for actions and events corresponding to different agents and interactions to be processed simultaneously. Due to lack of space, we only include some of the operational rules that formalise the execution semantics.
An attempt is defined by the structure AT T perf : A, act : ACT , where the performer represents the agent in charge of executing the specified action. This action is intended to alter the state of some target interaction (possibly, the performer"s context itself), and notify a collection of addressees of the changes resulting from a successful execution. Accordingly, the type ACT of actions, ranged over by meta-variable α, is specified as follows: ACT state : SACT , target : I, add : Set A def. : (12) αperf = a ⇔ α ∈ aatt 7 The close and leave actions update the finish and over function fields as explained in the next section. Additional actions, such as permit, forbid, empower, etc., to update other protocol"s fields are yet to be identified in future work.
where: the performer is formally defined as the agent who stores the action in its queue of attempts, and the state field represents the current phase of processing. This process goes through four major phases, as specified by the enumeration type SACT Enum {emp, perm, exec} : empowerment checking, permission checking and action execution, described in the sequel.
The post-condition of an attempt consists of inserting the action in the queue of attempts of the specified performer.
As rule 1 specifies8 , this will only be possible if the performer is empowered to execute that action according to the rules that govern the state of the target interaction. If this condition is not met, the attempt will simply be ignored.
Moreover, the performer agent must be in the playing state (this pre-condition is also required for any rule concerning the processing of attempts). If these pre-conditions are satisfied the rule is fired and the processing of the action continues in the permission checking stage. For instance, when the software component attached as a student in a degree attempts to join as a student the course in which some subject is teached, the empowerment rules of the course interaction are checked. If the (degree) student has passed the course"s prerequisite subjects the join action will be inserted in its queue of attempts and considered for execution. αtarget,prot,emp(a, α) a = playing, , , qACT , , a,α :AT T −→ playing, , , qACT , , (1) W here : (α )state = perm (qACT ) = insert(α , qACT )
The processing of the action resumes when the possible preceding actions in the performer"s queue of attempts are fully processed and removed from the queue. Moreover, there should be no pending events to be processed in the interaction, for these events may cause the member or the interaction to be finished (as will be shortly explained in the next sub-section). If these conditions are met the permissions to execute the given action (and notify the specified addressees) are checked (e.g. it will be checked whether the student paid the fee for the course"s subject). If the protocol of the target interaction grants permission, the processing of the attempt moves to the action execution stage (rule 2).
Otherwise, the action is discharged and removed from the queue. Unlike unempowered attempts, a forbidden one will cause an event to be generated and transfered to the event channel for further processing. αstate = perm ∧ acontext,ch,in,ev = ∅ ∧ αtarget,prot,perm(a, α) a = playing, , , [α| ], , −→ playing, , , [α | ], , (2) W here : (α )state = exec 8 Labels of record instances are omitted to allow for more compact specifications. Moreover, note that record updates in where clauses only affect the specified fields.
The transitions fired in this stage are classified according to the different types of actions to be executed. The intended effects of some actions may directly be achieved in a single step, while others will required an indirect approach and possibly several execution steps. Actions of the first kind are constructive ones such as set up and join.
The second group of actions include those, such as close and leave, whose effects are indirectly achieved by updating the interaction protocol.
As an example of constructive action, let"s consider the execution of a set up action, whose type is defined as follows9 : SetUp ACT · new : I inv. : (13) αnew,mem = αnew,res = αnew,sub = ∅ (14) αnew,state = open where the new field represents the new interaction to be initiated. Its sets of participants (agents and resources) and sub-interactions must be empty (inv. 13) and its state must be open (inv. 14). The setting up of the new interaction may thus affect its protocol and possible application-dependent fields (e.g. the subject of a course interaction). According to rule 3, the outcome of the execution is threefold: firstly, the performer"s attempt queue is updated so that the executing action is removed; secondly, the new interaction is added to the target"s set of sub-interactions (moreover, its initiator field is set to the performer agent); last, the event representing this change (which includes a description of the change, the agent that caused it and the action performed) is inserted in the output port of the target"s event channel. αstate = exec ∧ α : SetUp ∧ αnew = i a = playing, , , [α|qACT ], , −→ playing, , , qACT , , αtarget = open, , , , , sI , c −→ open, , , , , sI ∪ i , c (3) W here : (i )ini = a (c )out,ev = insert( a, α, sub(αtarget , i ) , cout,ev ) Let"s consider now the case of a close action. This action represents an attempt by the performer to force some interaction to finish, thus bypassing its current protocol rules (those concerning the finish function). The way to achieve this effect is to cause an update on the protocol so that the finish function returns true afterwards10 . Accordingly, we may specify this type of action as follows: Close ACT · upd : (→ Bool) → (→ Bool) inv. : (15) αtarget,state = open (16) αtarget,context = nil (17) αupd(αtarget,prot,finish)() where the inherited target field represents the interaction to be closed (which must be open and different to the topinteraction, according to invariants 15 and 16) and the new 9 The resulting type consists of the fields of the ACT record extended with an additional new field. 10 This strategy is also followed in the definition of leave and may also be used in the definition of other types of actions such as fire, permit, forbid, etc.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 893 update field represents a proper higher-order function to update the target"s protocol (inv. 17). The transition which models the execution of this action, specified by rule 4, defines two effects in the target interaction: its protocol is updated and the event representing this change is inserted in its output port. This event will actually trigger the closing process of the interaction as described in the next subsection. αstate = exec ∧ α : Close a = playing, , , [α|qACT ], , −→ playing, , , qACT , , αtarget = open, , , , , p, c −→ open, , , , , p , c (4) W here : (p )finish = αupd (pfinish ) (c )out,ev = insert( a, α, finish(αtarget ) , cout,ev )
The processing of events is encapsulated in the event channels of interactions. Channels, ranged over by meta-variable c, are defined by two input and output ports, according to the following definition: CH out : OutP, in : InP inv. : (18) ccontext ∈ cout,disp( , , finish(ccontext) ) (19) ccontext ∈ cout,disp( , , over(a) ) (20) ccontext,sub ⊆ cout,disp(closing(ccontext)) (21) apartsIn ⊆ cout,disp(leaving(a)) (22) ccontext ∈ cout,disp(closed(i)) (23) {ccontext, aplayer,context} ⊆ cout,disp(left(a)) OutP ev : Queue E, disp : E → Set I, int : Set I, ag : Set A InP ev : Queue E, stage : Enum {int, mem, obl}, ag : Set A The output port stores and processes the events originated within the scope of the channel"s interaction. Its first purpose is to dispatch the local events to the agents identified by the protocol"s monitoring function. Moreover, since these events may influence the results of the finishing, over and obligation functions of certain protocols, they will also be dispatched to the input ports of the interactions identified through a dispatching function - whose invariants will be explained later on. Thus, input ports serve as a coordination mechanism which activate the re-evaluation of the above functios whenever some event is received11 .
Accordingly, the processing of some event goes through four major stages: event dispatching, interaction state update, member state update and obligations update. The first one takes place in the output port of the interaction in which the event originated, whereas the other ones execute in separate control threads associated to the input ports of the interactions to which the event was dispatched.
The processing of some event stored in the output port is triggered when all its preceding events have been dispatched.
As a first step, the auxiliary int and ag fields are initialised 11 Alternatively, we may have assumed that interactions are fully aware of any change in the multi-agent community. In this scenario, interactions would trigger themselves without requiring any explicit notification. On the contrary, we adhere to the more realistic assumption of limited awareness. with the returned values of the dispatching and protocol"s monitoring functions, respectively (rule 5). Then, additional rules simply iterate over these collections until all agents and interactions have been notified (i.e., both sets are empty).
Last, the event is removed from the queue and the auxiliary fields are re-set to nil.
The dispatching function shall identify the set of interactions (possibly, empty) that may be affected by the event (which may include the channel"s interaction itself)12 . For instance, according to the finishing rule of university courses mentioned in the last section, the event representing the end of the admission stage, originated within the scope of the school interaction, will be dispatched to every course of the school"s degrees. Concerning the monitoring function, according to invariant 11 of protocols, if the event is generated as the result of an action performance, the agents to be notified will include the performer and addressees of that action. Thus, according to the monitoring rule of university courses, if a student of some degree joins a certain course and specifies a colleague as addressee of that action, the course"s teachers and itself will also be notified of the successful execution. ccontext,state s = open ∧ ccontext,prot,monitor s = mon cs = [e| ], d, nil, nil , −→ [e| ], , d(e), mon(e) , (5)
Input port activity is triggered when a new event is received. Irrespective of the kind of incoming event, the first processing action is to check whether the channel"s interaction must be finished. Thus, the dispatching of the finish event resulting from a close action (inv. 18) serves as a trigger of the closing procedure. If the interaction has not to be finished, the input port stage field is set to the member state update stage and the auxiliary ag field is initialised to the interaction members. Otherwise, we can consider two possible scenarios. In the first one, the interaction has no members and no sub-interactions. In this case, the interaction can be inmediately closed down. As rule 6 shows, the interaction is closed, removed from the context"s set of sub-interactions and a closed event is inserted in its output channel. According to invariant 22, this event will be later inserted to its input channel to allow for further treatment. cin,ev
, , , , {i} ∪ sI , , c −→ , , , , sI , , c i = , , ∅, , ∅, p, c1 −→ closed, , , , , , (6) W here : (c )out,ev = insert(closed(i), cout,ev ) In the second scenario, the interaction has some member or sub-interaction. In this case, clean-up is required prior to the disposal of the interaction (e.g. if the admission period ends and no student has matriculated for the course, teachers has to be finished before finishing the course itself). As rule 7 shows, the interaction is moved to the transient closing state and a corresponding event is inserted in the output port. According to invariant 20, the closing event will be dispatched to every sub-interaction in order to activate its closing procedure (guaranteed by invariant 8). Moreover, 12 This is essentially determined by the protocol rules of these interactions. The way in which the dispatching function is initialised and updated is out of the scope of this paper.
the stage and ag fields are properly initialised so that the process goes on in the next member state update stage. This stage will further initiate the leaving process of the members (according to invariant 9). cin,ev = ∅ ∧ cin,stage = int ∧ pfinish() ∧ (sA = ∅ ∨ sI = ∅) i = open, , sA, , sI , p, c −→ closing, , sA, , sI , p, c (7) W here : (c )out,ev = insert(closing(i), cout,ev ) (c )in,stage = mem (c )in,ag = sA Eventually, every member will leave the interaction and every sub-interaction will be closed. Corresponding events will be received by the interaction (according to invariants
hold.
This stage simply iterates over the members of the interaction to check whether they must be finished according to the protocol"s over function. When all members have been checked, the stage field will be set to the next obligation update stage and the auxiliary ag field will be initalised with the agents identified by the protocol"s obligation update function.
If some member has to end its participation in the interaction and it is not playing any role, it will be inmediately abandoned (successfully or unsuccessfully, according to the satisfaction of its purpose). The corresponding event will be forwarded to its interaction and to the interaction of its player agent to account for further changes (inv. 23).
Otherwise, the member enters the transient leaving state, thus preventing any action performance. Then, it waits for the completion of the leaving procedures of its played roles, triggered by proper dispatching of the leaving event (inv. 21).
In this stage, the obligations of agents (not necessaryly members of the interaction) towards the interaction are updated accordingly. When all the identified agents have been updated, the event is removed from the input queue and the stage field is set back to the interaction state update.
For instance, when a course interaction receives an event representing the assignment of some department member to its subject, an obligation to join the course as a teacher is created for that member. Moreover, the event representing this change is added to the output channel of the department interaction.
This paper has attempted to expose a possible semantic core underlying the wide spectrum of interaction types between autonomous, social and situated software components. In the realm of software architectures, this core has been formalised as an operational model of social connectors, intended to describe both the basic structure and dynamics of multi-agent interactions, from the largest (the agent society itself) down to the smallest ones (communicative actions). Thus, top-level interactions may represent the kind of agent-web pursued by large-scale initiatives such as the Agentcities/openNet one [25]. Large-scale interactions, modelling complex aggregates of agent interactions such as those represented by e-institutions or virtual organizations [2, 26], are also amenable to be conceptualised as particular kinds of first-level social interactions. The last levels of the interaction tree may represent small-scale multiagent interactions such as those represented by interaction protocols [11], dialogue games [16], or scenes [2]. Finally, bottom-level interactions may represent communicative actions. From this perspective, the member types of a CA include the speaker and possibly many listeners. The purpose of the speaker coincides with the illocutionary purpose of the CA [22], whereas the purpose of any listener is to declare that it (actually, the software component) successfully processed the meaning of the CA.
The analysis of social interactions put forward in this paper draws upon current proposals of the literature in several general respects, such as the institutional and organizational character of multi-agent systems [2, 26, 10, 7] and the normative perspective on multi-agent protocols [12, 23, 20].
These proposals as well as others focusing in relevant abstractions such as power relationships, contracts, trust and reputation mechanisms in organizational settings, etc., could be further exploited in order to characterize more accurately the organizational character of some multi-agent interactions. Similarly, the conceptualization of communicative actions as atomic interactions may similarly benefit from public semantics of communicative actions such as the one introduced in [3]. Last, the abstract model of protocols may be refined taking into account existing operational models of norms [12, 6]. These analyses shall result in new organizational and communicative abstractions obtained through a refinement and/or extension of the general model of social interactions. Thus, the proposed model is not intended to capture every organizational or communicative feature of multi-agent interactions, but to reveal their roots in basic interaction mechanisms. In turn, this would allow for the exploitation of common formalisms, particularly concerning protocols.
Unlike the development of individual agents, which has greatly benefited from the design of several agent programming languages [4], societal features of multi-agent systems are mostly implemented in terms of visual modelling [8, 18] and a fixed set of interaction abstractions. We argue that the current field of multi-agent system programming may greatly benefit from multi-agent programming languages that allow programmers to accommodate an open set of interaction mechanisms. The model of social interactions put forward in this paper is intended as the abstract machine of a language of this type. This abstract machine would be independent of particular agent architectures and languages (i.e. software components may be programmed in a BDI language such as Jason [5] or in a non-agent oriented language).
On top of the presented execution semantics, current and future work aims at the specification of the type system [19] which allows to program the abstract machine, the specification of the corresponding surface syntaxes (both textual and visual) and the design and implementation of a virtual machine over existing middleware technologies such as FIPA platforms or Web services. We also plan to study particular refinements and limitations to the proposed model, particularly with respect to the dispatching of events, semantics The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 895 of obligations, dynamic updates of protocols and rule formalisms. In this latter aspect, we plan to investigate the use of Answer Set Programming to specify the rules of protocols, attending to the role that incompleteness (rules may only specify either necessary or sufficient conditions, for instance), explicit negation (e.g. prohibitions) and defaults play in this domain.
The authors thank anonymous reviewers for their comments and suggestions. Research sponsored by the Spanish Ministry of Science and Education (MEC), project TIN200615455-C03-03.
[1] R. Allen and D. Garlan. A Formal Basis for Architectural Connection. ACM Transactions on Software Engineering and Methodology, 6(3):213-249,
June 1997. [2] J. L. Arcos, M. Esteva, P. Noriega, J. A. Rodr´ıguez, and C. Sierra. Engineering open environments with electronic institutions. Journal on Engineering Applications of Artificial Intelligence, 18(2):191-204,
[3] G. Boella, R. Damiano, J. Hulstijn, and L. W. N. van der Torre. Role-based semantics for agent communication: embedding of the "mental attitudes" and "social commitments" semantics. In AAMAS, pages 688-690, 2006. [4] R. H. Bordini, L. Braubach, M. Dastani, A. E. F.
Seghrouchni, J. J. G. Sanz, J. Leite, G. O"Hare,
A. Pokahr, and A. Ricci. A survey of programming languages and platforms for multi-agent systems.
Informatica, 30:33-44, 2006. [5] R. H. Bordini, J. F. H¨ubner, and R. Vieira. Jason and the golden fleece of agent-oriented programming. In R. H. Bordini, D. M., J. Dix, and A. El Fallah Seghrouchni, editors, Multi-Agent Programming: Languages, Platforms and Applications, chapter 1. Springer-Verlag, 2005. [6] O. Cliffe, M. D. Vos, and J. A. Padget. Specifying and analysing agent-based social institutions using answer set programming. In EUMAS, pages 476-477, 2005. [7] V. Dignum, J. V´azquez-Salceda, and F. Dignum.
Omni: Introducing social structure, norms and ontologies into agent organizations. In R. Bordini,
M. Dastani, J. Dix, and A. Seghrouchni, editors,
Programming Multi-Agent Systems Second International Workshop ProMAS 2004, volume 3346 of LNAI, pages 181-198. Springer, 2005. [8] M. Esteva, D. de la Cruz, and C. Sierra. ISLANDER: an electronic institutions editor. In M. Gini, T. Ishida,
C. Castelfranchi, and W. L. Johnson, editors,
Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS"02), pages 1045-1052. ACM Press,
July 2002. [9] M. Esteva, B. Rosell, J. A. Rodr´ıguez-Aguilar, and J. L. Arcos. AMELI: An agent-based middleware for electronic institutions. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, volume 1, pages 236-243,
[10] J. Ferber, O. Gutknecht, and F. Michel. From agents to organizations: An organizational view of multi-agent systems. In AOSE, pages 214-230, 2003. [11] Foundation for Intelligent Physical Agents. FIPA Interaction Protocol Library Specification. http://www.fipa.org/repository/ips.html, 2003. [12] A. Garc´ıa-Camino, J. A. Rodr´ıguez-Aguilar, C. Sierra, and W. Vasconcelos. Norm-oriented programming of electronic institutions. In AAMAS, pages 670-672,
[13] O. Gutknecht and J. Ferber. The MadKit agent platform architecture. Lecture Notes in Computer Science, 1887:48-55, 2001. [14] JADE. The JADE project home page. http://jade.cselt.it, 2005. [15] M. Luck, P. McBurney, O. Shehory, and S. Willmott.
Agent Technology: Computing as Interaction - A Roadmap for Agent-Based Computing. AgentLink III,
[16] P. McBurney and S. Parsons. A formal framework for inter-agent dialogues. In J. P. M¨uller, E. Andre,
S. Sen, and C. Frasson, editors, Proceedings of the Fifth International Conference on Autonomous Agents, pages 178-179, Montreal, Canada, May 2001.
ACM Press. [17] N. R. Mehta, N. Medvidovic, and S. Phadke. Towards a taxonomy of software connectors. In Proceedings of the 22nd International Conference on Software Engineering, pages 178-187. ACM Press, June 2000. [18] J. Pav´on and J. G´omez-Sanz. Agent oriented software engineering with ingenias. In V. Marik, J. Muller, and M. Pechoucek, editors, Proceedings of the 3rd International Central and Eastern European Conference on Multi-Agent Systems. Springer Verlag,
[19] B. C. Pierce. Types and Programming Languages. The MIT Press, Cambridge, MA, 2002. [20] J. Pitt, L. Kamara, M. Sergot, and A. Artikis. Voting in multi-agent systems. Feb. 27 2006. [21] G. Plotkin. A structural approach to operational semantics. Technical Report DAIMI FN-19, Aarhus University, Sept. 1981. [22] J. Searle. Speech Acts. Cambridge University Press,
[23] M. Sergot. A computational theory of normative positions. ACM Transactions on Computational Logic, 2(4):581-622, Oct. 2001. [24] M. P. Singh. Agent-based abstractions for software development. In F. Bergenti, M.-P. Gleizes, and F. Zambonelli, editors, Methodologies and Software Engineering for Agent Systems, chapter 1, pages 5-18.
Kluwer, 2004. [25] S. Willmot and al. Agentcities / opennet testbed. http://x-opennet.net, 2004. [26] F. Zambonelli, N. R. Jennings, and M. Wooldridge.
Developing multiagent systems: The Gaia methodology. ACM Transactions on Software Engineering and Methodology, 12(3):317-370, July

Normative systems, or social laws, have proved to be an attractive approach to coordination in multi-agent systems [13, 14, 10, 15, 1].
Although the various approaches to normative systems proposed in the literature differ on technical details, they all share the same basic intuition that a normative system is a set of constraints on the behaviour of agents in the system; by imposing these constraints, it is hoped that some desirable objective will emerge. The idea of using social laws to coordinate multi-agent systems was proposed by Shoham and Tennenholtz [13, 14]; their approach was extended by van der Hoek et al. to include the idea of specifying a desirable global objective for a social law as a logical formula, with the idea being that the normative system would be regarded as successful if, after implementing it (i.e., after eliminating all forbidden actions), the objective formula was guaranteed to be satisfied in the system [15]. However, this model did not take into account the preferences of individual agents, and hence neglected to account for possible strategic behaviour by agents when deciding whether to comply with the normative system or not. This model of normative systems was further extended by attributing to each agent a single goal in [16]. However, this model was still too impoverished to capture the kinds of decision making that take place when an agent decides whether or not to comply with a social law. In reality, strategic considerations come into play: an agent takes into account not just whether the normative system would be beneficial for itself, but also whether other agents will rationally choose to participate.
In this paper, we develop a model of normative systems in which agents are assumed to have multiple goals, of increasing priority.
We specify an agent"s goals as a hierarchy of formulae of Computation Tree Logic (CTL), a widely used logic for representing the properties of Kripke structures [8]: the intuition is that goals further up the hierarchy are preferred by the agent over those that appear further down the hierarchy. Using this scheme, we define a model of ordinal utility, which in turn allows us to interpret our Kripkebased normative systems as games, in which agents must determine whether to comply with the normative system or not. We thus provide a very natural bridge between logical structures and languages and the techniques and concepts of game theory, which have proved to be very powerful for analysing social contract-style scenarios such as normative systems [3, 4]. We then characterise the computational complexity of a number of decision problems associated with these Kripke-based normative system games; for example, we show that the complexity of checking whether there exists a normative system which has the property of being a Nash implementation is NP-complete.
We use Kripke structures as our basic semantic model for multiagent systems [8]. A Kripke structure is essentially a directed graph, with the vertex set S corresponding to possible states of the system being modelled, and the relation R ⊆ S × S capturing the 881 978-81-904262-7-5 (RPS) c 2007 IFAAMAS possible transitions of the system; intuitively, these transitions are caused by agents in the system performing actions, although we do not include such actions in our semantic model (see, e.g., [13, 2, 15] for related models which include actions as first class citizens).
We let S0 denote the set of possible initial states of the system.
Our model is intended to correspond to the well-known interleaved concurrency model from the reactive systems literature: thus an arc corresponds to the execution of an atomic action by one of the processes in the system, which we call agents.
It is important to note that, in contrast to such models as [2, 15], we are therefore here not modelling synchronous action. This assumption is not in fact essential for our analysis, but it greatly simplifies the presentation. However, we find it convenient to include within our model the agents that cause transitions. We therefore assume a set A of agents, and we label each transition in R with the agent that causes the transition via a function α : R → A.
Finally, we use a vocabulary Φ = {p, q, . . .} of Boolean variables to express the properties of individual states S: we use a function V : S → 2Φ to label each state with the Boolean variables true (or satisfied) in that state.
Collecting these components together, an agent-labelled Kripke structure (over Φ) is a 6-tuple: K = S, S0 , R, A, α, V , where: • S is a finite, non-empty set of states, • S0 ⊆ S (S0 = ∅) is the set of initial states; • R ⊆ S × S is a total binary relation on S, which we refer to as the transition relation1 ; • A = {1, . . . , n} is a set of agents; • α : R → A labels each transition in R with an agent; and • V : S → 2Φ labels each state with the set of propositional variables true in that state.
In the interests of brevity, we shall hereafter refer to an agentlabelled Kripke structure simply as a Kripke structure. A path over a transition relation R is an infinite sequence of states π = s0, s1, . . . which must satisfy the property that ∀u ∈ N: (su , su+1) ∈ R. If u ∈ N, then we denote by π[u] the component indexed by u in π (thus π[0] denotes the first element, π[1] the second, and so on). A path π such that π[0] = s is an s-path. Let ΠR(s) denote the set of s-paths over R; since it will usually be clear from context, we often omit reference to R, and simply write Π(s). We will sometimes refer to and think of an s-path as a possible computation, or system evolution, from s.
EXAMPLE 1. Our running example is of a system with a single non-sharable resource, which is desired by two agents. Consider the Kripke structure depicted in Figure 1. We have two states, s and t, and two corresponding Boolean variables p1 and p2, which are 1 In the branching time temporal logic literature, a relation R ⊆ S × S is said to be total iff ∀s ∃s : (s, s ) ∈ R. Note that the term total relation is sometimes used to refer to relations R ⊆ S × S such that for every pair of elements s, s ∈ S we have either (s, s ) ∈ R or (s , s) ∈ R; we are not using the term in this way here. It is also worth noting that for some domains, other constraints may be more appropriate than simple totality. For example, one might consider the agent totality requirement, that in every state, every agent has at least one possible transition available: ∀s∀i ∈ A∃s : (s, s ) ∈ R and α(s, s ) = i. 2p t p 2 2 1 s 1 1 Figure 1: The resource control running example. mutually exclusive. Think of pi as meaning agent i has currently control over the resource. Each agent has two possible actions, when in possession of the resource: either give it away, or keep it.
Obviously there are infinitely many different s-paths and t-paths.
Let us say that our set of initial states S0 equals {s, t}, i.e., we don"t make any assumptions about who initially has control over the resource.
We now define Computation Tree Logic (CTL), a branching time temporal logic intended for representing the properties of Kripke structures [8]. Note that since CTL is well known and widely documented in the literature, our presentation, though complete, will be somewhat terse. We will use CTL to express agents" goals.
The syntax of CTL is defined by the following grammar: ϕ ::= | p | ¬ϕ | ϕ ∨ ϕ | E fϕ | E(ϕ U ϕ) | A fϕ | A(ϕ U ϕ) where p ∈ Φ. We denote the set of CTL formula over Φ by LΦ; since Φ is understood, we usually omit reference to it.
The semantics of CTL are given with respect to the satisfaction relation |=, which holds between pairs of the form K, s, (where K is a Kripke structure and s is a state in K), and formulae of the language. The satisfaction relation is defined as follows: K, s |= ; K, s |= p iff p ∈ V (s) (where p ∈ Φ); K, s |= ¬ϕ iff not K, s |= ϕ; K, s |= ϕ ∨ ψ iff K, s |= ϕ or K, s |= ψ; K, s |= A fϕ iff ∀π ∈ Π(s) : K, π[1] |= ϕ; K, s |= E fϕ iff ∃π ∈ Π(s) : K, π[1] |= ϕ; K, s |= A(ϕ U ψ) iff ∀π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ K, s |= E(ϕ U ψ) iff ∃π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ The remaining classical logic connectives (∧, →, ↔) are assumed to be defined as abbreviations in terms of ¬, ∨, in the conventional manner. The remaining CTL temporal operators are defined: A♦ϕ ≡ A( U ϕ) E♦ϕ ≡ E( U ϕ) A ϕ ≡ ¬E♦¬ϕ E ϕ ≡ ¬A♦¬ϕ We say ϕ is satisfiable if K, s |= ϕ for some Kripke structure K and state s in K; ϕ is valid if K, s |= ϕ for all Kripke structures K and states s in K. The problem of checking whether K, s |= ϕ for given K, s, ϕ (model checking) can be done in deterministic polynomial time, while checking whether a given ϕ is satisfiable or whether ϕ is valid is EXPTIME-complete [8]. We write K |= ϕ if K, s0 |= ϕ for all s0 ∈ S0 , and |= ϕ if K |= ϕ for all K.
For our purposes, a normative system is simply a set of constraints on the behaviour of agents in a system [1]. More precisely, a normative system defines, for every possible system transition, whether or not that transition is considered to be legal or not. Different normative systems may differ on whether or not a transition is legal. Formally, a normative system η (w.r.t. a Kripke structure K = S, S0 , R, A, α, V ) is simply a subset of R, such that R \ η is a total relation. The requirement that R\η is total is a reasonableness constraint: it prevents normative systems which lead to states with no successor. Let N (R) = {η : (η ⊆ R) & (R \ η is total)} be the set of normative systems over R. The intended interpretation of a normative system η is that (s, s ) ∈ η means transition (s, s ) is forbidden in the context of η; hence R \ η denotes the legal transitions of η. Since it is assumed η is reasonable, we are guaranteed that a legal outward transition exists for every state. We denote the empty normative system by η∅, so η∅ = ∅. Note that the empty normative system η∅ is reasonable with respect to any transition relation R.
The effect of implementing a normative system on a Kripke structure is to eliminate from it all transitions that are forbidden according to this normative system (see [15, 1]). If K is a Kripke structure, and η is a normative system over K, then K † η denotes the Kripke structure obtained from K by deleting transitions forbidden in η. Formally, if K = S, S0 , R, A, α, V , and η ∈ N (R), then let K†η = K be the Kripke structure K = S , S0 , R , A , α , V where: • S = S , S0 = S0 , A = A , and V = V ; • R = R \ η; and • α is the restriction of α to R : α (s, s ) = j α(s, s ) if (s, s ) ∈ R undefined otherwise.
Notice that for all K, we have K † η∅ = K.
EXAMPLE 1. (continued) When thinking in terms of fairness, it seems natural to consider normative systems η that contain (s, s) or (t, t). A normative system with (s, t) would not be fair, in the sense that A♦A ¬p1 ∨ A♦A ¬p2 holds: in all paths, from some moment on, one agent will have control forever. Let us, for later reference, fix η1 = {(s, s)}, η2 = {(t, t)}, and η3 = {(s, s), (t, t)}.
Later, we will address the issue of whether or not agents should rationally choose to comply with a particular normative system. In this context, it is useful to define operators on normative systems which correspond to groups of agents defecting from the normative system. Formally, let K = S, S0 ,R, A,α, V be a Kripke structure, let C ⊆ A be a set of agents over K, and let η be a normative system over K. Then: • η C denotes the normative system that is the same as η except that it only contains the arcs of η that correspond to the actions of agents in C. We call η C the restriction of η to C, and it is defined as: η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only the agents in C choose to comply with the normative system. • η C denotes the normative system that is the same as η except that it only contains the arcs of η that do not correspond to actions of agents in C. We call η C the exclusion of C from η, and it is defined as: η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only the agents in C choose not to comply with the normative system (i.e., the only ones who comply are those in A \ C).
Note that we have η C = η (A\C) and η C = η (A\C).
EXAMPLE 1. (Continued) We have η1 {1} = η1 = {(s, s)}, while η1 {1} = η∅ = η1 {2}. Similarly, we have η3 {1} = {(s, s)} and η3 {1} = {(t, t)}.
Next, we want to be able to capture the goals that agents have, as these will drive an agent"s strategic considerations - particularly, as we will see, considerations about whether or not to comply with a normative system. We will model an agent"s goals as a prioritised list of CTL formulae, representing increasingly desired properties that the agent wishes to hold. The intended interpretation of such a goal hierarchy γi for agent i ∈ A is that the further up the hierarchy a goal is, the more it is desired by i. Note that we assume that if an agent can achieve a goal at a particular level in its goal hierarchy, then it is unconcerned about goals lower down the hierarchy. Formally, a goal hierarchy, γ, (over a Kripke structure K) is a finite, non-empty sequence of CTL formulae γ = (ϕ0, ϕ1, . . . , ϕk ) in which, by convention, ϕ0 = . We use a natural number indexing notation to extract the elements of a goal hierarchy, so if γ = (ϕ0, ϕ1, . . . , ϕk ) then γ[0] = ϕ0, γ[1] = ϕ1, and so on. We denote the largest index of any element in γ by |γ|.
A particular Kripke structure K is said to satisfy a goal at index x in goal hierarchy γ if K |= γ[x], i.e., if γ[x] is satisfied in all initial states S0 of K. An obvious potential property of goal hierarchies is monotonicity: where goals at higher levels in the hierarchy logically imply those at lower levels in the hierarchy. Formally, a goal hierarchy γ is monotonic if for all x ∈ {1, . . . , |γ|} ⊆ N, we have |= γ[x] → γ[x − 1]. The simplest type of monotonic goal hierarchy is where γ[x + 1] = γ[x] ∧ ψx+1 for some ψx+1, so at each successive level of the hierarchy, we add new constraints to the goal of the previous level. Although this is a natural property of many goal hierarchies, it is not a property we demand of all goal hierarchies.
EXAMPLE 1. (continued) Suppose the agents have similar, but opposing goals: each agent i wants to keep the source as often and long as possible for himself. Define each agent"s goal hierarchy as: γi = ( ϕi
ϕi
ϕi
ϕi
ϕi
The most desired goal of agent i is to, in every computation, always have the resource, pi (this is expressed in ϕi 8). Thanks to our reasonableness constraint, this goal implies ϕi
matter how the computation paths evolve, it will always be that all The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 883 continuations will hit a point in which pi , and, moreover, there is a continuation in which pi always holds. Goal ϕi
constraint implied by it. Note that A♦pi says that every computation eventually reaches a pi state. This may mean that after pi has happened, it will never happen again. ϕi
no matter where you are, there should be a future pi state. The goal ϕi
in some computation, eventually. ϕi
there is always a continuation that eventually gives pi . Goal ϕi 3 says that pi should be true on some branch, from some moment on.
It implies ϕi
everywhere during it, it is possible to choose a continuation that eventually satisfies pi . This implies ϕi 1, which says that pi should at least not be impossible. If we even drop that demand, we have the trivial goal ϕi
We remark that it may seem more natural to express a fairness constraint ϕi
formula. It is in fact a formula in CTL ∗ [9], and in this logic, the two expressions would be equivalent. However, our basic complexity results in the next sections would not hold for the richer language CTL ∗2 , and the price to pay for this is that we have to formulate our desired goals in a somewhat more cumbersome manner than we might ideally like. Of course, our basic framework does not demand that goals are expressed in CTL; they could equally well be expressed in CTL ∗ or indeed ATL [2] (as in [15]). We comment on the implications of alternative goal representations at the conclusion of the next section.
A multi-agent system collects together a Kripke structure (representing the basic properties of a system under consideration: its state space, and the possible state transitions that may occur in it), together with a goal hierarchy, one for each agent, representing the aspirations of the agents in the system. Formally, a multi-agent system, M , is an (n + 1)-tuple: M = K, γ1, . . . , γn where K is a Kripke structure, and for each agent i in K, γi is a goal hierarchy over K.
We can now define the utility of a Kripke structure for an agent.
The idea is that the utility of a Kripke structure is the highest index of any goal that is guaranteed for that agent in the Kripke structure.
We make this precise in the function ui (·): ui (K) = max{j : 0 ≤ j ≤ |γi | & K |= γi [j ]} Note that using these definitions of goals and utility, it never makes sense to have a goal ϕ at index n if there is a logically weaker goal ψ at index n + k in the hierarchy: by definition of utility, it could never be n for any structure K.
EXAMPLE 1. (continued) Let M = K, γ1, γ2 be the multiagent system of Figure 1, with γ1 and γ2 as defined earlier in this example. Recall that we have defined S0 as {s, t}. Then, u1(K) = u2(K) = 4: goal ϕ4 is true in S0 , but ϕ5 is not. To see that ϕ2
is always the case that there is a transition to t, in which p2 is true.
Notice that since for any goal hierarchy γi we have γ[0] = , then for all Kripke structures, ui (K) is well defined, with ui (K) ≥ 2 CTL ∗ model checking is PSPACE-complete, and hence much worse (under standard complexity theoretic assumptions) than model checking CTL [8]. η δ1(K, η) δ2(K, η) η∅ 0 0 η1 0 3 η2 3 0 η3 2 2 C D C (2, 2) (0, 3) D (3, 0) (0, 0) Figure 2: Benefits of implementing a normative system η (left) and pay-offs for the game ΣM .
given agent, the relative utility of different Kripke structures, but utility values are not on some standard system-wide scale. The fact that ui (K1) > ui (K2) certainly means that i strictly prefers K1 over K2, but the fact that ui (K) > uj (K) does not mean that i values K more highly than j . Thus, it does not make sense to compare utility values between agents, and so for example, some system wide measures of utility, (notably those measures that aggregate individual utilities, such as social welfare), do not make sense when applied in this setting. However, as we shall see shortly, other measures - such as Pareto efficiency - can be usefully applied.
There are other representations for goals, which would allow us to define cardinal utilities. The simplest would be to specify goals γ for an agent as a finite, non-empty, one-to-one relation: γ ⊆ L×R.
We assume that the x values in pairs (ϕ, x) ∈ γ are specified so that x for agent i means the same as x for agent j , and so we have cardinal utility. We then define the utility for i of a Kripke structure K asui (K) = max{x : (ϕ, x) ∈ γi & K |= ϕ}. The results of this paper in fact hold irrespective of which of these representations we actually choose; we fix upon the goal hierarchy approach in the interests of simplicity.
Our next step is to show how, in much the same way, we can lift the utility function from Kripke structures to normative systems.
Suppose we are given a multi-agent system M = K, γ1, . . . , γn and an associated normative system η over K. Let for agent i, δi (K, K ) be the difference in his utility when moving from K to K : δi (K, K ) = ui (K )− ui (K). Then the utility of η to agent i wrt K is δi (K, K † η). We will sometimes abuse notation and just write δi (K, η) for this, and refer to it as the benefit for agent i of implementing η in K. Note that this benefit can be negative.
Summarising, the utility of a normative system to an agent is the difference between the utility of the Kripke structure in which the normative system was implemented and the original Kripke structure. If this value is greater than 0, then the agent would be better off if the normative system were imposed, while if it is less than
original system. We say η is individually rational for i wrt K if δi (K, η) > 0, and individually rational simpliciter if η is individually rational for every agent.
A social system now is a pair Σ = M , η where M is a multi-agent system, and η is a normative system over M .
EXAMPLE 1. The table at the left hand in Figure 2 displays the utilities δi (K, η) of implementing η in the Kripke structure of our running example, for the normative systems η = η∅, η1, η2 and η3, introduced before. Recall that u1(K) = u2(K) = 4.
Keeping in mind that a norm η restricts the possible transitions of the model under consideration, we make the following observation, borrowing from [15]. Some classes of goals are monotonic or anti-monotonic with respect to adding additional constraints to a system. Let us therefore define two fragments of the language of CTL: the universal language Lu with typical element μ, and the existential fragment Le with typical element ε. μ ::= | p | ¬p | μ ∨ μ | A fμ | A μ | A(μ U μ) ε ::= | p | ¬p | ε ∨ ε | E fε | E♦ε | E(ε U ε) Let us say, for two Kripke structures K1 = S, S0 , R1, A, α, V and K2 = S, S0 , R2, A, α, V that K1 is a subsystem of K2 and K2 is a supersystem of K1, written K1 K2 iff R1 ⊆ R2. Note that typically K † η K. Then we have (cf. [15]).
THEOREM 1. Suppose K1 K2, and s ∈ S. Then ∀ε ∈ Le : K1, s |= ε ⇒ K2, s |= ε ∀μ ∈ Lu : K2, s |= μ ⇒ K1, s |= μ This has the following effect on imposing a new norm: COROLLARY 1. Let K be a structure, and η a normative system. Let γi denote a goal hierarchy for agent i.
, (i.e., γi [n] is a universal formula). Then, for any normative system η, δi (K, η) ≥ 0.
existential formula ε. Then, δi (K † η, K) ≥ 0.
Corollary 1"s first item says that an agent whose current maximal goal in a system is a universal formula, need never fear the imposition of a new norm η. The reason is that his current goal will at least remain true (in fact a goal higher up in the hierarchy may become true). It follows from this that an agent with only universal goals can only gain from the imposition of normative systems η.
The opposite is true for existential goals, according to the second item of the corollary: it can never be bad for an agent to undo a norm η. Hence, an agent with only existential goals might well fear any norm η.
However, these observations implicitly assume that all agents in the system will comply with the norm. Whether they will in fact do so, of course, is a strategic decision: it partly depends on what the agent thinks that other agents will do. This motivates us to consider normative system games.
We now have a principled way of talking about the utility of normative systems for agents, and so we can start to apply the technical apparatus of game theory to analyse them.
Suppose we have a multi-agent system M = K, γ1, . . . , γn and a normative system η over K. It is proposed to the agents in M that η should be imposed on K, (typically to achieve some coordination objective). Our agent - let"s say agent i - is then faced with a choice: should it comply with the strictures of the normative system, or not? Note that this reasoning takes place before the agent is in the system - it is a design time consideration.
We can understand the reasoning here as a game, as follows. A game in strategic normal form (cf. [11, p.11]) is a structure: G = AG, S1, . . . , Sn , U1, . . . , Un where: • AG = {1, . . . , n} is a set of agents - the players of the game; • Si is the set of strategies for each agent i ∈ AG (a strategy for an agent i is nothing else than a choice between alternative actions); and • Ui : (S1 × · · · × Sn ) → R is the utility function for agent i ∈ AG, which assigns a utility to every combination of strategy choices for the agents.
Now, suppose we are given a social system Σ = M , η where M = K, γ1, . . . , γn . Then we can associate a game - the normative system game - GΣ with Σ, as follows. The agents AG in GΣ are as in Σ. Each agent i has just two strategies available to it: • C - comply (cooperate) with the normative system; and • D - do not comply with (defect from) the normative system.
If S is a tuple of strategies, one for each agent, and x ∈ {C, D}, then we denote by AGx S the subset of agents that play strategy x in S. Hence, for a social system Σ = M , η , the normative system η AGC S only implements the restrictions for those agents that choose to cooperate in GΣ. Note that this is the same as η AGD S : the normative system that excludes all the restrictions of agents that play D in GΣ. We then define the utility functions Ui for each i ∈ AG as: Ui (S) = δi (K, η AGC S ).
So, for example, if SD is a collection of strategies in which every agent defects (i.e., does not comply with the norm), then Ui (SD ) = δi (K, (η AGD SD )) = ui (K † η∅) − ui (K) = 0.
In the same way, if SC is a collection of strategies in which every agent cooperates (i.e., complies with the norm), then Ui (SC ) = δi (K, (η AGD SC )) = ui (K † (η ∅)) = ui (K † η).
We can now start to investigate some properties of normative system games.
EXAMPLE 1. (continued) For our example system, we have displayed the different U values for our multi agent system with the norm η3, i.e., {(s, s), (t, t)} as the second table of Figure 2. For instance, the pair (0, 3) in the matrix under the entry S = C, D is obtained as follows. U1( C, D ) = δ1(K, η3 AGC C,D ) = u1(K † η3 AGC C,D ) − u1(K). The first term of this is the utility of 1 in the system K where we implement η3 for the cooperating agent, i.e., 1, only. This means that the transitions are R \ {(s, s)}. In this system, still ϕ1
goal for agent 1. This is the same utility for 1 as in K, and hence, δ1(K, η3 AGC C,D ) = 0. Agent 2 of course benefits if agent 1 complies with η3 while 2 does not. His utility would be 3, since η3 AGC C,D is in fact η1.
A normative system is individually rational if every agent would fare better if the normative system were imposed than otherwise.
This is a necessary, although not sufficient condition on a norm to expect that everybody respects it. Note that η3 of our example is individually rational for both 1 and 2, although this is not a stable situation: given that the other plays C, i is better of by playing D. We can easily characterise individually rationality with respect to the corresponding game in strategic form, as follows. Let Σ = M , η be a social system. Then the following are equivalent: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 885 f(xk) ... s0 s1 s2 s3 s4 s(2k−1) s2k t(x1) f(x1) t(x2) f(x2) t(xk) Figure 3: The Kripke structure produced in the reduction of Theorem 2; all transitions are associated with agent 1, the only initial state is s0.
The decision problem associated with individually rational normative systems is as follows: INDIVIDUALLY RATIONAL NORMATIVE SYSTEM (IRNS): Given: Multi-agent system M .
Question: Does there exist an individually rational normative system for M ?
THEOREM 2. IRNS is NP-complete, even in one-agent systems.
PROOF. For membership of NP, guess a normative system η, and verify that it is individually rational. Since η ⊆ R, we will be able to guess it in nondeterministic polynomial time. To verify that it is individually rational, we check that for all i, we have ui (K † η) > ui (K); computing K † η is just set subtraction, so can be done in polynomial time, while determining the value of ui (K) for any K can be done with a polynomial number of model checking calls, each of which requires only time polynomial in the K and γ.
Hence verifying that ui (K † η) > ui (K) requires only polynomial time.
For NP-hardness, we reduce SAT [12, p.77]. Given a SAT instance ϕ over Boolean variables x1, . . . , xk , we produce an instance of IRNS as follows. First, we define a single agent A = {1}. For each Boolean variable xi in the SAT instance, we create two Boolean variables t(xi ) and f (xi ) in the IRNS instance. We then create a Kripke structure Kϕ with 2k + 1 states, as shown in Figure 3: arcs in this graph correspond to transitions in Kϕ. Let ϕ∗ be the result of systematically substituting for every Boolean variable xi in ϕ the CTL expression (E ft(xi )). Next, consider the following formulae: k^ i=1 E f(t(xi ) ∨ f (xi )) (1) k^ i=1 ¬((E ft(xi )) ∧ (E ff (xi ))) (2) We then define the goal hierarchy for all agent 1 as follows: γ1[0] = γ1[1] = (1) ∧ (2) ∧ ϕ∗ We claim there is an individually rational normative system for the instance so constructed iff ϕ is satisfiable. First, notice that any individually rational normative system must force γ1[1] to be true, since in the original system, we do not have γ1[1].
For the ⇒ direction, if there is an individually rational normative system η, then we construct a satisfying assignment for ϕ by considering the arcs that are forbidden by η: formula (1) ensures that we must forbid an arc to either a t(xi ) or a f (xi ) state for all variables xi , but (2) ensures that we cannot forbid arcs to both. So, if we forbid an arc to a t(xi ) state then in the corresponding valuation for ϕ we make xi false, while if we forbid an arc to a f (xi ) state then we make xi true. The fact that ϕ∗ is part of the goal ensures that the normative system is indeed a valuation for ϕ.
For ⇐, note that for any satisfying valuation for ϕ we can construct an individually rational normative system η, as follows: if the valuation makes xi true, we forbid the arc to the f (xi ) state, while if the valuation makes xi false, we forbid the arc to the t(xi ) state. The resulting normative system ensures γ1[1], and is thus individually rational.
Notice that the Kripke structure constructed in the reduction contains just a single agent, and so the Theorem is proven.
Pareto efficiency is a basic measure of how good a particular outcome is for a group of agents [11, p.7]. Intuitively, an outcome is Pareto efficient if there is no other outcome that makes every agent better off. In our framework, suppose we are given a social system Σ = M , η , and asked whether η is Pareto efficient. This amounts to asking whether or not there is some other normative system η such that every agent would be better off under η than with η. If η makes every agent better off than η, then we say η Pareto dominates η. The decision problem is as follows: PARETO EFFICIENT NORMATIVE SYSTEM (PENS): Given: Multi-agent system M and normative system η over M .
Question: Is η Pareto efficient for M ?
THEOREM 3. PENS is co-NP-complete, even for one-agent systems.
PROOF. Let M and η be as in the Theorem. We show that the complement problem to PENS, which we refer to as PARETO DOMINATED, is NP-complete. In this problem, we are given M and η, and we are asked whether η is Pareto dominated, i.e., whether or not there exists some η over M such that η makes every agent better off than η. For membership of NP, simply guess a normative system η , and verify that for all i ∈ A, we have ui (K † η ) > ui (K † η) - verifying requires a polynomial number of model checking problems, each of which takes polynomial time. Since η ⊆ R, the normative system can be guessed in non-deterministic polynomial time. For NP-hardness, we reduce IRNS, which we know to be NPcomplete from Theorem 2. Given an instance M of IRNS, we let M in the instance of PARETO DOMINATED be as in the IRNS instance, and define the normative system for PARETO DOMINATED to be η∅, the empty normative system. Now, it is straightforward that there exists a normative system η which Pareto dominates η∅ in M iff there exist an individually rational normative system in M . Since the complement problem is NP-complete, it follows that PENS is co-NP-complete.
η0 η1 η2 η3 η4 η5 η6 η7 η8 u1(K † η) 4 4 7 6 5 0 0 8 0 u2(K † η) 4 7 4 6 0 5 8 0 0 Table 1: Utilities for all possible norms in our example How about Pareto efficient norms for our toy example? Settling this question amounts to finding the dominant normative systems among η0 = η∅, η1, η2, η3 defined before, and η4 = {(s, t)}, η5 = {(t, s)}, η6 = {(s, s), (t, s)}, η7 = {(t, t), (s, t)} and η8 = {(s, t), (t, s)}. The utilities for each system are given in Table 1.
From this, we infer that the Pareto efficient norms are η1, η2, η3, η6 and η7. Note that η8 prohibits the resource to be passed from one agent to another, and this is not good for any agent (since we have chosen S0 = {s, t}, no agent can be sure to ever get the resource, i.e., goal ϕi
The most famous solution concept in game theory is of course Nash equilibrium [11, p.14]. A collection of strategies, one for each agent, is said to form a Nash equilibrium if no agent can benefit by doing anything other than playing its strategy, under the assumption that the other agents play theirs. Nash equilibria are important because they provide stable solutions to the problem of what strategy an agent should play. Note that in our toy example, although η3 is individually rational for each agent, it is not a Nash equilibrium, since given this norm, it would be beneficial for agent 1 to deviate (and likewise for 2). In our framework, we say a social system Σ = M , η (where η = η∅) is a Nash implementation if SC (i.e., everyone complying with the normative system) forms a Nash equilibrium in the game GΣ. The intuition is that if Σ is a Nash implementation, then complying with the normative system is a reasonable solution for all concerned: there can be no benefit to deviating from it, indeed, there is a positive incentive for all to comply. If Σ is not a Nash implementation, then the normative system is unlikely to succeed, since compliance is not rational for some agents. (Our choice of terminology is deliberately chosen to reflect the way the term Nash implementation is used in implementation theory, or mechanism design [11, p.185], where a game designer seeks to achieve some outcomes by designing the rules of the game such that these outcomes are equilibria.) NASH IMPLEMENTATION (NI) : Given: Multi-agent system M .
Question: Does there exist a non-empty normative system η over M such that M , η forms a Nash implementation?
Verifying that a particular social system forms a Nash implementation can be done in polynomial time - it amounts to checking: ∀i ∈ A : ui (K † η) ≥ ui (K † (η {i})).
This, clearly requires only a polynomial number of model checking calls, each of which requires only polynomial time.
THEOREM 4. The NI problem is NP-complete, even for twoagent systems.
PROOF. For membership of NP, simply guess a normative system η and check that it forms a Nash implementation; since η ⊆ R, guessing can be done in non-deterministic polynomial time, and as s(2k+1) 1 1 1 1 1 1 11
11 2 2 2 2 2 2 2 2 2 2 2 t(x1) f(x1) t(x2) f(x2) t(xk) f(xk) 2 2 t(x1) f(x1) t(x2) f(x2) t(xk) f(xk) ...... s0 Figure 4: Reduction for Theorem 4. we argued above, verifying that it forms a Nash implementation can be done in polynomial time.
For NP-hardness, we reduce SAT. Suppose we are given a SAT instance ϕ over Boolean variables x1, . . . , xk . Then we construct an instance of NI as follows. We create two agents, A = {1, 2}. For each Boolean variable xi we create two Boolean variables, t(xi ) and f (xi ), and we then define a Kripke structure as shown in Figure 4, with s0 being the only initial state; the arc labelling in Figure 4 gives the α function, and each state is labelled with the propositions that are true in that state. For each Boolean variable xi , we define the formulae xi and x⊥ i as follows: xi = E f(t(xi ) ∧ E f((E f(t(xi ))) ∧ A f(¬f (xi )))) x⊥ i = E f(f (xi ) ∧ E f((E f(f (xi ))) ∧ A f(¬t(xi )))) Let ϕ∗ be the formula obtained from ϕ by systematically substituting xi for xi . Each agent has three goals: γi [0] = for both i ∈ {1, 2}, while γ1[1] = k^ i=1 ((E f(t(xi ))) ∧ (E f(f (xi )))) γ2[1] = E fE f k^ i=1 ((E f(t(xi ))) ∧ (E f(f (xi )))) and finally, for both agents, γi [2] being the conjunction of the following formulae: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 887 k^ i=1 (xi ∨ x⊥ i ) (3) k^ i=1 ¬(xi ∧ x⊥ i ) (4) k^ i=1 ¬(E f(t(xi )) ∧ E f(f (xi ))) (5) ϕ∗ (6) We denote the multi-agent system so constructed by Mϕ. Now, we prove that the SAT instance ϕ is satisfiable iff Mϕ has a Nash implementation normative system: For the ⇒ direction, suppose ϕ is satisfiable, and let X be a satisfying valuation, i.e., a set of Boolean variables making ϕ true.
We can extract from X a Nash implementation normative system η as follows: if xi ∈ X , then η includes the arc from s0 to the state in which f (xi ) is true, and also includes the arc from s(2k + 1) to the state in which f (xi ) is true; if xi ∈ X , then η includes the arc from s0 to the state in which t(xi ) is true, and also includes the arc from s(2k + 1) to the state in which t(xi ) is true. No other arcs, apart from those so defined, as included in η. Notice that η is individually rational for both agents: if they both comply with the normative system, then they will have their γi [2] goals achieved, which they do not in the basic system. To see that η forms a Nash implementation, observe that if either agent defects from η, then neither will have their γi [2] goals achieved: agent 1 strictly prefers (C, C) over (D, C), and agent 2 strictly prefers (C, C) over (C, D).
For the ⇐ direction, suppose there exists a Nash implementation normative system η, in which case η = ∅. Then ϕ is satisfiable; for suppose not. Then the goals γi [2] are not achievable by any normative system, (by construction). Now, since η must forbid at least one transition, then at least one agent would fail to have its γi [1] goal achieved if it complied, so at least one would do better by defecting, i.e., not complying with η. But this contradicts the assumption that η is a Nash implementation, i.e., that (C, C) forms a Nash equilibrium.
This result is perhaps of some technical interest beyond the specific concerns of the present paper, since it is related to two problems that are of wider interest: the complexity of mechanism design [5], and the complexity of computing Nash equilibria [6, 7]
It is interesting to consider what happens to the complexity of the problems we consider above if we allow richer languages for goals: in particular, CTL ∗ [9]. The main difference is that determining ui (K) in a given multi-agent system M when such a goal language is used involves solving a PSPACE-complete problem (since model checking for CTL ∗ is PSPACE-complete [8]). In fact, it seems that for each of the three problems we consider above, the corresponding problem under the assumption of a CTL ∗ representation for goals is also PSPACE-complete. It cannot be any easier, since determining the utility of a particular Kripke structure involves solving a PSPACE-complete problem. To see membership in PSPACE we can exploit the fact that PSPACE = NPSPACE [12, p.150], and so we can guess the desired normative system, applying a PSPACE verification procedure to check that it has the desired properties.
Social norms are supposed to restrict our behaviour. Of course, such a restriction does not have to be bad: the fact that an agent"s behaviour is restricted may seem a limitation, but there may be benefits if he can assume that others will also constrain their behaviour.
The question then, for an agent is, how to be sure that others will comply with a norm. And, for a system designer, how to be sure that the system will behave socially, that is, according to its norm.
Game theory is a very natural tool to analyse and answer these questions, which involve strategic considerations, and we have proposed a way to translate key questions concerning logic-based normative systems to game theoretical questions. We have proposed a logical framework to reason about such scenarios, and we have given some computational costs for settling some of the main questions about them. Of course, our approach is in many senses open for extension or enrichment. An obvious issue is to consider is the complexity of the questions we give for more practical representations of models (cf. [1]), and to consider other classes of allowable goals.

Multi-issue negotiation protocols represent an important field of study since negotiation problems in the real world are often complex ones involving multiple issues. To date, most of previous work in this area ([2, 3, 19, 13]) dealt almost exclusively with simple negotiations involving independent issues. However, real-world negotiation problems involve complex dependencies between multiple issues. When one wants to buy a car, for example, the value of a given car is highly dependent on its price, consumption, comfort and so on. The addition of such interdependencies greatly complicates the agents utility functions and classical utility functions, such as the weighted sum, are not sufficient to model this kind of preferences. In [10, 9, 17, 14, 20], the authors consider inter-dependencies between issues, most often defined with boolean values, except for [9], while we can deal with continuous and discrete dependent issues thanks to the modelling power of the Choquet integral. In [17], the authors deal with bilateral negotiation while we are interested in a multilateral negotiation setting. Klein et al. [10] present an approach similar to ours, using a mediator too and information about the strength of the approval or rejection that an agent makes during the negotiation. In our protocol, we use more precise information to improve the proposals thanks to the multi-criteria methodology and tools used to model the preferences of our agents. Lin, in [14, 20], also presents a mediation service but using an evolutionary algorithm to reach optimal solutions and as explained in [4], players in the evolutionary models need to repeatedly interact with each other until the stable state is reached.
As the population size increases, the time it takes for the population to stabilize also increases, resulting in excessive computation, communication, and time overheads that can become prohibitive, and for one-to-many and many-to-many negotiations, the overheads become higher as the number of players increases. In [9], the authors consider a non-linear utility function by using constraints on the domain of the issues and a mediation service to find a combination of bids maximizing the social welfare. Our preference model, a nonlinear utility function too, is more complex than [9] one since the Choquet integral takes into account the interactions and the importance of each decision criteria/issue, not only the dependencies between the values of the issues, to determine the utility. We also use an iterative protocol enabling us to find a solution even when no bid combination is possible.
In this paper, we propose a negotiation protocol suited for multiple agents with complex preferences and taking into account, at the same time, multiple interdependent issues and recommendations made by the agents to improve a proposal.
Moreover, the preferences of our agents are modelled using a multi-criteria methodology and tools enabling us to take into account information about the improvements that can be made to a proposal, in order to help in accelerating the search for a consensus between the agents. Therefore, we propose a negotiation protocol consisting of solving our decision problem using a MAS with a multi-criteria decision aiding modelling at the agent level and a cooperation-based multilateral multi-issue negotiation protocol. This protocol is studied under a non-cooperative approach and it is shown 943 978-81-904262-7-5 (RPS) c 2007 IFAAMAS that it has subgame perfect equilibria, provided that agents behave rationally in the sense of von Neumann and Morgenstern. The approach proposed in this paper has been first introduced and presented in [8]. In this paper, we present our first experiments, with some noteworthy results, and a more complex multi-agent system with representatives to enable us to have a more robust system.
In Section 2, we present our application, a crisis management problem. Section 3 deals with the general aspect of the proposed approach. The preference modelling is described in sect. 4, whereas the motivations of our protocol are considered in sect. 5 and the agent/multiagent modelling in sect. 6. Section 7 presents the formal modelling and properties of our protocol before presenting our first experiments in sect. 8. Finally, in Section 9, we conclude and present the future work.
This protocol is applied to a crisis management problem.
Crisis management is a relatively new field of management and is composed of three types of activities: crisis prevention, operational preparedness and management of declared crisis. The crisis prevention aims to bring the risk of crisis to an acceptable level and, when possible, avoid that the crisis actually happens. The operational preparedness includes strategic advanced planning, training and simulation to ensure availability, rapid mobilisation and deployment of resources to deal with possible emergencies. The management of declared crisis is the response to - including the evacuation, search and rescue - and the recovery from the crisis by minimising the effects of the crises, limiting the impact on the community and environment and, on a longer term, by bringing the community"s systems back to normal. In this paper, we focus on the response part of the management of declared crisis activity, and particularly on the evacuation of the injured people in disaster situations. When a crisis is declared, the plans defined during the operational preparedness activity are executed. For disasters, master plans are executed. These plans are elaborated by the authorities with the collaboration of civil protection agencies, police, health services, non-governmental organizations, etc.
When a victim is found, several actions follow. First, a rescue party is assigned to the victim who is examined and is given first aid on the spot. Then, the victims can be placed in an emergency centre on the ground called the medical advanced post. For all victims, a sorter physician - generally a hospital physician - examines the seriousness of their injuries and classifies the victims by pathology. The evacuation by emergency health transport if necessary can take place after these clinical examinations and classifications.
Nowadays, to evacuate the injured people, the physicians contact the emergency call centre to pass on the medical assessments of the most urgent cases. The emergency call centre then searches for available and appropriate spaces in the hospitals to care for these victims. The physicians are informed of the allocations, so they can proceed to the evacuations choosing the emergency health transports according to the pathologies and the transport modes provided. In this context, we can observe that the evacuation is based on three important elements: the examination and classification of the victims, the search for an allocation and the transport. In the case of the 11 March 2004 Madrid attacks, for instance, some injured people did not receive the appropriate health care because, during the search for space, the emergency call centre did not consider the transport constraints and, in particular, the traffic. Therefore, for a large scale crisis management problem, there is a need to support the emergency call centre and the physicians in the dispatching to take into account the hospitals and the transport constraints and availabilities.
To accept a proposal, an agent has to consider several issues such as, in the case of the crisis management problem, the availabilities in terms of number of beds by unit, medical and surgical staffs, theatres and so on. Therefore, each agent has its own preferences in correlation with its resource constraints and other decision criteria such as, for the case study, the level of congestion of a hospital. All the agents also make decisions by taking into account the dependencies between these decision criteria.
The first hypothesis of our approach is that there are several parties involved in and impacted by the decision, and so they have to decide together according to their own constraints and decision criteria. Negotiation is the process by which a group facing a conflict communicates with one another to try and come to a mutually acceptable agreement or decision and so, the agents have to negotiate. The conflict we have to resolve is finding an acceptable solution for all the parties by using a particular protocol. In our context, multilateral negotiation is a negotiation protocol type that is the best suited for this type of problem : this type of protocol enables the hospitals and the physicians to negotiate together. The negotiation also deals with multiple issues. Moreover, an other hypothesis is that we are in a cooperative context where all the parties have a common objective which is to provide the best possible solution for everyone. This implies the use of a negotiation protocol encouraging the parties involved to cooperate as satisfying its preferences.
Taking into account these aspects, a Multi-Agent System (MAS) seems to be a reliable method in the case of a distributed decision making process. Indeed, a MAS is a suitable answer when the solution has to combine, at least, distribution features and reasoning capabilities. Another motivation for using MAS lies in the fact that MAS is well known for facilitating automated negotiation at the operative decision making level in various applications.
Therefore, our approach consists of solving a multiparty decision problem using a MAS with • The preferences of the agents are modelled using a multi-criteria decision aid tool, MYRIAD, also enabling us to consider multi-issue problems by evaluating proposals on several criteria. • A cooperation-based multilateral and multi-issue negotiation protocol.
We consider a problem where an agent has several decision criteria, a set Nk = {1, . . . , nk} of criteria for each agent k involved in the negotiation protocol. These decision criteria enable the agents to evaluate the set of issues that are negotiated. The issues correspond directly or not to the decision criteria. However, for the example of the crisis management
problem, the issues are the set of victims to dispatch between the hospitals. These issues are translated to decision criteria enabling the hospital to evaluate its congestion and so to an updated number of available beds, medical teams and so on. In order to take into account the complexity that exists between the criteria/issues, we use a multi-criteria decision aiding (MCDA) tool named MYRIAD [12] developed at Thales for MCDA applications based on a two-additive Choquet integral which is a good compromise between versatility and ease to understand and model the interactions between decision criteria [6].
The set of the attributes of Nk is denoted by Xk
nk .
All the attributes are made commensurate thanks to the introduction of partial utility functions uk i : Xk i → [0, 1]. The [0, 1] scale depicts the satisfaction of the agent k regarding the values of the attributes. An option x is identified to an element of Xk = Xk
nk , with x = (x1, . . . , xnk ).
Then the overall assessment of x is given by Uk(x) = Hk(uk
nk (xnk )) (1) where Hk : [0, 1]nk → [0, 1] is the aggregation function. The overall preference relation over Xk is then x y ⇐⇒ Uk(x) ≥ Uk(y) .
The two-additive Choquet integral is defined for (z1, . . . , znk ) ∈ [0, 1]nk by [7] Hk(z1, . . . , znk ) = X i∈Nk 0 @vk i − 1 2 X j=i |Ik i,j| 1 A zi + X Ik i,j >0 Ik i,j zi ∧ zj + X Ii,j <0 |Ii,j| zi ∨ zj (2) where vk i is the relative importance of criterion i for agent k and Ik i,j is the interaction between criteria i and j, ∧ and ∨ denote the min and max functions respectively. Assume that zi < zj. A positive interaction between criteria i and j depicts complementarity between these criteria (positive synergy) [7]. Hence, the lower score of z on criterion i conceals the positive effect of the better score on criterion j to a larger extent on the overall evaluation than the impact of the relative importance of the criteria taken independently of the other ones. In other words, the score of z on criterion j is penalized by the lower score on criterion i. Conversely, a negative interaction between criteria i and j depicts substitutability between these criteria (negative synergy) [7]. The score of z on criterion i is then saved by a better score on criterion j.
In MYRIAD, we can also obtain some recommendations corresponding to an indicator ωC (H, x) measuring the worth to improve option x w.r.t. Hk on some criteria C ⊆ Nk as follows ωC (Hk, x)= Z 1 0 Hk ` (1 − τ)xC + τ, xNk\C ´ − Hk(x) EC (τ, x) dτ where ((1−τ)xC +τ, xNk\C ) is the compound act that equals (1 − τ)xi + τ if i ∈ C and equals xi if i ∈ Nk \ C. Moreover,
EC (τ, x) is the effort to go from the profile x to the profile ((1 − τ)xC + τ, xNk\C ). Function ωC (Hk, x) depicts the average improvement of Hk when the criteria of coalition A range from xC to 1C divided by the average effort needed for this improvement. We generally assume that EC is of order 1, that is EC (τ, x) = τ P i∈C (1 − xi). The expression of ωC (Hk, x) when Hk is a Choquet integral, is given in [11].
The agent is then recommended to improve of coalition C for which ωC (Hk, x) is maximum. This recommendation is very useful in a negotiation protocol since it helps the agents to know what to do if they want an offer to be accepted while not revealing their own preference model.
For multi-issue problems, there are two approaches: a complete package approach where the issues are negotiated simultaneously in opposition to the sequential approach where the issues are negotiated one by one. When the issues are dependant, then it is the best choice to bargain simultaneously over all issues [5]. Thus, the complete package is the adopted approach so that an offer will be on the overall set of injured people while taking into account the other decision criteria.
We have to consider that all the parties of the negotiation process have to agree on the decision since they are all involved in and impacted by this decision and so an unanimous agreement is required in the protocol. In addition, no party can leave the process until an agreement is reached, i.e. a consensus achieved. This makes sense since a proposal concerns all the parties. Moreover, we have to guarantee the availability of the resources needed by the parties to ensure that a proposal is realistic. To this end, the information about these availabilities are used to determine admissible proposals such that an offer cannot be made if one of the parties has not enough resources to execute/achieve it. At the beginning of the negotiation, each party provides its maximum availabilities, this defining the constraints that have to be satisfied for each offer submitted.
The negotiation has also to converge quickly on an unanimous agreement. We decided to introduce in the negotiation protocol an incentive to cooperate taking into account the passed negotiation time. This incentive is defined on the basis of a time dependent penalty, the discounting factor as in [18] or a time-dependent threshold. This penalty has to be used in the accept/reject stage of our consensus procedure. In fact, in the case of a discounting factor, each party will accept or reject an offer by evaluating the proposal using its utility function deducted from the discounting factor.
In the case of a time-dependent threshold, if the evaluation is greater or equal to this threshold, the offer is accepted, otherwise, in the next period, its threshold is reduced.
The use of a penalty is not enough alone since it does not help in finding a solution. Some information about the assessments of the parties involved in the negotiation are needed. In particular, it would be helpful to know why an offer has been rejected and/or what can be done to make a proposal that would be accepted. MYRIAD provides an analysis that determines the flaws an option, here a proposal. In particular, it gives this type of information: which criteria of a proposal should be improved so as to reach the highest possible overall evaluation [11]. As we use this tool to model the parties involved in the negotiation, the information about the criteria to improve can be used by the mediator to elaborate the proposals. We also consider that the dual function can be used to take into account another type of information: on which criteria of a proposal, no improvement is necessary so that the overall evaluation of a proposal is still acceptable, do not decrease. Thus, all information is a constraint to be satisfied as much as possible by The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 945 Figure 1: An illustration of some system. the parties to make a new proposal.
We are in a cooperative context and revealing one"s opinion on what can be improved is not prohibited, on the contrary, it is useful and recommended here seeing that it helps in converging on an agreement. Therefore, when one of the parties refuses an offer, some information will be communicated. In order to facilitate and speed up the negotiation, we introduce a mediator. This specific entity is in charge of making the proposals to the other parties in the system by taking into account their public constraints (e.g. their availabilities) and the recommendations they make. This mediator can also be considered as the representative of the general interest we can have, in some applications, such as in the crisis management problem, the physician will be the mediator and will also have some more information to consider when making an offer (e.g. traffic state, transport mode and time). Each party in a negotiation N, a negotiator, can also be a mediator of another negotiation N , this party becoming the representative of N in the negotiation N, as illustrated by fig. 1 what can also help in reducing the communication time.
How the problem is transposed in a MAS problem is a very important aspect when designing such a system. The agentification has an influence upon the systems efficiency in solving the problem. Therefore, in this section, we describe the elements and constraints taken into account during the modelling phase and for the model itself. However, for this negotiation application, the modelling is quite natural when one observes the negotiation protocol motivations and main properties.
First of all, it seems obvious that there should be one agent for each player of our multilateral multi-issue negotiation protocol. The agents have the involved parties" information and preferences. These agents are: • Autonomous: they decide for themselves what, when and under what conditions actions should be performed; • Rational: they have a means-ends competence to fit its decisions, according to its knowledge, preferences and goal; • Self-interested: they have their own interests which may conflict with the interests of other agents.
Moreover, their preferences are modelled and a proposal evaluated and analysed using MYRIAD. Each agent has private information and can access public information as knowledge.
In fact, there are two types of agents: the mediator type for the agents corresponding to the mediator of our negotiation protocol, the delegated physician in our application, and the negotiator type for the agents corresponding to the other parties, the hospitals. The main behaviours that an agent of type mediator needs to negotiate in our protocol are the following: • convert_improvements: converts the information given by the other agents involved in the negotiation about the improvements to be done, into constraints on the next proposal to be made; • convert_no_decrease: converts the information given by the other agents involved in the negotiation about the points that should not be changed into constraints on the next proposal to be made; • construct_proposal: constructs a new proposal according to the constraints obtained with convert_improvements, convert_no_decrease and the agent preferences; The main behaviours that an agent of type negotiator needs to negotiate in our protocol are the following: • convert_proposal: converts a proposal to a MYRIAD option of the agent according to its preferences model and its private data; • convert_improvements_wc: converts the agent recommendations for the improvements of a MYRIAD option into general information on the proposal; • convert_no_decrease_wc: converts the agent recommendations about the criteria that should not be changed in the MYRIAD option into general information on the proposal; In addition to these behaviours, there are, for the two types of agents, access behaviours to MYRIAD functionalities such as the evaluation and improvement functions: • evaluate_option: evaluates the MYRIAD option obtained using the agent behaviour convert_proposal; • improvements: gets the agent recommendations to improve a proposal from the MYRIAD option; • no_decrease: gets the agent recommendations to not change some criteria from the MYRIAD option; Of course, before running the system with such agents, we must have defined each party preferences model in MYRIAD. This model has to be part of the agent so that it could be used to make the assessments and to retrieve the improvements. In addition to these behaviours, the communication acts between the agents is as follows.
m1 m 1 1 m inform1 m mediator negotiator accept−proposal l 1 accept−proposal m−l reject−proposal propose propose Figure 2: The protocol diagram in AUML, and where m is the number of negotiator agents and l is the number of agents refusing current proposal. (a) propose: sends a message containing a proposal to all negotiator agents; (b) inform: sends a message to all negotiator agents to inform them that an agreement has been reached and containing the consensus outcome.
(a) accept-proposal: sends a message to the mediator agent containing the agent recommendations to improve the proposal and obtained with convert_improvements_wc; (b) reject-proposal: sends a message to the mediator agent containing the agent recommendations about the criteria that should not be changed and obtained with convert_no_decrease_wc.
Such agents are interchangeable, in a case of failure, since they all have the same properties and represent a user with his preference model, not depending on the agent, but on the model defined in MYRIAD. When the issues and the decision criteria are different from each other, the information about the criteria improvement have to be pre-processed to give some instructions on the directions to take and about the negotiated issues. It is the same for the evaluation of a proposal: each agent has to convert the information about the issues to update its private information and to obtain the values of each attribute of the decision criteria.
Formally, we consider negotiations where a set of players A = {1, 2, . . . , m} and a player a are negotiating over a set Q of size q. The player a is the protocol mediator, the mediator agent of the agentification. The utility/preference function of a player k ∈ A ∪ {a} is Uk, defined using MYRIAD, as presented in Section 4, with a set Nk of criteria, Xk an option, and so on. An offer is a vector P = (P1, P2, · · · , Pm), a partition of Q, in which Pk is player k"s share of Q. We have P ∈ P where P is the set of admissible proposals, a finite set. Note that P is determined using all players general constraints on the proposals and Q.
Moreover, let ˜P denote a particular proposal defined as a"s preferred proposal.
We also have the following notation: δk is the threshold decrease factor of player k, Φk : Pk → Xk is player k"s function to convert a proposal to an option and Ψk is the function indicating which points P has to be improved, with Ψk its dual function - on which points no improvement is necessary. Ψk is obtained using the dual function of ωC (Hk, x): eωC (Hk, x)= Z 1 0 Hk(x) − Hk ` τ xC , xNk\C ´ eEC (τ, x) dτ Where eEC (τ, x) is the cost/effort to go from (τxC , xNk\C ) to x.
In period t of our consensus procedure, player a proposes an agreement P. All players k ∈ A respond to a by accepting or rejecting P. The responses are made simultaneously.
If all players k ∈ A accept the offer, the game ends. If any player k rejects P, then the next period t+1 begins: player a makes another proposal P by taking into account information provided by the players and the ones that have rejected P apply a penalty. Therefore, our negotiation protocol can be as follows: Protocol P1. • At the beginning, we set period t = 0 • a makes a proposal P ∈ P that has not been proposed before. • Wait that all players of A give their opinion Yes or No to the player a. If all players agree on P, this later is chosen. Otherwise t is incremented and we go back to previous point. • If there is no more offer left from P, the default offer ˜P will be chosen. • The utility of players regarding a given offer decreases over time. More precisely, the utility of player k ∈ A at period t regarding offer P is Uk(Φk(Pk), t) = ft(Uk(Φk(Pk))), where one can take for instance ft(x) = x.(δk)t or ft(x) = x − δk.t, as penalty function.
Lemma 1. Protocol P1 has at least one subgame perfect equilibrium 1 .
Proof : Protocol P1 is first transformed in a game in extensive form. To this end, one shall specify the order in which the responders A react to the offer P of a. However the order in which the players answer has no influence on the course of the game and in particular on their personal utility. Hence protocol P1 is strictly equivalent to a game in 1 A subgame perfect equilibrium is an equilibrium such that players" strategies constitute a Nash equilibrium in every subgame of the original game [18, 16]. A Nash equilibrium is a set of strategies, one for each player, such that no player has incentive to unilaterally change his/her action [15].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 947 extensive form, considering any order of the players A. This game is clearly finite since P is finite and each offer can only be proposed once. Finally P1 corresponds to a game with perfect information. We end the proof by using a classical result stating that any finite game in extensive form with perfect information has at least one subgame perfect equilibrium (see e.g. [16]).
Rational players (in the sense of von Neumann and Morgenstern) involved in protocol P1 will necessarily come up with a subgame perfect equilibrium.
Example 1. Consider an example with A = {1, 2} and P = {P1 , P2 , P3 } where the default offer is P1 . Assume that ft(x) = x − 0.1 t. Consider the following table giving the utilities at t = 0.
P1 P2 P3 a 1 0.8 0.7
It is easy to see that there is one single subgame perfect equilibrium for protocol P1 corresponding to these values. This equilibrium consists of the following choices: first a proposes P3 ; player 1 rejects this offer; a proposes then P2 and both players 1 and 2 accepts otherwise they are threatened to receive the worse offer P1 for them. Finally offer P2 is chosen. Option P1 is the best one for a but the two other players vetoed it. It is interesting to point out that, even though a prefers P2 to P3 , offer P3 is first proposed and this make P2 being accepted. If a proposes P2 first, then the subgame perfect equilibrium in this situation is P3 . To sum up, the worse preferred options have to be proposed first in order to get finally the best one. But this entails a waste of time.
Analysing the previous example, one sees that the game outcome at the equilibrium is P2 that is not very attractive for player 2. Option P3 seems more balanced since no player judges it badly. It could be seen as a better solution as a consensus among the agents.
In order to introduce this notion of balanceness in the protocol, we introduce a condition under which a player will be obliged to accept the proposal, reducing the autonomy of the agents but for increasing rationality and cooperation.
More precisely if the utility of a player is larger than a given threshold then acceptance is required. The threshold decreases over time so that players have to make more and more concession. Therefore, the protocol becomes as follows.
Protocol P2. • At the beginning we set period t = 0 • a makes a proposal P ∈ P that has not been proposed before. • Wait that all players of A give their opinion Yes or No to the player a. A player k must accept the offer if Uk(Φk(Pk)) ≥ ρk(t) where ρk(t) tends to zero when t grows.
Moreover there exists T such that for all t ≥ T, ρk(t) = 0. If all players agree on P, this later is chosen. Otherwise t is incremented and we go back to previous point. • If there is no more offer left from P, the default offer ˜P will be chosen.
One can show exactly as in Lemma 1 that protocol P2 has at least one subgame perfect equilibrium. We expect that protocol P2 provides a solution not to far from P , so it favours fairness among the players. Therefore, our cooperation-based multilateral multi-issue protocol is the following: Protocol P. • At the beginning we set period t = 0 • a makes a proposal P ∈ P that has not been proposed before, considering Ψk(Pt ) and Ψk(Pt ) for all players k ∈ A. • Wait that all players of A give their opinion (Yes , Ψk(Pt )) or (No , Ψk(Pt )) to the player a. A player k must accept the offer if Uk(Φk(Pk)) ≥ ρk(t) where ρk(t) tends to zero when t grows. Moreover there exists T such that for all t ≥ T, ρk(t) = 0. If all players agree on P, this later is chosen.
Otherwise t is incremented and we go back to previous point. • If there is no more offer left from P, the default offer ˜P will be chosen.
We developed a MAS using the widely used JADE agent platform [1]. This MAS is designed to be as general as possible (e.g. a general framework to specialise according to the application) and enable us to make some preliminary experiments. The experiments aim at verifying that our approach gives solutions as close as possible to the Maximin solution and in a small number of rounds and hopefully in a short time since our context is highly cooperative. We defined the two types of agents and their behaviours as introduced in section 6. The agents and their behaviours correspond to the main classes of our prototype, NegotiatorAgent and NegotiatorBehaviour for the negotiator agents, and MediatorAgent and MediatorBehaviour for the mediator agent. These classes extend JADE classes and integrate MYRIAD into the agents, reducing the amount of communications in the system. Some functionalities depending on the application have to be implemented according to the application by extending these classes. In particular, all conversion parts of the agents have to be specified according to the application since to convert a proposal into decision criteria, we need to know, first, this model and the correlations between the proposals and this model.
First, to illustrate our protocol, we present a simple example of our dispatch problem. In this example, we have three hospitals, H1, H2 and H3. Each hospital can receive victims having a particular pathology in such a way that H1 can receive patients with the pathology burn, surgery or orthopedic, H2 can receive patients with the pathology surgery, orthopedic or cardiology and H3 can receive patients with the pathology burn or cardiology. All the hospitals have similar decision criteria reflecting their preferences on the level of congestion they can face for the overall hospital and the different services available, as briefly explained for hospital H1 hereafter.
For hospital H1, the preference model, fig. 3, is composed of five criteria. These criteria correspond to the preferences on the pathologies the hospital can treat. In the case of
Figure 3: The H1 preference model in MYRIAD. the pathology burn, the corresponding criterion, also named burn as shown in fig. 3, represents the preferences of H1 according to the value of Cburn which is the current capacity of burn. Therefore, the utility function of this criterion represents a preference such that the more there are patients of this pathology in the hospital, the less the hospital may satisfy them, and this with an initial capacity. In addition to reflecting this kind of viewpoint, the aggregation function as defined in MYRIAD introduces a veto on the criteria burn, surgery, orthopedic and EReceipt, where EReceipt is the criterion for the preferences about the capacity to receive a number of patients at the same time.
In this simplified example, the physician have no particular preferences on the dispatch and the mediator agent chooses a proposal randomly in a subset of the set of admissibility. This subset have to satisfy as much as possible the recommendations made by the hospitals. To solve this problem, for this example, we decided to solve a linear problem with the availability constraints and the recommendations as linear constraints on the dispatch values. The set of admissibility is then obtained by solving this linear problem by the use of Prolog. Moreover, only the recommendations on how to improve a proposal are taken into account. The problem to solve is then to dispatch to hospital H1, H2 and H3, the set of victims composed of 5 victims with the pathology burn, 10 with surgery, 3 with orthopedic and 7 with cardiology. The availabilities of the hospitals are as presented in the following table.
Available Overall burn surg. orthop. cardio.
H1 11 4 8 10H2 25 - 3 4 10 H3 7 10 - - 3 We obtain a multiagent system with the mediator agent and three agents of type negotiator for the three hospital in the problem. The hospitals threshold are fixed approximatively to the level where an evaluation is considered as good. To start, the negotiator agents send their availabilities. The mediator agent makes a proposal chosen randomly in admissible set obtained with these availabilities as linear constraints. This proposal is the vector P0 = [[H1,burn, 3], [H1, surgery, 8], [H1, orthopaedic, 0], [H2, surgery, 2], [H2, orthopaedic, 3], [H2, cardiology, 6], [H3, burn, 2], [H3, cardiology, 1]] and the mediator sends propose(P0) to H1,
H2 and H3 for approval. Each negotiator agent evaluates this proposal and answers back by accepting or rejecting P0: • Agent H1 rejects this offer since its evaluation is very far from the threshold (0.29, a bad score) and gives a recommendation to improve burn and surgery by sending the message reject_proposal([burn,surgery]); • Agent H2 accepts this offer by sending the message accept_proposal(), the proposal evaluation being good; • Agent H3 accepts P0 by sending the message accept_ proposal(), the proposal evaluation being good.
Just with the recommendations provided by agent H1, the mediator is able to make a new proposal by restricting the value of burn and surgery. The new proposal obtained is then P1 = [[H1,burn, 0], [H1, surgery, 8], [H1, orthopaedic, 1], [H2, surgery, 2], [H2, orthopaedic, 2], [H2, cardiology, 6], [H3, burn, 5], [H3, cardiology, 1]]. The mediator sends propose(P1) the negotiator agents. H1, H2 and H3 answer back by sending the message accept_proposal(), P1 being evaluated with a high enough score to be acceptable, and also considered as a good proposal when using the explanation function of MYRIAD. An agreement is reached with P1. Note that the evaluation of P1 by H3 has decreased in comparison with P0, but not enough to be rejected and that this solution is the Pareto one, P∗ .
Other examples have been tested with the same settings: issues in IN, three negotiator agents and the same mediator agent, with no preference model but selecting randomly the proposal. We obtained solutions either equal or close to the Maximin solution, the distance from the standard deviation being less than 0.0829, the evaluations not far from the ones obtained with P∗ and with less than seven proposals made.
This shows us that we are able to solve this multi-issue multilateral negotiation problem in a simple and efficient way, with solutions close to the Pareto solution.
This paper presents a new protocol to address multilateral multi-issue negotiation in a cooperative context. The first main contribution is that we take into account complex inter-dependencies between multiple issues with the use of a complex preference modelling. This contribution is reinforced by the use of multi-issue negotiation in a multilateral context. Our second contribution is the use of sharp recommendations in the protocol to help in accelerating the search of a consensus between the cooperative agents and in finding an optimal solution. We have also shown that the protocol has subgame perfect equilibria and these equilibria converge to the usual maximum solution. Moreover, we tested this protocol in a crisis management context where the negotiation aim is where to evacuate a whole set of injured people to predefined hospitals.
We have already developed a first MAS, in particular integrating MYRIAD, to test this protocol in order to know more about its efficiency in terms of solution quality and quickness in finding a consensus. This prototype enabled us to solve some examples with our approach and the results we obtained are encouraging since we obtained quickly good agreements, close to the Pareto solution, in the light of the initial constraints of the problem: the availabilities.
We still have to improve our MAS by taking into account The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 949 the two types of recommendations and by adding a preference model to the mediator of our system. Moreover, a comparative study has to be done in order to evaluate the performance of our framework against the existing ones and against some variations on the protocol.
This work is partly funded by the ICIS research project under the Dutch BSIK Program (BSIK 03024).
[1] JADE. http://jade.tilab.com/. [2] P. Faratin, C. Sierra, and N. R. Jennings. Using similarity criteria to make issue trade-offs in automated negotiations. Artificial Intelligence, 142(2):205-237, 2003. [3] S. S. Fatima, M. Wooldridge, and N. R. Jennings.
Optimal negotiation of multiple issues in incomplete information settings. In 3rd International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS"04), pages 1080-1087, New York,
USA, 2004. [4] S. S. Fatima, M. Wooldridge, and N. R. Jennings. A comparative study of game theoretic and evolutionary models of bargaining for software agents. Artificial Intelligence Review, 23:185-203, 2005. [5] S. S. Fatima, M. Wooldridge, and N. R. Jennings. On efficient procedures for multi-issue negotiation. In 8th International Workshop on Agent-Mediated Electronic Commerce(AMEC"06), pages 71-84, Hakodate, Japan,
[6] M. Grabisch. The application of fuzzy integrals in multicriteria decision making. European J. of Operational Research, 89:445-456, 1996. [7] M. Grabisch, T. Murofushi, and M. Sugeno. Fuzzy Measures and Integrals. Theory and Applications (edited volume). Studies in Fuzziness. Physica Verlag,
[8] M. Hemaissia, A. El Fallah-Seghrouchni,
C. Labreuche, and J. Mattioli. Cooperation-based multilateral multi-issue negotiation for crisis management. In 2th International Workshop on Rational, Robust and Secure Negotiation (RRS"06), pages 77-95, Hakodate, Japan, May 2006. [9] T. Ito, M. Klein, and H. Hattori. A negotiation protocol for agents with nonlinear utility functions. In AAAI, 2006. [10] M. Klein, P. Faratin, H. Sayama, and Y. Bar-Yam.
Negotiating complex contracts. Group Decision and Negotiation, 12:111-125, March 2003. [11] C. Labreuche. Determination of the criteria to be improved first in order to improve as much as possible the overall evaluation. In IPMU 2004, pages 609-616,
Perugia, Italy, 2004. [12] C. Labreuche and F. Le Hu´ed´e. MYRIAD: a tool suite for MCDA. In EUSFLAT"05, pages 204-209,
Barcelona, Spain, 2005. [13] R. Y. K. Lau. Towards genetically optimised multi-agent multi-issue negotiations. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS"05), Big Island, Hawaii, 2005. [14] R. J. Lin. Bilateral multi-issue contract negotiation for task redistribution using a mediation service. In Agent Mediated Electronic Commerce VI (AMEC"04), New York, USA, 2004. [15] J. F. Nash. Non cooperative games. Annals of Mathematics, 54:286-295, 1951. [16] G. Owen. Game Theory. Academic Press, New York,
[17] V. Robu, D. J. A. Somefun, and J. A. L. Poutr´e.
Modeling complex multi-issue negotiations using utility graphs. In 4th International Joint Conference on Autonomous agents and multiagent systems (AAMAS"05), pages 280-287, 2005. [18] A. Rubinstein. Perfect equilibrium in a bargaining model. Econometrica, 50:97-109, jan 1982. [19] L.-K. Soh and X. Li. Adaptive, confidence-based multiagent negotiation strategy. In 3rd International Joint Conference on Autonomous agents and multiagent systems (AAMAS"04), pages 1048-1055,
Los Alamitos, CA, USA, 2004. [20] H.-W. Tung and R. J. Lin. Automated contract negotiation using a mediation service. In 7th IEEE International Conference on E-Commerce Technology (CEC"05), pages 374-377, Munich, Germany, 2005.

With the increasing popularity of distributed computing technologies such as Grid [12] and Web services [20], the Internet is becoming a powerful computing platform where different software peers (e.g., agents) can use existing computing resources to perform tasks. In this sense, each agent is a resource consumer that acquires a certain amount of resources for the execution of its tasks. It is difficult for a central resource allocation mechanism to collect and manage the information about all shared resources and resource consumers to effectively perform the allocation of resources.
Hence, distributed solutions of the resource allocation problem are required. Researchers have recognised these requirements [10] and proposed techniques for distributed resource allocation. A promising kind of such distributed approaches are based on economic market models [4], inspired by principles of real stock markets. Even if those approaches are distributed, they usually require a facilitator for pricing, resource discovery and dispatching jobs to resources [5, 9].
Another mainly unsolved problem of those approaches is the fine-tuning of price and time, budget constraints to enable efficient resource allocation in large, dynamic systems [22].
In this paper we propose a distributed solution of the resource allocation problem based on self-organisation of the resource consumers in a system with limited resources. In our approach, agents dynamically allocate tasks to servers that provide a limited amount of resources. In our approach, agents select autonomously the execution platform for the task rather than ask a resource broker to do the allocation.
All control needed for our algorithm is distributed among the agents in the system. They optimise the resource allocation process continuously over their lifetime to changes in the availability of shared resources by learning from past allocation decisions. The only information available to all agents are resource load and allocation success information from past resource allocations. Additional resource load information about servers is not disseminated. The basic concept of our solution is inspired by inductive reasoning and bounded rationality introduced by W. Brian Arthur [2].
The proposed mechanism does not require a central controlling authority, resource management layer or introduce additional communication between agents to decide which task is allocated on which server. We demonstrate that this mechanism performs well dynamic systems with a large number of tasks and can easily be adapted to various system sizes. In addition, the overall system performance is not affected in case agents or servers fail or become unavailable. The proposed approach provides an easy way to implement distributed resource allocation and takes into account multi-agent system tendencies toward autonomy, heterogeneity and unreliability of resources and agents.
This proposed technique can be easily supplemented by techniques for queuing or rejecting resource allocation requests of agents [11]. Such self-managing capabilities of software agents allow a reliable resource allocation even in an environment with unreliable resource providers. This can be achieved by the mutual interactions between agents by applying techniques from complex system theory.
Selforganisation of all agents leads to a self-organisation of the 74 978-81-904262-7-5 (RPS) c 2007 IFAAMAS system resources and is an emergent property of the system [21].
The remainder of the paper is structured as follows: The next section gives an overview of the related work already done in the area of load balancing, resource allocation or scheduling. Section 3 describes the model of a multi-agent environment that was used to conduct simulations for a performance evaluation. Sections 4 and 5 describe the distributed resource allocation algorithm and presents various experimental results. A summary, conclusion and outlook to future work finish this paper.
Resource allocation is an important problem in the area of computer science. Over the past years, solutions based on different assumptions and constraints have been proposed by different research groups [7, 3, 15, 10]. Generally speaking, resource allocation is a mechanism or policy for the efficient and effective management of the access to a limited resource or set of resources by its consumers. In the simplest case, resource consumers ask a central broker or dispatcher for available resources where the resource consumer will be allocated. The broker usually has full knowledge about all system resources. All incoming requests are directed to the broker who is the solely decision maker. In those approaches, the resource consumer cannot influence the allocation decision process. Load balancing [3] is a special case of the resource allocation problem using a broker that tries to be fair to all resources by balancing the system load equally among all resource providers. This mechanism works best in a homogeneous system.
A simple distributed technique for resource management is capacity planning by refusing or queuing incoming agents to avoid resource overload [11]. From the resource owner perspective, this technique is important to prevent overload at the resource but it is not sufficient for effective resource allocation. This technique can only provide a good supplement for distributed resource allocation mechanisms.
Most of today"s techniques for resource allocation in grid computing toolkits like Globus [12] or Condor-G [13] coordinate the resource allocation with an auctioneer, arbitrator, dispatcher, scheduler or manager. Those coordinators usually need to have global knowledge on the state of all system resources. An example of a dynamic resource allocation algorithm is the Cactus project [1] for the allocation of computational very expensive jobs.
The value of distributed solutions for the resource allocation problem has been recognised by research [10]. Inspired by the principles in stock markets, economic market models have been developed for trading resources for the regulation of supply and demand in the grid. These approaches use different pricing strategies such as posted price models, different auction methods or a commodity market model.
Users try to purchase cheap resources required to run the job while providers try to make as much profit as possible and operate the available resources at full capacity. A collection of different distributed resource allocation techniques based on market models is presented in Clearwater [10]. Buyya et al. developed a resource allocation framework based on the regulation of supply and demand [4] for Nimrod-G [6] with the main focus on job deadlines and budget constraints.
The Agent based Resource Allocation Model (ARAM) for grids is designed to schedule computational expensive jobs using agents. Drawback of this model is the extensive use of message exchange between agents for periodic monitoring and information exchange within the hierarchical structure. Subtasks of a job migrate through the network until they find a resource that meets the price constraints. The job"s migration itinerary is determined by the resources in connecting them in different topologies [17]. The proposed mechanism in this paper eliminates the need of periodic information exchange about resource loads and does not need a connection topology between the resources.
There has been considerable work on decentralised resource allocation techniques using game theory published over recent years. Most of them are formulated as repetitive games in an idealistic and simplified environment. For example, Arthur [2] introduced the so called El Farol bar problem that does not allow a perfect, logical and rational solution. It is an ill-defined decision problem that assumes and models inductive reasoning. It is probably one of the most studied examples of complex adaptive systems derived from the human way of deciding ill-defined problems. A variation of the El Farol problem is the so called minority game [8]. In this repetitive decision game, an odd number of agents have to choose between two resources based on past success information trying to allocate itself at the resource with the minority. Galstyan et al. [14] studied a variation with more than two resources, changing resource capacities and information from neighbour agents. They showed that agents can adapt effectively to changing capacities in this environment using a set of simple look-up tables (strategies) per agent.
Another distributed technique that is employed for solving the resource allocation problem is based on reinforcement learning [18]. Similar to our approach, a set of agents compete for a limited number of resources based only on prior individual experience. In this paper, the system objective is to maximise system throughput while ensuring fairness to resources, measured as the average processing time per job unit.
A resource allocation approach for sensor networks based on self-organisation techniques and reinforcement learning is presented in [16] with main focus on the optimisation of energy consumption of network nodes. We [19] proposed a self-organising load balancing approach for a single server with focus on optimising the communication costs of mobile agents. A mobile agent will reject a migration to a remote agent server, if it expects the destination server to be already overloaded by other agents or server tasks. Agents make their decisions themselves based on forecasts of the server utilisation. In this paper a solution for a multi-server environment is presented without consideration of communication or migration costs.
We model a distributed multi-agent system as a network of servers L = {l1, . . . , lm}, agents A = {a1, . . . , an} and tasks T = {T1, ..., Tm}. Each agent has a number of tasks Ti that needs to be executed during its lifetime. A task Ti requires U(Ti, t) resources for its execution at time t independent from its execution server. Resources for the execution of tasks are provided by each server li. The task"s execution location in general is specified by the map L : T ×t → L. An agent has to know about the existence of server resources in order to allocate tasks at those resources. We write LS (ai) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 75 Sysytem Resources Host l4Host l3Host l2 2a 3a 4a a Host l1 1a 6a5 T1 T2 T3 T4 T5 T6 Figure 1: An illustration of our multi-server model with exclusive and shared resources for the agent execution. to address the set of resources known by agent ai.
Resources in the system can be used by all agents for the execution of tasks. The amount of provided resources C(li, t) of each server can vary over time. The resource utilisation of a server li at time t is calculated using equation 1, by adding the resource consumption U(Tj, t) of each task Tj that is executed at the resource at time t. All resource units used in our model represent real metrics such as memory or processor cycles.
U(li, t) = n j=1 U(Tj, t)| L(Tj, t) = li (1) Additional to the case that the total amount of system resources is enough to execute all tasks, we are also interested in the case that not enough system resources are provided to fulfil all allocation requests. That is, the overall shared resource capacity is lower than the amount of requested resources by agents. In this case, some agents must wait with their allocation request until free resources are expected.
The multi-agent system model used for our simulations is illustrated in Fig. 1.
ALLOCATION The resource allocation algorithm as described in this section is integrated in each agent. The only information required in order to make a resource allocation decision for a task is the server utilisation from completed task allocations at those servers. There is no additional information dissemination about server resource utilisation or information about free resources. Our solution demonstrates that agents can self-organise in a dynamic environment without active monitoring information that causes a lot of network traffic overhead. Additionally, we do not have any central controlling authority. All behaviour that leads to the resource allocation is created by the effective competition of the agents for shared resources and is a purely emergent effect.
The agents in our multi-agent system compete for resources or a set of resources to execute tasks. The collective action of these agents change the environment and, as time goes by, they have to adapt to these changes to compete more effectively in the newly created environment.
Our approach is based on different agent beliefs, represented by predictors and different information about their environment. Agents prefer a task allocation at a server with free resources. However, there is no way to be sure of the amount of free server resources in advance. All agents have the same preferences and a agent will allocate a task on a server if it expects enough free resources for its execution. There is no communication between agents. Actions taken by agents influence the actions of other agents indirectly. The applied mechanism is inspired by inductive reasoning and bounded rationality principles [2]. It is derived from the human way of deciding ill-defined problems. Humans tend to keep in mind many hypotheses and act on the most plausible one.
Therefore, each agent keeps track of the performance of a private collection of its predictors and selects the one that is currently most promising for decision making.
This section describes the decision mechanism for our selforganising resource allocation. All necessary control is integrated in the agents themselves. There is no higher controlling authority, management layer for decision support or information distribution. All agents have a set of predictors for each resource to forecast the future resource utilisation of these servers for potential task allocation. To do so, agents use historical information from past task allocations at those resources. Based on the forecasted resource utilisation, the agent will make its resource allocation decision. After the task has finished its execution and returned the results back to the agent, the predictor performances are evaluated and history information is updated.
Algorithm 1 shows the resource allocation algorithm for each agent. The agent first predicts the next step"s resource load for each server with historical information (line 3-7).
If the predicted resource load plus the task"s resource consumption is below the last known server capacity, this server is added to the list of candidates for the allocation. The agent then evaluates if any free shared resources for the task allocation are expected. In the case, no free resources are expected (line 9), the agent will explore resources by allocating the task at a randomly selected server from all not predictable servers to gather resource load information. This is the standard case at the beginning of the agent life-cycle as there is no information about the environment available.
The resource load prediction itself uses a set of r predictors P(a, l) := {pi|1 ≤ i ≤ r} per server. One predictor pA ∈ P of each set is called active predictor, which forecasts the next steps resource load. Each predictor is a function P : H → ℵ+ ∪ {0} from the space of history data H to a non-negative integer, which is the forecasted value. For example, a predictor could forecast a resource load equal to the average amount of occupied resources during the last execution at this resource. A history H of resource load information is a list of up to m history items hi = (xi, yi), comprising the observation date xi and the observed value yi. The most recent history item is h0.
Hm(li) = ((x0, y0), ..., (xk, yk))| 0 ≤ k < m (2) Our algorithm uses a set of predictors rather than only one, to avoid that all agents make the same decision based on the predicted value leading to an invalidation of their beliefs. Imagine that only one shared resource is known by a number of agents using one predictor forecasting the 76 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ResourceLoad Time (a) Predictor6 Predictor7 Predictor 8 Predictor9 Predictor10 Predictor2 Predictor 4 Predictor 3 Predictor5 Predictor1 (b) Figure 2: (a) Collected resource load information from previous task allocations that is used for future predictions. (b) Predictor"s probability distribution for selecting the new active predictor. same value as the last known server resource utilisation. All agents that allocated a task at a server that was slightly overloaded would dismiss another allocation at this server as they expect the server to be overloaded again based on the predictions. As the result, the server would have a large amount of free resources. A set of different predictors that predict different values avoids this situation of invalidating the beliefs of the agents [19].
An example of a collected resource load information from the last 5 visits of an agent at a shared resource can be seen in Fig. 2(a). It shows that the resource was visited frequently, which means free resources for execution were available and an exploration of other servers was unnecessary. This may change in the future as the resource load has significantly increased recently.
In the case where the set of servers predicted having free resources available is not empty (line 13), the agent selects one of those for allocation. We have implemented two alternative algorithms for the selection of a server for the task allocation.
Algorithm 1 Resource Allocation algorithm of an agent 1 L ← ∅ //server with free resources 2 u ← U(T, t + 1) //task resource consumption 3 for all P(a, l)|l ∈ LS (a) do 4 U(l) ← resourceLoadPrediction(P(a, l), t + 1) 5 if U(l) + u ≤ C(l) then 6 L ← L ∪ {P(a, l)} 7 end if 8 end for 9 if L = ∅ then 10 //all unpredictable shared resources 11 E ← LS /{l ∈ LS (a)|P(a, l) ∈ L} 12 allocationServer ← a random element of E 13 else 14 allocationServer ← serverSelection (L) 15 end if 16 return allocationServer Algorithm 2 shows the first method, which is a non-deterministic selection according to the predictability of the server resource utilisation. A probability distribution is calculated from the confidence levels of the resource predictions. The confidence level depends on three factors: the accuracy of the active predictor, the amount of historical information about the server and the average age of the history information (see Eq. 3. The server with the highest confidence level has the biggest chance to be selected as the active server.
G(P) = w1 · size(H) m + w2 · Age(H) max Age(H) + w3 · g(p) max (g(p)) (3) where: wi − weights size(H) − number of data in history m − maximal number of history values Age(H) − average age of historical data g(p) − see eq. 4 Algorithm 2 serverSelection(L)- best predictable server 1 for all P(a, l) ∈ L do 2 calculate G(P) 3 end for 4 transform all G(P) into a probability distribution 5 return l ∈ LS selected according to the probability distribution Algorithm 3 serverSelection(L) - most free resources 1 for all P(a, l) ∈ L do 2 c(l) ← C(l) − Ul 3 end for 4 return l ∈ LS |c(l) is maximum The second alternative selection method of a server from the set of predicted servers with free resources is deterministic and shown in algorithm 3. The server with most expected free resources from the set L of server with expected free resources is chosen. In the case where all agents predict the most free resources for one particular server, all agents will allocate the task at this server, which would invalidate the agent"s beliefs. However, our experiments show that different individual history information and the non-deterministic active predictor selection usually prevent this situation.
In the case, the resource allocation algorithm does not return any server (Alg. 1, line 16), the allocation at a resource in not recommended. The agent will not allocate the task at a resource. This case happens only if a resource load prediction for all servers is possible but no free resources are expected.
After the agent execution has finished, the evaluation process described in algorithm 4 is preformed. This process is divided into three cases. First, the task was not allocated at a resource. In this case, the agent cannot decide if the decision not to allocate the task was correct or not. The agent then removes old historical data. This is necessary for a successful adaptation in the future. If the agent would not delete old historical information, the prediction would always forecast that no free resources are available. The agent would never allocate a task at one of the resources in the future.
Old historical information is removed from the agent"s resource history using a decay rate. The decay rate is a cumulative distribution function that calculates the probability that a history item is deleted after it has reached a certain The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 77 Age of the history data Decayrate 0 1 older Figure 3: Decay rate of historical information age. The current implementation uses a constant probability density function in a configurable domain. Figure 3 shows an example of such a cumulative distribution function for the decay rate. Depending on the environment, the probability density function must be altered. If the number of potential server per agent is high, historical information must be kept longer to avoid the exploration of unexplored resources. In addition, a dynamic environment requires more up-to-date information to make more reliable predictions.
The second case in the evaluation process (Alg. 4, line 5) describes the actions taken after a server was visited the first time. The agent creates a new predictor set for this server and records the historical information. All predictors for this set are chosen randomly from some predefined set. g(p) = l i=0 ri (4) where: ri = ⎧ ⎨ ⎩ 1 if ith correct decision 0 if ith unknown outcome −1 if ith wrong decision The general case (Alg. 4, line 8) is the evaluation after the agent allocated the task at a resource. The agent evaluates all predictors of the predictor set for this resource by predicting the resource load with all predictors based on the old historical data. Predictors that made a correct prediction meaning the resource allocation was correct, will receive a positive rating. This is the case that the resource was not overloaded and free resources for execution were predicted, or the resource was overloaded and this predictor would have prevented the allocation. All predictors that predicted values which would lead to wrong decisions will receive negative ratings. In all other cases, which includes that no prediction was possible, a neutral rating is given to the predictors.
Based on these performance ratings, the confidence levels are calculated using equation 4. The confidence for all predictors that cannot predict with the current historical information about the server is set to zero to prevent the selection of those as the new active predictor. These values are transformed into a probability distribution. According to this probability distribution the new active predictor is chosen, implemented as a roulette wheel selection. Figure 2(b) illustrates the probabilities of a set of 10 predictors, which have been calculated from the predictor confidence levels.
Even if predictor 9 has the highest selection probability, its was not chosen by roulette wheel selection process as the active predictor. This non-deterministic predictor selection prevents the invalidation of the agents" beliefs in case agents have the same set of predictors.
The prediction accuracy that is the error of the prediction compared to the observed value is not taken into consideration. Suppose the active predictor predicts slightly above the resource capacity which leads not to a allocation on a resources. In fact, enough resources for the execution would be available. A less accurate prediction which is far below the capacity would lead to the correct decision and is therefore preferred.
The last action of the evaluation algorithm (Alg. 4, line 22) updates the history with the latest resource load information of the server. The oldest history data is overwritten if already m history values are recorded for the server.
Algorithm 4 Decision Evaluation 1 if l ∈ LE then 2 for all P(a, l)|l ∈ LS (a) do 3 evaporate old historical data 4 end for 5 else if P(a, l) = null then 6 create (P(a, l)) 7 update H(l) 8 else 9 for all p ∈ P(a, l) do 10 pred ← resourceLoadPrediction(p) 11 if (U(l) ≤ C(l) AND pred + U(a, t) ≤ C(l)) OR (U(l) > C(l) AND pred + U(a, t) > C(l)) then 12 addPositiveRating(p) 13 else if U(l) ≤ C(l) AND pred + U(a, t) > C(l) OR U(l) ≤ C(l) AND pred + U(a, t) > C(l) then 14 addNegativeRating(p) 15 else 16 addNeutralRating(p) 17 end if 18 end for 19 calculate all g(p); g(p) ← 0, if p is not working 20 transform all g(p) into a probability distribution 21 pA ← p ∈ P(a, l) is selected according to this probability distribution 22 update H(l) 23 end if
Our prediction mechanism uses a number of different types of simple predictors rather than of one sophisticated predictor. This method assures that agents can compete more effectively in a changing environment. Different types of predictors are suitable for different situations and environments. Therefore, all predictors are being evaluated after each decision and the active predictor is selected. This nondeterministic of the new active predictor supports that the agents" beliefs will not be invalidated, which happens in the case that all predictors are making the same decision.
Especially if there is only one shared resource available and all agents have only the choice to go the this one shared resource or not [19].
Our self-organising approach is robust against failures of resources or agents in the system. If they join or leave, the system can self-organise quickly and adapts to the new conditions. There is no classical bottleneck or single point of failure like in centralised mechanisms. The limitations are the reliance on historical resource utilisation information about other servers. A forecast of the resource utilisation of a remote server is only possible if an agent has a number of historical information about a shared resource. If the number of servers per agent is very large, there is no efficient way to gather historical information about remote servers. This problem occurs if the amount of provided shared resources 78 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) is limited and not enough for all resource consumers. In this case, the an agent would randomly try all known servers until it will find one with free resources or there is no one. In the worst case, by the time for trying all servers, historical information of the servers is already outdated.
The first part of this section gives a short overview of the setup of our simulation environment. In the rest of the section, results of the experiments are presented and discussed. All experiments are conducted in a special testbed that simulates and models a multi-agent system. We have implemented this test-bed in the Java programming language, independent from any specific agent toolkit. It allows a variety of experiments in in stable as well as dynamic environments with a configurable number of agents, tasks and servers. An event-driven model is used to trigger all activities in the system.
For all simulations, we limited the number of history data for each server to 10, the number of performance ratings per predictor to 10 and assigned 10 predictors to every predictor set for each agent. All predictors are chosen randomly from a arbitrary predefined set of 32 predictors of the following type. Predictors differ in different cycles or window sizes. - n-cycle predictor: p(n) = yn uses the nth -last history value - n-mean predictor: p(n) = 1 n · n i=1 yi uses the mean value of the n-last history values - n-linear regression predictor: p(n, t) = a·t+b uses the linear regression value from the last n history values where a, b are calculated using linear regression with least squares fitting under consideration of the last n history data. - n-distribution predictor: uses a random value from the frequency distribution of the n last history values - n-mirror predictor: p(n) = 2 · H − yn uses the mirror image around the mean of all history values of the nth last history value The efficiency of our proposed self-organising resource allocation is assessed by the resource load development of each server over the simulation as well as the total resource load development cumulated over all shared resources. Resource loads for each server are calculated using equation 1 as the sum of the resource consumption of all currently executed agents at this server. The total resource load of the system is calculated as the sum of the resources load of all resources.
The self-organising resource allocation algorithm has random elements. Therefore, the presented results show mean values and standard derivation calculated over 100 repeated experiments.
The following parameters have an impact on the resource allocation process. We give an overview of the parameters and a short description. - Agents: The number of agents involved in the resource allocation. This number varies in the experiments between 650 and 750 dependent on the total amount of available system resources. - Resource consumption: Each task consumes server resources for its execution. The resource consumption is assigned randomly to each task prior to its allocation from an interval. Resource consumption is specified in resource units which corresponds to real world metrics like memory or processor cycles. - Agent home server: All agents are located on a home agent server. The resources of those servers not considered in our simulation and does not affect the resource allocation performance. - Server resources: Experiments use servers with different amount of available shared resources. The first experiment is conducted in a static server environment that provides the same amount of shared resources, while the other experiment varies the available server resource during the simulation. The total amount of resources remains constant in both experiments. - Execution time: The execution time of a task for the execution, independent from the execution platform.
For this time the task consumes the assigned amount of server resources. This parameter is randomly assigned before the execution. - Task creation time: The time before the next task is created after successful or unsuccessful completion.
This parameter influences the age of the historical information about resources and has a major influence on the length of the initial adaptation phase. This parameter is randomly assigned after the task was completed.
This section shows results from selected experiments that demonstrate the performance of our proposed resource allocation mechanism. The first experiment show the performance in a stable environment where a number of agents allocate tasks to servers that provide a constant amount of resources. The second experiment was conducted in a dynamic server environment with a constant number of agents.
The first experiment shows our model in a stable 3-server environment that provide a total amount of 7000 resource units. The resource capacity of each server remains constant over the experiment. We used 650 agents with the parameters of the execution time between 1 and 15 time units and a task creation time in the interval [0 − 30] time units. The task"s resource consumption is randomly assigned from the interval [1 − 45] resource units. Figure 4 shows the results from 100 repetitions of this experiment. Figure 4(a) shows that the total amount of provided resources is larger than the demand of resource in average. At the beginning of the experiment, all agents allocate their tasks randomly at one of the available servers and explore the available capacities and resource utilisations for about 150 time units. This initial exploration phase shows that the average resource load of each server has a similar level. This causes an overload situation at server 1 because of its low capacity of shared resources, and a large amount of free resources on server
overload situation and explore randomly other available servers.
They find free resources at server 2. After learning period, the agents have self-organised themselves in this stable environment and find a stable solution for the allocation of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 79 0 250 500 750 1,000 1,250 1,500 Time 0 1,000 2,000 3,000 4,000 5,000 6,000 7,000 ResourceLoad (a) Total resource load versus total shared resource capacity 0 250 500 750 1,000 1,250 1,500 Time 0 500 1,000 1,500 2,000 2,500 ResourceLoad (b) Resource load server 0 0 250 500 750 1,000 1,250 1,500 Time 0 500 1,000 1,500 2,000 2,500 ResourceLoad (c) Resource load server 1 0 250 500 750 1,000 1,250 1,500 Time 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 ResourceLoad (d) Resource load server 2 Figure 4: Results of experiment 1 in a static 3-server environment averaged over 100 repetitions. all tasks. The standard deviation of the resource loads are small for each server, which indicates that our distributed approach find stable solutions in almost every run.
This experiment used algorithm 2 for the selection of the active server. We also ran the same experiment with the most free resources selection mechanism to select the active server. The resource allocation for each server is similar.
The absolute amount of free resources per server is almost the same.
Experiment 2 was conducted in a dynamic 3-server environment with a number of 750 agents. The amount of resources of server 0 and server 1 changes periodically, while the total amount of available resources remains constant.
Server 0 has an initial capacity of 1000 units, server 1 start with a capacity of 4000 units. The change in capacity starts after 150 time units, which is approximately the end of the learning phase. Figure 5 (b, c, d) shows the behaviour of our self-organising resource allocation in this environment. All agents use the deterministic most free resources selection mechanism to select the active server. It can bee seen in Fig. 5(b) and 5(c) that the number of allocated resources to server 0 and server 1 changes periodically with the amount of provided resources. This shows that agents can sense available resources in this dynamic environment and are able to adapt to those changes. The resource load development of server 2 (see Fig. 5(d)) shows a periodic change because some agent try to be allocated tasks to this server in case their previously favoured server reduce the amount of shared resources. The total resource load of all shared resources is constant over the experiments, which indicates the all agents allocate their tasks to one of the shared resource (comp. Fig. 4(a)).
In this paper a self-organising distributed resource allocation technique for multi-agent systems was presented. We enable agents to select the execution platform for their tasks themselves before each execution at run-time. In our approach the agents compete for an allocation at one of the 0 500 1,000 1,500 2,000 Time 0 2,500 5,000 7,500 ResourceLoad (a) Total resource load versus total shared resource capacity 0 500 1,000 1,500 2,000 Time 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 ResourceLoad (b) Resource load server 1 0 500 1,000 1,500 2,000 Time 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 ResourceLoad (c) Resource load server 2 0 500 1,000 1,500 2,000 Time 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 ResourceLoad (d) Resource load server 3 Figure 5: Results of experiment 2 in a dynamic server environment averaged over 100 repetitions. available shared resource. Agents sense their server environment and adopt their action to compete more efficient in the new created environment. This process is adaptive and has a strong feedback as allocation decisions influence indirectly decisions of other agents. The resource allocation is a purely emergent effect. Our mechanism demonstrates that resource allocation can be done by the effective competition of individual and autonomous agents. Neither do they need coordination or information from a higher authority nor is an additional direct communication between agents required.
This mechanism was inspired by inductive reasoning and bounded rationality principles which enables the agents" adaptation of their strategies to compete effectively in a dynamic environment. In the case of a server becomes unavailable, the agents can adapt quickly to this new situation by exploring new resources or remain at the home server if an allocation is not possible. Especially in dynamic and scalable environments such as grid systems, a robust and distributed mechanism for resource allocation is required.
Our self-organising resource allocation approach was evaluated with a number of simulation experiments in a dynamic environment of agents and server resources. The presented results for this new approach for strategic migration optimisation are very promising and justify further investigation in a real multi-agent system environment.
It is a distributed, scalable and easy-to-understand policy for the regulation of supply and demand of resources.
All control is implemented in the agents. A simple decision mechanism based on different beliefs of the agent creates an emergent behaviour that leads to effective resource allocation. This approach can be easily extended or supported by resource balancing/queuing mechanisms provided by resources.
Our approach adapts to changes in the environment but it is not evolutionary. There is no discovery of new strategies by the agents. The set of predictors stays the same over the whole life. In fact, we believe that this could further improve the system"s behaviour over a long term period and could be 80 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) investigated in the future. The evolution would be very slow and selective and will not influence the system behaviour in a short-term period that is covered by our experimental results.
In the near future we will investigate if an automatic adaptation of the decay rate of historical information our algorithm is possible and can improve the resource allocation performance. The decay rate is currently predefined and must be altered manually depending on the environment.
A large number of shared resources requires older historical information to avoid a too frequently resources exploration.
In contrast, a dynamic environment with varying capacities requires more up-to-date information to make more reliable predictions.
We are aware of the long learning phase in environments with a large number of shared resources known by each agent. In the case that more resources are requested by agents than shared resources are provided by all servers, all agents will randomly explore all known servers. This process of acquiring resource load information about all servers can take a long time in the case that no not enough shared resources for all tasks are provided. In the worst case, by the time for exploring all servers, historical information of some servers could be already outdated and the exploration starts again. In this situation, it is difficult for an agent to efficiently gather historical information about all remote servers. This issue needs more investigation in the future.

Notions like time, knowledge, and beliefs are very important for analyzing the behavior of agents and multi-agent systems. In this paper, we extend modal logics of time and knowledge with a concept of plausible behavior: this notion is added to the language of CTLK [19], which is a straightforward combination of the branching-time temporal logic CTL [4, 3] and standard epistemic logic [9, 5].
In our approach, plausibility can be seen as a temporal property of behaviors. That is, some behaviors of the system can be assumed plausible and others implausible, with the underlying idea that the latter should perhaps be ignored in practical reasoning about possible future courses of action. Moreover, behaviors can be formally understood as temporal paths in the Kripke structure modeling a multiagent system. As a consequence, we obtain a language to reason about what can (or must) plausibly happen. We propose a particular notion of beliefs (inspired by [20, 7]), defined in terms of epistemic relations and plausibility. The main intuition is that beliefs are facts that an agent would know if he assumed that only plausible things could happen.
We believe that humans use such a concept of plausibility and practical beliefs quite often in their everyday reasoning. Restricting one"s reasoning to plausible possibilities is essential to make the reasoning feasible, as the space of all possibilities is exceedingly large in real life. We investigate some important properties of plausibility, knowledge, and belief in this new framework. In particular, we show that knowledge is an S5 modality, and that beliefs satisfy axioms K45 in general, and KD45 for the class of plausibly serial models. Finally, we show that the relationship between knowledge and belief for plausibly serial models is natural and reflects the initial intuition well. We also show how plausibility assumptions can be specified in the object language via a plausibility update operator, and we study properties of such updates. Finally, we show that model checking of the new logic is no more complex than model checking CTL and CTLK.
Our ultimate goal is to come up with a logic that allows the study of strategies, time, knowledge, and plausible/rational behavior under both perfect and imperfect information. As combining all these dimensions is highly nontrivial (cf. [12, 14]) it seems reasonable to split this task.
While this paper deals with knowledge, plausibility, and belief, the companion paper [11] proposes a general framework for multi-agent systems that regard game-theoretical rationality criteria like Nash equilibrium, Pareto optimality, etc.
The latter approach is based on the more powerful logic ATL [1].
The paper is structured as follows. Firstly, we briefly present branching-time logic with knowledge, CTLK. In Section 3 we present our approach to plausibility and formally define CTLK with plausibility. We also show how 582 978-81-904262-7-5 (RPS) c 2007 IFAAMAS temporal formulae can be used to describe plausible paths, and we compare our logic with existing related work. In Section 4, properties of knowledge, belief, and plausibility are explored. Finally, we present verification complexity results for CTLKP in Section 5.
In this paper we develop a framework for agents" beliefs about how the world can (or must) evolve. Thus, we need a notion of time and change, plus a notion of what the agents are supposed to know in particular situations. CTLK [19] is a straightforward combination of the computation tree logic CTL [4, 3] and standard epistemic logic [9, 5].
CTL includes operators for temporal properties of systems: i.e., path quantifier E (there is a path), together with temporal operators: f(in the next state), 2 (always from now on) and U (until).1 Every occurrence of a temporal operator is preceded by exactly one path quantifier in CTL (this variant of the language is sometimes called vanilla CTL). Epistemic logic uses operators for representing agents" knowledge: Kaϕ is read as agent a knows that ϕ.
Let Π be a set of atomic propositions with a typical element p, and Agt = {1, ..., k} be a set of agents with a typical element a. The language of CTLK consists of formulae ϕ, given as follows: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Eγ | Kaϕ γ ::= fϕ | 2 ϕ | ϕU ϕ.
We will sometimes refer to formulae ϕ as (vanilla) state formulae and to formulae γ as (vanilla) path formulae.
The semantics of CTLK is based on Kripke models M = Q, R, ∼1, ..., ∼k, π , which include a nonempty set of states Q, a state transition relation R ⊆ Q × Q, epistemic indistinguishability relations ∼a⊆ Q × Q (one per agent), and a valuation of propositions π : Π → P(Q). We assume that relation R is serial and that all ∼a are equivalence relations.
A path λ in M refers to a possible behavior (or computation) of system M, and can be represented as an infinite sequence of states that follow relation R, that is, a sequence q0q1q2... such that qiRqi+1 for every i = 0, 1, 2, ... We denote the ith state in λ by λ[i]. The set of all paths in M is denoted by ΛM (if the model is clear from context, M will be omitted). A q-path is a path that starts from q, i.e., λ[0] = q. A q-subpath is a sequence of states, starting from q, which is a subpath of some path in the model, i.e. a sequence q0q1... such that q = q0 and there are q0 , ..., qi such that q0 ...qi q0q1... ∈ ΛM.2 The semantics of CTLK is defined as follows: M, q |= p iff q ∈ π(p); M, q |= ¬ϕ iff M, q |= ϕ; M, q |= ϕ ∧ ψ iff M, q |= ϕ and M, q |= ψ; M, q |= E fϕ iff there is a q-path λ such that M, λ[1] |= ϕ; M, q |= E2 ϕ iff there is a q-path λ such that M, λ[i] |= ϕ for every i ≥ 0; 1 Additional operators A (for every path) and ♦ (sometime in the future) are defined in the usual way. 2 For CTLK models, λ is a q-subpath iff it is a q-path. It will not always be so when plausible paths are introduced.
M, q |= EϕU ψ iff there is a q-path λ and i ≥ 0 such that M, λ[i] |= ψ, and M, λ[j] |= ϕ for every 0 ≤ j < i; M, q |= Kaϕ iff M, q |= ϕ for every q such that q ∼a q .
WITH PLAUSIBILITY AND BELIEFS In this section we discuss the central concept of this paper, i.e. the concept of plausibility. First, we outline the idea informally. Then, we extend CTLK with the notion of plausibility by adding plausible path operators Pl a and physical path operator Ph to the logic. Formula Pl aϕ has the intended meaning: according to agent a, it is plausible that ϕ holds; formula Ph ϕ reads as: ϕ holds in all physically possible scenarios (i.e., even in implausible ones). The plausible path operator restricts statements only to those paths which are defined to be sensible, whereas the physical path operator generates statements about all paths that may theoretically occur. Furthermore, we define beliefs on top of plausibility and knowledge, as the facts that an agent would know if he assumed that only plausible things could happen. Finally, we discuss related work [7, 8, 20, 18, 16], and compare it with our approach.
It is well known how knowledge (or beliefs) can be modeled with Kripke structures. However, it is not so obvious how we can capture knowledge and beliefs in a sensible way in one framework. Clearly, there should be a connection between these two notions. Our approach is to use the notion of plausibility for this purpose. Plausibility can serve as a primitive concept that helps to define the semantics of beliefs, in a similar way as indistinguishability of states (represented by relation ∼a) is the semantic concept that underlies knowledge. In this sense, our work follows [7, 20]: essentially, beliefs are what an agent would know if he took only plausible options into account. In our approach, however, plausibility is explicitly seen as a temporal property.
That is, we do not consider states (or possible worlds) to be more plausible than others but rather define some behaviors to be plausible, and others implausible. Moreover, behaviors can be formally understood as temporal paths in the Kripke structure modeling a multi-agent system.
An actual notion of plausibility (that is, a particular set of plausible paths) can emerge in many different ways. It may result from observations and learning; an agent can learn from its observations and see specific patterns of events as plausible (a lot of people wear black shoes if they wear a suit). Knowledge exchange is another possibility (e.g., an agent a can tell agent b that player c always bluffs when he is smiling). Game theory, with its rationality criteria (undominated strategies, maxmin, Nash equilibrium etc.) is another viable source of plausibility assumptions. Last but not least, folk knowledge can be used to establish plausibilityrelated classifications of behavior (players normally want to win a game, people want to live).
In any case, restricting the reasoning to plausible possibilities can be essential if we want to make the reasoning feasible, as the space of all possibilities (we call them physical possibilities in the rest of the paper) is exceedingly large in real life. Of course, this does not exclude a more extensive analysis in special cases, e.g. when our plausibility assumptions do not seem accurate any more, or when the cost of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 583 inaccurate assumptions can be too high (as in the case of high-budget business decisions). But even in these cases, we usually do not get rid of plausibility assumptions completely - we only revise them to make them more cautious.3 To formalize this idea, we extend models of CTLK with sets of plausible paths and add plausibility operators Pl a, physical paths operator Ph , and belief operators Ba to the language of CTLK. Now, it is possible to make statements that refer to plausible paths only, as well as statements that regard all paths that may occur in the system.
In this section, we extend the logic of CTLK with plausibility; we call the resulting logic CTLKP. Formally, the language of CTLKP is defined as: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Eγ | Pl aϕ | Ph ϕ | Kaϕ | Baϕ γ ::= fϕ | 2 ϕ | ϕU ϕ.
For instance, we may claim it is plausible to assume that a shop is closed after the opening hours, though the manager may be physically able to open it at any time: Pl aA2 (late → ¬open) ∧ Ph E♦ (late ∧ open).
The semantics of CTLKP extends that of CTLK as follows. Firstly, we augment the models with sets of plausible paths. A model with plausibility is given as M = Q, R, ∼1, ..., ∼k, Υ1, ..., Υk, π , where Q, R, ∼1, ..., ∼k, π is a CTLK model, and Υa ⊆ ΛM is the set of paths in M that are plausible according to agent a. If we want to make it clear that Υa is taken from model M, we will write ΥM a . It seems worth emphasizing that this notion of plausibility is subjective and holistic. It is subjective because Υa represents agent a"s subjective view on what is plausible - and indeed, different agents may have different ideas on plausibility (i.e., Υa may differ from Υb). It is holistic because Υa represents agent a"s idea of the plausible behavior of the whole system (including the behavior of other agents).
Remark 1. In our models, plausibility is also global, i.e., plausibility sets do not depend on the state of the system.
Investigating systems, in which plausibility is relativized with respect to states (like in [7]), might be an interesting avenue of future work. However, such an approach - while obviously more flexible - allows for potentially counterintuitive system descriptions. For example, it might be the case that path λ is plausible in q = λ[0], but the set of plausible paths in q = λ[1] is empty. That is, by following plausible path λ we are bound to get to an implausible situation. But then, does it make sense to consider λ as plausible?
Secondly, we use a non-standard satisfaction relation |=P , which we call plausible satisfaction. Let M be a CTLKP 3 That is, when planning to open an industrial plant in the UK, we will probably consider the possibility of our main contractor taking her life, but we will still not take into account the possibilities of: an invasion of UFO, England being destroyed by a meteorite, Fidel Castro becoming the British Prime Minister etc. Note that this is fundamentally different from using a probabilistic model in which all these unlikely scenarios are assigned very low probabilities: in that case, they also have a very small influence on our final decision, but we must process the whole space of physical possibilities to evaluate the options. model and P ⊆ ΛM be an arbitrary subset of paths in M (not necessarily any ΥM a ). |=P restricts the evaluation of temporal formulae to the paths given in P only. The absolute satisfaction relation |= is defined as |=ΛM .
Let on(P) be the set of all states that lie on at least one path in P, i.e. on(P) = {q ∈ Q | ∃λ ∈ P∃i (λ[i] = q)}. Now, the semantics of CTLKP can be given through the following clauses: M, q |=P p iff q ∈ π(p); M, q |=P ¬ϕ iff M, q |=P ϕ; M, q |=P ϕ ∧ ψ iff M, q |=P ϕ and M, q |=P ψ; M, q |=P E fϕ iff there is a q-subpath λ ∈ P such that M, λ[1] |=P ϕ; M, q |=P E2 ϕ iff there is a q-subpath λ ∈ P such that M, λ[i] |=P ϕ for every i ≥ 0; M, q |=P EϕU ψ iff there is a q-subpath λ ∈ P and i ≥ 0 such that M, λ[i] |=P ψ, and M, λ[j] |=P ϕ for every
M, q |=P Pl aϕ iff M, q |=Υa ϕ; M, q |=P Ph ϕ iff M, q |= ϕ; M, q |=P Kaϕ iff M, q |= ϕ for every q such that q ∼a q ; M, q |=P Baϕ iff for all q ∈ on(Υa) with q ∼a q , we have that M, q |=Υa ϕ.
One of the main reasons for using the concept of plausibility is that we want to define agents" beliefs out of more primitive concepts - in our case, these are plausibility and indistinguishability - in a way analogous to [20, 7]. If an agent knows that ϕ, he must be sure about it. However, beliefs of an agent are not necessarily about reliable facts.
Still, they should make sense to the agent; if he believes that ϕ, then the formula should at least hold in all futures that he envisages as plausible. Thus, beliefs of an agent may be seen as things known to him if he disregards all non-plausible possibilities.
We say that ϕ is M-true (M |= ϕ) if M, q |= ϕ for all q ∈ QM. ϕ is valid (|= ϕ) if M |= ϕ for all models M. ϕ is M-strongly true (M |≡ ϕ) if M, q |=P ϕ for all q ∈ QM and all P ⊆ ΛM. ϕ is strongly valid ( |≡ ϕ) if M |≡ ϕ for all models M.
Proposition 2. Strong truth and strong validity imply truth and validity, respectively. The reverse does not hold.
Ultimately, we are going to be interested in normal (not strong) validity, as parameterizing the satisfaction relation with a set P is just a technical device for propagating sets of plausible paths Υa into the semantics of nested formulae.
The importance of strong validity, however, lies in the fact that |≡ ϕ ↔ ψ makes ϕ and ψ completely interchangeable, while the same is not true for normal validity.
Proposition 3. Let Φ[ϕ/ψ] denote formula Φ in which every occurrence of ψ was replaced by ϕ. Also, let |≡ ϕ ↔ ψ.
Then for all M, q, P: M, q |=P Φ iff M, q |=P Φ[ϕ/ψ] (in particular, M, q |= Φ iff M, q |= Φ[ϕ/ψ]).
Note that |= ϕ ↔ ψ does not even imply that M, q |= Φ iff M, q |= Φ[ϕ/ψ].
Figure 1: Guessing Robots game Example 1 (Guessing Robots). Consider a simple game with two agents a and b, shown in Figure 1. First, a chooses a real number r ∈ [0, 1] (without revealing the number to b); then, b chooses a real number r ∈ [0, 1].
The agents win the game (and collect EUR 1, 000, 000) if both chose 1, otherwise they lose. Formally, we model the game with a CTLKP model M, in which the set of states Q includes qs for the initial situation, states qr, r ∈ [0, 1], for the situations after a has chosen number r, and final states qw, ql for the winning and the losing situation, respectively. The transition relation is as follows: qsRqr and qrRql for all r ∈ [0, 1]; q1Rqw, qwRqw, and qlRql. Moreover, π(one) = {q1} and π(win) = {qw}. Player a has perfect information in the game (i.e., q ∼a q iff q = q ), but player b does not distinguish between states qr (i.e., qr ∼b qr for all r, r ∈ [0, 1]). Obviously, the only sensible thing to do for both agents is to choose 1 (using game-theoretical vocabulary, these strategies are strongly dominant for the respective players). Thus, there is only one plausible course of events if we assume that our players are rational, and hence Υa = Υb = {qsq1qwqw . . .}.
Note that, in principle, the outcome of the game is uncertain: M, qs |= ¬A♦ win∧¬A2 ¬win. However, assuming rationality of the players makes it only plausible that the game must end up with a win: M, qs |= Pla A♦ win ∧ Plb A♦ win, and the agents believe that this will be the case: M, qs |= BaA♦ win ∧ BbA♦ win. Note also that, in any of the states qr, agent b believes that a (being rational) has played 1: M, qr |= Bbone for all r ∈ [0, 1].
So far, we have assumed that sets of plausible paths are somehow given in models. In this section we present a dynamic approach where an actual notion of plausibility can be specified in the object language. Note that we want to specify (usually infinite) sets of infinite paths, and we need a finite representation of these structures. One logical solution is given by using path formulae γ. These formulae describe properties of paths; therefore, a specific formula can be used to characterize a set of paths. For instance, think about a country in Africa where it has never snowed. Then, plausible paths might be defined as ones in which it never snows, i.e., all paths that satisfy 2 ¬snows. Formally, let γ be a CTLK path formula. We define |γ|M to be the set of paths that satisfy γ in model M: | fϕ|M = {λ | M, λ[1] |= ϕ} |2 ϕ|M = {λ | ∀i (M, λ[i] |= ϕ)} |ϕ1U ϕ2|M = {λ | ∃i ` M, λ[i] |= ϕ2 ∧ ∀j(0 ≤ j < i ⇒ M, λ[j] |= ϕ1) ´ }.
Moreover, we define the plausible paths model update as follows. Let M = Q, R, ∼1, ..., ∼k, Υ1, ..., Υk, π be a CTLKP model, and let P ⊆ ΛM be a set of paths. Then Ma,P = Q, R, ∼1, ..., ∼k, Υ1, ..., Υa−1, P, Υa+1, ..., Υk, π denotes model M with a"s set of plausible paths reset to P.
Now we can extend the language of CTLKP with formulae (set-pla γ)ϕ with the intuitive reading: suppose that γ exactly characterizes the set of plausible paths, then ϕ holds, and formal semantics given below: M, q |=P (set-pla γ)ϕ iff Ma,|γ|M , q |=P ϕ.
We observe that this update scheme is similar to the one proposed in [13].
Several modal notions of plausibility were already discussed in the existing literature [7, 8, 20, 18, 16]. In these papers, like in ours, plausibility is used as a primitive semantic concept that helps to define beliefs on top of agents" knowledge. A similar idea was introduced by Moses and Shoham in [18]. Their work preceded both [7, 8] and [20]and although Moses and Shoham do not explicitly mention the term plausibility, it seems appropriate to summarize their idea first.
Moses and Shoham: Beliefs as Conditional Knowledge In [18], beliefs are relativized with respect to a formula α (which can be seen as a plausibility assumption expressed in the object language). More precisely, worlds that satisfy α can be considered as plausible. This concept is expressed via symbols Bα i ϕ; the index i ∈ {1, 2, 3} is used to distinguish between three different implementations of beliefs. The first version is given by Bα
A drawback of this version is that if α is false, then everything will be believed with respect to α. The second version overcomes this problem: Bα
only believed if it is known that ϕ follows from assumption α, and ϕ must be known if assumption α is known to be false.
Finally, Bα
known to be false, nothing should be believed with respect to α. The strength of these different notions is given as follows: Bα
belief is strongly connected to knowledge in the sense that belief is knowledge with respect to a given assumption.
Friedman and Halpern: Plausibility Spaces The work of Friedman and Halpern [7] extends the concepts of knowledge and belief with an explicit notion of plausibility; i.e., some worlds are more plausible for an agent than others. To implement this idea, Kripke models are extended with function P which assigns a plausibility space P(q, a) = (Ω(q,a), (q,a)) to every state, or more generally every possible world q, and agent a. The plausibility space 4 Unlike in most approaches, K is interpreted over all worlds and not only over the indistinguishable worlds.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 585 is just a partially ordered subset of states/worlds; that is,
Ω(q, a) ⊆ Q, and (q,a)⊆ Q ×Q is a reflexive and transitive relation. Let S, T ⊆ Ω(q,a) be finite subsets of states; now,
T is defined to be plausible given S with respect to P(q, a), denoted by S →P (q,a) T, iff all minimal points/states in S (with respect to (q,a)) are also in T.5 Friedman and Halpern"s view to modal plausibility is closely related to probability and, more generally, plausibility measures.
Logics of plausibility can be seen as a qualitative description of agents preferences/knowledge; logics of probability [6, 15], on the other hand, offer a quantitative description.
The logic from [7] is defined by the following grammar: ϕ ::= p | ϕ∧ϕ | ¬ϕ | Kaϕ | ϕ →a ϕ, where the semantics of all operators except →a is given as usual, and formulae ϕ →a ψ have the meaning that ψ is true in the most plausible worlds in which ϕ holds. Formally, the semantics for →a is given as: M, q |= ϕ →a ψ iff Sϕ P (q,a) →P(q,a) Sψ P (q,a), where Sϕ (q,a) = {q ∈ Ω(q,a) | M, q |= ϕ} are the states in Ω(q,a) that satisfy ϕ. The idea of defining beliefs is given by the assumption that an agent believes in something if he knows that it is true in the most plausible worlds of Ω(q,a); formally, this can be stated as Baϕ ≡ Ka( →a ϕ).
Friedman and Halpern have shown that the KD45 axioms are valid for operator Ba if plausibility spaces satisfy consistency (for all states q ∈ Q it holds that Ω(q,a) ⊆ { q ∈ Q | q ∼a q }) and normality (for all states q ∈ Q it holds that Ω(q,a) = ∅).6 A temporal extension of the language (mentioned briefly in [7], and discussed in more detail in [8]) uses the interpreted systems approach [10, 5]. A system R is given by runs, where a run r : N → Q is a function from time moments (modeled by N) to global states, and a time point (r, i) is given by a time point i ∈ N and a run r. A global state is a combination of local states, one per agent.
An interpreted system M = (R, π) is given by a system R and a valuation of propositions π. Epistemic relations are defined over time points, i.e., (r , m ) ∼a (r, m) iff agent a"s local states ra(m ) and ra(m) of (r , m ) and (r, m) are equal. Formulae are interpreted in a straightforward way with respect to interpreted systems, e.g. M, r, m |= Kaϕ iff M, r , m |= ϕ for all (r , m ) ∼a (r, m). Now, these are time points that play the role of possible worlds; consequently, plausibility spaces P(r,m,a) are assigned to each point (r, m) and agent a.
Su et al.: KBC Logic Su et al. [20] have developed a multi-modal, computationally grounded logic with modalities K, B, and C (knowledge, belief, and certainty). The computational model consists of (global) states q = (qvis , qinv , qper , Qpls ) where the environment is divided into a visible (qvis ) and an invisible part (qinv ), and qper captures the agent"s perception of the visible part of the environment. External sources may provide the agent with information about the invisible part of a state, which results in a set of states Qpls that are plausible for the agent.
Given a global state q, we additionally define V is(q) = qvis , Inv(q) = qinv , Per(q) = qper , and Pls(q) = Qpls . The 5 When there are infinite chains . . . q3 q2 a q1, the definition is much more sophisticated. An interested reader is referred to [7] for more details. 6 Note that this normality is essentially seriality of states wrt plausibility spaces. semantics is given by an extension of interpreted systems [10, 5], here, it is called interpreted KBC systems. KBC formulae are defined as ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Kϕ | Bϕ | Cϕ.
The epistemic relation ∼vis is captured in the following way: (r, i) ∼vis (r , i ) iff V is(r(i)) = V is(r (i )). The semantic clauses for belief and certainty are given below.
M, r, i |= Bϕ iff M, r , i |= ϕ for all (r , i ) with V is(r (i )) = Per(r(i)) and Inv(r (i )) ∈ Pls(r(i)) M, r, i |= Cϕ iff M, r , i |= ϕ for all (r (i )) with V is(r (i )) = Per(r(i)) Thus, an agent believes ϕ if, and only if, ϕ is true in all states which look like what he sees now and seem plausible in the current state. Certainty is stronger: if an agent is certain about ϕ, the formula must hold in all states with a visible part equal to the current perception, regardless of whether the invisible part is plausible or not.
The logic does not include temporal formulae, although it might be extended with temporal operators, as time is already present in KBC models.
What Are the Differences to Our Logic?
In our approach, plausibility is explicitly seen as a temporal property, i.e., it is a property of temporal paths rather than states. In the object language, this is reflected by the fact that plausibility assumptions are specified through path formulae. In contrast, the approach of [18] and [20] is static: not only the logics do not include operators for talking about time and/or change, but these are states that are assumed plausible or not in their semantics.
The differences to [7, 8] are more subtle. Firstly, the framework of Friedman and Halpern is static in the sense that plausibility is taken as a property of (abstract) possible worlds. This formulation is flexible enough to allow for incorporating time; still, in our approach, time is inherent to plausibility rather than incidental.
Secondly, our framework is more computationally oriented.
The implementation of temporal plausibility in [7, 8] is based on the interpreted systems approach with time points (r, m) being subject to plausibility. As runs are included in time points, they can also be defined plausible or implausible.7 However, it also means that time points serve the role of possible worlds in the basic formulation, which yields Kripke structures with uncountable possible world spaces in all but the most trivial cases.
Thirdly, [7, 8] build on linear time: a run (more precisely, a time moment (r, m)) is fixed when a formula is interpreted.
In contrast, we use branching time with explicit quantification over temporal paths.8 We believe that branching time is more suitable for non-deterministic domains (cf. e.g. [4]), of which multi-agent systems are a prime example. Note that branching time makes our notion of belief different from Friedman and Halpern"s. Most notably, property Kϕ → Bϕ is valid in their approach, but not in ours: an agent may 7 Friedman and Halpern even briefly mention how plausibility of runs can be embedded in their framework. 8 To be more precise, time in [7] does implicitly branch at epistemic states. This is because (r, m) ∼a (r , m ) iff a"s local state corresponding to both time points is the same (ra(m) = ra(m )). In consequence, the semantics of Kaϕ can be read as for every run, and every moment on this run that yields the same local state as now, ϕ holds.
know that some course of events is in principle possible, without believing that it can really become the case (see Section 4.2). As Proposition 13 suggests, such a subtle distinction between knowledge and beliefs is possible in our approach because branching time logics allow for existential quantification over runs.
Fourthly, while Friedman and Halpern"s models are very flexible, they also enable system descriptions that may seem counterintuitive. Suppose that (r, m) is plausible in itself (formally: (r, m) is minimal wrt (r,m,a)), but (r, m + 1) is not plausible in (r, m + 1). This means that following the plausible path makes it implausible (cf. Remark 1), which is even stranger in the case of linear time. Combining the argument with computational aspects, we suggest that our approach can be more natural and straightforward for many applications.
Last but not least, our logic provides a mechanism for specifying (and updating) sets of plausible paths in the object language. Thus, plausibility sets can be specified in a succinct way, which is another feature that makes our framework computation-friendly. The model checking results from Section 5 are especially encouraging in this light.
BELIEFS IN CTLKP In this section we study some relevant properties of plausibility, knowledge, and beliefs; in particular, axioms KDT45 are examined. But first, we identify two important subclasses of models with plausibility.
A CTLKP model is plausibly serial (or p-serial) for agent a if every state of the system is part of a plausible path according to a, i.e. on(Υa) = Q. As we will see further, a weaker requirement is sometimes sufficient. We call a model weakly p-serial if every state has at least one indistinguishable counterpart which lies on a plausible path, i.e. for each q ∈ Q there is a q ∈ Q such that q ∼a q and q ∈ on(Υa).
Obviously, p-seriality implies weak p-seriality. We get the following characterization of both model classes.
Proposition 4. M is plausibly serial for agent a iff formula Pl aE f is valid in M. M is weakly p-serial for agent a iff ¬KaPl aA f⊥ is valid in M.
Theorem 5. Axioms K, D, 4, and 5 for knowledge are strongly valid, and axiom T is valid. That is, modalities Ka form system S5 in the sense of normal validity, and KD45 in the sense of strong validity.
We do not include proofs here due to lack of space. The interested reader is referred to [2], where detailed proofs are given.
Proposition 6. Axioms K, 4, and 5 for beliefs are strongly valid. That is, we have: |≡ (Baϕ ∧ Ba(ϕ → ψ)) → Baψ, |≡ (Baϕ → BaBaϕ), and |≡ (¬Baϕ → Ba¬Baϕ).
The next proposition concerns the consistency axiom D: Baϕ → ¬Ba¬ϕ. It is easy to see that the axiom is not valid in general: as we have no restrictions on plausibility sets Υa, it may be as well that Υa = ∅. In that case we have Baϕ ∧ Ba¬ϕ for all formulae ϕ, because the set of states to be considered becomes empty. However, it turns out that D is valid for a very natural class of models.
Proposition 7. Axiom D for beliefs is not valid in the class of all CTLKP models. However, it is strongly valid in the class of weak p-serial models (and therefore also in the class of p-serial models).
Moreover, as one may expect, beliefs do not have to be always true.
Proposition 8. Axiom T for beliefs is not valid; i.e., |= (Baϕ → ϕ). The axiom is not even valid in the class of p-serial models.
Theorem 9. Belief modalities Ba form system K45 in the class of all models, and KD45 in the class of weakly plausibly serial models (in the sense of both normal and strong validity). Axiom T is not even valid for p-serial models.
First, we investigate the relationship between knowledge and plausibility/physicality operators. Then, we look at the interaction between knowledge and beliefs.
Proposition 10. Let ϕ be a CTLKP formula, and M be a CTLKP model. We have the following strong validities: (i) |≡ Pl aKaϕ ↔ Kaϕ (ii) |≡ Ph Kaϕ ↔ KaPh ϕ and |≡ KaPh ϕ ↔ Kaϕ We now want to examine the relationship between knowledge and belief. For instance, if agent a believes in something, he knows that he believes it. Or, if he knows a fact, he also believes that he knows it. On the other hand, for instance, an agent does not necessarily believe in all the things he knows. For example, we may know that an invasion from another galaxy is in principle possible (KaE♦ invasion), but if we do not take this possibility as plausible (¬Pl aE♦ invasion), then we reject the corresponding belief in consequence (¬BaE♦ invasion). Note that this property reflects the strong connection between belief and plausibility in our framework.
Proposition 11. The following formulae are strongly valid: (i) Baϕ → KaBaϕ, (ii) KaBaϕ → Baϕ, (iii) Kaϕ → BaKaϕ.
The following formulae are not valid: (iv) Baϕ → BaKaϕ, (v) Kaϕ → Baϕ The last invalidity is especially important: it is not the case that knowing something implies believing in it. This emphasizes that we study a specific concept of beliefs here.
Note that its specific is not due to the plausibility-based definition of beliefs. The reason lies rather in the fact that we investigate knowledge, beliefs and plausibility in a temporal framework, as Proposition 12 shows.
Proposition 12. Let ϕ be a CTLKP formula that does not include any temporal operators. Then Kaϕ → Baϕ is strongly valid, and in the class of p-serial models we have even that |≡ Kaϕ ↔ Baϕ.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 587 Moreover, it is important that we use branching time with explicit quantification over paths; this observation is formalized in Proposition 13.
Definition 1. We define the universal sublanguage of CTLK in a way similar to [21]: ϕu ::= p | ¬p | ϕu ∧ ϕu | ϕu ∨ ϕu | Aγu | Kaϕu, γu ::= fϕu | 2 ϕu | ϕuU ϕu.
We call such ϕu universal formulae, and γu universal path formulae.
Proposition 13. Let ϕu be a universal CTLK formula.
Then |≡ Kaϕu → Baϕu.
The following two theorems characterize the relationship between knowledge and beliefs: first for the class of p-serial models, and then, finally, for all models.
Theorem 14. The following formulae are strongly valid in the class of plausibly serial CTLKP models: (i) Baϕ ↔ KaPl aϕ, (ii) Kaϕ ↔ BaPh ϕ.
Theorem 15. Formula Baϕ ↔ KaPl a(E f → ϕ) is strongly valid.
Note that this characterization has a strong commonsense reading: believing in ϕ is knowing that ϕ plausibly holds in all plausibly imaginable situations.
The first notable property of plausibility update is that it influences only formulae in which plausibility plays a role, i.e. ones in which belief or plausibility modalities occur.
Proposition 16. Let ϕ be a CTLKP formula that does not include operators Pl a and Ba, and γ be a CTLKP path formula. Then, we have |≡ ϕ ↔ (set-pla γ)ϕ.
What can be said about the result of an update? At first sight, formula (set-pla γ)Pl aAγ seems a natural characterization; however, it is not valid. This is because, by leaving the other (implausible) paths out of scope, we may leave out of |γ| some paths that were needed to satisfy γ (see the example in Section 4.2). We propose two alternative ways out: the first one restricts the language of the update similarly to [21]; the other refers to physical possibilities, in a way analogous to [13].
Proposition 17. The CTLKP formula (set-pla γ)Pl aAγ is not valid. However, we have the following validities: (i) |≡ (set-pla γu)Pl aAγu, where γu is a universal CTLK path formula from Definition 1. (ii) If ϕ, ϕ1, ϕ2 are arbitrary CTLK formulae, then: |≡ (set-pla fϕ)Pl aA f(Ph ϕ), |≡ (set-pla 2 ϕ)Pl aA2 (Ph ϕ), and |≡ (set-pla ϕ1U ϕ2)Pl aA(Ph ϕ1)U (Ph ϕ2).
TIME AND BELIEFS In this section we report preliminary results on model checking CTLKP formulae. Clearly, verifying CTLKP properties directly against models with plausibility does not make much sense, since these models are inherently infinite; what we need is a finite representation of plausibility sets.
One such representation has been discussed in Section 3.3: plausibility sets can be defined by path formulae and the update operator (set-pla γ).
We follow this idea here, studying the complexity of model checking CTLKP formulae against CTLK models (which can be seen as a compact representation of CTLKP models in which all the paths are assumed plausible), with the underlying idea that plausibility sets, when needed, must be defined explicitly in the object language. Below we sketch an algorithm that model-checks CTLKP formulae in time linear wrt the size of the model and the length of the formula. This means that we have extended CTLK to a more expressive language with no computational price to pay.
First of all, we get rid of the belief operators (due to Theorem 15), replacing every occurrence of Baϕ with KaPl a(E f → ϕ). Now, let −→γ = γ1, ..., γk be a vector of vanilla path formulae (one per agent), with the initial vector −→γ0 = , ..., , and −→γ [γ /a] denoting vector −→γ , in which −→γ [a] is replaced with γ . Additionally, we define −→γ [0] = . We translate the resulting CTLKP formulae to ones without plausibility via function tr(ϕ) = tr−→γ0,0(ϕ), defined as follows: tr−→γ ,i(p) = p, tr−→γ ,i(ϕ1 ∧ ϕ2) = tr−→γ ,i(ϕ1) ∧ tr−→γ ,i(ϕ2), tr−→γ ,i(¬ϕ) = ¬tr−→γ ,i(ϕ), tr−→γ ,i(Kaϕ) = Ka tr−→γ ,0(ϕ), tr−→γ ,i(Pla ϕ) = tr−→γ ,a(ϕ), tr−→γ ,i((set-pla γ )ϕ) = tr−→γ [γ /a],i(ϕ), tr−→γ ,i(Ph ϕ) = tr−→γ ,0(ϕ), tr−→γ ,i( fϕ) = ftr−→γ ,i(ϕ), tr−→γ ,i(2 ϕ) = 2 tr−→γ ,i(ϕ), tr−→γ ,i(ϕ1U ϕ2) = tr−→γ ,i(ϕ1)U tr−→γ ,i(ϕ2), tr−→γ ,i(Eγ ) = E(−→γ [i] ∧ tr−→γ ,i(γ )).
Note that the resulting sentences belong to the logic of CTLK+, that is CTL+ (where each path quantifier can be followed by a Boolean combination of vanilla path formulae)9 with epistemic modalities. The following proposition justifies the translation.
Proposition 18. For any CTLKP formula ϕ without Ba, we have that M, q |=CTLKP ϕ iff M, q |=CTLK+ tr(ϕ).
In general, model checking CTL+ (and also CTLK+) is ΔP
combinations of path subformulae are always conjunctions of at most two non-negated elements, which allows us to propose the following model checking algorithm. First, subformulae are evaluated recursively: for every subformula ψ of ϕ, the set of states in M that satisfy ψ is computed and labeled with a new proposition pψ. Now, it is enough to define checking M, q |= ϕ for ϕ in which all (state) subformulae are propositions, with the following cases: Case M, q |= E(2 p ∧ γ): If M, q |= p, then return no.
Otherwise, remove from M all the states that do not satisfy p (yielding a sparser model M ), and check the CTL formula Eγ in M , q with any CTL model-checker.
Case M, q |= E( fp ∧ γ): Create M by adding a copy q of state q, in which only the transitions to states satisfying p are kept (i.e., M, q |= r iff M, q |= r; and q Rq iff qRq and M, q |= p). Then, check Eγ in M , q . 9 For the semantics of CTL+, and discussion of model checking complexity, cf. [17].
Case M, q |= E(p1U p2 ∧ p3U p4): Note that this is equivalent to checking E(p1 ∧ p3)U (p2 ∧ Ep3U p4) ∨ E(p1 ∧ p3)U (p4 ∧ Ep1U p2), which is a CTL formula.
Other cases: The above cases cover all possible formulas that begin with a path quantifier. For other cases, standard CTLK model checking can be used.
Theorem 19. Model checking CTLKP against CTLK models is PTIME-complete, and can be done in time O(ml), where m is the number of transitions in the model, and l is the length of the formula to be checked. That is, the complexity is no worse than for CTLK itself.
In this paper a notion of plausible behavior is considered, with the underlying idea that implausible options should be usually ignored in practical reasoning about possible future courses of action. We add the new notion of plausibility to the logic of CTLK [19], and obtain a language which enables reasoning about what can (or must) plausibly happen.
As a technical device to define the semantics of the resulting logic, we use a non-standard satisfaction relation |=P that allows to propagate the current set of plausible paths into subformulae. Furthermore, we propose a non-standard notion of beliefs, defined in terms of indistinguishability and plausibility. We also propose how plausibility assumptions can be specified in the object language via a plausibility update operator (in a way similar to [13]).
We use this new framework to investigate some important properties of plausibility, knowledge, beliefs, and updates.
In particular, we show that knowledge is an S5 modality, and that beliefs satisfy axioms K45 in general, and KD45 for the class of plausibly serial models. We also prove that believing in ϕ is knowing that ϕ plausibly holds in all plausibly possible situations. That is, the relationship between knowledge and beliefs is very natural and reflects the initial intuition precisely. Moreover, the model checking results from Section 5 show that verification for CTLKP is no more complex than for CTL and CTLK.
We would like to stress that we do not see this contribution as a mere technical exercise in formal logic. Human agents use a similar concept of plausibility and practical beliefs in their everyday reasoning in order to reduce the search space and make the reasoning feasible. As a consequence, we suggest that the framework we propose may prove suitable for modeling, design, and analysis resource-bounded agents in general.
We would like to thank Juergen Dix for fruitful discussions, useful comments and improvements.

Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution. In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.
Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation. Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand. However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.
Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.
Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]). Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments. However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation. In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience. Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.
Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples. Counterexamples offer the possibility of agents learning during the argumentation process. Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments. Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.
This paper presents a case-based approach to address both issues. The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation. We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases. In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments. Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.
The paper is structured as follows. Section 2 discusses the relation among argumentation, collaboration and learning. Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction. After that, Section 4 formally defines our argumentation framework. Sections 5 and 6 present our case-based preference relation and argument generation policies respectively. Later, Section 7 presents the argumentation protocol in our AMAL framework. After that, Section 8 presents an exemplification of the argumentation framework. Finally, Section 9 presents an empirical evaluation of our two main hypotheses. The paper closes with related work and conclusions sections.
AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance. In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.
Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.
Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.
In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance. An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.
In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand. Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process. Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.
In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way.
A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci. Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci. A case base Ci = {c1, ..., cm} is a collection of cases. Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.
In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes. In the following we will note the set of all the solution classes by S = {S1, ..., SK }. Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S.
In the following, we will use the terms problem and case description indistinctly. Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.
Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process. However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents). The next section addresses this issue.
Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user. The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.
Most of the existing work on explanation generation focuses on generating explanations to be provided to the user. However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents. We are interested in justifications since they can be used as arguments.
For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.
A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar. Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.
For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems). In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait. Thus, since only this attribute has been used, it is the only one appearing in the justification. The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same.
Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system. Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.
In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class. In the rest of the paper, we will use to denote the subsumption relation. In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1. When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1. A Justified Prediction is a tuple J = A, P,
S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P.
Justifications can have many uses for CBR systems [8, 9]. In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes.
COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct. In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate. In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D . Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.
A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D. In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.
A counterargument β is an argument offered in opposition to another argument α. In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D . In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.
A counterexample c is a case that contradicts an argument α.
Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.
By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.
However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples). In the following sections we will present these elements.
A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data). For that reason, we are going to define a preference relation over contradicting justified predictions based on cases. Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.
The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it. The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence. Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D. With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agent"s case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agent"s case base subsumed by justification α.D that do not belong to that solution class.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +   - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.
An agent estimates the confidence of an argument as: CAi (α) = Y Ai α
α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples. Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.
Notice that this correction follows the same idea than the Laplace correction to estimate probabilities. Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α
α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.
In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β).
In our framework, arguments are generated by the agents from cases, using learning methods. Any learning method able to provide a justified prediction can be used to generate arguments. For instance, decision trees and LID [2] are suitable learning methods.
Specifically, in the experiments reported in this paper agents use LID. Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.
For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification. In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent. The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge. Thus, the argument generated will be α = A1, P, hadromerida, D1 .
As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples. Let us explain how they can be generated.
An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai. Moreover, while generating such counterargument β, Ai expects that β is preferred over α. For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].
The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed. Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut. However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence. Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence. Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).
Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).
The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task. For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term. Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.
When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.
To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch. In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α. However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D. Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID. Moreover, Figure 4 shows the generation of a counterargument β1
1 (in Figure 3) that is a specialization of α0
Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.
Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy:
specific than α; if found, β is sent to the other agent as a counterargument of α.
of α. If a case c is found, then c is sent to the other agent as a counterexample of α.
argument α.
MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process. If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.
Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.
The AMAL protocol consists on a series of rounds. In the initial round, each agent states which is its individual prediction for P.
Then, at each round an agent can try to rebut the prediction made by any of the other agents. The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent. Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds). When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not. Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.
When all the agents have had the token once, the token returns to the first agent, and so on. If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends. Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).
At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α. An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.
We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.
The protocol is initiated because one of the agents receives a problem P to be solved. After that, the agent informs all the other agents about the problem P to solve, and the protocol starts:
and builds a justified prediction using its own CBR method.
Then, each agent Ai sends the performative assert(α0 i ) to the other agents. Thus, the agents know H0 = α0 i , ..., α0 n .
Once all the predictions have been sent the token is given to the first agent A1.
arguments in Ht agree. If they do, the protocol moves to step
counterexample or counterargument, the protocol also moves to step 5. Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1). Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj). If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj. Otherwise (i.e. CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj. In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj. The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2.
i , locally compares it against its own argument, αt j, by locally assessing their confidence. If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.
Otherwise (i.e. CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly. Any of the two situations start a new round t + 1,
Ai sends the token to the next agent, and the protocol moves back to state 2.
it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them. Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2.
the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction. The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered.
Let us consider a system composed of three agents A1, A2 and A3. One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it. For that reason, invites A2 and A3 to take part in the argumentation process. They accept the invitation, and the argumentation protocol starts.
Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents. Thus, all of them can compute H0 = α0 1, α0 2, α0
• α0
• α0
• α0
A1 starts owning the token and tries to generate counterarguments for α0
3, but does not succeed, however it has one counterexample c13 for α0
c13, α0 3) to A3. A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration. A3 comes up with the justified prediction α1
D4 , and broadcasts it to the rest of the agents with the message assert(α1 3). Thus, all of them know the new H1 = α0 1, α0 2, α1
Round 1 starts and A2 gets the token. A2 tries to generate counterarguments for α0
counterargument β1
counterargument is sent to A3 with the message rebut(β1
3).
Agent A3 receives the counterargument and assesses its local confidence. The result is that the individual confidence of the counterargument β1
does not accept the counterargument, and thus H2 = α0 1, α0 2, α1
Round 2 starts and A3 gets the token. A3 generates a counterargument β2
A2 with the message rebut(β2
2). Agent A2 receives the counterargument and assesses its local confidence. The result is that the local confidence of the counterargument β2
confidence of α0
informs the rest of the agents with the message assert(β2
that, H3 = α0 1, β2
At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P.
SPONGE 75 77 79 81 83 85 87 89 91
AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90
AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.
In this section we empirically evaluate the AMAL argumentation framework. We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set). The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes. In an experimental run, the data set is divided in 2 sets: the training set and the test set. The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents. In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.
The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.
Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).
Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).
Figure 5 shows the result of those experiments in the sponge and soybean data sets. Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown. For each number of agents, three bars are shown: individual, Voting, and AMAL. The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation. The results shown are the average of
Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving. Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account. We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.
For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%). Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set. The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions). These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).
Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process. Figure 6 shows the result of that experiment for the two data sets. Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).
Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation). For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience. The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication). In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication). Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect.
Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents. Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning. Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation. The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.
Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation. Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13]. Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.
In our framework we have addressed both argument selection and preference relations using a case-based approach.
In this paper we have presented an argumentation-based framework for multi-agent learning. Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments. The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.
The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.
Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents. Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication.
[1] Agnar Aamodt and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza. Lazy induction of descriptions for relational case-based learning. In ECML"2001, pages 13-24,
[3] Gerhard Brewka. Dynamic argument systems: A formal model of argumentation processes based on situation calculus. Journal of Logic and Computation, 11(2):257-282,
[4] Carlos I. Chesñevar and Guillermo R. Simari. Formalizing Defeasible Argumentation using Labelled Deductive Systems. Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi. Automatically selecting strategies for multi-case-base reasoning. In S. Craw and A. Preece, editors, ECCBR"2002, pages 204-219, Berlin,
[6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.
Knowledge and experience reuse through communications among competent (peer) agents. International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth. Collaborative case-based reasoning: Applications in personalized route planning. In I. Watson and Q. Yang, editors, ICCBR, number
[8] Santi Ontañón and Enric Plaza. Justification-based multiagent learning. In ICML"2003, pages 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón. The explanatory power of symbolic similarity in case-based reasoning. Artificial Intelligence Review, 24(2):145-161,
[10] David Poole. On the comparison of theories: Preferring the most specific explanation. In IJCAI-85, pages 144-147,
[11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.
Retrieval and reasoning in distributed case bases. Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik. Reaching agreements through argumentation: a logical model and implementation.
Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra. Agents that reason and negotiate by arguing. Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley. Explanation component of software systems. ACM CrossRoads, 5.1, 1998.

Roughly speaking, negotiation is a process aiming at finding some compromise or consensus between two or several agents about some matters of collective agreement, such as pricing products, allocating resources, or choosing candidates. Negotiation models have been proposed for the design of systems able to bargain in an optimal way with other agents for example, buying or selling products in ecommerce.
Different approaches to automated negotiation have been investigated, including game-theoretic approaches (which usually assume complete information and unlimited computation capabilities) [11], heuristic-based approaches which try to cope with these limitations [6], and argumentation-based approaches [2, 3, 7, 8, 9, 12, 13] which emphasize the importance of exchanging information and explanations between negotiating agents in order to mutually influence their behaviors (e.g. an agent may concede a goal having a small priority), and consequently the outcome of the dialogue.
Indeed, the two first types of settings do not allow for the addition of information or for exchanging opinions about offers.
Integrating argumentation theory in negotiation provides a good means for supplying additional information and also helps agents to convince each other by adequate arguments during a negotiation dialogue. Indeed, an offer supported by a good argument has a better chance to be accepted by an agent, and can also make him reveal his goals or give up some of them. The basic idea behind an argumentationbased approach is that by exchanging arguments, the theories of the agents (i.e. their mental states) may evolve, and consequently, the status of offers may change. For instance, an agent may reject an offer because it is not acceptable for it. However, the agent may change its mind if it receives a strong argument in favor of this offer.
Several proposals have been made in the literature for modeling such an approach. However, the work is still preliminary. Some researchers have mainly focused on relating argumentation with protocols. They have shown how and when arguments in favor of offers can be computed and exchanged. Others have emphasized on the decision making problem. In [3, 7], the authors argued that selecting an offer to propose at a given step of the dialogue is a decision making problem. They have thus proposed an argumentationbased decision model, and have shown how such a model can be related to the dialogue protocol.
In most existing works, there is no deep formal analysis of the role of argumentation in negotiation dialogues. It is not clear how argumentation can influence the outcome of the dialogue. Moreover, basic concepts in negotiation such as agreement (i.e. optimal solutions, or compromise) and concession are neither defined nor studied.
This paper aims to propose a unified and general framework for argumentation-based negotiation, in which the role of argumentation is formally analyzed, and where the existing systems can be restated. In this framework, a negotiation dialogue takes place between two agents on a set O of offers, whose structure is not known. The goal of a negotiation is to find among elements of O, an offer that satisfies more or less 967 978-81-904262-7-5 (RPS) c 2007 IFAAMAS the preferences of both agents. Each agent is supposed to have a theory represented in an abstract way. A theory consists of a set A of arguments whose structure and origin are not known, a function specifying for each possible offer in O, the arguments of A that support it, a non specified conflict relation among the arguments, and finally a preference relation between the arguments. The status of each argument is defined using Dung"s acceptability semantics. Consequently, the set of offers is partitioned into four subsets: acceptable, rejected, negotiable and non-supported offers. We show how an agent"s theory may evolve during a negotiation dialogue.
We define formally the notions of concession, compromise, and optimal solution. Then, we propose a protocol that allows agents i) to exchange offers and arguments, and ii) to make concessions when necessary. We show that dialogues generated under such a protocol terminate, and even reach optimal solutions when they exist.
This paper is organized as follows: Section 2 introduces the logical language that is used in the rest of the paper.
Section 3 defines the agents as well as their theories. In section 4, we study the properties of these agents" theories.
Section 5 defines formally an argumentation-based negotiation, shows how the theories of agents may evolve during a dialogue, and how this evolution may influence the outcome of the dialogue. Two kinds of outcomes: optimal solution and compromise are defined, and we show when such outcomes are reached. Section 6 illustrates our general framework through some examples. Section 7 compares our formalism with existing ones. Section 8 concludes and presents some perspectives. Due to lack of space, the proofs are not included. These last are in a technical report that we will make available online at some later time.
In what follows, L will denote a logical language, and ≡ is an equivalence relation associated with it.
From L, a set O = {o1, . . . , on} of n offers is identified, such that oi, oj ∈ O such that oi ≡ oj. This means that the offers are different. Offers correspond to the different alternatives that can be exchanged during a negotiation dialogue.
For instance, if the agents try to decide the place of their next meeting, then the set O will contain different towns.
Different arguments can be built from L. The set Args(L) will contain all those arguments. By argument, we mean a reason in believing or of doing something. In [3], it has been argued that the selection of the best offer to propose at a given step of the dialogue is a decision problem. In [4], it has been shown that in an argumentation-based approach for decision making, two kinds of arguments are distinguished: arguments supporting choices (or decisions), and arguments supporting beliefs. Moreover, it has been acknowledged that the two categories of arguments are formally defined in different ways, and they play different roles. Indeed, an argument in favor of a decision, built both on an agent"s beliefs and goals, tries to justify the choice; whereas an argument in favor of a belief, built only from beliefs, tries to destroy the decision arguments, in particular the beliefs part of those decision arguments. Consequently, in a negotiation dialogue, those two kinds of arguments are generally exchanged between agents. In what follows, the set Args(L) is then divided into two subsets: a subset Argso(L) of arguments supporting offers, and a subset Argsb(L) of arguments supporting beliefs. Thus, Args(L) = Argso(L) ∪ Argsb(L).
As in [5], in what follows, we consider that the structure of the arguments is not known.
Since the knowledge bases from which arguments are built may be inconsistent, the arguments may be conflicting too.
In what follows, those conflicts will be captured by the relation RL, thus RL ⊆ Args(L) × Args(L). Three assumptions are made on this relation: First the arguments supporting different offers are conflicting. The idea behind this assumption is that since offers are exclusive, an agent has to choose only one at a given step of the dialogue. Note that, the relation RL is not necessarily symmetric between the arguments of Argsb(L). The second hypothesis says that arguments supporting the same offer are also conflicting. The idea here is to return the strongest argument among these arguments. The third condition does not allow an argument in favor of an offer to attack an argument supporting a belief. This avoids wishful thinking. Formally: Definition 1. RL ⊆ Args(L) × Args(L) is a conflict relation among arguments such that: • ∀a, a ∈ Argso(L), s.t. a = a , a RL a • a ∈ Argso(L) and a ∈ Argsb(L) such that a RL a Note that the relation RL is not symmetric. This is due to the fact that arguments of Argsb(L) may be conflicting but not necessarily in a symmetric way. In what follows, we assume that the set Args(L) of arguments is finite, and each argument is attacked by a finite number of arguments.
REASONING MODELS In this section we define formally the negotiating agents, i.e. their theories, as well as the reasoning model used by those agents in a negotiation dialogue.
Agents involved in a negotiation dialogue, called negotiating agents, are supposed to have theories. In this paper, the theory of an agent will not refer, as usual, to its mental states (i.e. its beliefs, desires and intentions). However, it will be encoded in a more abstract way in terms of the arguments owned by the agent, a conflict relation among those arguments, a preference relation between the arguments, and a function that specifies which arguments support offers of the set O. We assume that an agent is aware of all the arguments of the set Args(L). The agent is even able to express a preference between any pair of arguments. This does not mean that the agent will use all the arguments of Args(L), but it encodes the fact that when an agent receives an argument from another agent, it can interpret it correctly, and it can also compare it with its own arguments. Similarly, each agent is supposed to be aware of the conflicts between arguments. This also allows us to encode the fact that an agent can recognize whether the received argument is in conflict or not with its arguments. However, in its theory, only the conflicts between its own arguments are considered.
Definition 2 (Negotiating agent theory). Let O be a set of n offers. A negotiating agent theory is a tuple A, F, , R, Def such that: • A ⊆ Args(L).
• F: O → 2A s.t ∀i, j with i = j, F(oi) ∩ F(oj) = ∅.
Let AO = ∪F(oi) with i = 1, . . . , n. • ⊆ Args(L) × Args(L) is a partial preorder denoting a preference relation between arguments. • R ⊆ RL such that R ⊆ A × A • Def ⊆ A × A such that ∀ a, b ∈ A, a defeats b, denoted a Def b iff: - a R b, and - not (b a) The function F returns the arguments supporting offers in O. In [4], it has been argued that any decision may have arguments supporting it, called arguments PRO, and arguments against it, called arguments CONS. Moreover, these two types of arguments are not necessarily conflicting. For simplicity reasons, in this paper we consider only arguments PRO. Moreover, we assume that an argument cannot support two distinct offers. However, it may be the case that an offer is not supported at all by arguments, thus F(oi) may be empty.
Example 1. Let O = {o1, o2, o3} be a set of offers. The following theory is the theory of agent i: • A = {a1, a2, a3, a4} • F(o1) = {a1}, F(o2) = {a2}, F(o3) = ∅. Thus, Ao = {a1, a2} • = {(a1, a2), (a2, a1), (a3, a2), (a4, a3)} • R = {a1, a2), (a2, a1), (a3, a2), (a4, a3)} • Def = {(a4, a3), (a3, a2)} From the above definition of agent theory, the following hold: Property 1. • Def ⊆ R • ∀a, a ∈ F(oi), a R a
From the theory of an agent, one can define the argumentation system used by that agent for reasoning about the offers and the arguments, i.e. for computing the status of the different offers and arguments.
Definition 3 (Argumentation system). Let A, F, , R, Def be the theory of an agent. The argumentation system of that agent is the pair A, Def .
In [5], different acceptability semantics have been introduced for computing the status of arguments. These are based on two basic concepts, defence and conflict-free, defined as follows: Definition 4 (Defence/conflict-free). Let S ⊆ A. • S defends an argument a iff each argument that defeats a is defeated by some argument in S. • S is conflict-free iff there exist no a, a in S such that a Def a .
Definition 5 (Acceptability semantics). Let S be a conflict-free set of arguments, and let T : 2A → 2A be a function such that T (S) = {a | a is defended by S}. • S is a complete extension iff S = T (S). • S is a preferred extension iff S is a maximal (w.r.t set ⊆) complete extension. • S is a grounded extension iff it is the smallest (w.r.t set ⊆) complete extension.
Let E1, . . . , Ex denote the different extensions under a given semantics.
Note that there is only one grounded extension. It contains all the arguments that are not defeated, and those arguments that are defended directly or indirectly by nondefeated arguments.
Theorem 1. Let A, Def the argumentation system defined as shown above.
T (∅).
Note that when the grounded extension (or the preferred extension) is empty, this means that there is no acceptable offer for the negotiating agent.
Example 2. In example 1, there is one preferred extension, E = {a1, a2, a4}.
Now that the acceptability semantics is defined, we are ready to define the status of any argument.
Definition 6 (Argument status). Let A, Def be an argumentation system, and E1, . . . , Ex its extensions under a given semantics. Let a ∈ A.
This means that a is in some extensions and not in others.
Note that A = {a|a is accepted} ∪ {a|a is rejected} ∪ {a|a is undecided}.
Example 3. In example 1, the arguments a1, a2 and a4 are accepted, whereas the argument a3 is rejected.
As said before, agents use argumentation systems for reasoning about offers. In a negotiation dialogue, agents propose and accept offers that are acceptable for them, and reject bad ones. In what follows, we will define the status of an offer. According to the status of arguments, one can define four statuses of the offers as follows: Definition 7 (Offers status). Let o ∈ O. • The offer o is acceptable for the negotiating agent iff ∃ a ∈ F(o) such that a is accepted. Oa = {oi ∈ O, such that oi is acceptable}. • The offer o is rejected for the negotiating agent iff ∀ a ∈ F(o), a is rejected. Or = {oi ∈ O, such that oi is rejected}.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 969 • The offer o is negotiable iff ∀ a ∈ F(o), a is undecided.
On = {oi ∈ O, such that oi is negotiable}. • The offer o is non-supported iff it is neither acceptable, nor rejected or negotiable. Ons = {oi ∈ O, such that oi is non-supported offers}.
Example 4. In example 1, the two offers o1 and o2 are acceptable since they are supported by accepted arguments, whereas the offer o3 is non-supported since it has no argument in its favor.
From the above definitions, the following results hold: Property 2. Let o ∈ O. • O = Oa ∪ Or ∪ On ∪ Ons. • The set Oa may contain more than one offer.
From the above partition of the set O of offers, a preference relation between offers is defined. Let Ox and Oy be two subsets of O. Ox Oy means that any offer in Ox is preferred to any offer in the set Oy. We can write also for two offers oi, oj, oi oj iff oi ∈ Ox, oj ∈ Oy and Ox Oy.
Definition 8 (Preference between offers). Let O be a set of offers, and Oa, Or, On, Ons its partition. Oa On Ons Or.
Example 5. In example 1, we have o1 o3, and o2 o3.
However, o1 and o2 are indifferent.
THEORIES In this section, we study the properties of the system developed above. We first show that in the particular case where A = AO (ie. all of the agent"s arguments refer to offers), the corresponding argumentation system will return at least one non-empty preferred extension.
Theorem 2. Let A, Def an argumentation system such that A = AO. Then the system returns at least one extension E, such that |E| ≥ 1.
We now present some results that demonstrate the importance of indifference in negotiating agents, and more specifically its relation to acceptable outcomes. We first show that the set Oa may contain several offers when their corresponding accepted arguments are indifferent w.r.t the preference relation .
Theorem 3. Let o1, o2 ∈ O. o1, o2 ∈ Oa iff ∃ a1 ∈ F(o1), ∃ a2 ∈ F(o2), such that a1 and a2 are accepted and are indifferent w.r.t (i.e. a b and b a).
We now study acyclic preference relations that are defined formally as follows.
Definition 9 (Acyclic relation). A relation R on a set A is acyclic if there is no sequence a1, a2, . . . , an ∈ A, with n > 1, such that (ai, ai+1) ∈ R and (an, a1) ∈ R, with
Note that acyclicity prohibits pairs of arguments a, b such that a b and b a, ie., an acyclic preference relation disallows indifference.
Theorem 4. Let A be a set of arguments, R the attacking relation of A defined as R ⊆ A × A, and an acyclic relation on A. Then for any pair of arguments a, b ∈ A, such that (a, b) ∈ R, either (a, b) ∈ Def or (b, a) ∈ Def (or both).
The previous result is used in the proof of the following theorem that states that acyclic preference relations sanction extensions that support exactly one offer.
Theorem 5. Let A be a set of arguments, and an acyclic relation on A. If E is an extension of <A, Def>, then |E ∩ AO| = 1.
An immediate consequence of the above is the following.
Property 3. Let A be a set of arguments such that A = AO. If the relation on A is acyclic, then each extension Ei of <A, Def>, |Ei| = 1.
Another direct consequence of the above theorem is that in acyclic preference relations, arguments that support offers can participate in only one preferred extension.
Theorem 6. Let A be a set of arguments, and an acyclic relation on A. Then the preferred extensions of A, Def are pairwise disjoint w.r.t arguments of AO.
Using the above results we can prove the main theorem of this section that states that negotiating agents with acyclic preference relations do not have acceptable offers.
Theorem 7. Let A, F, R, , Def be a negotiating agent such that A = AO and is an acyclic relation. Then the set of accepted arguments w.r.t A, Def is emtpy.
Consequently, the set of acceptable offers, Oa is empty as well.
In this section, we define formally a protocol that generates argumentation-based negotiation dialogues between two negotiating agents P and C. The two agents negotiate about an object whose possible values belong to a set O. This set O is supposed to be known and the same for both agents. For simplicity reasons, we assume that this set does not change during the dialogue. The agents are equipped with theories denoted respectively AP , FP , P ,
RP , DefP , and AC , FC , C , RC , DefC . Note that the two theories may be different in the sense that the agents may have different sets of arguments, and different preference relations. Worst yet, they may have different arguments in favor of the same offers. Moreover, these theories may evolve during the dialogue.
Before defining formally the evolution of an agent"s theory, let us first introduce the notion of dialogue moves, or moves for short.
Definition 10 (Move). A move is a tuple mi = pi, ai, oi, ti such that: • pi ∈ {P, C} • ai ∈ Args(L) ∪ θ1 1 In what follows θ denotes the fact that no argument, or no offer is given
• oi ∈ O ∪ θ • ti ∈ N∗ is the target of the move, such that ti < i The function Player (resp. Argument, Offer, Target) returns the player of the move (i.e. pi) (resp. the argument of a move, i.e ai, the offer oi, and the target of the move, ti). Let M denote the set of all the moves that can be built from {P, C}, Arg(L), O .
Note that the set M is finite since Arg(L) and O are assumed to be finite. Let us now see how an agent"s theory evolves and why. The idea is that if an agent receives an argument from another agent, it will add the new argument to its theory. Moreover, since an argument may bring new information for the agent, thus new arguments can emerge.
Let us take the following example: Example 6. Suppose that an agent P has the following propositional knowledge base: ΣP = {x, y → z}. From this base one cannot deduce z. Let"s assume that this agent receives the following argument {a, a → y} that justifies y.
It is clear that now P can build an argument, say {a, a → y, y → z} in favor of z.
In a similar way, if a received argument is in conflict with the arguments of the agent i, then those conflicts are also added to its relation Ri . Note that new conflicts may arise between the original arguments of the agent and the ones that emerge after adding the received arguments to its theory. Those new conflicts should also be considered. As a direct consequence of the evolution of the sets Ai and Ri , the defeat relation Defi is also updated.
The initial theory of an agent i, (i.e. its theory before the dialogue starts), is denoted by Ai 0, Fi 0, i 0, Ri 0, Defi
i ∈ {P, C}. Besides, in this paper, we suppose that the preference relation i of an agent does not change during the dialogue.
Definition 11 (Theory evolution). Let m1, . . ., mt, . . ., mj be a sequence of moves. The theory of an agent i at a step t > 0 is: Ai t, Fi t , i t, Ri t, Defi t such that: • Ai t = Ai
A with A ⊆ Args(L) • Fi t = O → 2Ai t • i t = i 0 • Ri t = Ri
aj = Argument(mj), i, j ≤ t, and ai RL aj} ∪ R with R ⊆ RL • Defi t ⊆ Ai t × Ai t The above definition captures the monotonic aspect of an argument. Indeed, an argument cannot be removed.
However, its status may change. An argument that is accepted at step t of the dialogue by an agent may become rejected at step t + i. Consequently, the status of offers also change.
Thus, the sets Oa, Or, On, and Ons may change from one step of the dialogue to another. That means for example that some offers could move from the set Oa to the set Or and vice-versa. Note that in the definition of Rt, the relation RL is used to denote a conflict between exchanged arguments. The reason is that, such a conflict may not be in the set Ri of the agent i. Thus, in order to recognize such conflicts, we have supposed that the set RL is known to the agents. This allows us to capture the situation where an agent is able to prove an argument that it was unable to prove before, by incorporating in its beliefs some information conveyed through the exchange of arguments with another agent. This, unknown at the beginning of the dialogue argument, could give to this agent the possibility to defeat an argument that it could not by using its initial arguments. This could even lead to a change of the status of these initial arguments and this change would lead to the one of the associated offers" status.
In what follows, Oi t,x denotes the set of offers of type x, where x ∈ {a, n, r, ns}, of the agent i at step t of the dialogue. In some places, we can use for short the notation Oi t to denote the partition of the set O at step t for agent i.
Note that we have: not(Oi t,x ⊆ Oi t+1,x).
As said in the introduction, negotiation is a process aiming at finding an agreement about some matters. By agreement, one means a solution that satisfies to the largest possible extent the preferences of both agents. In case there is no such solution, we say that the negotiation fails. In what follows, we will discuss the different kinds of solutions that may be reached in a negotiation. The first one is the optimal solution. An optimal solution is the best offer for both agents.
Formally: Definition 12 (Optimal solution). Let O be a set of offers, and o ∈ O. The offer o is an optimal solution at a step t ≥ 0 iff o ∈ OP t,a ∩ OC t,a Such a solution does not always exist since agents may have conflicting preferences. Thus, agents make concessions by proposing/accepting less preferred offers.
Definition 13 (Concession). Let o ∈ O be an offer.
The offer o is a concession for an agent i iff o ∈ Oi x such that ∃Oi y = ∅, and Oi y Oi x.
During a negotiation dialogue, agents exchange first their most preferred offers, and if these last are rejected, they make concessions. In this case, we say that their best offers are no longer defendable. In an argumentation setting, this means that the agent has already presented all its arguments supporting its best offers, and it has no counter argument against the ones presented by the other agent. Formally: Definition 14 (Defendable offer). Let Ai t, Fi t , i t,
Ri t, Defi t be the theory of agent i at a step t > 0 of the dialogue. Let o ∈ O such that ∃j ≤ t with Player(mj) = i and offer(mj) = o. The offer o is defendable by the agent i iff: • ∃a ∈ Fi t (o), and k ≤ t s.t. Argument(mk) = a, or • ∃a ∈ At \Fi t (o) s.t. a Defi t b with - Argument(mk) = b, k ≤ t, and Player(mk) = i - l ≤ t, Argument(ml) = a The offer o is said non-defendable otherwise and NDi t is the set of non-defendable offers of agent i at a step t.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 971
Now that we have shown how the theories of the agents evolve during a dialogue, we are ready to define formally an argumentation-based negotiation dialogue. For that purpose, we need to define first the notion of a legal continuation.
Definition 15 (Legal move). A move m is a legal continuation of a sequence of moves m1, . . . , ml iff j, k < l, such that: • Offer(mj) = Offer(mk), and • Player(mj) = Player(mk) The idea here is that if the two agents present the same offer, then the dialogue should terminate, and there is no longer possible continuation of the dialogue.
Definition 16 (Argumentation-based negotiation).
An argumentation-based negotiation dialogue d between two agents P and C is a non-empty sequence of moves m1, . . . , ml such that: • pi = P iff i is even, and pi = C iff i is odd • Player(m1) = P, Argument(m1) = θ, Offer(m1) = θ, and Target(m1) = 02 • ∀ mi, if Offer(mi) = θ, then Offer(mi) oj, ∀ oj ∈ O\(O Player(mi) i,r ∪ ND Player(mi) i ) • ∀i = 1, . . . , l, mi is a legal continuation of m1, . . . , mi−1 • Target(mi) = mj such that j < i and Player(mi) = Player(mj) • If Argument(mi) = θ, then: - if Offer(mi) = θ then Argument(mi) ∈ F(Offer(mi)) - if Offer(mi) = θ then Argument(mi) Def Player(mi) i Argument(Target(mi)) • i, j ≤ l such that mi = mj • m ∈ M such that m is a legal continuation of m1, . . . , ml Let D be the set of all possible dialogues.
The first condition says that the two agents take turn. The second condition says that agent P starts the negotiation dialogue by presenting an offer. Note that, in the first turn, we suppose that the agent does not present an argument.
This assumption is made for strategical purposes. Indeed, arguments are exchanged as soon as a conflict appears. The third condition ensures that agents exchange their best offers, but never the rejected ones. This condition takes also into account the concessions that an agent will have to make if it was established that a concession is the only option for it at the current state of the dialogue. Of course, as we have shown in a previous section, an agent may have several good or acceptable offers. In this case, the agent chooses one of them randomly. The fourth condition ensures that the moves are legal. This condition allows to terminate the dialogue as soon as an offer is presented by both agents.
The fifth condition allows agents to backtrack. The sixth 2 The first move has no target. condition says that an agent may send arguments in favor of offers, and in this case the offer should be stated in the same move. An agent can also send arguments in order to defeat arguments of the other agent. The next condition prevents repeating the same move. This is useful for avoiding loops. The last condition ensures that all the possible legal moves have been presented.
The outcome of a negotiation dialogue is computed as follows: Definition 17 (Dialogue outcome). Let d = m1, . . ., ml be a argumentation-based negotiation dialogue. The outcome of this dialogue, denoted Outcome, is Outcome(d) = Offer(ml) iff ∃j < l s.t. Offer(ml) = Offer(mj), and Player(ml) = Player(mj). Otherwise, Outcome(d) = θ.
Note that when Outcome(d) = θ, the negotiation fails, and no agreement is reached by the two agents. However, if Outcome(d) = θ, the negotiation succeeds, and a solution that is either optimal or a compromise is found.
Theorem 8. ∀di ∈ D, the argumentation-based negotiation di terminates.
The above result is of great importance, since it shows that the proposed protocol avoids loops, and dialogues terminate.
Another important result shows that the proposed protocol ensures to reach an optimal solution if it exists. Formally: Theorem 9 (Completeness). Let d = m1, . . . , ml be a argumentation-based negotiation dialogue. If ∃t ≤ l such that OP t,a ∩ OC t,a = ∅, then Outcome(d) ∈ OP t,a ∩ OC t,a.
We show also that the proposed dialogue protocol is sound in the sense that, if a dialogue returns a solution, then that solution is for sure a compromise. In other words, that solution is a common agreement at a given step of the dialogue. We show also that if the negotiation fails, then there is no possible solution.
Theorem 10 (Soundness). Let d = m1, . . . , ml be a argumentation-based negotiation dialogue.
OP t,x ∩ OC t,y, with x, y ∈ {a, n, ns}.
t,x ∩ OC t,y = ∅, ∀ x, y ∈ {a, n, ns}.
A direct consequence of the above theorem is the following: Property 4. Let d = m1, . . . , ml be a argumentationbased negotiation dialogue. If Outcome(d) = θ, then ∀t ≤ l, • OP t,r = OC t,a ∪ OC t,n ∪ OC t,ns, and • OC t,r = OP t,a ∪ OP t,n ∪ OP t,ns.
In this section we will present some examples in order to illustrate our general framework.
Example 7 (No argumentation). Let O = {o1, o2} be the set of all possible offers. Let P and C be two agents, equipped with the same theory: A, F, , R, Def such that A = ∅, F(o1) = F(o2) = ∅, = ∅, R = ∅, Def = ∅. In this case, it is clear that the two offers o1 and o2 are nonsupported. The proposed protocol (see Definition 16) will generate one of the following dialogues:
P: m1 = P, θ, o1, 0 C: m2 = C, θ, o1, 1 This dialogue ends with o1 as a compromise. Note that this solution is not considered as optimal since it is not an acceptable offer for the agents.
P: m1 = P, θ, o1, 0 C: m2 = C, θ, o2, 1 P: m3 = P, θ, o2, 2 This dialogue ends with o2 as a compromise.
P: m1 = P, θ, o2, 0 C: m2 = C, θ, o2, 1 This dialogue also ends with o2 as a compromise. The last possible dialgue is the following that ends with o1 as a compromise.
P: m1 = P, θ, o2, 0 C: m2 = C, θ, o1, 1 P: m3 = P, θ, o1, 2 Note that in the above example, since there is no exchange of arguments, the theories of both agents do not change. Let us now consider the following example.
Example 8 (Static theories). Let O = {o1, o2} be the set of all possible offers. The theory of agent P is AP ,
FP , P , RP , DefP such that: AP = {a1, a2}, FP (o1) = {a1}, FP (o2) = {a2}, P = {(a1, a2)}, RP = {(a1, a2), (a2, a1)},
DefP = {a1, a2}. The argumentation system AP , DefP of this agent will return a1 as an accepted argument, and a2 as a rejected one. Consequently, the offer o1 is acceptable and o2 is rejected.
The theory of agent C is AC , FC , C , RC , DefC such that: AC = {a1, a2}, FC (o1) = {a1}, FC (o2) = {a2}, C = {(a2, a1)}, RC = {(a1, a2), (a2, a1)}, DefC = {a2, a1}. The argumentation system AC , DefC of this agent will return a2 as an accepted argument, and a1 as a rejected one.
Consequently, the offer o2 is acceptable and o1 is rejected.
The only possible dialogues that may take place between the two agents are the following: P: m1 = P, θ, o1, 0 C: m2 = C, θ, o2, 1 P: m3 = P, a1, o1, 2 C: m4 = C, a2, o2, 3 The second possible dialogue is the following: P: m1 = P, θ, o1, 0 C: m2 = C, a2, o2, 1 P: m3 = P, a1, o1, 2 C: m4 = C, θ, o2, 3 Both dialogues end with failure. Note that in both dialogues, the theories of both agents do not change. The reason is that the exchanged arguments are already known to both agents.
The negotiation fails because the agents have conflicting preferences.
Let us now consider an example in which argumentation will allow agents to reach an agreement.
Example 9 (Dynamic theories). Let O = {o1, o2} be the set of all possible offers. The theory of agent P is AP ,
FP , P , RP , DefP such that: AP = {a1, a2}, FP (o1) = {a1}, FP (o2) = {a2}, P = {(a1, a2), (a3, a1)}, RP = {(a1, a2), (a2, a1)}, DefP = {(a1, a2)}. The argumentation system AP , DefP of this agent will return a1 as an accepted argument, and a2 as a rejected one. Consequently, the offer o1 is acceptable and o2 is rejected.
The theory of agent C is AC , FC , C , RC , DefC such that: AC = {a1, a2, a3}, FC (o1) = {a1}, FC (o2) = {a2},
C = {(a1, a2), (a3, a1)}, RC = {(a1, a2), (a2, a1), (a3, a1)},
DefC = {(a1, a2), (a3, a1)}. The argumentation system AC , DefC of this agent will return a3 and a2 as accepted arguments, and a1 as a rejected one. Consequently, the offer o2 is acceptable and o1 is rejected.
The following dialogue may take place between the two agents: P: m1 = P, θ, o1, 0 C: m2 = C, θ, o2, 1 P: m3 = P, a1, o1, 2 C: m4 = C, a3, θ, 3 C: m5 = P, θ, o2, 4 At step 4 of the dialogue, the agent P receives the argument a3 from P. Thus, its theory evolves as follows: AP = {a1, a2, a3}, RP = {(a1, a2), (a2, a1), (a3, a1)}, DefP = {(a1, a2), (a3, a1)}. At this step, the argument a1 which was accepted will become rejected, and the argument a2 which was at the beginning of the dialogue rejected will become accepted. Thus, the offer o2 will be acceptable for the agent, whereas o1 will become rejected. At this step 4, the offer o2 is acceptable for both agents, thus it is an optimal solution.
The dialogue ends by returning this offer as an outcome.
Argumentation has been integrated in negotiation dialogues at the early nineties by Sycara [12]. In that work, the author has emphasized the advantages of using argumentation in negotiation dialogues, and a specific framework has been introduced. In [8], the different types of arguments that are used in a negotiation dialogue, such as threats and rewards, have been discussed. Moreover, a particular framework for negotiation have been proposed. In [9, 13], different other frameworks have been proposed. Even if all these frameworks are based on different logics, and use different definitions of arguments, they all have at their heart an exchange of offers and arguments. However, none of those proposals explain when arguments can be used within a negotiation, and how they should be dealt with by the agent that receives them. Thus the protocol for handling arguments was missing. Another limitation of the above frameworks is the fact that the argumentation frameworks they The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 973 use are quite poor, since they use a very simple acceptability semantics. In [2] a negotiation framework that fills the gap has been suggested. A protocol that handles the arguments was proposed. However, the notion of concession is not modeled in that framework, and it is not clear what is the status of the outcome of the dialogue. Moreover, it is not clear how an agent chooses the offer to propose at a given step of the dialogue. In [1, 7], the authors have focused mainly on this decision problem. They have proposed an argumentation-based decision framework that is used by agents in order to choose the offer to propose or to accept during the dialogue. In that work, agents are supposed to have a beliefs base and a goals base.
Our framework is more general since it does not impose any specific structure for the arguments, the offers, or the beliefs. The negotiation protocol is general as well. Thus this framework can be instantiated in different ways by creating, in such manner, different specific argumentation-based negotiation frameworks, all of them respecting the same properties. Our framework is also a unified one because frameworks like the ones presented above can be represented within this framework. For example the decision making mechanism proposed in [7] for the evaluation of arguments and therefore of offers, which is based on a priority relation between mutually attacked arguments, can be captured by the relation defeat proposed in our framework. This relation takes simultaneously into account the attacking and preference relations that may exist between two arguments.
In this paper we have presented a unified and general framework for argumentation-based negotiation. Like any other argumentation-based negotiation framework, as it is evoked in (e.g. [10]), our framework has all the advantages that argumentation-based negotiation approaches present when related to the negotiation approaches based either on game theoretic models (see e.g. [11]) or heuristics ([6]). This work is a first attempt to formally define the role of argumentation in the negotiation process. More precisely, for the first time, it formally establishes the link that exists between the status of the arguments and the offers they support, it defines the notion of concession and shows how it influences the evolution of the negotiation, it determines how the theories of agents evolve during the dialogue and performs an analysis of the negotiation outcomes. It is also the first time where a study of the formal properties of the negotiation theories of the agents as well as of an argumentative negotiation dialogue is presented.
Our future work concerns several points. A first point is to relax the assumption that the set of possible offers is the same to both agents. Indeed, it is more natural to assume that agents may have different sets of offers. During a negotiation dialogue, these sets will evolve. Arguments in favor of the new offers may be built from the agent theory. Thus, the set of offers will be part of the agent theory. Another possible extension of this work would be to allow agents to handle both arguments PRO and CONS offers. This is more akin to the way human take decisions. Considering both types of arguments will refine the evaluation of the offers status. In the proposed model, a preference relation between offers is defined on the basis of the partition of the set of offers. This preference relation can be refined. For instance, among the acceptable offers, one may prefer the offer that is supported by the strongest argument. In [4], different criteria have been proposed for comparing decisions.
Our framework can thus be extended by integrating those criteria. Another interesting point to investigate is that of considering negotiation dialogues between two agents with different profiles. By profile, we mean the criterion used by an agent to compare its offers.
[1] L. Amgoud, S. Belabbes, and H. Prade. Towards a formal framework for the search of a consensus between autonomous agents. In Proceedings of the 4th International Joint Conference on Autonomous Agents and Multi-Agents systems, pages 537-543, 2005. [2] L. Amgoud, S. Parsons, and N. Maudet. Arguments, dialogue, and negotiation. In Proceedings of the 14th European Conference on Artificial Intelligence, 2000. [3] L. Amgoud and H. Prade. Reaching agreement through argumentation: A possibilistic approach. In 9 th International Conference on the Principles of Knowledge Representation and Reasoning, KR"2004,
[4] L. Amgoud and H. Prade. Explaining qualitative decision under uncertainty by argumentation. In 21st National Conference on Artificial Intelligence,
AAAI"06, pages 16 - 20, 2006. [5] P. M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence, 77:321-357, 1995. [6] N. R. Jennings, P. Faratin, A. R. Lumuscio,
S. Parsons, and C. Sierra. Automated negotiation: Prospects, methods and challenges. International Journal of Group Decision and Negotiation, 2001. [7] A. Kakas and P. Moraitis. Adaptive agent negotiation via argumentation. In Proceedings of the 5th International Joint Conference on Autonomous Agents and Multi-Agents systems, pages 384-391, 2006. [8] S. Kraus, K. Sycara, and A. Evenchik. Reaching agreements through argumentation: a logical model and implementation. Artificial Intelligence, 104:1-69,
[9] S. Parsons and N. R. Jennings. Negotiation through argumentation-a preliminary report. In Proceedings of the 2nd International Conference on Multi Agent Systems, pages 267-274, 1996. [10] I. Rahwan, S. D. Ramchurn, N. R. Jennings,
P. McBurney, S. Parsons, and E. Sonenberg.
Argumentation-based negotiation. Knowledge Engineering Review, 18 (4):343-375, 2003. [11] J. Rosenschein and G. Zlotkin. Rules of Encounter: Designing Conventions for Automated Negotiation Among Computers,. MIT Press, Cambridge,
Massachusetts, 1994., 1994. [12] K. Sycara. Persuasive argumentation in negotiation.
Theory and Decision, 28:203-242, 1990. [13] F. Tohm´e. Negotiation and defeasible reasons for choice. In Proceedings of the Stanford Spring Symposium on Qualitative Preferences in Deliberation and Practical Reasoning, pages 95-102, 1997.

Coalition formation, a key form of interaction in multi-agent systems, is the process of joining together two or more agents so as to achieve goals that individuals on their own cannot, or to achieve them more efficiently [1, 11, 14, 13]. Often, in such situations, there is more than one possible coalition and a player"s payoff depends on which one it joins. Given this, a key problem is to ensure that none of the parties in a coalition has any incentive to break away from it and join another coalition (i.e., the coalitions should be stable). However, in many cases there may be more than one solution (i.e., a stable coalition). In such cases, it becomes difficult to select a single solution from among the possible ones, especially if the parties are self-interested (i.e., they have different preferences over stable coalitions).
In this context, cooperative game theory deals with the problem of coalition formation and offers a number of solution concepts that possess desirable properties like stability, fair division of joint gains, and uniqueness [16, 14]. Cooperative game theory differs from its non-cooperative counterpart in that for the former the players are allowed to form binding agreements and so there is a strong incentive to work together to receive the largest total payoff. Also, unlike non-cooperative game theory, cooperative game theory does not specify a game through a description of the strategic environment (including the order of players" moves and the set of actions at each move) and the resulting payoffs, but, instead, it reduces this collection of data to the coalitional form where each coalition is represented by a single real number: there are no actions, moves, or individual payoffs. The chief advantage of this approach, at least in multiple-player environments, is its practical usefulness. Thus, many more real-life situations fit more easily into a coalitional form game, whose structure is more tractable than that of a non-cooperative game, whether that be in normal or extensive form and it is for this reason that we focus on such forms in this paper.
Given these observations, a number of multiagent systems researchers have used and extended cooperative game-theoretic solutions to facilitate automated coalition formation [20, 21, 18].
Moreover, in this work, one of the most extensively studied solution concepts is the Shapley value [19]. A player"s Shapley value gives an indication of its prospects of playing the game - the higher the Shapley value, the better its prospects. The main advantage of the Shapley value is that it provides a solution that is both unique and fair (see Section 2.1 for a discussion of the property of fairness).
However, while these are both desirable properties, the Shapley value has one major drawback: for many coalition games, it cannot be determined in polynomial time. For instance, finding this value for the weighted voting game is, in general, #P-complete [6].
A problem is #P-hard if solving it is as hard as counting satisfying assignments of propositional logic formulae [15, p442]. Since #P-completeness thus subsumes NP-completeness, this implies that computing the Shapley value for the weighted voting game will be intractable in general. In other words, it is practically infeasible to try to compute the exact Shapley value. However, the voting game has practical relevance to multi-agent systems as it is an important means of reaching consensus between multiple agents. Hence, our objective is to overcome the computational complexity of finding the Shapley value for this game. Specifically, we first show that there are some specific voting games for which the exact value can 959 978-81-904262-7-5 (RPS) c 2007 IFAAMAS be computed in polynomial time. By identifying such games, we show, for the first tme, when it is feasible to find the exact value and when it is not. For the computationally complex voting games, we present a new randomised method, along the lines of Monte-Carlo simulation, for computing the approximate Shapley value.
The computational complexity of such games has typically been tackled using two main approaches. The first is to use generating functions [3]. This method trades time complexity for storage space. The second uses an approximation technique based on Monte Carlo simulation [12, 7]. However the method we propose is more general than either of these (see Section 6 for details).
Moreover, no work has previously analysed the approximation error. The approximation error relates to how close the approximate is to the true Shapley value. Specifically, it is the difference between the true and the approximate Shapley value. It is important to determine this error because the performance of an approximation method is evaluated in terms of two criteria: its time complexity, and its approximation error. Thus, our contribution lies in also in providing, for the first time, an analysis of the percentage error in the approximate Shapley value. This analysis is carried out empirically.
Our experiments show that the error is always less than 20%, and in most cases it is under 5%. Finally, our method has time complexity linear in the number of players and it does not require any arrays (i.e., it is economical in terms of both computing time and storage space). Given this, and the fact that software agents have limited computational resources and therefore cannot compute the true Shapley value, our results are especially relevant to such resource bounded agents.
The rest of the paper is organised as follows. Section 2 defines the Shapley value and describes the weighted voting game. In Section 3 we describe voting games whose Shapley value can be found in polynomial time. In Section 4, we present a randomized method for finding the approximate Shapley value and analyse its performance in Section 5. Section 6 discusses related literature. Finally,
Section 7 concludes.
We begin by introducing coalition games and the Shapley value and then define the weighted voting game. A coalition game is a game where groups of players (coalitions) may enforce cooperative behaviour between their members. Hence the game is a competition between coalitions of players, rather than between individual players.
Depending on how the players measure utility, coalition game theory is split into two parts. If the players measure utility or the payoff in the same units and there is a means of exchange of utility such as side payments, we say the game has transferable utility; otherwise it has non-transferable utility. More formally, a coalition game with transferable utility, N, v , consists of:
of N (i.e., a coalition) a real number v(S) (the worth of S).
For each coalition S, the number v(S) is the total payoff that is available for division among the members of S (i.e., the set of joint actions that coalition S can take consists of all possible divisions of v(S) among the members of S). Coalition games with nontransferable payoffs differ from ones with transferable payoffs in the following way. For the former, each coalition is associated with a set of payoff vectors that is not necessarily the set of all possible divisions of some fixed amount. The focus of this paper is on the weighted voting game (described in Section 2.1) which is a game with transferable payoffs.
Thus, in either case, the players will only join a coalition if they expect to gain from it. Here, the players are allowed to form binding agreements, and so there is strong incentive to work together to receive the largest total payoff. The problem then is how to split the total payoff between or among the players. In this context, Shapley [19] constructed a solution using an axiomatic approach. Shapley defined a value for games to be a function that assigns to a game (N, v), a number ϕi(N, v) for each i in N. This function satisfies three axioms [17]:
play no role in determining the value.
players i in any carrier C equal v(C). A carrier C is a subset of N such that v(S) = v(S ∩ C) for any subset of players S ⊂ N.
games must be related to one another. It requires that for any games ϕi(N, v) and ϕi(N, v ), ϕi(N, v)+ϕi(N, v ) = ϕi(N, v + v ) for all i in N.
Shapley showed that there is a unique function that satisfies these three axioms.
Shapley viewed this value as an index for measuring the power of players in a game. Like a price index or other market indices, the value uses averages (or weighted averages in some of its generalizations) to aggregate the power of players in their various cooperation opportunities. Alternatively, one can think of the Shapley value as a measure of the utility of risk neutral players in a game.
We first introduce some notation and then define the Shapley value. Let S denote the set N − {i} and fi : S → 2N−{i} be a random variable that takes its values in the set of all subsets of N − {i}, and has the probability distribution function (g) defined as: g(fi(S) = S) = |S|!(n − |S| − 1)! n! The random variable fi is interpreted as the random choice of a coalition that player i joins. Then, a player"s Shapley value is defined in terms of its marginal contribution. Thus, the marginal contribution of player i to coalition S with i /∈ S is a function Δiv that is defined as follows: Δiv(S) = v(S ∪ {i}) − v(S) Thus a player"s marginal contribution to a coalition S is the increase in the value of S as a result of i joining it.
DEFINITION 1. The Shapley value (ϕi) of the game N, v for player i is the expectation (E) of its marginal contribution to a coalition that is chosen randomly: ϕi(N, v) = E[Δiv ◦ fi] (1) The Shapley value is interpreted as follows. Suppose that all the players are arranged in some order, all orderings being equally likely. Then ϕi(N, v) is the expected marginal contribution, over all orderings, of player i to the set of players who precede him.
The method for finding a player"s Shapley value depends on the definition of the value function (v). This function is different for different games, but here we focus specifically on the weighted voting game for the reasons outlined in Section 1.
We adopt the definition of the voting game given in [6]. Thus, there is a set of n players that may, for example, represent shareholders in a company or members in a parliament. The weighted voting game is then a game G = N, v in which: v(S) = j
for some q ∈ IR+ and wi ∈ IRN + , where: w(S) = X i∈S wi for any coalition S. Thus wi is the number of votes that player i has and q is the number of votes needed to win the game (i.e., the quota).
Note that for this game (denoted q; w1, . . . , wn ), a player"s marginal contribution is either zero or one. This is because the value of any coalition is either zero or one. A coalition with value zero is called a losing coalition and with value one a winning coalition. If a player"s entry to a coalition changes it from losing to winning, then the player"s marginal contribution for that coalition is one; otherwise it is zero.
The main advantage of the Shapley value is that it gives a solution that is both unique and fair. The property of uniqueness is desirable because it leaves no ambiguity. The property of fairness relates to how the gains from cooperation are split between the members of a coalition. In this case, a player"s Shapley value is proportional to the contribution it makes as a member of a coalition; the more contribution it makes, the higher its value. Thus, from a player"s perspective, both uniqueness and fairness are desirable properties.
TIME SOLUTIONS Here we describe those voting games for which the Shapley value can be determined in polynomial time. This is achieved using the direct enumeration approach (i.e., listing all possible coalitions and finding a player"s marginal contribution to each of them). We characterise such games in terms of the number of players and their weights.
Consider the game q; j, . . . , j with m parties. Each party has j votes. If q ≤ j, then there would be no need for the players to form a coalition. On the other hand, if q = mj (m = |N| is the number of players), only the grand coalition is possible. The interesting games are those for which the quota (q) satisfies the constraint: (j + 1) ≤ q ≤ j(m − 1). For these games, the value of a coalition is one if the weight of the coalition is greater than or equal to q, otherwise it is zero.
Let ϕ denote the Shapley value for a player. Consider any one player. This player can join a coalition as the ith member where
only if it joins a coalition as the q/j th member. In all other cases, its marginal contribution is zero. Thus, the Shapley value for each player ϕ = 1/m. Since ϕ requires one division operation, it can be found in constant time (i.e., O(1)).
Consider a game in which there are two types of players: large (with weight wl > ws) and small (with weight ws). There is one large player and m small ones. The quota for this game is q; i.e., we have a game of the form q; wl, ws, ws, . . . , ws . The total number of players is (m + 1). The value of a coalition is one if the weight of the coalition is greater than or equal to q, otherwise it is zero.
Let ϕl denote the Shapley value for the large player and ϕs that for each small player.
We first consider ws = 1 and then ws > 1. The smallest possible value for q is wl + 1. This is because, if q ≤ wl, then the large party can win the election on its own without the need for a coalition. Thus, the quota for the game satisfies the constraint wl + 1 ≤ q ≤ m + wl − 1. Also, the lower and upper limits for wl are 2 and (q − 1) respectively. The lower limit is 2 because the weight of the large party has to be greater than each small one.
Furthermore, the weight of the large party cannot be greater than q, since in that case, there would be no need for the large party to form a coalition. Recall that for our voting game, a player"s marginal contribution to a coalition can only be zero or one.
Consider the large party. This party can join a coalition as the ith member where 1 ≤ i ≤ (m + 1). However, the marginal contribution of the large party is one if it joins a coalition as the ith member where (q − wl) ≤ i < q. In all the remaining cases, its marginal contribution is zero. Thus, out of the total (m + 1) possible cases, its marginal contribution is one in wl cases. Hence, the Shapley value of the large party is: ϕl = wl/(m + 1). In the same way, we obtain the Shapley value of the large party for the general case where ws > 1 as: ϕl = wl/ws /(m + 1) Now consider a small player. We know that the sum of the Shapley values of all the m+1 players is one. Also, since the small parties have equal weights, their Shapley values are the same. Hence, we get: ϕs =
m Thus, both ϕl and ϕs can be computed in constant time. This is because both require a constant number of basic operations (addition, subtraction, multiplication, and division). In the same way, the Shapley value for a voting game with a single large party and multiple small parties can be determined in constant time.
We now consider a voting game that has two player types: large and small (as in Section 3.2), but now there are multiple large and multiple small parties. The set of parties consists of ml large parties and ms small parties. The weight of each large party is wl and that of each small one is ws, where ws < wl. We show the computational tractability for this game by considering the following four possible scenarios: S1 q ≤ mlwl and q ≤ msws S2 q ≤ mlwl and q ≥ msws S3 q ≥ mlwl and q ≥ msws S4 q ≥ mlwl and q ≤ msws For the first scenario, consider a large player. In order to determine the Shapley value for this player, we need to consider the number of all possible coalitions that give it a marginal contribution of one.
It is possible for the marginal contribution of this player to be one if it joins a coalition in which the number of large players is between zero and (q −1)/wl. In other words, there are (q −1)/wl +1 such cases and we now consider each of them.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 961 Consider a coalition such that when the large player joins in, there are i large players and (q − iwl − 1)/ws small players already in it, and the remaining players join after the large player.
Such a coalition gives the large player unit marginal contribution.
Let C2 l (i, q) denote the number of all such coalitions. To begin, consider the case i = 0: C2 l (0, q) = C „ ms, q − 1 ws « × FACTORIAL „ q − 1 ws « × FACTORIAL „ ml + ms − q − 1 ws − 1 « where C(y, x) denotes the number of possible combinations of x items from a set of y items. For i = 1, we get: C2 l (1, q) = C(ml, 1) × C „ ms, q − wl − 1 ws « × FACTORIAL „ q − wl − 1 ws « × FACTORIAL „ ml + ms − q − wl − 1 ws − 1 « In general, for i > 1, we get: C2 l (i, q) = C(ml, i) × C „ ms, q − iwl − 1 ws « × FACTORIAL „ q − iwl − 1 ws « × FACTORIAL „ ml + ms − q − wl − 1 ws − 1 « Thus the large player"s Shapley value is: ϕl = q−1 wlX i=0 C2 l (i, q)/FACTORIAL(ml + ms) For a given i, the time to find C2 l (i, q) is O(T) where T = (mlms(q − iwl − 1)(ml + ms))/ws Hence, the time to find the Shapley value is O(T q/wl).
In the same way, a small player"s Shapley value is: ϕs = q−1 wsX i=0 C2 s (i, q)/FACTORIAL(ml + ms) and can be found in time O(T q/ws). Likewise, the remaining three scenarios (S2 to S4) can be shown to have the same time complexity.
We now consider a voting game that has three player types: 1, 2, and 3. The set of parties consists of m1 players of type 1 (each with weight w1), m2 players of type 2 (each with weight w2), and m3 players of type 3 (each with weight w3).
For this voting game, consider a player of type 1. It is possible for the marginal contribution of this player to be one if it joins a coalition in which the number of type 1 players is between zero and (q − 1)/w1. In other words, there are (q − 1)/w1 + 1 such cases and we now consider each of them.
Consider a coalition such that when the type 1 player joins in, there are i type 1 players already in it. The remaining players join after the type 1 player. Let C3 l (i, q) denote the number of all such coalitions that give a marginal contribution of one to the type 1 player where: C3
q−1 w1X i=0 q−iw1−1 w2X j=0 C2
Therefore the Shapley value of the type 1 player is: ϕ1 = q−1 w1X i=0 C3
The time complexity of finding this value is O(T q2 /w1w2) where: T = ( 3Y i=1 mi)(q − iwl − 1)( 3X i=1 mi)/(w2 + w3) Likewise, for the other two player types (2 and 3).
Thus, we have identified games for which the exact Shapley value can be easily determined. However, the computational complexity of the above direct enumeration method increases with the number of player types. For a voting game with more than three player types, the time complexity of the above method is a polynomial of degree four or more. To deal with such situations, therefore, the following section presents a faster randomised method for finding the approximate Shapley value.
VALUE We first give a brief introduction to randomized algorithms and then present our randomized method for finding the approximate Shapley value. Randomized algorithms are the most commonly used approach for finding approximate solutions to computationally hard problems. A randomized algorithm is an algorithm that, during some of its steps, performs random choices [2]. The random steps performed by the algorithm imply that by executing the algorithm several times with the same input we are not guaranteed to find the same solution. Now, since such algorithms generate approximate solutions, their performance is evaluated in terms of two criteria: their time complexity, and their error of approximation.
The approximation error refers to the difference between the exact solution and its approximation. Against this background, we present a randomized method for finding the approximate Shapley value and empirically evaluate its error.
We first describe the general voting game and then present our randomized algorithm. In its general form, a voting game has more than two types of players. Let wi denote the weight of player i. Thus, for m players and for quota q the game is of the form q; w1, w2, . . . , wm . The weights are specified in terms of a probability distribution function. For such a game, we want to find the approximate Shapley value.
We let P denote a population of players. The players" weights in this population are defined by a probability distribution function.
Irrespective of the actual probability distribution function, let μ be the mean weight for the population of players and ν the variance in the players" weights. From this population of players we randomly draw samples and find the sum of the players" weights in the sample using the following rule from Sampling Theory (see [8] p425): If w1, w2, . . . , wn is a random sample of size n drawn from any distribution with mean μ and variance ν, then the sample sum has an approximate Normal distribution with mean nμ and variance ν n (the larger the n the better the approximation).
R-SHAPLEYVALUE (P, μ, ν, q, wi) P: Population of players μ: Mean weight of the population P ν: Variance in the weights for poulation P q: Quota for the voting game wi: Player i"s weight
population P
i ) of player i to SX as: ΔX i ← 1√ (2πν/X) R b a e−X (x−Xμ)2 2ν dx
i
ϕi ← Ti/m Table 1: Randomized algorithm to find the Shapley value for player i.
We know from Definition 1, that the Shapley value for a player is the expectation (E) of its marginal contribution to a coalition that is chosen randomly. We use this rule to determine the Shapley value as follows.
For player i with weight wi, let ϕi denote the Shapley value. Let X denote the size of a random sample drawn from a population in which the individual player weights have any distribution. The marginal contribution of player i to this random sample is one if the total weight of the X players in the sample is greater than or equal to a = q −wi but less than b = q − (where is an inifinitesimally small quantity). Otherwise, its marginal contribution is zero. Thus, the expected marginal contribution of player i (denoted ΔX i ) to the sample coalition is the area under the curve defined by N(Xμ, ν X ) in the interval [a, b]. This area is shown as the region B in Figure 1 (the dotted line in the figure is Xμ). Hence we get: ΔX i = 1 p (2πν/X) Z b a e−X (x−Xμ)2 2ν dx (2) and the Shapley value is: ϕi = 1 m mX X=1 ΔX i (3) The above steps are described in Table 1. In more detail, Step
and repeatedly do the following. In Step 2.1, we randomly select a sample SX of size X from the population P. Player i"s marginal contribution to the random coalition SX is found in Step 2.2. The average marginal contribution is found in Step 3 - and this is the Shapley value for player i.
THEOREM 1. The time complexity of the proposed randomized method is linear in the number of players.
PROOF. As per Equation 3, ΔX i must be computed m times.
This is done in the for loop of Step 2 in Table 1. Hence, the time complexity of computing a player"s Shapley value is O(m).
The following section analyses the approximation error for the proposed method.
METHOD We first derive the formula for measuring the error in the approximate Shapley value and then conduct experiments for evaluating this error in a wide range of settings. However, before doing so, we introduce the idea of error.
The concept of error relates to a measurement made of a quantity which has an accepted value [22, 4]. Obviously, it cannot be determined exactly how far off a measurement is from the accepted value; if this could be done, it would be possible to just give a more accurate, corrected value. Thus, error has to do with uncertainty in measurements that nothing can be done about. If a measurement is repeated, the values obtained will differ and none of the results can be preferred over the others. However, although it is not possible to do anything about such error, it can be characterized.
As described in Section 4, we make measurements on samples that are drawn randomly from a given population (P) of players.
Now, there are statistical errors associated with sampling which are unavoidable and must be lived with. Hence, if the result of a measurement is to have meaning it cannot consist of the measured value alone. An indication of how accurate the result is must be included also. Thus, the result of any physical measurement has two essential components:
quantity measured, and
For example, if the estimate of a quantity is x and the uncertainty is e(x) the quantity would lie in x ± e(x).
For sampling experiments, the standard error is by far the most common way of characterising uncertainty [22]. Given this, the following section defines this error and uses it to evaluate the performance of the proposed randomized method.
The accuracy of the above randomized method depends on its sampling error which is defined as follows [22, 4]: DEFINITION 2. The sampling error (or standard error) is defined as the standard deviation for a set of measurements divided by the square root of the number of measurements.
To this end, let e(σX ) be the sampling error in the sum of the weights for a sample of size X drawn from the distribution N(Xμ, ν X ) where: e(σX ) = p (ν/X)/ p (X) = p (ν)/X (4) Let e(ΔX i ) denote the error in the marginal contribution for player i (given in Equation 2). This error is obtained by propagating the error in Equation 4 to Equation 2. In Equation 2, a and b are the lower and upper limits for the sum of the players" weights for a The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 963 b C B a − e (σX ) a A )X(σeb+ Sum of weights Z1 Z2 Figure 1: A normal distribution for the sum of players" weights in a coalition of size X. 40 60 80 100 0 50 100 0 5 10 15 20 25 QuotaWeight PercentageerrorintheShapleyvalue Figure 2: Performance of the randomized method for m = 10 players. coalition of size X. Since the error in this sum is e(σX ), the actual values of a and b lie in the interval a ± e(σX ) and b ± e(σX ) respectively. Hence, the error in Equation 2 is either the probability that the sum lies between the limits a − e(σX ) and a (i.e., the area under the curve defined by N(Xμ, ν X ) between a − e(σX ) and a, which is the shaded region A in Figure 1) or the probability that the sum of weights lies between the limits b and b+e(σX ) (i.e., the area under the curve defined by N(Xμ, ν X ) between b and b + e(σX ), which is the shaded region C in Figure 1). More specifically, the error is the maximum of these two probabilities: e(ΔX i ) = 1 p (2πν/X) × MAX „Z a a−e(σX ) e−X (x−Xμ)2 2ν dx,
Z b+e(σX ) b e−X (x−Xμ)2 2ν dx « On the basis of the above error, we find the error in the Shapley value by using the following standard error propagation rules [22]: R1 If x and y are two random variables with errors e(x) and e(y) respectively, then the error in the random variable z = x + y is given by: e(z) = e(x) + e(y) R2 If x is a random variable with error e(x) and z = kx where 0 100 200 300 400 500 0 100 200 300 400 500 0 5 10 15 20 25 QuotaWeight PercentageerrorintheShapleyvalue Figure 3: Performance of the randomized method for m = 50 players. the constant k has no error, then the error in z is: e(z) = |k|e(x) Using the above rules, the error in the Shapley value (given in Equation 3) is obtained by propagating the error in Equation 4 to all coalitions between the sizes X = 1 and X = m. Let e(ϕi) denote this error where: e(ϕi) = 1 m mX X=1 e(ΔX i ) We analyze the performance of our method in terms of the percentage error PE in the approximate Shapley value which is defined as follows: PE = 100 × e(ϕi)/ϕi (5)
We now compute the percentage error in the Shapley value using the above equation for PE. Since this error depends on the parameters of the voting game, we evaluate it in a range of settings by systematically varying the parameters of the voting game.
In particular, we conduct experiments in the following setting.
For a player with weight w, the percentage error in a player"s Shapley value depends on the following five parameters (see Equation 3):
We fix μ = 10 and ν = 1. This is because, for the normal distribution, μ = 10 ensures that for almost all the players the weight is positive, and ν = 1 is used most commonly in statistical experiments (ν can be higher or lower but PE is increasing in νsee Equations 4 and 5). We then vary m, q, and w as follows. We vary m between 5 and 100 (since beyond 100 we found that the error is close to zero), for each m we vary q between 4μ and mμ (we impose these limits because they ensure that the size of the
0 200 400 600 800 1000 0 200 400 600 800 1000 0 5 10 15 20 25 QuotaWeight PercentageerrorintheShapleyvalue Figure 4: Performance of the randomized method for m = 100 players. winning coalition is more than one and less than m - see Section 3 for details), and for each q, we vary w between 1 and q−1 (because a winning coalition must contain at least two players). The results of these experiments are shown in Figures 2, 3, and 4. As seen in the figures, the maximum PE is around 20% and in most cases it is below 5%.
We now analyse the effect of the three parameters: w, q, and m on the percentage error in more detail. - Effect of w. The PE depends on e(σX ) because, in Equation 5, the limits of integration depend on e(σX ). The interval over which the first integration in Equation 5 is done is a − a + e(σX ) = e(σX ), and the interval over which the second one is done is b + e(σX ) − b = e(σX ). Thus, the interval is the same for both integrations and it is independent of wi. Note that each of the two functions that are integrated in Equation 5 are the same as the function that is integrated in Equation 2. Only the limits of the integration are different.
Also, the interval over which the integration for the marginal contribution of Equation 2 is done is b − a = wi − (see Figure 1). The error in the marginal contribution is either the area of the shaded region A (between a − e(σX ) and a) in Figure 1, or the shaded area C (between b and b + e(σX )).
As per Equation 5, it is the maximum of these two areas.
Since e(σX ) is independent of wi, as wi increases, e(σX ) remains unchanged. However, the area of the unshaded region B increases. Hence, as wi increases, the error in the marginal contribution decreases and PE also decreases. - Effect of q. For a given q, the Shapley value for player i is as given in Equation 3. We know that, for a sample of size X, the sum of the players" weights is distributed normally with mean Xμ and variance ν/X. Since 99% of a normal distribution lies within two standard deviations of its mean [8], player i"s marginal contribution to a sample of size X is almost zero if: a < Xμ + 2 p ν/X or b > Xμ − 2 p ν/X This is because the three regions A, B, and C (in Figure 1) lie either to the right of Z2 or to the left of Z1. However, player i"s marginal contribution is greater than zero for those X for which the following constraint is satisfied: Xμ − 2 p ν/X < a < b < Xμ + 2 p ν/X For this constraint, the three regions A, B, and C lie somewhere between Z1 and Z2. Since a = q −wi and b = q − ,
Equation 6 can also be written as: Xμ − 2 p ν/X < q − wi < q − < Xμ + 2 p ν/X The smallest X that satisfies the constraint in Equation 6 strictly increases with q. As X increases, the error in sum of weights in a sample (i.e., e(σX ) = p (ν)/X) decreases.
Consequently, the error in a player"s marginal contribution (see Equation 5) also decreases. This implies that as q increases, the error in the marginal contribution (and consequently the error in the Shapley value) decreases. - Effect of m. It is clear from Equation 4 that the error e(σX ) is highest for X = 1 and it decreases with X. Hence, for small m, e(σ1 ) has a significant effect on PE. But as m increases, the effect of e(σ1 ) on PE decreases and, as a result,
PE decreases.
In order to overcome the computational complexity of finding the Shapley value, two main approaches have been proposed in the literature. One approach is to use generating functions [3]. This method is an exact procedure that overcomes the problem of time complexity, but its storage requirements are substantial - it requires huge arrays. It also has the limitation (not shared by other approaches) that it can only be applied to games with integer weights and quotas.
The other method uses an approximation technique based on Monte Carlo simulation. In [12], for instance, the Shapley value is computed by considering a random sample from a large population of players. The method we propose differs from this in that they define the Shapley value by treating a player"s number of swings (if a player can change a losing coalition to a winning one, then, for the player, the coalition is counted as a swing) as a random variable, while we treat the players" weights as random variables. In [12], however, the question remains how to get the number of swings from the definition of a voting game and what is the time complexity of doing this. Since the voting game is defined in terms of the players" weights and the number of swings are obtained from these weights, our method corresponds more closely to the definition of the voting game. Our method also differs from [7] in that while [7] presents a method for the case where all the players" weights are distributed normally, our method applies to any type of distribution for these weights. Thus, as stated in Section 1, our method is more general than [3, 12, 7]. Also, unlike all the above mentioned work, we provide an analysis of the performance of our method in terms of the percentage error in the approximate Shapley value.
A method for finding the Shapley value was also proposed in [5]. This method gives the exact Shapley value, but its time complexity is exponential. Furthermore, the method can be used only if the game is represented in a specific form (viz., the multi-issue representation), not otherwise. Finally, [9, 10] present a polynomial time method for finding the Shapley value. This method can be used if the coalition game is represented as a marginal contribution net. Furthermore, they assume that the Shapley value of a component of a given coalition game is given by an oracle, and on the basis of this assumption aggregate these values to find the value for the overall game. In contrast, our method is independent The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 965 of the representation and gives an approximate Shapley value in linear time, without the need for an oracle.
Coalition formation is an important form of interaction in multiagent systems. An important issue in such work is for the agents to decide how to split the gains from cooperation between the members of a coalition. In this context, cooperative game theory offers a solution concept called the Shapley value. The main advantage of the Shapley value is that it provides a solution that is both unique and fair. However, its main problem is that, for many coalition games, the Shapley value cannot be determined in polynomial time.
In particular, the problem of finding this value for the voting game is #P-complete. Although this problem is, in general #P-complete, we show that there are some specific voting games for which the Shapley value can be determined in polynomial time and characterise such games. By doing so, we have shown when it is computationally feasible to find the exact Shapley value. For other complex voting games, we presented a new randomized method for determining the approximate Shapley value. The time complexity of the proposed method is linear in the number of players. We analysed the performance of this method in terms of the percentage error in the approximate Shapley value.
Our experiments show that the percentage error in the Shapley value is at most 20. Furthermore, in most cases, the error is less than 5%. Finally, we analyse the effect of the different parameters of the voting game on this error. Our study shows that the error decreases as
Given the fact that software agents have limited computational resources and therefore cannot compute the true Shapley value, our results are especially relevant to such resource bounded agents. In future, we will explore the problem of determining the Shapley value for other commonly occurring coalition games like the production economy and the market economy.
[1] R. Aumann. Acceptable points in general cooperative n-person games. In Contributions to theTheory of Games volume IV. Princeton University Press, 1959. [2] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann,
A. Marchetti-Spaccamela, and M. Protasi. Complexity and approximation: Combinatorial optimization problems and their approximability properties. Springer, 2003. [3] J. M. Bilbao, J. R. Fernandez, A. J. Losada, and J. J. Lopez.
Generating functions for computing power indices efficiently. Top 8, 2:191-213, 2000. [4] P. Bork, H. Grote, D. Notz, and M. Regler. Data Analysis Techniques in High Energy Physics Experiments. Cambridge University Press, 1993. [5] V. Conitzer and T. Sandholm. Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains. In Proceedings of the National Conference on Artificial Intelligence, pages 219-225, San Jose, California, 2004. [6] X. Deng and C. H. Papadimitriou. On the complexity of cooperative solution concepts. Mathematics of Operations Research, 19(2):257-266, 1994. [7] S. S. Fatima, M. Wooldridge, and N. R. Jennings. An analysis of the shapley value and its uncertainty for the voting game. In Proc. 7th Int. Workshop on Agent Mediated Electronic Commerce, pages 39-52, 2005. [8] A. Francis. Advanced Level Statistics. Stanley Thornes Publishers, 1979. [9] S. Ieong and Y. Shoham. Marginal contribution nets: A compact representation scheme for coalitional games. In Proceedings of the Sixth ACM Conference on Electronic Commerce, pages 193-202, Vancouver, Canada, 2005. [10] S. Ieong and Y. Shoham. Multi-attribute coalition games. In Proceedings of the Seventh ACM Conference on Electronic Commerce, pages 170-179, Ann Arbor, Michigan, 2006. [11] J. P. Kahan and A. Rapoport. Theories of Coalition Formation. Lawrence Erlbaum Associates Publishers, 1984. [12] I. Mann and L. S. Shapley. Values for large games iv: Evaluating the electoral college exactly. Technical report,
The RAND Corporation, Santa Monica, 1962. [13] A. MasColell, M. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, 1995. [14] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press, 1994. [15] C. H. Papadimitriou. Computational Complexity. Addison Wesley Longman, 1994. [16] A. Rapoport. N-person Game Theory : Concepts and Applications. Dover Publications, Mineola, NY, 2001. [17] A. E. Roth. Introduction to the shapley value. In A. E. Roth, editor, The Shapley value, pages 1-27. University of Cambridge Press, Cambridge, 1988. [18] T. Sandholm and V. Lesser. Coalitions among computationally bounded agents. Artificial Intelligence Journal, 94(1):99-137, 1997. [19] L. S. Shapley. A value for n person games. In A. E. Roth, editor, The Shapley value, pages 31-40. University of Cambridge Press, Cambridge, 1988. [20] O. Shehory and S. Kraus. A kernel-oriented model for coalition-formation in general environments: Implemetation and results. In In Proceedings of the National Conference on Artificial Intelligence (AAAI-96), pages 131-140, 1996. [21] O. Shehory and S. Kraus. Methods for task allocation via agent coalition formation. Artificial Intelligence Journal, 101(2):165-200, 1998. [22] J. R. Taylor. An introduction to error analysis: The study of uncertainties in physical measurements. University Science Books, 1982.

Negotiation is a key form of interaction in multiagent systems. It is a process in which disputing agents decide how to divide the gains from cooperation. Since this decision is made jointly by the agents themselves [20, 19, 13, 15], each party can only obtain what the other is prepared to allow them. Now, the simplest form of negotiation involves two agents and a single issue. For example, consider a scenario in which a buyer and a seller negotiate on the price of a good. To begin, the two agents are likely to differ on the price at which they believe the trade should take place, but through a process of joint decision-making they either arrive at a price that is mutually acceptable or they fail to reach an agreement. Since agents are likely to begin with different prices, one or both of them must move toward the other, through a series of offers and counter offers, in order to obtain a mutually acceptable outcome. However, before the agents can actually perform such negotiations, they must decide the rules for making offers and counter offers. That is, they must set the negotiation protocol [20]. On the basis of this protocol, each agent chooses its strategy (i.e., what offers it should make during the course of negotiation). Given this context, this work focuses on competitive scenarios with self-interested agents. For such cases, each participant defines its strategy so as to maximise its individual utility.
However, in most bilateral negotiations, the parties involved need to settle more than one issue. For this case, the issues may be divisible or indivisible [4]. For the former, the problem for the agents is to decide how to split each issue between themselves [21]. For the latter, the individual issues cannot be divided. An issue, in its entirety, must be allocated to either of the two agents. Since the agents value different issues differently, they must come to terms about who will take which issue. To date, most of the existing work on multi-issue negotiation has focussed on the former case [7, 2, 5, 23, 11, 6]. However, in many real-world settings, the issues are indivisible. Hence, our focus here is on negotiation for indivisible issues. Such negotiations are very common in multiagent systems. For example, consider the case of task allocation between two agents. There is a set of tasks to be carried out and different agents have different preferences for the tasks. The tasks cannot be partitioned; a task must be carried out by one agent. The problem then is for the agents to negotiate about who will carry out which task.
A key problem in the study of multi-issue negotiation is to determine the equilibrium strategies. An equally important problem, especially in the context of software agents, is to find the time complexity of computing the equilibrium offers. However, such computational issues have so far received little attention. As we will show, this is mainly due to the fact that existing work (describe in Section 5) has mostly focused on negotiation for divisible issues 951 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and finding the equilibrium for this case is computationally easier than that for the case of indivisible issues. Our primary objective is, therefore, to answer the computational questions for the latter case for the types of situations that are commonly faced by agents in real-world contexts. Thus, we consider negotiations in which there is incomplete information and time constraints. Incompleteness of information on the part of negotiators is a common feature of most practical negotiations. Also, agents typically have time constraints in the form of both deadlines and discount factors. Deadlines are an essential element since negotiation cannot go on indefinitely, rather it must end within a reasonable time limit. Likewise, discount factors are essential since the goods may be perishable or their value may decline due to inflation. Moreover, the strategic behaviour of agents with deadlines and discount factors differs from those without (see [21] for single issue bargaining without deadlines and [23, 13] for bargaining with deadlines and discount factors in the context of divisible issues).
Given this, we consider indivisible issues and first analyze the strategic behaviour of agents to obtain the equilibrium strategies for the case where all the issues for negotiation are known a priori to both agents. For this case, we show that the problem of finding the equilibrium offers is NP-hard, even in a complete information setting. Then, in order to overcome the problem of time complexity, we present strategies that are approximately optimal but computationally efficient, and show that they form an equilibrium. We also analyze the relative error (i.e., the difference between the true optimum and the approximate). The time complexity of the approximate equilibrium strategies is O(nm/ 2 ) where n is the negotiation deadline and the relative error. Finally, we extend the analysis to online negotiation where different issues become available at different time points and the agents are uncertain about their valuations for these issues. Specifically, we show that an approximate equilibrium exists for online negotiation and show that the expected difference between the optimum and the approximate is O( √ m) . These approximate strategies also have polynomial time complexity.
In so doing, our contribution lies in analyzing the computational complexity of the above multi-issue negotiation problem, and finding the approximate and online equilibria. No previous work has determined these equilibria. Since software agents have limited computational resources, our results are especially relevant to such resource bounded agents.
The remainder of the paper is organised as follows. We begin by giving a brief overview of single-issue negotiation in Section 2. In Section 3, we obtain the equilibrium for multi-issue negotiation and show that finding equilibrium offers is an NP-hard problem. We then present an approximate equilibrium and evaluate its approximation error. Section 4 analyzes online multi-issue negotiation.
Section 5 discusses the related literature and Section 6 concludes.
We adopt the single issue model of [27] because this is a model where, during negotiation, the parties are allowed to make offers from a set of discrete offers. Since our focus is on indivisible issues (i.e., parties are allowed to make one of two possible offers: zero or one), our scenario fits in well with [27]. Hence we use this basic single issue model and extend it to multiple issues. Before doing so, we give an overview of this model and its equilibrium strategies.
There are two strategic agents: a and b. Each agent has time constraints in the form of deadlines and discount factors. The two agents negotiate over a single indivisible issue (i). This issue is a ‘pie" of size 1 and the agents want to determine who gets the pie.
There is a deadline (i.e., a number of rounds by which negotiation must end). Let n ∈ N+ denote this deadline. The agents use an alternating offers protocol (as the one of Rubinstein [18]), which proceeds through a series of time periods. One of the agents, say a, starts negotiation in the first time period (i.e., t = 1) by making an offer (xi = 0 or 1) to b. Agent b can either accept or reject the offer. If it accepts, negotiation ends in an agreement with a getting xi and b getting yi = 1 − xi. Otherwise, negotiation proceeds to the next time period, in which agent b makes a counter-offer. This process of making offers continues until one of the agents either accepts an offer or quits negotiation (resulting in a conflict). Thus, there are three possible actions an agent can take during any time period: accept the last offer, make a new counter-offer, or quit the negotiation.
An essential feature of negotiations involving alternating offers is that the agents" utilities decrease with time [21]. Specifically, the decrease occurs at each step of offer and counteroffer. This decrease is represented with a discount factor denoted 0 < δi ≤ 1 for both1 agents.
Let [xt i, yt i ] denote the offer made at time period t where xt i and yt i denote the share for agent a and b respectively. Then, for a given pie, the set of possible offers is: {[xt i, yt i ] : xt i = 0 or 1, yt i = 0 or 1, and xt i + yt i = 1} At time t, if a and b receive a share of xt i and yt i respectively, then their utilities are: ua i (xt i, t) = j xt i × δt−1 if t ≤ n
ub i (yt i , t) = j yt i × δt−1 if t ≤ n
The conflict utility (i.e., the utility received in the event that no deal is struck) is zero for both agents.
For the above setting, the agents reason as follows in order to determine what to offer at t = 1. We let A(1) (B(1)) denote a"s (b"s) equilibrium offer for the first time period. Let agent a denote the first mover (i.e., at t = 1, a proposes to b who should get the pie). To begin, consider the case where the deadline for both agents is n = 1. If b accepts, the division occurs as agreed; if not, neither agent gets anything (since n = 1 is the deadline). Here, a is in a powerful position and is able to propose to keep 100 percent of the pie and give nothing to b 2 . Since the deadline is n = 1, b accepts this offer and agreement takes place in the first time period.
Now, consider the case where the deadline is n = 2. In order to decide what to offer in the first round, a looks ahead to t = 2 and reasons backwards. Agent a reasons that if negotiation proceeds to the second round, b will take 100 percent of the pie by offering [0, 1] and leave nothing for a. Thus, in the first time period, if a offers b anything less than the whole pie, b will reject the offer.
Hence, during the first time period, agent a offers [0, 1]. Agent b accepts this and an agreement occurs in the first time period.
In general, if the deadline is n, negotiation proceeds as follows.
As before, agent a decides what to offer in the first round by looking ahead as far as t = n and then reasoning backwards. Agent a"s 1 Having a different discount factor for different agents only makes the presentation more involved without leading to any changes in the analysis of the strategic behaviour of the agents or the time complexity of finding the equilibrium offers. Hence we have a single discount factor for both agents. 2 It is possible that b may reject such a proposal. However, irrespective of whether b accepts or rejects the proposal, it gets zero utility (because the deadline is n = 1). Thus, we assume that b accepts a"s offer.
offer for t = 1 depends on who the offering agent is for the last time period. This, in turn, depends on whether n is odd or even.
Since a makes an offer at t = 1 and the agents use the alternating offers protocol, the offering agent for the last time period is b if n is even and it is a if n is odd. Thus, depending on whether n is odd or even, a makes the following offer at t = 1: A(1) = j OFFER [1, 0] IF ODD n ACCEPT IF b"s TURN B(1) = j OFFER [0, 1] IF EVEN n ACCEPT IF a"s TURN Agent b accepts this offer and negotiation ends in the first time period. Note that the equilibrium outcome depends on who makes the first move. Since we have two agents and either of them could move first, we get two possible equilibrium outcomes.
On the basis of the above equilibrium for single-issue negotiation with complete information, we first obtain the equilibrium for multiple issues and then show that computing these offers is a hard problem. We then present a time efficient approximate equilibrium.
We first analyse the complete information setting. This section forms the base which we extend to the case of information uncertainty in Section 4.
Here a and b negotiate over m > 1 indivisible issues. These issues are m distinct pies and the agents want to determine how to distribute the pies between themselves. Let S = {1, 2, . . . , m} denote the set of m pies. As before, each pie is of size 1. Let the discount factor for issue c, where 1 ≤ c ≤ m, be 0 < δc ≤ 1.
For each issue, let n denote each agent"s deadline. In the offer for time period t (where 1 ≤ t ≤ n), agent a"s (b"s) share for each of the m issues is now represented as an m element vector xt ∈ Bm (yt ∈ Bm ) where B denotes the set {0, 1}. Thus, if agent a"s share for issue c at time t is xt c, then agent b"s share is yt c = (1−xt c). The shares for a and b are together represented as the package [xt , yt ].
As is traditional in multi-issue utility theory, we define an agent"s cumulative utility using the standard additive form [12]. The functions Ua : Bm × Bm × N+ → R and Ub : Bm × Bm × N+ → R give the cumulative utilities for a and b respectively at time t. These are defined as follows: Ua ([xt , yt ], t) = ( Σm c=1ka c ua c (xt c, t) if t ≤ n
(1) Ub ([xt , yt ], t) = ( Σm c=1kb cub c(yt c, t) if t ≤ n
(2) where ka ∈ Nm + denotes an m element vector of constants for agent a and kb ∈ Nm + that for b. Here N+ denotes the set of positive integers. These vectors indicate how the agents value different issues. For example, if ka c > ka c+1, then agent a values issue c more than issue c + 1. Likewise for agent b. In other words, the m issues are perfect substitutes (i.e., all that matters to an agent is its total utility for all the m issues and not that for any subset of them).
In all the settings we study, the issues will be perfect substitutes.
To begin each agent has complete information about all negotiation parameters (i.e., n, m, ka c , kb c, and δc for 1 ≤ c ≤ m).
Now, multi-issue negotiation can be done using different procedures. Broadly speaking, there are three key procedures for negotiating multiple issues [19]:
together as a bundle,
after another, and
parallel.
Between these three procedures, the package deal is known to generate Pareto optimal outcomes [19, 6]. Hence we adopt it here. We first give a brief description of the procedure and then determine the equilibrium strategies for it.
In this procedure, the agents use the same protocol as for singleissue negotiation (described in Section 2). However, an offer for the package deal includes a proposal for each issue under negotiation.
Thus, for m issues, an offer includes m divisions, one for each issue. Agents are allowed to either accept a complete offer (i.e., all m issues) or reject a complete offer. An agreement can therefore take place either on all m issues or on none of them.
As per the single-issue negotiation, an agent decides what to offer by looking ahead and reasoning backwards. However, since an offer for the package deal includes a share for all the m issues, the agents can now make tradeoffs across the issues in order to maximise their cumulative utilities.
For 1 ≤ c ≤ m, the equilibrium offer for issue c at time t is denoted as [at c, bt c] where at c and bt c denote the shares for agent a and b respectively. We denote the equilibrium package at time t as [at , bt ] where at ∈ Bm (bt ∈ Bm ) is an m element vector that denotes a"s (b"s) share for each of the m issues. Also, for
and 1 denote m element vectors of zeroes and ones respectively.
Note that for 1 ≤ t ≤ n, at c + bt c = 1 (i.e., the sum of the agents" shares (at time t) for each pie is one). Finally, for time period t (for
strategy for agent a (respectively b).
As mentioned in Section 1, the package deal allows agents to make tradeoffs. We let TRADEOFFA (TRADEOFFB) denote agent a"s (b"s) function for making tradeoffs. We let P denote a set of parameters to the procedure TRADEOFFA (TRADEOFFB) where P = {ka , kb , δ, m}.
Given this, the following theorem characterises the equilibrium for the package deal procedure.
THEOREM 1. For the package deal procedure, the following strategies form a Nash equilibrium. The equilibrium strategy for t = n is: A(n) = j OFFER [1, 0] IF a"s TURN ACCEPT IF b"s TURN B(n) = j OFFER [0, 1] IF b"s TURN ACCEPT IF a"s TURN For all preceding time periods t < n, if [xt , yt ] denotes the offer made at time t, then the equilibrium strategies are defined as follows: A(t) = 8 < : OFFER TRADEOFFA(P, UB(t), t) IF a"s TURN If (Ua ([xt , yt ], t) ≥ UA(t)) ACCEPT else REJECT IF b"s TURN B(t) = 8 < : OFFER TRADEOFFB(P, UA(t), t) IF b"s TURN If (Ub ([xt , yt ], t) ≥ UB(t)) ACCEPT else REJECT IF a"s TURN The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 953 where UA(t) = Ua ([at+1 , bt+1 ], t + 1) and UB(t) = Ub ([at+1 , bt+1 ], t + 1).
PROOF. We look ahead to the last time period (i.e., t = n) and then reason backwards. To begin, if negotiation reaches the deadline (n), then the agent whose turn it is takes everything and leaves nothing for its opponent. Hence, we get the strategies A(n) and B(n) as given in the statement of the theorem.
In all the preceding time periods (t < n), the offering agent proposes a package that gives its opponent a cumulative utility equal to what the opponent would get from its own equilibrium offer for the next time period. During time period t, either a or b could be the offering agent. Consider the case where a makes an offer at t. The package that a offers at t gives b a cumulative utility of Ub ([at+1 , bt+1 ], t + 1). However, since there is more than one issue, there is more than one package that gives b this cumulative utility. From among these packages, a offers the one that maximises its own cumulative utility (because it is a utility maximiser). Thus, the problem for a is to find the package [at , bt ] so as to: maximize mX c=1 ka c (1 − bt c)δt−1 c (3) such that mX c=1 bt ckb c ≥ UB(t) bt c = 0 or 1 for 1 ≤ c ≤ m where UB(t), δt−1 c , ka c , and kb c are constants and bt c (1 ≤ c ≤ m) is a variable.
Assume that the function TRADEOFFA takes parameters P, UB(t), and t, to solve the maximisation problem given in Equation 3 and returns the corresponding package. If there is more than one package that solves Equation 3, then TRADEOFFA returns any one of them (because agent a gets equal utility from all such packages and so does agent b). The function TRADEOFFB for agent b is analogous to that for a.
On the other hand, the equilibrium strategy for the agent that receives an offer is as follows. For time period t, let b denote the receiving agent. Then, b accepts [xt , yt ] if UB(t) ≤ Ub ([xt , yt ], t), otherwise it rejects the offer because it can get a higher utility in the next time period. The equilibrium strategy for a as receiving agent is defined analogously.
In this way, we reason backwards and obtain the offers for the first time period. Thus, we get the equilibrium strategies (A(t) and B(t)) given in the statement of the theorem.
The following example illustrates how the agents make tradeoffs using the above equilibrium strategies.
EXAMPLE 1. Assume there are m = 2 issues for negotiation, the deadline for both issues is n = 2, and the discount factor for both issues for both agents is δ = 1/2. Let ka
kb
using backward reasoning, a knows that if negotiation reaches the second time period (which is the deadline), then b will get a hundred percent of both the issues. This gives b a cumulative utility of UB(2) = 1/2 + 5/2 = 3. Thus, in the first time period, if b gets anything less than a utility of 3, it will reject a"s offer. So, at t = 1, a offers the package where it gets issue 1 and b gets issue 2. This gives a cumulative utility of 3 to a and 5 to b. Agent b accepts the package and an agreement takes place in the first time period.
The maximization problem in Equation 3 can be viewed as the 0-1 knapsack problem3 . In the 0-1 knapsack problem, we have a set 3 Note that for the case of divisible issues this is the fractional knapof m items where each item has a profit and a weight. There is a knapsack with a given capacity. The objective is to fill the knapsack with items so as to maximize the cumulative profit of the items in the knapsack. This problem is analogous to the negotiation problem we want to solve (i.e., the maximization problem of Equation 3).
Since ka c and δt−1 c are constants, maximizing Pm c=1 ka c (1−bt c)δt−1 c is the same as minimizing Pm c=1 ka c bt c. Hence Equation 3 can be written as: minimize mX c=1 ka c bt c (4) such that mX c=1 bt ckb c ≥ UB(t) bt c = 0 or 1 for 1 ≤ c ≤ m Equation 4 is a minimization version of the standard 0-1 knapsack problem4 with m items where ka c represents the profit for item c, kb c the weight for item c, and UB(t) the knapsack capacity.
Example 1 was for two issues and so it was easy to find the equilibrium offers. But, in general, it is not computationally easy to find the equilibrium offers of Theorem 1. The following theorem proves this.
THEOREM 2. For the package deal procedure, the problem of finding the equilibrium offers given in Theorem 1 is NP-hard.
PROOF. Finding the equilibrium offers given in Theorem 1 requires solving the 0-1 knapsack problem given in Equation 4. Since the 0-1 knapsack problem is NP-hard [17], the problem of finding equilibrium for the package deal is also NP-hard.
Researchers in the area of algorithms have found time efficient methods for computing approximate solutions to 0-1 knapsack problems [10]. Hence we use these methods to find a solution to our negotiation problem. At this stage, we would like to point out the main difference between solving the 0-1 knapsack problem and solving our negotiation problem. The 0-1 knapsack problem involves decision making by a single agent regarding which items to place in the knapsack. On the other hand, our negotiation problem involves two players and they are both strategic. Hence, in our case, it is not enough to just find an approximate solution to the knapsack problem, we must also show that such an approximation forms an equilibrium.
The traditional approach for overcoming the computational complexity in finding an equilibrium has been to use an approximate equilibrium (see [14, 26] for example). In this approach, a strategy profile is said to form an approximate Nash equilibrium if neither agent can gain more than the constant by deviating. Hence, our aim is to use the solution to the 0-1 knapsack problem proposed in [10] and show that it forms an approximate equilibrium to our negotiation problem. Before doing so, we give a brief overview of the key ideas that underlie approximation algorithms.
There are two key issues in the design of approximate algorithms [1]: sack problem. The factional knapsack problem is computationally easy; it can be solved in time polynomial in the number of items in the knapsack problem [17]. In contrast, the 0-1 knapsack problem is computationally hard. 4 Note that for the standard 0-1 knapsack problem the weights, profits and the capacity are positive integers. However a 0-1 knapsack problem with fractions and non positive values can easily be transformed to one with positive integers in time linear in m using the methods given in [8, 17].
The quality of an approximate algorithm is determined by comparing its performance to that of the optimal algorithm and measuring the relative error [3, 1]. The relative error is defined as (z−z∗ )/z∗ where z is the approximate solution and z∗ the optimal one. In general, we are interested in finding approximate algorithms whose relative error is bounded from above by a certain constant , i.e., (z − z∗ )/z∗ ≤ (5) Regarding the second issue of time complexity, we are interested in finding fully polynomial approximation algorithms. An approximation algorithm is said to be fully polynomial if for any > 0 it finds a solution satisfying Equation 5 in time polynomially bounded by size of the problem (for the 0-1 knapsack problem, the problem size is equal to the number of items) and by 1/ [1].
For the 0-1 knapsack problem, Ibarra and Kim [10] presented a fully polynomial approximation method. This method is based on dynamic programming. It is a parametric method that takes as a parameter and for any > 0, finds a heuristic solution z with relative error at most , such that the time and space complexity grow polynomially with the number of items m and 1/ . More specifically, the space and time complexity are both O(m/ 2 ) and hence polynomial in m and 1/ (see [10] for the detailed approximation algorithm and proof of time and space complexity).
Since the Ibarra and Kim method is fully polynomial, we use it to solve our negotiation problem. This is done as follows. For agent a, let APRX-TRADEOFFA(P, UB(t), t, ) denote a procedure that returns an approximate solution to Equation 4 using the Ibarra and Kim method. The procedure APRX-TRADEOFFB(P, UA(t), t, ) for agent b is analogous.
For 1 ≤ c ≤ m, the approximate equilibrium offer for issue c at time t is denoted as [¯at c,¯bt c] where ¯at c and ¯bt c denote the shares for agent a and b respectively. We denote the equilibrium package at time t as [¯at ,¯bt ] where ¯at ∈ Bm (¯bt ∈ Bm ) is an m element vector that denotes a"s (b"s) share for each of the m issues. Also, as before, for 1 ≤ c ≤ m, δc is the discount factor for issue c.
Note that for 1 ≤ t ≤ n, ¯at c + ¯bt c = 1 (i.e., the sum of the agents" shares (at time t) for each pie is one). Finally, for time period t (for
equilibrium strategy for agent a (respectively b).The following theorem uses this notation and characterizes an approximate equilibrium for multi-issue negotiation.
THEOREM 3. For the package deal procedure, the following strategies form an approximate Nash equilibrium. The equilibrium strategy for t = n is: ¯A(n) = j OFFER [1, 0] IF a"s TURN ACCEPT IF b"s TURN ¯B(n) = j OFFER [0, 1] IF b"s TURN ACCEPT IF a"s TURN For all preceding time periods t < n, if [xt , yt ] denotes the offer made at time t, then the equilibrium strategies are defined as follows: ¯A(t) = 8 < : OFFER APRX-TRADEOFFA(P, UB(t), t, ) IF a"s TURN If (Ua ([xt , yt ], t) ≥ UA(t)) ACCEPT else REJECT IF b"s TURN ¯B(t) = 8 < : OFFER APRX-TRADEOFFB(P, UA(t), t, ) IF b"s TURN If (Ub ([xt , yt ], t) ≥ UB(t)) ACCEPT else REJECT IF a"s TURN where UA(t) = Ua ([¯at+1 ,¯bt+1 ], t + 1) and UB(t) = Ub ([¯at+1 , ¯bt+1 ], t + 1). An agreement takes place at t = 1.
PROOF. As in the proof for Theorem 1, we use backward reasoning. We first obtain the strategies for the last time period t = n.
It is straightforward to get these strategies; the offering agent gets a hundred percent of all the issues.
Then for t = n − 1, the offering agent must solve the maximization problem of Equation 4 by substituting t = n−1 in it. For agent a (b), this is done by APPROX-TRADEOFFA (APPROX-TRADEOFFB).
These two functions are nothing but the Ibarra and Kim"s approximation method for solving the 0-1 knapsack problem. These two functions take as a parameter and use the Ibarra and Kim"s approximation method to return a package that approximately maximizes Equation 4. Thus, the relative error for these two functions is the same as that for Ibarra and Kim"s method (i.e., it is at most where is given in Equation 5).
Assume that a is the offering agent for t = n − 1. Agent a must offer a package that gives b a cumulative utility equal to what it would get from its own approximate equilibrium offer for the next time period (i.e., Ub ([¯at+1 ,¯bt+1 ], t + 1) where [¯at+1 ,¯bt+1 ] is the approximate equilibrium package for the next time period). Recall that for the last time period, the offering agent gets a hundred percent of all the issues. Since a is the offering agent for t = n − 1 and the agents use the alternating offers protocol, it is b"s turn at t = n. Thus Ub ([¯at+1 ,¯bt+1 ], t + 1) is equal to b"s cumulative utility from receiving a hundred percent of all the issues. Using this utility as the capacity of the knapsack, a uses APPROX-TRADEOFFA and obtains the approximate equilibrium package for t = n − 1.
On the other hand, if b is the offering agent at t = n − 1, it uses APPROX-TRADEOFFB to obtain the approximate equilibrium package.
In the same way for t < n − 1, the offering agent (say a) uses APPROX-TRADEOFFA to find an approximate equilibrium package that gives b a utility of Ub ([¯at+1 ,¯bt+1 ], t + 1). By reasoning backwards, we obtain the offer for time period t = 1. If a (b) is the offering agent, it proposes the offer APPROX-TRADEOFFA(P, UB(1), 1, ) (APPROX-TRADEOFFB(P, UA(1), 1, )). The receiving agent accepts the offer. This is because the relative error in its cumulative utility from the offer is at most . An agreement therefore takes place in the first time period.
THEOREM 4. The time complexity of finding the approximate equilibrium offer for the first time period is O(nm/ 2 ).
PROOF. The time complexity of APPROX-TRADEOFFA and APPROXTRADEOFFB is the same as the time complexity of the Ibarra and Kim method [10] i.e., O(m/ 2 )). In order to find the equilibrium offer for the first time period using backward reasoning,
APPROXTRADEOFFA (or APPROX- TRADEOFFB) is invoked n times. Hence the time complexity of finding the approximate equilibrium offer for the first time period is O(nm/ 2 ).
This analysis was done in a complete information setting.
However an extension of this analysis to an incomplete information setting where the agents have probability distributions over some uncertain parameter is straightforward, as long as the negotiation is done offline; i.e., the agents know their preference for each individual issue before negotiation begins. For instance, consider the case where different agents have different discount factors, and each agent is uncertain about its opponent"s discount factor although it knows its own. This uncertainty is modelled with a probability distribution over the possible values for the opponent"s discount factor and having this distribution as common knowledge to the agents.
All our analysis for the complete information setting still holds for The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 955 this incomplete information setting, except for the fact that an agent must now use the given probability distribution and find its opponent"s expected utility instead of its actual utility. Hence, instead of analyzing an incomplete information setting for offline negotiation, we focus on online multi-issue negotiation.
We now consider a more general and, arguably more realistic, version of multi-issue negotiation, where the agents are uncertain about the issues they will have to negotiate about in future. In this setting, when negotiating an issue, the agents know that they will negotiate more issues in the future, but they are uncertain about the details of those issues. As before, let m be the total number of issues that are up for negotiation. The agents have a probability distribution over the possible values of ka c and kb c. For 1 ≤ c ≤ m let ka c and kb c be uniformly distributed over [0,1]. This probability distribution, n, and m are common knowledge to the agents. However, the agents come to know ka c and kb c only just before negotiation for issue c begins. Once the agents reach an agreement on issue c, it cannot be re-negotiated.
This scenario requires online negotiation since the agents must make decisions about an issue prior to having the information about the future issues [3]. We first give a brief introduction to online problems and then draw an analogy between the online knapsack problem and the negotiation problem we want to solve.
In an online problem, data is given to the algorithm incrementally, one unit at a time [3]. The online algorithm must also produce the output incrementally: after seeing i units of input it must output the ith unit of output. Since decisions about the output are made with incomplete knowledge about the entire input, an online algorithm often cannot produce an optimal solution. Such an algorithm can only approximate the performance of the optimal algorithm that sees all the inputs in advance. In the design of online algorithms, the main aim is to achieve a performance that is close to that of the optimal offline algorithm on each input. An online algorithm is said to be stochastic if it makes decisions on the basis of the probability distributions for the future inputs. The performance of stochastic online algorithms is assessed in terms of the expected difference between the optimum and the approximate solution (denoted E[z∗ m −zm] where z∗ m is the optimal and zm the approximate solution). Note that the subscript m is used to indicate the fact that this difference depends on m.
We now describe the protocol for online negotiation and then obtain an approximate equilibrium. The protocol is defined as follows. Let agent a denote the first mover (since we focus on the package deal procedure, the first mover is the same for all the m issues).
Step 1. For c = 1, the agents are given the values of ka c and kb c.
These two values are now common5 knowledge.
Step 2. The agents settle issue c using the alternating offers protocol described in Section 2. Negotiation for issue c must end within n time periods from the start of negotiation on the issue. If an agreement is not reached within this time, then negotiation fails on this and on all remaining issues.
Step 3. The above steps are repeated for issues c = 2, 3, . . . , m.
Negotiation for issue c (2 ≤ c ≤ m) begins in the time period following an agreement on issue c − 1. 5 We assume common knowledge because it simplifies exposition.
However, if ka c (kb c) is a"s (b"s) private knowledge, then our analysis will still hold but now an agent must find its opponent"s expected utility on the basis of the p.d.fs for ka c and kb c.
Thus, during time period t, the problem for the offering agent (say a) is to find the optimal offer for issue c on the basis of ka c and kb c and the probability distribution for ka i and kb i (c < i ≤ m).
In order to solve this online negotiation problem we draw analogy with the online knapsack problem. Before doing so, however, we give a brief overview of the online knapsack problem.
In the online knapsack problem, there are m items. The agent must examine the m items one at a time according to the order they are input (i.e., as their profit and size coefficients become known).
Hence, the algorithm is required to decide whether or not to include each item in the knapsack as soon as its weight and profit become known, without knowledge concerning the items still to be seen, except for their total number. Note that since the agents have a probability distribution over the weights and profits of the future items, this is a case of stochastic online knapsack problem. Our online negotiation problem is analogous to the online knapsack problem. This analogy is described in detail in the proof for Theorem 5.
Again, researchers in algorithms have developed time efficient approximate solutions to the online knapsack problem [16]. Hence we use this solution and show that it forms an equilibrium.
The following theorem characterizes an approximate equilibrium for online negotiation. Here the agents have to choose a strategy without knowing the features of the future issues. Because of this information incompleteness, the relevant equilibrium solution is that of a Bayes" Nash Equilibrium (BNE) in which each agent plays the best response to the other agents with respect to their expected utilities [18]. However, finding an agent"s BNE strategy is analogous to solving the online 0-1 knapsack problem. Also, the online knapsack can only be solved approximately [16]. Hence the relevant equilibrium solution concept is approximate BNE (see [26] for example). The following theorem finds this equilibrium using procedures ONLINE- TRADEOFFA and ONLINE-TRADEOFFB which are defined in the proof of the theorem. For a given time period, we let zm denote the approximately optimal solution generated by ONLINE-TRADEOFFA (or ONLINE-TRADEOFFB) and z∗ m the actual optimum.
THEOREM 5. For the package deal procedure, the following strategies form an approximate Bayes" Nash equilibrium. The equilibrium strategy for t = n is: A(n) = j OFFER [1, 0] IF a"s TURN ACCEPT IF b"s TURN B(n) = j OFFER [0, 1] IF b"s TURN ACCEPT IF a"s TURN For all preceding time periods t < n, if [xt , yt ] denotes the offer made at time t, then the equilibrium strategies are defined as follows: A(t) = 8 < : OFFER ONLINE-TRADEOFFA(P, UB(t), t) IF a"s TURN If (Ua ([xt , yt ], t) ≥ UA(t)) ACCEPT else REJECT IF b"s TURN B(t) = 8 < : OFFER ONLINE-TRADEOFFB(P, UA(t), t) IF b"s TURN If (Ub ([xt , yt ], t) ≥ UB(t)) ACCEPT else REJECT IF a"s TURN where UA(t) = Ua ([¯at+1 ,¯bt+1 ], t + 1) and UB(t) = Ub ([¯at+1 , ¯bt+1 ], t + 1). An agreement on issue c takes place at t = c. For a given time period, the expected difference between the solution generated by the optimal strategy and that by the approximate strategy is E[z∗ m − zm] = O( √ m).
PROOF. As in Theorem 1 we find the equilibrium offer for time period t = 1 using backward induction. Let a be the offering agent for t = 1 for all the m issues. Consider the last time period t = n (recall from Step 2 of the online protocol that n is the deadline for completing negotiation on the first issue). Since the first mover is the same for all the issues, and the agents make offers alternately, the offering agent for t = n is also the same for all the m issues.
Assume that b is the offering agent for t = n. As in Section 3, the offering agent for t = n gets a hundred percent of all the m issues. Since b is the offering agent for t = n, his utility for this time period is: UB(n) = kb 1δn−1
mX i=2 δ i(n−1) i (6) Recall that ka i and kb i (for c < i ≤ m) are not known to the agents. Hence, the agents can only find their expected utilities from the future issues on the basis of the probability distribution functions for ka i and kb i . However, during the negotiation for issue c the agents know ka c but not kb c (see Step 1 of the online protocol).
Hence, a computes UB(n) as follows. Agent b"s utility from issue c = 1 is kb 1δn−1
on the basis of the probability distribution functions for ka i and kb i , agent a computes b"s expected utility from each future issue i as δ i(n−1) i /2 (since ka i and kb i are uniformly distributed on [0, 1]).
Thus, b"s expected cumulative utility from these m − c issues is 1/2 Pm i=2 δ i(n−1) i (which is the second term of Equation 6).
Now, in order to decide what to offer for issue c = 1, the offering agent for t = n − 1 (i.e., agent a) must solve the following online knapsack problem: maximize Σm i=1ka i (1 − ¯bt i)δn−1 i (7) such that Σm i=1kb i ¯bt i ≥ UB(n) ¯bt i = 0 or 1 for 1 ≤ i ≤ m The only variables in the above maximization problem are ¯bt i. Now, maximizing Σm i=1ka i (1−¯bt i)δn−1 i is the same as minimizing Σm i=1ka i ¯bt i since δn−1 i and ka i are constants. Thus, we write Equation 7 as: minimize Σm i=1ka i ¯bt i (8) such that Σm i=1kb i ¯bt i ≥ UB(n) ¯bt i = 0 or 1 for 1 ≤ i ≤ m The above optimization problem is analogous to the online 0-1 knapsack problem. An algorithm to solve the online knapsack problem has already proposed in [16]. This algorithm is called the fixed-choice online algorithm. It has time complexity linear in the number of items (m) in the knapsack problem. We use this to solve our online negotiation problem. Thus, our ONLINE-TRADEOFFA algorithm is nothing but the fixed-choice online algorithm and therefore has the same time complexity as the latter. This algorithm takes the values of ka i and kb i one at a time and generates an approximate solution to the above knapsack problem. The expected difference between the optimum and approximate solution is E[z∗ m − zm] = O( √ m) [16] (see [16] for the detailed fixed-choice online algorithm and a proof for E[z∗ m − zm] = O( √ m)).
The fixed-choice online algorithm of [16] is a generalization of the basic greedy algorithm for the offline knapsack problem; the idea behind it is as follows. A threshold value is determined on the basis of the information regarding weights and profits for the 0-1 knapsack problem. The method then includes into the knapsack all items whose profit density (profit density of an item is its profit per unit weight) exceeds the threshold until either the knapsack is filled or all the m items have been considered.
In more detail, the algorithm ONLINE-TRADEOFFA works as follows. It first gets the values of ka
c. Since we have a 0-1 knapsack problem, ¯bt c can be either zero or one. Now, if ¯bt c = 1 for t = n, then ¯bt c must be one for 1 ≤ t < n (i.e., a must offer ¯bt c = 1 at t = 1). If ¯bt c = 1 for t = n, but a offers ¯bt c = 0 at t = 1, then agent b gets less utility than what it expects from a"s offer and rejects the proposal. Thus, if ¯bt c = 1 for t = n, then the optimal strategy for a is to offer ¯bt c = 1 at t = 1. Agent b accepts the offer. Thus, negotiation on the first issue starts at t = 1 and an agreement on it is also reached at t = 1.
In the next time period (i.e., t = 2), negotiation proceeds to the next issue. The deadline for the second issue is n time periods from the start of negotiation on the issue. For c = 2, the algorithm ONLINE-TRADEOFFA is given the values of ka
c as described above. Agent offers bc at t = 2 and b accepts. Thus, negotiation on the second issue starts at t = 2 and an agreement on it is also reached at t = 2.
This process repeats for the remaining issues c = 3, . . . , m.
Thus, each issue is agreed upon in the same time period in which it starts. As negotiation for the next issue starts in the following time period (see step 3 of the online protocol), agreement on issue i occurs at time t = i.
On the other hand, if b is the offering agent at t = 1, he uses the algorithm ONLINE-TRADEOFFB which is defined analogously.
Thus, irrespective of who makes the first move, all the m issues are settled at time t = m.
THEOREM 6. The time complexity of finding the approximate equilibrium offers of Theorem 5 is linear in m.
PROOF. The time complexity of ONLINE-TRADEOFFA and ONLINETRADEOFFB is the same as the time complexity of the fixed-choice online algorithm of [16]. Since the latter has time complexity linear in m, the time complexity of ONLINE-TRADEOFFA and ONLINETRADEOFFB is also linear in m.
It is worth noting that, for the 0-1 knapsack problem, the lower bound on the expected difference between the optimum and the solution found by any online algorithm is Ω(1) [16]. Thus, it follows that this lower bound also holds for our negotiation problem.
Work on multi-issue negotiation can be divided into two main types: that for indivisible issues and that for divisible issues. We first describe the existing work for the case of divisible issues. Since Schelling [24] first noted that the outcome of negotiation depends on the choice of negotiation procedure, much research effort has been devoted to the study of different procedures for negotiating multiple issues. However, most of this work has focussed on the sequential procedure [7, 2]. For this procedure, a key issue is the negotiation agenda. Here the term agenda refers to the order in which the issues are negotiated. The agenda is important because each agent"s cumulative utility depends on the agenda; if we change the agenda then these utilities change. Hence, the agents must decide what agenda they will use. Now, the agenda can be decided before negotiating the issues (such an agenda is called exogenous) or it may be decided during the process of negotiation (such an agenda is called endogenous). For instance, Fershtman [7] analyze sequential negotiation with exogenous agenda. A number of researchers have also studied negotiations with an endogenous agenda [2].
In contrast to the above work that mainly deals with sequential negotiation, [6] studies the equilibrium for the package deal procedure. However, all the above mentioned work differs from ours in that we focus on indivisible issues while others focus on the case The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 957 where each issue is divisible. Specifically, no previous work has determined an approximate equilibrium for multi-issue negotiation or for online negotiation.
Existing work for the case of indivisible issues has mostly dealt with task allocation problems (for tasks that cannot be partioned) to a group of agents. The problem of task allocation has been previously studied in the context of coalitions involving more than two agents. For example [25] analyze the problem for the case where the agents act so as to maximize the benefit of the system as a whole. In contrast, our focus is on two agents where both of them are self-interested and want to maximize their individual utilities. On the other hand [22] focus on the use of contracts for task allocation to multiple self interested agents but this work concerns finding ways of decommitting contracts (after the initial allocation has been done) so as to improve an agent"s utility. In contrast, our focuses on negotiation regarding who will carry out which task.
Finally, online and approximate mechanisms have been studied in the context of auctions [14, 9] but not for bilateral negotiations (which is the focus of our work).
This paper has studied bilateral multi-issue negotiation between self-interested autonomous agents with time constraints. The issues are indivisible and different agents value different issues differently. Thus, the problem is for the agents to decide how to allocate the issues between themselves so as to maximize their individual utilities. Specifically, we first showed that finding the equilibrium offers is an NP-hard problem even in a complete information setting. We then presented approximately optimal negotiation strategies and showed that they form an equilibrium. These strategies have polynomial time complexity. We also analysed the difference between the true optimum and the approximate optimum. Finally, we extended the analysis to online negotiation where the issues become available at different time points and the agents are uncertain about the features of these issues. Specifically, we showed that an approximate equilibrium exists for online negotiation and analysed the approximation error. These approximate strategies also have polynomial time complexity.
There are several interesting directions for future work. First, for online negotiation, we assumed that the constants ka c and kb c are both uniformly distributed. It will be interesting to analyze the case where ka c and kb c have other, possibly different, probability distributions. Apart from this, we treated the number of issues as being common knowledge to the agents. In future, it will be interesting to treat the number of issues as uncertain.

Given that negotiation is perhaps one of the oldest activities in the history of human communication, it"s perhaps surprising that conducted experiments on negotiations have shown that negotiators more often than not reach inefficient compromises [1, 21]. Raiffa [17] and Sebenius [20] provide analyses on the negotiators" failure to achieve efficient agreements in practice and their unwillingness to disclose private information due to strategic reasons. According to conflict theorists Lax and Sebenius [13], most negotiation actually involves both integrative and distributive bargaining which they refer to as creating value and claiming value. They argue that negotiation necessarily includes both cooperative and competitive elements, and that these elements exist in tension. Negotiators face a dilemma in deciding whether to pursue a cooperative or a competitive strategy at a particular time during a negotiation. They refer to this problem as the Negotiator"s Dilemma.
We argue that the Negotiator"s Dilemma is essentially informationbased, due to the private information held by the agents. Such private information contains both the information that implies the agent"s bottom lines (or, her walk-away positions) and the information that enforces her bargaining strength. For instance, when bargaining to sell a house to a potential buyer, the seller would try to hide her actual reserve price as much as possible for she hopes to reach an agreement at a much higher price than her reserve price. On the other hand, the outside options available to her (e.g. other buyers who have expressed genuine interest with fairly good offers) consist in the information that improves her bargaining strength about which she would like to convey to her opponent.
But at the same time, her opponent is well aware of the fact that it is her incentive to boost her bargaining strength and thus will not accept every information she sends out unless it is substantiated by evidence.
Coming back to the Negotiator"s Dilemma, it"s not always possible to separate the integrative bargaining process from the distributive bargaining process. In fact, more often than not, the two processes interplay with each other making information manipulation become part of the integrative bargaining process. This is because a negotiator could use the information about his opponent"s interests against her during the distributive negotiation process. That is, a negotiator may refuse to concede on an important conflicting issue by claiming that he has made a major concession (on another issue) to meet his opponent"s interests even though the concession he made could be insignificant to him. For instance, few buyers would start a bargaining with a dealer over a deal for a notebook computer by declaring that he is most interested in an extended warranty for the item and therefore prepared to pay a high price to get such an extended warranty.
Negotiation Support Systems (NSSs) and negotiating software 508 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agents (NSAs) have been introduced either to assist humans in making decisions or to enable automated negotiation to allow computer processes to engage in meaningful negotiation to reach agreements (see, for instance, [14, 15, 19, 6, 5]). However, because of the Negotiator"s Dilemma and given even bargaining power and incomplete information, the following two undesirable situations often arise: (i) negotiators reach inefficient compromises, or (ii) negotiators engage in a deadlock situation in which both negotiators refuse to act upon with incomplete information and at the same time do not want to disclose more information.
In this paper, we argue for the role of a mediator to resolve the above two issues. The mediator thus plays two roles in a negotiation: (i) to encourage cooperative behaviour among the negotiators, and (ii) to absorb the information disclosure by the negotiators to prevent negotiators from using uncertainty and private information as a strategic device. To take advantage of existing results in negotiation analysis and operations research (OR) literatures [18], we employ multi-criteria decision making (MCDM) theory to allow the negotiation problem to be represented and analysed. Section 2 provides background on MCDM theory and the negotiation framework. Section 3 formulates the problem. In Section 4, we discuss our approach to integrative negotiation. Section 5 discusses the future work with some concluding remarks.
Let A denote the set of feasible alternatives available to a decision maker M. As an act, or decision, a in A may involve multiple aspects, we usually describe the alternatives a with a set of attributes j; (j = 1, . . . , m). (Attributes are also referred to as issues, or decision variables.) A typical decision maker also has several objectives X1, . . . , Xk. We assume that Xi, (i = 1, . . . , k), maps the alternatives to real numbers. Thus, a tuple (x1, . . . , xk) = (X1(a), . . . , Xk(a)) denotes the consequence of the act a to the decision maker M. By definition, objectives are statements that delineate the desires of a decision maker. Thus, M wishes to maximise his objectives. However, as discussed thoroughly by Keeney and Raiffa [8], it is quite likely that a decision maker"s objectives will conflict with each other in that the improved achievement with one objective can only be accomplished at the expense of another.
For instance, most businesses and public services have objectives like minimise cost and maximise the quality of services. Since better services can often only be attained for a price, these objectives conflict.
Due to the conflicting nature of a decision maker"s objectives, M usually has to settle at a compromise solution. That is, he may have to choose an act a ∈ A that does not optimise every objective. This is the topic of the multi-criteria decision making theory. Part of the solution to this problem is that M has to try to identify the Pareto frontier in the consequence space {(X1(a), . . . , Xk(a))}a∈A.
DEFINITION 1. (Dominant) Let x = (x1, . . . , xk) and x = (x1, . . . , xk) be two consequences. x dominates x iff xi > xi for all i, and the inequality is strict for at least one i.
The Pareto frontier in a consequence space then consists of all consequences that are not dominated by any other consequence.
This is illustrated in Fig. 1 in which an alternative consists of two attributes d1 and d2 and the decision maker tries to maximise the two objectives X1 and X2. A decision a ∈ A whose consequence does not lie on the Pareto frontier is inefficient. While the Pareto 1x d2 a (X (a),X (a)) d1 1 x2 2 Alternative spaceA Pareto frontier Consequence space optimal consequenc Figure 1: The Pareto frontier frontier allows M to avoid taking inefficient decisions, M still has to decide which of the efficient consequences on the Pareto frontier is most preferred by him.
MCDM theorists introduce a mechanism to allow the objective components of consequences to be normalised to the payoff valuations for the objectives. Consequences can then be ordered: if the gains in satisfaction brought about by C1 (in comparison to C2) equals to the losses in satisfaction brought about by C1 (in comparison to C2), then the two consequences C1 and C2 are considered indifferent. M can now construct the set of indifference curves1 in the consequence space (the dashed curves in Fig. 1). The most preferred indifference curve that intersects with the Pareto frontier is in focus: its intersection with the Pareto frontier is the sought after consequence (i.e., the optimal consequence in Fig. 1).
A multi-agent negotiation framework consists of:
issues the agents are negotiating over. Each attribute α can take a value from the set V alα;
represented by an assignment of values to the corresponding attributes in Att.
making [8], we define the agents" utility as follows: • Objectives: Agent i has a set of ni objectives, or interests; denoted by j (j = 1, . . . , ni). To measure how much an outcome o fulfills an objective j to an agent i, we use objective functions: for each agent i, we define i"s interests using the objective vector function fi = [fij ] : O → Rni . • Value functions: Instead of directly evaluating an outcome o, agent i looks at how much his objectives are fulfilled and will make a valuation based on these more basic criteria. Thus, for each agent i, there is a value function σi : Rni → R.
In particular, Raiffa [17] shows how to systematically construct an additive value function to each party involved in a negotiation. • Utility: Now, given an outcome o ∈ O, an agent i is able to determine its value, i.e., σi(fi(o)). However, a negotiation infrastructure is usually required to facilitate negotiation.
This might involve other mechanisms and factors/parties, e.g., a mediator, a legal institution, participation fees, etc. The standard way to implement such a thing is to allow money 1 In fact, given the k-dimensional space, these should be called indifference surfaces. However, we will not bog down to that level of details.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 509 and side-payments. In this paper, we ignore those side-effects and assume that agent i"s utility function ui is normalised so that ui : O → [0, 1].
EXAMPLE 1. There are two agents, A and B. Agent A has a task T that needs to be done and also 100 units of a resource R. Agent B has the capacity to perform task T and would like to obtain at least 10 and at most 20 units of the resource R. Agent B is indifferent on any amount between 10 and 20 units of the resource R. The objective functions for both agents A and B are cost and revenue. And they both aim at minimising costs while maximising revenues. Having T done generates for A a revenue rA,T while doing T incurs a cost cB,T to B. Agent B obtains a revenue rB,R for each unit of the resource R while providing each unit of the resource R costs agent A cA,R.
Assuming that money transfer between agents is possible, the set Att then contains three attributes: • T, taking values from the set {0, 1}, indicates whether the task T is assigned to agent B; • R, taking values from the set of non-negative integer, indicates the amount of resource R being allocated to agent B; and • MT, taking values from R, indicates the payment p to be transferred from A to B.
Consider the outcome o = [T = 1, R = k, MT = p], i.e., the task T is assigned to B, and A allocates to B with k units of the resource R, and A transfers p dollars to B. Then, costA(o) = k.cA,R + p and revA(o) = rA,T ; and costB(o) = cB,T and revA(o) = j k.rB,R + p if 10 ≤ k ≤ 20 p otherwise.
And, σi(costi(o), revi(o)) = revi(o) − costi(o), (i = A, B).
Consider Example 1, assume that rA,T = $150 and cB,T = $100 and rB,R = $10 and cA,R = $7. That is, the revenues generated for A exceeds the costs incurred to B to do task T, and B values resource R more highly than the cost for A to provide it.
The optimal solution to this problem scenario is to assign task T to agent B and to allocate 20 units of resource R (i.e., the maximal amount of resource R required by agent B) from agent A to agent B. This outcome regarding the resource and task allocation problems leaves payoffs of $10 to agent A and $100 to agent B.2 Any other outcome would leave at least one of the agents worse off. In other words, the presented outcome is Pareto-efficient and should be part of the solution outcome for this problem scenario.
However, as the agents still have to bargain over the amount of money transfer p, neither agent would be willing to disclose their respective costs and revenues regarding the task T and the resource R. As a consequence, agents often do not achieve the optimal outcome presented above in practice. To address this issue, we introduce a mediator to help the agents discover better agreements than the ones they might try to settle on. Note that this problem is essentially the problem of searching for joint gains in a multilateral negotiation in which the involved parties hold strategic information, i.e., the integrative part in a negotiation. In order to help facilitate this process, we introduce the role of a neutral mediator. Before formalising the decision problems faced by the mediator and the 2 Certainly, without money transfer to compensate agent A, this outcome is not a fair one. negotiating agents, we discuss the properties of the solution outcomes to be achieved by the mediator. In a negotiation setting, the two typical design goals would be: • Efficiency: Avoid the agents from settling on an outcome that is not Pareto-optimal; and • Fairness: Avoid agreements that give the most of the gains to a subset of agents while leaving the rest with too little.
The above goals are axiomatised in Nash"s seminal work [16] on cooperative negotiation games. Essentially, Nash advocates for the following properties to be satisfied by solution to the bilateral negotiation problem: (i) it produces only Pareto-optimal outcomes; (ii) it is invariant to affine transformation (to the consequence space); (iii) it is symmetric; and (iv) it is independent from irrelevant alternatives. A solution satisfying Nash"s axioms is called a Nash bargaining solution.
It then turns out that, by taking the negotiators" utilities as its objectives the mediator itself faces a multi-criteria decision making problem. The issues faced by the mediator are: (i) the mediator requires access to the negotiators" utility functions, and (ii) making (fair) tradeoffs between different agents" utilities. Our methods allow the agents to repeatedly interact with the mediator so that a Nash solution outcome could be found by the parties.
Informally, the problem faced by both the mediator and the negotiators is construction of the indifference curves. Why are the indifference curves so important? • To the negotiators, knowing the options available along indifference curves opens up opportunities to reach more efficient outcomes. For instance, consider an agent A who is presenting his opponent with an offer θA which she refuses to accept. Rather than having to concede, A could look at his indifference curve going through θA and choose another proposal θA. To him, θA and θA are indifferent but θA could give some gains to B and thus will be more acceptable to B.
In other words, the outcome θA is more efficient than θA to these two negotiators. • To the mediator, constructing indifference curves requires a measure of fairness between the negotiators. The mediator needs to determine how much utility it needs to take away from the other negotiators to give a particular negotiator a specific gain G (in utility).
In order to search for integrative solutions within the outcome space O, we characterise the relationship between the agents over the set of attributes Att. As the agents hold different objectives and have different capacities, it may be the case that changing between two values of a specific attribute implies different shifts in utility of the agents. However, the problem of finding the exact Paretooptimal set3 is NP-hard [2].
Our approach is thus to solve this optimisation problem in two steps. In the first steps, the more manageable attributes will be solved. These are attributes that take a finite set of values. The result of this step would be a subset of outcomes that contains the Pareto-optimal set. In the second step, we employ an iterative procedure that allows the mediator to interact with the negotiators to find joint improvements that move towards a Pareto-optimal outcome. This approach will not work unless the attributes from Att 3 The Pareto-optimal set is the set of outcomes whose consequences (in the consequence space) correspond to the Pareto frontier.
are independent. Most works on multi-attribute, or multi-issue, negotiation (e.g. [17]) assume that the attributes or the issues are independent, resulting in an additive value function for each agent.4 ASSUMPTION 1. Let i ∈ N and S ⊆ Att. Denote by ¯S the set Att \ S. Assume that vS and vS are two assignments of values to the attributes of S and v1 ¯S, v2 ¯S are two arbitrary value assignments to the attributes of ¯S, then (ui([vS, v1 ¯S]) − ui([vS, v2 ¯S])) = (ui([vS, v1 ¯S])−ui([vS, v2 ¯S])). That is, the utility function of agent i will be defined on the attributes from S independently of any value assignment to other attributes.
NEGOTIATIONS As discussed by Lax and Sebenius [13], under incomplete information the tension between creating and claiming values is the primary cause of inefficient outcomes. This can be seen most easily in negotiations involving two negotiators; during the distributive phase of the negotiation, the two negotiators"s objectives are directly opposing each other. We will now formally characterise this relationship between negotiators by defining the opposition between two negotiating parties. The following exposition will be mainly reproduced from [9].
Assuming for the moment that all attributes from Att take values from the set of real numbers R, i.e., V alj ⊆ R for all j ∈ Att. We further assume that the set O = ×j∈AttV alj of feasible outcomes is defined by constraints that all parties must obey and O is convex.
Now, an outcome o ∈ O is just a point in the m-dimensional space of real numbers. Then, the questions are: (i) from the point of view of an agent i, is o already the best outcome for i? (ii) if o is not the best outcome for i then is there another outcome o such that o gives i a better utility than o and o does not cause a utility loss to the other agent j in comparison to o?
The above questions can be answered by looking at the directions of improvement of the negotiating parties at o, i.e., the directions in the outcome space O into which their utilities increase at point o. Under the assumption that the parties" utility functions ui are differentiable concave, the set of all directions of improvement for a party at a point o can be defined in terms of his most preferred, or gradient, direction at that point. When the gradient direction ∇ui(o) of agent i at point o is outright opposing to the gradient direction ∇uj (o) of agent j at point o then the two parties strongly disagree at o and no joint improvements can be achieved for i and j in the locality surrounding o.
Since opposition between the two parties can vary considerably over the outcome space (with one pair of outcomes considered highly antagonistic and another pair being highly cooperative), we need to describe the local properties of the relationship. We begin with the opposition at any point of the outcome space Rm . The following definition is reproduced from [9]: DEFINITION 2. 1. The parties are in local strict opposition at a point x ∈ Rm iff for all points x ∈ Rm that are sufficiently close to x (i.e., for some > 0 such that ∀x x −x < ), an increase of one utility can be achieved only at the expense of a decrease of the other utility.
Rm iff they are not in local strict opposition at x, i.e., iff it is possible for both parties to raise their utilities by moving an infinitesimal distance from x. 4 Klein et al. [10] explore several implications of complex contracts in which attributes are possibly inter-dependent.
iff ∇u1(x).∇u2(x) ≥ 0, i.e., iff the gradients at x of the two utility functions form an acute or right angle.
iff ∇u1(x).∇u2(x) < 0, i.e., iff the gradients at x form an obtuse angle.
opposition iff for every x ∈ Rm they are in local strict (nonstrict, weak, strong) opposition.
Global strict and nonstrict oppositions are complementary cases.
Essentially, under global strict opposition the whole outcome space O becomes the Pareto-optimal set as at no point in O can the negotiating parties make a joint improvement, i.e., every point in O is a Pareto-efficient outcome. In other words, under global strict opposition the outcome space O can be flattened out into a single line such that for each pair of outcomes x, y ∈ O, u1(x) < u1(y) iff u2(x) > u2(y), i.e., at every point in O, the gradient of the two utility functions point to two different ends of the line.
Intuitively, global strict opposition implies that there is no way to obtain joint improvements for both agents. As a consequence, the negotiation degenerates to a distributive negotiation, i.e., the negotiating parties should try to claim as much shares from the negotiation issues as possible while the mediator should aim for the fairness of the division. On the other hand, global nonstrict opposition allows room for joint improvements and all parties might be better off trying to realise the potential gains by reaching Pareto-efficient agreements. Weak and strong oppositions indicate different levels of opposition. The weaker the opposition, the more potential gains can be realised making cooperation the better strategy to employ during negotiation. On the other hand, stronger opposition suggests that the negotiating parties tend to behave strategically leading to misrepresentation of their respective objectives and utility functions and making joint gains more difficult to realise.
We have been temporarily making the assumption that the outcome space O is the subset of Rm . In many real-world negotiations, this assumption would be too restrictive. We will continue our exposition by lifting this restriction and allowing discrete attributes. However, as most negotiations involve only discrete issues with a bounded number of options, we will assume that each attribute takes values either from a finite set or from the set of real numbers R. In the rest of the paper, we will refer to attributes whose values are from finite sets as simple attributes and attributes whose values are from R as continuous attributes. The notions of local oppositions, i.e., strict, nonstrict, weak and strong, are not applicable to outcome spaces that contain simple attributes and nor are the notions of global weak and strong oppositions. However, the notions of global strict and nonstrict oppositions can be generalised for outcome spaces that contain simple attributes.
DEFINITION 3. Given an outcome space O, the parties are in global strict opposition iff ∀x, y ∈ O, u1(x) < u1(y) iff u2(x) > u2(y).
The parties are in global nonstrict opposition if they are not in global strict opposition.
In order to extract the optimal values for a subset of attributes, in the first step of this optimisation process the mediator requests the negotiators to submit their respective utility functions over the set of simple attributes. Let Simp ⊆ Att denote the set of all simple attributes from Att. Note that, due to Assumption 1, agent i"s The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 511 utility function can be characterised as follows: ui([vSimp, vSimp]) = wi
where Simp = Att \ Simp, and ui,1 and ui,2 are the utility components of ui over the sets of attributes Simp and Simp, respectively, and 0 < wi 1, wi
As attributes are independent of each other regarding the agents" utility functions, the optimisation problem over the attributes from Simp can be carried out by fixing ui([vSimp]) to a constant C, and then search for the optimal values within the set of attributes Simp. Now, how does the mediator determine the optimal values for the attributes in Simp? Several well-known optimisation strategies could be applicable here: • The utilitarian solution: The sum of the agents" utilities are maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v∈V alSimp X i∈N ui(v) • The Nash solution: The product of the agents" utilities are maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v∈V alSimp Y i∈N ui(v) • The egalitarian solution (aka. the maximin solution): The utility of the agent with minimum utility is maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v∈V alSimp min i∈N ui(v) The question now is of course whether a negotiator has the incentive to misrepresent his utility function. First of all, recall that the agents" utility functions are bounded, i.e., ∀o ∈ O.0 ≤ ui(o) ≤ 1.
Thus, the agents have no incentive to overstate their utility regarding an outcome o: If o is the most preferred outcome to an agent i then he already assigns the maximal utility to o. On the other hand, if o is not the most preferred outcome to i then by overstating the utility he assigns to o, the agent i runs the risk of having to settle on an agreement which would give him less payoffs than he is supposed to receive. However, agents do have an incentive to understate their utility if the final settlement will be based on the above solutions alone. Essentially, the mechanism to avoid an agent to understate his utility regarding particular outcomes is to guarantee a certain measure of fairness for the final settlement. That is, the agents lose the incentive to be dishonest to obtain gains from taking advantage of the known solutions to determine the settlement outcome for they would be offset by the fairness maintenance mechanism. Firsts, we state an easy lemma.
LEMMA 1. When Simp contains one single attributes, the agents have the incentive to understate their utility functions regarding outcomes that are not attractive to them.
By way of illustration, consider the set Simp containing only one attribute that could take values from the finite set {A, B, C, D}.
Assume that negotiator 1 assigns utilities of 0.4, 0.7, 0.9, and 1 to A, B, C, and D, respectively. Assume also that negotiator 2 assigns utilities of 1, 0.9, 0.7, and 0.4 to A, B, C, and D, respectively. If agent 1 misrepresents his utility function to the mediator by reporting utility 0 for all values A, B and C and utility 1 for value D then the agent 2 who plays honestly in his report to the mediator will obtain the worst outcome D given any of the above solutions. Note that agent 1 doesn"t need to know agent 2"s utility function, nor does he need to know the strategy employed by agent
the above three solutions, then the above misrepresentation is the dominant strategy for this game.
However, when the set Simp contains more than one attribute and none of the attributes strongly dominate the other attributes then the above problem disminishes by itself thanks to the integrative solution. We of course have to define clearly what it means for an attribute to strongly dominate other attributes. Intuitively, if most of an agent"s utility concentrates on one of the attributes then this attribute strongly dominates other attributes. We again appeal to the Assumption 1 on additivity of utility functions to achieve a measure of fairness within this negotiation setting. Due to Assumption 1, we can characterise agent i"s utility component over the set of attributes Simp by the following equation: ui,1([vSimp]) = X j∈Simp wi j ∗ ui,j([vj]) (1) where P j∈Simp wj = 1.
Then, an attribute ∈ Simp strongly dominates the rest of the attributes in Simp (for agent i) iff wi > P j∈(Simp− ) wi j . Attribute is said to be strongly dominant (for agent i) wrt. the set of simple attributes Simp.
The following theorem shows that if the set of attributes Simp does not contain a strongly dominant attribute then the negotiators have no incentive to be dishonest.
THEOREM 1. Given a negotiation framework, if for every agent the set of simple attributes doesn"t contain a strongly dominant attribute, then truth-telling is an equilibrium strategy for the negotiators during the optimisation of simple attributes.
So far, we have been concentrating on the efficiency issue while leaving the fairness issue aside. A fair framework does not only support a more satisfactory distribution of utility among the agents, but also often a good measure to prevent misrepresentation of private information by the agents. Of the three solutions presented above, the utilitarian solution does not support fairness. On the other hand, Nash [16] proves that the Nash solution satisfies the above four axioms for the cooperative bargaining games and is considered a fair solution. The egalitarian solution is another mechanism to achieve fairness by essentially helping the worst off. The problem with these solutions, as discussed earlier, is that they are vulnerable to strategic behaviours when one of the attributes strongly dominates the rest of attributes.
However, there is yet another solution that aims to guarantee fairness, the minimax solution. That is, the utility of the agent with maximum utility is minimised. It"s obvious that the minimax solution produces inefficient outcomes. However, to get around this problem (given that the Pareto-optimal set can be tractably computed), we can apply this solution over the Pareto-optimal set only.
Let POSet ⊆ V alSimp be the Pareto-optimal subset of the simple outcomes, the minimax solution is defined to be the solution of the following optimisation problem. arg min v∈P OSet max i∈N ui(v) While overall efficiency often suffers under a minimax solution, i.e., the sum of all agents" utilities are often lower than under other solutions, it can be shown that the minimax solution is less vulnerable to manipulation.
THEOREM 2. Given a negotiation framework, under the minimax solution, if the negotiators are uncertain about their opponents" preferences then truth-telling is an equilibrium strategy for the negotiators during the optimisation of simple attributes.
That is, even when there is only one single simple attribute, if an agent is uncertain whether the other agent"s most preferred resolution is also his own most preferred resolution then he should opt for truth-telling as the optimal strategy.
When the attributes take values from infinite sets, we assume that they are continuous. This is similar to the common practice in operations research in which linear programming solutions/techniques are applied to integer programming problems.
We denote the number of continuous attributes by k, i.e., Att = Simp ∪ Simp and |Simp| = k. Then, the outcome space O can be represented as follows: O = ( Q j∈Simp V alj) × ( Q l∈Simp V all), where Q l∈Simp V all ⊆ Rk is the continuous component of O. Let Oc denote the set Q l∈Simp V all. We"ll refer to Oc as the feasible set and assume that Oc is closed and convex. After carrying out the optimisation over the set of simple attributes, we are able to assign the optimal values to the simple attributes from Simp. Thus, we reduce the original problem to the problem of searching for optimal (and fair) outcomes within the feasible set Oc . Recall that, by Assumption 1, we can characterise agent i"s utility function as follows: ui([v∗ Simp, vSimp]) = C + wi
where C is the constant wi
Simp]) and v∗ Simp denotes the optimal values of the simple attributes in Simp. Hence, without loss of generality (albeit with a blatant abuse of notation), we can take the agent i"s utility function as ui : Rk → [0, 1]. Accordingly we will also take the set of outcomes under consideration by the agents to be the feasible set Oc . We now state another assumption to be used in this section: ASSUMPTION 2. The negotiators" utility functions can be described by continuously differentiable and concave functions ui : Rk → [0, 1], (i = 1, 2).
It should be emphasised that we do not assume that agents explicitly know their utility functions. For the method to be described in the following to work, we only assume that the agents know the relevant information, e.g. at certain point within the feasible set Oc , the gradient direction of their own utility functions and some section of their respective indifference curves. Assume that a tentative agreement (which is a point x ∈ Rk ) is currently on the table, the process for the agents to jointly improve this agreement in order to reach a Pareto-optimal agreement can be described as follows. The mediator asks the negotiators to discretely submit their respective gradient directions at x, i.e., ∇u1(x) and ∇u2(x).
Note that the goal of the process to be described here is to search for agreements that are more efficient than the tentative agreement currently on the table. That is, we are searching for points x within the feasible set Oc such that moving to x from the current tentative agreement x brings more gains to at least one of the agents while not hurting any of the agents. Due to the assumption made above, i.e. the feasible set Oc is bounded, the conditions for an alternative x ∈ Oc to be efficient vary depending on the position of x. The following results are proved in [9]: Let B(x) = 0 denote the equation of the boundary of Oc , defining x ∈ Oc iff B(x) ≥ 0. An alternative x∗ ∈ Oc is efficient iff, either A. x∗ is in the interior of Oc and the parties are in local strict opposition at x∗ , i.e., ∇u1(x∗ ) = −γ∇u2(x∗ ) (2) where γ > 0; or B. x∗ is on the boundary of Oc , and for some α, β ≥ 0: α∇u1(x∗ ) + β∇u2(x∗ ) = ∇B(x∗ ) (3) We are now interested in answering the following questions: (i) What is the initial tentative agreement x0? (ii) How to find the more efficient agreement xh+1, given the current tentative agreement xh?
It should be emphasised that the choice of the initial tentative agreement affects the fairness of the final agreement to be reached by the presented method. For instance, if the initial tentative agreement x0 is chosen to be the most preferred alternative to one of the agents then it is also a Pareto-optimal outcome, making it impossible to find any joint improvement from x0. However, if x0 will then be chosen to be the final settlement and if x0 turns out to be the worst alternative to the other agent then this outcome is a very unfair one. Thus, it"s important that the choice of the initial tentative agreement be sensibly made.
Ehtamo et al [3] present several methods to choose the initial tentative agreement (called reference point in their paper). However, their goal is to approximate the Pareto-optimal set by systematically choosing a set of reference points. Once an (approximate) Pareto-optimal set is generated, it is left to the negotiators to decide which of the generated Pareto-optimal outcomes to be chosen as the final settlement. That is, distributive negotiation will then be required to settle the issue.
We, on the other hand, are interested in a fair initial tentative agreement which is not necessarily efficient. Improving a given tentative agreement to yield a Pareto-optimal agreement is considered in the next section. For each attribute j ∈ Simp, an agent i will be asked to discretely submit three values (from the set V alj): the most preferred value, denoted by pvi,j, the least preferred value, denoted by wvi,j, and a value that gives i an approximately average payoff, denoted by avi,j. (Note that this is possible because the set V alj is bounded.) If pv1,j and pv2,j are sufficiently close, i.e., |pv1,j − pv2,j| < Δ for some pre-defined Δ > 0, then pv1,j and pv2,j are chosen to be the two core values, denoted by cv1 and cv2. Otherwise, between the two values pv1,j and av1,j, we eliminate the one that is closer to wv2,j, the remaining value is denoted by cv1. Similarly, we obtain cv2 from the two values pv2,j and av2,j. If cv1 = cv2 then cv1 is selected as the initial value for the attribute j as part of the initial tentative agreement. Otherwise, without loss of generality, we assume that cv1 < cv2. The mediator selects randomly p values mv1, . . . , mvp from the open interval (cv1, cv2), where p ≥ 1. The mediator then asks the agents to submit their valuations over the set of values {cv1, cv2, mv1, . . . , mvp}. The value whose the two valuations of two agents are closest is selected as the initial value for the attribute j as part of the initial tentative agreement.
The above procedure guarantees that the agents do not gain by behaving strategically. By performing the above procedure on every attribute j ∈ Simp, we are able to identify the initial tentative agreement x0 such that x0 ∈ Oc . The next step is to compute a new tentative agreement from an existing tentative agreement so that the new one would be more efficient than the existing one.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 513
Our procedure is a combination of the method of jointly improving direction introduced by Ehtamo et al [4] and a method we propose in the coming section. Basically, the idea is to see how strong the opposition the parties are in. If the two parties are in (local) weak opposition at the current tentative agreement xh, i.e., their improving directions at xh are close to each other, then the compromise direction proposed by Ehtamo et al [4] is likely to point to a better agreement for both agents. However, if the two parties are in local strong opposition at the current point xh then it"s unclear whether the compromise direction would really not hurt one of the agents whilst bringing some benefit to the other.
We will first review the method proposed by Ehtamo et al [4] to compute the compromise direction for a group of negotiators at a given point x ∈ Oc . Ehtamo et al define a a function T(x) that describes the mediator"s choice for a compromise direction at x. For the case of two-party negotiations, the following bisecting function, denoted by T BS , can be defined over the interior set of Oc .
Note that the closed set Oc contains two disjoint subsets: Oc = Oc
B , where Oc
and Oc B denotes the boundary of Oc . The bisecting compromise is defined by a function T BS : Oc
,
T BS (x) = ∇u1(x) ∇u1(x) + ∇u2(x) ∇u2(x) , x ∈ Oc
Given the current tentative agreement xh (h ≥ 0), the mediator has to choose a point xh+1 along d = T(xh) so that all parties gain. Ehtamo et al then define a mechanism to generate a sequence of points and prove that when the generated sequence is bounded and when all generated points (from the sequence) belong to the interior set Oc
Paretooptimal agreement [4, pp. 59-60].5 As the above mechanism does not work at the boundary points of Oc , we will introduce a procedure that works everywhere in an alternative space Oc . Let x ∈ Oc and let θ(x) denote the angle between the gradients ∇u1(x) and ∇u2(x) at x. That is, θ(x) = arccos( ∇u1(x).∇u2(x) ∇u1(x) . ∇u2(x) ) From Definition 2, it is obvious that the two parties are in local strict opposition (at x) iff θ(x) = π, and they are in local strong opposition iff π ≥ θ(x) > π/2, and they are in local weak opposition iff π/2 ≥ θ(x) ≥ 0. Note also that the two vectors ∇u1(x) and ∇u2(x) define a hyperplane, denoted by h∇(x), in the kdimensional space Rk . Furthermore, there are two indifference curves of agents 1 and 2 going through point x, denoted by IC1(x) and IC2(x), respectively. Let hT1(x) and hT2(x) denote the tangent hyperplanes to the indifference curves IC1(x) and IC2(x), respectively, at point x. The planes hT1(x) and hT2(x) intersect h∇(x) in the lines IS1(x) and IS2(x), respectively. Note that given a line L(x) going through the point x, there are two (unit) vectors from x along L(x) pointing to two opposite directions, denoted by L+ (x) and L− (x).
We can now informally explain our solution to the problem of searching for joint gains. When it isn"t possible to obtain a compromise direction for joint improvements at a point x ∈ Oc either because the compromise vector points to the space outside of the feasible set Oc or because the two parties are in local strong opposition at x, we will consider to move along the indifference curve of one party while trying to improve the utility of the other party. As 5 Let S be the set of alternatives, x∗ is weakly Pareto optimal if there is no x ∈ S such that ui(x) > ui(x∗ ) for all agents i. the mediator does not know the indifference curves of the parties, he has to use the tangent hyperplanes to the indifference curves of the parties at point x. Note that the tangent hyperplane to a curve is a useful approximation of the curve in the immediate vicinity of the point of tangency, x.
We are now describing an iteration step to reach the next tentative agreement xh+1 from the current tentative agreement xh ∈ Oc . A vector v whose tail is xh is said to be bounded in Oc if ∃λ > 0 such that xh +λv ∈ Oc . To start, the mediator asks the negotiators for their gradients ∇u1(xh) and ∇u2(xh), respectively, at xh.
equation 3, then the process is terminated.
(xh) is bounded in Oc then the mediator chooses the compromise improving direction d = T BS (xh) and apply the method described by Ehtamo et al [4] to generate the next tentative agreement xh+1.
i (xh), i = 1, 2 and σ = +/−, the mediator chooses the vector that (i) is bounded in Oc , and (ii) is closest to the gradient of the other agent, ∇uj (xh)(j = i). Denote this vector by T G(xh). That is, we will be searching for a point on the indifference curve of agent i, ICi(xh), while trying to improve the utility of agent j. Note that when xh is an interior point of Oc then the situation is symmetric for the two agents 1 and 2, and the mediator has the choice of either finding a point on IC1(xh) to improve the utility of agent 2, or finding a point on IC2(xh) to improve the utility of agent 1. To decide on which choice to make, the mediator has to compute the distribution of gains throughout the whole process to avoid giving more gains to one agent than to the other. Now, the point xh+1 to be generated lies somewhere on the intersection of ICi(xh) and the hyperplane defined by ∇ui(xh) and T G(xh). This intersection is approximated by T G(xh). Thus, the sought after point xh+1 can be generated by first finding a point yh along the direction of T G(xh) and then move from yh to the same direction of ∇ui(xh) until we intersect with ICi(xh).
Mathematically, let ζ and ξ denote the vectors T G(xh) and ∇ui(xh), respectively, xh+1 is the solution to the following optimisation problem: max λ1,λ2∈L uj(xh + λ1ζ + λ2ξ) s.t. xh+λ1ζ+λ2ξ ∈ Oc , and ui(xh+λ1ζ+λ2ξ) = ui(xh), where L is a suitable interval of positive real numbers; e.g.,
L = {λ|λ > 0}, or L = {λ|a < λ ≤ b}, 0 ≤ a < b.
Given an initial tentative agreement x0, the method described above allows a sequence of tentative agreements x1, x2, . . . to be iteratively generated. The iteration stops whenever a weakly Pareto optimal agreement is reached.
THEOREM 3. If the sequence of agreements generated by the above method is bounded then the method converges to a point x∗ ∈ Oc that is weakly Pareto optimal.
In this paper we have established a framework for negotiation that is based on MCDM theory for representing the agents" objectives and utilities. The focus of the paper is on integrative negotiation in which agents aim to maximise joint gains, or create value.
We have introduced a mediator into the negotiation in order to allow negotiators to disclose information about their utilities, without providing this information to their opponents. Furthermore, the mediator also works toward the goal of achieving fairness of the negotiation outcome.
That is, the approach that we describe aims for both efficiency, in the sense that it produces Pareto optimal outcomes (i.e. no aspect can be improved for one of the parties without worsening the outcome for another party), and also for fairness, which chooses optimal solutions which distribute gains amongst the agents in some appropriate manner. We have developed a two step process for addressing the NP-hard problem of finding a solution for a set of integrative attributes, which is within the Pareto-optimal set for those attributes. For simple attributes (i.e. those which have a finite set of values) we use known optimisation techniques to find a Paretooptimal solution. In order to discourage agents from misrepresenting their utilities to gain an advantage, we look for solutions that are least vulnerable to manipulation. We have shown that as long as one of the simple attributes does not strongly dominate the others, then truth telling is an equilibrium strategy for the negotiators during the stage of optimising simple attributes. For non-simple attributes we propose a mechanism that provides stepwise improvements to move the proposed solution in the direction of a Paretooptimal solution.
The approach presented in this paper is similar to the ideas behind negotiation analysis [18]. Ehtamo et al [4] presents an approach to searching for joint gains in multi-party negotiations. The relation of their approach to our approach is discussed in the preceding section. Lai et al [12] provide an alternative approach to integrative negotiation. While their approach was clearly described for the case of two-issue negotiations, the generalisation to negotiations with more than two issues is not entirely clear.
Zhang et at [22] discuss the use of integrative negotiation in agent organisations. They assume that agents are honest. Their main result is an experiment showing that in some situations, agents" cooperativeness may not bring the most benefits to the organisation as a whole, while giving no explanation. Jonker et al [7] consider an approach to multi-attribute negotiation without the use of a mediator. Thus, their approach can be considered a complement of ours. Their experimental results show that agents can reach Paretooptimal outcomes using their approach.
The details of the approach have currently been shown only for bilateral negotiation, and while we believe they are generalisable to multiple negotiators, this work remains to be done. There is also future work to be done in more fully characterising the outcomes of the determination of values for the non-simple attributes. In order to provide a complete framework we are also working on the distributive phase using the mediator.
Acknowledgement The authors acknowledge financial support by ARC Dicovery Grant (2006-2009, grant DP0663147) and DEST IAP grant (2004-2006, grant CG040014). The authors would like to thank Lawrence Cavedon and the RMIT Agents research group for their helpful comments and suggestions.
[1] F. Alemi, P. Fos, and W. Lacorte. A demonstration of methods for studying negotiations between physicians and health care managers. Decision Science, 21:633-641, 1990. [2] M. Ehrgott. Multicriteria Optimization. Springer-Verlag,
Berlin, 2000. [3] H. Ehtamo, R. P. Hamalainen, P. Heiskanen, J. Teich,
M. Verkama, and S. Zionts. Generating pareto solutions in a two-party setting: Constraint proposal methods.
Management Science, 45(12):1697-1709, 1999. [4] H. Ehtamo, E. Kettunen, and R. P. Hmlinen. Searching for joint gains in multi-party negotiations. European Journal of Operational Research, 130:54-69, 2001. [5] P. Faratin. Automated Service Negotiation Between Autonomous Computational Agents. PhD thesis, University of London, 2000. [6] A. Foroughi. Minimizing negotiation process losses with computerized negotiation support systems. The Journal of Applied Business Research, 14(4):15-26, 1998. [7] C. M. Jonker, V. Robu, and J. Treur. An agent architecture for multi-attribute negotiation using incomplete preference information. J. Autonomous Agents and Multi-Agent Systems, (to appear). [8] R. L. Keeney and H. Raiffa. Decisions with Multiple Objectives: Preferences and Value Trade-Offs. John Wiley and Sons, Inc., New York, 1976. [9] G. Kersten and S. Noronha. Rational agents, contract curves, and non-efficient compromises. IEEE Systems, Man, and Cybernetics, 28(3):326-338, 1998. [10] M. Klein, P. Faratin, H. Sayama, and Y. Bar-Yam. Protocols for negotiating complex contracts. IEEE Intelligent Systems, 18(6):32-38, 2003. [11] S. Kraus, J. Wilkenfeld, and G. Zlotkin. Multiagent negotiation under time constraints. Artificial Intelligence Journal, 75(2):297-345, 1995. [12] G. Lai, C. Li, and K. Sycara. Efficient multi-attribute negotiation with incomplete information. Group Decision and Negotiation, 15:511-528, 2006. [13] D. Lax and J. Sebenius. The manager as negotiator: The negotiator"s dilemma: Creating and claiming value, 2nd ed.
In S. Goldberg, F. Sander & N. Rogers, editors, Dispute Resolution, 2nd ed., pages 49-62. Little Brown & Co., 1992. [14] M. Lomuscio and N. Jennings. A classification scheme for negotiation in electronic commerce. In Agent-Mediated Electronic Commerce: A European Agentlink Perspective.
Springer-Verlag, 2001. [15] R. Maes and A. Moukas. Agents that buy and sell.
Communications of the ACM, 42(3):81-91, 1999. [16] J. Nash. Two-person cooperative games. Econometrica, 21(1):128-140, April 1953. [17] H. Raiffa. The Art and Science of Negotiation. Harvard University Press, Cambridge, USA, 1982. [18] H. Raiffa, J. Richardson, and D. Metcalfe. Negotiation Analysis: The Science and Art of Collaborative Decision Making. Belknap Press, Cambridge, MA, 2002. [19] T. Sandholm. Agents in electronic commerce: Component technologies for automated negotiation and coalition formation. JAAMAS, 3(1):73-96, 2000. [20] J. Sebenius. Negotiation analysis: A characterization and review. Management Science, 38(1):18-38, 1992. [21] L. Weingart, E. Hyder, and M. Pietrula. Knowledge matters: The effect of tactical descriptions on negotiation behavior and outcome. Tech. Report, CMU, 1995. [22] X. Zhang, V. R. Lesser, and T. Wagner. Integrative negotiation among agents situated in organizations. IEEE Trans. on Systems, Man, and Cybernetics, Part C, 36(1):19-30, 2006.

At the core of many emerging distributed applications is the distributed constraint satisfaction problem (DCSP) - one which involves finding a consistent combination of actions (abstracted as domain values) to satisfy the constraints among multiple agents in a shared environment. Important application examples include distributed resource allocation [1] and distributed scheduling [2].
Many important algorithms, such as distributed breakout (DBO) [3], asynchronous backtracking (ABT) [4], asynchronous partial overlay (APO) [5] and asynchronous weak-commitment (AWC) [4], have been developed to address the DCSP and provide the agent solution basis for its applications. Broadly speaking, these algorithms are based on two different approaches, either extending from classical backtracking algorithms [6] or introducing mediation among the agents.
While there has been no lack of efforts in this promising research field, especially in dealing with outstanding issues such as resource restrictions (e.g., limits on time and communication) [7] and privacy requirements [8], there is unfortunately no conceptually clear treatment to prise open the model-theoretic workings of the various agent algorithms that have been developed. As a result, for instance, a deeper intellectual understanding on why one algorithm is better than the other, beyond computational issues, is not possible.
In this paper, we present a novel, unified distributed constraint satisfaction framework based on automated negotiation [9].
Negotiation is viewed as a process of several agents searching for a solution called an agreement. The search can be realized via a negotiation mechanism (or algorithm) by which the agents follow a high level protocol prescribing the rules of interactions, using a set of strategies devised to select their own preferences at each negotiation step.
Anchoring the DCSP search on automated negotiation, we show in this paper that several well-known DCSP algorithms [3] are actually mechanisms that share the same Belief-DesireIntention (BDI) interaction protocol to reach agreements, but use different action or value selection strategies. The proposed framework provides not only a clearer understanding of existing DCSP algorithms from a unified BDI agent perspective, but also opens up the opportunities to extend and develop new strategies for DCSP. To this end, a new strategy called Unsolicited Mutual Advice (UMA) is proposed. Our performance evaluation shows that UMA can outperform ABT and AWC in terms of the average number of computational cycles for both the sparse and critical coloring problems [6].
The rest of this paper is organized as follows. In Section 2, we provide a formal overview of DCSP. Section 3 presents a BDI negotiation model by which a DCSP agent reasons. Section 4 presents the existing algorithms ABT, AWC and DBO as different strategies formalized on a common protocol. A new strategy called Unsolicited Mutual Advice is proposed in Section 5; our empirical results and discussion attempt to highlight the merits of the new strategy over existing ones. Section 6 concludes the paper and points to some future work.
The DCSP [4] considers the following environment. • There are n agents with k variables x0, x1, · · · , xk−1, n ≤ k, which have values in domains D1, D2, · · · , Dk, respectively. We define a partial function B over the productrange {0, 1, . . . , (n−1)}×{0, 1, . . . , (k −1)} such that, that variable xj belongs to agent i is denoted by B(i, j)!. The exclamation mark ‘!" means ‘is defined". • There are m constraints c0, c1, · · · cm−1 to be conjunctively satisfied. In a similar fashion as defined for B(i, j), we use E(l, j)!, (0 ≤ l < m, 0 ≤ j < k), to denote that xj is relevant to the constraint cl.
The DCSP may be formally stated as follows.
Problem Statement: ∀i, j (0 ≤ i < n)(0 ≤ j < k) where B(i, j)!, find the assignment xj = dj ∈ Dj such that ∀l (0 ≤ l < m) where E(l, j)!, cl is satisfied.
A constraint may consist of different variables belonging to different agents. An agent cannot change or modify the assignment values of other agents" variables. Therefore, in cooperatively searching for a DCSP solution, the agents would need to communicate with one another, and adjust and re-adjust their own variable assignments in the process.
In general, all DCSP agents must cooperatively interact, and essentially perform the assignment and reassignment of domain values to variables to resolve all constraint violations. If the agents succeed in their resolution, a solution is found.
In order to engage in cooperative behavior, a DCSP agent needs five fundamental parameters, namely, (i) a variable [4] or a variable set [10], (ii) domains, (iii) priority, (iv) a neighbor list and (v) a constraint list.
Each variable assumes a range of values called a domain. A domain value, which usually abstracts an action, is a possible option that an agent may take. Each agent has an assigned priority.
These priority values help decide the order in which they revise or modify their variable assignments. An agent"s priority may be fixed (static) or changing (dynamic) when searching for a solution. If an agent has more than one variable, each variable can be assigned a different priority, to help determine which variable assignment the agent should modify first.
An agent which shares the same constraint with another agent is called the latter"s neighbor. Each agent needs to refer to its list of neighbors during the search process. This list may also be kept unchanged or updated accordingly in runtime. Similarly, each agent maintains a constraint list. The agent needs to ensure that there is no violation of the constraints in this list. Constraints can be added or removed from an agent"s constraint list in runtime.
As with an agent, a constraint can also be associated with a priority value. Constraints with a high priority are said to be more important than constraints with a lower priority. To distinguish it from the priority of an agent, the priority of a constraint is called its weight.
The BDI model originates with the work of M. Bratman [11].
According to [12, Ch.1], the BDI architecture is based on a philosophical model of human practical reasoning, and draws out the process of reasoning by which an agent decides which actions to perform at consecutive moments when pursuing certain goals.
Grounding the scope to the DCSP framework, the common goal of all agents is finding a combination of domain values to satisfy a set of predefined constraints. In automated negotiation [9], such a solution is called an agreement among the agents. Within this scope, we found that we were able to unearth the generic behavior of a DCSP agent and formulate it in a negotiation protocol, prescribed using the powerful concepts of BDI. Thus, our proposed negotiation model can be said to combine the BDI concepts with automated negotiation in a multiagent framework, allowing us to conceptually separate DCSP mechanisms into a common BDI interaction protocol and the adopted strategies.
Figure 1 shows the basic reasoning steps in an arbitrary round of negotiation that constitute the new protocol. The solid line indicates the common component or transition which always exists regardless of the strategy used. The dotted line indicates the Percept Belief Desire Intention Mediation Execution P B D I I I Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 1: The BDI interaction protocol component or transition which may or may not appear depending on the adopted strategy.
Two types of messages are exchanged through this protocol, namely, the info message and the negotiation message.
An info message perceived is a message sent by another agent.
The message will contain the current selected values and priorities of the variables of that sending agent. The main purpose of this message is to update the agent about the current environment.
Info message is sent out at the end of one negotiation round (also called a negotiation cycle), and received at the beginning of next round.
A negotiation message is a message which may be sent within a round. This message is for mediation purposes. The agent may put different contents into this type of message as long as it is agreed among the group. The format of the negotiation message and when it is to be sent out are subject to the strategy. A negotiation message can be sent out at the end of one reasoning step and received at the beginning of the next step.
Mediation is a step of the protocol that depends on whether the agent"s interaction with others is synchronous or asynchronous.
In synchronous mechanism, mediation is required in every negotiation round. In an asynchronous one, mediation is needed only in a negotiation round when the agent receives a negotiation message. A more in-depth view of this mediation step is provided later in this section.
The BDI protocol prescribes the skeletal structure for DCSP negotiation. We will show in Section 4 that several well-known DCSP mechanisms all inherit this generic model.
The details of the six main reasoning steps for the protocol (see Figure 1) are described as follows for a DCSP agent. For a conceptually clearer description, we assume that there is only one variable per agent. • Percept. In this step, the agent receives info messages from its neighbors in the environment, and using its Percept function, returns an image P. This image contains the current values assigned to the variables of all agents in its neighbor list. The image P will drive the agent"s actions in subsequent steps. The agent also updates its constraint list C using some criteria of the adopted strategy. • Belief. Using the image P and constraint list C, the agent will check if there is any violated constraint. If there is no violation, the agent will believe it is choosing a correct option and therefore will take no action. The agent will do nothing if it is in a local stable state - a snapshot of the variables assignments of the agent and all its neighbors by which they satisfy their shared constraints. When all agents are in their local stable states, the whole environment is said to be in a global stable state and an agreeThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 525 ment is found. In case the agent finds its value in conflict with some of its neighbors", i.e., the combination of values assigned to the variables leads to a constraint violation, the agent will first try to reassign its own variable using a specific strategy. If it finds a suitable option which meets some criteria of the adopted strategy, the agent will believe it should change to the new option. However it does not always happen that an agent can successfully find such an option. If no option can be found, the agent will believe it has no option, and therefore will request its neighbors to reconsider their variable assignments.
To summarize, there are three types of beliefs that a DCSP agent can form: (i) it can change its variable assignment to improve the current situation, (ii) it cannot change its variable assignment and some constraints violations cannot be resolved and (iii) it need not change its variable assignment as all the constraints are satisfied.
Once the beliefs are formed, the agent will determine its desires, which are the options that attempt to resolve the current constraint violations. • Desire. If the agent takes Belief (i), it will generate a list of its own suitable domain values as its desire set. If the agent takes Belief (ii), it cannot ascertain its desire set, but will generate a sublist of agents from its neighbor list, whom it will ask to reconsider their variable assignments. How this sublist is created depends on the strategy devised for the agent. In this situation, the agent will use a virtual desire set that it determines based on its adopted strategy. If the agent takes Belief (iii), it will have no desire to revise its domain value, and hence no intention. • Intention. The agent will select a value from its desire set as its intention. An intention is the best desired option that the agent assigns to its variable. The criteria for selecting a desire as the agent"s intention depend on the strategy used. Once the intention is formed, the agent may either proceed to the execution step, or undergo mediation.
Again, the decision to do so is determined by some criteria of the adopted strategy. • Mediation. This is an important function of the agent.
Since, if the agent executes its intention without performing intention mediation with its neighbors, the constraint violation between the agents may not be resolved. Take for example, suppose two agents have variables, x1 and x2, associated with the same domain {1, 2}, and their shared constraint is (x1 + x2 = 3). Then if both the variables are initialized with value 1, they will both concurrently switch between the values 2 and 1 in the absence of mediation between them.
There are two types of mediation: local mediation and group mediation. In the former, the agents exchange their intentions. When an agent receives another"s intention which conflicts with its own, the agent must mediate between the intentions, by either changing its own intention or informing the other agent to change its intention. In the latter, there is an agent which acts as a group mediator.
This mediator will collect the intentions from the group - a union of the agent and its neighbors - and determine which intention is to be executed. The result of this mediation is passed back to the agents in the group. Following mediation, the agent may proceed to the next reasoning step to execute its intention or begin a new negotiation round. • Execution. This is the last step of a negotiation round.
The agent will execute by updating its variable assignment if the intention obtained at this step is its own. Following execution, the agent will inform its neighbors about its new variable assignment and updated priority. To do so, the agent will send out an info message.
A strategy plays an important role in the negotiation process.
Within the protocol, it will often determine the efficiency of the Percept Belief Desire Intention Mediation Execution P B D I Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 2: BDI protocol with Asynchronous Backtracking strategy search process in terms of computational cycles and message communication costs.
The design space when devising a strategy is influenced by the following dimensions: (i) asynchronous or synchronous, (ii) dynamic or static priority, (iii) dynamic or static constraint weight, (iv) number of negotiation messages to be communicated, (v) the negotiation message format and (vi) the completeness property.
In other words, these dimensions provide technical considerations for a strategy design.
+ STRATEGIES In this section, we apply the proposed BDI negotiation model presented in Section 3 to expose the BDI protocol and the different strategies used for three well-known algorithms, ABT, AWC and DBO. All these algorithms assume that there is only one variable per agent. Under our framework, we call the strategies applied the ABT, AWC and DBO strategies, respectively.
To describe each strategy formally, the following mathematical notations are used: • n is the number of agents, m is the number of constraints; • xi denotes the variable held by agent i, (0 ≤ i < n); • Di denotes the domain of variable xi; Fi denotes the neighbor list of agent i; Ci denotes its constraint list; • pi denotes the priority of agent i; and Pi = {(xj = vj, pj = k) | agent j ∈ Fi, vj ∈ Dj is the current value assigned to xj and the priority value k is a positive integer } is the perception of agent i; • wl denotes the weight of constraint l, (0 ≤ l < m); • Si(v) is the total weight of the violated constraints in Ci when its variable has the value v ∈ Di.
Figure 2 presents the BDI negotiation model incorporating the Asynchronous Backtracking (ABT) strategy. As mentioned in Section 3, for an asynchronous mechanism that ABT is, the mediation step is needed only in a negotiation round when an agent receives a negotiation message.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m); pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven ABT strategy is described as follows.
Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of
constraints which only consists of agents in Fi that have equal or higher priority than this agent.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi ∈ {0, 1, 2}, decided as follows: • bi = 0 when agent i can find an optimal option, i.e., if (Si(vi) = 0 or vi is in bad values list) and (∃a ∈ Di)(Si(a) = 0) and a is not in a list of domain values called bad values list. Initially this list is empty and it will be cleared when a neighbor of higher priority changes its variable assignment. • bi = 1 when it cannot find an optimal option, i.e., if (∀a ∈ Di)(Si(a) = 0) or a is in bad values list. • bi = 2 when its current variable assignment is an optimal option, i.e., if Si(vi) = 0 and vi is not in bad value list.
Step 3 - Desire: The desire function GD (bi) will return a desire set denoted by DS, decided as follows: • If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and a is not in the bad value list }. • If bi = 1, then DS = ∅, the agent also finds agent k which is determined by {k | pk = min(pj) with agent j ∈ Fi and pk > pi }. • If bi = 2, then DS = ∅.
Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: • If DS = ∅, then select an arbitrary value (say, vi) from DS as the intention. • If DS = ∅, then assign nil as the intention (to denote its lack thereof).
Step 5 - Execution: • If agent i has a domain value as its intention, the agent will update its variable assignment with this value. • If bi = 1, agent i will send a negotiation message to agent k, then remove k from Fi and begin its next negotiation round. The negotiation message will contain the list of variable assignments of those agents in its neighbor list Fi that have a higher priority than agent i in the current image Pi.
Mediation: When agent i receives a negotiation message, several sub-steps are carried out, as follows: • If the list of agents associated with the negotiation message contains agents which are not in Fi, it will add these agents to Fi, and request these agents to add itself to their neighbor lists. The request is considered as a type of negotiation message. • Agent i will first check if the sender agent is updated with its current value vi. The agent will add vi to its bad values list if it is so, or otherwise send its current value to the sender agent.
Following this step, agent i proceeds to the next negotiation round.
Figure 3 presents the BDI negotiation model incorporating the Asynchronous Weak Commitment (AWC) strategy. The model is similar to that of incorporating the ABT strategy (see Figure 2).
This is not surprising; AWC and ABT are found to be strategically similar, differing only in the details of some reasoning steps.
The distinguishing point of AWC is that when the agent cannot find a suitable variable assignment, it will change its priority to the highest among its group members ({i} ∪ Fi).
For agent i, beginning initially with (wl = 1, (0 ≤ l < m); pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven AWC strategy is described as follows.
Step 1 - Percept: This step is identical to the Percept step of ABT.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi ∈ {0, 1, 2}, decided as follows: Percept Belief Desire Intention Mediation Execution P B D I Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 3: BDI protocol with Asynchronous WeakCommitment strategy • bi = 0 when the agent can find an optimal option i.e., if (Si(vi) = 0 or the assignment xi = vi and the current variables assignments of the neighbors in Fi who have higher priority form a nogood [4]) stored in a list called nogood list and ∃a ∈ Di, Si(a) = 0 (initially the list is empty). • bi = 1 when the agent cannot find any optimal option i.e., if ∀a ∈ Di, Si(a) = 0. • bi = 2 when the current assignment is an optimal option i.e., if Si(vi) = 0 and the current state is not a nogood in nogood list.
Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: • If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and the number of constraint violations with lower priority agents is minimized }. • If bi = 1, then DS = {a | a ∈ Di and the number of violations of all relevant constraints is minimized }. • If bi = 2, then DS = ∅.
Following, if bi = 1, agent i will find a list Ki of higher priority neighbors, defined by Ki = {k | agent k ∈ Fi and pk > pi}.
Step 4 - Intention: This step is similar to the Intention step of ABT. However, for this strategy, the negotiation message will contain the variable assignments (of the current image Pi) for all the agents in Ki. This list of assignment is considered as a nogood. If the same negotiation message had been sent out before, agent i will have nil intention. Otherwise, the agent will send the message and save the nogood in the nogood list.
Step 5 - Execution: • If agent i has a domain value as its intention, the agent will update its variable assignment with this value. • If bi = 1, it will send the negotiation message to its neighbors in Ki, and set pi = max{pj} + 1, with agent j ∈ Fi.
Mediation: This step is identical to the Mediation step of ABT, except that agent i will now add the nogood contained in the negotiation message received to its own nogood list.
Figure 4 presents the BDI negotiation model incorporating the Distributed Breakout (DBO) strategy. Essentially, by this synchronous strategy, each agent will search iteratively for improvement by reducing the total weight of the violated constraints.
The iteration will continue until no agent can improve further, at which time if some constraints remain violated, the weights of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 527 Percept Belief Desire Intention Mediation Execution P B D I I A Info Message Info Message Negotiation Message Negotiation Message Figure 4: BDI protocol with Distributed Breakout strategy these constraints will be increased by 1 to help ‘breakout" from a local minimum.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m), pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven DBO strategy is described as follows.
Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of its relevant constraints.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi ∈ {0, 1, 2}, decided as follows: • bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if ∃a ∈ Di, Si(a) < Si(vi). • bi = 1 when it cannot find any option to improve situation, i.e., if ∀a ∈ Di, a = vi, Si(a) ≥ Si(vi). • bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0.
Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: • If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi)−Si(a)) is maximized }. (max{(Si(vi)−Si(a))} will be referenced by hmax i in subsequent steps, and it defines the maximal reduction in constraint violations). • Otherwise, DS = ∅.
Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: • If DS = ∅, then select an arbitrary value (say, vi) from DS as the intention. • If DS = ∅, then assign nil as the intention.
Following, agent i will send its intention to all its neighbors.
In return, it will receive intentions from these agents before proceeding to Mediation step.
Mediation: Agent i receives all the intentions from its neighbors. If it finds that the intention received from a neighbor agent j is associated with hmax j > hmax i , the agent will automatically cancel its current intention.
Step 5 - Execution: • If agent i did not cancel its intention, it will update its variable assignment with the intended value.
Percept Belief Desire Intention Mediation Execution P B D I I A Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 5: BDI protocol with Unsolicited Mutual Advice strategy • If all intentions received and its own one are nil intention, the agent will increase the weight of each currently violated constraint by 1.
Figure 5 presents the BDI negotiation model incorporating the Unsolicited Mutual Advice(UMA) strategy.
Unlike when using the strategies of the previous section, a DCSP agent using UMA will not only send out a negotiation message when concluding its Intention step, but also when concluding its Desire step. The negotiation message that it sends out to conclude the Desire step constitutes an unsolicited advice for all its neighbors. In turn, the agent will wait to receive unsolicited advices from all its neighbors, before proceeding on to determine its intention.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m), pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven UMA strategy is described as follows.
Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of constraints relevant to agent i.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi ∈ {0, 1, 2}, decided as follows: • bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if ∃a ∈ Di, Si(a) < Si(vi) and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in a list called bad states list (initially this list is empty). • bi = 1 when it cannot find a value a such as a ∈ Di, Si(a) < Si(vi), and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in the bad states list. • bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0.
Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: • If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi) − Si(a)) is maximized } and the assignment xi = a and the current variable assignments of agent i"s neighbors do not form a state in the bad states list. In this case, DS is called a set of voluntary desires. max{(Si(vi)−Si(a))} will be referenced by hmax i in subsequent steps, and it defines
the maximal reduction in constraint violations. It is also referred to as an improvement). • If bi = 1, then DS = {a | a = vi, Si(a) is minimized } and the assignment xi = a and the current variable assignments of agent i"s neighbors do not form a state in the bad states list. In this case, DS is called a set of reluctant desires • If bi = 2, then DS = ∅.
Following, if bi = 0, agent i will send a negotiation message containing hmax i to all its neighbors. This message is called a voluntary advice. If bi = 1, agent i will send a negotiation message called change advice to the neighbors in Fi who share the violated constraints with agent i.
Agent i receives advices from all its neighbors and stores them in a list called A, before proceeding to the next step.
Step 4 - Intention: The intention function GI (DS, A) will return an intention, decided as follows: • If there is a voluntary advice from an agent j which is associated with hmax j > hmax i , assign nil as the intention. • If DS = ∅, DS is a set of voluntary desires and hmax i is the biggest improvement among those associated with the voluntary advices received, select an arbitrary value (say, vi) from DS as the intention. This intention is called a voluntary intention. • If DS = ∅, DS is a set of reluctant desires and agent i receives some change advices, select an arbitrary value (say, vi) from DS as the intention. This intention is called reluctant intention. • If DS = ∅, then assign nil as the intention.
Following, if the improvement hmax i is the biggest improvement and equal to some improvements associated with the received voluntary advices, agent i will send its computed intention to all its neighbors. If agent i has a reluctant intention, it will also send this intention to all its neighbors. In both cases, agent i will attach the number of received change advices in the current negotiation round with its intention. In return, agent i will receive the intentions from its neighbors before proceeding to Mediation step.
Mediation: If agent i does not send out its intention before this step, i.e., the agent has either a nil intention or a voluntary intention with biggest improvement, it will proceed to next step.
Otherwise, agent i will select the best intention among all the intentions received, including its own (if any). The criteria to select the best intention are listed, applied in descending order of importance as follows. • A voluntary intention is preferred over a reluctant intention. • A voluntary intention (if any) with biggest improvement is selected. • If there is no voluntary intention, the reluctant intention with the lowest number of constraint violations is selected. • The intention from an agent who has received a higher number of change advices in the current negotiation round is selected. • Intention from an agent with highest priority is selected.
If the selected intention is not agent i"s intention, it will cancel its intention.
Step 5 - Execution: If agent i does not cancel its intention, it will update its variable assignment with the intended value.
Termination Condition: Since each agent does not have full information about the global state, it may not know when it has reached a solution, i.e., when all the agents are in a global stable state. Hence an observer is needed that will keep track of the negotiation messages communicated in the environment.
Following a certain period of time when there is no more message communication (and this happens when all the agents have no more intention to update their variable assignments), the observer will inform the agents in the environment that a solution has been found. 1 2 3 4 5
8 9 10 Figure 6: Example problem
To illustrate how UMA works, consider a 2-color graph problem [6] as shown in Figure 6. In this example, each agent has a color variable representing a node. There are 10 color variables sharing the same domain {Black, White}.
The following records the outcome of each step in every negotiation round executed.
Round 1: Step 1 - Percept: Each agent obtains the current color assignments of those nodes (agents) adjacent to it, i.e., its neighbors".
Step 2 - Belief: Agents which have positive improvements are agent 1 (this agent believes it should change its color to White), agent 2 (this believes should change its color to White), agent 7 (this agent believes it should change its color to Black) and agent 10 (this agent believes it should change its value to Black). In this negotiation round, the improvements achieved by these agents are 1. Agents which do not have any improvements are agents 4, 5 and 8. Agents 3, 6 and 9 need not change as all their relevant constraints are satisfied.
Step 3 - Desire: Agents 1, 2, 7 and 10 have the voluntary desire (White color for agents 1, 2 and Black color for agents 7, 10). These agents will send the voluntary advices to all their neighbors. Meanwhile, agents 4, 5 and 8 have the reluctant desires (White color for agent 4 and Black color for agents 5, 8). Agent 4 will send a change advice to agent 2 as agent 2 is sharing the violated constraint with it. Similarly, agents 5 and 8 will send change advices to agents 7 and 10 respectively. Agents 3, 6 and 9 do not have any desire to update their color assignments.
Step 4 - Intention: Agents 2, 7 and 10 receive the change advices from agents 4, 5 and 8, respectively. They form their voluntary intentions. Agents 4, 5 and 8 receive the voluntary advices from agents 2, 7 and 10, hence they will not have any intention. Agents 3, 6 and 9 do not have any intention. Following, the intention from the agents will be sent to all their neighbors.
Mediation: Agent 1 finds that the intention from agent 2 is better than its intention. This is because, although both agents have voluntary intentions with improvement of 1, agent 2 has received one change advice from agent 4 while agent 1 has not received any. Hence agent 1 cancels its intention. Agent 2 will keep its intention.
Agents 7 and 10 keep their intentions since none of their neighbors has an intention.
The rest of the agents do nothing in this step as they do not have any intention.
Step 5 - Execution: Agent 2 changes its color to White. Agents
The new state after round 1 is shown in Figure 7.
Round 2: Step 1 - Percept: The agents obtain the current color assignments of their neighbors.
Step 2 - Belief: Agent 3 is the only agent who has a positive improvement which is 1. It believes it should change its The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 529 1 2 3 4 5
8 9 10 Figure 7: The graph after round 1 color to Black. Agent 2 does not have any positive improvement. The rest of the agents need not make any change as all their relevant constraints are satisfied. They will have no desire, and hence no intention.
Step 3 - Desire: Agent 3 desires to change its color to Black voluntarily, hence it sends out a voluntary advice to its neighbor, i.e., agent 2. Agent 2 does not have any value for its reluctant desire set as the only option, Black color, will bring agent 2 and its neighbors to the previous state which is known to be a bad state. Since agent 2 is sharing the constraint violation with agent 3, it sends a change advice to agent 3.
Step 4 - Intention: Agent 3 will have a voluntary intention while agent 2 will not have any intention as it receives the voluntary advice from agent 3.
Mediation: Agent 3 will keep its intention as its only neighbor, agent 2, does not have any intention.
Step 5 - Execution: Agent 3 changes its color to Black.
The new state after round 2 is shown in Figure 8.
Round 3: In this round, every agent finds that it has no desire and hence no intention to revise its variable assignment.
Following, with no more negotiation message communication in the environment, the observer will inform all the agents that a solution has been found. 2 3 4 5
8 91 10 Figure 8: The solution obtained
To facilitate credible comparisons with existing strategies, we measured the execution time in terms of computational cycles as defined in [4], and built a simulator that could reproduce the published results for ABT and AWC. The definition of a computational cycle is as follows. • In one cycle, each agent receives all the incoming messages, performs local computation and sends out a reply. • A message which is sent at time t will be received at time t + 1. The network delay is neglected. • Each agent has it own clock. The initial clock"s value is
outgoing message and use the time-stamp in the incoming message to update their own clock"s value.
Four benchmark problems [6] were considered, namely, n-queens and node coloring for sparse, dense and critical graphs. For each problem, a finite number of test cases were generated for various problem sizes n. The maximum execution time was set to 0 200 400 600 800 1000
Number of queens Cycles Asynchronous Backtracking Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 9: Relationship between execution time and problem size
for other problems. The simulator program was terminated after this period and the algorithm was considered to fail a test case if it did not find a solution by then. In such a case, the execution time for the test was counted as 1000 cycles.
The n-queens problem is a traditional problem of constraint satisfaction. 10 test cases were generated for each problem size n ∈ {10, 50 and 100}.
Figure 9 shows the execution time for different problem sizes when ABT, AWC and UMA were run.
The graph coloring problem can be characterized by three parameters: (i) the number of colors k, the number of nodes/agents n and the number of links m. Based on the ratio m/n, the problem can be classified into three types [3]: (i) sparse (with m/n = 2), (ii) critical (with m/n = 2.7 or 4.7) and (iii) dense (with m/n = (n − 1)/4). For this problem, we did not include ABT in our empirical results as its failure rate was found to be very high. This poor performance of ABT was expected since the graph coloring problem is more difficult than the n-queens problem, on which ABT already did not perform well (see Figure 9).
The sparse and dense (coloring) problem types are relatively easy while the critical type is difficult to solve. In the experiments, we fix k = 3. 10 test cases were created using the method described in [13] for each value of n ∈ {60, 90, 120}, for each problem type.
The simulation results for each type of problem are shown in Figures 10 - 12. 0 40 80 120 160 200
Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 10: Comparison between AWC and UMA (sparse graph coloring)
0 1000 2000 3000 4000 5000 6000
Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 11: Comparison between AWC and UMA (critical graph coloring) 0 10 20 30 40 50
Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 12: Comparison between AWC and UMA (dense graph coloring) Figure 10 shows that the average performance of UMA is slightly better than AWC for the sparse problem. UMA outperforms AWC in solving the critical problem as shown in Figure 11. It was observed that the latter strategy failed in some test cases.
However, as seen in Figure 12, both the strategies are very efficient when solving the dense problem, with AWC showing slightly better performance.
The performance of UMA, in the worst (time complexity) case, is similar to that of all evaluated strategies. The worst case occurs when all the possible global states of the search are reached.
Since only a few agents have the right to change their variable assignments in a negotiation round, the number of redundant computational cycles and info messages is reduced. As we observe from the backtracking in ABT and AWC, the difference in the ordering of incoming messages can result in a different number of computational cycles to be executed by the agents.
The computational performance of UMA is arguably better than DBO for the following reasons: • UMA can guarantee that there will be a variable reassignment following every negotiation round whereas DBO cannot. • UMA introduces one more communication round trip (that of sending a message and awaiting a reply) than DBO, which occurs due to the need to communicate unsolicited advices. Although this increases the communication cost per negotiation round, we observed from our simulations that the overall communication cost incurred by UMA is lower due to the significantly lower number of negotiation rounds. • Using UMA, in the worst case, an agent will only take 2 or 3 communication round trips per negotiation round, following which the agent or its neighbor will do a variable assignment update. Using DBO, this number of round trips is uncertain as each agent might have to increase the weights of the violated constraints until an agent has a positive improvement; this could result in a infinite loop [3].
Applying automated negotiation to DCSP, this paper has proposed a protocol that prescribes the generic reasoning of a DCSP agent in a BDI architecture. Our work shows that several wellknown DCSP algorithms, namely ABT, AWC and DBO, can be described as mechanisms sharing the same proposed protocol, and only differ in the strategies employed for the reasoning steps per negotiation round as governed by the protocol. Importantly, this means that it might furnish a unified framework for DCSP that not only provides a clearer BDI agent-theoretic view of existing DCSP approaches, but also opens up the opportunities to enhance or develop new strategies. Towards the latter, we have proposed and formulated a new strategy - the UMA strategy. Empirical results and our discussion suggest that UMA is superior to ABT,
AWC and DBO in some specific aspects.
It was observed from our simulations that UMA possesses the completeness property. Future work will attempt to formally establish this property, as well as formalize other existing DSCP algorithms as BDI negotiation mechanisms, including the recent endeavor that employs a group mediator [5]. The idea of DCSP agents using different strategies in the same environment will also be investigated.

The role of computational models of trust within multi-agent systems in particular, and open distributed systems in general, has recently generated a great deal of research interest. In such systems, agents must typically choose between interaction partners, and in this context trust can be viewed to provide a means for agents to represent and estimate the reliability with which these interaction partners will fulfill their commitments. To date, however, much of the work within this area has used domain specific or ad-hoc trust metrics, and has focused on providing heuristics to evaluate and update these metrics using direct experience and reputation reports from other agents (see [8] for a review).
Recent work has attempted to place the notion of computational trust within the framework of probability theory [6, 11]. This approach allows many of the desiderata of computational trust models to be addressed through principled means. In particular: (i) it allows agents to update their estimates of the trustworthiness of a supplier as they acquire direct experience, (ii) it provides a natural framework for agents to express their uncertainty this trustworthiness, and, (iii) it allows agents to exchange, combine and filter reputation reports received from other agents.
Whilst this approach is attractive, it is somewhat limited in that it has so far only considered single dimensional outcomes (i.e. whether the contract has succeeded or failed in its entirety). However, in many real world settings the success or failure of an interaction may be decomposed into several dimensions [7]. This presents the challenge of combining these multiple dimensions into a single metric over which a decision can be made. Furthermore, these dimensions will typically also exhibit correlations. For example, a contract within a supply chain may specify criteria for timeliness, quality and quantity. A supplier who is suffering delays may attempt a trade-off between these dimensions by supplying the full amount late, or supplying as much as possible (but less than the quantity specified within the contract) on time. Thus, correlations will naturally arise between these dimensions, and hence, between the probabilities that describe the successful fulfillment of each contract dimension. To date, however, no such principled framework exists to describe these multi-dimensional contracts, nor the correlations between these dimensions (although some ad-hoc models do exist - see section 2 for more details).
To rectify this shortcoming, in this paper we develop a probabilistic model of computational trust that explicitly deals with correlated multi-dimensional contracts. The starting point for our work is to consider how an agent can estimate the utility that it will derive from interacting with a supplier. Here we use standard approaches from the literature of data fusion (since this is a well developed field where the notion of multi-dimensional correlated estimates is well established1 ) to show that this naturally leads to a trust model where the agent must estimate probabilities and correlations over 1 In this context, the multiple dimensions typically represent the physical coordinates of a target being tracked, and correlations arise through the operation and orientation of sensors. 1070 978-81-904262-7-5 (RPS) c 2007 IFAAMAS multiple dimensions. Building upon this, we then devise a novel trust model that addresses the three desiderata discussed above. In more detail, in this paper we extend the state of the art in four key ways:
that enables an agent to estimate the expected utility of a contract, by estimating (i) the probability that each contract dimension will be successfully fulfilled, and (ii) the correlations between these estimates.
Dirichlet distribution that allows agents to use their direct experience of contract outcomes to calculate the probabilities and correlations described above. We then benchmark this solution and show that it leads to good estimates.
Dirichlet distribution in order to exchange reputation reports with one another. The sufficient statistics represent aggregations of their direct experience, and thus, express contract outcomes in a compact format with no loss of information.
contract outcomes can lead to double counting, and rumour propagation, in decentralised reputation systems. Thus, we present a novel solution based upon the idea of private and shared information. We show that it yields estimates consistent with a centralised reputation system, whilst maintaining the anonymity of the agents, and avoiding overconfidence.
The remainder of this paper is organised as follows: in section 2 we review related work. In section 3 we present our notation for a single dimensional contract, before introducing our multi-dimensional trust model in section 4. In sections 5 and 6 we discuss communicating reputation, and present our solution to rumour propagation in decentralised reputation systems. We conclude in section 7.
The need for a multi-dimensional trust model has been recognised by a number of researchers. Sabater and Sierra present a model of reputation, in which agents form contracts based on multiple variables (such as delivery date and quality), and define impressions as subjective evaluations of the outcome of these contracts. They provide heuristic approaches to combining these impressions to form a measure they call subjective reputation.
Likewise, Griffiths decomposes overall trust into a number of different dimensions such as success, cost, timeliness and quality [4]. In his case, each dimension is scored as a real number that represents a comparative value with no strong semantic meaning. He develops an heuristic rule to update these values based on the direct experiences of the individual agent, and an heuristic function that takes the individual trust dimensions and generates a single scalar that is then used to select between suppliers. Whilst, he comments that the trust values could have some associated confidence level, heuristics for updating these levels are not presented.
Gujral et al. take a similar approach and present a trust model over multiple domain specific dimensions [5]. They define multidimensional goal requirements, and evaluate an expected payoff based on a supplier"s estimated behaviour. These estimates are, however, simple aggregations over the direct experience of several agents, and there is no measure of the uncertainty. Nevertheless, they show that agents who select suppliers based on these multiple dimensions outperform those who consider just a single one.
By contrast, a number of researchers have presented more principled computational trust models based on probability theory, albeit limited to a single dimension. Jøsang and Ismail describe the Beta Reputation System whereby the reputation of an agent is compiled from the positive and negative reports from other agents who have interacted with it [6]. The beta distribution represents a natural choice for representing these binary outcomes, and it provides a principled means of representing uncertainty. Moreover, they provide a number of extensions to this initial model including an approach to exchanging reputation reports using the sufficient statistics of the beta distribution, methods to discount the opinions of agents who themselves have low reputation ratings, and techniques to deal with reputations that may change over time.
Likewise, Teacy et al. use the beta distribution to describe an agent"s belief in the probability that another agent will successfully fulfill its commitments [11]. They present a formalism using a beta distribution that allows the agent to estimate this probability based upon its direct experience, and again they use the sufficient statistics of this distribution to communicate this estimate to other agents. They provide a number of extensions to this initial model, and, in particular, they consider that agents may not always truthfully report their trust estimates. Thus, they present a principled approach to detecting and removing inconsistent reports.
Our work builds upon these more principled approaches.
However, the starting point of our approach is to consider an agent that is attempting to estimate the expected utility of a contract. We show that estimating this expected utility requires that an agent must estimate the probability with which the supplier will fulfill its contract.
In the single-dimensional case, this naturally leads to a trust model using the beta distribution (as per Jøsang and Ismail and Teacy et al.). However, we then go on to extend this analysis to multiple dimensions, where we use the natural extension of the beta distribution, namely the Dirichlet distribution, to represent the agent"s belief over multiple dimensions.
Before presenting our multi-dimensional trust model, we first introduce the notation and formalism that we will use by describing the more familiar single dimensional case. We consider an agent who must decide whether to engage in a future contract with a supplier.
This contract will lead to some outcome, o, and we consider that o = 1 if the contract is successfully fulfilled, and o = 0 if not2 .
In order for the agent to make a rational decision, it should consider the utility that it will derive from this contract. We assume that in the case that the contract is successfully fulfilled, the agent derives a utility u(o = 1), otherwise it receives no utility3 . Now, given that the agent is uncertain of the reliability with which the supplier will fulfill the contract, it should consider the expected utility that it will derive, E[U], and this is given by: E[U] = p(o = 1)u(o = 1) (1) where p(o = 1) is the probability that the supplier will successfully fulfill the contract. However, whilst u(o = 1) is known by the agent, p(o = 1) is not. The best the agent can do is to determine a distribution over possible values of p(o = 1) given its direct experience of previous contract outcomes. Given that it has been able to do so, it can then determine an estimate of the expected utility4 of the contract, E[E[U]], and a measure of its uncertainty in this expected utility, Var(E[U]). This uncertainty is important since a risk averse agent may make a decision regarding a contract, 2 Note that we only consider binary contract outcomes, although extending this to partial outcomes is part of our future work. 3 Clearly this can be extended to the case where some utility is derived from an unsuccessful outcome. 4 Note that this is often called the expected expected utility, and this is the notation that we adopt here [2].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1071 not only on its estimate of the expected utility of the contract, but also on the probability that the expected utility will exceed some minimum amount. These two properties are given by: E[E[U]] = ˆp(o = 1)u(o = 1) (2) Var(E[U]) = Var(p(o = 1))u(o = 1)2 (3) where ˆp(o = 1) and Var(p(o = 1)) are the estimate and uncertainty of the probability that a contract will be successfully fulfilled, and are calculated from the distribution over possible values of p(o = 1) that the agent determines from its direct experience.
The utility based approach that we present here provides an attractive motivation for this model of Teacy et al. [11].
Now, in the case of binary contract outcomes, the beta distribution is the natural choice to represent the distribution over possible values of p(o = 1) since within Bayesian statistics this well known to be the conjugate prior for binomial observations [3]. By adopting the beta distribution, we can calculate ˆp(o = 1) and Var(p(o = 1)) using standard results, and thus, if an agent observed N previous contracts of which n were successfully fulfilled, then: ˆp(o = 1) = n + 1 N + 2 and: Var(p(o = 1)) = (n + 1)(N − n + 1) (N + 2)2(N + 3) Note that as expected, the greater the number of contracts the agent observes, the smaller the variance term Var(p(o = 1)), and, thus, the less the uncertainty regarding the probability that a contract will be successfully fulfilled, ˆp(o = 1).
We now extend the description above, to consider contracts between suppliers and agents that are represented by multiple dimensions, and hence the success or failure of a contract can be decomposed into the success or failure of each separate dimension. Consider again the example of the supply chain that specifies the timeliness, quantity, and quality of the goods that are to be delivered. Thus, within our trust model oa = 1 now indicates a successful outcome over dimension a of the contract and oa = 0 indicates an unsuccessful one. A contract outcome, X, is now composed of a vector of individual contract part outcomes (e.g. X = {oa = 1, ob = 0, oc = 0, . . .}).
Given a multi-dimensional contract whose outcome is described by the vector X, we again consider that in order for an agent to make a rational decision, it should consider the utility that it will derive from this contract. To this end, we can make the general statement that the expected utility of a contract is given by: E[U] = p(X)U(X)T (4) where p(X) is a joint probability distribution over all possible contract outcomes: p(X) = ⎛ ⎜ ⎜ ⎜ ⎝ p(oa = 1, ob = 0, oc = 0, . . .) p(oa = 1, ob = 1, oc = 0, . . .) p(oa = 0, ob = 1, oc = 0, . . .) ... ⎞ ⎟ ⎟ ⎟ ⎠ (5) and U(X) is the utility derived from these possible outcomes: U(X) = ⎛ ⎜ ⎜ ⎜ ⎝ u(oa = 1, ob = 0, oc = 0, . . .) u(oa = 1, ob = 1, oc = 0, . . .) u(oa = 0, ob = 1, oc = 0, . . .) ... ⎞ ⎟ ⎟ ⎟ ⎠ (6) As before, whilst U(X) is known to the agent, the probability distribution p(X) is not. Rather, given the agent"s direct experience of the supplier, the agent can determine a distribution over possible values for p(X). In the single dimensional case, a beta distribution was the natural choice over possible values of p(o = 1). In the multi-dimensional case, where p(X) itself is a vector of probabilities, the corresponding natural choice is the Dirichlet distribution, since this is a conjugate prior for multinomial proportions [3].
Given this distribution, the agent is then able to calculate an estimate of the expected utility of a contract. As before, this estimate is itself represented by an expected value given by: E[E[U]] = ˆp(X)U(X)T (7) and a variance, describing the uncertainty in this expected utility: Var(E[U]) = U(X)Cov(p(X))U(X)T (8) where: Cov(p(X)) E[(p(X) − ˆp(X))(p(X) − ˆp(X))T ] (9) Thus, whilst the single dimensional case naturally leads to a trust model in which the agents attempt to derive an estimate of probability that a contract will be successfully fulfilled, ˆp(o = 1), along with a scalar variance that describes the uncertainty in this probability, Var(p(o = 1)), in this case, the agents must derive an estimate of a vector of probabilities, ˆp(X), along with a covariance matrix, Cov(p(X)), that represents the uncertainty in p(X) given the observed contractual outcomes. At this point, it is interesting to note that the estimate in the single dimensional case, ˆp(o = 1), has a clear semantic meaning in relation to trust; it is the agent"s belief in the probability of a supplier successfully fulfilling a contract. However, in the multi-dimensional case the agent must determine ˆp(X), and since this describes the probability of all possible contract outcomes, including those that are completely un-fulfilled, this direct semantic interpretation is not present. In the next section, we describe the exemplar utility function that we shall use in the remainder of this paper.
The approach described so far is completely general, in that it applies to any utility function of the form described above, and also applies to the estimation of any joint probability distribution. In the remainder of this paper, for illustrative purposes, we shall limit the discussion to the simplest possible utility function that exhibits a dependence upon the correlations between the contract dimensions. That is, we consider the case that expected utility is dependent only on the marginal probabilities of each contract dimension being successfully fulfilled, rather than the full joint probabilities: U(X) = ⎛ ⎜ ⎜ ⎜ ⎝ u(oa = 1) u(ob = 1) u(oc = 1) ... ⎞ ⎟ ⎟ ⎟ ⎠ (10) Thus, ˆp(X) is a vector estimate of the probability of each contract dimension being successfully fulfilled, and maintains the clear semantic interpretation seen in the single dimensional case: ˆp(X) = ⎛ ⎜ ⎜ ⎜ ⎝ ˆp(oa = 1) ˆp(ob = 1) ˆp(oc = 1) ... ⎞ ⎟ ⎟ ⎟ ⎠ (11) The correlations between the contract dimensions affect the uncertainty in the expected utility. To see this, consider the covariance
matrix that describes this uncertainty, Cov(p(X)), is now given by: Cov(p(X)) = ⎛ ⎜ ⎜ ⎜ ⎝ Va Cab Cac . . .
Cab Vb Cbc . . .
Cac Cbc Vc . . . ... ... ... ⎞ ⎟ ⎟ ⎟ ⎠ (12) In this matrix, the diagonal terms, Va, Vb and Vc, represent the uncertainties in p(oa = 1), p(ob = 1) and p(oc = 1) within p(X). The off-diagonal terms, Cab, Cac and Cbc, represent the correlations between these probabilities. In the next section, we use the Dirichlet distribution to calculate both ˆp(X) and Cov(p(X)) from an agent"s direct experience of previous contract outcomes.
We first illustrate why this is necessary by considering an alternative approach to modelling multi-dimensional contracts whereby an agent na¨ıvely assumes that the dimensions are independent, and thus, it models each individually by separate beta distributions (as in the single dimensional case we presented in section 3). This is actually equivalent to setting the off-diagonal terms within the covariance matrix, Cov(p(X)), to zero. However, doing so can lead an agent to assume that its estimate of the expected utility of the contract is more accurate than it actually is. To illustrate this, consider a specific scenario with the following values: u(oa = 1) = u(ob = 1) = 1 and Va = Vb = 0.2. In this case,
Var(E[U]) = 0.4(1 + Cab), and thus, if the correlation Cab is ignored then the variance in the expected utility is 0.4. However, if the contract outcomes are completely correlated then Cab = 1 and Var(E[U]) is actually 0.8. Thus, in order to have an accurate estimate of the variance of the expected contract utility, and to make a rational decision, it is essential that the agent is able to represent and calculate these correlation terms. In the next section, we describe how an agent may do so using the Dirichlet distribution.
In this section, we describe how the agent may use its direct experience of previous contracts, and the standard results of the Dirichlet distribution, to determine an estimate of the probability that each contract dimension will be successful fulfilled, ˆp(X), and a measure of the uncertainties in these probabilities that expresses the correlations between the contract dimensions, Cov(p(X)).
We first consider the calculation of ˆp(X) and the diagonal terms of the covariance matrix Cov(p(X)). As described above, the derivation of these results is identical to the case of the single dimensional beta distribution, where out of N contract outcomes, n are successfully fulfilled. In the multi-dimensional case, however, we have a vector {na, nb, nc, . . .} that represents the number of outcomes for which each of the individual contract dimensions were successfully fulfilled. Thus, in terms of the standard Dirichlet parameters where αa = na + 1 and α0 = N + 2, the agent can estimate the probability of this contract dimension being successfully fulfilled: ˆp(oa = 1) = αa α0 = na + 1 N + 2 and can also calculate the variance in any contract dimension: Va = αa(α0 − αa) α2 0(1 + α0) = (na + 1)(N − na + 1) (N + 2)2(N + 3) However, calculating the off-diagonal terms within Cov(p(X)) is more complex since it is necessary to consider the correlations between the contract dimensions. Thus, for each pair of dimensions (i.e. a and b), we must consider all possible combinations of contract outcomes, and thus we define nab ij as the number of contract outcomes for which both oa = i and ob = j. For example, nab 10 represents the number of contracts for which oa = 1 and ob = 0.
Now, using the standard Dirichlet notation, we can define αab ij nab ij + 1 for all i and j taking values 0 and 1, and then, to calculate the cross-correlations between contract pairs a and b, we note that the Dirichlet distribution over pair-wise joint probabilities is: Prob(pab) = Kab i∈{0,1} j∈{0,1} p(oa = i, ob = j)αab ij −1 where: i∈{0,1} j∈{0,1} p(oa = i, ob = j) = 1 and Kab is a normalising constant [3]. From this we can derive pair-wise probability estimates and variances: E[p(oa = i, ob = j)] = αab ij α0 (13) V [p(oa = i, ob = j)] = αab ij (α0 − αab ij ) α2 0(1 + α0) (14) where: α0 = i∈{0,1} j∈{0,1} αab ij (15) and in fact, α0 = N + 2, where N is the total number of contracts observed. Likewise, we can express the covariance in these pairwise probabilities in similar terms: C[p(oa = i, ob = j), p(oa = m, ob = n)] = −αab ij αab mn α2 0(1 + α0) Finally, we can use the expression: p(oa = 1) = j∈{0,1} p(oa = 1, ob = j) to determine the covariance Cab. To do so, we first simplify the notation by defining V ab ij V [p(oa = i, ob = j)] and Cab ijmn C[p(oa = i, ob = j), p(oa = m, ob = n)]. The covariance for the probability of positive contract outcomes is then the covariance between j∈{0,1} p(oa = 1, ob = j) and i∈{0,1} p(oa = i, ob = 1), and thus: Cab = Cab
Thus, given a set of contract outcomes that represent the agent"s previous interactions with a supplier, we may use the Dirichlet distribution to calculate the mean and variance of the probability of any contract dimension being successfully fulfilled (i.e. ˆp(oa = 1) and Va). In addition, by a somewhat more complex procedure we can also calculate the correlations between these probabilities (i.e.
Cab). This allows us to calculate an estimate of the probability that any contract dimension will be successfully fulfilled, ˆp(X), and also represent the uncertainty and correlations in these probabilities by the covariance matrix, Cov(p(X)). In turn, these results may be used to calculate the estimate and uncertainty in the expected utility of the contract. In the next section we present empirical results that show that in practise this formalism yields significant improvements in these estimates compared to the na¨ıve approximation using multiple independent beta distributions.
In order to evaluate the effectiveness of our formalism, and show the importance of the off-diagonal terms in Cov(p(X)), we compare two approaches: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1073 −1 −0.5 0 0.5 1
Correlation (ρ) Var(E[U]) Dirichlet Distribution Indepedent Beta Distributions −1 −0.5 0 0.5 1
1
2
x 10 4 Correlation (ρ) Information(I) Dirichlet Distribution Indepedent Beta Distributions Figure 1: Plots showing (i) the variance of the expected contract utility and (ii) the information content of the estimates computed using the Dirichlet distribution and multiple independent beta distributions. Results are averaged over 106 runs, and the error bars show the standard error in the mean. • Dirichlet Distribution: We use the full Dirichlet distribution, as described above, to calculate ˆp(X) and Cov(p(X)) including all its off-diagonal terms that represent the correlations between the contract dimensions. • Independent Beta Distributions: We use independent beta distributions to represent each contract dimension, in order to calculate ˆp(X), and then, as described earlier, we approximate Cov(p(X)) and ignore the correlations by setting all the off-diagonal terms to zero.
We consider a two-dimensional case where u(oa = 1) = 6 and u(ob = 1) = 2, since this allows us to plot ˆp(X) and Cov(p(X)) as ellipses in a two-dimensional plane, and thus explain the differences between the two approaches. Specifically, we initially allocate the agent some previous contract outcomes that represents its direct experience with a supplier. The number of contracts is drawn uniformly between 10 and 20, and the actual contract outcomes are drawn from an arbitrary joint distribution intended to induce correlations between the contract dimensions. For each set of contracts, we use the approaches described above to calculate ˆp(X) and Cov(p(X)), and hence, the variance in the expected contract utility, Var(E[U]). In addition, we calculate a scalar measure of the information content, I, of the covariance matrix Cov(p(X)), which is a standard way of measuring the uncertainty encoded within the covariance matrix [1]. More specifically, we calculate the determinant of the inverse of the covariance matrix: I = det(Cov(p(X))−1 ) (16) and note that the larger the information content, the more precise ˆp(X) will be, and thus, the better the estimate of the expected utility that the agent is able to calculate. Finally, we use the results
p(o =1) p(o=1) a b Dirichlet Distribution Indepedent Beta Distributions Figure 2: Examples of ˆp(X) and Cov(p(X)) plotted as second standard error ellipses. presented in section 4.2 to calculate the actual correlation, ρ, associated with this particular set of contract outcomes: ρ = Cab √ VaVb (17) where Cab, Va and Vb are calculated as described in section 4.2.
The results of this analysis are shown in figure 1. Here we show the values of I and Var(E[U]) calculated by the agents, plotted against the correlation of the contract outcomes, ρ, that constituted their direct experience. The results are averaged over 106  simulation runs. Note that as expected, when the dimensions of the contract outcomes are uncorrelated (i.e. ρ = 0), then both approaches give the same results. However, the value of using our formalism with the full Dirichlet distribution is shown when the correlation between the dimensions increases (either negatively or positively).
As can be seen, if we approximate the Dirichlet distribution with multiple independent beta distributions, all of the correlation information contained within the covariance matrix, Cov(p(X)), is lost, and thus, the information content of the matrix is much lower.
The loss of this correlation information leads the variance of the expected utility of the contract to be incorrect (either over or under estimated depending on the correlation)5 , with the exact amount of mis-estimation depending on the actual utility function chosen (i.e. the values of u(oa = 1) and u(ob = 1)).
In addition, in figure 2 we illustrate an example of the estimates calculated through both methods, for a single exemplar set of contract outcomes. We represent the probability estimates, ˆp(X), and the covariance matrix, Cov(p(X)), in the standard way as an ellipse [1]. That is, ˆp(X) determines the position of the center of the ellipse, Cov(p(X)) defines its size and shape. Note that whilst the ellipse resulting from the full Dirichlet formalism accurately reflects the true distribution (samples of which are plotted as points), that calculated by using multiple independent Beta distributions (and thus ignoring the correlations) results in a much larger ellipse that does not reflect the true distribution. The larger size of this ellipse is a result of the off-diagonal terms of the covariance matrix being set to zero, and corresponds to the agent miscalculating the uncertainty in the probability of each contract dimension being fulfilled. This, in turn, leads it to miscalculate the uncertainty in the expected utility of a contract (shown in figure 1 as Var(E[U]).
Having described how an individual agent can use its own direct experience of contract outcomes in order to estimate the probabil5 Note that the plots are not smooth due to the fact that given a limited number of contract outcomes, then the mean of Va and Vb do not vary smoothly with ρ.
ity that a multi-dimensional contract will be successfully fulfilled, we now go on to consider how agents within an open multi-agent system can communicate these estimates to one another. This is commonly referred to as reputation and allows agents with limited direct experience of a supplier to make rational decisions.
Both Jøsang and Ismail, and Teacy et al. present models whereby reputation is communicated between agents using the sufficient statistics of the beta distribution [6, 11]. This approach is attractive since these sufficient statistics are simple aggregations of contract outcomes (more precisely, they are simply the total number of contracts observed, N, and the number of these that were successfully fulfilled, n). Under the probabilistic framework of the beta distribution, reputation reports in this form may simply be aggregated with an agent"s own direct experience, in order to gain a more precise estimate based on a larger set of contract outcomes.
We can immediately extend this approach to the multi-dimensional case considered here, by requiring that the agents exchange the sufficient statistics of the Dirichlet distribution instead of the beta distribution. In this case, for each pair of dimensions (i.e. a and b), the agents must communicate a vector of contract outcomes, N, which are the sufficient statistics of the Dirichlet distribution, given by: N =< nab ij > ∀a, b, i ∈ {0, 1}, j ∈ {0, 1} (18) Thus, an agent is able to communicate the sufficient statistics of its own Dirichlet distribution in terms of just 2d(d − 1) numbers (where d is the number of contract dimensions). For instance, in the case of three dimensions, N, is given by: N =< nab 00, nab 01, nab 10, nab 11, nac 00, nac 01, nac 10, nac 11, nbc 00, nbc 01, nbc 10, nbc
and, hence, large sets of contract outcomes may be communicated within a relatively small message size, with no loss of information.
Again, agents receiving these sufficient statistics may simply aggregate them with their own direct experience in order to gain a more precise estimate of the trustworthiness of a supplier.
Finally, we note that whilst it is not the focus of our work here, by adopting the same principled approach as Jøsang and Ismail, and Teacy et al., many of the techniques that they have developed (such as discounting reports from unreliable agents, and filtering inconsistent reports from selfish agents) may be directly applied within this multi-dimensional model. However, we now go on to consider a new issue that arises in both the single and multi-dimensional models, namely the problems that arise when such aggregated sufficient statistics are propagated within decentralised agent networks.
WITHIN REPUTATION SYSTEMS In the previous section, we described the use of sufficient statistics to communicate reputation, and we showed that by aggregating contract outcomes together into these sufficient statistics, a large number of contract outcomes can be represented and communicated in a compact form. Whilst, this is an attractive property, it can be problematic in practise, since the individual provenance of each contract outcome is lost in the aggregation. Thus, to ensure an accurate estimate, the reputation system must ensure that each observation of a contract outcome is included within the aggregated statistics no more than once.
Within a centralised reputation system, where all agents report their direct experience to a trusted center, such double counting of contract outcomes is easy to avoid. However, in a decentralised reputation system, where agents communicate reputation to one another, and aggregate their direct experience with these reputation reports on-the-fly, avoiding double counting is much more difficult. a1 a2 a3 ¨ ¨¨ ¨¨ ¨¨B E T N1 N1 N1 + N2 Figure 3: Example of rumour propagation in a decentralised reputation system.
For example, consider the case shown in figure 3 where three agents (a1 . . . a3), each with some direct experience of a supplier, share reputation reports regarding this supplier. If agent a1 were to provide its estimate to agents a2 and a3 in the form of the sufficient statistics of its Dirichlet distribution, then these agents can aggregate these contract outcomes with their own, and thus obtain more precise estimates. If at a later stage, agent a2 were to send its aggregate vector of contract outcomes to agent a3, then agent a3 being unaware of the full history of exchanges, may attempt to combine these contract outcomes with its own aggregated vector.
However, since both vectors contain a contribution from agent a1, these will be counted twice in the final aggregated vector, and will result in a biased and overconfident estimate. This is termed rumour propagation or data incest in the data fusion literature [9].
One possible solution would be to uniquely identify the source of each contract outcome, and then propagate each vector, along with its label, through the network. Agents can thus identify identical observations that have arrived through different routes, and after removing the duplicates, can aggregate these together to form their estimates. Whilst this appears to be attractive in principle, for a number of reasons, it is not always a viable solution in practise [12].
Firstly, agents may not actually wish to have their uniquely labelled contract outcomes passed around an open system, since such information may have commercial or practical significance that could be used to their disadvantage. As such, agents may only be willing to exchange identifiable contract outcomes with a small number of other agents (perhaps those that they have some sort of reciprocal relationship with). Secondly, the fact that there is no aggregation of the contract outcomes as they pass around the network means that the message size increases over time, and the ultimate size of these messages is bounded only by the number of agents within the system (possibly an extremely large number for a global system).
Finally, it may actually be difficult to assign globally agreeable, consistent, and unique labels for each agent within an open system.
In the next section, we develop a novel solution to the problem of rumour propagation within decentralised reputation systems. Our solution is based on an approach developed within the area of target tracking and data fusion [9]. It avoids the need to uniquely identify an agent, it allows agents to restrict the number of other agents who they reveal their private estimates to, and yet it still allows information to propagate throughout the network.
Our solution to rumour propagation within decentralised reputation systems introduces the notion of private information that an agent knows it has not communicated to any other agent, and shared information that has been communicated to, or received from, another agent. Thus, the agent can decompose its contract outcome vector, N, into two vectors, a private one, Np, that has not been communicated to another agent, and a shared one, Ns, that has been shared with, or received from, another agent: N = Np + Ns (19) Now, whenever an agent communicates reputation, it communicates both its private and shared vectors separately. Both the origThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1075 inating and receiving agents then update their two vectors appropriately. To understand this, consider the case that agent aα sends its private and shared contract outcome vectors, Nα p and Nα s , to agent aβ that itself has private and shared contract outcomes Nβ p and Nβ s . Each agent updates its vectors of contract outcomes according to the following procedure: • Originating Agent: Once the originating agent has sent both its shared and private contract outcome vectors to another agent, its private information is no longer private. Thus, it must remove the contract outcomes that were in its private vector, and add them into its shared vector: Nα s ← Nα s + Nα p Nα p ← ∅. • Receiving Agent: The goal of the receiving agent is to accumulate the largest number contract outcomes (since this will result in the most precise estimate) without including shared information from both itself and the other agent (since this may result in double counting of contract outcomes). It has two choices depending on the total number of contract outcomes6 within its own shared vector, Nβ s , and within that of the originating agent, Nα s . Thus, it updates its vector according to the procedure below: - Nβ s > Nα s : If the receiving agent"s shared vector represents a greater number of contract outcomes than that of the shared vector of the originating agent, then the agent combines its shared vector with the private vector of the originating agent: Nβ s ← Nβ s + Nα p Nβ p unchanged. - Nβ s < Nα s : Alternatively if the receiving agent"s shared vector represents a smaller number contract outcomes than that of the shared vector of the originating agent, then the receiving agent discards its own shared vector and forms a new one from both the private and shared vectors of the originating agent: Nβ s ← Nα s + Nα p Nβ p unchanged.
In the case that Nβ s = Nα s then either option is appropriate. Once the receiving agent has updated its sets, it uses the contract outcomes within both to form its trust estimate. If agents receive several vectors simultaneously, this approach generalises to the receiving agent using the largest shared vector, and the private vectors of itself and all the originating agents to form its new shared vector.
This procedure has a number of attractive properties. Firstly, since contract outcomes in an agent"s shared vector are never combined with those in the shared vector of another agent, outcomes that originated from the same agent are never combined together, and thus, rumour propagation is completely avoided. However, since the receiving agent may discard its own shared vector, and adopt the shared vector of the originating agent, information is still propagated around the network. Moreover, since contract outcomes are aggregated together within the private and shared vectors, the message size is constant and does not increase as the number of interactions increases. Finally, an agent only communicates its own private contract outcomes to its immediate neighbours. If this agent 6 Note that this may be calculated from N = nab
subsequently passes it on, it does so as unidentifiable aggregated information within its shared information. Thus, an agent may limit the number of agents with which it is willing to reveal identifiable contract outcomes, and yet these contract outcomes can still propagate within the network, and thus, improve estimates of other agents. Next, we demonstrate empirically that these properties can indeed be realised in practise.
In order to evaluate the effectiveness of this procedure we simulated random networks consisting of ten agents. Each agent has some direct experience of interacting with a supplier (as described in section 4.3). At each iteration of the simulation, it interacts with its immediate neighbours and exchanges reputation reports through the sufficient statistics of their Dirichlet distributions. We compare our solution to two of the most obvious decentralised alternatives: • Private and Shared Information: The agents follow the procedure described in the previous section. That is, they maintain separate private and shared vectors of contract outcomes, and at each iteration they communicate both these vectors to their immediate neighbours. • Rumour Propagation: The agents do not differentiate between private and shared contract outcomes. At the first iteration they communicate all of the contract outcomes that constitute their direct experience. In subsequent iterations, they propagate contract outcomes that they receive from any of the neighbours, to all their other immediate neighbours. • Private Information Only: The agents only communicate the contract outcomes that constitute their direct experience.
In all cases, at each iteration, the agents use the Dirichlet distribution in order to calculate their trust estimates. We compare these three decentralised approaches to a centralised reputation system: • Centralised Reputation: All the agents pass their direct experience to a centralised reputation system that aggregates them together, and passes this estimate back to each agent.
This centralised solution makes the most effective use of information available in the network. However, most real world problems demand decentralised solutions due to scalability, modularity and communication concerns. Thus, this centralised solution is included since it represents the optimal case, and allows us to benchmark our decentralised solution.
The results of these comparisons are shown in figure 4. Here we show the sum of the information content of each agent"s covariance matrix (calculated as discussed earlier in section 4.3), for each of these four different approaches. We first note that where private information only is communicated, there is no change in information after the first iteration. Once each agent has received the direct experience of its immediate neighbours, no further increase in information can be achieved. This represents the minimum communication, and it exhibits the lowest total information of the four cases. Next, we note that in the case of rumour propagation, the information content increases continually, and rapidly exceeds the centralised reputation result. The fact that the rumour propagation case incorrectly exceeds this limit, indicates that it is continuously counting the same contract outcomes as they cycle around the network, in the belief that they are independent events. Finally, we note that using private and shared information represents a compromise between the private information only case and the centralised reputation case. Information is still allowed to propagate around the network, however rumours are eliminated.
As before, we also plot a single instance of the trust estimates from one agent (i.e. ˆp(X) and Cov(p(X))) as a set of ellipses on a
10 4 10 6 10 8 10 10 Iteration Information(I) Private & Shared Information Rumour Propagation Private Information Only Centralised Reputation Figure 4: Sum of information over all agents as a function of the communication iteration. two-dimensional plane (along with samples from the true distribution). As expected, the centralised reputation system achieves the best estimate of the true distribution, since it uses the direct experience of all agents. The private information only case shows the largest ellipse since it propagates the least information around the network. The rumour propagation case shows the smallest ellipse, but it is inconsistent with the actual distribution p(X). Thus, propagating rumours around the network and double counting contract outcomes in the belief that they are independent events, results in an overconfident estimate. However, we note that our solution, using separate vectors of private and shared information, allows us to propagate more information than the private information only case, but we completely avoid the problems of rumour propagation.
Finally, we consider the effect that this has on the agents" calculation of the expected utility of the contract. We assume the same utility function as used in section 4.3 (i.e. u(oa = 1) = 6 and u(ob = 1) = 2), and in table 1 we present the estimate of the expected utility, and its standard deviation calculated for all four cases by a single agent at iteration five (after communication has ceased to have any further effect for all methods other than rumour propagation). We note that the rumour propagation case is clearly inconsistent with the centralised reputation system, since its standard deviation is too small and does not reflect the true uncertainty in the expected utility, given the contract outcomes. However, we observe that our solution represents the closest case to the centralised reputation system, and thus succeeds in propagating information throughout the network, whilst also avoiding bias and overconfidence. The exact difference between it and the centralised reputation system depends upon the topology of the network, and the history of exchanges that take place within it.
In this paper we addressed the need for a principled probabilistic model of computational trust that deals with contracts that have multiple correlated dimensions. Our starting point was an agent estimating the expected utility of a contract, and we showed that this leads to a model of computational trust that uses the Dirichlet distribution to calculate a trust estimate from the direct experience of an agent. We then showed how agents may use the sufficient statistics of this Dirichlet distribution to represent and communicate reputation within a decentralised reputation system, and we presented a solution to rumour propagation within these systems.
Our future work in this area is to extend the exchange of reputation to the case where contracts are not homogeneous. That is, not all agents observe the same contract dimensions. This is a challenging extension, since in this case, the sufficient statistics of the Dirichlet distribution can not be used directly. However, by
p(o =1) p(o=1) a b Private & Shared Information Rumour Propagation Private Information Only Centralised Reputation Figure 5: Instances of ˆp(X) and Cov(p(X)) plotted as second standard error ellipses after 5 communication iterations.
Method E[E[U]] ± Var(E[U]) Private and Shared Information 3.18 ± 0.54 Rumour Propagation 3.33 ± 0.07 Private Information Only 3.20 ± 0.65 Centralised Reputation 3.17 ± 0.42 Table 1: Estimated expected utility and its standard error as calculated by a single agent after 5 communication iterations. addressing this challenge, we hope to be able to apply these techniques to a setting in which a suppliers provides a range of services whose failures are correlated, and agents only have direct experiences of different subsets of these services.
This research was undertaken as part of the ALADDIN (Autonomous Learning Agents for Decentralised Data and Information Networks) project and is jointly funded by a BAE Systems and EPSRC strategic partnership (EP/C548051/1).
[1] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan. Estimation with Applications to Tracking and Navigation. Wiley Interscience, 2001. [2] C. Boutilier. The foundations of expected expected utility. In Proc. of the 4th Int. Joint Conf. on on Artificial Intelligence, pages 285-290, Acapulco,
Mexico, 2003. [3] M. Evans, N. Hastings, and B. Peacock. Statistical Distributions. John Wiley & Sons, Inc., 1993. [4] N. Griffiths. Task delegation using experience-based multi-dimensional trust.
In Proc. of the 4th Int. Joint Conf. on Autonomous Agents and Multiagent Systems, pages 489-496, New York, USA, 2005. [5] N. Gukrai, D. DeAngelis, K. K. Fullam, and K. S. Barber. Modelling multi-dimensional trust. In Proc. of the 9th Int. Workshop on Trust in Agent Systems, Hakodate, Japan, 2006. [6] A. Jøsang and R. Ismail. The beta reputation system. In Proc. of the 15th Bled Electronic Commerce Conf., pages 324-337, Bled, Slovenia, 2002. [7] E. M. Maximilien and M. P. Singh. Agent-based trust model involving multiple qualities. In Proc. of the 4th Int. Joint Conf. on Autonomous Agents and Multiagent Systems, pages 519-526, Utrecht, The Netherlands, 2005. [8] S. D. Ramchurn, D. Hunyh, and N. R. Jennings. Trust in multi-agent systems.
Knowledge Engineering Review, 19(1):1-25, 2004. [9] S. Reece and S. Roberts. Robust, low-bandwidth, multi-vehicle mapping. In Proc. of the 8th Int. Conf. on Information Fusion, Philadelphia, USA, 2005. [10] J. Sabater and C. Sierra. REGRET: A reputation model for gregarious societies. In Proc. of the 4th Workshop on Deception, Fraud and Trust in Agent Societies, pages 61-69, Montreal, Canada, 2001. [11] W. T. L. Teacy, J. Patel, N. R. Jennings, and M. Luck. TRAVOS: Trust and reputation in the context of inaccurate information sources. Autonomous Agents and Multi-Agent Systems, 12(2):183-198, 2006. [12] S. Utete. Network Management in Decentralised Sensing Systems. PhD thesis,
University of Oxford, UK, 1994.

In many multiagent domains, agents must act in order to provide security against attacks by adversaries. A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing. For example, a security robot may need to make a choice about which areas to patrol, and how often [16]. However, it will not know in advance exactly where a robber will choose to strike. A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy. They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location. It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely. However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.
A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games. A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.
The distribution of adversary types that an agent will face may be known or inferred from historical data. Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games. However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents" strategies are chosen simultaneously [5].
In some settings, one player can (or must) commit to a strategy before the other players choose their strategies. These scenarios are known as Stackelberg games [6]. In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader. For example, the security agent (leader) must first commit to a strategy for patrolling various areas. This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers). The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).
Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously. To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1. The leader is the row player and the follower is the column player. Here, the leader"s payoff is listed first.
Table 1: Payoff table for example normal form game.
The only Nash equilibrium for this game is when the leader plays
311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the follower"s best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability). The leader"s payoff would then be 4 (3 and 5 with equal probability). In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5). However, this would cause the follower to deviate to strategy
to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.
The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers. Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.
Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8]. If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game. However, by transforming the game, the compact structure of the Bayesian game is lost. In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.
This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies). This method has three key advantages. First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example. Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12]. This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.
Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].
The rest of the paper is organized as follows. In Section 2 we fully describe the patrolling domain and its properties. Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leader"s strategy in a Stackelberg game. Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries. Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7.
In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time. Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents" own resources (number of security agents, amount of available time, fuel, etc.). It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14]. To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.
The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agent"s set of pure strategies consists of possible routes of d houses to patrol (in an order). The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen. For example, the robber can observe over time how often the security agent patrols each area. With this knowledge, the robber must choose a single house to rob. We assume that the robber generally takes a long time to rob a house. If the house chosen by the robber is not on the security agent"s route, then the robber successfully robs it. Otherwise, if it is on the security agent"s route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.
We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).
The security agent"s set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house). The robber"s set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.
With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently. If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6].
A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn. For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types. Since there is only one type of security agent, θ1 contains only one element.
During the game, the robber knows its type but the security agent does not know the robber"s type. For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .
A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8]. Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game. While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a
single equilibrium in the general case, which may not be of high reward. Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent. However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.
The next two subsections elaborate on how this is done.
The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game. Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations. Let us assume there are two robber types a and b in the Bayesian game. Robber a will be active with probability α, and robber b will be active with probability 1 − α. The rules described in Section 2 allow us to construct simple payoff tables.
Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}. The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l). Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α. First, consider robber type a. Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Using these values we construct a base payoff table as the payoff for the game against robber type a. For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber. The payoffs for the game against robber type b are constructed using different values.
Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robber"s type, thus transforming the security agent"s incomplete information regarding the robber into imperfect information [3]. The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game. The transformed, normal-form game is shown in Table 3. In the transformed game, the security agent is the column player, and the set of all robber types together is the row player. Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}. Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α.
Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance. Since the followers (the robbers) will know the leader"s strategy, the optimal response for the followers will be a pure strategy. Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].
From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3. The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that player"s types. Therefore, we denote X = σθ1
index sets of the security agent and robbers" pure strategies respectively, with R and C as the corresponding payoff matrices. Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j. A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1. Here, pxi is the probability that the security agent will choose its ith pure strategy.
The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].
For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q,
P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).
The pxi variables give the optimal strategy for the security agent.
Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types. Using this method for a Bayesian game thus requires running |σ2||θ2|  separate linear programs. This is no surprise, since finding the leader"s optimal strategy in a Bayesian Stackelberg game is NP-hard [5].
Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach. In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents" utilities are completely unknown. In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.
One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations. We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers" rewards on the security agent"s rewards. Thus, the ASAP heuristic will produce strategies which are k-uniform. A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3. ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.
Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation. This is because the different follower (robber) types are independent of each other. Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types. This independence property is exploited in ASAP to yield a decomposition scheme. Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems. Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].
For a single follower type, the algorithm works the following way. Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leader"s payoff from x when the follower plays a reward-maximizing pure strategy. We then take the mixed strategy with the highest payoff.
We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards. If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.
Note also that because we limit the leader"s strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leader"s favor is not significant, since ties will be unlikely to arise. This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.
Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming. We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].
Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c.
These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.
Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality. In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ . These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.
Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.
In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP).
We begin with the case of a single type of follower. Let the leader be the row player and the follower the column player. We denote by x the vector of strategies of the leader and q the vector of strategies of the follower. We also denote X and Q the index sets of the leader and follower"s pure strategies, respectively. The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j. Let k be the size of the multiset.
We first fix the policy of the leader to some k-uniform policy x. The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k. We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.
P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the follower"s expected reward given x, while the constraints make feasible any mixed strategy q for the follower. The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the follower"s maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy. Therefore each of these pure strategies is optimal.
Optimal solutions to the follower"s problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.
These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x.
The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).
Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.
P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leader"s reward with the follower"s best response (qj for fixed leader"s policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions. To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x). The leader"s problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.
P i xi = kP j∈Q qj = 1
P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number. The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower. The third constraint enforces dual feasibility of the follower"s problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality). In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.
We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite. Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question. In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist.
We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.
P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1
P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1. Problems (5) and (6) are equivalent.
Proof: Consider x, q a feasible solution of (5). We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value. The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction. The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and
P i∈X zij = kqj.
Let us now consider q, z feasible for (6). We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.
In fact all constraints of (5) are readily satisfied by construction. To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.
This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.
Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types.
ADVERSARIES The MILP developed in the previous section handles only one follower. Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types.
To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types. We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types. We also denote by X and Q the index sets of leader and follower l"s pure strategies, respectively. We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .
Using this modified notation, we characterize the optimal solution of follower l"s problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower l"s problem we can linearize the complementarity constraint above.
We incorporate these constraints on the leader"s problem that selects the optimal k-uniform policy. Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.
P i xi = kP j∈Q ql j = 1
− P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game. In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl . The same relation holds between C and Cl . These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent.
We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.
P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1
− P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2. Problems (7) and (8) are equivalent.
Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).
We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value. The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction. The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8). Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.
Lets now consider ql , zl , al feasible for (8). We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value. In fact all constraints of (7) are readily satisfied by construction. To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0. Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl. In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8). Therefore xiql j = zl ijl ql j = zl ij. This last equality is because both are 0 when j = jl.
Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.
This shows that the transformation preserves the objective function value, completing the proof.
We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables. We implemented the decomposed MILP and the results are shown in the following section.
The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3. We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses. The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.
The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case. All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.
Using the data generated, we performed the experiments using four methods for generating the security agent"s strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1. Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8]. The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes. We use this method as a simple baseline to measure the performance of our heuristics. We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14]. The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.
Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.
In the first two sets of graphs, ASAP is run using a multiset of
graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains. Each of the three rows of graphs corresponds to a different randomly-generated scenario. The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds. All experiments that were not concluded in 30 minutes (1800 seconds) were cut off. The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.
The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime. For a
Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios. MIP-Nash solves for even fewer robber types within the cutoff time. On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time. The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.
The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains. This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased. The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward. The ASAP method remains consistently close to the optimal, even as the number of robber types increases.
The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP. This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.
The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and
reward. Results here are for the three-house domain. The trend is that as as the multiset size is increased, the runtime and reward level both increase. Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain. In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset. The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds. In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space. However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem.
This paper focuses on security for agents patrolling in hostile environments. In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information. Specifically, we deal with situations where the adversaries" actions and payoffs are known but the exact adversary type is unknown to the security agent. Agents acting in the real world quite frequently have such incomplete information about other agents. Bayesian games have been a popular choice to model such incomplete information games [3]. The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Gala"s guarantees are focused on fully competitive games. Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18]. Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]). However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.
Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem. First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement.
Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.
Our k-uniform strategies are similar to the k-uniform strategies of [12]. While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies. This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.
Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13]. However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].
Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE). It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010. Sarit Kraus is also affiliated with UMIACS.
[1] R. W. Beard and T. McLain. Multiple UAV cooperative search under collision avoidance and limited range communication constraints. In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg. Bayesian games for threat prediction and situation analysis. In FUSION, 2004. [4] Y. Chevaleyre. Theoretical analysis of multi-agent patrolling problem. In AAMAS, 2004. [5] V. Conitzer and T. Sandholm. Choosing the best strategy to commit to. In ACM Conference on Electronic Commerce,
[6] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991. [7] C. Gui and P. Mohapatra. Virtual patrol: A new power conservation design for surveillance using sensor networks.
In IPSN, 2005. [8] J. C. Harsanyi and R. Selten. A generalized Nash solution for two-person bargaining games with incomplete information.
Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer. Generating and solving imperfect information games. In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer. Representations and solutions for game-theoretic problems. Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson. Equilibrium points of bimatrix games. Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta. Playing large games using simple strategies. In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.
Multi-agent patrolling: an empirical analysis on alternative architectures. In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In AAMAS,
[15] T. Roughgarden. Stackelberg scheduling strategies. In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.
Patrolling in a stochastic environment. In 10th Intl.
Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer. Mixed-integer programming methods for finding nash equilibria. In AAAI,
[18] S. Singh, V. Soni, and M. Wellman. Computing approximate Bayes-Nash equilibria with tree-games of incomplete information. In ACM Conference on Electronic Commerce,

Information Filtering (IF) systems aim at countering information overload by extracting information that is relevant for a given user out of a large body of information available via an information provider. In contrast to Information Retrieval (IR) systems, where relevant information is extracted based on search queries, IF architectures generate personalized information based on user profiles containing, for each given user, personal data, preferences, and rated items. The provided body of information is usually structured and collected in provider profiles. Filtering techniques operate on these profiles in order to generate recommendations of items that are probably relevant for a given user, or in order to determine users with similar interests, or both. Depending on the respective goal, the resulting systems constitute Recommender Systems [5], Matchmaker Systems [10], or a combination thereof.
The aspect of privacy is an essential issue in all IF systems: Generating personalized information obviously requires the use of personal data. According to surveys indicating major privacy concerns of users in the context of Recommender Systems and e-commerce in general [23], users can be expected to be less reluctant to provide personal information if they trust the system to be privacy-preserving with regard to personal data. Similar considerations also apply to the information provider, who may want to control the dissemination of the provided information, and to the provider of the filtering techniques, who may not want the details of the utilized filtering algorithms to become common knowledge. A privacy-preserving IF system should therefore balance these requirements and protect the privacy of all parties involved in a multilateral way, while addressing general requirements regarding performance, security and quality of the recommendations as well. As described in the following section, there are several approaches with similar goals, but none of these provide a generic approach in which the privacy of all parties is preserved.
We have developed an agent-based approach for privacypreserving IF which has been utilized for realizing a combined Recommender/Matchmaker System as part of an application supporting users in planning entertainment-related activities. In this paper, we focus on the Recommender System functionality. Our approach is based on Multi-Agent System (MAS) technology because fundamental features of agents such as autonomy, adaptability and the ability to communicate are essential requirements of our approach. In other words, the realized approach does not merely constitute a solution for privacy-preserving IF within a MAS context, but rather utilizes a MAS architecture in order to realize a solution for privacy-preserving IF, which could not be realized easily otherwise.
The paper is structured as follows: Section 2 describes related work. Section 3 describes the general ideas of our approach. In Section 4, we describe essential details of the 319 978-81-904262-7-5 (RPS) c 2007 IFAAMAS modules of our approach and their implementation. In Section 5, we evaluate the approach, mainly via the realized application. Section 6 concludes the paper with an outlook and outlines further work.
There is a large amount of work in related areas, such as Private Information Retrieval [7], Privacy-Preserving Data Mining [2], and other privacy-preserving protocols [4, 16], most of which is based on Secure Multi-Party Computation [27]. We have ruled out Secure Multi-Party Computation approaches mainly because of their complexity, and because the algorithm that is computed securely is not considered to be private in these approaches.
Various enforcement mechanisms have been suggested that are applicable in the context of privacy-preserving Information Filtering, such as enterprise privacy policies [17] or hippocratic databases [1], both of which annotate user data with additional meta-information specifying how the data is to be handled on the provider side. These approaches ultimately assume that the provider actually intends to protect the privacy of the user data, and offer support for this task, but they are not intended to prevent the provider from acting in a malicious manner. Trusted computing, as specified by the Trusted Computing Group, aims at realizing trusted systems by increasing the security of open systems to a level comparable with the level of security that is achievable in closed systems. It is based on a combination of tamper-proof hardware and various software components. Some example applications, including peer-to-peer networks, distributed firewalls, and distributed computing in general, are listed in [13].
There are some approaches for privacy-preserving Recommender Systems based on distributed collaborative filtering, in which recommendations are generated via a public model aggregating the distributed user profiles without containing explicit information about user profiles themselves. This is achieved via Secure Multi-Party Computation [6], or via random perturbation of the user data [20]. In [19], various approaches are integrated within a single architecture.
In [10], an agent-based approach is described in which user agents representing similar users are discovered via a transitive traversal of user agents. Privacy is preserved through pseudonymous interaction between the agents and through adding obfuscating data to personal information. More recent related approaches are described in [18].
In [3], an agent-based architecture for privacy-preserving demographic filtering is described which may be generalized in order to support other kinds of filtering techniques.
While in some aspects similar to our approach, this architecture addresses at least two aspects inadequately, namely the protection of the filter against manipulation attempts, and the prevention of collusions between the filter and the provider.
INFORMATION FILTERING We identify three main abstract entities participating in an information filtering process within a distributed system: A user entity, a provider entity, and a filter entity. Whereas in some applications the provider and filter entities explicitly trust each other, because they are deployed by the same party, our solution is applicable more generically because it does not require any trust between the main abstract entities. In this paper, we focus on aspects related to the information filtering process itself, and omit all aspects related to information collection and processing, i.e. the stages in which profiles are generated and maintained, mainly because these stages are less critical with regard to privacy, as they involve fewer different entities.
Our solution aims at meeting the following requirements with regard to privacy: • User Privacy: No linkable information about user profiles should be acquired permanently by any other entity or external party, including other user entities.
Single user profile items, however, may be acquired permanently if they are unlinkable, i.e. if they cannot be attributed to a specific user or linked to other user profile items. Temporary acquisition of private information is permitted as well. Sets of recommendations may be acquired permanently by the provider, but they should not be linkable to a specific user.
These concessions simplify the resulting protocol and allow the provider to obtain recommendations and single unlinkable user profile items, and thus to determine frequently requested information and optimize the offered information accordingly. • Provider Privacy: No information about provider profiles, with the exception of the recommendations, should be acquired permanently by other entities or external parties. Again, temporary acquisition of private information is permitted. Additionally, the propagation of provider information is entirely under the control of the provider. Thus, the provider is enabled to prevent misuse such as the automatic large-scale extraction of information. • Filter Privacy: Details of the algorithms applied by the filtering techniques should not be acquired permanently by any other entity or external party. General information about the algorithm may be provided by the filter entity in order to help other entities to reach a decision on whether to apply the respective filtering technique.
In addition, general requirements regarding the quality of the recommendations as well as security aspects, performance and broadness of the resulting system have to be addressed as well. While minor trade-offs may be acceptable, the resulting system should reach a level similar to regular Recommender Systems with regard to these requirements.
The basic idea for realizing a protocol fulfilling these privacy-related requirements in Recommender Systems is implied by allowing the temporary acquisition of private information (see [8] for the original approach): User and provider entity both propagate the respective profile data to the filter entity. The filter entity provides the recommendations, and subsequently deletes all private information, thus fulfilling the requirement regarding permanent acquisition of private information.
The entities whose private information is propagated have to be certain that the respective information is actually acquired temporarily only. Trust in this regard may be established in two main ways: • Trusted Software: The respective entity itself is trusted to remove the respective information as specified. • Trusted Environment: The respective entity operates in an environment that is trusted to control the communication and life cycle of the entity to an extent that the removal of the respective information may be achieved regardless of the attempted actions of the entity itself. Additionally, the environment itself is trusted not to act in a malicious manner (e.g. it is trusted not to acquire and propagate the respective information itself).
In both cases, trust may be established in various ways.
Reputation-based mechanisms, additional trusted third parties certifying entities or environments, or trusted computing mechanisms may be used. Our approach is based on a trusted environment realized via trusted computing mechanisms, because we see this solution as the most generic and realistic approach. This decision is discussed briefly in Section 5.
We are now able to specify the abstract information filtering protocol as shown in Figure 1: The filter entity deploys a Temporary Filter Entity (TFE) operating in a trusted environment. The user entity deploys an additional relay entity operating in the same environment. Through mechanisms provided by this environment, the relay entity is able to control the communication of the TFE, and the provider entity is able to control the communication of both relay entity and the TFE. Thus, it is possible to ensure that the controlled entities are only able to propagate recommendations, but no other private information. In the first stage (steps
the TFE, and thus prevents it from propagating user profile information. User profile data is propagated without participation of the provider entity from the user entity to the TFE via the relay entity. In the second stage (steps 2.1 to
both relay and TFE, and thus prevents them from propagating provider profile information. Provider profile data is propagated from the provider entity to the TFE via the relay entity. In the third stage (steps 3.1 to 3.5 of Figure 1), the TFE returns the recommendations via the relay entity, and the controlled entities are terminated. Taken together, these steps ensure that all private information is acquired temporarily only by the other main entities. The problems of determining acceptable queries on the provider profile and ensuring unlinkability of the recommendations are discussed in the following section.
Our approach requires each entity in the distributed architecture to have the following five main abilities: The ability to perform certain well-defined tasks (such as carrying out a filtering process) with a high degree of autonomy, i.e. largely independent of other entities (e.g. because the respective entity is not able to communicate in an unrestricted manner), the ability to be deployable dynamically in a well-defined environment, the ability to communicate with other entities, the ability to achieve protection against external manipulation attempts, and the ability to control and restrict the communication of other entities.
Figure 1: The abstract privacy-preserving information filtering protocol. All communication across the environments indicated by dashed lines is prevented with the exception of communication with the controlling entity.
MAS architectures are an ideal solution for realizing a distributed system characterized by these features, because they provide agents constituting entities that are actually characterized by autonomy, mobility and the ability to communicate [26], as well as agent platforms as environments providing means to realize the security of agents. In this context, the issue of malicious hosts, i.e. hosts attacking agents, has to be addressed explicitly. Furthermore, existing MAS architectures generally do not allow agents to control the communication of other agents. It is possible, however, to expand a MAS architecture and to provide designated agents with this ability. For these reasons, our solution is based on a FIPA[11]-compliant MAS architecture. The entities introduced above are mapped directly to agents, and the trusted environment in which they exist is realized in the form of agent platforms.
In addition to the MAS architecture itself, which is assumed as given, our solution consists of the following five main modules: • The Controller Module described in Section 4.1 provides functionality for controlling the communication capabilities of agents. • The Transparent Persistence Module facilitates the use of different data storage mechanisms, and provides a uniform interface for accessing persistent information, which may be utilized for monitoring critical interactions involving potentially private information e.g. as part of queries. Its description is outside the scope of this paper. • The Recommender Module, details of which are described in Section 4.2, provides Recommender System functionality. • The Matchmaker Module provides Matchmaker System functionality. It additionally utilizes social aspects of MAS technology. Its description is outside the scope of this paper.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 321 • Finally, a separate module described in Section 4.4 provides Exemplary Filtering Techniques in order to show that various restrictions imposed on filtering techniques by our approach may actually be fulfilled.
The trusted environment introduced above encompasses the MAS architecture itself and the Controller Module, which have to be trusted to act in a non-malicious manner in order to rule out the possibility of malicious hosts.
IMPLEMENTATION In this section, we describe the main modules of our approach, and outline the implementation. While we have chosen a specific architecture for the implementation, the specification of the module is applicable to any FIPA-compliant MAS architecture. A module basically encompasses ontologies, functionality provided by agents via agent services, and internal functionality. Throughout this paper, {m}KX denotes a message m encrypted via a non-specified symmetric encryption scheme with a secret key KX used for encryption and decryption which is initially known only to participant X. A key KXY is a key shared by participants X and Y . A cryptographic hash function is used at various points of the protocol, i.e. a function returning a hash value h(x) for given data x that is both preimage-resistant and collision-resistant1 . We denote a set of hash values for a data set X = {x1, .., xn} as H(X) = {h(x1), .., h(xn)}, whereas h(X) denotes a single hash value of a data set.
As noted above, the ability to control the communication of agents is generally not a feature of existing MAS architectures2 but at the same time a central requirement of our approach for privacy-preserving Information Filtering. The required functionality cannot be realized based on regular agent services or components, because an agent on a platform is usually not allowed to interfere with the actions of other agents in any way. Therefore, we add additional infrastructure providing the required functionality to the MAS architecture itself, resulting in an agent environment with extended functionality and responsibilities.
Controlling the communication capabilities of an agent is realized by restricting via rules, in a manner similar to a firewall, but with the consent of the respective agent, its incoming and outgoing communication to specific platforms or agents on external platforms as well as other possible communication channels, such as the file system. Consent is required because otherwise the overall security would be compromised, as attackers could arbitrarily block various communication channels. Our approach does not require controlling the communication between agents on the same platform, and therefore this aspect is not addressed.
Consequently, all rules addressing communication capabilities have to be enforced across entire platforms, because otherwise a controlled agent could just use a non-controlled agent 1 In the implementation, we have used the Advanced Encryption Standard (AES) as the symmetric encryption scheme and SHA-1 as the cryptographic hash function. 2 A recent survey on agent environments [24] concludes that aspects related to agent environments are often neglected, and does not indicate any existing work in this particular area. on the same platform as a relay for communicating with agents residing on external platforms. Various agent services provide functionality for adding and revoking control of platforms, including functionality required in complex scenarios where controlled agents in turn control further platforms.
The implementation of the actual control mechanism depends on the actual MAS architecture. In our implementation, we have utilized methods provided via the Java Security Manager as part of the Java security model. Thus, the supervisor agent is enabled to define custom security policies, thereby granting or denying other agents access to resources required for communication with other agents as well as communication in general, such as files or sockets for TCP/IP-based communication.
The Recommender Module is mainly responsible for carrying out information filtering processes, according to the protocol described in Table 1. The participating entities are realized as agents, and the interactions as agent services. We assume that mechanisms for secure agent communication are available within the respective MAS architecture. Two issues have to be addressed in this module: The relevant parts of the provider profile have to be retrieved without compromising the user"s privacy, and the recommendations have to be propagated in a privacy-preserving way.
Our solution is based on a threat model in which no main abstract entity may safely assume any other abstract entity to act in an honest manner: Each entity has to assume that other entities may attempt to obtain private information, either while following the specified protocol or even by deviating from the protocol. According to [15], we classify the former case as honest-but-curious behavior (as an example, the TFE may propagate recommendations as specified, but may additionally attempt to propagate private information), and the latter case as malicious behavior (as an example, the filter may attempt to propagate private information instead of the recommendations).
As outlined above, the relay agent relays data between the TFE agent and the provider agent. These agents are not allowed to communicate directly, because the TFE agent cannot be assumed to act in an honest manner. Unlike the user profile, which is usually rather small, the provider profile is often too voluminous to be propagated as a whole efficiently. A typical example is a user profile containing ratings of about 100 movies, while the provider profile contains some 10,000 movies. Retrieving only the relevant part of the provider profile, however, is problematic because it has to be done without leaking sensitive information about the user profile. Therefore, the relay agent has to analyze all queries on the provider profile, and reject potentially critical queries, such as queries containing a set of user profile items.
Because the propagation of single unlinkable user profile items is assumed to be uncritical, we extend the information filtering protocol as follows: The relevant parts of the provider profile are retrieved based on single anonymous interactions between the relay and the provider. If the MAS architecture used for the implementation does not provide an infrastructure for anonymous agent communication, this feature has to be provided explicitly: The most straightforward way is to use additional relay agents deployed via
Table 1: The basic information filtering protocol with participants U = user agent, P = provider agent, F = TFE agent, R = relay agent, based on the abstract protocol shown in Figure 1. UP denotes the user profile with UP = {up1, .., upn}, PP denotes the provider profile, and REC denotes the set of recommendations with REC = {rec1, .., recm}.
Phase. Sender → Message or Action Step Receiver
the main relay agent and used once for a single anonymous interaction. Obviously, unlinkability is only achieved if multiple instances of the protocol are executed simultaneously between the provider and different users. Because agents on controlled platforms are unable to communicate anonymously with the respective controlling agent, control has to be established after the anonymous interactions have been completed. To prevent the uncontrolled relay agents from propagating provider profile data, the respective data is encrypted and the key is provided only after control has been established. Therefore, the second phase of the protocol described in Table 1 is replaced as described in Table 2.
Additionally, the relay agent may allow other interactions as long as no user profile items are used within the queries.
In this case, the relay agent has to ensure that the provider does not obtain any information exceeding the information deducible via the recommendations themselves. The clusterbased filtering technique described in Section 4.3 is an example for a filtering technique operating in this manner.
The propagation of the recommendations is even more problematic mainly because more participants are involved: Recommendations have to be propagated from the TFE agent via the relay and provider agent to the user agent. No participant should be able to alter the recommendations or use them for the propagation of private information.
Therefore, every participant in this chain has to obtain and verify the recommendations in unencrypted form prior to the next agent in the chain, i.e. the relay agent has to verify the recommendations before the provider obtains them, and so on.
Therefore, the final phase of the protocol described in Table
two parts (Step 3.1 to 3.4, and Step 3.5 to Step 3.8), each of which provide a solution for a problem related to the prisoners" problem [22], in which two participants (the prisoners) intend to exchange a message via a third, untrusted participant (the warden) who may read the message but must not be able to alter it in an undetectable manner. There are various solutions for protocols addressing the prisoners" probTable 2: The updated second stage of the information filtering protocol with definitions as above. PPq is the part of the provider profile PP returned as the result of the query q.
Phase. Sender → Message or Action Step Receiver repeat 2.1 to 2.3 ∀ up ∈ UP:
→ P q(up) (R remains anonymous)
{PPq(up)}KP
Table 3: The updated final stage of the information filtering protocol with definitions as above.
Phase. Sender → Message or Action Step Receiver
repeat 3.5 ∀ rec ∈ REC:
repeat 3.6 ∀ rec ∈ REC:
}KPrec repeat 3.7 to 3.8 ∀ rec ∈ REC:
lem. The more obvious of these, however, such as protocols based on the use of digital signatures, introduce additional threats e.g. via the possibility of additional subliminal channels [22]. In order to minimize the risk of possible threats, we have decided to use a protocol that only requires a symmetric encryption scheme.
The first part of the final phase is carried out as follows: In order to prevent the relay from altering recommendations, they are propagated by the filter together with an encrypted hash in Step 3.1. Thus, the relay is able to verify the recommendations before they are propagated further.
The relay, however, may suspect the data propagated as the encrypted hash to contain private information instead of the actual hash value. Therefore, the encrypted hash is encrypted again and propagated together with a hash on the respective key in Step 3.2. In Step 3.3, the key KP F is revealed to the relay, allowing the relay to validate the encrypted hash. In Step 3.4, the key KR is revealed to the provider, allowing the provider to decrypt the data received in Step 3.2 and thus to obtain H(REC). Propagating the hash of the key KR prevents the relay from altering the recommendations to REC after Step 3.3, which would be undetectable otherwise because the relay could choose a key KR so that {{H(REC)}KPF }KR = {{H(REC )}KPF }KR .
The encryption scheme used for encrypting the hash has to be secure against known-plaintext attacks, because otherwise the relay may be able to obtain KP F after Step 3.1 and subsequently alter the recommendations in an undetectable The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 323 way. Additionally, the encryption scheme must not be commutative for similar reasons.
The remaining protocol steps are interactions between relay, provider and user agent. The interactions of Step 3.5 to Step 3.8 ensure, via the same mechanisms used in Step
recommendations before the user obtains them, but at the same time prevent the provider from altering the recommendations. Additionally, the recommendations are not processed at once, but rather one at a time, to prevent the provider from withholding all recommendations.
Upon completion of the protocol, both user and provider have obtained a set of recommendations. If the user wants these recommendations to be unlinkable to himself, the user agent has to carry out the entire protocol anonymously.
Again, the most straightforward way to achieve this is to use additional relay agents deployed via the user agent which are used once for a single information filtering process.
The filtering technique applied by the TFE agent cannot be chosen freely: All collaboration-based approaches, such as collaborative filtering techniques based on the profiles of a set of users, are not applicable because the provider profile does not contain user profile data (unless this data has been collected externally). Instead, these approaches are realized via the Matchmaker Module, which is outside the scope of this paper. Learning-based approaches are not applicable because the TFE agent cannot propagate any acquired data to the filter, which effectively means that the filter is incapable of learning. Filtering techniques that are actually applicable are feature-based approaches, such as content-based filtering (in which profile items are compared via their attributes) and knowledge-based filtering (in which domain-specific knowledge is applied in order to match user and provider profile items). An overview of different classes and hybrid combinations of filtering techniques is given in [5]. We have implemented two generic content-based filtering approaches that are applicable within our approach: A direct content-based filtering technique based on the class of item-based top-N recommendation algorithms [9] is used in cases where the user profile contains items that are also contained in the provider profile. In a preprocessing stage, i.e. prior to the actual information filtering processes, a model is generated containing the k most similar items for each provider profile item. While computationally rather complex, this approach is feasible because it has to be done only once, and it is carried out in a privacy-preserving way via interactions between the provider agent and a TFE agent. The resulting model is stored by the provider agent and can be seen as an additional part of the provider profile. In the actual information filtering process, the k most similar items are retrieved for each single user profile item via queries on the model (as described in Section 4.2.1, this is possible in a privacy-preserving way via anonymous communication). Recommendations are generated by selecting the n most frequent items from the result sets that are not already contained within the user profile.
As an alternative approach applicable when the user profile contains information in addition to provider profile items, we provide a cluster-based approach in which provider profile items are clustered in a preprocessing stage via an agglomerative hierarchical clustering approach. Each cluster is represented by a centroid item, and the cluster elements are either sub-clusters or, on the lowest level, the items themselves. In the information filtering stage, the relevant items are retrieved by descending through the cluster hierarchy in the following manner: The cluster items of the highest level are retrieved independent of the user profile. By comparing these items with the user profile data, the most relevant sub-clusters are determined and retrieved in a subsequent iteration. This process is repeated until the lowest level is reached, which contains the items themselves as recommendations. Throughout the process, user profile items are never propagated to the provider as such. The information deducible about the user profile does not exceed the information deducible via the recommendations themselves (because essentially only a chain of cluster centroids leading to the recommendations is retrieved), and therefore it is not regarded as privacy-critical.
We have implemented the approach for privacy-preserving IF based on JIAC IV [12], a FIPA-compliant MAS architecture. JIAC IV integrates fundamental aspects of autonomous agents regarding pro-activeness, intelligence, communication capabilities and mobility by providing a scalable component-based architecture. Additionally, JIAC IV offers components realizing management and security functionality, and provides a methodology for Agent-Oriented Software Engineering. JIAC IV stands out among MAS architectures as the only security-certified architecture, since it has been certified by the German Federal Office for Information Security according to the EAL3 of the Common Criteria for Information Technology Security standard [14]. JIAC IV offers several security features in the areas of access control for agent services, secure communication between agents, and low-level security based on Java security policies [21], and thus provides all security-related functionality required for our approach.
We have extended the JIAC IV architecture by adding the mechanisms for communication control described in Section
assume all providers of agent platforms to be trusted. We are additionally developing a solution that is actually based on a trusted computing infrastructure.
For the evaluation of our approach, we have examined whether and to which extent the requirements (mainly regarding privacy, performance, and quality) are actually met.
Privacy aspects are directly addressed by the modules and protocols described above and therefore not evaluated further here. Performance is a critical issue, mainly because of the overhead caused by creating additional agents and agent platforms for controlling communication, and by the additional interactions within the Recommender Module.
Overall, a single information filtering process takes about ten times longer than a non-privacy-preserving information filtering process leading to the same results, which is a considerable overhead but still acceptable under certain conditions, as described in the following section.
As a proof of concept, and in order to evaluate performance and quality under real-life conditions, we have ap324 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 2: The Smart Event Assistant, a privacypreserving Recommender System supporting users in planning entertainment-related activities. plied our approach within the Smart Event Assistant, a MAS-based Recommender System which integrates various personalized services for entertainment planning in different German cities, such as a restaurant finder and a movie finder [25]. Additional services, such as a calendar, a routing service and news services complement the information services. An intelligent day planner integrates all functionality by providing personalized recommendations for the various information services, based on the user"s preferences and taking into account the location of the user as well as the potential venues. All services are accessible via mobile devices as well3 . Figure 2 shows a screenshot of the intelligent day planner"s result dialog. The Smart Event Assistant is entirely realized as a MAS system providing, among other functionality, various filter agents and different service provider agents, which together with the user agents utilize the functionality provided by our approach.
Recommendations are generated in two ways: A push service delivers new recommendations to the user in regular intervals (e.g. once per day) via email or SMS. Because the user is not online during these interactions, they are less critical with regard to performance and the protracted duration of the information filtering process is acceptable in this case. Recommendations generated for the intelligent day planner, however, have to be delivered with very little latency because the process is triggered by the user, who expects to receive results promptly. In this scenario, the overall performance is substantially improved by setting up the relay agent and the TFE agent oﬄine, i.e. prior to the user"s request, and by an efficient retrieval of the relevant 3 The Smart Event Assistant may be accessed online via http://www.smarteventassistant.de.
Table 4: Complexity of typical privacy-preserving (PP) vs. non-privacy-preserving (NPP) filtering processes in the realized application. In the nonprivacy-preserving version, an agent retrieves the profiles directly and propagates the result to a provider agent. scenario push day planning version NPP PP NPP PP profile size (retrieved/total amount of items) user 25/25 25/25 provider 125/10,000 500/10,000 elapsed time in filtering process (in seconds) setup n/a 2.2 n/a oﬄine database access 0.2 0.5 0.4 0.4 profile propagation n/a 0.8 n/a 0.3 filtering algorithm 0.2 0.2 0.2 0.2 result propagation 0.1 1.1 0.1 1.1 complete time 0.5 4.8 0.7 2.0 part of the provider profile: Because the user is only interested in items, such as movies, available within a certain time period and related to specific locations, such as screenings at cinemas in a specific city, the relevant part of the provider profile is usually small enough to be propagated entirely. Because these additional parameters are not seen as privacy-critical (as they are not based on the user profile, but rather constitute a short-term information need), the relevant part of the provider profile may be propagated as a whole, with no need for complex interactions. Taken together, these improvements result in a filtering process that takes about three times as long as the respective nonprivacy-preserving filtering process, which we regard as an acceptable trade-off for the increased level of privacy. Table
detail. In these scenarios, a direct content-based filtering technique similar to the one described in Section 4.3 is applied. Because equivalent filtering techniques have been applied successfully in regular Recommender Systems [9], there are no negative consequences with regard to the quality of the recommendations.
As described in Section 3.2, our solution is based on trusted computing. There are more straightforward ways to realize privacy-preserving IF, e.g. by utilizing a centralized architecture in which the privacy-preserving providerside functionality is realized as trusted software based on trusted computing. However, we consider these approaches to be unsuitable because they are far less generic: Whenever some part of the respective software is patched, upgraded or replaced, the entire system has to be analyzed again in order to determine its trustworthiness, a process that is problematic in itself due to its complexity. In our solution, only a comparatively small part of the overall system is based on trusted computing. Because agent platforms can be utilized for a large variety of tasks, and because we see trusted computing as the most promising approach to realize secure and trusted agent environments, it seems reasonable to assume that these respective mechanisms will be generally available in the future, independent of specific solutions such as the one described here.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 325
We have developed an agent-based approach for privacypreserving Recommender Systems. By utilizing fundamental features of agents such as autonomy, adaptability and the ability to communicate, by extending the capabilities of agent platform managers regarding control of agent communication, by providing a privacy-preserving protocol for information filtering processes, and by utilizing suitable filtering techniques we have been able to realize an approach which actually preserves privacy in Information Filtering architectures in a multilateral way. As a proof of concept, we have used the approach within an application supporting users in planning entertainment-related activities.
We envision various areas of future work: To achieve complete user privacy, the protocol should be extended in order to keep the recommendations themselves private as well.
Generally, the feedback we have obtained from users of the Smart Event Assistant indicates that most users are indifferent to privacy in the context of entertainment-related personal information. Therefore, we intend to utilize the approach to realize a Recommender System in a more privacysensitive domain, such as health or finance, which would enable us to better evaluate user acceptance.
We would like to thank our colleagues Andreas Rieger and Nicolas Braun, who co-developed the Smart Event Assistant. The Smart Event Assistant is based on a project funded by the German Federal Ministry of Education and Research under Grant No. 01AK037, and a project funded by the German Federal Ministry of Economics and Labour under Grant No. 01MD506.
[1] R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu.
Hippocratic databases. In 28th Int"l Conf. on Very Large Databases (VLDB), Hong Kong, 2002. [2] R. Agrawal and R. Srikant. Privacy-preserving data mining. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 439-450. ACM Press,
May 2000. [3] E. A¨ımeur, G. Brassard, J. M. Fernandez, and F. S.
Mani Onana. Privacy-preserving demographic filtering. In SAC "06: Proceedings of the 2006 ACM symposium on Applied computing, pages 872-878, New York, NY, USA, 2006. ACM Press. [4] M. Bawa, R. Bayardo, Jr., and R. Agrawal.
Privacy-preserving indexing of documents on the network. In Proc. of the 2003 VLDB, 2003. [5] R. Burke. Hybrid recommender systems: Survey and experiments. User Modeling and User-Adapted Interaction, 12(4):331-370, 2002. [6] J. Canny. Collaborative filtering with privacy. In IEEE Symposium on Security and Privacy, pages 45-57, 2002. [7] B. Chor, O. Goldreich, E. Kushilevitz, and M. Sudan.
Private information retrieval. In IEEE Symposium on Foundations of Computer Science, pages 41-50, 1995. [8] R. Ciss´ee. An architecture for agent-based privacy-preserving information filtering. In Proceedings of the 6th International Workshop on Trust, Privacy,
Deception and Fraud in Agent Systems, 2003. [9] M. Deshpande and G. Karypis. Item-based top-N recommendation algorithms. ACM Trans. Inf. Syst., 22(1):143-177, 2004. [10] L. Foner. Political artifacts and personal privacy: The yenta multi-agent distributed matchmaking system.
PhD thesis, MIT, 1999. [11] Foundation for Intelligent Physical Agents. FIPA Abstract Architecture Specification, Version L, 2002. [12] S. Fricke, K. Bsufka, J. Keiser, T. Schmidt,
R. Sesseler, and S. Albayrak. Agent-based telematic services and telecom applications. Communications of the ACM, 44(4), April 2001. [13] T. Garfinkel, M. Rosenblum, and D. Boneh. Flexible OS support and applications for trusted computing. In Proceedings of HotOS-IX, May 2003. [14] T. Geissler and O. Kroll-Peters. Applying security standards to multi agent systems. In AAMAS Workshop: Safety & Security in Multiagent Systems, 2004. [15] O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game. In Proc. of STOC "87, pages 218-229, New York, NY, USA, 1987. ACM Press. [16] S. Jha, L. Kruger, and P. McDaniel. Privacy preserving clustering. In ESORICS 2005, volume 3679 of LNCS. Springer, 2005. [17] G. Karjoth, M. Schunter, and M. Waidner. The platform for enterprise privacy practices: Privacy-enabled management of customer data. In PET 2002, volume 2482 of LNCS. Springer, 2003. [18] H. Link, J. Saia, T. Lane, and R. A. LaViolette. The impact of social networks on multi-agent recommender systems. In Proc. of the Workshop on Cooperative Multi-Agent Learning (ECML/PKDD "05), 2005. [19] B. N. Miller, J. A. Konstan, and J. Riedl. PocketLens: Toward a personal recommender system. ACM Trans.
Inf. Syst., 22(3):437-476, 2004. [20] H. Polat and W. Du. SVD-based collaborative filtering with privacy. In Proc. of SAC "05, pages 791-795,
New York, NY, USA, 2005. ACM Press. [21] T. Schmidt. Advanced Security Infrastructure for Multi-Agent-Applications in the Telematic Area. PhD thesis, Technische Universit¨at Berlin, 2002. [22] G. J. Simmons. The prisoners" problem and the subliminal channel. In D. Chaum, editor, Proc. of Crypto "83, pages 51-67. Plenum Press, 1984. [23] M. Teltzrow and A. Kobsa. Impacts of user privacy preferences on personalized systems: a comparative study. In Designing personalized user experiences in eCommerce, pages 315-332. 2004. [24] D. Weyns, H. Parunak, F. Michel, T. Holvoet, and J. Ferber. Environments for multiagent systems: State-of-the-art and research challenges. In Environments for Multiagent Systems, volume 3477 of LNCS. Springer, 2005. [25] J. Wohltorf, R. Ciss´ee, and A. Rieger. Berlintainment: An agent-based context-aware entertainment planning system. IEEE Communications Magazine, 43(6), 2005. [26] M. Wooldridge and N. R. Jennings. Intelligent agents: Theory and practice. Knowledge Engineering Review, 10(2):115-152, 1995. [27] A. Yao. Protocols for secure computation. In Proc. of IEEE FOGS "82, pages 160-164, 1982.

The field of agent communication language (ACL) research has long been plagued by problems of verifiability and grounding [10, 13, 17]. Early mentalistic semantics that specify the semantics of speech acts in terms of preand post-conditions contingent on mental states of the participants (e.g. [3, 4, 12, 15]) lack verifiability regarding compliance of agents with the intended semantics (as the mental states of agents cannot be observed in open multiagent systems (MASs)). Unable to safeguard themselves against abuse by malicious, deceptive or malfunctioning agents, mentalistic semantics are inherently unreliable and inappropriate for use in open MAS in which agents with potentially conflicting objectives might deliberately exploit their adversaries" conceptions of message semantics to provoke a certain behaviour.
Commitment-based semantics [6, 8, 14], on the other hand, define the meaning of messages exchanged among agents in terms of publicly observable commitments, i.e. pledges to bring about a state of affairs or to perform certain actions. Such semantics solve the verifiability problem as they allow for tracing the status of existing commitments at any point in time given observed messages and actions so that any observer can, for example, establish whether an agent has performed a promised action. However, this can only be done a posteriori, and this creates a grounding problem as no expectations regarding what will happen in the future can be formed at the time of uttering or receiving a message purely on the grounds of the ACL semantics.
Further, this implies that the semantics specification does not provide an interface to agents" deliberation and planning mechanisms and hence it is unclear how rational agents would be able to decide whether to subscribe to a suggested ACL semantics when it is deployed.
Finally, none of the existing approaches allows the ACL to specify how to respond to a violation of its semantics by individual agents. This has two implications: Firstly, it is left it up to the individual agent to reason about potential violations, i.e. to bear the burden of planning its own reaction to others" non-compliant behaviour (e.g. in order to sanction them) and to anticipate others" reactions to own misconduct without any guidance from the ACL specification. Secondly, existing approaches fail to exploit the possibilities of sanctioning and rewarding certain behaviours in a communication-inherent way by modifying the future meaning of messages uttered or received by compliant/deviant agents.
In this paper, we propose dynamic semantics (DSs) for ACLs as a solution to these problems. Our notion of DS is based on the very simple idea of defining different alternatives for the meaning of individual speech acts (so-called semantic variants) in an ACL semantics specification, and transition rules between semantic states (i.e. collections of variants for different speech acts) that describe the current meaning of the ACL. These elements taken together result in a FSM-like view of ACL specifications where each individual state provides a complete ACL semantics and state transitions are triggered by observed agent behaviour in order to (1) reflect future expectations based on previous interaction experience and (2) sanction or reward certain kinds of behaviour.
In defining a DS framework for commitment-based ACLs, this paper makes three contributions:
provide an improved notion of grounding commitments in agent interaction and to allow ACL specifications to be directly used for planning-based rational decision making.
expected behaviour with respect to an ACL specification that enables reasoning about the potential behaviour of agents purely from an ACL semantics perspective.
with agent behaviour and how this can be used to describe communication-inherent sanctioning and rewarding mechanisms essential to the design of open MASs.
Furthermore, we discuss desiderata for DS design that can be derived from our framework, present examples and analyse their properties.
The remainder of this paper is structured as follows: Section 2 introduces a formal framework for dynamic ACL semantics. In section 3 we present an analysis and discussion of this framework and discuss desiderata for the design of ACLs with dynamic semantics. Section 4 reviews related approaches, and section 5 concludes.
Our general framework for describing the kind of MASs we are interested in is fairly simple. Let Ag = {1, . . . , n} a finite set of agents, {Aci}i∈Ag a collection of action sets (where Aci are the actions of agent i), A = ×n i=1Aci the joint action space, and Env a set of environment states. A run is a sequence r = e1 a1 → . . . at−1 → et where ai ∈ A (ai[j] denotes the action of agent j in this tuple), and ei ∈ Env.
We define |r| = t, last(r) = et, r[1 : j] is short for the j-long initial sub-sequence of r, and we write r r for any run r iff ∃j ∈ N.r = r[1 : j].
Writing R(Env, A) for the set of all possible runs, we can view each agent i as a function gi : R(Env, A) → Aci describing the agent"s action choices depending on the history of previous environment states and joint actions. The set of all agent functions for i given A and Env is denoted by Gi(Env, A). The (finite, discrete, stationary, fully accessible, deterministic) environment is defined by a state transformer function f : Env × A → Env, so that the system"s operation for an initial state e1 is defined by ei+1 = f(ei, g(e1 a1 → . . . ai−1 → ei)) for all i ≥ 1 (g is the joint vector of functions gi). This definition implies that execution of actions is synchronised among agents, so that the system evolves though an execution of rounds where all agents perform their actions simultaneously.
We denote the set of all runs given a particular configuration of agent functions g by R(Env, A, g). We write gi ∼ r where gi an agent function and r a run iff ∀1 ≤ j ≤ |r|.gi(r[1 : j]) = aj [i] (i.e. gi is compatible with r in every time step as far as i"s actions are concerned).
We use a (standard) propositional logical language L with entailment relation e |= ϕ for e ∈ Env and ϕ ∈ L deunset pending cancelled active violated fulfilled Figure 1: Commitment states and state transitions in the Fornara and Colombetti model: edges drawn using solid lines indicate transitions brought about by agent communication, dashed lines indicate physical agent action or environmental events that cause state transitions fined in the usual way.1 We introduce special propositions Done(i, a) for each action a ∈ ∪n i=1Aci in L to denote it is true that action a has just been performed, extending |= to runs r in the following way: r |= ϕ if last(r) |= ϕ r |= Done(i, a) if r = e1 a1 → . . . at−1 → et ∧ a = at−1[i] i.e. Done(i, a) is exactly true for those actions that made up part of the joint action vector ai−1 in the predecessor state, and all other formulae that were entailed by the last state of r are still valid. Our model implies that each agent executes exactly one action in each time step.
Our notion of commitments is based on a slight variation of the framework proposed by Fornara and Colombetti [6]: Commitments come into existence as unset, e.g. when a request for achieving χ if a certain condition ϕ becomes true is issued from i to j. The commitment becomes pending if the debtor j is required to fulfill it, e.g. after having accepted it. A pending commitment will become active if its condition ϕ becomes true, and if χ is brought about in that case it becomes fulfilled, otherwise violated. Commitments can become cancelled in different situations, e.g. if an unset commitment is rejected. Also, environmental events can lead χ to become true in which case the commitment becomes fulfilled without the debtor"s contribution. Figure 1 provides a graphic representation of commitment state transitions in this framework.
Apart from a slightly different notation used to maintain a more detailed history of commitments, we will extend them to also contain a deactivation condition ψ apart from ϕ (which we call activation condition) which causes any commitment to be cancelled if it becomes true. 1 More precisely L contains atomic propositions P = {p, q, . . .}, the usual connectives ∨ and ¬ (with abbreviations ⇒ and ∧). As for semantics, a function interpretation function I : P ×Env → { , ⊥} assigns a truth value to each proposition in each environmental state, and the entailment relation e |= ϕ for e ∈ Env and ϕ ∈ L is defined inductively: e |= ϕ if ϕ ∈ P and I(ϕ, e) = ; e |= ¬ϕ if e |= ϕ; e |= ϕ ∨ ψ if e |= ϕ or e |= ψ.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 101 D : CS ← CS∪{ ι, c : χ ⊕ ϕ ψ t| ι, s : χ ⊕ ϕ ψ ∈ CS, r |= ψ, s ∈ {u, p, a}, ι, c : χ ⊕ ϕ ψ /∈ CS} A : CS ← CS∪{ ι, a : χ ⊕ ϕ ψ t| ι, p : χ ⊕ ϕ ψ ∈ CS, r |= ϕ, ι, a : χ ⊕ ϕ ψ /∈ CS} S : CS ← CS∪{ ι, f : χ ⊕ ϕ ψ t| ι, a : χ ⊕ ϕ ψ ∈ CS, r |= χ, ι, f : χ ⊕ ϕ ψ /∈ CS} F : CS ← CS∪{ ι, f : χ ⊕ ϕ ψ i→j t | ι, a : χ ⊕ ϕ ψ i→j t−1 ∈ CS, r |= Done(i, a), causes(a, χ)} V : CS ← CS∪{ ι, v : χ ⊕ ϕ ψ i→j t | ι, a : χ ⊕ ϕ ψ i→j t−1 ∈ CS, r |= Done(i, a), ¬causes(a, χ)} Table 1: Environmental commitment processing rules for current run r with |r| = t Definition 1. A commitment is a structure ι, s : χ ⊕ ϕ ψ i→j t where - ι is a unique commitment identifier, - s denotes the commitment state (any of unset, pending, active, violated, fulfilled, or cancelled, abbreviated by the respective initial), - i is the debtor, j is the creditor, - χ ∈ L is the debitum (i.e. the proposition that i commits to making true in front of j), - ϕ, ψ ∈ L are the activation/deactivation conditions, - and t is the instant (in a run) at which this commitment entered its current state s.
As an example, x, v : received(5, $500) ⊕ received(3, toys) returned(3, toys) 3→5 12 denotes that agent 3 violated commitment x towards agent 5 to pay him $500 in timestep 12. He was supposed to make the payment after receiving the toys unless he sent back the toys. We introduce deactivation conditions so as to be able to completely revoke existing commitments: Sending back the money does not constitute a fulfillment of the original contract, but instead an annulment thereof. This provides us with the capability to define validity conditions using ϕ and ψ, which is useful for things like deadlines for unset commitments (if I don"t get a response within 3 time-steps my request will expire).
For brevity, we sometimes omit indices or content elements when clear from the context (in particular, we often write Γ for the content χ ⊕ ϕ ψ). We write C for the set of all possible commitments and denote sets of commitments (so-called commitment stores) by CS ∈ ℘fin (C).
To handle the effects of environmental events and agent actions on a commitment store CS, table 1 introduces five commitment transition rules which are executed in each time step by the system or any observer who intends to clarify the status of existing commitments in the order shown: the deactivation rule D is the first to fire and cancels any unset, pending or active commitments if ψ becomes true. For the remaining pending commitments2 , the activation rule A describes how these become active if ϕ becomes true. Note that when ϕ is true in subsequent states we check whether 2 To avoid problems with contradictory commitment specifications (e.g. when both ϕ and ψ become true), we give deactivation strict precedence over activation. this active commitment is contained in CS to avoid duplicates (this is because we keep a full record of the commitment history for reasons which will become clear below).3 Rule S caters for serendipity i.e. fulfillment of commitments not brought about by the respective agent, but simply by environmental changes that made the debitum true.
Finally, the fulfilment/violation rules F/V record whether the action performed by the debtor in the previous step (r |= Done(i, a)) has caused the debitum χ of any commitment which became active in the previous timestep to become true. We need only consider those commitments that became active in the previous step t − 1 since we can verify their fulfilment status in t. This verification hinges on a domain-dependent predicate causes(a, χ) which we have not mentioned so far. It should be true if action a is supposed to bring about χ, and delineates the existing social notion of what constitutes a reasonable attempt to achieve χ in the given context (its definition may range from requiring that χ has actually been achieved to allowing any action a that does not necessarily result in ¬χ).
In Fornara and Colombetti"s and similar approaches, the status of commitments is verifiable, but they are not grounded in expectations about interaction. Such semantics (similar in style to what he have just defined in terms of CS update rules) tell us what commitments exist and which state they are in, but not how this will affect future agent behaviour.
To provide such grounding, we introduce notions of compliant and expected behaviour. An agent is behaving in compliance with its commitments if it always immediately fulfills all active commitments. More precisely, the behaviour of agent i is said to be compliant with CS at time t iff ∀k ≤ t  ι, a : Γ i→j k ∈ CS ⇒ ι, f : Γ i→j k ∈ CS  Though simple, this definition of compliance is not very useful because it places constraints on CSs but not on actual agent functions. To achieve this, we can instead use the contents of the CS to restrict the range of admissible agent functions to those that are in accordance with it using the following definition: Definition 2. For any run r ∈ R(Env, A), let CS(r) the set of commitments that has resulted from execution of r assuming that certain actions (including messages) create commitments or change their status. The set of compliant agent functions with respect to a commitment store CS is 3 While commitment identifiers adversely affect the readability of our notation, they are necessary here to uniquely determine which pending commitment is activated. 102 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) defined as compliant(CS) := ˘ gi ∈ Gi(Env, A) ˛ ˛ ∀r ∼ gi. ι, p : χ ⊕ ϕ ψ i→j ∈ CS(r) = CS. ∀r r. ι, a : χ ⊕ ϕ ψ i→j |r | ∈ CS(r ) ⇒ ` ∃a ∈ Aci.causes(a, χ) ∧ gi(r ) = a ´ ¯ What this definition captures is the following characterisation of a compliant agent function gi: for all runs r that the agent function gi contributes to: if r has created a pending commitment regarding χ, then if this commitment becomes active at the end of some extension r of r in the future, gi will cause the agent to perform an action a that causes χ.4 Next, to cater for the anticipation of non-compliant behaviour we need to introduce a notion of expected behaviour that overrides compliant behaviour. For this, we introduce a second type of commitments which we will call expectations to avoid confusion and distinguish from ordinary (now called normative) commitments by using round brackets (ι, s : Γ)i→j t . They are treated exactly like other commitments in terms of the rules introduced above but express what the agent is expected to do (in the non-normative sense of an objective prediction of behaviour) rather than what it is supposed to do in a normative sense.
To define the notions we need below, we introduce the following constructs: CS := { ι, s : Γ ∈ CS|s ∈ {u, p, a, f, v}} CS := {(ι, s : Γ) ∈ CS|(ι, s : Γ) ∈ CS, ι, s : Γ ∈ CS, s, s ∈ {u, p, a, f, v}} CS simply restricts the commitment store to all normative commitments. Hence, compliant( CS ) specifies what agents are supposed to do. CS , on the other hand, overrides all normative commitment elements in CS for which an expectation also exists, i.e. expectations are given precedence over the normative commitments. With this, we can define expected behaviour as expected(CS) := compliant( CS ) i.e. behaviour that adheres to expectations where such expectations exist and is compliant otherwise. The separate, parallel, treatment of compliant and expected behaviour has two advantages: Firstly, we can respond to unexpected compliant behaviour, i.e. when we expect that someone will not obey their commitments we can still respond to it if they do (and, for example, regain trust in them). Secondly, we can cater for a variety of rules for translating commitment stores to actual future events which a reasoning agent can use in its planning process.
For the purposes of this paper, we will assume that agents base their predictions about others on expected behaviour if it is different from compliant behaviour, and that they predict compliant behaviour, else. 4 Note the quantification in this definition: the property has to hold for every run that gave rise to ι and is compatible with gi. In particular, this must be independent of any part of the history (e.g. other agents" actions and previous environment states) given CS(r). We also quantify over all extensions r of r, i.e. fulfillment of the commitment has to happen if the appropriate conditions arise regardless of other factors.
Table 2 shows an example for a small fragment of an ACL semantics defined using our framework, with two alternative definitions (AC and AC2) for the semantics of the accept message type. Each of the so-called dialogue operators (similar to AI planning action schemata) is defined using the graphical notation p a q where p, a, and q are schemata for preconditions, messages (of a certain type), and post-conditions, respectively.
Preconditions determine whether an action schema is applicable in a certain situation or not and contain formulae from L and/or constraints on the current contents of CS.
PostConditions contain changes to the knowledge base and modifications to CS, i.e. they are interpreted like add/deletelists in traditional AI planning. For any such operator o = p, a, q we define pre(o) = p, action(o) = a and post(o) = q. All elements of a dialogue operator can contain logical variables in their pre- and post-conditions and sender/receiver/content variables in the action slot.
In our example fragment, the operator RQ for requests creates an unset commitment with a fresh identifier ι and current timestamp (we assume that r |= time(t) ⇔ |r| = t, and there is a global system time that can be inspected by all agents), and AC/RJ add a pending/cancelled equivalent of ι to CS. A fragment consisting of {RQ, RJ, AC} is equivalent to the standard semantics of the respective performative types defined in [6].5 Note that our operators only contain objectively verifiable pre- and post-conditions, and if agents want to conform to it they need to comply with these operators. In the following, we will assume that agents always adhere to the ACL specification syntactically6 .
Using AC2 instead of AC enables us to exploit the power of our distinction between compliant and expected behaviour, expressing that we don"t trust i to adhere to the normal semantics of accept: its postcondition specifies that expected(CS) is not restricted to behaviours that will fulfill the commitment but suggest that it has actually been cancelled. At the same time, we maintain the normative commitment that ι is pending so that i"s behaviour would be seen to lie within compliant(CS) if i deviates from our (pessimistic) expectation and does the right thing instead.
To define DS for ACLs we now introduce a state transition system in which each state specifies an ordinary (static) commitment-based semantics and a range of agent pairs for which these semantics are assumed to apply. 5 Note that we allow for requesting identical things before receiving a response and responding several times to the same request. Simple additional conditions can be introduced to avoid these effects which we omit here for lack of space. The same is true of additional constraints to manage control flow issues in actual dialogues (e.g. turn-taking). 6 This means that, for an appropriate variable substitution ϑ, r |= pre(o)ϑ holds when o is applied at r and that CS(r) is transformed according to post(o)ϑ after its application.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 103 RQ : time(t), new(ι) request(i, j, ι : Γ) CS ← CS ∪ { ι, u : Γ i→j t } RJ : ι, u : Γ j→i t ∈ CS, time(t) reject(i, j, ι : Γ) CS ← CS ∪ { ι, c : Γ i→j t } AC : ι, u : Γ j→i t ∈ CS, time(t) accept(i, j, ι : Γ) CS ← CS ∪ { ι, p : Γ i→j t } AC2 : ι, u : Γ j→i t ∈ CS, time(t) accept(i, j, ι : Γ) CS ← CS ∪ { ι, p : Γ i→j t } ∪ {(ι, c : Γ)i→j t } Table 2: Example commitment-based semantics for a small ACL fragment ι, v : Γ i→j ∈ CS : {(i, ∗)} ∪ {(j, i)} s0 s1 ∀ ι, v : Γ i→j t ∈ CS ∃ ι, f : Γ i→j t ∈ CS.t > t : {(i, ∗)} Figure 2: FSM-like state transition diagram describing the Δ-relation in a DS specification Definition 3. A dynamic semantics (DS) is a structure O, S, s0, Δ where - O = {o1, o2, . . . , on} a set of dialogue operators, - S ⊆ ℘(O) is a set of semantic states specified as subsets of dialogue operators which are valid in this state, - s0 ∈ S is the initial semantic state, - and the transition relation Δ ⊆ S × ℘(C) × ℘(Ag × Ag) × S defines the transitions over S triggered by conditions expressed as elements of ℘(C) (C is the set of all possible commitments).
The meaning of a transition (s, c, {(i1, j1), . . . , (in, jn)}, s ) ∈ Δ is as follows: Assume a mapping act : Ag × Ag → S which specifies that the semantics of operators in s holds for messages sent from i to j . Then, if CS ∈ c (i.e. the current CS matches the constraint c given as a collection of possible CSs) this will trigger a transition to state s for all pairs of agents in {(i1, j1), . . . , (in, jn)} for which the constraint was satisfied and will update act accordingly. In other words, the act mapping tracks which version of the semantics is valid for which pairs of communication partners over time.
To illustrate these concepts, consider the following example: Let O = {RQ, RJ, AC, AC2}, S = {s0, s1} where s0 = {RQ, RJ, AC} and s1 = {RQ, RJ, AC2}, i.e. there are two possible states of the semantics which only differ in their definition of accept (we call alternative versions of a single dialogue operator like AC and AC2 semantic variants). We assume that initially act(i, j) = s0 for all agents i, j ∈ Ag.
We describe δ by the transition diagram shown in figure 2.
In this diagram, edges carry labels c : A where c is a constraint on the contents of CS followed by a description of the set of agent pairs A for which the transition should be made to the target state. Writing A(s) = act−1 (s) for the so-called range of agent pairs for which s is active, we use agent variables like i and j and the wildcard symbol ∗ that can be bound to any agent in A(s), and we assume that this binding carries over to descriptions of A. For example, the edge with label  ι, v : Γ i→j ∈ CS : {(i, ∗)} ∪ {(j, i)} can be interpreted as follows: select all pairs (i, j) ∈ A(s0) for which ι, v : Γ i→j ∈ CS applies (i.e. i has violated some commitment toward j) and make s1 valid for the set of agents {(i, k)|k ∈ A(s0)} ∪ {(j, i)}. This means that for all agents i who have lied, s1 will become active for (i, j ) where j ∈ A(s0) and s1 will also become active for (j, i).
The way the DS of the diagram above works is as follows: initially the semantics says (for every agent i) that they will fulfill any commitment truthfully (the use of AC ensures that expected behaviour is equivalent to compliant behaviour). If an agent i violates a commitment once then s1 will become active for i towards all other agents, so that they won"t expect i to fulfill any future commitments.
Moreover, this will also apply to (j, i) so that the culprit i should not expect the deceived agent j to keep its promises towards i either in the future. However, this will not affect expectations regarding their interactions with i by agents other than i (i.e. they still have no right to violate their own commitments). This reflects the idea that (only) agents that have been fooled are allowed to trespass (only) against those agents who trespassed against them. However, if i ever fulfills any commitment again (after the latest violation, this is ensured by the complex constraint used as a label for the transition from s1 to s0), the semantics in s0 will become valid for i again. In this case, though, s1 will still be valid for the pair (j, i), i.e. agent j will regain trust in i but cannot be expected to be trustworthy toward i ever again.
Rather than suggesting that this is a particularly useful communication-inherent mechanism for sanctioning and rewarding specific kinds of behaviour, this example serves to illustrate the expressiveness of our framework and the kind of distinctions it enables us to make.
The semantics of a DS can be defined inductively as follows: Let CS(r) denote the contents of the commitment store after run r as before. We use the notation A(δ, CS) = {(i, j)|CS|i,j ∈ c} ∩ A(s) ∩ A to denote the set of agents that are to be moved from s to s due to transition rule δ = (s, c, A, s ) ∈ Δ given CS, where CS|i,j is the set of commitments that mention i and/or j (in their sender/receiver/content slots). In other words, A(δ, CS) contains those pairs of agents who are (i) mentioned in the commitments covered by the constraint c, (ii) contained in the range of s, and (iii) explicitly listed in A as belonging to those pairs of agents that should be affected by the transition δ. 104 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Definition 4. The state of a dynamic semantics O, S, s0, Δ after run r with immediate predecessor r is defined as a mapping actr as follows:
actr(i, j) = 8 >< >: s if ∃δ = (s, c, A, s ) ∈ Δ. (i, j) ∈ A(δ, CS(r)) actr (i, j) else This maintains the property act−1 r (s) = act−1 r (s) − A(δ, CS(r )), which specifies that the agent pairs to be moved from s to s are removed from the range of s and added to the range of s .
What is not ensured by this definition is consistency of the state transition system, i.e. making sure that the semantic successor state is uniquely identified for any state of the commitment store and previous state so that every agent pair is only assigned one active state in each step, i.e. actr is actually a function for any r.7
Once the DS itself has been specified, we need to integrate the different components of our framework to monitor the dynamics of our ACL semantics and its implications for expected agent behaviour.
Starting with an initially empty commitment store CS and initial semantic state s0 such that actε(i, j) = s0 for any two agents i and j, the agent (or external observer) observes (a partial subset of) everything that is communicated in the system in each step. By applying the commitment transition rules (D, A, S, F and V ) we can update CS accordingly, ignoring any observed message sent from i to j that does not syntactically match the dialogue operator set defined in actr(i, j) for a current run r. After this update has been performed for all observed messages and actions in this cycle, which should not depend on the ordering of messages8 , we can compute for any message sent from i to j the new value of actr (i, j) depending on the semantic transition rules of the DS if r is the successor run of r. With this, we can then determine what the compliant and expected behaviour of agents will be under these new conditions.
Thus, an agent can use information about expected behaviour in its own planning processes by assuming that all agents involved will exhibit their expected (rather than just compliant) behaviours. This prediction will not always be more accurate than under normal (static) ACL semantics, but since it is common knowledge that agents assume expected behaviour to occur (and, by virtue of the DS-ACL specification, have the right to do that) most reasonable dynamic ACL specifications will make provisions to ensure that it is safer to assume expected rather than fully compliant behaviour if they want to promote their use by agents. 7 One way of ensuring this is to require that ∀s ∈ S. (∩{c|(s, c, A, s ) ∈ Δ(s)} = ∅) so that no two constraints pertaining to outgoing edges of s can be fulfilled by CS at a time. In some cases this may be too coarse-grained - it would be sufficient for constraints to be mutually exclusive for the same pair of agents at any point in time - but this would have to be verified for an individual DS on a case-bycase basis. 8 This is the case for our operators, because their pre- and post-conditions never concern or affect any commitments other than those that involve both i and j - avoiding any connection to third parties helps us keep the CS-update independent of the order in which observations are processed.
The main disadvantage of our approach is the space complexity of the dynamic ACL specification: If d is the number of dialogue operators in a language and b is the maximum number of semantic variants of a single dialogue operator within this language, the DS specification would have to specify O(db ) states. In many cases, however, most of the speech acts will not have different variants (like RQ and RJ in our example) and this may significantly reduce the number of DS states that need to be specified.
As for the run-time behaviour of our semantics processing mechanism, we can assume that n messages/actions are sent/performed in each processing step in a system with n agents. Every commitment processing rule (D, S, etc.) has to perform a pass over the contents of CS. In the worst case every originally created commitment (of which there may be nt after t steps) may have immediately become pending, active and violated (which doesn"t require any further physical actions, so that every agent can create a new commitment in each step).Thus, if any agent creates a new commitment in each step without ever fulfilling it, this will result in the total size of CS being in O(nt).9 Regarding semantic state transitions, as many as n different pairs of agents could be affected in a single iteration by n messages. Assuming that the verification of CS-constraints for these transitions would take O(nt), this yields a total update time of O(n2 t) for tracking DS evolution. This bound can be reduced to O(n2 ) if a quasi-stationarity assumption is made by limiting the window of earlier commitments that are being considered when verifying transition constraints to a constant size (and thus obtaining a finite set of possible commitment stores).10
The main strength of our framework is that it allows us to exploit the three main elements of reciprocity: • Reputation-based adaptation: The DS adapts the expectations toward agent i according to i"s previous behaviour by modifying the semantic state to better reflect this behaviour (based on the assumption that it will repeat itself in the future). • Mutuality of expectations: The DS adapts the expectations toward j"s behaviour according to i"s previous behaviour toward j to better reflect j"s response to i"s observed behaviour (in particular, allowing j to behave toward i as i behaved toward j earlier). • Recovery mechanisms: The DS allows i to revert to an earlier semantic state after having undone a change in expectations by a further, later change of behaviour (e.g. by means of redemption).
In open systems in which we cannot enforce certain behaviours, these are effectively the only available means for indirect sanctions and rewards. 9 This is actually only a lower bound on the complexity for commitment processing which could become even worse if dominated by the complexity of verifying entailment |=; however, this would also hold for a static ACL semantics. 10 For example, this could be useful if we want to discard commitments whose status was last modified more than k time steps ago (this is problematic, as it might force us to discard certain unset/pending commitments before they become pending/active).
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 105 There are two further dimensions that affect DS-based sanctioning and reward mechanisms and are orthogonal to the above: One concerns the character of the semantic state changes (i.e. whether it is a reward or punishment), the other the degree of adaptation (reputation-based mechanisms, for example, need not realistically reflect the behaviour of the culprit, but may instead utilise immediate (exaggerated) stigmatisation of agents as a deterrent).
Albeit simple, our example DS described above makes use of all these aspects, and apart from consistency and completeness, it also satisfies some other useful properties:
should have identical pre- and post-conditions, and any two semantic variants of an operator must differ in terms of pre- and/or post-conditions: ∀o, o ∈ O .(pre(o) = pre(o )∧post(o) = post(o ) ⇒ o = o ) ∀o, o ∈ O .(action(o) = action(o ) ⇒ pre(o) = pre(o ) ∨ post(o) = post(o))
causing a transition must be satisfiable in principle when using the dialogue operators and physical actions that are provided: ∀(s, c, A, s ) ∈ Δ ∃r ∈ R(Env, A).CS(r) ∩ c = ∅
behaviour: The content of expectations must differ from that of normative commitments at least for some semantic variants (giving rise to non-compliant expectations for some runs): ∃r ∈ R(Env, A) .expected(CS(r)) = compliant(CS(r))
possible for agents in principle to comply with normative commitments or deviate from them in principle: ∃r ∈ R(Env, A) .expected(CS(r)) = ∅∧ compliant(CS(r)) = ∅ While not absolutely essential, these constitute desiderata for the design of DS-ACLs as they add to the simplicity and clarity of a given semantics specification. Our framework raises interesting questions regarding further potential properties of DS such as:
must not allow an agent to create a pending commitment for another agent or to violate a commitment on behalf of another agent. While in some cases some agents should be able to enforce commitments upon others, this should generally be avoided to ensure agent autonomy.
either disallow commitment to contradictory actions or beliefs, or at least provide operators for rectifying such contradictory claims. Under contradictory commitments, no possible behaviour can be compliantit is up to the designer to decide to which extent this should be permitted.
prediction must not deviate from compliant behaviour prediction if deviant behaviour has not been observed so far (in particular this must hold for the initial semantic state). This might not always be desirable as initial distrust is necessary in some systems, but it increases the chances that agents will agree to participate in communication.
dialogue operators will remain stable after a finite number of transitions, regardless of any further agent behaviour11 . If this property holds, this would imply that agents can stop tracking semantic state transitions after some amount of initial interaction. The advantage of this is reduced complexity, which of course comes at the price of giving up adaptiveness.
behaviour of an agent should lead to a semantic state that predicts compliant behaviour for that agent again.
Here, we have to trade off cautiousness against the provision of incentives to resume cooperative behaviour.
Trusting an agent makes others vulnerable to exploitation - blacklisting an agent forever, though, might lead that agent to keep up its unpredictable and potentially malicious behaviour.
constraints, the same dynamics of semantics should apply to all parties involved.
Our simple example semantics satisfies all these properties apart from convergence. Many of the above properties are debatable, as we have to trade off cautiousness against the provision of incentives for cooperative behaviour.
While we cannot make any general statements here regarding optimal DS-ACL design, our framework provides the tools to test and evaluate the performance of different such communication-inherent sanctioning and rewarding mechanisms (i.e. social rules that do not presuppose ability to direct punishment or reward through physical actions) in real-world applications.
Expectation-based reasoning about interaction was first proposed in [2], considering the evolution of expectations described as probabilistic expectations of communication and action sequences. The same authors suggested a more general framework for expectation-based communication semantics [9], and argue for a consequentialist view of semantics that is based on defining the meaning of utterances in terms of their expected consequences and updating these expectations with new observations [11]. However, their approach does not use an explicit notion of commitments which in our framework mediates between communication and behaviour-based grounding, and provides a clear distinction between a normative notion of compliance and a more empirical notion of expectation.
Grounding for (mentalistic) ACL semantics has been investigated in [7] where grounded information is viewed as information that is publicly expressed and accepted as being true by all the agents participating in a conversation.
Like [1] (which bases the notion of publicly expressed on roles rather than internal states of agents) these authors" main concern is to provide a verifiable basis for determining the semantics of expressed mental states and commitments.
Though our framework is only concerned with commitment to the achievement of states of affairs rather than exchanged information, in a sense, DS provides an alternative view by specifying what will happen if the assumptions on which what is publicly accepted is based are violated. 11 In a non-trivial sense, i.e. when some initial transitions are possible in principle 106 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Our framework is also related to deontic methods for the specification of obligations, norms and sanctions. In this area, [16] is the only framework that we are aware of which considers dynamic obligations, norms and sanctions.
However, as we have described above we solely utilise semantic evolution as a sanctioning and rewarding mechanism, i.e. unlike this work we do not assume that agents can be directly punished or rewarded.
Finally, the FSM-like structure of the DS transition systems in combination with agent communication is reminiscent of work on electronic institutions [5], but there the focus is on providing different means of communication in different scenes of the interaction process (e.g. different protocols for different phases of market-based interaction) whereas we focus on different semantic variants that are to be used in the same interaction context.
This paper introduces dynamic semantics for ACLs as a method for dealing with some fundamental problems of agent communication in open systems, the simple underlying idea being that different courses of agent behaviour can give rise to different interpretations of meaning of the messages exchanged among agents. Based on a common framework of commitment-based semantics, we presented a notion of grounding for commitments based on notions of compliant and expected behaviour. We then defined dynamic semantics as state transition systems over different semantic states that can be viewed as different versions of ACL semantics in the traditional sense, and can be easily associated with a planning-based view of reasoning about communication. Thereby, our focus was on simplicity and on providing mechanisms for tracking semantic evolution in a down-toearth, algorithmic fashion to ensure applicability to many different agent designs.
We discussed the properties of our framework showing how it can be used as a powerful communication-inherent mechanism for rewarding and sanctioning agent behaviour in open systems without compromising agent autonomy, discussed its integration with agents" planning processes, complexity issues, and presented a list of desiderata for the design of ACLs with such semantics.
Currently, we are working on fully-fledged specifications of dynamic semantics for more complex languages and on extending our approach to mentalistic semantics where we view statements about mental states as commitments regarding the rational implications of these mental states (a simple example for this is that an agent commits itself to dropping an ostensible intention that it is claiming to maintain if that intention turns out to be unachievable). In this context, we are particularly interested in appropriate mechanisms to detect and respond to lying by interrogating suspicious agents and forcing them to commit themselves to (sets of) mental states publicly while sanctioning them when these are inconsistent with their actions.
[1] G. Boella, R. Damiano, J. Hulstijn, and L. van der Torre. ACL Semantics between Social Commitments and Mental Attitudes. In Proceedings of the International Workshop on Agent Communication , 2006. [2] W. Brauer, M. Nickles, M. Rovatsos, G. Weiß, and K. F. Lorentzen. Expectation-Oriented Analysis and Design. In Proceedings of the 2nd Workshop on Agent-Oriented Software Engineering , LNCS 2222,
[3] P. R. Cohen and H. J. Levesque. Communicative actions for artificial agents. In Proceedings of the First International Conference on Multi-Agent Systems, pages 65-72, 1995. [4] P. R. Cohen and C. R. Perrault. Elements of a Plan-Based Theory of Speech Acts. Cognitive Science, 3:177-212, 1979. [5] M. Esteva, J. Rodriguez, J. Arcos, C. Sierra, and P. Garcia. Formalising Agent Mediated Electronic Institutions. In Catalan Congres on AI, pages 29-38,
[6] N. Fornara and M. Colombetti. Operational specification of a commitment-based agent communication language. In Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, pages 536-542, Bologna, Italy,
[7] B. Gaudou, A. Herzig, D. Longin, and M. Nickles. A New Semantics for the FIPA Agent Communication Language based on Social Attitudes. In Proceedings of the 17th European Conference on Artificial Intelligence, Riva del Garda, Italy, 2006. IOS Press. [8] F. Guerin and J. Pitt. Denotational Semantics for Agent Communication Languages. In Proceedings of the Fifth International Conference on Autonomous Agents, pages 497-504. ACM Press, 2001. [9] M. Nickles, M. Rovatsos, and G. Weiss.
EmpiricalRational Semantics of Agent Communication. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, New York, NY, 2004. [10] J. Pitt and A. Mamdani. Some Remarks on the Semantics of FIPA"s Agent Communication Language.
Autonomous Agents and Multi-Agent Systems, 2:333-356, 1999. [11] M. Rovatsos, M. Nickles, and G. Weiß. Interaction is Meaning: A New Model for Communication in Open Systems. In Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems, Melbourne, Australia, 2003. [12] M. D. Sadek. Dialogue acts are rational plans. In Proceedings of the ESCA/ETRW Workshop on the Structure of Multimodal Dialogue, pages 1-29, 1991. [13] M. Singh. Agent communication languages: Rethinking the principles. IEEE Computer, 31(12):55-61, 1998. [14] M. Singh. A social semantics for agent communication languages. In Proceedings of the IJCAI Workshop on Agent Communication Languages, 2000. [15] M. P. Singh. A semantics for speech acts. Annals of Mathematics and Artificial Intelligence, 8(1-2):47-71,
[16] G. Weiß, M. Nickles, M. Rovatsos, and F. Fischer.
Specifying the Intertwining of Cooperation and Autonomy in Agent-based Systems. Journal of Networks and Computer Applications, 29, 2007. [17] M. J. Wooldridge. Verifiable semantics for agent communication languages. In Proceedings of the Third International Conference on Multi-Agent Systems, pages 349-356, Paris, France, 1998.

As technology advances, more and more cars are being equipped with devices, which enable them to act as autonomous agents. An important advancement in this respect is the introduction of ad-hoc communication networks (such as Wi-Fi), which enable the exchange of information between cars, e.g., for locating road congestions [1] and optimal routes [15] or improving traffic safety [2].
Vehicle-To-Vehicle (V2V) communication is already onboard by some car manufactures, enabling the collaboration between different cars on the road. For example, GM"s proprietary algorithm [6], called the threat assessment algorithm, constantly calculates, in real time, other vehicles" positions and speeds, and enables messaging other cars when a collision is imminent; Also, Honda has began testing its system in which vehicles talk with each other and with the highway system itself [7].
In this paper, we investigate the attraction of being a selfish agent in vehicular networks. That is, we investigate the benefits achieved by car owners, who tamper with on-board devices and incorporate their own self-interested agents in them, which act for their benefit. We build on the notion of Gossip Networks, introduced by Shavitt and Shay [15], in which the agents can obtain road congestion information by gossiping with peer agents using ad-hoc communication.
We recognize two typical behaviors that the self-interested agents could embark upon, in the context of vehicular networks. In the first behavior, described in Section 4, the objective of the self-interested agents is to maximize their own utility, expressed by their average journey duration on the road. This situation can be modeled in real life by car owners, whose aim is to reach their destination as fast as possible, and would like to have their way free of other cars. To this end they will let their agents cheat the other agents, by injecting false information into the network. This is achieved by reporting heavy traffic values for the roads on their route to other agents in the network in the hope of making the other agents believe that the route is jammed, and causing them to choose a different route.
The second type of behavior, described in Section 5, is modeled by the self-interested agents" objective to cause disorder in the network, more than they are interested in maximizing their own utility. This kind of behavior could be generated, for example, by vandalism or terrorists, who aim to cause as much mayhem in the network as possible.
We note that the introduction of self-interested agents to the network, would most probably motivate other agents to try and detect these agents in order to minimize their effect.
This is similar, though in a different context, to the problem introduced by Lamport et al. [8] as the Byzantine Generals Problem. However, the introduction of mechanisms to deal with self-interested agents is costly and time consuming. In this paper we focus mainly on the attractiveness of selfish behavior by these agents, while we also provide some insights 327 978-81-904262-7-5 (RPS) c 2007 IFAAMAS into the possibility of detecting self-interested agents and minimizing their effect.
To demonstrate the benefits achieved by self-interested agents, we have used a simulation environment, which models the transportation network in a central part of a large real city. The simulation environment is further described in Section 3. Our simulations provide insights to the benefits of self-interested agents cheating. Our findings can motivate future research in this field in order to minimize the effect of selfish-agents.
The rest of this paper is organized as follows. In Section 2 we review related work in the field of self-interested agents and V2V communications. We continue and formally describe our environment and simulation settings in Section 3.
Sections 4 and 5 describe the different behaviors of the selfinterested agents and our findings. Finally, we conclude the paper with open questions and future research directions.
In their seminal paper, Lamport et al. [8] describe the Byzantine Generals problem, in which processors need to handle malfunctioning components that give conflicting information to different parts of the system. They also present a model in which not all agents are connected, and thus an agent cannot send a message to all the other agents. Dolev et al. [5] has built on this problem and has analyzed the number of faulty agents that can be tolerated in order to eventually reach the right conclusion about true data. Similar work is presented by Minsky et al. [11], who discuss techniques for constructing gossip protocols that are resilient to up to t malicious host failures. As opposed to the above works, our work focuses on vehicular networks, in which the agents are constantly roaming the network and exchanging data.
Also, the domain of transportation networks introduces dynamic data, as the load of the roads is subject to change. In addition, the system in transportation networks has a feedback mechanism, since the load in the roads depends on the reports and the movement of the agents themselves.
Malkhi et al. [10] present a gossip algorithm for propagating information in a network of processors, in the presence of malicious parties. Their algorithm prevents the spreading of spurious gossip and diffuses genuine data. This is done in time, which is logarithmic in the number of processes and linear in the number of corrupt parties. Nevertheless, their work assumes that the network is static and also that the agents are static (they discuss a network of processors).
This is not true for transportation networks. For example, in our model, agents might gossip about heavy traffic load of a specific road, which is currently jammed, yet this information might be false several minutes later, leaving the agents to speculate whether the spreading agents are indeed malicious or not. In addition, as the agents are constantly moving, each agent cannot choose with whom he interacts and exchanges data.
In the context of analyzing the data and deciding whether the data is true or not, researchers have focused on distributed reputation systems or decision mechanisms to decide whether or not to share data.
Yu and Singh [18] build a social network of agents" reputations. Every agent keeps a list of its neighbors, which can be changed over time, and computes the trustworthiness of other agents by updating the current values of testimonies obtained from reliable referral chains. After a bad experience with another agent every agent decreases the rating of the "bad" agent and propagates this bad experience throughout the network so that other agents can update their ratings accordingly. This approach might be implemented in our domain to allow gossip agents to identify self-interested agents and thus minimize their effect. However, the implementation of such a mechanism is an expensive addition to the infrastructure of autonomous agents in transportation networks. This is mainly due to the dynamic nature of the list of neighbors in transportation networks. Thus, not only does it require maintaining the neighbors" list, since the neighbors change frequently, but it is also harder to build a good reputation system.
Leckie et al. [9] focus on the issue of when to share information between the agents in the network. Their domain involves monitoring distributed sensors. Each agent monitors a subset of the sensors and evaluates a hypothesis based on the local measurements of its sensors. If the agent believes that a hypothesis is sufficient likely he exchanges this information with the other agents. In their domain, the goal of all the agents is to reach a global consensus about the likelihood of the hypothesis. In our domain, however, as the agents constantly move, they have many samples, which they exchange with each other. Also, the data might also vary (e.g., a road might be reported as jammed, but a few minutes later it could be free), thus making it harder to decide whether to trust the agent, who sent the data.
Moreover, the agent might lie only about a subset of its samples, thus making it even harder to detect his cheating.
Some work has been done in the context of gossip networks or transportation networks regarding the spreading of data and its dissemination.
Datta et al. [4] focus on information dissemination in mobile ad-hoc networks (MANET). They propose an autonomous gossiping algorithm for an infrastructure-less mobile ad-hoc networking environment. Their autonomous gossiping algorithm uses a greedy mechanism to spread data items in the network. The data items are spread to immediate neighbors that are interested in the information, and avoid ones that are not interested. The decision which node is interested in the information is made by the data item itself, using heuristics. However, their work concentrates on the movement of the data itself, and not on the agents who propagate the data. This is different from our scenario in which each agent maintains the data it has gathered, while the agent itself roams the road and is responsible (and has the capabilities) for spreading the data to other agents in the network.
Das et al. [3] propose a cooperative strategy for content delivery in vehicular networks. In their domain, peers download a file from a mesh and exchange pieces of the file among themselves. We, on the other hand, are interested in vehicular networks in which there is no rule forcing the agents to cooperate among themselves.
Shibata et al. [16] propose a method for cars to cooperatively and autonomously collect traffic jam statistics to estimate arrival time to destinations for each car. The communication is based on IEEE 802.11, without using a fixed infrastructure on the ground. While we use the same domain, we focus on a different problem. Shibata et al. [16] mainly focus on efficiently broadcasting the data between agents (e.g., avoid duplicates and communication overhead), as we focus on the case where agents are not cooperative in
nature, and on how selfish agents affect other agents and the network load.
Wang et al. [17] also assert, in the context of wireless networks, that individual agents are likely to do what is most beneficial for their owners, and will act selfishly. They design a protocol for communication in networks in which all agents are selfish. Their protocol motivates every agent to maximize its profit only when it behaves truthfully (a mechanism of incentive compatibility). However, the domain of wireless networks is quite different from the domain of transportation networks. In the wireless network, the wireless terminal is required to contribute its local resources to transmit data.
Thus, Wang et al. [17] use a payment mechanism, which attaches costs to terminals when transmitting data, and thus enables them to maximize their utility when transmitting data, instead of acting selfishly. Unlike this, in the context of transportation networks, constructing such a mechanism is not quite a straightforward task, as self-interested agents and regular gossip agents might incur the same cost when transmitting data. The difference between the two types of agents only exists regarding the credibility of the data they exchange.
In the next section, we will describe our transportation network model and gossiping between the agents. We will also describe the different agents in our system.
We first describe the formal transportation network model, and then we describe the simulations designs.
Following Shavitt and Shay [15] and Parshani [13], the transportation network is represented by a directed graph G(V, E), where V is the set of vertices representing junctions, and E is the set of edges, representing roads. An edge e ∈ E is associated with a weight w > 0, which specifies the time it takes to traverse the road associated with that edge. The roads" weights vary in time according to the network (traffic) load. Each car, which is associated with an autonomous agent, is given a pair of origin and destination points (vertices). A journey is defined as the (not necessarily simple) path taken by an agent between the origin vertex and the destination vertex. We assume that there is always a path between a source and a destination. A journey length is defined as the sum of all weights of the edges constituting this path. Every agent has to travel between its origin and destination points and aims to minimize its journey length.
Initially, agents are ignorant about the state of the roads.
Regular agents are only capable of gathering information about the roads as they traverse them. However, we assume that some agents have means of inter-vehicle communication (e.g., IEEE 802.11) with a given communication range, which enables them to communicate with other agents with the same device. Those agents are referred to as gossip agents. Since the communication range is limited, the exchange of information using gossiping is done in one of two ways: (a) between gossip agents passing one another, or (b) between gossip agents located at the same junction. We assume that each agent stores the most recent information it has received or gathered around the edges in the network.
A subset of the gossip agents are those agents who are selfinterested and manipulate the devices for their own benefit.
We will refer to these agents as self-interested agents. A detailed description of their behavior is given in Sections 4 and 5.
Building on [13], the network in our simulations replicates a central part of a large city, and consists of 50 junctions and 150 roads, which are approximately the number of main streets in the city. Each simulation consists of 6 iterations.
The basic time unit of the iteration is a step, which equivalents to about 30 seconds. Each iteration simulates six hours of movements. The average number of cars passing through the network during the iteration is about 70,000 and the average number of cars in the network at a specific time unit is about 3,500 cars. In each iteration the same agents are used with the same origin and destination points, whereas the data collected in earlier iterations is preserved in the future iterations (referred to as the history of the agent).
This allows us to simulate somewhat a daily routine in the transportation network (e.g., a working week).
Each of the experiments that we describe below is run with 5 different traffic scenarios. Each such traffic scenario differs from one another by the initial load of the roads and the designated routes of the agents (cars) in the network.
For each such scenario 5 simulations are run, creating a total of 25 simulations for each experiment.
It has been shown by Parshani et al. [13, 14] that the information propagation in the network is very efficient when the percentage of gossiping agents is 10% or more. Yet, due to congestion caused by too many cars rushing to what is reported as the less congested part of the network 20-30% of gossiping agents leads to the most efficient routing results in their experiments. Thus, in our simulation, we focus only on simulations in which the percentage of gossip agents is 20%.
The simulations were done with different percentages of self-interested agents. To gain statistical significance we ran each simulation with changes in the set of the gossip agents, and the set of the self-interested agents.
In order to gain a similar ordinal scale, the results were normalized. The normalized values were calculated by comparing each agent"s result to his results when the same scenario was run with no self-interested agents. This was done for all of the iterations. Using the normalized values enabled us to see how worse (or better) each agent would perform compared to the basic setting. For example, if an average journey length of a certain agent in iteration 1 with no selfinterested agent was 50, and the length was 60 in the same scenario and iteration in which self-interested agents were involved, then the normalized value for that agent would be 60/50 = 1.2.
More details regarding the simulations are described in Sections 4 and 5.
UTILITY In the first set of experiments we investigated the benefits achieved by the self-interested agents, whose aim was to minimize their own journey length. The self-interested agents adopted a cheating approach, in which they sent false data to their peers.
In this section we first describe the simulations with the self-interested agents. Then, we model the scenario as a The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 329 game with two types of agents, and prove that the equilibrium result can only be achieved when there is no efficient exchange of gossiping information in the network.
Behavior While the gossip agents gather data and send it to other agents, the self-interested agents" behavior is modeled as follows:
(a) If the road is not in the agent"s route - send the true data about it (e.g., data about roads it has received from other agents) (b) For all roads in the agent"s route, which the agent has not yet traversed, send a random high weight.
Basically, the self-interested agent acts the same as the gossip agent. It collects data regarding the weight of the roads (either by traversing the road or by getting the data from other agents) and sends the data it has collected to other agents. However, the self-interested agent acts differently when the road is in its route. Since the agent"s goal is to reach its destination as fast as possible, the agent will falsely report that all the roads in its route are heavily congested.
This is in order to free the path for itself, by making other agents recalculate their paths, this time without including roads on the self-interested agent"s route. To this end, for all the roads in its route, which the agent has not yet passed, the agent generates a random weight, which is above the average weight of the roads in the network. It then associates these new weights with the roads in its route and sends them to the other agents.
While an agent can also divert cars from its route by falsely reporting congested roads in parallel to its route as free, this behavior is not very likely since other agents, attempting to use the roads, will find the mistake within a short time and spread the true congestion on the road. On the other hand, if an agent manages to persuade other agents not to use a road, it will be harder for them to detect that the said roads are not congested.
In addition, to avoid being influenced by its own lies and other lies spreading in the network, all self-interested agents will ignore data received about roads with heavy traffic (note that data about roads that are not heavily traffic will not be ignored)1 .
In the next subsection we describe the simulation results, involving the self-interested agents.
To test the benefits of cheating by the self-interested agents we ran several experiments. In the first set of experiments, we created a scenario, in which a small group of self-interested agents spread lies on the same route, and tested its effect on the journey length of all the agents in the network. 1 In other simulations we have run, in which there had been several real congestions in the network, we indeed saw that even when the roads are jammed, the self-interested agents were less affected if they ignored all reported heavy traffic, since by such they also discarded all lies roaming the network Table 1: Normalized journey length values, selfinterested agents with the same route Iteration Self-Interested Gossip - Gossip - Regular Number Agents SR Others Agents
Thus, several cars, which had the same origin and destination points, were designated as self-interested agents. In this simulation, we selected only 6 agents to be part of the group of the self-interested agents, as we wanted to investigate the effect achieved by only a small number of agents.
In each simulation in this experiment, 6 different agents were randomly chosen to be part of the group of self-interested agents, as described above. In addition, one road, on the route of these agents, was randomly selected to be partially blocked, letting only one car go through that road at each time step. About 8,000 agents were randomly selected as regular gossip agents, and the other 32,000 agents were designated as regular agents.
We analyzed the average journey length of the self-interested agents as opposed to the average journey length of other regular gossip agents traveling along the same route. Table
agents, the gossip agents (those having the same origin and destination points as the self-interested agents, denoted Gossip - SR, and all other gossip agents, denoted Gossip - Others) and the regular agents, as a function of the iteration number.
We can see from the results that the first time the selfinterested agents traveled the route while spreading the false data about the roads did not help them (using the paired t-test we show that those agents had significantly lower journey lengths in the scenario in which they did not spread any lies, with p < 0.01). This is mainly due to the fact that the lies do not bypass the self-interested agent and reach other cars that are ahead of the self-interested car on the same route. Thus, spreading the lies in the first iteration does not help the self-interested agent to free the route he is about to travel, in the first iteration.
Only when the self-interested agents had repeated their journey in the next iteration (iteration 2) did it help them significantly (p = 0.04). The reason for this is that other gossip agents received this data and used it to recalculate their shortest path, thus avoiding entrance to the roads, for which the self-interested agents had spread false information about congestion. It is also interesting to note the large value attained by the self-interested agents in the first iteration.
This is mainly due to several self-interested agents, who entered the jammed road. This situation occurred since the self-interested agents ignored all heavy traffic data, and thus ignored the fact that the road was jammed. As they started spreading lies about this road, more cars shifted from this route, thus making the road free for the future iterations.
However, we also recall that the self-interested agents ignore all information about the heavy traffic roads. Thus,
Table 2: Normalized journey length values, spreading lies for a beneficiary agent Iteration Beneficiary Gossip - Gossip - Regular Number Agent SR Others Agents
when the network becomes congested, more self-interested cars are affected, since they might enter jammed roads, which they would otherwise not have entered. This can be seen, for example, in iterations 4-6, in which the normalized value of the self-interested agents increased above 1.00.
Using the paired t-test to compare these values with the values achieved by these agents when no lies are used, we see that there is no significant difference between the two scenarios.
As opposed to the gossip agents, we can see how little effect the self-interested agents have on the regular agents.
As compared to the gossip agents on the same route that have traveled as much as 193% more, when self-interested agents are introduced, the average journey length for the regular agents has only increased by about 15%. This result is even lower than the effect on other gossip agents in the entire network.
Since we noticed that cheating by the self-interested agents does not benefit them in the first iteration, we devised another set of experiments. In the second set of experiments, the self-interested agents have the objective to help another agent, who is supposed to enter the network some time after the self-interested agent entered. We refer to the latter agent as the beneficiary agent. Just like a self-interested agent, the beneficiary agent also ignores all data regarding heavy traffic. In real-life this can be modeled, for example, by a husband, who would like to help his wife find a faster route to her destination. Table 2 summarizes the normalized values for the different agents. As in the first set of experiments, 5 simulations were run for each scenario, with a total of 25 simulations. In each of these simulation one agent was randomly selected as a self-interested agent, and then another agent, with the same origin as the selfinterested agent, was randomly selected as the beneficiary agent. The other 8,000 and 32,000 agents were designated as regular gossip agents and regular agents, respectively.
We can see that as the number of iterations advances, the lower the normalized value for the beneficiary agent. In this scenario, just like the previous one, in the first iterations, not only does the beneficiary agent not avoid the jammed roads, since he ignores all heavy traffic, he also does not benefit from the lies spread by the self-interested agent. This is due to the fact that the lies are not yet incorporated by other gossip agents. Thus, if we compare the average journey length in the first iteration when lies are spread and when there are no lies, the average is significantly lower when there are no lies (p < 0.03). On the other hand, if we compare the average journey length in all of the iterations, there is no significant difference between the two settings. Still, in most of the iterations, the average journey length of the beneficiary agent is longer than in the case when no lies are spread.
We can also see the impact on the other agents in the system. While the gossip agents, which are not on the route of the beneficiary agent, virtually are not affected by the self-interested agent, those on the route and the regular agents are affected and have higher normalized values.
That is, even with just one self-interested car, we can see that both the gossip agents that follow the same route as the lies spread by the self-interested agents, and other regular agents, increase their journey length by more than 14%.
In our third set of experiments we examined a setting in which there was an increasing number of agents, and the agents did not necessarily have the same origin and destination points. To model this we randomly selected selfinterested agents, whose objective was to minimize their average journey length, assuming the cars were repeating their journeys (that is, more than one iteration was made).
As opposed to the first set of experiments, in this set the self-interested agents were selected randomly, and we did not enforce the constraint that they will all have the same origin and destination points.
As in the previous sets of experiments we ran 5 different simulations per scenario. In each simulation 11 runs were made, each run with different numbers of self-interested agents: 0 (no self-interested agents), 1, 2, 4, 8, and 16. Each agent adopted the behavior modeled in Section 4.1. Figure
agents as a function of their number. The figure shows these values for iterations 2-6. The first iteration is not shown intentionally, as we assume repeated journeys. Also, we have seen in the previous set of experiments and we have provided explanations as to why the self-interested agents do not gain much from their behavior in the first iteration.
1
Self-Interested Agents Number NormalizedValue Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Figure 1: Self-interested agents normalized values as a function of the number of self-interested agents.
Using these simulations we examined what the threshold could be for the number of randomly selected self-interested agents in order to allow themselves to benefit from their selfish behavior. We can see that up to 8 self-interested agents, the average normalized value is below 1. That is, they benefit from their malicious behavior. In the case of one self-interested agent there is a significant difference between the average journey length of when the agent spread lies and when no lies are spread (p < 0.001), while when The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 331 there are 2, 4, 8 and 16 self-interested agents there is no significance difference. Yet, as the number of self-interested agents increases, the normalized value also increases. In such cases, the normalized value is larger than 1, and the self-interested agents journey length becomes significantly higher than their journey length, in cases where there are no self-interested agents in the system.
In the next subsection we analyze the scenario as a game and show that when in equilibrium the exchange of gossiping between the agents becomes inefficient.
We continued and modeled our scenario as a game, in order to find the equilibrium. There are two possible types for the agents: (a) regular gossip agents, and (b) self-interested agents. Each of these agents is a representative of its group, and thus all agents in the same group have similar behavior.
We note that the advantage of using gossiping in transportation networks is to allow the agents to detect anomalies in the network (e.g., traffic jams) and to quickly adapt to them by recalculating their routes [14]. We also assume that the objective of the self-interested agents is to minimize their own journey length, thus they spread lies on their routes, as described in Section 4.1. We also assume that sophisticated methods for identifying the self-interested agents or managing reputation are not used. This is mainly due to the complexity of incorporating and maintaining such mechanisms, as well as due to the dynamics of the network, in which interactions between different agents are frequent, agents may leave the network, and data about the road might change as time progresses (e.g., a road might be reported by a regular gossip agent as free at a given time, yet it may currently be jammed due to heavy traffic on the road).
Let Tavg be the average time it takes to traverse an edge in the transportation network (that is, the average load of an edge). Let Tmax be the maximum time it takes to traverse an edge. We will investigate the game, in which the self-interested and the regular gossip agents can choose the following actions. The self-interested agents can choose how much to lie, that is, they can choose to spread how long (not necessarily the true duration) it takes to traverse certain roads. Since the objective of the self-interested agents is to spread messages as though some roads are jammed, the traversal time they report is obviously larger than the average time. We denote the time the self-interested agents spread as Ts, such that Tavg ≤ Ts ≤ Tmax. Motivated by the results of the simulations we have described above, we saw that the agents are less affected if they discard the heavy traffic values. Thus, the regular gossip cars, attempting to mitigate the effect of the liars, can choose a strategy to ignore abnormal congestion values above a certain threshold, Tg.
Obviously, Tavg ≤ Tg ≤ Tmax. In order to prevent the gossip agents from detecting the lies and just discarding those values, the self-interested agents send lies in a given range, [Ts, Tmax], with an inverse geometric distribution, that is, the higher the T value, the higher its frequency.
Now we construct the utility functions for each type of agents, which is defined by the values of Ts and Tg. If the self-interested agents spread traversal times higher than or equal to the regular gossip cars" threshold, they will not benefit from those lies. Thus, the utility value of the selfinterested agents in this case is 0. On the other hand, if the self-interested agents spread traversal time which is lower than the threshold, they will gain a positive utility value.
From the regular gossip agents point-of-view, if they accept messages from the self-interested agents, then they incorporate the lies in their calculation, thus they will lose utility points. On the other hand, if they discard the false values the self-interested agents send, that is, they do not incorporate the lies, they will gain utility values. Formally, we use us to denote the utility of the self-interested agents and ug to denote the utility of the regular gossip agents. We also denote the strategy profile in the game as {Ts, Tg}. The utility functions are defined as: us =
Ts − Tavg + 1 if Ts < Tg (1) ug = Tg − Tavg if Ts ≥ Tg Ts − Tg if Ts < Tg (2) We are interested in finding the Nash equilibrium. We recall from [12], that the Nash equilibrium is a strategy profile, where no player has anything to gain by deviating from his strategy, given that the other agent follows his strategy profile. Formally, let (S, u) denote the game, where S is the set of strategy profiles and u is the set of utility functions. When each agent i ∈ {regular gossip, self-interested} chooses a strategy Ti resulting in a strategy profile T = (Ts, Tg) then agent i obtains a utility of ui (T). A strategy profile T∗ ∈ S is a Nash equilibrium if no deviation in the strategy by any single agent is profitable, that is, if for all i, ui (T∗ ) ≥ ui (Ti, T∗ −i). That is, (Ts, Tg) is a Nash equilibrium if the self-interested agents have no other value Ts such that us (Ts, Tg) > us (Ts, Tg), and similarly for the gossip agents.
We now have the following theorem.
Theorem 4.1. (Tavg, Tavg) is the only Nash equilibrium.
Proof. First we will show that (Tavg, Tavg) is a Nash equilibrium. Assume, by contradiction, that the gossip agents choose another value Tg > Tavg. Thus, ug (Tavg, Tg ) = Tavg − Tg < 0. On the other hand, ug (Tavg, Tavg) = 0.
Thus, the regular gossip agents have no incentive to deviate from this strategy. The self-interested agents also have no incentive to deviate from this strategy. By contradiction, again assume that the self-interested agents choose another value Ts > Tavg. Thus, us (Ts , Tavg) = 0, while us (Tavg, Tavg) = 0.
We will now show that the above solution is unique. We will show that any other tuple (Ts, Tg), such that Tavg < Tg ≤ Tmax and Tavg < Ts ≤ Tmax is not a Nash equilibrium.
We have three cases. In the first Tavg < Tg < Ts ≤ Tmax.
Thus, us (Ts, Tg) = 0 and ug (Ts, Tg) = Tg − Tavg. In this case, the regular gossip agents have an incentive to deviate and choose another strategy Tg + 1, since by doing so they increase their own utility: ug (Ts, Tg + 1) = Tg + 1 − Tavg.
In the second case we have Tavg < Ts < Tg ≤ Tmax. Thus, ug (Ts, Tg) = Ts − Tg < 0. Also, the regular gossip agents have an incentive to deviate and choose another strategy Tg −1, in which their utility value is higher: ug (Ts, Tg −1) = Ts − Tg + 1.
In the last case we have Tavg < Ts = Tg ≤ Tmax. Thus, us (Ts, Tg) = Ts − Tg = 0. In this case, the self-interested agents have an incentive to deviate and choose another strategy Tg − 1, in which their utility value is higher: us (Tg − 1, Tg) = Tg − 1 − Tavg + 1 = Tg − Tavg > 0.
Table 3: Normalized journey length values for the first iteration Self-Interested Self-Interested Gossip Regular Agents Number Agents Agents Agents
Table 4: Normalized journey length values for all iterations Self-Interested Self-Interested Gossip Regular Agents Number Agents Agents Agents
The above theorem proves that the equilibrium point is reached only when the self-interested agents send the time to traverse certain edges equals the average time, and on the other hand the regular gossip agents discard all data regarding roads that are associated with an average time or higher. Thus, for this equilibrium point the exchange of gossiping information between agents is inefficient, as the gossip agents are unable to detect any anomalies in the network.
In the next section we describe another scenario for the self-interested agents, in which they are not concerned with their own utility, but rather interested in maximizing the average journey length of other gossip agents.
Another possible behavior that can be adopted by selfinterested agents is characterized by their goal to cause disorder in the network. This can be achieved, for example, by maximizing the average journey length of all agents, even at the cost of maximizing their own journey length.
To understand the vulnerability of the gossip based transportation support system, we ran 5 different simulations for each scenario. In each simulation different agents were randomly chosen (using a uniform distribution) to act as gossip agents, among them self-interested agents were chosen.
Each self-interested agent behaved in the same manner as described in Section 4.1.
Every simulation consisted of 11 runs with each run comprising different numbers of self-interested agents: 0 (no selfinterested agents), 1, 2, 4, 8, 16, 32, 50, 64, 80 and 100.
Also, in each run the number of self-interested agents was increased incrementally. For example: the run with 50 selfinterested agents consisted of all the self-interested agents that were used in the run with 32 self-interested agents, but with an additional 18 self-interested agents.
Tables 3 and 4 summarize the normalized journey length for the self-interested agents, the regular gossip agents and the regular (non-gossip) agents. Table 3 summarizes the data for the first iteration and Table 4 summarizes the data for the average of all iterations. Figure 2 demonstrates the changes in the normalized values for the regular gossip agents and the regular agents, as a function of the iteration number. Similar to the results in our first set of experiments, described in Section 4.2, we can see that randomly selected self-interested agents who follow different randomly selected routes do not benefit from their malicious behavior (that is, their average journey length does not decrease). However, when only one self-interested agent is involved, it does benefit from the malicious behavior, even in the first iteration.
The results also indicate that the regular gossip agents are more sensitive to malicious behavior than regular agentsthe average journey length for the gossip agents increases significantly (e.g., with 32 self-interested agents the average journey length for the gossip agents was 146% higher than in the setting with no self-interested agents at all, as opposed to an increase of only 25% for the regular agents). In contrast, these results also indicate that the self-interested agents do not succeed in causing a significant load in the network by their malicious behavior. 1
2
Iteration Number NormalizedValue
Figure 2: Gossip and regular agents normalized values, as a function of the iteration.
Since the goal of the self-interested agents in this case is to cause disorder in the network rather than use the lies for their own benefits, the question arises as to why would the behavior of the self-interested agents be to send lies about their routes only. Furthermore, we hypothesize that if they all send lies about the same major roads the damage they might inflict on the entire network would be larger that had each of them sent lies about its own route. To examine this hypothesis, we designed another set of experiments. In this set of experiments, all the self-interested agents spread lies about the same 13 main roads in the network. However, the results show quite a smaller impact on other gossip and reguThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 333 Table 5: Normalized journey length values for all iterations. Network with congestions.
Self-Interested Self-Interested Gossip Regular Agents Number Agents Agents Agents
lar agents in the network. The average normalized value for the gossip agents in these simulations was only about 1.07, as opposed to 1.7 in the original scenario. When analyzing the results we saw that although the false data was spread, it did not cause other gossip cars to change their route. The main reason was that the lies were spread on roads that were not on the route of the self-interested agents. Thus, it took the data longer to reach agents on the main roads, and when the agents reached the relevant roads this data was too old to be incorporated in the other agents calculations.
We also examined the impact of sending lies in order to cause chaos when there are already congestions in the network. To this end, we simulated a network in which 13 main roads are jammed. The behavior of the self-interested agents is as described in Section 4.1, and the self-interested agents spread lies about their own route. The simulation results, detailed in Table 5, show that there is a greater incentive for the self-interested agents to cheat when the network is already congested, as their cheating causes more damage to the other agents in the network. For example, whereas the average journey length of the regular agents increased only by about 15% in the original scenario, in which the network was not congested, in this scenario the average journey length of the agents had increased by about 60%.
In this paper we investigated the benefits achieved by self-interested agents in vehicular networks. Using simulations we investigated two behaviors that might be taken by self-interested agents: (a) trying to minimize their journey length, and (b) trying to cause chaos in the network.
Our simulations indicate that in both behaviors the selfinterested agents have only limited success achieving their goal, even if no counter-measures are taken. This is in contrast to the greater impact inflicted by self-interested agents in other domains (e.g., E-Commerce). Some reasons for this are the special characteristics of vehicular networks and their dynamic nature. While the self-interested agents spread lies, they cannot choose which agents with whom they will interact. Also, by the time their lies reach other agents, they might become irrelevant, as more recent data has reached the same agents.
Motivated by the simulation results, future research in this field will focus on modeling different behaviors of the self-interested agents, which might cause more damage to the network. Another research direction would be to find ways of minimizing the effect of selfish-agents by using distributed reputation or other measures.

The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today. On a typical day, more than 40,000 commercial flights operate within the US airspace [14]. In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours. As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions. In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays. The total cost of these delays was estimated to exceed three billion dollars by industry [7].
Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes. The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths. Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem. There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.
An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan. Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system. As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers). In this paper, we focus on agent based system that can be implemented readily. In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D. Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 . In this approach, the agents" actions are to set the separation that approaching aircraft are required to keep. This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow. Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].
In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system. In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search). The first explored reward consisted of the system reward. The second reward was a personalized agent reward based on collectives [3, 17, 18].
The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation. All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents" actions.
Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12]. Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].
The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET. In Section 2, we describe the air traffic flow problem and the simulation tool, FACET. In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents" learning algorithms and reward structures. In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance. Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold.
With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem. Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management). In order to address such issues, the management of this traffic flow occurs over four hierarchical levels:
1 We discuss how flight plans with few fixes can be handled in more detail in Section 2.
Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper. Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either. Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours. The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.
The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights). The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over
and handle changes to the policies caused by weather patterns. Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities. Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.
Figure 1: FACET screenshot displaying traffic routes and air flow statistics.
FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4]. It is based on propagating the trajectories of proposed flights forward in time. FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11]. FACET is extensively used by The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and
FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1). FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes. The user can then observe the effects of these changes to congestion. In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents. The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings.
The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities. Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay. The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z. More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty. The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.
The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise. Intuitively,
Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.
In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness. This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late. In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.
Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA. Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity. Each sector capacity is computed using various metrics which include the number of air traffic controllers available. The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities.
The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above. To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection.
Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent. That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy. However, there are several problems with that approach. First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system. Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.
As an alternative, we assign agents to individual ground locations throughout the airspace called fixes. Each agent is then responsible for any aircraft going through its fix. Fixes offer many advantages as agents:
system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions).
matching behavior to reward is easier.
have the ability to affect traffic flow patterns.
routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.
Figure 2 shows a schematic of this agent based system.
Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions.
The second issue that needs to be addressed, is determining the action set of the agents. Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans. Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).
Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have
to maintain, when going through the agent"s fix. This is known as setting the Miles in Trail or MIT. When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d). When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.
By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.
Figure 2: Schematic of agent architecture. The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times.
The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]). For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used. However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards. As a consequence, simple table-based immediate reward reinforcement learning is used. Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15]. At every episode an agent takes an action and then receives a reward evaluating that action. After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate. At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability . In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25. The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters.
The final issue that needs to be addressed is selecting the reward structure for the learning agents. The first and most direct approach is to let each agent receive the system performance as its reward. However, in many domains such a reward structure leads to slow learning. We will therefore also set up a second set of reward structures based on agent-specific rewards. Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance. In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agent"s actions and aligned with the overall system reward [2, 17, 18].
Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i. All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .
In many situations it is possible to use a ci that is equivalent to taking agent i out of the system. Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agent"s contribution to the system performance. There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17]. Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17].
Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agent"s reward, one issue that may plague D is computational cost. Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known. Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function. We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors. The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation. This form of G matches our system evaluation in the air traffic domain. When we arrange agents so that each aircraft is typically only affected by a single agent, each agent"s impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.
These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf . Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.
To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) .(11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown. However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci. We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .
Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)). These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen. This estimate should improve as the number of samples increases. To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)).
In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator. In all experiments we test the performance of five different methods. The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen. The other four methods are agent based methods where the agents are maximizing one of the following rewards:
can calculate counterfactuals.
i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci).
i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).
These methods are first tested on an air traffic domain with
a single point of congestion over a four hour simulation.
Agents are responsible for reducing congestion at this single point, while trying to minimize delay. The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.
In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to
the time at which most of the aircraft leave the sectors, when no congestion control is being performed. Except where noted, the trade-off between congestion and lateness, α is set to 0.5. In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results. All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.
Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5.
In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents. This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in
the sector of interest to be significantly more than the number allowed by the FAA. The results displayed in Figures
different system evaluations. In both cases, the agent based methods significantly outperform the Monte Carlo method.
This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.
Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.
Among the agent based methods, agents using difference rewards perform better than agents using the system reward. Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward. Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor. In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.
This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.
While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward. Note, however, that the benefit of the estimated difference rewards are only present later in learning. Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward.
In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion. On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.
The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.
Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.
Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.
The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case. Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward. To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.
The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50. Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents. Agents using Dest2  perform slightly better than agents using Dest1 in all cases but for 50 agents. This slight advantage stems from Dest2  providing the agents with a cleaner signal, since its estimate uses more data points.
The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness. This evaluation function The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance. Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α. With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness. To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75. Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.
Next, we perform a series of experiments where α ranges from 0.0 to 1.0 . Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution. This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays. All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.
Therefore, unless D is far from being optimal, the two penalties are not independent. Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives. For both algorithms, the performance degrades significantly for mid-ranges of α.
The results in the previous section show the performance of the different algorithms after a specific number of episodes.
Those results show that D is significantly superior to the other algorithms. One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.
The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.
Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.
Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET. Except when D is used, the values of k are computed once per episode. However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode. While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.
Table 1 shows the performance of the algorithms after
simulations presented in Figure 5 where there were 20 agents,
fully computed D reach 2100 k computations at time step
once for each agent, leading to 21 computations per time step. It therefore reaches 2100 computations at time step
at t=2100, which needs 44100 computations of k as D44K .
Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).
Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost. Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2. This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box.
The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year. The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry. Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix. It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable. The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.
We are currently extending this work in three directions.
First, we are exploring new methods of estimating agent rewards, to further speed up the simulations. Second we are investigating deployment strategies and looking for modifications that would have larger impact. One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion. Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.
Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET.
[1] A. Agogino and K. Tumer. Efficient evaluation functions for multi-rover systems. In The Genetic and Evolutionary Computation Conference, pages 1-12,
Seatle, WA, June 2004. [2] A. Agogino and K. Tumer. Multi agent reward analysis for learning in noisy domains. In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems,
Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer. Handling communiction restrictions and team formation in congestion games.
Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S.
Shethand, and S. R. Grabbe. FACET: Future ATM concepts evaluation tool. Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria. A geometric optimization approach to aircraft conflict resolution. In AIAA Guidance,
Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III. Free flight separation assurance using distributed algorithms. In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005. US Department of Transportation website. [8] S. Grabbe and B. Sridhar. Central east pacific flight routing. In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald,
Richard L. Frost, and Wynn C. Stirling. A cooperative multi-agent approach to free flight. In AAMAS "05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.
Optimal strategies for free flight air traffic conflict resolution. Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.
FACET: Future ATM concepts evaluation tool. Case no. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.
Autonomous agents for air-traffic deconfliction. In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe. Benefits of direct-to in national airspace system. In AIAA Guidance,
Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.
Aggregate flow model for air-traffic management.
Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,
MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry. Conflict resolution for air traffic management. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors. Collectives and the Design of Complex Systems. Springer, New York,
[18] D. H. Wolpert and K. Tumer. Optimal payoff functions for members of collectives. Advances in Complex Systems, 4(2/3):265-279, 2001.

This paper aims to contribute to solve complex stochastic resource allocation problems. In general, resource allocation problems are known to be NP-Complete [12]. In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment. When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state. In general, the number of states is the combination of all possible specific states of each task and available resources. In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks. The very high number of states and actions in this type of problem makes it very complex.
There can be many types of resource allocation problems.
Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent. A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9]. In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources. The planning process starts with the initial state s0. In s0, each agent computes their respective Q-value. Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents. When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.
On the other hand, when the resources are available to all agents, no Q-decomposition is possible. A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently. For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment. Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions. To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states. McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds. This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.
On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds. This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach. Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn. Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp. The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation. The problem is now modelled.
A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}. These two tasks are either in the realized state, or not realized state. To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}. A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks. In this problem, a state represents a conjunction of the particular state of each task, and the available resources. The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint). Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task. For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.
When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable. Indeed, our model may consider consumable and non-consumable resource types. A consumable resource type is one where the amount of available resource is decreased when it is used. On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used. For example, a brush is a non-consumable resource, while the detergent is a consumable resource.
In our problem, the transition function and the reward function are both known. A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process. This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world. An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process. Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total. The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta. Also, S contains a non empty set sg ⊆ S of goal states. A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments). The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta. The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta. The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta. For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.
A solution of an mdp is a policy π mapping states s into actions a ∈ A(s). In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks. The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \ res(a), where res(a) are the consumable resources used by action a. Indeed, since an action a is a resource assignment, Resc \ res(a) is the new set of available resources after the execution of action a. Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).
The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res. The global constraint is defined according to all system trajectories tra ∈ T RA. A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π. For example, state s is entered, which may transit to s or to s , according to action a. The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .
The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra. Since the available consumable resources are represented in the state space, this condition is verified by itself. In other words, the model is Markovian as the history has not to be considered in the state space. Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp. Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed. Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced.
There can be many types of resource allocation problems.
Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.
A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. For instance, a group of agents which manages the oil consummated by a country falls in this group. These agents desire to maximize their specific reward by consuming the right amount of oil. However, all the agents are penalized when an agent consumes oil because of the pollution it generates. Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements). In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2. This can happen depending on the type of missiles, their range, and so on. In this case, two agents can plan for both set of tasks to determine the policy. However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned. IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.
To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning. The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents. That is, R = i∈Ag Ri. It requires each agent to compute a value, from its perspective, for every action. To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration. The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S.
The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value. That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si). The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition. Russell and Zimdars called this approach local Sarsa. In this way, an ideal compromise can be found for the agents to reach a global optimum. Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \ resi(ai) for a resource allocation problem. Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum. Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.
For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution. Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1. In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value. Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s . An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach. In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents. Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation. An agent considers as a possible state transition only the possible states of the set of tasks it manages. Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly. Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.
Then, the arbitrator functionalities are in Lines 8 to 20.
The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a. In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s). Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup. Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent
for action a.
Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions. On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i. Since |SAg| is combinatorial with the number of tasks, so |Si| |S|. Also, |A| is combinatorial with the number of resource types. If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents. In these circumstances, |Ai| |A|. In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup. Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup. Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.
However, when the resources are available to all agents, no Q-decomposition is possible. In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced.
Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.
Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized). For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem. Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version. It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4]. The convergence is accomplished by means of a labeling procedure called checkSolved(s, ). This procedure tries to label as solved each traversed state in the current trajectory. When the initial state is labelled as solved, the algorithm has converged.
In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions. This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s). Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved. On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s). For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).
The values of the bounds are computed in Lines 3 and
Q-values is made simultaneously as the state transitions are the same for both Q-values. Only the values of the state transitions change. Thus, having to compute two Q-values instead of one does not augment the complexity of the approach. In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained. In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A. Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than . When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.
In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|). Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made. This strategy has been proven as efficient [11] [6].
As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked. Since the upper bound for state s is known, it may be estimated The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm. Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal. If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of one"s choice, which outputs a fast and near optimal solution.
Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival. Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.
The next sections describe two separate methods to define hL(s) and hU (s). First of all, the method of Singh and Cohn [10] is briefly described. Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space.
Singh and Cohn [10] defined lower and upper bounds to prune the action space. Their approach is pretty straightforward. First of all, a value function is computed for all tasks to realize, using a standard rtdp approach. Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined. In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta). For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL. The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state. To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s). The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria. Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state. This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].
In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach. The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space.
SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound. To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.
Theorem 2.1. The upper bound defined by Equation 4 is admissible.
Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a. However, hU (s) still overestimates V (s) because the global resource constraint is not enforced. Indeed, each task may use all consumable resources for its own purpose.
Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.
Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU. A standard Bellman backup has a complexity of O(|A| × |S|). Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state. Thus, the computation time of the upper bound is negligible.
The idea to increase SinghL is to allocate the resources a priori among the tasks. When each task has its own set of resources, each task may be solved independently. The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori. The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2]. In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm. Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves. The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state. In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach. These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources. The Line 5 initializes the valueta variable. This variable is the estimated value of each task ta ∈ T a. In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a. Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated. Here, a domain expert may separate all available resources in many types or parts to be allocated. The resources are allocated in the order of its specialization. In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.
Allocating the resources in this order improves the quality of the resulting lower bound. The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a. For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable. The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta. This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation. In Line 21, the marginal revenue is updated in function of the resources already allocated to each task. R(sgta ) is the reward to realize task ta. Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks. The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high. Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.
In Line 24, the resource type res is allocated to the group of resources Resta of task ta. Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta. The first part of the equation to compute valueta represents the expected residual value for task ta.
This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res. In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res). For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.
All resource types are allocated in this manner until Res is empty. All consumable and non-consumable resource types are allocated to each task. When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32. When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL. In particular, the SinghL bound may The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain. As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.
The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource. However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line
Theorem 2.2. The lower bound of Equation 6 is admissible.
Proof: Lowta(sta) is computed with the resource being shared. Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints. Indeed, as the resources are shared, the tasks cannot overuse them. Thus, hL(s) is a realizable policy, and an admissible lower bound.
The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements). For the experiments,
for each approach, and possible number of tasks. In our problem, |Sta| = 4, thus each task can be in four distinct states. There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states. The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states. In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task. When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered. The effectiveness of each resource is modified randomly by ±15% at the start of a scenario. There are also local and global resource constraints on the amount that may be used. For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state. This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies. Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type. The global constraint is generated randomly at the start of a scenario for each consumable resource type. The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.
For this problem a standard lrtdp approach has been implemented. A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved. This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta. Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident. Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy. The approaches described in this paper are compared in Figures 1 and 2.
Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context. In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.
To implement Qdec-lrtdp, we divided the set of tasks in two equal parts. The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi . Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type. When the number of tasks is odd, one more task was assigned to T ai . There are constraint between the group of resource Resi and Resi such that some assignments are not possible. These constraints are managed by the arbitrator as described in Section 2.2. Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.
To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated. For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.
In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex. For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1). The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds. In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds. Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.
Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound. We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp. From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound. Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task. On the other hand, the lower bound of mr-rtdp takes into account the value of all
1 10 100 1000 10000 100000
Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP.
1 10 100 1000 10000
Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.
Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that.
The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.
On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search. In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp. Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments. The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.
An interesting research avenue would be to experiment our bounds with other heuristic search algorithms. For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms. In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds. Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence. Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time.
[1] A. Barto, S. Bradtke, and S. Singh. Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib. An iterative algorithm for solving constrained decentralized markov decision processes. In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner. Faster heuristic search algorithms for planning with uncertainty and full feedback. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner. Labeled lrtdp approach: Improving the convergence of real-time dynamic programming. In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy,
[5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.
Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.
Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.
In ICML "05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005. ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld. Microeconomics.
Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars. Q-decomposition for reinforcement learning agents. In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn. How to dynamically merge markov decision processes. In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998. MIT Press. [11] T. Smith and R. Simmons. Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic. In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston,
USA, 2006. [12] W. Zhang. Modeling and solving a resource allocation problem with soft constraint techniques. Technical report: wucs-2002-13, Washington University,
Saint-Louis, Missouri, 2002.

The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors. In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable. Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].
Such computational issues have recently spawned several threads of work in using compact models of agents" preferences. One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3]. An alternative is to directly model the mechanisms that define the agents" utility functions and perform resource allocation directly with these models [9]. A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes. In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources. This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].
However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs. This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.
In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals. In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs. We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.
In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals). We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.
In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3. In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling. Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method.
Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.
However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents" planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.
In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4. We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here.
A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s.
Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agent"s (finite) lifetime. The agent"s optimal policy is then a function of current state s and the time until the horizon. An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].
This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.
The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP. However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t). An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)). Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).
However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist. In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α. This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6].
A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP. Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.
When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons. Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m]. Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1. Let bτ be the global time horizon for the problem, before which all of the agents" MDPs must finish. We assume τd m < bτ, ∀m ∈ M.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use. Let Ω be the set of resources to be allocated among the agents.
An agent will get at most one resource bundle for one of the time horizons. Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).
The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent. This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements). This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.
The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.
For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon. Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.
Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.
The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined. In this case, the number of ψ values is exponential in each agent"s planning horizon Tm, resulting in a much larger program.
This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t  (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm.
We now formally introduce our model of the resourcescheduling problem. The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω. An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0. We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.
Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}. A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem. The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agent"s lifetime. The second formulation allows reassignment of resources between agents at every time step within their lifetimes.
Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11. The agents" arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively. A solution to this problem is shown via horizontal bars within each agents" box, where the bars correspond to the allocation of the three resource types. Figure 1a shows a solution to a static scheduling problem. According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3. Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively. Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.
Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].
Figure 1b shows a possible solution to the dynamic version of the same problem. There, resources can be reallocated between agents at every time step. For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6. Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).
Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties. We discuss some of those assumptions and their implications in Section 6.
(a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents" lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step).
Our resource-scheduling algorithm proceeds in two stages.
First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.
Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2.
In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system. In other words, the MDPs cannot be paused and resumed. For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.
Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively. Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).
To accomplish this, we augment each agent"s MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2. The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s). For example, in Figure 1a, for agent m2 this would happen at time τ = 4. Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.
More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero. Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1. This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section. For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3. Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .
Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward.
Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem. In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.
Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents" MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid. The resulting optimization problem then simultaneously solves the agents" MDPs and resource-scheduling problems. In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.
In the absence of resource constraints, the agents" finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.
Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ. Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.) Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active. Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.
Tm = τd m − τa m + 1 is the time horizon for the agent"s MDP.
Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents" occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ]. To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ. These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.
The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ. The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 . This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents" occupation measures xm and the activity
indicators θ, as shown in (6) in Table 1.
Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window. This is accomplished by constraint (7) in Table 1.
Furthermore, agents should not be using resources while they are inactive. This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).
Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm. In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm. This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].
After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents" resource usage never exceeds the amounts of available resources. This condition is also trivially expressed as a linear inequality (10) in Table 1.
Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).
This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0. This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.
To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.
As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints. Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents. Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|,
TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ). However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8). The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.
Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2. This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2). We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables.
Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems. In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithm"s scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.
The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop. Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed. In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop. These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards. Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.
This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.
All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM. Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.
Figure 3 shows the runtime and policy value for independent modifications to the parameter set. The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|. Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem. However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance. This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems. We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.
The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules. We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version). We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.
Figure 4 shows runtime and policy value for trials in which common input variables are scaled together. This allows The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225
10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic
10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic
10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic
200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic
400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic
500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3). Top row shows CPU time, and bottom row shows the joint reward of agents" MDP policies. Error bars show the 1st and 3rd quartiles (25% and 75%).
10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic
10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic
10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic
200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic
0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables. The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|). The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively. Error bars show the 1st and 3rd quartiles (25% and 75%).
us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).
Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size.
Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution. We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return. It is easy to relax this assumption for domains where agents" MDPs can be paused and restarted. All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time. We used a reward model where agents" rewards depend only on the time horizon of their MDPs and not the global start time. This is a consequence of our MDP-augmentation procedure from Section 4.1. It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements. For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents. The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.
This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7]. In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.
Finally, we have assumed that agents" arrival and departure times (τa m and τd m) are deterministic and known a priori. This assumption is fundamental to our solution method. While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems. In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].
In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents" values for possible resource assignments are defined by finitehorizon MDPs. This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect. As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems. Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.
We would like to thank the anonymous reviewers for their insightful comments and suggestions.

In this paper, we are primarily interested in the organizational design of a multiagent system - the roles enacted by the agents, ∗Primary author is a student the coordination between the roles and the number and assignment of roles and resources to the individual agents. The organizational design is complicated by the fact that there is no best way to organize and all ways of organizing are not equally effective [2].
Instead, the optimal organizational structure depends both on the problem at hand and the environmental conditions under which the problem needs to be solved. The environmental conditions may not be known a priori, or may change over time, which would preclude the use of a static organizational structure. On the other hand, all problem instances and environmental conditions are not always unique, which would render inefficient the use of a new, bespoke organizational structure for every problem instance.
Organizational Self-Design (OSD) [4, 10] has been proposed as an approach to designing organizations at run-time in which the agents are responsible for generating their own organizational structures. We believe that OSD is especially suited to the above scenario in which the environment is semi-dynamic as the agents can adapt to changes in the task structures and environmental conditions, while still being able to generate relatively stable organizational structures that exploit the common characteristics across problem instances.
In our approach (as in [10]), we define two operators for OSD - agent spawning and composition - when an agent becomes overloaded, it spawns off a new agent to handle part of its task load/responsibility; when an agent lies idle for an extended period of time, it may decide to compose with another agent.
We use TÆMS as the underlying representation for our problem solving requests. TÆMS [11] (Task Analysis, Environment Modeling and Simulation) is a computational framework for representing and reasoning about complex task environments in which tasks (problems) are represented using extended hierarchical task structures [3]. The root node of the task structure represents the high-level goal that the agent is trying to achieve. The sub-nodes of a node represent the subtasks and methods that make up the highlevel task. The leaf nodes are at the lowest level of abstraction and represent executable methods - the primitive actions that the agents can perform. The executable methods, themselves, may have multiple outcomes, with different probabilities and different characteristics such as quality, cost and duration. TÆMS also allows various mechanisms for specifying subtask variations and alternatives, i.e. each node in TÆMS is labeled with a characteristic accumulation function that describes how many or which subgoals or sets of subgoals need to be achieved in order to achieve a particular higherlevel goal. TÆMS has been used to model many different problemsolving environments including distributed sensor networks, information gathering, hospital scheduling, EMS, and military planning. [5, 6, 3, 15].
The main contributions of this paper are as follows:
underlying problem representation, which allows us to model and use OSD for worth-oriented domains. This in turn allows us to reason about (1) alternative task and role assignments that make different quality/cost tradeoffs and generate different organizational structures and (2) uncertainties in the execution of tasks.
resources.
The concept of OSD is not new and has been around since the work of Corkill and Lesser on the DVMT system[4], even though the concept was not fully developed by them. More recently Dignum et. al.[8] have described OSD in the context of the reorganization of agent societies and attempt to classify the various kinds of reorganization possible according to the the reason for reorganization, the type of reorganization and who is responsible for the reorganization decision. According to their scheme, the type of reorganization done by our agents falls into the category of structural changes and the reorganization decision can be described as shared command.
Our research primarily builds on the work done by Gasser and Ishida [10], in which they use OSD in the context of a production system in order to perform adaptive work allocation and load balancing. In their approach, they define two organizational primitives - composition and decomposition, which are similar to our organizational primitives for agent spawning and composition. The main difference between their work and our work is that we use TÆMS as the underlying representation for our problems, which allows, firstly, the representation of a larger, more general class of problems and, secondly, quantitative reasoning over task structures.
The latter also allows us to incorporate different design-to-criteria schedulers [16].
Horling and Lesser [9] present a different, top-down approach to OSD that also uses TÆMS as the underlying representation.
However, their approach assumes a fixed number of agents with designated (and fixed) roles. OSD is used in their work to change the interaction patterns between the agents and results in the agents using different subtasks or different resources to achieve their goals.
We also extend on the work done by Sycara et. al.,[13] on Agent Cloning, which is another approach to resource allocation and load balancing. In this approach, the authors present agent cloning as a possible response to agent overload - if an agent detects that it is overloaded and that there are spare (unused) resources in the system, the agent clones itself and gives its clone some part of its task load. Hence, agent cloning can be thought of as akin to agent spawning in our approach. However, the two approaches are different in that there is no specialization of the agents in the formerthe cloned agents are perfect replicas of the original agents and fulfill the same roles and responsibilities as the original agents. In our approach, on the other hand, the spawned agents are specialized on a subpart of the spawning agent"s task structure, which is no longer the responsibility of the spawning agent. Hence, our approach also deals with explicit organization formation and the coordination of the agents" tasks which are not handled by their approach.
Other approaches to OSD include the work of So and Durfee [14], who describe a top-down model of OSD in the context of Cooperative Distributive Problem Solving (CDPS) and Barber and Martin [1], who describe an adaptive decision making framework in which agents are able to reorganize decision-making groups by dynamically changing (1) who makes the decisions for a particular goal and (2) who must carry out these decisions.The latter work is primarily concerned with coordination decisions and can be used to complement our OSD work, which primarily deals with task and resource allocation.
To ground our discussion of OSD, we now formally describe our task and resource model. In our model, the primary input to the multi-agent system (MAS) is an ordered set of problem solving requests or task instances, < P1, P2, P3, ..., Pn >, where each problem solving request, Pi, can be represented using the tuple < ti, ai, di >. In this scheme, ti is the underlying TÆMS task structure, ai ∈ N+ is the arrival time and di ∈ N+ is the deadline of the ith task instance1 . The MAS has no prior knowledge about the task ti before the arrival time, ai. In order for the MAS to accrue quality, the task ti must be completed before the deadline, di.
Furthermore, every underlying task structure, ti, can be represented using the tuple < T, τ, M, Q, E, R, ρ, C >, where: • T is the set of tasks. The tasks are non-leaf nodes in a TÆMS task structure and are used to denote goals that the agents must achieve. Tasks have a characteristic accumulation function (see below) and are themselves composed of other subtasks and/or methods that need to be achieved in order to achieve the goal represented by that task. Formally, each task Tj can be represented using the pair (qj, sj), where qj ∈ Q and sj ⊂ (T ∪ M). For our convenience, we define two functions SUBTASKS(Task) : T → P(T ∪ M) and SUPERTASKS(TÆMS node) : T ∪ M → P(T), that return the subtasks and supertasks of a TÆMS node respectively2 . • τ ∈ T, is the root of the task structure, i.e. the highest level goal that the organization is trying to achieve. The quality accrued on a problem is equal to the quality of task τ. • M is the set executable methods, i.e., M = {m1, m2, ..., mn}, where each method, mk, is represented using the outcome distribution, {(o1, p1), (o2, p2), ..., (om, pm)}. In the pair (ol, pl), ol is an outcome and pl is the probability that executing mk will result in the outcome ol. Furthermore, each outcome, ol is represented using the triple (ql, cl, dl), where ql is the quality distribution, cl is the cost distribution and dl is the duration distribution of outcome ol. Each discrete distribution is itself a set of pairs, {(n1, p1), (n2, p2), ..., (nn, pn)}, where pi ∈ + is the probability that the outcome will have a quality/cost/duration of nl ∈ N depending on the type of distribution and Pm i=1 pl = 1. • Q is the set of quality/characteristic accumulation functions (CAFs). The CAFs determine how a task group accrues quality given the quality accrued by its subtasks/methods. For our research, we use four CAFs: MIN, MAX, SUM and EXACTLY ONE. See [5] for formal definitions. • E is the set of (non-local) effects. Again, see [5] for formal definitions. • R is the set of resources. • ρ is a mapping from an executable method and resource to the quantity of that resource needed (by an agent) to schedule/execute that method. That is ρ(method, resource) : M × R → N. 1 N is the set of natural numbers including zero and N+ is the set of positive natural numbers excluding zero. 2 P is the power set of set, i.e., the set of all subsets of a set The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1229 • C is a mapping from a resource to the cost of that resource, that is C(resource) : R → N+ We also make the following set of assumptions in our research:
{a1, a2, a3, ...}. That is, we do not assume a fixed set of agents - instead agents are created (spawned) and destroyed (combined) as needed.
structure, i.e. ∃t∀iti = t, where t is the task structure of the problem that the MAS is trying to solve. We believe that this assumption holds for many of the practical problems that we have in mind because TÆMS task structures are basically high-level plans for achieving some goal in which the steps required for achieving the goal-as well as the possible contingency situations-have been pre-computed offline and represented in the task structure. Because it represents many contingencies, alternatives, uncertain characteristics and runtime flexible choices, the same underlying task structure can play out very differently across specific instances.
resource at any given time. Furthermore, we assume that each agent has to own the set of resources that it needseven though the resource ownership can change during the evolution of the organization.
The organizational structure is primarily composed of roles and the relationships between the roles. One or more agents may enact a particular role and one or more roles must be enacted by every agent. The roles may be thought of as the parts played by the agents enacting the roles in the solution to the problem and reflect the long-term commitments made by the agents in question to a certain course of action (that includes task responsibility, authority, and mechanisms for coordination). The relationships between the roles are the coordination relationships that exist between the subparts of a problem.
In our approach, the organizational design is directly contingent on the task structure and the environmental conditions under which the problems need to be solved. We define a role as a TÆMS subtree rooted at a particular node. Hence, the set (T ∪ M) encompasses the space of all possible roles. Note, by definition, a role may consist of one or more other (sub-) roles as a particular TÆMS node may itself be made up of one or more subtrees. Hence, we will use the terms role, task node and task interchangeably.
We, also, differentiate between local and managed (non-local) roles. Local roles are roles that are the sole responsibility of a single agent, that is, the agent concerned is responsible for solving all the subproblems of the tree rooted at that node. For such roles, the agent concerned can do one or more subtasks, solely at its discretion and without consultation with any other agent. Managed roles, on the other hand, must be coordinated between two or more agents as such roles will have two or more descendent local roles that are the responsibility of two or more separate agents. Any of the existing coordination mechanisms (such as GPGP [11]) can be used to achieve this coordination.
Formally, if the function TYPE(Agent, TÆMS Node) : A×(T ∪ M) → {Local, Managed, Unassigned}, returns the type of the responsibility of the agent towards the specified role, then TYPE(a, r) = Local ⇐⇒ ∀ri∈SUBTASKS(r)TYPE(a, ri) = Local TYPE(a, r) = Managed ⇐⇒ [∃a1∃r1(r1 ∈ SUBTASKS(r)) ∧ (TYPE(a1, r1) = Managed)] ∨ [∃a2∃a3∃r2∃r3(a2 = a3) ∧ (r2 = r3) ∧ (r2 ∈ SUBTASKS(r)) ∧ (r3 ∈ SUBTASKS(r)) ∧ (TYPE(a2, r2) = Local) ∧ (TYPE(a3, r3) = Local)]
To form or adapt their organizational structure, the agents use two organizational primitives: agent spawning and composition.
These two primitives result in a change in the assignment of roles to the agents. Agent spawning is the generation of a new agent to handle a subset of the roles of the spawning agent. Agent composition, on the other hand, is orthogonal to agent spawning and involves the merging of two or more agents together - the combined agent is responsible for enacting all the roles of the agents being merged.
In order to participate in the formation and adaption of an organization, the agents need to explicitly represent and reason about the role assignments. Hence, as a part of its organizational knowledge, each agent keeps a list of the local roles that it is enacting and the non-local roles that it is managing. Note that each agent only has limited organizational knowledge and is individually responsible for spawning off or combining with another agent, as needed, based on its estimate of its performance so far.
To see how the organizational primitives work, we first describe four rules that can be thought of as the organizational invariants which will always hold before and after any organizational change:
local.
TYPE(a, r) = Local =⇒ ∀ri∈SUBTASKS(r)TYPE(a, ri) = Local
nodes of that role will be managed.
TYPE(a, r) = Managed =⇒ ∀ri∈SUPERTASKS(r)∃ai(ai ∈ A) ∧ (TYPE(ai, ri) = Managed)
share a common ancestor, that ancestor will be a managed role. (TYPE(a1, r1) = Local) ∧ (TYPE(a2, r2) = Local)∧ (a1 = a2) ∧ (r1 = r2) =⇒ ∀ri∈(SUPERTASKS(r1)∩SUPERTASKS(r2))∃ai(ai ∈ A)∧ (TYPE(ai, ri) = Managed)
responsibility of a single agent, that role will be a local role. ∃a∃r∀ri∈SUBTASKS(r)(a ∈ A) ∧ (r ∈ (T ∪ M))∧ (TYPE(a, ri) = Local) =⇒ (TYPE(a, r) = Local) When a new agent is spawned, the agent doing the spawning will assign one or more of its local roles to the newly spawned agent (Algorithm 1). To preserve invariant rules 2 and 3, the spawning agent will change the type of all the ascendent roles of the nodes assigned to the newly spawned agent from local to managed. Note that the spawning agent is only changing its local organizational knowledge and not the global organizational knowledge. At the
same time, the spawning agent is taking on the task of managing the previously local roles. Similarly, the newly spawned agent will only know of its just assigned local roles.
When an agent (the composing agent) decides to compose with another agent (the composed agent), the organizational knowledge of the composing agent is merged with the organizational knowledge of the composed agent. To do this, the composed agent takes on the roles of all the local and managed tasks of the composing agent. Care is taken to preserve the organizational invariant rules 1 and 4.
Algorithm 1 SpawnAgent(SpawningAgent) : A → A 1: LocalRoles ← {r ⊆ (T ∪ M) | TYPE(SpawningAgent, r)= Local} 2: NewAgent ← CREATENEWAGENT() 3: NewAgentRoles ← FINDROLESFORSPAWNEDAGENT (LocalRoles) 4: for role in NewAgentRoles do 5: TYPE(NewAgent, role) ← Local 6: TYPE(SpawningAgent, role) ← Unassigned 7: PRESERVEORGANIZATIONALINVARIANTS() 8: return NewAgent Algorithm 2 FINDROLESFORSPAWNEDAGENT (SpawningAgentRoles) : (T ∪ M) → (T ∪ M) 1: R ← SpawningAgentRoles 2: selectedRoles ← nil 3: for roleSet in [P(R) − {φ, R}] do 4: if COST(R, roleSet) < COST(R, selectedRoles) then 5: selectedRoles ← roleSet 6: return selectedRoles Algorithm 3 GETRESOURCECOST(Roles) : (T ∪ M) → 1: M ← (Roles ∩ M) 2: cost ← 0 3: for resource in R do 4: maxResourceUsage ← 0 5: for method in M do 6: if ρ(method, resource) > maxResourceUsage then 7: max ← ρ(method, resource) 8: cost ← cost + [C(resource) × maxResourceUsage] 9: return cost
One of the key questions that the agent doing the spawning needs to answer is - which of its local-roles should it assign to the newly spawned agent and which of its local roles should it keep to itself? The onus of answering this question falls on the FINDROLESFORSPAWNEDAGENT() function, shown in Algorithm 2 above. This function takes the set of local roles that are the responsibility of the spawning agent and returns a subset of those roles for allocation to the newly spawned agent. This subset is selected based on the results of a cost function as is evident from line 4 of the algorithm.
Since the use of different cost functions will result in different organizational structures and since we have no a priori reason to believe that one cost function will out-perform the other, we evaluated the performance of three different cost functions based on the following three different heuristics: Algorithm 4 GETEXPECTEDDURATION(Roles) : (T ∪ M) → N+ 1: M ← (Roles ∩ M) 2: exptDuration ← 0 3: for [outcome =< (q, c, d), outcomeProb >] in M do 4: exptOutcomeDuration ← 0 5: for (n,p) in d do 6: exptOutcomeDuration ← n × p 7: exptDuration ← exptDuration + [exptOutcomeDuration × outcomeProb] 8: return exptDuration Allocating top-most roles first: This heuristic always breaks up at the top-most nodes first. That is, if the nodes of a task structure were numbered, starting from the root, in a breadth-first fashion, then this heuristic would select the local-role of the spawning agent that had the lowest number and breakup that node (by allocating one of its subtasks to the newly spawned agent). We selected this heuristic because (a) it is the simplest to implement, (b) fastest to run (the role allocation can be done in constant time without the need of a search through the task structure) and (c) it makes sense from a human-organizational perspective as this heuristic corresponds to dividing an organization along functional lines.
Minimizing total resources: This heuristic attempts to minimize the total cost of the resources needed by the agents in the organization to execute their roles. If R be the local roles of the spawning agent and R be the subset of roles being evaluated for allocation to the newly spawned agent, the cost function for this heuristic is given by: COST(R, R ) ← GETRESOURCECOST(R − R )+GETRESOURCECOST(R ) Balancing execution time: This heuristic attempts to allocate roles in a way that tries to ensure that each agent has an equal amount of work to do. For each potential role allocation, this heuristic works by calculating the absolute value of the difference between the expected duration of its own roles after spawning and the expected duration of the roles of the newly spawned agent.
If this difference is close to zero, then the both the agents have roughly the same amount of work to do. Formally, if R be the local roles of the spawning agent and R be the subset of roles being evaluated for allocation to the newly spawned agent, then the cost function for this heuristic is given by: COST(R, R ) ← |GETEXPECTEDDURATION(R−R )−GETEXPECTEDDURATION(R )| To evaluate these heuristics, we ran a series of experiments that tested the performance of the resultant organization on randomly generated task structures. The results are given in Section 6.
As organizational change is expensive (requiring clock cycles, allocation/deallocation of resources, etc.) we want a stable organizational structure that is suited to the task and environmental conditions at hand. Hence, we wish to change the organizational structure only if the task structure and/or environmental conditions change. Also to allow temporary changes to the environmental conditions to be overlooked, we want the probability of an organizational change to be inversely proportional to the time since the last organizational change. If this time is relatively short, the agents are still adjusting to the changes in the environment - hence the probability of an agent initiating an organizational change should be high. Similarly, if the time since the last organizational change is relatively large, we wish to have a low probability of organizational change.
To allow this variation in probability of organizational change, we use simulated annealing to determine the probability of keepThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1231 ing an existing organizational structure. This probability is calculated using the annealing formula: p = e− ΔE kT where ΔE is the amount of overload/underload, T is the time since the last organizational change and k is a constant. The mechanism of computing ΔE is different for agent spawning than for agent composition and is described below. From this formula, if T is large, p, or the probability of keeping the existing organizational structure is large.
Note that the value of p is capped at a certain threshold in order to prevent the organization from being too sluggish in its reaction to environmental change.
To compute if agent spawning is necessary, we use the annealing equation with ΔE = 1 α∗Slack where α is a constant and Slack is the difference between the total time available for completion of the outstanding tasks and the sum of the expected time required for completion of each task on the task queue. Also, if the amount of Slack is negative, immediate agent spawning will occur without use of the annealing equation.
To calculate if agent composition is necessary, we again use the simulated annealing equation. However, in this case, ΔE = β ∗ Idle Time, where β is a constant and Idle Time is the amount of time for which the agent was idle. If the agent has been sitting idle for a long period of time, ΔE is large, which implies that p, the probability of keeping the existing organizational structure, is low.
There are two approaches commonly used to achieve robustness in multiagent systems:
domain agents in order to allow the replicas to take over should the original agents fail; and
monitoring agents (called Sentinel Agents) in order to detect agent failure and dynamically startup new agents in lieu of the failed ones.
The advantage of the survivalist approach is that recovery is relatively fast, since the replicas are pre-existing in the organization and can take over as soon as a failure is detected. The advantages of the citizen approach are that it requires fewer resources, little modification to the existing organizational structure and coordination protocol and is simpler to implement.
Both of these approaches can be applied to achieve robustness in our OSD agents and it is not clear which approach would be better.
Rather a thorough empirical evaluation of both approaches would be required. In this paper, we present the citizen approach as it has been shown by [7], to have a better performance than the survivalist approach in the Contract Net protocol, and leave the presentation and evaluation of the survivalist approach to a future paper.
To implement the citizen approach, we designed special monitoring agents, that periodically poll the domain agents by sending them are you alive messages that the agents must respond to. If an agent fails, it will not respond to such messages - the monitoring agents can then create a new agent and delegate the responsibilities of the dead agent to the new agent.
This delegation of responsibilities is non-trivial as the monitoring agents do not have access to the internal state of the domain agents, which is itself composed of two components - the organizational knowledge and the task information. The former consists of the information about the local and managerial roles of the agent while the latter is composed of the methods that are being scheduled and executed and the tasks that have been delegated to other agents. This state information can only be deduced by monitoring and recording the messages being sent and received by the domain agents. For example, in order to deduce the organizational knowledge, the monitoring agents need to keep a track of the spawn and compose messages sent by the agents in order to trigger the spawning and composition operations respectively. The deduction process is particularly complicated in the case of the task information as the monitoring agents do not have access to the private schedules of the domain agents. The details are beyond the scope of this paper.
To evaluate our approach, we ran a series of experiments that simulated the operation of both the OSD agents and the Contract Net agents on various task structures with varied arrival rates and deadlines. At the start of each experiment, a random TÆMS task structure was generated with a specified depth and branching factor. During the course of the experiment, a series of task instances (problems) arrive at the organization and must be completed by the agents before their specified deadlines.
To directly compare the OSD approach with the Contract Net approach, each experiment was repeated several times - using OSD agents on the first run and a different number of Contract Net agents on each subsequent run. We were careful to use the same task structure, task arrival times, task deadlines and random numbers for each of these trials.
We divided the experiments into two groups: experiments in which the environment was static (fixed task arrival rates and deadlines) and experiments in which the environment was dynamic (varying arrival rates and/or deadlines).
The two graphs in Figure 1, show the average performance of the OSD organization against the Contract Net organizations with 8, 10, 12 and 14 agents. The results shown are the averages of running
with a fixed task arrival time of 15 cycles and a deadline window of
rate - the task arrival rate was changed from 15 cycles to 30 cycles and back to 15 cycles after every 20 tasks. In all the experiments, the task structures were randomly generated with a maximum depth of 4 and a maximum branching factor of 3. The runtime of all the experiments was 2500 cycles.
We tested several hypotheses relating to the comparative performance of our OSD approach using the Wilcoxon Matched-Pair Signed-Rank tests. Matched-Pair signifies that we are comparing the performance of each system on precisely the same randomized task set within each separate experiment. The tested hypothesis are: The OSD organization requires fewer agents to complete an equal or larger number of tasks when compared to the Contract Net organization: To test this hypothesis, we tested the stronger null hypothesis that states that the contract net agents complete more tasks. This null hypothesis is rejected for all contract net organizations with less than 14 agents (static: p < 0.0003; dynamic: p < 0.03). For large contract net organizations, the number of tasks completed is statistically equivalent to the number completed by the OSD agents, however the number of agents used by the OSD organization is smaller: 9.59 agents (in the static case) and
.
Thus the original hypothesis, that OSD requires fewer agents to 3 These values should not be construed as an indication of the scalability of our approach. We have tested our approach on organizations with more than 300 agents, which is significantly greater than the number of agents needed for the kind of applications that we have in mind (i.e. web service choreography, efficient dynamic use of grid computing, distributed information gathering, etc.).
Figure 1: Graph comparing the average performance of the OSD organization with the Contract Net organizations (with 8, 10, 12 and 14 agents). The error bars show the standard deviations. complete an equal or larger number of tasks, is upheld.
The OSD organizations achieve an equal or greater average quality than the Contract Net organizations: The null hypothesis is that the Contract Net agents achieve a greater average quality.
We can reject the null hypothesis for contract net organizations with less than 12 agents (static: p < 0.01; dynamic: p < 0.05). For larger contract net organizations, the average quality is statistically equivalent to that achieved by OSD.
The OSD agents have a lower average response time as compared to the Contract Net agents: The null hypothesis that OSD has the same or higher response time is rejected for all contract net organizations (static: p < 0.0002; dynamic: p < 0.0004).
The OSD agents send less messages than the Contract Net Agents: The null hypothesis that OSD sends the same or more messages is rejected for all contract net organizations (p < .0003 in all cases except 8 contract net agents in a static environment where p < 0.02) Hence, as demonstrated by the above tests, our agents perform better than the contract net agents as they complete a larger number of tasks, achieve a greater quality and also have a lower response time and communication overhead. These results make intuitive sense given our goals for the OSD approach. We expected the OSD organizations to have a faster average response time and to send less messages because the agents in the OSD organization are not wasting time and messages sending bid requests and replying to bids. The quality gained on the tasks is directly dependent on the Criteria/Heuristic BET TF MR Rand Number of Agents 572 567 100 139 No-Org-Changes 641 51 5 177 Total-Messages-Sent 586 499 13 11 Resource-Cost 346 418 337 66 Tasks-Completed 427 560 154 166 Average-Quality 367 492 298 339 Average-Response-Time 356 321 370 283 Average-Runtime 543 323 74 116 Average-Turnaround-Time 560 314 74 126 Table 1: The number of times that each heuristic performed the best or statistically equivalent to the best for each of the performance criteria. Heuristic Key: BET is Balancing Execution Time, TF is Topmost First, MR is Minimizing Resources and Rand is a random allocation strategy, in which every TÆMS node has a uniform probability of being selected for allocation. number of tasks completed, hence the more the number of tasks completed, the greater average quality. The results of testing the first hypothesis were slightly more surprising. It appears that due to the inherent inefficiency of the contract net protocol in bidding for each and every task instance, a greater number of agents are needed to complete an equal number of tasks.
Next, we evaluated the performance of the three heuristics for allocating tasks. Some preliminary experiments (that are not reported here due to space constraints) demonstrated the lack of a clear winner amongst the three heuristics for most of the performance criteria that we evaluated. We suspected this to be the case because different heuristics are better for different task structures and environmental conditions, and since each experiment starts with a different random task structure, we couldn"t find one allocation strategy that always dominated the other for all the performance criteria.
To determine which heuristic performs the best, given a set of task structures, environmental conditions and performance criteria, we performed a series of experiments that were controlled using the following five variables: • The depth of the task structure was varied from 3 to 5. • The branching factor was varied from 3 to 5. • The probability of any given task node having a MIN CAF was varied from 0.0 to 1.0 in increments of 0.2. The probability of any node having a SUM CAF was in turn modified to ensure that the probabilities add up to 14 . • The arrival rate: from 10 to 40 cycles in increments of 10. • The deadline slack: from 5 to 15 in increments of 5.
Each experiment was repeated 20 times, with a new task structure being generated each time - these 20 experiments formed an experimental set. Hence, all the experiments in an experimental set had the same values for the exogenous variables that were used to control the experiment. Note that a static environment was used in each of these experiments, as we wanted to see the performance of the arrival rate and deadline slack on each of the three heuristics.
Also the results of any experiment in which the OSD organization consisted of a single agent ware culled from the results. Similarly, 4 Since our preliminary analysis led is to believe that the number of MAX and EXACTLY ONE CAFs in a task structure have a minimal effect on the performance of the allocation strategies being evaluated, we set the probabilities of the MAX and EXACTLY ONE CAFs to 0 in order to reduce the combinatorial explosion of the full factorial experimental design.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1233 experiments in which the generated task structures were unsatisfiable (given the deadline constraints), were removed from the final results. If any experimental set had more than 15 experiments thus removed, the whole set was ignored for performing the evaluation.
The final evaluation was done on 673 experimental sets.
We tested the potential of these three heuristics on the following performance criteria:
as the total quality accrued during the experimental run divided by the sum of the number of tasks completed and the number of tasks failed.
time of a task is defined as the difference between the time at which any agent in the organization starts working on the task (the start time) and the time at which the task was generated (the generation time). Hence, the response time is equivalent to the wait time. For tasks that are never attempted/started, the response time is set at final runtime minus the generation time.
organization. This time is defined as the difference between the time at which the task completed or failed and the start time. For tasks that were never stated, this time is set to zero.
time and runtime of a task.
Except for the number of tasks completed and the average quality accrued, lower values for the various performance criteria indicate better performance. Again we ran the Wilcoxon Matched-Pair Signed-Rank tests on the experiments in each of the experimental sets. The null hypothesis in each case was that there is no difference between the pair of heuristics for the performance criteria under consideration. We were interested in the cases in which we could reject the null hypothesis with 95% confidence (p < 0.05).
We noted the number of times that a heuristic performed the best or was in a group that performed statistically better than the rest.
These counts are given in Tables 1 and 2.
The number of experimental sets in which each heuristic performed the best or statistically equivalent to the best is shown in Table 1. The breakup of these numbers into (1) the number of times that each heuristic performed better than all the other heuristics and (2) the number of times each heuristic was statistically equivalent to another group of heuristics, all of which performed the best, is shown in Table 2. Both of these tables allow us to glean important information about the performance of the three heuristics.
Particularly interesting were the following results: • Whereas Balancing Execution Time (BET) used the lowest number of agents in largest number of experimental sets (572), in most of these cases (337 experimental sets) it was statistically equivalent to Topmost First (TF). When these two heuristics didn"t perform equally, there was an almost even split between the number of experimental sets in which one outperformed the other.
We believe this was the case because BET always bifurcates the agents into two agents that have a more or less equal task load. This often results in organizations that have an even Figure 2: Graph demonstrating the robustness of the citizen approach. The baseline shows the number of tasks completed in the absence of any failure. number of agents - none of which are small5 enough to combine into a larger agent. With TF, on the other hand, a large agent can successively spawn off smaller agents until it and the spawned agents are small enough to complete their tasks before the deadlines - this often results in organizations with an odd number of agents that is less than those used by BET. • As expected, BET achieved the lowest number of organizational changes in the largest number of experimental sets. In fact, it was over ten times as good as its second best competitor (TF). This shows that if the agents are conscientious in their initial task allocation, there is a lesser need for organizational change later on, especially for static environments. • A particularly interesting, yet easily explainable, result was that of the average response time. We found that the Minimizing Resources (MR) heuristic performed the best when it came to minimizing the average response time! This can be explained by the fact the MR heuristic is extremely greedy and prefers to spawn off small agents that have a tiny resource footprint (so as to minimize the total increase in the resource cost to the organization at the time of spawning).
Whereas most of these small agents might compose with other agents over time, the presence of a single small agent is sufficient to reduce the response time.
In fact the MR heuristic is not the most effective heuristic when it comes to minimizing the resource-cost of the organization - in fact, it only outperforms a random task/resource allocation. We believe this is in part due to the greedy nature of this heuristic and in part because of the fact that all spawning and composition operations only use local information. We believe that using some non-local information about the resource allocation might help in making better decisions, something that we plan to look at in the future.
Finally we evaluated the performance of the citizens approach to robustness as applied to our OSD mechanism (Figure 2). As expected, as the probability of failure increases, the number of agents failing during a run also increases. This results in a slight decrease in the number of tasks completed, which can be explained by the fact that whenever an agent fails, its looses whatever work it was doing at the time. The newly created agent that fills in for the failed 5 For this discussion small agents are agents that have a low expected duration for their local roles (as calculated by Algorithm 4).
Criteria/Heuristic BET TF MR Rand BET+TF BET+Rand MR+Rand TF+MR BET+TF+MR All Number of Agents 94 88 3 7 337 2 0 0 12 85 No-Org-Changes 480 0 0 29 16 113 0 0 0 5 Total-Messages-Sent 170 85 0 2 399 1 0 0 7 5 Resource-Cost 26 100 170 42 167 0 7 6 128 15 Tasks-Completed 77 197 4 28 184 1 3 9 36 99 Average-Quality 38 147 26 104 76 0 11 11 34 208 Average-Response-Time 104 74 162 43 31 20 16 8 7 169 Average-Runtime 322 110 0 12 121 13 1 1 1 69 Average-Turnaround-Time 318 94 1 11 125 26 1 0 7 64 Table 2: Table showing the number of times that each individual heuristic performed the best and the number of times that a certain group of statistically equivalent heuristics performed the best. Only the more interesting heuristic groupings are shown. All shows the number of experimental sets in which there was no statistical difference between the three heuristics and a random allocation strategy one must redo the work, thus wasting precious time which might not be available close to a deadline.
As a part of our future research, we wish to, firstly, evaluate the survivalist approach to robustness. The survivalist approach might actually be better than the citizen approach for higher probabilities of agent failure, as the replicated agents may be processing the task structures in parallel and can take over the moment the original agents fail - thus saving time around tight deadlines. Also, we strongly believe that the optimal organizational structure may vary, depending on the probability of failure and the desired level of robustness. For example, one way of achieving a higher level of robustness in the survivalist approach, given a large numbers of agent failures, would be to relax the task deadlines. However, such a relaxation would result in the system using fewer agents in order to conserve resources, which in turn would have a detrimental effect on the robustness. Therefore, towards this end, we have begun exploring the robustness properties of task structures and the ways in which the organizational design can be modified to take such properties into account.
In this paper, we have presented a run-time approach to organization in which the agents use Organizational Self-Design to come up with a suitable organizational structure. We have also evaluated the performance of the organizations generated by the agents following our approach with the bespoke organization formation that takes place in the Contract Net protocol and have demonstrated that our approach is better than the Contract Net approach as evident by the larger number of tasks completed, larger quality achieved and lower response time. Finally, we tested the performance of three different resource allocation heuristics on various performance metrics and also evaluated the robustness of our approach.
[1] K. S. Barber and C. E. Martin. Dynamic reorganization of decision-making groups. In AGENTS "01, pages 513-520,
New York, NY, USA, 2001. [2] K. M. Carley and L. Gasser. Computational organization theory. In G. Wiess, editor, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence, pages 299-330, MIT Press, 1999. [3] W. Chen and K. S. Decker. The analysis of coordination in an information system application - emergency medical services. In Lecture Notes in Computer Science (LNCS), number 3508, pages 36-51. Springer-Verlag, May 2005. [4] D. Corkill and V. Lesser. The use of meta-level control for coordination in a distributed problem solving network.
Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pages 748-756, August 1983. [5] K. S. Decker. Environment centered analysis and design of coordination mechanisms. Ph.D. Thesis, Dept. of Comp.
Science, University of Massachusetts, Amherst, May 1995. [6] K. S. Decker and J. Li. Coordinating mutually exclusive resources using GPGP. Autonomous Agents and Multi-Agent Systems, 3(2):133-157, 2000. [7] C. Dellarocas and M. Klein. An experimental evaluation of domain-independent fault handling services in open multi-agent systems. Proceedings of the International Conference on Multi-Agent Systems (ICMAS-2000), July
[8] V. Dignum, F. Dignum, and L. Sonenberg. Towards Dynamic Reorganization of Agent Societies. In Proceedings of CEAS: Workshop on Coordination in Emergent Agent Societies at ECAI, pages 22-27, Valencia, Spain, September 2004. [9] B. Horling, B. Benyo, and V. Lesser. Using self-diagnosis to adapt organizational structures. In AGENTS "01, pages 529-536, New York, NY, USA, 2001. ACM Press. [10] T. Ishida, L. Gasser, and M. Yokoo. Organization self-design of distributed production systems. IEEE Transactions on Knowledge and Data Engineering, 4(2):123-134, 1992. [11] V. R. Lesser et. al. Evolution of the gpgp/tæms domain-independent coordination framework. Autonomous Agents and Multi-Agent Systems, 9(1-2):87-143, 2004. [12] O. Marin, P. Sens, J. Briot, and Z. Guessoum. Towards adaptive fault tolerance for distributed multi-agent systems.
Proceedings of ERSADS 2001, May 2001. [13] O. Shehory, K. Sycara, et. al. Agent cloning: an approach to agent mobility and resource allocation. IEEE Communications Magazine, 36(7):58-67, 1998. [14] Y. So and E. Durfee. An organizational self-design model for organizational change. In AAAI-93 Workshop on AI and Theories of Groups and Organizations, pages 8-15,
Washington, D.C., July 1993. [15] T. Wagner. Coordination decision support assistants (coordinators). Technical Report 04-29, BAA, 2004. [16] T. Wagner and V. Lesser. Design-to-criteria scheduling: Real-time agent control. Proc. of AAAI 2000 Spring Symposium on Real-Time Autonomous Systems, 89-96.

Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments. They generalize POMDPs [13] to multiagent settings by including the other agents" computable models in the state space along with the states of the physical environment. The models encompass all information influencing the agents" behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11]. I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-maker"s perspective in the interaction.
In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs. I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.
I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8]. These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agent"s actions and chance nodes capturing the agent"s private information. MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure. NIDs extend MAIDs to include agents" uncertainty over the game being played and over models of the other agents. Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent. Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently. However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games. Matters are more complex when we consider interactions that are extended over time, where predictions about others" future actions must be made using models that change as the agents act and observe. I-DIDs address this gap by allowing the representation of other agents" models as the values of a special model node. Both, other agents" models and the original agent"s beliefs over these models are updated over time using special-purpose implementations.
In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID. Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently. Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links. In the previous representation of the I-DID, the update of the agent"s belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time. We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.
The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs. We show how IDIDs may be used to model an agent"s uncertainty over others" models, that may themselves be I-DIDs. Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others" models. Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.
IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents" models as part of the state space [9].
Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents" models and their beliefs about others. For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.
A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment. Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj,
OCj . Here, j is Bayes rational and OCj is j"s optimality criterion.
SMj is the set of subintentional models of j. Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.
We give a recursive bottom-up construction of the interactive state space below.
ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .
ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent i"s preferences over its interactive states. Usually only the physical states will matter.
Agent i"s policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i. Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai).
Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes. However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones. First, since the state of the physical environment depends on the actions of both agents, i"s prediction of how the physical state changes has to be made based on its prediction of j"s actions. Second, changes in j"s models have to be included in i"s belief update. Specifically, if j is intentional then an update of j"s beliefs due to its action and observation has to be included. In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief. If j"s model is subintentional, then j"s probable observations are appended to the observation history contained in the model. Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update. For a version of the belief update when j"s model is subintentional, see [9].
If agent j is also modeled as an I-POMDP, then i"s belief update invokes j"s belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke i"s belief update and so on. This recursion in belief nesting bottoms out at the 0th level. At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9].
Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)). Eq. 2 is a basis for value iteration in I-POMDPs.
Agent i"s optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3)
A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes. However, this approach assumes that the agents" actions are controlled using a probability distribution that does not change over time. Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs.
In addition to the usual chance, decision, and utility nodes,
IIDs include a new type of node called the model node. We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon. We note that the probability distribution over the chance node, S, and the model node together represents agent i"s belief over its interactive states. In addition to the model 1 The 0th level model is a POMDP: Other agent"s actions are treated as exogenous events and folded into the T, O, and R functions.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j. The hexagon is the model node (Mj,l−1) whose structure we show in (b). Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ). Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node,
Aj, that represents the distribution over the other agent"s actions given its model. In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.
The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2. Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional. Because the model node contains the alternative models of the other agent as its values, its representation is not trivial. In particular, some of the models within the node are I-IDs that when solved generate the agent"s optimal policy in their decision nodes. Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.
Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b). The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.
The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj. Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent i"s beliefs. The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj]. The values of Mod[Mj] denote the different models of j. In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1. The distribution over the node, Mod[Mj], is the agent i"s belief over the models of j given a physical state. For more agents, we will have as many model nodes as there are agents. Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.
In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them. In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes. This allows I-IDs to be represented and implemented using conventional application tools that target IDs.
Note that we may view the level l I-ID as a NID. Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2). If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.
Note that within the I-IDs (or IDs) at each level, there is only a single decision node. Thus, our NID does not contain any MAIDs.
Figure 2: A level l I-ID represented as a NID. The probabilities assigned to the blocks of the NID are i"s beliefs over j"s models conditioned on a physical state.
The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively. We start by solving the level 0 models, which, if intentional, are traditional IDs. Their solutions provide probability distributions over the other agents" actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID. The mapping from the level 0 models" decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.
Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c). During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj]. As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent i"s belief over the models of j conditioned on the physical state. The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j. Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18]. This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief. Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agent"s current belief is known.
DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps. Just as DIDs are structured graphical representations of POMDPs,
I-DIDs are the graphical online analogs for finitely nested I-POMDPs.
I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents.
We depict a general two time-slice I-DID in Fig. 3(a). In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a). We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.
The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1. Recall from Section 2 that an agent"s intentional model includes its belief. Because the agents act and receive observations, their models are updated to reflect their changed beliefs. Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models. Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models. Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model. These steps are a part of agent i"s belief update formalized using Eq. 1.
In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID. If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ). These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation. The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously. Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed. The probability that j"s updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node. In order to obtain the probability of j"s possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ]. Because the probability of j"s observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ]. Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.
Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1. Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices. In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.
Chance nodes and dependency links that not in bold are standard, usually found in DIDs.
Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents j"s observation at time t + 1.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links. We note that the possible set of models of the other agent j grows exponentially with the number of time steps. For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node.
Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively. For the purpose of illustration, let l=1 and T=2. The solution method uses the standard look-ahead technique, projecting the agent"s action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step. Because agent i has a belief over j"s models as well, the lookahead includes finding out the possible models that j could have in the future. Consequently, each of j"s subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions. These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b). We note the recursive nature of this solution: in solving agent i"s level 1 I-DID, j"s level 0 DIDs must be solved. If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.
We briefly outline the recursive algorithm for solving agent i"s Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase
Populate Mt+1 j,l−1
j in Range(Mt j,l−1) do
that represents mt j and the horizon, T − t + 1
OPT(mt j), to a chance node Aj
j) do
j) do
j ← SE(bt j, aj, oj)
j ← New I-ID (or ID) with bt+1 j as the initial belief
j,l−1) ∪ ← {mt+1 j }
j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b))
slice and the dependency links between them
Look-Ahead Phase
the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5. We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node. We particularly focus on establishing and populating the model nodes (lines 3-11).
Note that Range(·) returns the values (lower level models) of the random variable given as input (model node). In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs. Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.
As we mentioned previously, the 0-th level models are the traditional DIDs. Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1. Given probability distributions over other agent"s actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models. Assume that the number of models considered at each level is bound by a number, M.
Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs.
To illustrate the usefulness of I-DIDs, we apply them to three problem domains. We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it.
Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].
The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L). In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agent"s opening one of the doors. When any door is opened, the tiger persists in its original location with a probability of 95%. Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%. Agent j, on the other hand, hears growls with a reliability of 95%. Thus, the setting is such that agent i hears agent j opening doors more reliably than the tiger"s growls. This suggests that i could use j"s actions as an indication of the location of the tiger, as we discuss below. Each agent"s preferences are as in the single agent game discussed in [13]. The transition, observation, and reward functions are shown in [16].
A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions. In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship. In particular, we analyze the situational and epistemological conditions sufficient for their emergence. The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the other"s actions in order to maximize its payoffs.
Let us consider a particular setting of the tiger problem in which agent i believes that j"s preferences are aligned with its own - both of them just want to get the gold - and j"s hearing is more reliable in comparison to itself. As an example, suppose that j, on listening can discern the tiger"s location 95% of the times compared to i"s 65% accuracy. Additionally, agent i does not have any initial information about the tiger"s location. In other words, i"s single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger. In addition, i considers two models of j, which differ in j"s flat level 0 initial beliefs. This is represented in the level 1 I-ID shown in Fig. 6(a). According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other
Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)). Agent i is undecided on these two models of j. If we vary i"s hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior. If i"s probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow j"s actions: i opens the same door that j opened previously iff i"s own assessment of the tiger"s location confirms j"s pick. If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).
Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem. Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.
We observed that a single level of belief nesting - beliefs about the other"s models - was sufficient for followership to emerge in the tiger problem. However, the epistemological requirements for the emergence of leadership are more complex. For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i. As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow j"s actions over time. Agent j emerges as a leader if it believes that i will follow it, which implies that j"s belief must be nested two levels deep to enable it to recognize its leadership role. Realizing that i will follow presents j with an opportunity to influence i"s actions in the benefit of the collective good or its self-interest alone. For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original. If j alone selects the correct door, it gets the payoff of 10. On the other hand, if both agents pick the wrong door, their penalties are cut in half. In this setting, it is in both j"s best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader. However, consider a slightly different problem in which j gains from i"s loss and is penalized if i gains. Specifically, let i"s payoff be subtracted from j"s, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then i"s loss of 100 becomes j"s gain. Agent j believes that i incorrectly thinks that j"s preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is. Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level
normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a). The policy demonstrates that i will blindly follow j"s actions. Since the tiger persists in its original location with a probability of 0.95, i will select the same door again. If j begins the game with a 99% probability that the tiger is on the right, solving j"s I-DID nested two levels deep, results in the policy shown in Fig. 8(b). Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL. Agent j"s intention is to deceive i who, it believes, will follow j"s actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.
Figure 8: Emergence of deception between agents in the tiger problem. Behaviors of interest are in bold. * denotes as before. (a) Agent i"s policy demonstrating that it will blindly follow j"s actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i.
Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves. Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot. However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes. Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others" contributions. However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions. The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4]. Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting. These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.
For simplicity, we assume that the game is played between M =
amount of resources. While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions. Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute. The latter action is deThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D). We assume that the actions are not observable to others. The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return. We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain. Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.
In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment. Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.
For simplicity, we assume that the cost of punishing is same for both the agents. The one-shot PG game with punishment is shown in Table. 1. Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action. If P < XT − ciXT , then defection is the dominating action for both. If P = XT − ciXT , then the game is not dominance-solvable.
Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).
We formulate a sequential version of the PG problem with punishment from the perspective of agent i. Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents. Each agent may contribute a fixed amount, xc, or defect. An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot. Notice that the observations are also indirectly indicative of agent j"s actions because the state of the public pot is influenced by them. The amount of resources in agent i"s private pot, is perfectly observable to i. The payoffs are analogous to Table. 1.
Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)). Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect. Let xc = 1 and the level 0 agent be punished half the times it defects. With one action remaining, both types of agents choose to contribute to avoid being punished. With two actions to go, the altruistic type chooses to contribute, while the other defects. This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids. Because cj for the non-altruistic type is less, it prefers not to contribute. With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects. For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.
We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level
a probability 1 that j is altruistic, i chooses to contribute for each of the three steps. This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type. However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1. These results demonstrate that the behavior of our altruistic type resembles that found experimentally.
The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic. We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection. The reciprocal type"s marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other. We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full. For this prior belief, i chooses to defect. On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)). This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from j"s action to contribute. Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step. Agent i therefore chooses to contribute to reciprocate j"s action. An analogous reasoning leads i to defect when it observes a meager pot.
With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.
Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic).
Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2]. Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck. While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands. To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit. Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot. During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the
H pile, indicating that the card had a rank greater than or equal to
the probability of receiving a low card in exchange is now reduced.
We show the level 1 I-ID for the simplified two-player Poker in Fig. 11. We considered two models (personality types) of agent j.
The conservative type believes that it is likely that its opponent has a high numbered card in its hand. On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card. Thus, the two types differ in their beliefs over their opponent"s hand. In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution. With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand. This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange. The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.
Figure 11: (a) Level 1 I-ID of agent i. The observation reveals information about j"s hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).
The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in j"s hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. i"s own hand contains the card numbered 8. The agent starts by keeping its card. On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card. If i observes that j discarded its card into the L or H pile, i believes that j is aggressive. On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange. Because the probability of receiving a low card is high now, i chooses to keep its card. On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card. In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff. This is partly due to the fact that an observation of, say, L resets the agent i"s previous time step beliefs over j"s hand to the low numbered cards only.
We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.
Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent i"s three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs. I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.
I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs. We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.
Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work. The first author would like to acknowledge the support of a UGARF grant.

Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are emerging as a popular approach for modeling sequential decision making in teams operating under uncertainty [9, 4, 1, 2, 13]. The uncertainty arises on account of nondeterminism in the outcomes of actions and because the world state may only be partially (or incorrectly) observable. Unfortunately, as shown by Bernstein et al. [3], the problem of finding the optimal joint policy for general distributed POMDPs is NEXP-Complete.
Researchers have attempted two different types of approaches towards solving these models. The first category consists of highly efficient approximate techniques, that may not reach globally optimal solutions [2, 9, 11]. The key problem with these techniques has been their inability to provide any guarantees on the quality of the solution. In contrast, the second less popular category of approaches has focused on a global optimal result [13, 5, 10]. Though these approaches obtain optimal solutions, they typically consider only two agents. Furthermore, they fail to exploit structure in the interactions of the agents and hence are severely hampered with respect to scalability when considering more than two agents.
To address these problems with the existing approaches, we propose approximate techniques that provide guarantees on the quality of the solution while focussing on a network of more than two agents. We first propose the basic SPIDER (Search for Policies In Distributed EnviRonments) algorithm. There are two key novel features in SPIDER: (i) it is a branch and bound heuristic search technique that uses a MDP-based heuristic function to search for an optimal joint policy; (ii) it exploits network structure of agents by organizing agents into a Depth First Search (DFS) pseudo tree and takes advantage of the independence in the different branches of the DFS tree. We then provide three enhancements to improve the efficiency of the basic SPIDER algorithm while providing guarantees on the quality of the solution. The first enhancement uses abstractions for speedup, but does not sacrifice solution quality. In particular, it initially performs branch and bound search on abstract policies and then extends to complete policies. The second enhancement obtains speedups by sacrificing solution quality, but within an input parameter that provides the tolerable expected value difference from the optimal solution. The third enhancement is again based on bounding the search for efficiency, however with a tolerance parameter that is provided as a percentage of optimal.
We experimented with the sensor network domain presented in Nair et al. [10], a domain representative of an important class of problems with networks of agents working in uncertain environments. In our experiments, we illustrate that SPIDER dominates an existing global optimal approach called GOA [10], the only known global optimal algorithm with demonstrated experimental results for more than two agents. Furthermore, we demonstrate that abstraction improves the performance of SPIDER significantly (while providing optimal solutions). We finally demonstrate a key feature of SPIDER: by utilizing the approximation enhancements it enables principled tradeoffs in run-time versus solution quality. 822 978-81-904262-7-5 (RPS) c 2007 IFAAMAS
Distributed sensor networks are a large, important class of domains that motivate our work. This paper focuses on a set of target tracking problems that arise in certain types of sensor networks [6] first introduced in [10]. Figure 1 shows a specific problem instance within this type consisting of three sensors. Here, each sensor node can scan in one of four directions: North, South, East or West (see Figure 1). To track a target and obtain associated reward, two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously. In Figure 1, to track a target in Loc11, sensor1 needs to scan ‘East" and sensor2 needs to scan ‘West" simultaneously. Thus, sensors have to act in a coordinated fashion.
We assume that there are two independent targets and that each target"s movement is uncertain and unaffected by the sensor agents.
Based on the area it is scanning, each sensor receives observations that can have false positives and false negatives. The sensors" observations and transitions are independent of each other"s actions e.g.the observations that sensor1 receives are independent of sensor2"s actions. Each agent incurs a cost for scanning whether the target is present or not, but no cost if it turns off. Given the sensors" observational uncertainty, the targets" uncertain transitions and the distributed nature of the sensor nodes, these sensor nets provide a useful domains for applying distributed POMDP models.
Figure 1: A 3-chain sensor configuration
The ND-POMDP model was introduced in [10], motivated by domains such as the sensor networks introduced in Section 2. It is defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi × Su is the set of world states. Si refers to the set of local states of agent i and Su is the set of unaffectable states. Unaffectable state refers to that part of the world state that cannot be affected by the agents" actions, e.g. environmental factors like target locations that no agent can control. A = ×1≤i≤nAi is the set of joint actions, where Ai is the set of action for agent i.
ND-POMDP assumes transition independence, where the transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n Pi(si, su, ai, si), where a = a1, . . . , an is the joint action performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is the resulting state.
Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is the set of observations for agents i. Observational independence is assumed in ND-POMDPs i.e., the joint observation function is defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s = s1, . . . , sn, su is the world state that results from the agents performing a = a1, . . . , an in the previous state, and ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This implies that each agent"s observation depends only on the unaffectable state, its local action and on its resulting local state.
The reward function, R, is defined as R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l could refer to any sub-group of agents and r = |l|. Based on the reward function, an interaction hypergraph is constructed. A hyper-link, l, exists between a subset of agents for all Rl that comprise R. The interaction hypergraph is defined as G = (Ag, E), where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R} are the edges.
The initial belief state (distribution over the initial state), b, is defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer to the distribution over initial unaffectable state and agent i"s initial belief state, respectively. The goal in ND-POMDP is to compute the joint policy π = π1, . . . , πn that maximizes team"s expected reward over a finite horizon T starting from the belief state b.
An ND-POMDP is similar to an n-ary Distributed Constraint Optimization Problem (DCOP)[8, 12] where the variable at each node represents the policy selected by an individual agent, πi with the domain of the variable being the set of all local policies, Πi.
The reward component Rl where |l| = 1 can be thought of as a local constraint while the reward component Rl where l > 1 corresponds to a non-local constraint in the constraint graph.
In previous work, GOA has been defined as a global optimal algorithm for ND-POMDPs [10]. We will use GOA in our experimental comparisons, since GOA is a state-of-the-art global optimal algorithm, and in fact the only one with experimental results available for networks of more than two agents. GOA borrows from a global optimal DCOP algorithm called DPOP[12]. GOA"s message passing follows that of DPOP. The first phase is the UTIL propagation, where the utility messages, in this case values of policies, are passed up from the leaves to the root. Value for a policy at an agent is defined as the sum of best response values from its children and the joint policy reward associated with the parent policy.
Thus, given a policy for a parent node, GOA requires an agent to iterate through all its policies, finding the best response policy and returning the value to the parent - while at the parent node, to find the best policy, an agent requires its children to return their best responses to each of its policies. This UTIL propagation process is repeated at each level in the tree, until the root exhausts all its policies. In the second phase of VALUE propagation, where the optimal policies are passed down from the root till the leaves.
GOA takes advantage of the local interactions in the interaction graph, by pruning out unnecessary joint policy evaluations (associated with nodes not connected directly in the tree). Since the interaction graph captures all the reward interactions among agents and as this algorithm iterates through all the relevant joint policy evaluations, this algorithm yields a globally optimal solution.
As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward. The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies. The key idea in SPIDER is to avoid computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.
Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure. Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children. We employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed. MCN heuristic tries to place agents with more number of constraints at the top of the tree. This tree governs how the search for the optimal joint polThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER. The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.
SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs. In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).
Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root. πroot+ ⇒ joint policy of all agents. πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i). πi ⇒ policy of the ith agent. ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e. πi− . ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child. v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− . v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− . vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.
Figure 2: Execution of SPIDER, an example
SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint policies. Figure 2 shows an example search tree for the SPIDER algorithm, using an example of the three agent chain. Before SPIDER begins its search we create a DFS tree (i.e. pseudo tree) from the three agent chain, with the middle agent as the root of this tree.
SPIDER exploits the structure of this DFS tree while engaging in its search. Note that in our example figure, each agent is assigned a policy with T=2. Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy. Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle. If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.
SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree). Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agent"s policies. Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents). The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER potentially performs two steps:
computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies. An MDP based heuristic is used to compute these upper bounds on the expected values. Detailed description about this MDP heuristic is provided in Section 4.2. All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order. Exploration of these policies (in step 2 below) are performed in this descending order. As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy. The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values.
the best response joint policy πi+,∗ corresponding to fixed ancestor policies of agent i, πi− . This is performed by iterating through all policies of agent i i.e. Πi and summing two quantities for each policy: (i) the best response for all of i"s children (obtained by performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors. Thus, exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ]. The policy with the highest expected value is the best response policy.
Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ]. Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold. A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold. This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.
On the other hand, when considering a leaf agent, SPIDER computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− . This is accomplished by computing expected values for each of the policies (corresponding to fixed policies of ancestors) and selecting the highest expected value policy. In Figure 2, SPIDER assigns best response policies to leaf agents at level 3. The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.
These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.
Algorithm 1 provides the pseudo code for SPIDER. This algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i). Lines 3-8 compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i). This best response computation for a non-leaf agent i includes: (a) Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the children for fixed policies of agent i in lines 16-20; and (c) Maintaining
Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.
Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.
Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children. Line 4 computes the expected value obtained from ancestors of the agent (using JOINT-REWARD function), while lines 5-7 compute the heuristic value from the children. The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds.
The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i). The subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP. To reiterate this in terms of the agents in DFS tree interaction structure, we assume full observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .
We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i). Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together. We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability assumption. This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)). Thus, the equation for the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above. While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ .
Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the policies has ended. We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality). The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies. In this paper, we propose two types of abstraction:
defined as a shorter horizon policy. It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.
In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.
For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract policy. This is same as the heuristic computation defined by the GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy). (b) Computing the maximum possible reward that can be accumulated in one time step (using GET-ABS-HEURISTIC()) and multiplying it by the number of time steps to time horizon. This maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.
Sum of (a) and (b) is the heuristic value for a HBA abstract policy.
obtained by not associating actions to certain nodes of the policy tree.
Unlike in HBA, this implies multiple levels of abstraction. This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation ‘TP". These incomplete T=2 policies are abstractions for T=2 complete policies. Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases. For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes. In such cases, the immediate reward is taken as Rmax (maximum reward for any action).
We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS. Algorithm 5 provides the algorithm for this abstraction technique. For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract policy heuristic computations (line 17). The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24). Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30). However, if those conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).
EXTEND-POLICY() function is also responsible for initializing the horizon and absNodes of a policy. absNodes represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them. If πi.absNodes = |Ωi|πi.horizon−1 (i.e. total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1. Otherwise, πi.absNodes is increased by 1. Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes. Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33). Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14).
In this section, we present an approximate enhancement to SPIDER called VAX. The input to this technique is an approximation parameter , which determines the difference from the optimal solution quality. This approximation parameter is used at each agent for pruning out joint policies. The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value. However, the
Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ]. Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.
In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs. However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned. This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e. 239 would have been greater than the heuristic value for that joint policy (238). It can be noted from the example (just discussed) that this kind of pruning can lead to fewer explorations and hence lead to an improvement in the overall run-time performance. However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate optimal solution. A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3.
In this section, we present the second approximation enhancement over SPIDER called PAX. Input to this technique is a parameter, δ that represents the minimum percentage of the optimal solution quality that is desired. Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality. A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ]. Like in VAX, the only difference between PAX and SPIDER/SPIDER-ABS is this pruning condition.
Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234). This type of pruning leads to fewer explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution. Proposition 4 provides the bound on quality loss.
PROPOSITION 1. Heuristic provided using the centralized MDP heuristic is admissible.
Proof. For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.
Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.
Base case: t = T − 1. Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents. Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .
Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.
We now have to prove that the proposition holds for t = η − 1.
We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ . The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.
PROPOSITION 2. SPIDER provides an optimal solution.
Proof. SPIDER examines all possible joint policies given the interaction structure of the agents. The only exception being when a joint policy is pruned based on the heuristic value. Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy. As proved in Proposition 1, the expected value for a joint policy is always an upper bound. Hence when a joint policy is pruned, it cannot be an optimal solution.
PROPOSITION 3. Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof. We prove this proposition using mathematical induction on the depth of the DFS tree.
Base case: depth = 1 (i.e. one node). Best response is computed by iterating through all policies, Πk. A policy,πk is pruned if ˆv[πk, πk− ] < threshold + . Thus the best response policy computed by VAX would be at most away from the optimal best response. Hence the proposition holds for the base case.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d.
We now have to prove that the proposition holds for d + 1.
Without loss of generality, lets assume that the root node of this tree has k children. Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root. Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.
In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ]. However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk . Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ . Hence proved.
PROPOSITION 4. For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.
Proof. We prove this proposition using mathematical induction on the depth of the DFS tree.
Base case: depth = 1 (i.e. one node). Best response is computed by iterating through all policies, Πk. A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold. Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response. Hence the proposition holds for the base case.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d.
We now have to prove that the proposition holds for d + 1.
Without loss of generality, lets assume that the root node of this tree has k children. Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX. With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ]. Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error introduced at the root agent and all the error is introduced in the children. Thus, overall solution quality is at least δ 100 of the optimal solution. Hence proved.
All our experiments were conducted on the sensor network domain from Section 2. The five network configurations employed are shown in Figure 4. Algorithms that we experimented with are GOA, SPIDER, SPIDER-ABS, PAX and VAX. We compare against GOA because it is the only global optimal algorithm that considers more than two agents. We performed two sets of experiments: (i) firstly, we compared the run-time performance of the above algorithms and (ii) secondly, we experimented with PAX and VAX to study the tradeoff between run-time and solution quality.
Experiments were terminated after 10000 seconds1 .
Figure 5(a) provides run-time comparisons between the optimal algorithms GOA, SPIDER, SPIDER-Abs and the approximate algorithms, PAX ( of 30) and VAX(δ of 80). X-axis denotes the 1 Machine specs for all experiments: Intel Xeon 3.6 GHZ processor, 2GB RAM sensor network configuration used, while Y-axis indicates the runtime (on a log-scale). The time horizon of policy computation was
are five bars indicating the time taken by GOA, SPIDER,
SPIDERAbs, PAX and VAX. GOA did not terminate within the time limit for 4-star and 5-star configurations. SPIDER-Abs dominated the SPIDER and GOA for all the configurations. For instance, in the 3chain configuration, SPIDER-ABS provides 230-fold speedup over GOA and 2-fold speedup over SPIDER and for the 4-chain configuration it provides 58-fold speedup over GOA and 2-fold speedup over SPIDER. The two approximation approaches, VAX and PAX provided further improvement in performance over SPIDER-Abs.
For instance, in the 5-star configuration VAX provides a 15-fold speedup and PAX provides a 8-fold speedup over SPIDER-Abs.
Figures 5(b) provides a comparison of the solution quality obtained using the different algorithms for the problems tested in Figure 5(a). X-axis denotes the sensor network configuration while Y-axis indicates the solution quality. Since GOA, SPIDER, and SPIDER-Abs are all global optimal algorithms, the solution quality is the same for all those algorithms. For 5-P configuration, the global optimal algorithms did not terminate within the limit of
bound on the optimal solution quality. With both the approximations, we obtained a solution quality that was close to the optimal solution quality. In 3-chain and 4-star configurations, it is remarkable that both PAX and VAX obtained almost the same actual quality as the global optimal algorithms, despite the approximation parameter and δ. For other configurations as well, the loss in quality was less than 20% of the optimal solution quality.
Figure 5(c) provides the time to solution with PAX (for varying epsilons). X-axis denotes the approximation parameter, δ (percentage to optimal) used, while Y-axis denotes the time taken to compute the solution (on a log-scale). The time horizon for all the configurations was 4. As δ was decreased from 70 to 30, the time to solution decreased drastically. For instance, in the 3-chain case there was a total speedup of 170-fold when the δ was changed from 70 to 30. Interestingly, even with a low δ of 30%, the actual solution quality remained equal to the one obtained at 70%.
Figure 5(d) provides the time to solution for all the configurations with VAX (for varying epsilons). X-axis denotes the approximation parameter, used, while Y-axis denotes the time taken to compute the solution (on a log-scale). The time horizon for all the configurations was 4. As was increased, the time to solution decreased drastically. For instance, in the 4-star case there was a total speedup of 73-fold when the was changed from 60 to 140. Again, the actual solution quality did not change with varying epsilon.
Figure 4: Sensor network configurations
Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4
This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e. easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve runtime performance without sacrificing solution quality; (iv) providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX. These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under uncertainty. Experimental results show orders of magnitude improvement in performance over previous global optimal algorithms.
Researchers have typically employed two types of techniques for solving distributed POMDPs. The first set of techniques compute global optimal solutions. Hansen et al. [5] present an algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs. Szer et al. [13] provide an optimal heuristic search method for solving Decentralized POMDPs. This algorithm is based on the combination of a classical heuristic search algorithm, A∗ and decentralized control theory. The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant computational complexity; (b) Due to MAA*"s inability to exploit interaction structure, it was illustrated only with two agents. However,
SPIDER has been illustrated for networks of agents; and (c) SPIDER explores the joint policy one agent at a time, while MAA* expands it one time step at a time (simultaneously for all the agents).
The second set of techniques seek approximate policies.
EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value, trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic). Nair et al. [9]"s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.
Peshkin et al. [11] and Bernstein et al. [2] are examples of policy search techniques that search for locally optimal policies. Though all the above techniques improve the efficiency of policy computation considerably, they are unable to provide error bounds on the quality of the solution. This aspect of quality bounds differentiates SPIDER from all the above techniques.
Acknowledgements. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No. NBCHD030010. The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government.

The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2]. Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.
Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs). Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research. In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other,
Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses. Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.
To remedy that, locally optimal algorithms have been proposed [12] [4] [5]. In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons. Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains. OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration. However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values. The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval. Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods. This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.
In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP. VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.
Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions. Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.
This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building. In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers. Sections 5 and
OC-DEC-MDP algorithm that our VFP algorithm implements.
Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm
We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome. One example domain is large-scale disaster, like a fire in a skyscraper. Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless. In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.
Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 . General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.
The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B. As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3). Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc. We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.
One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians. If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians. In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan. Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B. Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out.
We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 . Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents. Agents cannot communicate during mission execution. Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø. Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time. Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations. In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system. Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints. For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates. In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints. We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.
For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints. Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline. Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.
Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents. Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W). In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + . In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t.
Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution. Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.
The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a. A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents.
Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used. Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}. This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ . Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11]. The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used. However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ.
Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising. Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods. The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible. At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ]. It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ]. This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration. We call this step a value propagation phase.
Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π . It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods. We call this step a probability propagation phase. If policy π does not improve π, the algorithm terminates.
There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper. First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed. While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t.
Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 . Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.
Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods. As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again. As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences. In the next two sections, we address both of these shortcomings.
The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase. However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase. To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t.
Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.
Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t.
Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.
Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.
We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase
Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods. At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived. Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ]. The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.
Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ]
(1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).
Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6. We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).
Figure 2: Fragment of an MDP of agent Ak. Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).
Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.
It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 . Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).
Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.
Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable. Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .
Furthermore, Vj0,i0 should be non-increasing. Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.
Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 . Let Ak be an agent assigned to the method mi0 .
If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .
Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .
Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.
We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 . In general, the value function propagation scheme starts with sink nodes. It then visits at each time a method m, such that all the methods that m enables have already been marked as visited. The value function propagation phase terminates when all the source methods have been marked as visited.
In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W.
One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically.
Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified. Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration. We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived. Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase. If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.
Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .
We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 . The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default. We then visit at each time a method m such that all the methods that enable The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited. The probability function propagation phase terminates when all the sink methods have been marked as visited.
Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .
Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1. These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.
Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration.
So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation. In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation. However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.
When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t). If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does. As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ). Since Pi values are in range [0, 1] and Vi values are in range [0,
P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P . We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1. Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively. The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺|  V + ((1 + P )|C≺| − 1) P mi∈M ri  .
PROOF. In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.
Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.
Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1. We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ). Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.
Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺|  V + ((1 + P )|C≺| − 1) P mi∈M ri  .
FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 . So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). We refer to this approach, as heuristic H 1,1 . Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation. Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed. For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik . If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t). If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.
As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.
Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on
maximizing the chance of obtaining its immediate reward r0. Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.
Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies. We call such problem a starvation of method mk. That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided. We now prove that: THEOREM 2. Heuristic H 1,1 can overestimate the opportunity cost.
PROOF. We prove the theorem by showing a case where the overestimation occurs. For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively. Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).
From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing. Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure
observable in practice.
To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 . To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split. Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).
For the new heuristics, we now prove, that: THEOREM 3. Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.
PROOF. When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 . Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.
For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).
For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).
Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does. However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.
Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods. However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable. Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems.
Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.
Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present. Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100). We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400. We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost. The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function. In particular,
Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%. When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .
We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)). To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)). We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics. Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.
Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.
We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.
Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.
We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm. We then run both algorithms for a total of 100 policy improvement iterations.
Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).
As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%. For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.
We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)). We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached. We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale). As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.
We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)). In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.
We show the results in Figure (7b). Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.
We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. In particular, we consider
Figure 4: Visualization of heuristics for opportunity costs splitting.
Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods. For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially. We therefore vary the time horizons from 3000 to 4000, and then to 5000. We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP.
Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains. In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs. Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.
In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4]. Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11]. Unfortunately, they fail to scale up to large-scale domains at present time. Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints. Finally, value function techniques have been studied in context of single agent MDPs [7] [9]. However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.
Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No. FA875005C0030. The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments.
[1] R. Becker, V. Lesser, and S. Zilberstein. Decentralized MDPs with Event-Driven Interactions. In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.
Transition-Independent Decentralized Markov Decision Processes. In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of decentralized control of Markov decision processes. In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib. A polynomial algorithm for decentralized Markov decision processes with temporal constraints.
In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib. An iterative algorithm for solving constrained decentralized Markov decision processes. In AAAI, pages 1089-1094, 2006. [6] C. Boutilier. Sequential optimality and coordination in multiagent systems. In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman. Exact solutions to time-dependent MDPs.
In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein. Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman. Lazy approximation for solving continuous finite-horizon MDPs. In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig. Risk-sensitive planning with one-switch utility functions: Value iteration. In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy. Coordinated plan management using multiagent MDPs. In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella. Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs. In IJCAI, pages 1758-1760, 2005.

On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent"s expected utility in situations of interactive decision making.
A slightly divergent view is expounded by Schelling when he states that strategy [. . . ] is not concerned with the efficient application of force but with the exploitation of potential force [9, page 5]. From this perspective, the formal model of a game in strategic form only outlines the strategic features of an interactive situation. Apart from merely choosing and performing an action from a set of actions, there may also be other courses open to an agent. E.g., the strategic lie of the land may be such that a promise, a threat, or a combination of both would be more conductive to his ends.
The potency of a promise, however, essentially depends on the extent the promisee can be convinced of the promiser"s resolve to see to its fulfillment. Likewise, a threat only succeeds in deterring an agent if the latter can be made to believe that the threatener is bound to execute the threat, should it be ignored. In this sense, promises and threats essentially involve a commitment on the part of the one who makes them, thus purposely restricting his freedom of choice. Promises and threats epitomize one of the fundamental and at first sight perhaps most surprising phenomena in game theory: it may occur that a player can improve his strategic position by limiting his own freedom of action. By commitments we will understand such limitations of one"s action space. Action itself could be seen as the ultimate commitment. Performing a particular action means doing so to the exclusion of all other actions.
Commitments come in different forms and it may depend on the circumstances which ones can and which ones cannot credibly be made. Besides simply committing to the performance of an action, an agent might make his commitment conditional on the actions of other agents, as, e.g., the kidnapper does, when he promises to set free a hostage on receiving a ransom, while threatening to cut off another toe, otherwise. Some situations even allow for commitments on commitments or for commitments to randomized actions.
By focusing on the selection of actions rather than on commitments, it might seem that the conception of game theory as mere interactive decision theory is too narrow. In this respect, Schelling"s view might seem to evince a more comprehensive understanding of what game theory tries to accomplish. One might object, that commitments could be seen as the actions of a larger game. In reply to this criticism Schelling remarks: While it is instructive and intellectually satisfying to see how such tactics as threats, commitments, and promises can be absorbed in an enlarged, abstract supergame (game in normal form), it should be emphasized that we cannot learn anything about those tactics by studying games that are already in normal form. [. . . ] What we want is a theory that systematizes the study of the various universal ingredients that make up the move-structure of games; too abstract a model will miss them. [9, pp. 156-7] 108 978-81-904262-7-5 (RPS) c 2007 IFAAMAS Our concern is with these commitment tactics, be it that our analysis is confined to situations in which the players can commit in a given order and where we assume the commitments the players can make are given. Despite Schelling"s warning for too abstract a framework, our approach will be based on the formal notion of an extortion, which we will propose in Section 4 as a uniform tactic for a comprehensive class of situations in which commitments can be made sequentially. On this basis we tackle such issues as the usefulness of certain types of commitment in different situations (strategic games) or whether it is better to commit early rather than late. We also provide a framework for the assessment of more general game theoretic matters like the relationship of extortions to backward induction or Pareto efficiency.
Insight into these matters has proved itself invaluable for a proper understanding of diplomatic policy during the Cold War.
Nowadays, we believe, these issues are equally significant for applications and developments in such fields as multiagent systems, distributed computing and electronic markets. For example, commitments have been argued to be of importance for interacting software agents as well as for mechanism design. In the former setting, the inability to re-program a software agent on the fly can be seen as a commitment to its specification and thus exploited to strengthen its strategic position in a multiagent setting. A mechanism, on the other hand, could be seen as a set of commitments that steers the players" behavior in a certain desired way (see, e.g., [2]).
Our analysis is conceptually similar to that of Stackelberg or leadership games [15], which have been extensively studied in the economic literature (cf., [16]). These games analyze situations in which a leader commits to a pure or mixed strategy, and a number of followers, who then act simultaneously. Our approach, however, differs in that it is assumed that the players all move in a particular order-first, second, third and so on-and that it is specifically aimed at incorporating a wide range of possible commitments, in particular conditional commitments.
After briefly discussing related work in Section 2, we present the formal game theoretic framework, in which we define the notions of a commitment type as well as conditional and unconditional commitments (Section 3). In Section 4 we propose the generic concept of an extortion, which for each commitment type captures the idea of an optimal commitment profile. We point out an equivalence between extortions and backward induction solutions, and investigate whether it is advantageous to commit earlier rather than later and how the outcomes obtained through extortions relate to Pareto efficiency. Section 5 briefly reviews some other commitment types, such as inductive, mixed and mixed conditional commitments. The paper concludes with an overview of the results and an outlook for future research in Section 6.
Commitment is a central concept in game theory. The possibility to make commitments distinguishes cooperative from noncooperative game theory [4, 6]. Leadership games, as mentioned in the introduction, analyze commitments to pure or mixed strategies in what is essentially a two-player setting [15, 16]. Informally,
Schelling [9] has emphasized the importance of promises, threats and the like for a proper understanding of social interaction. On a more formal level, threats have also figured in bargaining theory.
Nash"s threat game [5] and Harsanyi"s rational threats [3] are two important early examples. Also, commitments have played a significant role in the theory of equilibrium selection (see, e.g., [13].
Over the last few years, game theory has become almost indispensable as a research tool for computer science and (multi)agent research. Commitments have by no means gone unnoticed (see, ⎡ ⎢⎢⎢⎢⎣ (1, 3) (3, 2) (0, 0) (2, 1) ⎤ ⎥⎥⎥⎥⎦ Figure 1: Committing to a dominated strategy can be advantageous. e.g., [1, 11]). Recently, also the strategic aspects of commitments have attracted the attention of computer scientists. Thus, Conitzer and Sandholm [2] have studied the computational complexity of computing the optimal strategy to commit to in normal form and Bayesian games. Sandholm and Lesser [8] employ levelled commitments for the design of multiagent systems in which contractual agreements are not fully binding. Another connection between commitments and computer science has been pointed out by Samet [7] and Tennenholtz [12]. Their point of departure is the observation that programs can be used to formulate commitments that are conditional on the programs of other systems.
Our approach is similar to the Stackleberg setting in that we assume an order in which the players commit. We, however, consider a number of different commitment types, among which conditional commitments, and propose a generic solution concept.
By committing, an agent can improve his strategic position. It may even be advantageous to commit to a strategy that is strongly dominated, i.e., one for which there is another strategy that yields a better payoff no matter how the other agents act. Consider for example the 2×2 game in Figure 1, in which one player, Row, chooses rows and another, Col, chooses columns. The entries in the matrix indicate the payoffs to Row and Col, respectively. Then, top-left is the solution obtained by iterative elimination of strongly dominated strategies: for Row, playing top is always better than playing bottom, and assuming that Row will therefore never play bottom, left is always better than right for Col. However, if Row succeeds in convincing Col of his commitment to play bottom, the latter had better choose the right column. Thus, Row attains a payoff of two instead of one. Along a similar line of reasoning, however, Col would wish to commit to the left column, as convincing Row of this commitment guarantees him the most desirable outcome. If, on the other hand, both players actually commit themselves in this way but without convincing the other party of their having done so, the game ends in misery for both.
Important types of commitments, however, cannot simply be analyzed as unconditional commitments to actions. The essence of a threat, for example, is deterrence. If successful, it is not carried out. (This is also the reason why the credibility of a threat is not necessarily undermined if its putting into effect means that the threatener is also harmed.) By contrast, promises are made to entice and, as such, meant to be fulfilled. Thus, both threats and promises would be strategically void if they were unconditional.
Figure 2 shows an example, in which Col can guarantee himself a payoff of three by threatening to choose the right column if Row chooses top. (This will suffice to deter Row, and there is no need for an additional promise on the part of Col.) He cannot do so by merely committing unconditionally, and neither can Row if he were to commit first.
In the case of conditional commitments, however, a particular kind of inconsistency can arise. It is not in general the case that any two commitments can both be credible. In a 2 × 2 game, it could occur that Row commits conditionally on playing top if the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 109 ⎡ ⎢⎢⎢⎢⎣ (2, 2) (0, 0) (1, 3) (3, 1) ⎤ ⎥⎥⎥⎥⎦ Figure 2: The column player Col can guarantee himself a payoff of three by threatening to play right if the row player Row plays top.
Col plays left, and bottom, otherwise. If now, Col simultaneously were able to commit to the conditional strategy to play right if Row plays top, and left, otherwise, there is no strategy profile that can be played without one of the players" bluff being called.
To get around this problem, one can write down conditional commitments in the form of rules and define appropriate fixed point constructions, as suggested by Samet [7] and Tennenholtz [12].
Since checking the semantic equivalence of two commitments (or commitment conditions) is undecidable in general, Tennenholtz bases his definition of program equilibrium on syntactic equivalence. We, by contrast, try to steer clear from fixed point constructions by assuming that the players make their commitment in a particular order. Each player can then make his commitments dependent on the actions of the players to commit after him, but not on the commitments of the players that committed before. On the issue how this order comes about we do not here enter. Rather, we assume it to be determined by the circumstances, which may force or permit some players to commit earlier and others later. We will find that it is not always beneficial to commit earlier than later or vice versa.
Another point to heed is that we only consider the case in which the commitments are considered absolutely binding. We do not take into account commitments that can be violated. Intuitively, this could be understood as that the possibility of violation fatally undermines the credibility of the commitment. We also assume commitments to be complete, in the sense that they fully lay down a player"s behavior in all foreseeable circumstances. These assumptions imply that the outcome of the game is entirely determined by the commitments the players make. Although these might be implausible assumptions for some situations, we had better study the idealized case first, before tackling the complications of the more general case. To make these concepts formally precise, we first have to fix some notation.
A strategic game is a tuple (N, (Ai)i∈N, (ui)i∈N), where N = {1, . . . , n} is a finite set of players, Ai is a set of actions available to player i and ui a real-valued utility function for player i on the set of (pure) strategy profiles S = A1×· · ·×An. We call a game finite if for all players i the action set Ai is finite. A mixed strategy σi for a player i is a probability distribution over Ai. We write Σi for the set of mixed strategies available to player i, and Σ = Σ1 × · · · × Σn for the set of mixed strategy profiles. We further have σ(a) and σi(a) denote the probability of action a in mixed strategy profile σ or mixed strategy σi, respectively. In settings involving expected utility, we will generally assume that utility functions represent von Neumann-Morgenstern preferences. For a player i and (mixed) strategy profiles σ and τ we write σ i τ if ui (σ) ui (τ).
Relative to a strategic game (N, (Ai)i∈N, (ui)i∈N) and an ordering π = (π1, . . . , πn) of the players, we define the set Fπi of (pure) conditional commitments of a player πi as the set of functions from Aπ1 × · · · × Aπi−1 to Aπi . For π1 we have the set of conditional commitments coincide with Aπ1 . By a conditional commitment profile f we understand any combination of conditional commitments in Fπ1 × · · · × Fπn .
Intuitively, π reflects the sequential order in which the players can make their commitments, with πn committing first, πn−1 second, and so on. Each player can condition his action on the actions of all players that are to commit after him. In this manner, each conditional commitment profile f can be seen to determine a unique strategy profile, denoted by f , which will be played if all players stick to their conditional commitments. More formally, the strategy profile f = (fπ1 , . . . , fπn ) for a conditional commitment profile f is defined inductively as fπ1 =df. fπ1 , fπi+1 =df. fπi+1 (fπ1 , . . . , fπi ).
The sequence fπ1 , (fπ1 , fπ2 ), . . . , (fπ1 , . . . , fπn ) will be called the path of f . E.g., in the two-player game of Figure 2 and given the order (Row, Col), Row has two conditional commitments, top and bottom, which we will henceforth denote t and b. Col, on the other hand, has four conditional commitments, corresponding to the different functions mapping strategies of Row to those of Col. If we consider a conditional commitment f for Col such that f (t) = l and f (b) = r, then (t, f ) is a conditional commitment profile and (t, f ) = (t, f (t)) = (t, l).
There is a natural way in which a strategic game G together with an ordering (π1, . . . , πn) of the players can be interpreted as an extensive form game with perfect information (see, e.g., [4, 6])1 , in which π1 chooses his action first, π2 second, and so on. Observe that under this assumption the strategies in the extensive form game and the conditional commitments in the strategic game G with ordering π are mathematically the same objects. Applying backward induction to the extensive form game yields subgame perfect equilibria, which arguably provide appropriate solutions in this setting.
From the perspective of conditional commitments, however, players move in reverse order. We will argue that under this interpretation other strategy profiles should be singled out as appropriate.
To illustrate this point, consider once more the game in Figure 2 and observe that neither player can improve on the outcome obtained via iterated strong dominance by committing unconditionally to some strategy. Situations like this, in which players can make unconditional commitments in a fixed order, can fruitfully be analyzed as extensive form games, and the most lucrative unconditional commitment can be found through backward induction.
Figure 3 shows the extensive form associated with the game of Figure 2. The strategies available to the row player are the same as in the strategic form: choosing the top or the bottom row. The strategies for the column player in the extensive game are given by the four functions that map strategies of the row player in the strategic game to one of his own. Transforming this extensive form back into a strategic game (see Figure 4), we find that there exists a second equilibrium besides the one found by means of backward induction. This equilibrium with outcome (1, 3), indicated by the thick lines in Figure 3, has been argued to be unacceptable in the sequential game as it would involve an incredible threat by Col: once Row has played top, Col finds himself confronted with a fait accompli. He had better make the best of a bad bargain and opt for the left column after all. This is in essence the line of thought Selten followed in his famous argument for subgame perfect equilibria [10]. If, however, the strategies of Col in the extensive form are thought of as his conditional commitments he can make in case 1 For a formal definition of a game in extensive form, the reader consult one of the standard textbooks, such as [4] or [6]. In this paper all formal definitions are based on strategic games and orderings of the players only. 110 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2 2 3 1 0 0 1 3 Row Col Figure 3: Extensive form obtained from the strategic game of Figure 2 when the row player chooses an action first. The backward induction solution is indicated by dashed lines, the conditional commitment solution by solid ones. (The horizontal dotted lines do not indicate information sets, but merely indicate which players are to move when.) he moves first, the situation is radically different. Thus we also assume that it is possible for Col to make credible the threat to choose the right column if Row were to play top, so as to ensure the latter is always better off to play the bottom row. If Col can make a conditional commitment of playing the right column if Row chooses top, and the left column otherwise, this leaves Row with the easy choice between a payoff of zero or one, and Col may expect a payoff of three.
This line of reasoning can be generalized to yield an algorithm for finding optimal conditional commitments for general twoplayer games:
, sπ2 ) with maximum payoff to player π2, and set fπ1 = sπ1 and fπ2 (sπ1 ) = sπ2 .
∈ Aπ1 with tπ1 sπ1 , find a strategy tπ2 ∈ Aπ2 that minimizes uπ1 (tπ1 , tπ2 ), and set fπ2 (tπ1 ) = tπ2 .
(tπ1 , fπ2 (tπ1 )) uπ1 (sπ1 , sπ2 ) for all tπ1 sπ1 , return f .
, sπ2 ) with the highest payoff to π2 among the ones that have not yet been considered. Set fπ1 = sπ1 and fπ2 (sπ1 ) = sπ2 , and continue with Step 2.
Generalizing the idea underlying this algorithm, we present in Section 4 the concept of an extortion, which applies to games with any number of players. For any order of the players an extortion contains, for each player, an optimal commitment given the commitments of the players that committed earlier.
So far, we have distinguished between conditional and unconditional commitments. If made sequentially, both of them determine a unique strategy profile in a given strategic game. This notion of sequential commitment allows for generalization and gives rise to the following definition of a (sequential) commitment type.
Definition 3.1. (Sequential commitment type) A (sequential) commitment type τ associates with each strategic game G and each ordering π of its players, a tuple Xπ1 , . . . , Xπn , φ , where Xπ1 , . . . , Xπn are (abstract) sets of commitments and φ is a function mapping each profile in X = Xπ1 × · · · × Xπn to a (mixed) strategy profile of G. A commitment type Xπ1 , . . . , Xπn , φ is finite whenever Xπi is finite for each i with 1 i n.
Thus, the type of unconditional commitments associates with a game and an ordering π of its players the tuple S π1 , . . . , S πn , id , ⎡ ⎢⎢⎢⎢⎣ (2, 2) (2, 2) (0, 0) (0, 0) (1, 3) (3, 1) (1, 3) (3, 1) ⎤ ⎥⎥⎥⎥⎦ Figure 4: The strategic game corresponding to the extensive form of Figure 3 where id is the identity function. Similarly, Fπ1 , . . . , Fπn , is the tuple associated with the same game by the type of (pure) conditional commitments.
In the introduction, we argued informally how players could improve their position by conditionally committing. How well they can do, could be analyzed by means of an extensive game with the actions of each player being defined as the possible commitments he can make. Here, we introduce for each commitment type a corresponding notion of extortion, which is defined relative to a strategic game and an ordering of the players. Extortions are meant to capture the concept of a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. A complicating factor is that in finding a player"s optimal commitment, one should not only take into account how such a commitment affects other players" actions, but also how it enables them to make their commitments.
Definition 4.1. (Extortions) Let G be a strategic game, π an ordering of its players, and τ a commitment type. Let τ(G, π) be given by Xπ1 , . . . , Xπn , φ . A τ-extortion of order 0 is any commitment profile x ∈ Xπ1 × · · · × Xπn . For m > 0, a commitment profile x ∈ Xπ1 × · · · × Xπn is a τ-extortion of order m in G given π if x is an τ-extortion of order m − 1 with φ yπ1 , . . . , yπm , xπm+1 , . . . , xπn πm φ xπ1 , . . . , xπm , xπm+1 , . . . , xπn for all commitment profiles g in X with (yπ1 , . . . , yπm , xπm+1 , . . . , xπn ) a τ-extortion of order m − 1. A τ-extortion is a commitment profile that is a τ-extortion of order m for all m with 0 m n.
Furthermore, we say that a (mixed) strategy profile σ is τ-extortionable if there is some τ-extortion x with φ(x) = s.
Thus, an extortion of order 1 is a commitment profile in which player π1, makes a commitment that maximizes his payoff, given fixed commitments of the other players. An extortion of order m is an extortion of order m − 1 that maximizes player m"s payoff, given fixed commitments of the players πm+1 through πn.
For the type of conditional commitments we have that any conditional commitment profile f is an extortion of order 0 and an extortion of an order m greater than 0 is any extortion of order m − 1 for which: gπ1 , . . . , gπm , fπm+1 , . . . , fπn πm fπ1 , . . . , fπm , fπm+1 , . . . , fπn , for each conditional commitment profile g such that gπ1 , . . . , gπm , fπm+1 , . . . , fπn an extortion of order m − 1.
To illustrate the concept of an extortion for conditional commitments consider the three-player game in Figure 5 and assume ⎡ ⎢⎢⎢⎢⎣ (1, 4, 0) (1, 4, 0) (3, 3, 2) (0, 0, 2) ⎤ ⎥⎥⎥⎥⎦ ⎡ ⎢⎢⎢⎢⎣ (4, 1, 1) (4, 0, 0) (3, 3, 2) (0, 0, 2) ⎤ ⎥⎥⎥⎥⎦ Figure 5: A three-player strategic game The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 111 ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 4 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 3 3 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 4 1 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 3 3 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Row Col Mat ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 4 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 4 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 4 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 3 3 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 4 1 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 3 3 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Row Col Mat ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 4 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 4 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Figure 6: A conditional extortion f of order 1 (left) and an extortion g of order 3 (right). (Row, Col, Mat) to be the order in which the players commit.
Figure 6 depicts the possible conditional commitments of the players in extensive forms, with the left branch corresponding to Row"s strategy of playing the top row. Let f and g be the conditional commitment strategies indicated by the thick lines in the left and right figures respectively. Both f and g are extortions of order 1.
In both f and g Row guarantees himself the higher payoff given the conditional commitments of Mat and Col. Only g, however, is also an extortion of order 2. To appreciate that f is not, consider the conditional commitment profile h in which Row chooses top and Col chooses right no matter how Row decides, i.e., h is such that hRow = t and hCol(t) = hCol(b) = r. Then, (hRow, hCol, fMat) is also an extortion of order 1, but yields Col a higher payoff than f does. We leave it to the reader to check that, by contrast, g is an extortion of order 3, and therewith an extortion per se.
One way of understanding conditional extortions is by conceiving of them as combinations of precisely one promise and a number of threats. From the strategy profiles that can still be realized given the conditional commitments of players that have committed before him, a player tries to enforce the strategy profile that yields him as much payoff as possible. Hence, he chooses his commitment so as to render deviations from the path that leads to this strategy profile as unattractive as possible (‘threats") and the desired strategy profile as appealing as possible (‘promises") for the relevant players. If (sπ1 , . . . , sπn ) is such a desirable strategy profile for player πi and fπi his conditional commitment, the value of fπi (sπ1 , . . . , sπi−1 ) could be taken as his promise, whereas the values of fπi for all other (tπ1 , . . . , tπi−1 ) could be seen as constituting his threats. The higher the payoff is to the other players in a strategy profile a player aims for, the easier it is for him to formulate an effective threat. However, making appropriate threats in this respect does not merely come down to minimizing the payoffs to players to commit later wherever possible. A player should also take into account the commitments, promises and threats the following players can make on the basis of his and his predecessors" commitments.
This is what makes extortionate reasoning sometimes so complicated, especially in situations with more than two players.
For example, in the game of Figure 5, there is no conditional extortion that ensures Mat a payoff of two. To appreciate this, consider the possible commitments Mat can make in case Row plays top and Col plays left (tl) and in case Row plays top and Col plays right (tr). If Mat commits to the right matrix in both cases, he virtually promises Row a payoff of four, leaving himself with a payoff of at most one. Otherwise, he puts Col in a position to deter Row from choosing bottom by threatening to choose the right column if the latter does so. Again, Mat cannot expect a payoff higher than one.
In short, no matter how Mat conditionally commits, he will either enable Col to threaten Row into playing top or fail to lure Row into playing the bottom row.
The solutions extortions provide can also be obtained by modeling the situation as an extensive form game and applying a backward inductive type of argument. The actions of the players in any such extensive form game are then given by their conditional commitments, which they then choose sequentially. For higher types of commitment, such as conditional commitments, such ‘metagames", however, grow exponentially in the number of strategies available to the players and are generally much larger than the original game. The correspondence between the backward induction solutions in the meta-game and the extortions of the original strategic game rather signifies that the concept of an extortion is defined properly. First we define the concept of benign backward induction in general relative to a game in strategic form together with an ordering of the players. Intuitively it reflects the idea that each player chooses for each possible combination of actions of his predecessors the action that yields the highest payoff, given that his successors do similarly. The concept is called benign backward induction, because it implies that a player, when indifferent between a number of actions, chooses the one that benefits his predecessors most. For an ordering π of the players, we have πR denote its reversal (πn, . . . , π1).
Definition 4.2. (Benign backward induction) Let G be a strategic game and π an ordering of its players. A benign backward induction of order 0 is any conditional commitment profile f subject to π. For m > 0, a conditional commitment strategy profile f is a benign backward induction (solution) of order m if f is a benign backward induction of order m − 1 and (gπR n , . . . , gπR m+1 , gπR m , . . . , gπR 1 ) πR m (gπR n , . . . , gπR m+1 , fπR m , . . . , fπR 1 ) for any backward induction (gπR n ,..., gπR m+1 , gπR m ,..., gπR 1 ) of order m−1.
A conditional commitment profile f is a benign backward induction if it is a benign backward induction of order k for each k with 0 k n.
For games with a finite action set for each player, the following result follows straightforwardly from Kuhn"s Theorem (cf. [6, p. 99]). In particular, this result holds if the players" actions are commitments of a finite type.
Fact 4.3. For each finite game and each ordering of the players, benign backward inductions exist.
For each game, each ordering of its players and each commitment type, we can define another game G∗ with the the actions of each player i given by his τ-commitments Xi in G. The utility 112 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) of a strategy profile (xπ1 , . . . , xπn ) for a player i in G∗ can then be equated to his utility of the strategy profile φ(xπn , . . . , xπ1 ) in G. We now find that the extortions of G can be retrieved as the paths of the benign backward induction solutions of the game G∗ for the ordering πR of the players, provided that the commitment type is finite.
Theorem 4.4. Let G = (N, (Ai)i∈N, (ui)i∈N) be a game and π an ordering of its players with which the finite commitment type τ associates the tuple Xπ1 , . . . , Xπn , φ . Let further G∗ = N, (Xπi )i∈N, (u∗ πi )i∈N , where u∗ πi (xπn , . . . , xπ1 ) = uπi (φ(xπ1 , . . . , xπn )), for each τ-commitment profile (xπ1 , . . . , xπn ). Then, a πcommitment profile (xπ1 , . . . , xπn ) is a τ-extortion in G given π if and only if there is some benign backward induction f in G∗ given πR with f = (xπn , . . . , xπ1 ).
Proof. Assume that f is a benign backward induction in G∗ relative to πR . Then, f = (xπn , . . . , xπ1 ), for some commitment profile (xπ1 , . . . , xπn ) of G relative to π. We show by induction that (xπ1 , . . . , xπn ) is an extortion of order m, for all m with 0 m n. For m = 0, the proof is trivial. For the induction step, consider an arbitrary commitment profile (yπ1 , . . . , yπn ) such that (yπ1 , . . . , yπm , xπm+1 , . . . , xπn ) is an extortion of order m − 1. In virtue of the induction hypothesis, there is a benign backward induction g of order m − 1 in G∗ with g = (xπn , . . . , xπm+1 , yπm , . . . , yπ1 ). As f is also a benign backward induction of order m: (gπn , . . . , gπ1 ) ∗ πm (gπn , . . . , gπm+1 , fπm , . . . , fπ1 ) .
Hence, (xπn , . . . , xπm+1 , yπm , . . . , yπ1 ) ∗ πm (xπn , . . . , xπ1 ). By definition of u∗ πm , then also: φ(yπ1 , . . . , yπm , xπm+1 , . . . , xπn ) πm φ(xπ1 , . . . , xπn ).
We may conclude that x is an extortion of order m.
For the only if direction, assume that x is an extortion of G given π. We prove that there is a benign backward induction f (∗) in G∗ for πR with f (∗) = x. In virtue of Fact 4.3, there is a benign backward induction h in G∗ given πR . Now define f (∗) in such a way that f (∗) πi (zπn , . . . , zπi−1 ) = xπi , if (zπn , . . . , zπi−1 ) = (xπn , . . . , xπi−1 ), and f (∗) πi (zπn , . . . , zπi−1 ) = hπi (zπn , . . . , zπi−1 ), otherwise. We prove by induction on m, that f (∗) is a benign backward induction of order m, for each m with 0 m n. The basis is trivial. So assume that f (∗) is a backward induction of order m − 1 in G∗ given πR and consider an arbitrary benign backward induction g of order m − 1 in G∗ given πR . Let g be given by (yπn , . . . , yπ1 ).
Either (yπn , . . . , yπm+1 ) = (xπn , . . . , xπm+1 ), or this is not the case. If the latter, it can readily be appreciated that: (gπn , . . . , gπm+1 , f (∗) πm , . . . , f (∗) π1 ) = (gπn , . . . , gπm+1 , hπm , . . . , hπ1 ) .
Having assumed that h is a benign backward induction, subsequently, (gπn , . . . , gπ1 ) ∗ m (gπn , . . . , gπm+1 , hπm , . . . , hπ1 ) , and (gπn , . . . , gπ1 ) ∗ m (gπn , . . . , gπm+1 , f (∗) πm , . . . , f (∗) π1 ) . Hence, f (∗) is a benign backward induction of order m. In the former case the reasoning is slightly different. Then, (gπn , . . . , gπ1 ) = (xπn , . . . , xπm+1 , yπm , . . . , yπ1 ). It follows that: (gπn , . . . , gπm+1 , f (∗) πm , . . . , f (∗) π1 ) = (f (∗) πn , . . . , f (∗) π1 ) = (xπn , . . . , xπ1 ).
In virtue of the induction hypothesis, (yπ1 , . . . , yπn ) is an extortion of order m − 1 in G given π. As the reasoning takes place under the assumption that x is an extortion in G given π, we also have: φ(yπ1 , . . . , yπm , xπm+1 , . . . , xπn ) πm φ(xπ1 , . . . , xπn ).
Then, (xπn , . . . , xπm+1 , yπm , . . . , yπ1 , ) ∗ πm (xπn , . . . , xπ1 )., by definition of u∗ . We may conclude that: (gπn , . . . , gπ1 ) ∗ πm (gπn , . . . , gπm+1 , f (∗) πm , . . . , f (∗) π1 ) , signifying that f (∗) is a benign backward induction of order m.
As an immediate consequence of Theorem 4.4 and Fact 4.3 we also have the following result.
Corollary 4.5. Let τ be a finite commitment type. Then, τ-extortions exist for each strategic game and for each ordering of the players.
In the case of unconditional commitments, it is not always favorable to be the first to commit. This is well illustrated by the familiar game rock-paper-scissors. If, on the other hand, the players are in a position to make conditional commitments in this particular game, moving first is an advantage. Rather, we find that it can never harm to move first in a two-player game with conditional commitments.
Theorem 4.6. Let G be a two-player strategic game involving player i. Further let f be an extortion of G in which i commits first, and g an extortion in which i commits second. Then, g i f .
Proof sketch. Let f be a conditional extortion in G given π. It suffices to show that there is some conditional extortion h of order 1 in G given π with h = f . Assume for a contradiction that there is no such extortion of order 1 in G given π . Then there must be some b∗ ∈ Aj such that f ≺j b∗ , a , for all a ∈ Ai. (Otherwise we could define (gj, gi) such that gj = fj(fi), gi(gj) = fi, and for any other b ∈ Aj, gi(b) = a∗ , where a∗ is an action in Ai such that (b, a∗ ) j f . Then g would be an extortion of order 1 in G given π with g .) Now consider a conditional commitment profile h for G and π such that hj(a) = b∗ , for all a ∈ Ai. Let further hi be such that (a, hj) i (hi, hj) , for all a ∈ Ai. Then, h is an extortion of order 1 in G given π. Observe that (hi, hj) = (fi , b∗ ).
Hence, f ≺j h , contradicting the assumption that f is an extortion in G given π.
Theorem 4.6 does not generalize to games with more than two players. Consider the three-player game in Figure 7, with extensive forms as in Figure 8. Here, Row and Mat have identical preferences. The latter"s extortionate powers relative Col, however, are very weak if he is to commit first: any conditional commitment he makes puts Col in a situation in which she can enforce a payoff of two, leaving Mat and Row in the cold with a payoff of one.
However, if Mat is last to commit and Row first, then the latter can exploit his strategic powers, threaten Col so that she plays left, and guarantee both himself and Mat a payoff of two.
Another issue concerns the Pareto efficiency of the strategy profiles extortionable through conditional commitments. We say that a strategy profile s (weakly) Pareto dominates another strategy profile t if t i s for all players i and s it for some. Moreover, a strategy profile s is (weakly) Pareto efficient if it is not (weakly) Pareto dominated by any other strategy profile. We extend this terminology to conditional commitment profiles by saying that a conditional commitment profile f is (weakly) Pareto efficient or (weakly) Pareto dominates another conditional commitment profile if f is or does so. We now have the following result. ⎡ ⎢⎢⎢⎢⎣ (0, 1, 0) (0, 0, 0) (0, 0, 0) (1, 2, 1) ⎤ ⎥⎥⎥⎥⎦ ⎡ ⎢⎢⎢⎢⎣ (2, 1, 2) (0, 0, 0) (0, 0, 0) (1, 2, 1) ⎤ ⎥⎥⎥⎥⎦ Figure 7: A three-person game.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 113 ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 1 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 2 1 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Row Col Mat ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 2 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 2 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 1 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 2 1 2 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Row Col Mat ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 2 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 0 0 0 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎜⎜⎜⎝ 1 2 1 ⎞ ⎟⎟⎟⎟⎟⎟⎟⎠ Figure 8: It is not always better to commit early than late, even in the case of conditional or inductive commitments.
Theorem 4.7. In each game, Pareto efficient conditional extortions exist. Moreover, any strategy profile that Pareto dominates an extortion is also extortionable through a conditional commitment.
Proof sketch. Since, in virtue of Fact 4.5, extortions generally exists in each game, it suffices to recognize that the second claim holds. Let s be the strategy profile (sπ1 , . . . , sπn ). Let further the conditional extortion f be Pareto dominated by s. An extortion g with g = s can then be constructed by adopting all threats of f while promising g . I.e., for all players πi we have gπi (sπ1 , . . . , sπi−1 ) = si and gπi (tπ1 , . . . , tπn ) = fπi (tπ1 , . . . , tπn ), for all other tπ1 , . . . , tπn . As s Pareto dominates f , the threats of f remain effective as threats of g given that s is being promised.
This result hints at a difference between (benign) backward induction and extortions. In general, solutions of benign backward inductions can be Pareto dominated by outcomes that are no benign backward induction solutions. Therefore, although every extortion can be seen as a benign backward induction in a larger game, it is not the case that all formal properties of extortions are shared by benign backward inductions in general.
Conditional and unconditional commitments are only two possible commitment types. The definition also provides for types of commitment that allow for committing on commitments, thus achieving a finer adjustment of promises and threats. Similarly, it subsumes commitments on and to mixed strategies. In this section we comment on some of these possibilities.
Apart from making commitments conditional on the actions of the players to commit later, one could also commit on the commitments of the following players. Informally, such commitments would have the form of if you only dare to commit in such and such a way, then I do such and such, otherwise I promise to act so and so.
For a strategic game G and an ordering π of the players, we define the inductive commitments of the players inductively. The inductive commitments available to π1 coincide with the actions that are available to him. An inductive commitment for player πi+1 is a function mapping each profile of inductive commitments of players π1 through πi to one of his basic actions. Formally we define the type of inductive commitments Fπ1 , . . . , Fπn , such that for each player πi in a game G and given π: Fπ1 =df. Aπ1 ,
Fπi+1 =df. A Fπ1 ×···×Fπi πi+1 .
Let fπi = fπi fπ1 , . . . , fπi−1 , for each player πi and have f denote the pure strategy profile fπ1 , . . . , fπn .
Inductive commitments have a greater extortionate power than conditional commitments. To appreciate this, consider once more the game in Figure 5. We found that the strategy profile in which Row chooses bottom and Col and Mat both choose left is not extortionable through conditional commitments. By means of inductive commitments, however, this is possible. Let f be the inductive commitment profile such that fRow is Row choosing the bottom row (b), fCol is the column player choosing the left column (l) no matter how Row decides, and fMat is defined such that: fMat fRow, fCol = ⎧ ⎪⎪⎨ ⎪⎪⎩ r if fRow = t and fCol (b) = r, l otherwise.
Instead of showing formally that f is an inductive extortion of the strategy profile (b, l, l), we point out informally how this can be done. We argued that in order to exact a payoff of two by means of a conditional extortion, Mat would have to lure Row into choosing the bottom row without at the same time putting Col in a position to successfully threaten Row not to choose top. This, we found, is an impossibility if the players can only make conditional commitments. By contrast, if Mat can commit to commitments, he can undermine Col"s efforts to threaten Row by playing the right matrix, if Col were to do so. Yet, Mat can still force Row to choose the bottom row, in case Col desists form making this threat.
As can readily be observed, in any game, the inductive commitments of the first two players to commit coincide with their conditional commitments. Hence, as an immediate consequence of Theorem 4.6, it can never harm to be the first to commit to an inductive commitment in the two player case. Similarly, we find that the game depicted in Figure 7 also serves as an example showing that, in case there are more than two players, it is not always better to commit to an inductive commitment early. In this example the strategic position of Mat is so weak if he is to commit first, that even the possibility to commit inductively does not strengthen it, whereas, in a similar fashion as with conditional commitments, Row can enforce a payoff of two to both himself and Mat if he is the first to commit.
So far we have merely considered commitments to and on pure strategies. A natural extension would be also to consider commitments to and on mixed strategies. We distinguish between conditional, unconditional as well as inductive mixed commitments.
We find that they are generally quite incomparable with their pure counterparts: in some situations a player can achieve more using a mixed commitment, in another using a pure commitment type.
A complicating factor with mixed commitment types is that they 114 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) can result in a mixed strategy profile being played. This makes that the distinction between promises and threats, as delineated in Section 4.1, gets blurred for mixed commitment types.
The type of mixed unconditional commitments associates with each game G and ordering π of its players the tuple Σπ1 , . . . , Σπn , id . The two-player case has been extensively studied (e.g., [2, 16]). As a matter of fact, von Neumann"s famous minimax theorem shows that for two-player zero-sum games, it does not matter which player commits first. If the second player to commit plays a mixed strategy that ensures his security level, the first player to commit can do no better than to do so as well [14].
In the game of Figure 5 we found that, with conditional commitments, Mat is unable to enforce an outcome that awards him a payoff of two. Recall that the reason of this failure is that any effort to deter Row from choosing the top row is flawed, as it would put Col in an excellent position to threaten Row not to choose the bottom row. If Mat has inductive commitments at his disposal, however, this is a possibility. We now find that in case the players can dispose of unconditional mixed strategies, Mat is in a much similar position. He could randomize uniformly between the left and right matrix. Then, Row"s expected utility is 21 2 if he plays the top row, no matter how Col randomizes. The expected payoff of Col does not exceed 21 2 , either, in case Row chooses top. By purely committing to the left column, Col player entices Row to play bottom, as his expected utility then amounts to 3. This ensures an expected utility of three for Col as well.
However, a player is not always better off with unconditional mixed commitments than with pure conditional commitments. For an example, consider the game in Figure 2. Using pure conditional commitments, he can ensure a payoff of three, whereas with unconditional mixed commitments 21 2 would be the most he could achieve. Neither is it in general advantageous to commit first to a mixed strategy in a three-player game. To appreciate this, consider once more the game in Figure 7. Again committing to a mixed strategy will not achieve much for Mat if he is to move first, and as before the other players have no reason to commit to anything other than a pure strategy. This holds for all players if Row commits first,
Col second and Mat last, be it that in this case Mat obtains the best payoff he can get.
Analogous to conditional and inductive commitments one can also define the types of mixed conditional and mixed inductive commitments. With the former, a player can condition his mixed strategies on the mixed strategies of the players to commit after him.
These tend to be very large objects and, knowing little about them yet, we shelve their formal analysis for future research.
Conceptually, it might not be immediately clear how such mixed conditional commitments can be made with credibility. For one, when one"s commitments are conditional on a particular mixed strategy being played, how can it be recognized that it was in fact this mixed strategy that was played rather than another one? If this proves to be impossible, how can one know how his conditional commitments is to be effectuated? A possible answer would be, that all depends on the circumstances in which the commitments were made. E.g., if the different agents can submit their mixed conditional commitments to an independent party, the latter can execute the randomizations and determine the unique mixed strategy profile that their commitments induce.
In some situations agents can strengthen their strategic position by committing themselves to a particular course of action. There are various types of commitment, e.g., pure, mixed and conditional.
Which type of commitment an agent is in a position in to make essentially depends on the situation under consideration. If the agents commit in a particular order, there is a tactic common to making commitments of any type, which we have formalized by means the concept of an extortion. This generic concept of extortion can be analyzed in abstracto. Moreover, on its basis the various commitment types can be compared formally and systematically.
We have seen that the type of commitment an agent can make has a profound impact on what an agent can achieve in a gamelike situation. In some situations a player is much helped if he is in a position to commit conditionally, whereas in others mixed commitments would be more profitable. This raises the question as to the characteristic formal features of the situations in which it is advantageous for a player to be able to make commitments of a particular type.
Another issue which we leave for future research is the computational complexity of finding an extortion for the different commitment types.

Nowadays, it is well established that ontologies are needed for semantic web, knowledge management, B2B... For knowledge management, ontologies are used to annotate documents and to enhance the information retrieval. But building an ontology manually is a slow, tedious, costly, complex and time consuming process.
Currently, a real challenge lies in building them automatically or semi-automatically and keeping them up to date. It would mean creating dynamic ontologies [10] and it justifies the emergence of ontology learning techniques [14] [13].
Our research focuses on Dynamo (an acronym of DYNAMic Ontologies), a tool based on an adaptive multi-agent system to construct and maintain an ontology from a domain specific set of texts.
Our aim is not to build an exhaustive, general hierarchical ontology but a domain specific one. We propose a semi-automated tool since an external resource is required: the "ontologist". An ontologist is a kind of cognitive engineer, or analyst, who is using information from texts and expert interviews to design ontologies.
In the multi-agent field, ontologies generally enable agents to understand each other [12]. They"re sometimes used to ease the ontology building process, in particular for collaborative contexts [3], but they rarely represent the ontology itself [16]. Most works interested in the construction of ontologies [7] propose the refinement of ontologies. This process consists in using an existing ontology and building a new one from it. This approach is different from our approach because Dynamo starts from scratch. Researchers, working on the construction of ontologies from texts, claim that the work to be automated requires external resources such as a dictionary [14], or web access [5]. In our work, we propose an interaction between the ontologist and the system, our external resource lies both in the texts and the ontologist.
This paper first presents, in section 2, the big picture of the Dynamo system. In particular the motives that led to its creation and its general architecture. Then, in section 3 we discuss the distributed clustering algorithm used in Dynamo and compare it to a more classic centralized approach. Section 4 is dedicated to some enhancement of the agents behavior that got designed by taking into account criteria ignored by clustering. And finally, in section 5, we discuss the limitations of our approach and explain how it will be addressed in further work.
Dynamo aims at reducing the need for manual actions in processing the text analysis results and at suggesting a concept network kick-off in order to build ontologies more efficiently. The chosen approach is completely original to our knowledge and uses an adaptive multi-agent system. This choice comes from the qualities offered by multi-agent system: they can ease the interactive design of a system [8] (in our case, a conceptual network), they allow its incremental building by progressively taking into account new data (coming from text analysis and user interaction), and last but not least they can be easily distributed across a computer network.
Dynamo takes a syntactical and terminological analysis of texts as input. It uses several criteria based on statistics computed from the linguistic contexts of terms to create and position the concepts.
As output, Dynamo provides to the analyst a hierarchical organization of concepts (the multi-agent system itself) that can be validated, refined of modified, until he/she obtains a satisfying state of 1286 978-81-904262-7-5 (RPS) c 2007 IFAAMAS the semantic network.
An ontology can be seen as a stable map constituted of conceptual entities, represented here by agents, linked by labelled relations. Thus, our approach considers an ontology as a type of equilibrium between its concept-agents where their forces are defined by their potential relationships. The ontology modification is a perturbation of the previous equilibrium by the appearance or disappearance of agents or relationships. In this way, a dynamic ontology is a self-organizing process occurring when new texts are included into the corpus, or when the ontologist interacts with it.
To support the needed flexibility of such a system we use a selforganizing multi-agent system based on a cooperative approach [9].
We followed the ADELFE method [4] proposed to drive the design of this kind of multi-agent system. It justifies how we designed some of the rules used by our agents in order to maximize the cooperation degree within Dynamo"s multi-agent system.
In this section, we present our system architecture. It addresses the needs of Knowledge Engineering in the context of dynamic ontology management and maintenance when the ontology is linked to a document collection.
The Dynamo system consists of three parts (cf. figure 1): • a term network, obtained thanks to a term extraction tool used to preprocess the textual corpus, • a multi-agent system which uses the term network to make a hierarchical clustering in order to obtain a taxonomy of concepts, • an interface allowing the ontologist to visualize and control the clustering process. ??
Ontologist Interface System Concept Agent Term Term network Terms Extraction Tool Figure 1: System architecture The term extractor we use is Syntex, a software that has efficiently been used for ontology building tasks [11]. We mainly selected it because of its robustness and the great amount of information extracted. In particular, it creates a "Head-Expansion" network which has already proven to be interesting for a clustering system [1]. In such a network, each term is linked to its head term1 and 1 i.e. the maximum sub-phrase located as head of the term its expansion term2 , and also to all the terms for which it is a head or an expansion term. For example, "knowledge engineering from text" has "knowledge engineering" as head term and "text" as expansion term. Moreover, "knowledge engineering" is composed of "knowledge" as head term and "engineering" as expansion term.
With Dynamo, the term network obtained as the output of the extractor is stored in a database. For each term pair, we assume that it is possible to compute a similarity value in order to make a clustering [6] [1]. Because of the nature of the data, we are only focusing on similarity computation between objects described thanks to binary variables, that means that each item is described by the presence or absence of a characteristic set [15]. In the case of terms we are generally dealing with their usage contexts. With Syntex, those contexts are identified by terms and characterized by some syntactic relations.
The Dynamo multi-agent system implements the distributed clustering algorithm described in detail in section 3 and the rules described in section 4. It is designed to be both the system producing the resulting structure and the structure itself. It means that each agent represent a class in the taxonomy. Then, the system output is the organization obtained from the interaction between agents, while taking into account feedback coming from the ontologist when he/she modifies the taxonomy given his needs or expertise.
This section presents the distributed clustering algorithm used in Dynamo. For the sake of understanding, and because of its evaluation in section 3.1, we recall the basic centralized algorithm used for a hierarchical ascending clustering in a non metric space, when a symmetrical similarity measure is available [15] (which is the case of the measures used in our system).
Algorithm 1: Centralized hierarchical ascending clustering algorithm Data: List L of items to organize as a hierarchy Result: Root R of the hierarchy while length(L) > 1 do max ← 0; A ← nil; B ← nil; for i ← 1 to length(L) do I ← L[i]; for j ← i + 1 to length(L) do J ← L[j]; sim ← similarity(I, J); if sim > max then max ← sim; A ← I; B ← J; end end end remove(A, L); remove(B, L); append((A, B), L); end R ← L[1]; In algorithm 1, for each clustering step, the pair of the most similar elements is determined. Those two elements are grouped in a cluster, and the resulting class is appended to the list of remaining elements. This algorithm stops when the list has only one element left. 2 i.e. the maximum sub-phrase located as tail of the term The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1287 The hierarchy resulting from algorithm 1 is always a binary tree because of the way grouping is done. Moreover grouping the most similar elements is equivalent to moving them away from the least similar ones. Our distributed algorithm is designed relying on those two facts. It is executed concurrently in each of the agents of the system.
Note that, in the following of this paper, we used for both algorithms an Anderberg similarity (with α = 0.75) and an average link clustering strategy [15]. Those choices have an impact on the resulting tree, but they impact neither the global execution of the algorithm nor its complexity.
We now present the distributed algorithm used in our system. It is bootstrapped in the following way: • a TOP agent having no parent is created, it will be the root of the resulting taxonomy, • an agent is created for each term to be positioned in the taxonomy, they all have TOP as parent.
Once this basic structure is set, the algorithm runs until it reaches equilibrium and then provides the resulting taxonomy.
Ak−1 Ak AnA2A1 P ...... ......
A1 Figure 2: Distributed classification: Step 1 The process first step (figure 2) is triggered when an agent (here Ak) has more than one brother (since we want to obtain a binary tree). Then it sends a message to its parent P indicating its most dissimilar brother (here A1). Then P receives the same kind of message from each of its children. In the following, this kind of message will be called a "vote".
Ak−1 Ak AnA2A1 P P" ...... ......
P" P" Figure 3: Distributed clustering: Step 2 Next, when P has got messages from all its children, it starts the second step (figure 3). Thanks to the received messages indicating the preferences of its children, P can determine three sub-groups among its children: • the child which got the most "votes" by its brothers, that is the child being the most dissimilar from the greatest number of its brothers. In case of a draw, one of the winners is chosen randomly (here A1), • the children that allowed the "election" of the first group, that is the agents which chose their brother of the first group as being the most dissimilar one (here Ak to An), • the remaining children (here A2 to Ak−1).
Then P creates a new agent P (having P as parent) and asks agents from the second group (here agents Ak to An) to make it their new parent.
Ak−1 Ak AnA2A1 P P" ...... ......
Figure 4: Distributed clustering: Step 3 Finally, step 3 (figure 4) is trivial. The children rejected by P (here agent A2 to An) take its message into account and choose P as their new parent. The hierarchy just created a new intermediate level.
Note that this algorithm generally converges, since the number of brothers of an agent drops. When an agent has only one remaining brother, its activity stops (although it keeps processing messages coming from its children). However in a few cases we can reach a "circular conflict" in the voting procedure when for example A votes against B, B against C and C against A. With the current system no decision can be taken. The current procedure should be improved to address this, probably using a ranked voting method.
Now, we evaluate the properties of our distributed algorithm. It requires to begin with a quantitative evaluation, based on its complexity, while comparing it with the algorithm 1 from the previous section.
Its theoretical complexity is calculated for the worst case, by considering the similarity computation operation as elementary. For the distributed algorithm, the worst case means that for each run, only a two-item group can be created. Under those conditions, for a given dataset of n items, we can determine the amount of similarity computations.
For algorithm 1, we note l = length(L), then the most enclosed "for" loop is run l − i times. And its body has the only similarity computation, so its cost is l−i. The second "for" loop is ran l times for i ranging from 1 to l. Then its cost is Pl i=1(l − i) which can be simplified in l×(l−1) 2 . Finally for each run of the "while" loop, l is decreased from n to 1 which gives us t1(n) as the amount of similarity computations for algorithm 1: t1(n) = nX l=1 l × (l − 1) 2 (1) For the distributed algorithm, at a given step, each one of the l agents evaluates the similarity with its l −1 brothers. So each steps has a l × (l − 1) cost. Then, groups are created and another vote occurs with l decreased by one (since we assume worst case, only groups of size 2 or l −1 are built). Since l is equal to n on first run, we obtain tdist(n) as the amount of similarity computations for the distributed algorithm: tdist(n) = nX l=1 l × (l − 1) (2) Both algorithms then have an O(n3 ) complexity. But in the worst case, the distributed algorithm does twice the number of el1288 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ementary operations done by the centralized algorithm. This gap comes from the local decision making in each agent. Because of this, the similarity computations are done twice for each agent pair.
We could conceive that an agent sends its computation result to its peer. But, it would simply move the problem by generating more communication in the system. 0 20000 40000 60000 80000 100000 120000 140000 160000 180000
Amountofcomparisons Amount of input terms
Figure 5: Experimental results In a second step, the average complexity of the algorithm has been determined by experiments. The multi-agent system has been executed with randomly generated input data sets ranging from ten to one hundred terms. The given value is the average of comparisons made for one hundred of runs without any user interaction.
It results in the plots of figure 5. The algorithm is then more efficient on average than the centralized algorithm, and its average complexity is below the worst case. It can be explained by the low probability that a data set forces the system to create only minimal groups (two items) or maximal (n − 1 elements) for each step of reasoning. Curve number 2 represents the logarithmic polynomial minimizing the error with curve number 1. The highest degree term of this polynomial is in n2 log(n), then our distributed algorithm has a O(n2 log(n)) complexity on average. Finally, let"s note the reduced variation of the average performances with the maximum and the minimum. In the worst case for 100 terms, the variation is of 1,960.75 for an average of 40,550.10 (around 5%) which shows the good stability of the system.
Although the quantitative results are interesting, the real advantage of this approach comes from more qualitative characteristics that we will present in this section. All are advantages obtained thanks to the use of an adaptive multi-agent system.
The main advantage to the use of a multi-agent system for a clustering task is to introduce dynamic in such a system. The ontologist can make modifications and the hierarchy adapts depending on the request. It is particularly interesting in a knowledge engineering context. Indeed, the hierarchy created by the system is meant to be modified by the ontologist since it is the result of a statistic computation. During the necessary look at the texts to examine the usage contexts of terms [2], the ontologist will be able to interpret the real content and to revise the system proposal. It is extremely difficult to realize this with a centralized "black-box" approach. In most cases, one has to find which reasoning step generated the error and to manually modify the resulting class. Unfortunately, in this case, all the reasoning steps that occurred after the creation of the modified class are lost and must be recalculated by taking the modification into account. That is why a system like ASIUM [6] tries to soften the problem with a system-user collaboration by showing to the ontologist the created classes after each step of reasoning. But, the ontologist can make a mistake, and become aware of it too late.
Figure 6: Concept agent tree after autonomous stabilization of the system In order to illustrate our claims, we present an example thanks to a few screenshots from the working prototype tested on a medical related corpus. By using test data and letting the system work by itself, we obtain the hierarchy from figure 6 after stabilization. It is clear that the concept described by the term "lésion" (lesion) is misplaced. It happens that the similarity computations place it closer to "femme" (woman) and "chirurgien" (surgeon) than to "infection", "gastro-entérite" (gastro-enteritis) and "hépatite" (hepatitis). This wrong position for "lesion" is explained by the fact that without ontologist input the reasoning is only done on statistics criteria.
Figure 7: Concept agent tree after ontologist modification Then, the ontologist replaces the concept in the right branch, by affecting "ConceptAgent:8" as its new parent. The name "ConceptAgent:X" is automatically given to a concept agent that is not described by a term. The system reacts by itself and refines the clustering hierarchy to obtain a binary tree by creating "ConceptAgent:11". The new stable state if the one of figure 7.
This system-user coupling is necessary to build an ontology, but no particular adjustment to the distributed algorithm principle is needed since each agent does an autonomous local processing and communicates with its neighborhood by messages.
Moreover, this algorithm can de facto be distributed on a computer network. The communication between agents is then done by sending messages and each one keeps its decision autonomy. Then, a system modification to make it run networked would not require to adjust the algorithm. On the contrary, it would only require to rework the communication layer and the agent creation process since in our current implementation those are not networked.
In the previous sections, we assumed that similarity can be computed for any term pair. But, as soon as one uses real data this property is not verified anymore. Some terms do not have any similarity value with any extracted term. Moreover for leaf nodes it is sometimes interesting to use other means to position them in the hierarchy. For this low level structuring, ontologists generally base The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1289 their choices on simple heuristics. Using this observation, we built a new set of rules, which are not based on similarity to support low level structuring.
In this case, agents can act with a very local point of view simply by looking at the parent/child relation. Each agent can try to determine if its parent is adequate. It is possible to guess this because each concept agent is described by a set of terms and thanks to the "Head-Expansion" term network.
In the following TX will be the set of terms describing concept agent X and head(TX ) the set of all the terms that are head of at least one element of TX . Thanks to those two notations we can describe the parent adequacy function a(P, C) between a parent P and a child C: a(P, C) = |TP ∩ head(TC )| |TP ∪ head(TC )| (3) Then, the best parent for C is the P agent that maximizes a(P, C).
An agent unsatisfied by its parent can then try to find a better one by evaluating adequacy with candidates. We designed a complementary algorithm to drive this search: When an agent C is unsatisfied by its parent P, it evaluates a(Bi, C) with all its brothers (noted Bi) the one maximizing a(Bi, C) is then chosen as the new parent.
Figure 8: Concept agent tree after autonomous stabilization of the system without head coverage rule We now illustrate this rule behavior with an example. Figure 8 shows the state of the system after stabilization on test data. We can notice that "hépatite viral" (viral hepatitis) is still linked to the taxonomy root. It is caused by the fact that there is no similarity value between the "viral hepatitis" term and any of the term of the other concept agents.
Figure 9: Concept agent tree after activation of the head coverage rule After activating the head coverage rule and letting the system stabilize again we obtain figure 9. We can see that "viral hepatitis" slipped through the branch leading to "hepatitis" and chose it as its new parent. It is a sensible default choice since "viral hepatitis" is a more specific term than "hepatitis".
This rule tends to push agents described by a set of term to become leafs of the concept tree. It addresses our concern to improve the low level structuring of our taxonomy. But obviously our agents lack a way to backtrack in case of modifications in the taxonomy which would make them be located in the wrong branch. That is one of the point where our system still has to be improved by adding another set of rules.
In the previous sections and examples, we only used one algorithm at a time. The distributed clustering algorithm tends to introduce new layers in the taxonomy, while the head coverage algorithm tends to push some of the agents toward the leafs of the taxonomy. It obviously raises the question on how to deal with multiple criteria in our taxonomy building, and how agents determine their priorities at a given time.
The solution we chose came from the search for minimizing non cooperation within the system in accordance with the ADELFE method. Each agent computes three non cooperation degrees and chooses its current priority depending on which degree is the highest. For a given agent A having a parent P, a set of brothers Bi and which received a set of messages Mk having the priority pk the three non cooperation degrees are: • μH (A) = 1 − a(P, A), is the "head coverage" non cooperation degree, determined by the head coverage of the parent, • μB(A) = max(1 − similarity(A, Bi)), is the "brotherhood" non cooperation degree, determined by the worst brother of A regarding similarities, • μM (A) = max(pk), is the "message" non cooperation degree, determined by the most urgent message received.
Then, the non cooperation degree μ(A) of agent A is: μ(A) = max(μH (A), μB(A), μM (A)) (4) Then, we have three cases determining which kind of action A will choose: • if μ(A) = μH (A) then A will use the head coverage algorithm we detailed in the previous subsection • if μ(A) = μB(A) then A will use the distributed clustering algorithm (see section 3) • if μ(A) = μM (A) then A will process Mk immediately in order to help its sender Those three cases summarize the current activities of our agents: they have to find the best parent for them (μ(A) = μH (A)), improve the structuring through clustering (μ(A) = μB(A)) and process other agent messages (μ(A) = μM (A)) in order to help them fulfill their own goals.
We evaluated the experimental complexity of the whole multiagent system when all the rules are activated. In this case, the metric used is the number of messages exchanged in the system. Once again the system has been executed with input data sets ranging from ten to one hundred terms. The given value is the average of message amount sent in the system as a whole for one hundred runs without user interaction. It results in the plots of figure 10.
Curve number 1 represents the average of the value obtained.
Curve number 2 represents the average of the value obtained when only the distributed clustering algorithm is activated, not the full rule set. Curve number 3 represents the polynomial minimizing the error with curve number 1. The highest degree term of this polynomial is in n3 , then our multi-agent system has a O(n3 ) complexity
0 5000 10000 15000 20000 25000
Amountofmessages Amount of input terms
Figure 10: Experimental results on average. Moreover, let"s note the very small variation of the average performances with the maximum and the minimum. In the worst case for 100 terms, the variation is of 126.73 for an average of 20,737.03 (around 0.6%) which proves the excellent stability of the system.
Finally the extra head coverage rules are a real improvement on the distributed algorithm alone. They introduce more constraints and stability point is reached with less interactions and decision making by the agents. It means that less messages are exchanged in the system while obtaining a tree of higher quality for the ontologist.
The most important limitation of our current algorithm is that the result depends on the order the data gets added. When the system works by itself on a fixed data set given during initialization, the final result is equivalent to what we could obtain with a centralized algorithm. On the contrary, adding a new item after a first stabilization has an impact on the final result.
Figure 11: Concept agent tree after autonomous stabilization of the system To illustrate our claims, we present another example of the working system. By using test data and letting the system work by itself, we obtain the hierarchy of figure 11 after stabilization.
Figure 12: Concept agent tree after taking in account "hepatitis" Then, the ontologist interacts with the system and adds a new concept described by the term "hepatitis" and linked to the root.
The system reacts and stabilizes, we then obtain figure 12 as a result. "hepatitis" is located in the right branch, but we have not obtained the same organization as the figure 6 of the previous example. We need to improve our distributed algorithm to allow a concept to move along a branch. We are currently working on the required rules, but the comparison with centralized algorithm will become very difficult. In particular since they will take into account criteria ignored by the centralized algorithm.
In section 3, we presented the distributed clustering algorithm used in the Dynamo system. Since this work was first based on this algorithm, it introduced a clear bias toward binary trees as a result.
But we have to keep in mind that we are trying to obtain taxonomies which are more refined and concise. Although the head coverage rule is an improvement because it is based on how the ontologists generally work, it only addresses low level structuring but not the intermediate levels of the tree.
By looking at figure 7, it is clear that some pruning could be done in the taxonomy. In particular, since "lésion" moved, "ConceptAgent:9" could be removed, it is not needed anymore.
Moreover the branch starting with "ConceptAgent:8" clearly respects the constraint to make a binary tree, but it would be more useful to the user in a more compact and meaningful form. In this case "ConceptAgent:10" and "ConceptAgent:11" could probably be merged.
Currently, our system has the necessary rules to create intermediate levels in the taxonomy, or to have concepts shifting towards the leaf. As we pointed, it is not enough, so new rules are needed to allow removing nodes from the tree, or move them toward the root.
Most of the work needed to develop those rules consists in finding the relevant statistic information that will support the ontologist.
After being presented as a promising solution, ensuring model quality and their terminological richness, ontology building from textual corpus analysis is difficult and costly. It requires analyst supervising and taking in account the ontology aim. Using natural languages processing tools ease the knowledge localization in texts through language uses. That said, those tools produce a huge amount of lexical or grammatical data which is not trivial to examine in order to define conceptual elements. Our contribution lies in this step of the modeling process from texts, before any attempts to normalize or formalize the result.
We proposed an approach based on an adaptive multi-agent system to provide the ontologist with a first taxonomic structure of concepts. Our system makes use of a terminological network resulting from an analysis made by Syntex. The current state of our software allows to produce simple structures, to propose them to the ontologist and to make them evolve depending on the modifications he made. Performances of the system are interesting and some aspects are even comparable to their centralized counterpart.
Its strengths are mostly qualitative since it allows more subtle user interactions and a progressive adaptation to new linguistic based information.
From the point of view of ontology building, this work is a first step showing the relevance of our approach. It must continue, both to ensure a better robustness during classification, and to obtain richer structures semantic wise than simple trees. From this improvements we are mostly focusing on the pruning to obtain better taxonomies. We"re currently working on the criterion to trigger the complementary actions of the structure changes applied by our clustering algorithm. In other words this algorithm introduces inThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1291 termediate levels, and we need to be able to remove them if necessary, in order to reach a dynamic equilibrium.
Also from the multi-agent engineering point of view, their use in a dynamic ontology context has shown its relevance. This dynamic ontologies can be seen as complex problem solving, in such a case self-organization through cooperation has been an efficient solution. And, more generally it"s likely to be interesting for other design related tasks, even if we"re focusing only on knowledge engineering in this paper. Of course, our system still requires more evaluation and validation work to accurately determine the advantages and flaws of this approach. We"re planning to work on such benchmarking in the near future.

An ontology is commonly defined as a specification of the conceptualisation of a particular domain. It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers. As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web. But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3]. Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.
This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems. Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time. In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).
Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].
Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g.,
WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.
We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism. Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain. Imagine a situation in which two agents are facing each other in front of a checker board. Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right. Although the conceptualisation of ‘left" and ‘right" is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board. Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.
The same agents situated differently may produce a different alignment.
This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12]. This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment. In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation. In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligman"s theory of information flow [1]. This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.
Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.
We shall therefore consider a scenario with two or more agents situated in an environment. Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state. Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents" respective ontologies). We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents" viewpoint of the environment. The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.
In Section 2 we describe our formal model for Situated Semantic Alignment (SSA). First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents" viewpoints of the environment. Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.
These approximations gradually become more reliable as the method is applied. In Section 3 we report on an application of our method. Conclusions and further work are analyzed in Section 4. Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper. We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper.
Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward). We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states. We further assume that each agent is able to observe the environment and has its own perception of it. This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.
According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A). So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.
For each i ∈ {1, 2}, we consider a classification Ai that models Ai"s viewpoint of E. First, tok(Ai) is composed of Ai"s perceptions of E states, that is, tok(Ai) = Pi. Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.
Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.
Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε. The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment. However, the set of types of the power classification includes all possible token configurations potentially described by types. Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.
The notion of channel (see appendix A) is fundamental in Barwise and Seligman"s theory. The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.
The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}. Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents" perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E        ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1        tok(A2) |=A2        Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents" syntactic entities, that is, agents" types. We state that meaningfulness must be in accord with E.
The sum operation (see appendix A) gives us a way of putting the two agents" classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E.
A1 + A2 assembles agents" classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents" perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α. We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E.
Classification A1 + A2 seems to be the natural place in which to search for relations among agents" types. Now,
Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A). The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated,
Log(A1 + A2), involve all those constraints among agents" types valid according to A1 +A2. Notice however that these constraints are obvious. As we stated above, meaningfulness must be in accord with channel E.
Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents" types are in accord with channel E because they are computed making use of f as we expound below.
As important as the notion of channel is the concept of distributed logic (see appendix A). Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L.
If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.
In our case, Log(E) explains the relationship between the agents" viewpoints of the environment in a logical fashion.
On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2). On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents" perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).
All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic). In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).
Log(E) is the logic of SSA. Th(Log(E)) comprises the most meaningful constraints among agents" types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.
Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B). If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B). That means there is no significant relation between agents" points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents" viewpoints. This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E).
through communication We have dubbed Log(E) the logic of SSA. Th(Log(E)) comprehends the most meaningful constraints among agents" types according to E. The problem is that neither agent can make use of this theory because they do not know E completely. In this section, we present a method by which agents obtain approximations to Th(Log(E)). We also prove these approximations gradually become more reliable as the method is applied.
Agents can obtain approximations to Th(Log(E)) through communication. A1 and A2 communicate by exchanging information about their perceptions of environment states.
This information is expressed in terms of their own classification relations. Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not. This exchange generates a channel C = {fi : Ai →
C}i∈{1,2} and Th(Log(C)) contains the constraints among agents" types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information. Th(Log(C )) comprises the constraints among agents" types justified by the fact that agents have observed e and e . The significant point is that C is a refinement of C (see appendix A). Theorem 2.1 below ensures that the refined channel involves more reliable information.
The communication supposedly ends when agents have observed all the environment states. Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.
Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).
Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.
Theorem 2.1. Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels. If C is a refinement of C then:
Proof. Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi . Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2.
Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ]. We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].
We proceed by reductio ad absurdum. Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] . Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.
Let us choose an arbitrary γ ∈ Γ. We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}. Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).
Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ. Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ). A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] . Hence Γ Log(C) Δ as we wanted to prove.
Therefore, there exists c token in C such that a1, a2 = ˇf(c). Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}. Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ). Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.
Remark 2.1. Theorem 2.1 asserts that the more refined channel gives more reliable information. Even though its theory has less constraints, it has more normal tokens to which they apply.
In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.
Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form. We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .
We omit informorphisms superscripts when no confusion arises. Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).
Agents communication starts from the observation of E.
Let us suppose that E is in state e1 ∈ S = tok(E). A1"s perception of e1 is f1(e1 ) and A2"s perception of e1 is f2(e1 ).
We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1. So can A2 do. Since both typ(A1) and typ(A2) are finite, this process eventually finishes. After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).
C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication. It is easy to prove that Th(Log(C1 )) = Th(C1 ). The significant point is that both agents know C1 as the result of the communication. Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents" types justified by the fact that agents have observed e1 .
Now, let us assume that E turns to a new state e2 . Agents can proceed as before, exchanging this time information about their perceptions of e2 . Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up. We define C2 so as to take also into account the information provided by the previous stage of communication.
On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents. Note this set is chosen with the same cardinality of S.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages. Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents" types justified by the fact that agents have observed e1 and e2 . A1 and A2 knows C2 so they can use these constraints. The key point is that channel C2 is a refinement of C1 . It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3). By Theorem 2.1, C2 constraints are more reliable than C1 constraints.
In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment. Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3). Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents" types justified by the fact that agents have observed e1 , e2 , . . . , en .
Cn fn−1  A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 "",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ                           f1 2 ØØ Cn−1  . . .  C2 f1  C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.
It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.
At that point, the family of channels {Cn }n∈N would inform of all the communication stages. It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks. But the study of possible termination criteria is outside the scope of this paper and left for future work. From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.
On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper. It ensures, together with Theorem
that approximates more closely to the theory generated by the logic of SSA.
Theorem 2.2. The following statements hold:
is a refinement of Cn .
) = Th(Log(C∗ )).
Proof.
defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn .
follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn  A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????
Cn
In the previous section we have described in great detail our formal model for SSA. However, we have not tackled the practical aspect of the model yet. In this section, we give a brushstroke of the pragmatic view of our approach.
We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.
Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4). Robots can carry only one package at a time and they can not move through a package.
Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.
Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5). We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.
Describing the environment states as well as the robots" perception functions is rather tedious and even unnecessary.
We assume the reader has all those descriptions in mind.
All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other. In order to communicate, agents send messages using some ontology. In our scenario, there coexist two ontologies, the UDRL and LCR ontologies. Both of them are very simple and are just confined to describe what robots observe.
Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance. Let us suppose two URDL robots are in a situation like the one depicted in Figure 6. Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.
This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1"s Up-Right square, Robot1 is situated in Robot2"s Up-Left square and a package is located in Robot1"s Up square, then a package is located in Robot2"s Up square.
Now, problems arise when a LCR robot and a URDL robot try to interoperate. See Figure 7. Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 . Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}. From these, only the following constraints are plausible according to C1 : The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place. C2 also considers the previous information and hence refines C1 . The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic. Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.
Figure 8: Refinement
In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states. Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligman"s insight to focus on tokens as the enablers of information flow. Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process. Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.
In the future, our effort will focus on the practical side of the situated semantic alignment problem. We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.
The formal model exposed in this paper will constitute a solid base of future practical results.
Acknowledgements This work is supported under the UPIC project, sponsored by Spain"s Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253. Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spain"s Ministry of Education and Science, partially funded by the European Social Fund.
[1] J. Barwise and J. Seligman. Information Flow: The Logic of Distributed Systems. Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia. Local models semantics, or contextual reasoning = locality + compatibility. Artificial Intelligence, 127(2):221-259,
[3] F. Giunchiglia and P. Shvaiko. Semantic matching.
The Knowledge Engineering Review, 18(3):265-280,
[4] Y. Kalfoglou and M. Schorlemmer. IF-Map: An ontology-mapping method based on information-flow theory. In Journal on Data Semantics I, LNCS 2800,
[5] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: The sate of the art. The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent. Semantic integration in the Information Flow Framework. In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391,
[7] D. Lenat. CyC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11),
[8] V. L´opez, M. Sabou, and E. Motta. PowerMap: Mapping the real Semantic Web on the fly.
Proceedings of the ISWC"06, 2006. [9] F. McNeill. Dynamic Ontology Refinement. PhD
thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou. Progressive ontology alignment for meaning coordination: An information-theoretic foundation. In 4th Int. Joint Conf. on Autonomous Agents and Multiagent Systems,
[11] P. Shvaiko and J. Euzenat. A survey of schema-based matching approaches. In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels. The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.
In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al. ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int. Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A. CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A). If a |=A α then a is said to be of type α.
Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).
Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .
Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ. A binary relation between subsets of Σ is called a consequence relation on Σ. A theory is a pair T = Σ, where is a consequence relation on Σ. A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies:
2 In fact, this is the definition of a binary channel. A channel can be defined with an arbitrary index set.
Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification. A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ. The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .
Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where:
Cla(L),
of L, which satisfy all constraints of Th(L).
A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L). L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).
Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).
Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.
Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.
Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2. A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr). Channel C is a refinement of C if there exists a refinement infomorphism r from C to C.
B. CHANNEL THEORY THEOREMS Theorem B.1. The logic generated by a classification is sound and complete. Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).
Theorem B.2. Let L be a logic on a classification B and f : A → B an infomorphism.
[L] is complete.
[L] is sound. 3 All theories considered in this paper are regular.

Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9]. However, negotiation on price presupposes that other properties of the service have already been agreed upon.
Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3]. When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].
However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2]. However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.
For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer. Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.
For this purpose, evaluation of the service components with different weights can be useful. Some studies take these weights as a priori and uses the fixed weights [4]. On the other hand, mostly the producer does not know the consumer"s preferences before the negotiation. Hence, it is more appropriate for the producer to learn these preferences for each consumer.
Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time. We represent service requests as a vector of service features. We use an ontology in order to capture the relations between services and to construct the features for a given service. By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation. The particular service we have used is a wine selling service. The wine seller learns the wine preferences of the customer to sell better targeted wines. The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer. Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.
Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services. In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer. For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine. What should the producer"s offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine? If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers. However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.
The rest of this paper is organized as follows: Section 2 explains our proposed architecture. Section 3 explains the learning algorithms that were studied to learn consumer preferences. Section 4 studies the different service offering mechanisms. Section 5 contains the similarity metrics used in the experiments. The details of the developed system is analyzed in Section 6. Section 7 provides our experimental setup, test cases, and results. Finally, Section 8 discusses and compares our work with other related work.
Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation. Figure 1 depicts our architecture. The consumer agent represents the customer and hence has access to the preferences of the customer. The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences. Similarly, the producer agent has access to the producer"s inventory and knows which wines are available or not.
A shared ontology provides the necessary vocabulary and hence enables a common language for agents. This ontology describes the content of the service. Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.
Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used. However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system. The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on. With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.
The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer. The data repository includes information on the products the producer owns, the number of the products and ratings of those products.
Ratings indicate the popularity of the products among customers.
Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.
The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request. The request is composed of significant features of the service. In the wine example, these features include color, winery and so on. This is the particular wine that the customer is interested in purchasing. If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends. Otherwise, the producer offers an alternative wine from the inventory. When the consumer receives a counter offer from the producer, it will evaluate it. If it is acceptable, then the negotiation will end.
Otherwise, the customer will generate a new request or stick to the previous request. This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.
One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer. When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producer"s available services. Both the consumer"s current request and the producer"s own available services are accessible by the producer. However, the consumer"s preferences in most cases will not be available. Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer. This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customer"s preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumer"s preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5)
The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature. The requests of the consumers represent individual wine products whereas their preferences are constraints over service features. For example, a consumer may have preference for red wine. This means that the consumer is willing to accept any wine offered by the producers as long as the color is red. Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).
At the beginning of negotiation, the producer agent does not know the consumer"s preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer. The preferences denote the relative importance of the features of the services demanded by the consumer agents.
For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all
Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ?}, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ?}, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ?}, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose. On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.
To tackle this problem, we propose to use incremental learning algorithms [6]. This is necessary since no training data is available before the interactions start. We particularly investigate two approaches. The first one is inductive learning. This technique is applied to learn the preferences as concepts. We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10]. CEA is known to perform poorly if the information to be learned is disjunctive. Interestingly, most of the time consumer preferences are disjunctive. Say, we are considering an agent that is buying wine.
The consumer may prefer red wine or rose wine but not white wine.
To use CEA with such preferences, a solid modification is necessary. The second approach is decision trees. Decision trees can learn from examples easily and classify new instances as positive or negative. A well-known incremental decision tree is ID5R [18].
However, ID5R is known to suffer from high computational complexity. For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning.
CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples. The algorithm maintains two sets to model the concept to be learned. The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain. As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept. The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned. At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.
During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example. At each interaction between the producer and the consumer, both G and S are modified. The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive. When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance. As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones. Incrementally, G specializes and S generalizes until G and S are equal to each other. When these sets are equal, the algorithm converges by means of reaching the target concept.
Unfortunately, CEA is primarily targeted for conjunctive concepts. On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes. There are several studies on learning disjunctive concepts via Version Space. Some of these approaches use multiple version space. For instance, Hong et al. maintain several version spaces by split and merge operation [7]. To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S.
We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation. Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.
EXAMPLE 1. Assume that the most specific set is {(Light,
Delicate, Red)} and a positive example, (Light, Delicate, White) comes.
The original CEA will generalize this as (Light, Delicate, ?), meaning the color can take any value. However, in fact, we only know that the color can be red or white. In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}. Only when all the values exist in the list, they will be replaced by ?. In other words, we let the algorithm generalize more slowly than before.
We modify the CEA algorithm to deal with this change. The modified algorithm, DCEA, is given as Algorithm 1. Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space. The initialization phase is the same as the original algorithm (lines 1, 2). If any positive sample comes, we add the sample to the special set as before (line 4). However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other. Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it. After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).
When a negative sample comes, we do not change S as before.
We only modify the most general hypotheses not to cover this negative sample (lines 11-15). Different from the original CEA, we try to specialize the G minimally. The algorithm removes the hypothesis covering the negative sample (line 13). Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.
For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis. Thus, all possible hypotheses that do not cover the negative sample are generated (line 14). Note that, exclusive list contains the values that the attribute cannot take. For example, consider the color attribute.
If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2. Table 1 illustrates the first three interactions and the workings of DCEA. The most general set and the most specific set show the contents of G and S after the sample comes in. After the first positive sample, S is generalized to also cover the instance.
The second sample is negative. Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized. In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set. The third sample is positive and generalizes S even more.
Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong,
White). This stems from the possibility of using this rule in the generation of other hypotheses. For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}. By Algorithm 1, we do not miss any information.
ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs. Applying this algorithm to our system with the intention of learning the consumer"s preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.
The ID3 algorithm is used in the learning process with the purpose of classification of offers. There are two classes: positive and negative. Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer. Consumer"s requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.
The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held. The test attribute in a non-leaf node is one of the attributes making up the service description. For instance, body, flavor, color and so on are potential test attributes for wine service.
When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.
The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning. To overcome this problem, the system keeps consumer"s requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples. After each coming request, the decision tree is rebuilt. Without doubt, there is a drawback of reconstruction such as additional process load. However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible.
After learning the consumer"s preferences, the producer needs to make a counter offer that is compatible with the consumer"s preferences.
To generate the best offer, the producer agent uses its service ontology and the CEA algorithm. The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.
When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample. The learning components, the most specific set S and the most general set G are actively used in offering service. The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent. In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer. The most specific set, S is used in order to find best offer, which is similar to the consumer"s preferences. Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.
When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.
This list constitutes the provider"s inventory of services. Upon receiving a request, if the producer can offer an exactly matching service, then it does so. For example, for a wine this corresponds to selling a wine that matches the specified features of the consumer"s request identically. When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation. To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).
We compute the similarities in various ways as will be explained in Section 5. After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity. The producer agent can break the tie in a number of ways. Here, we have associated a rating value with each service and the producer prefers the higher rated service to others.
If the producer learns the consumer"s preferences with ID3, a similar mechanism is applied with two differences. First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list. Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests.
In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms..
• Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumer"s preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumer"s current request but does not consider previous requests.
Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are. There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].
The similarity metric affects the performance of the system while deciding which service is the closest to the consumer"s request. We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity.
Tversky"s similarity metric compares two vectors in terms of the number of exactly matching features [17]. In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes. Our current assumption is that α and β is equal to each other.
SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.
Tversky"s similarity metric is designed to compare two feature vectors. In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector. S consists of hypotheses of feature vectors. Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.
EXAMPLE 3. Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.
Take service s as (Light, Strong, Rose). Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1). Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2. However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request. As a result, we expect the effect of the first hypothesis to be greater than that of the second. Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.
Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service. We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover. We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S.
AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4. For the above example, the similarity of (Light,
Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9. The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute. For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}. When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2).
A taxonomy can be used while estimating semantic similarity between two concepts. Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts. The links among the nodes can be considered as distances. Then, the length of the path between the nodes indicates how closely similar the concepts are. An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8]. The equation (3) [8] shows Lin"s similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them. Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C.
Similarity(c1, c2) =
log P(c1) + log P(c2) (3)
Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20]. The semantic similarity is represented with Equation (4) [20]. Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes. N1 is the number of edges between c1 and c0. N2 is the number of edges between c2 and c0. N0 is the number of IS-A links of c0 from the root of the taxonomy.
SimW u&P almer(c1, c2) =
N1 + N2 + 2 × N0 (4)
We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions. We use Figure
• Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that. Generalization of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept. The more general concepts are, the less similar they are. For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red. Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling. For instance, Red and Rose are children of ReddishColor. In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to it"s sibling then to its grandparent. To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.
Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.
As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.
At starting node related to the first concept, the similarity value is constant and equal to one. This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept. The shorter the path between the concepts, the higher the similarity between nodes.
Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way. Starting from c1, the tree is traversed to reach c2.
At each hop, the similarity decreases since the concepts are getting farther away from each other. However, based on our intuitions, not all hops decrease the similarity equally.
Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling. Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 . According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one. Algorithm 2 shows the distance calculation.
According to the algorithm, firstly the similarity is initialized with the value of one (line 1). If the concepts are equal to each other then, similarity will be one (lines 2-4). Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).
If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts. For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation. As a result, we decrease the similarity at each level with the rate of m (line9). Otherwise, there has to be a sibling relation. This means that we have to consider the effect of n when measuring similarity. Recall that we have counted N1+N2 edges between the concepts. Since there is a sibling relation, two of these edges constitute the sibling relation. Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).
Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2. In this example, m is taken as 2/3 and n is taken as 4/7.
Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology. In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.
Then the result is equal to the average semantic similarity of the entire feature vector.
We have implemented our architecture in Java. To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests. The producer agent is fully automated and the learning and service offering operations work as explained before. In this section, we explain the implementation details of the developed system.
We use OWL [11] as our ontology language and JENA as our ontology reasoner. The shared ontology is the modified version of the Wine Ontology [19]. It includes the description of wine as a concept and different types of wine. All participants of the negotiation use this ontology for understanding each other. According to the ontology, seven properties make up the wine concept. The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology. Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents. Also a variety of wine types are described in this ontology such as Burgundy,
Chardonnay, CheninBlanc and so on. Intuitively, any wine type described in the ontology also represents a wine concept. This allows us to consider instances of Chardonnay wine as instances of Wine class.
In addition to wine description, the hierarchical information of some features can be inferred from the ontology. For instance, we can represent the information Europe Continent covers Western Country. Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on. This hierarchical information is used in estimation of semantic similarity. In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z. For example, Europe Continent covers Bordeaux.
For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.
When that is the case, we give the reasonable similarity values for these features. For example, the body can be light, medium, or strong. In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.
WineStock Ontology is the producer"s inventory and describes a product class as WineProduct. This class is necessary for the producer to record the wines that it sells. Ontology involves the individuals of this class. The individuals represent available services that the producer owns. We have prepared two separate WineStock ontologies for testing. In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products.
We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).
We apply a variety of scenarios on this dataset in order to see the performance differences. Each test scenario contains a list of preferences for the user and number of matches from the product list.
Table 3 shows these preferences and availability of those products in the inventory for first five scenarios. Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.
Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19)
In comparison of learning algorithms, we use the five scenarios in Table 3. Here, first we use Tversky"s similarity measure. With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer. Since the performance also depends on the initial request, we repeat our experiments with different initial requests. Consequently, for each case, we run the algorithms five times with several variations of the initial requests. In each experiment, we count the number of iterations that were needed to reach an agreement. We take the average of these numbers in order to evaluate these systems fairly. As is customary, we test each algorithm with the same initial requests.
Table 4 compares the approaches using different learning algorithm. When the large parts of inventory is compatible with the customer"s preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1). As the number of compatible services drops, RO performs poorly as expected. The second worst method is SCR since it only considers the customer"s most recent request and does not learn from previous requests.
CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5. ID3 and DCEA achieve the best results. Their performance is comparable and they can handle all cases including Scenario 5.
Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56
To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA. In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.
Moreover, the winery of the wine should not be expensive. There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape. In addition, the location of wine should be in Europe. The sweetness degree is wished to be dry or off dry. The flavor should be delicate or moderate where the body should be medium or light. Furthermore, the winery of the wine should be an expensive winery. There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region. The category of winery should be Moderate Winery. There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region. There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery. There are two available products.
The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.
Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus. Tversky"s metric gives the worst results since it does not consider the semantic similarity.
Lin"s performance are better than Tversky but worse than others.
Wu Palmer"s metric and RP similarity measure nearly give the same performance and better than others. When the results are examined, considering semantic closeness increases the performance.
We review the recent literature in comparison to our work. Tama et al. [16] propose a new approach based on ontology for negotiation. According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies. Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details. While The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated. Further, we have built a system with which negotiation preferences can be learned.
Sadri et al. study negotiation in the context of resource allocation [14]. Agents have limited resources and need to require missing resources from other agents. A mechanism which is based on dialogue sequences among agents is proposed as a solution. The mechanism relies on observe-think-action agent cycle. These dialogues include offering resources, resource exchanges and offering alternative resource. Each agent in the system plans its actions to reach a goal state. Contrary to our approach, Sadri et al."s study is not concerned with learning preferences of each other.
Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1]. For achieving this, they use case-based reasoning.
Their approach is probabilistic since the behavior of the partners can change at each iteration. In our approach, we are interested in negotiation the content of the service. After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.
Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].
In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price. Time interval affects these agents differently. Compared to Fatima et al. our focus is different. While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.
Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4]. They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers. Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences. Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer. Unlike them, we also take into account the previous requests of the consumer. In their experiments,
Faratin et al. assume that the weights for service variables are fixed a priori. On the contrary, we learn these preferences over time.
In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations. Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge. These are interesting directions that we will pursue in our future work.
[1] J. Brzostowski and R. Kowalczyk. On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation. In Proceedings of the 4th Intl. Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman. A comment on issue-by-issue negotiations. Games and Economic Behavior, 19:144-148,
[3] J. K. Debenham. Managing e-market negotiation in context with a multiagent system. In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES"2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings. Using similarity criteria to make issue trade-offs in automated negotiations.
Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings. Optimal agents for multi-issue negotiation. In Proceeding of the 2nd Intl.
Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier. A note on the utility of incremental learning. AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng. Splitting and merging version spaces to learn disjunctive concepts. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. An information-theoretic definition of similarity. In Proc. 15th International Conf. on Machine Learning, pages 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas. Agents that buy and sell. Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Machine Learning. McGraw Hill, NY, 1997. [11] OWL. OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu. Foundations of Soft Case-Based Reasoning. John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan. Induction of decision trees. Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni. Dialogues for negotiation: Agent varieties and dialogue sequences. In ATAL 2001,
Revised Papers, volume 2333 of LNAI, pages 405-421.
Springer-Verlag, 2002. [15] M. P. Singh. Value-oriented electronic commerce. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.
Ontologies for supporting negotiation in e-commerce.
Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky. Features of similarity. Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff. Incremental induction of decision trees.
Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer. Verb semantics and lexical selection.
In 32nd. Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994.

Open multiagent systems (MAS) are composed of autonomous distributed agents that may enter and leave the agent society at their will because open systems have no centralized control over the development of its parts [1]. Since agents are considered as autonomous entities, we cannot assume that there is a way to control their internal behavior. These features are interesting to obtain flexible and adaptive systems but they also create new risks about the reliability and the robustness of the system. Solutions to this problem have been proposed by the way of trust models where agents are endowed with a model of other agents that allows them to decide if they can or cannot trust another agent.
Such trust decision is very important because it is an essential condition to the formation of agents' cooperation. The trust decision processes use the concept of reputation as the basis of a decision. Reputation is a subject that has been studied in several works [4][5][8][9] with different approaches, but also with different semantics attached to the reputation concept. Casare and Sichman [2][3] proposed a Functional Ontology of Reputation (FORe) and some directions about how it could be used to allow the interoperability among different agent reputation models. This paper describes how the FORe can be applied to allow interoperability among agents that have different reputation models. An outline of this approach is sketched in the context of a testbed for the experimentation and comparison of trust models, the ART testbed [6].
REPUTATION (FORe) In the last years several computational models of reputation have been proposed [7][10][13][14]. As an example of research produced in the MAS field we refer to three of them: a cognitive reputation model [5], a typology of reputation [7] and the reputation model used in the ReGret system [9][10]. Each model includes its own specific concepts that may not exist in other models, or exist with a different name. For instance, Image and Reputation are two central concepts in the cognitive reputation model. These concepts do not exist in the typology of reputation or in the ReGret model. In the typology of reputation, we can find some similar concepts such as direct reputation and indirect reputation but there are some slight semantic differences. In the same way, the ReGret model includes four kinds of reputation (direct, witness, neighborhood and system) that overlap with the concepts of other models but that are not exactly the same.
The Functional Ontology of Reputation (FORe) was defined as a common semantic basis that subsumes the concepts of the main reputation models. The FORe includes, as its kernel, the following concepts: reputation nature, roles involved in reputation formation and propagation, information sources for reputation, evaluation of reputation, and reputation maintenance. The ontology concept ReputationNature is composed of concepts such as IndividualReputation, GroupReputation and ProductReputation.
Reputation formation and propagation involves several roles, played by the entities or agents that participate in those processes.
The ontology defines the concepts ReputationProcess and ReputationRole. Moreover, reputation can be classified according to the origin of beliefs and opinions that can derive from several sources. The ontology defines the concept ReputationType which can be PrimaryReputation or SecondaryReputation.
PrimaryReputation is composed of concepts ObservedReputation and DirectReputation and the concept SecondaryReputation is composed of concepts such as PropagatedReputation and CollectiveReputation. More details about the FORe can be found on [2][3].
MODELS TO THE FORe Visser et al [12] suggest three different ways to support semantic integration of different sources of information: a centralized approach, where each source of information is related to one common domain ontology; a decentralized approach, where every source of information is related to its own ontology; and a hybrid approach, where every source of information has its own ontology and the vocabulary of these ontologies are related to a common ontology. This latter organizes the common global vocabulary in order to support the source ontologies comparison. Casare and Sichman [3] used the hybrid approach to show that the FORe serves as a common ontology for several reputation models.
Therefore, considering the ontologies which describe the agent reputation models we can define a mapping between these ontologies and the FORe whenever the ontologies use a common vocabulary. Also, the information concerning the mappings between the agent reputation models and the FORe can be directly inferred by simply classifying the resulting ontology from the integration of a given reputation model ontology and the FORe in an ontology tool with reasoning engine.
For instance, a mapping between the Cognitive Reputation Model ontology and the FORe relates the concepts Image and Reputation to PrimaryReputation and SecondaryReputation from FORe, respectively. Also, a mapping between the Typology of Reputation and the FORe relates the concepts Direct Reputation and Indirect Reputation to PrimaryReputation and SecondaryReputation from FORe, respectively. Nevertheless, the concepts Direct Trust and Witness Reputation from the Regret System Reputation Model are mapped to PrimaryReputation and PropagatedReputation from FORe. Since PropagatedReputation is a sub-concept of SecondaryReputation, it can be inferred that Witness Reputation is also mapped to SecondaryReputation.
THE ART TESTBED To exemplify the use of mappings from last section, we define a scenario where several agents are implemented using different agent reputation models. This scenario includes the agents" interaction during the simulation of the game defined by ART [6] in order to describe the ways interoperability is possible between different trust models using the FORe.
The ART testbed provides a simulation engine on which several agents, using different trust models, may run. The simulation consists in a game where the agents have to decide to trust or not other agents. The game"s domain is art appraisal, in which agents are required to evaluate the value of paintings based on information exchanged among other agents during agents" interaction. The information can be an opinion transaction, when an agent asks other agents to help it in its evaluation of a painting; or a reputation transaction, when the information required is about the reputation of another agent (a target) for a given era.
More details about the ART testbed can be found in [6].
The ART common reputation model was enhanced with semantic data obtained from FORe. A general agent architecture for interoperability was defined [11] to allow agents to reason about the information received from reputation interactions. This architecture contains two main modules: the Reputation Mapping Module (RMM) which is responsible for mapping concepts between an agent reputation model and FORe; and the Reputation Reasoning Module (RRM) which is responsible for deal with information about reputation according to the agent reputation model.
While including the FORe to the ART common reputation model, we have incremented it to allow richer interactions that involve reputation transaction. In this section we describe scenarios concerning reputation transactions in the context of ART testbed, but the first is valid for any kind of reputation transaction and the second is specific for the ART domain.
Suppose that agents A, B and C are implemented according to the aforementioned general agent architecture with the enhanced ART common reputation model, using different reputation models.
Agent A uses the Typology of Reputation model, agent B uses the Cognitive Reputation Model and agent C uses the ReGret System model. Consider the interaction about reputation where agents A and B receive from agent C information about the reputation of agent Y. A big picture of this interaction is showed in Figure 2.
ReGret Ontology (Y, value=0.8, witnessreputation) C Typol.
Ontology (Y, value=0.8, propagatedreputation) A CogMod.
Ontology (Y, value=0.8, reputation) B (Y, value=0.8,
PropagatedReputation) (Y, value=0.8,
PropagatedReputation) ReGret Ontology (Y, value=0.8, witnessreputation) C ReGret Ontology (Y, value=0.8, witnessreputation) ReGret Ontology (Y, value=0.8, witnessreputation) (Y, value=0.8, witnessreputation) C Typol.
Ontology (Y, value=0.8, propagatedreputation) A Typol.
Ontology (Y, value=0.8, propagatedreputation) Typol.
Ontology (Y, value=0.8, propagatedreputation) (Y, value=0.8, propagatedreputation) A CogMod.
Ontology (Y, value=0.8, reputation) B CogMod.
Ontology (Y, value=0.8, reputation) CogMod.
Ontology (Y, value=0.8, reputation) (Y, value=0.8, reputation) B (Y, value=0.8,
PropagatedReputation) (Y, value=0.8,
PropagatedReputation) (Y, value=0.8,
PropagatedReputation) (Y, value=0.8,
PropagatedReputation) Figure 1. Interaction about reputation The information witness reputation from agent C is treated by its RMM and is sent as PropagatedReputation to both agents. The corresponding information in agent A reputation model is propagated reputation and in agent B reputation model is reputation. The way agents A and B make use of the information depends on their internal reputation model and their RRM implementation.
Considering the same agents A and B and the art appraisal domain of ART, another interesting scenario describes the following situation: agent A asks to agent B information about agents it knows that have skill on some specific painting era. In this case agent A wants information concerning the direct reputation agent B has about agents that have skill on an specific era, such as cubism. Following the same steps of the previous scenario, agent A message is prepared in its RRM using information from its internal model. A big picture of this interaction is in Figure 2.
Typol.
Ontology (agent = ?, value = ?, skill = cubism, reputation = directreputation) A (agent = ?, value = ?, skill = cubism, reputation = PrimaryReputation) CogMod.
Ontology (agent = ?, value = ?, skill = cubism, reputation = image) B Typol.
Ontology (agent = ?, value = ?, skill = cubism, reputation = directreputation) A (agent = ?, value = ?, skill = cubism, reputation = PrimaryReputation) CogMod.
Ontology (agent = ?, value = ?, skill = cubism, reputation = image) B Figure 2. Interaction about specific types of reputation values Agent B response to agent A is processed in its RRM and it is composed of tuples (agent, value, cubism, image) , where the pair (agent, value) is composed of all agents and associated reputation values whose agent B knows their expertise about cubism by its own opinion. This response is forwarded to the RMM in order to be translated to the enriched common model and to be sent to agent A. After receiving the information sent by agent B, agent A processes it in its RMM and translates it to its own reputation model to be analyzed by its RRM.
In this paper we present a proposal for reducing the incompatibility between reputation models by using a general agent architecture for reputation interaction which relies on a functional ontology of reputation (FORe), used as a globally shared reputation model. A reputation mapping module allows agents to translate information from their internal reputation model into the shared model and vice versa. The ART testbed has been enriched to use the ontology during agent transactions. Some scenarios were described to illustrate our proposal and they seem to be a promising way to improve the process of building reputation just using existing technologies.
Anarosa A. F. Brandão is supported by CNPq/Brazil grant 310087/2006-6 and Jaime Sichman is partially supported by CNPq/Brazil grants 304605/2004-2, 482019/2004-2 and 506881/2004-1. Laurent Vercouter was partially supported by FAPESP grant 2005/02902-5.

Finding ways for agents to reach agreements in multiagent systems is an area of active research. One mechanism for achieving agreement is through the use of argumentation - where one agent tries to convince another agent of something during the course of some dialogue. Early examples of argumentation-based approaches to multiagent agreement include the work of Dignum et al. [7], Kraus [14],
Parsons and Jennings [16], Reed [23], Schroeder et al. [25] and Sycara [26].
The work of Walton and Krabbe [27], popularised in the multiagent systems community by Reed [23], has been particularly influential in the field of argumentation-based dialogue. This work influenced the field in a number of ways, perhaps most deeply in framing multi-agent interactions as dialogue games in the tradition of Hamblin [13]. Viewing dialogues in this way, as in [2, 21], provides a powerful framework for analysing the formal properties of dialogues, and for identifying suitable protocols under which dialogues can be conducted [18, 20]. The dialogue game view overlaps with work on conversation policies (see, for example, [6, 10]), but differs in considering the entire dialogue rather than dialogue segments.
In this paper, we extend the work of [18] by considering the role of relevance - the relationship between utterances in a dialogue. Relevance is a topic of increasing interest in argumentation-based dialogue because it relates to the scope that an agent has for applying strategic manoeuvering to obtain the outcomes that it requires [19, 22, 24]. Our work identifes the limits on such rhetorical manoeuvering, showing when it can and cannot have an effect.
We begin by introducing the formal system of argumentation that underpins our approach, as well as the corresponding terminology and notation, all taken from [2, 8, 17].
A dialogue is a sequence of messages passed between two or more members of a set of agents A. An agent α maintains a knowledge base, Σα, containing formulas of a propositional language L and having no deductive closure. Agent α also maintains the set of its past utterances, called the commitment store, CSα. We refer to this as an agent"s public knowledge, since it contains information that is shared with other agents. In contrast, the contents of Σα are private to α.
Note that in the description that follows, we assume that is the classical inference relation, that ≡ stands for logical equivalence, and we use Δ to denote all the information available to an agent. Thus in a dialogue between two agents α and β, Δα = Σα ∪ CSα ∪ CSβ, so the commitment store CSα can be loosely thought of as a subset of Δα consisting of the assertions that have been made public. In some dialogue games, such as those in [18] anything in CSα is either in Σα or can be derived from it. In other dialogue games, such as 1006 978-81-904262-7-5 (RPS) c 2007 IFAAMAS those in [2], CSα may contain things that cannot be derived from Σα.
Definition 2.1. An argument A is a pair (S, p) where p is a formula of L and S a subset of Δ such that (i) S is consistent; (ii) S p; and (iii) S is minimal, so no proper subset of S satisfying both (1) and (2) exists.
S is called the support of A, written S = Support(A) and p is the conclusion of A, written p = Conclusion(A). Thus we talk of p being supported by the argument (S, p).
In general, since Δ may be inconsistent, arguments in A(Δ), the set of all arguments which can be made from Δ, may conflict, and we make this idea precise with the notion of undercutting: Definition 2.2. Let A1 and A2 be arguments in A(Δ).
A1 undercuts A2 iff ∃¬p ∈ Support(A2) such that p ≡ Conclusion(A1).
In other words, an argument is undercut if and only if there is another argument which has as its conclusion the negation of an element of the support for the first argument.
To capture the fact that some beliefs are more strongly held than others, we assume that any set of beliefs has a preference order over it. We consider all information available to an agent, Δ, to be stratified into non-overlapping subsets Δ1, . . . , Δn such that beliefs in Δi are all equally preferred and are preferred over elements in Δj where i > j.
The preference level of a nonempty subset S ⊂ Δ, where different elements s ∈ S may belong to different layers Δi, is valued at the highest numbered layer which has a member in S and is referred to as level(S). In other words, S is only as strong as its weakest member. Note that the strength of a belief as used in this context is a separate concept from the notion of support discussed earlier.
Definition 2.3. Let A1 and A2 be arguments in A(Δ).
A1 is preferred to A2 according to Pref , A1 Pref A2, iff level(Support(A1)) > level(Support(A2)). If A1 is preferred to A2, we say that A1 is stronger than A2.
We can now define the argumentation system we will use: Definition 2.4. An argumentation system is a triple: A(Δ), Undercut, Pref such that: • A(Δ) is a set of the arguments built from Δ, • Undercut is a binary relation representing the defeat relationship between arguments, Undercut ⊆ A(Δ) × A(Δ), and • Pref is a pre-ordering on A(Δ) × A(Δ).
The preference order makes it possible to distinguish different types of relations between arguments: Definition 2.5. Let A1, A2 be two arguments of A(Δ). • If A2 undercuts A1 then A1 defends itself against A2 iff A1 Pref A2. Otherwise, A1 does not defend itself. • A set of arguments A defends A1 iff for every A2 that undercuts A1, where A1 does not defend itself against A2, then there is some A3 ∈ A such that A3 undercuts A2 and A2 does not defend itself against A3.
We write AUndercut,Pref to denote the set of all non-undercut arguments and arguments defending themselves against all their undercutting arguments. The set A(Δ) of acceptable arguments of the argumentation system A(Δ), Undercut, Pref is [1] the least fixpoint of a function F: A ⊆ A(Δ) F(A) = {(S, p) ∈ A(Δ) | (S, p) is defended by A} Definition 2.6. The set of acceptable arguments for an argumentation system A(Δ), Undercut, Pref is recursively defined as: A(Δ) = [ Fi≥0(∅) = AUndercut,Pref ∪ h[ Fi≥1(AUndercut,Pref ) i An argument is acceptable if it is a member of the acceptable set, and a proposition is acceptable if it is the conclusion of an acceptable argument.
An acceptable argument is one which is, in some sense, proven since all the arguments which might undermine it are themselves undermined.
Definition 2.7. If there is an acceptable argument for a proposition p, then the status of p is accepted, while if there is not an acceptable argument for p, the status of p is not accepted.
Argument A is said to affect the status of another argument A if changing the status of A will change the status of A .
Systems like those described in [2, 18], lay down sets of locutions that agents can make to put forward propositions and the arguments that support them, and protocols that define precisely which locutions can be made at which points in the dialogue. We are not concerned with such a level of detail here. Instead we are interested in the interplay between arguments that agents put forth. As a result, we will consider only that agents are allowed to put forward arguments. We do not discuss the detail of the mechanism that is used to put these arguments forward - we just assume that arguments of the form (S, p) are inserted into an agent"s commitment store where they are then visible to other agents.
We then have a typical definition of a dialogue: Definition 3.1. A dialogue D is a sequence of moves: m1, m2, . . . , mn.
A given move mi is a pair α, Ai where Ai is an argument that α places into its commitment store CSα.
Moves in an argumentation-based dialogue typically attack moves that have been made previously. While, in general, a dialogue can include moves that undercut several arguments, in the remainder of this paper, we will only consider dialogues that put forward moves that undercut at most one argument. For now we place no additional constraints on the moves that make up a dialogue. Later we will see how different restrictions on moves lead to different kinds of dialogue.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1007 The sequence of arguments put forward in the dialogue is determined by the agents who are taking part in the dialogue, but they are usually not completely free to choose what arguments they make. As indicated earlier, their choice is typically limited by a protocol. If we write the sequence of n moves m1, m2, . . . , mn as mn, and denote the empty sequence as m0, then we can define a profocol in the following way: Definition 3.2. A protocol P is a function on a sequence of moves mi in a dialogue D that, for all i ≥ 0, identifies a set of possible moves Mi+1 from which the mi+1th move may be drawn: P : mi → Mi+1 In other words, for our purposes here, at every point in a dialogue, a protocol determines a set of possible moves that agents may make as part of the dialogue. If a dialogue D always picks its moves m from the set M identified by protocol P, then D is said to conform to P.
Even if a dialogue conforms to a protocol, it is typically the case that the agent engaging in the dialogue has to make a choice of move - it has to choose which of the moves in M to make. This excercise of choice is what we refer to as an agent"s use of rhetoric (in its oratorical sense of influencing the thought and conduct of an audience). Some of our results will give a sense of how much scope an agent has to exercise rhetoric under different protocols.
As arguments are placed into commitment stores, and hence become public, agents can determine the relationships between them. In general, after several moves in a dialogue, some arguments will undercut others. We will denote the set of arguments {A1, A2, . . . , Aj} asserted after moves m1, m2, . . . , mj of a dialogue to be Aj - the relationship of the arguments in Aj can be described as an argumentation graph, similar to those described in, for example, [3, 4, 9]: Definition 3.3. An argumentation graph AG over a set of arguments A is a directed graph (V, E) such that every vertex v, v ∈ V denotes one argument A ∈ A, every argument A is denoted by one vertex v, and every directed edge e ∈ E from v to v denotes that v undercuts v .
We will use the term argument graph as a synonym for argumentation graph.
Note that we do not require that the argumentation graph is connected. In other words the notion of an argumentation graph allows for the representation of arguments that do not relate, by undercutting or being undercut, to any other arguments (we will come back to this point very shortly).
We adapt some standard graph theoretic notions in order to describe various aspects of the argumentation graph. If there is an edge e from vertex v to vertex v , then v is said to be the parent of v and v is said to be the child of v.
In a reversal of the usual notion, we define a root of an argumentation graph1 as follows: Definition 3.4. A root of an argumentation graph AG = (V, E) is a node v ∈ V that has no children.
Thus a root of a graph is a node to which directed edges may be connected, but from which no directed edges connect to other nodes. Thus a root is a node representing an 1 Note that we talk of a root rather than the root - as defined, an argumentation graph need not be a tree. v v" Figure 1: An example argument graph argument that is undercut, but which itself does no undercutting. Similarly: Definition 3.5. A leaf of an argumentation graph AG = (V, E) is a node v ∈ V that has no parents.
Thus a leaf in an argumentation graph represents an argument that undercuts another argument, but does no undercutting. Thus in Figure 1, v is a root, and v is a leaf. The reason for the reversal of the usual notions of root and leaf is that, as we shall see, we will consider dialogues to construct argumentation graphs from the roots (in our sense) to the leaves. The reversal of the terminology means that it matches the natural process of tree construction.
Since, as described above, argumentation graphs are allowed to be not connected (in the usual graph theory sense), it is helpful to distinguish nodes that are connected to other nodes, in particular to the root of the tree. We say that node v is connected to node v if and only if there is a path from v to v . Since edges represent undercut relations, the notion of connectedness between nodes captures the influence that one argument may have on another: Proposition 3.1. Given an argumentation graph AG, if there is any argument A, denoted by node v that affects the status of another argument A , denoted by v , then v is connected to v . The converse does not hold.
Proof. Given Definitions 2.5 and 2.6, the only ways in which A can affect the status of A is if A either undercuts A , or if A undercuts some argument A that undercuts A , or if A undercuts some A that undercuts some A that undercuts A , and so on. In all such cases, a sequence of undercut relations relates the two arguments, and if they are both in an argumentation graph, this means that they are connected.
Since the notion of path ignores the direction of the directed arcs, nodes v and v are connected whether the edge between them runs from v to v or vice versa. Since A only undercuts A if the edge runs from v to v , we cannot infer that A will affect the status of A from information about whether or not they are connected.
The reason that we need the concept of the argumentation graph is that the properties of the argumentation graph tell us something about the set of arguments A the graph represents. When that set of arguments is constructed through a dialogue, there is a relationship between the structure of the argumentation graph and the protocol that governs the dialogue. It is the extent of the relationship between structure and protocol that is the main subject of this paper. To study this relationship, we need to establish a correspondence between a dialogue and an argumentation graph. Given the definitions we have so far, this is simple: Definition 3.6. A dialogue D, consisting of a sequence of moves mn, and an argument graph AG = (V, E) correspond to one another iff ∀m ∈ mn, the argument Ai that
is advanced at move mi is represented by exactly one node v ∈ V , and ∀v ∈ V , v represents exactly one argument Ai that has been advanced by a move m ∈ mn.
Thus a dialogue corresponds to an argumentation graph if and only if every argument made in the dialogue corresponds to a node in the graph, and every node in the graph corresponds to an argument made in the dialogue. This one-toone correspondence allows us to consider each node v in the graph to have an index i which is the index of the move in the dialogue that put forward the argument which that node represents. Thus we can, for example, refer to the third node in the argumentation graph, meaning the node that represents the argument put forward in the third move of the dialogue.
Most work on dialogues is concerned with what we might call coherent dialogues, that is dialogues in which the participants are, as in the work of Walton and Krabbe [27], focused on resolving some question through the dialogue2 To capture this coherence, it seems we need a notion of relevance to constrain the statements made by agents. Here we study three notions of relevance: Definition 4.1. Consider a dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG. The move mi+1, i > 1, is said to be relevant if one or more of the following hold: R1 Making mi+1 will change the status of the argument denoted by the first node of AG.
R2 Making mi+1 will add a node vi+1 that is connected to the first node of AG.
R3 Making mi+1 will add a node vi+1 that is connected to the last node to be added to AG.
R2-relevance is the form of relevance defined by [3] in their study of strategic and tactical reasoning3 . R1-relevance was suggested by the notion used in [15], and though it differs somewhat from that suggested there, we believe it captures the essence of its predecessor.
Note that we only define relevance for the second move of the dialogue onwards because the first move is taken to identify the subject of the dialogue, that is, the central question that the dialogue is intended to answer, and hence it must be relevant to the dialogue, no matter what it is. In assuming this, we focus our attention on the same kind of dialogues as [18].
We can think of relevance as enforcing a form of parsimony on a dialogue - it prevents agents from making statements that do not bear on the current state of the dialogue.
This promotes efficiency, in the sense of limiting the number of moves in the dialogue, and, as in [15], prevents agents revealing information that they might better keep hidden.
Another form of parsimony is to insist that agents are not allowed to put forward arguments that will be undercut by arguments that have already been made during the dialogue.
We therefore distinguish such arguments. 2 See [11, 12] for examples of dialogues where this is not the case. 3 We consider such reasoning sub-types of rhetoric.
Definition 4.2. Consider a dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG. The move mi+1 and the argument it puts forward,
Ai+1, are both said to be pre-empted, if Ai+1 is undercut by some A ∈ Ai.
We use the term pre-empted because if such an argument is put forward, it can seem as though another agent anticipated the argument being made, and already made an argument that would render it useless. In the rest of this paper, we will only deal with protocols that permit moves that are relevant, in any of the senses introduced above, and are not allowed to be pre-empted. We call such protocols basic protocols, and dialogues carried out under such protocols basic dialogues.
The argument graph of a basic dialogue is somewhat restricted.
Proposition 4.1. Consider a basic dialogue D. The argumentation graph AG that corresponds to D is a tree with a single root.
Proof. Recall that Definition 3.3 requires only that AG be a directed graph. To show that it is a tree, we have to show that it is acyclic and connected.
That the graph is connected follows from the construction of the graph under a protocol that enforces relevance. If the notion of relevance is R3, each move adds a node that is connected to the previous node. If the notion of relevance is R2, then every move adds a node that is connected to the root, and thus is connected to some node in the graph. If the notion of relevance is R1, then every move has to change the status of the argument denoted by the root. Proposition 3.1 tells us that to affect the status of an argument A , the node v representing the argument A that is effecting the change has to be connected to v , the node representing A , and so it follows that every new node added as a result of an R1relevant move will be connected to the argumentation graph.
Thus AG is connected.
Since a basic dialogue does not allow moves that are preempted, every edge that is added during construction is directed from the node that is added to one already in the graph (thus denoting that the argument A denoted by the added node, v, undercuts the argument A denoted by the node to which the connection is made, v , rather than the other way around). Since every edge that is added is directed from the new node to the rest of the graph, there can be no cycles.
Thus AG is a tree.
To show that AG has a single root, consider its construction from the initial node. After m1 the graph has one node, v1 that is both a root and a leaf. After m2, the graph is two nodes connected by an edge, and v1 is now a root and not a leaf. v2 is a leaf and not a root. However the third node is added, the argument earlier in this proof demonstrates that there will be a directed edge from it to some other node, making it a leaf. Thus v1 will always be the only root. The ruling out of pre-empted moves means that v1 will never cease to be a root, and so the argumentation graph will always have one root.
Since every argumentation graph constructed by a basic dialogue is a tree with a single root, this means that the first node of every argumentation graph is the root.
Although these results are straightforward to obtain, they allow us to show how the notions of relevance are related.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1009 Proposition 4.2. Consider a basic dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG.
The converse does not hold.
The converse does not hold.
and not every move mi+1 that is R3-relevant is R1relevant Proof. For 1, consider how move mi+1 can satisfy R1.
Proposition 3.1 tells us that if Ai+1 can change the status of the argument denoted by the root v1 (which, as observed above, is the first node) of AG, then vi+1 must be connected to the root. This is precisely what is required to satisfy R2, and the relatiosnhip is proved to hold.
To see that the converse does not hold, we have to consider what it takes to change the status of r (since Proposition 3.1 tells us that connectedness is not enough to ensure a change of status - if it did, R1 and R2 relevance would coincide).
For mi+1 to change the status of the root, it will have to (1) make the argument A represented by r either unacceptable, if it were acceptable before the move, or (2) acceptable if it were unacceptable before the move. Given the definition of acceptability, it can achieve (1) either by directly undercutting the argument represented by r, in which case vi+1 will be directly connected to r by some edge, or by undercutting some argument A that is part of the set of non-undercut arguments defending A. In the latter case, vi+1 will be directly connected to the node representing A and by Proposition 4.1 to r. To achieve (2), vi+1 will have to undercut an argument A that is either currently undercutting A, or is undercutting an argument that would otherwise defend A.
Now, further consider that mi+1 puts forward an argument Ai+1 that undercuts the argument denoted by some node v , but this latter argument defends itself against Ai+1. In such a case, the set of acceptable arguments will not change, and so the status of Ar will not change. Thus a move that is R2-relevant need not be R1-relevant.
For 2, consider that mi+1 can satisfy R3 simply by adding a node that is connected to vi, the last node to be added to AG. By Proposition 4.1, it is connected to r and so is R2-relevant.
To see that the converse does not hold, consider that an R2-relevant move can connect to any node in AG.
The first part of 3 follows by a similar argument to that we just used - an R1-relevant move does not have to connect to vi, just to some v that is part of the graph - and the second part follows since a move that is R3-relevant may introduce an argument Ai+1 that undercuts the argument Ai put forward by the previous move (and so vi+1 is connected to vi), but finds that Ai defends itself against Ai+1, preventing a change of status at the root.
What is most interesting is not so much the results but why they hold, since this reveals some aspects of the interplay between relevance and the structure of argument graphs. For example, to restate a case from the proof of Proposition 4.2, a move that is R3-relevant by definition has to add a node to the argument graph that is connected to the last node that was added. Since a move that is R2-relevant can add a node that connects anywhere on an argument graph, any move that is R3-relevant will be R2-relevant, but the converse does not hold.
It turns out that we can exploit the interplay between structure and relevance that Propositions 4.1 and 4.2 have started to illuminate to establish relationships between the protocols that govern dialogues and the argument graphs constructed during such dialogues. To do this we need to define protocols in such a way that they refer to the structure of the graph. We have: Definition 4.3. A protocol is single-path if all dialogues that conform to it construct argument graphs that have only one branch.
Proposition 4.3. A basic protocol P is single-path if, for all i, the set of permitted moves Mi at move i are all R3relevant. The converse does not hold.
Proof. R3-relevance requires that every node added to the argument graph be connected to the previous node.
Starting from the first node this recursively constructs a tree with just one branch, and the relationship holds. The converse does not hold because even if one or more moves in the protocol are R1- or R2-relevant, it may be the case that, because of an agent"s rhetorical choice or because of its knowledge, every argument that is chosen to be put forward will undercut the previous argument and so the argument graph is a one-branch tree.
Looking for more complex kinds of protocol that construct more complex kinds of argument graph, it is an obvious move to turn to: Definition 4.4. A basic protocol is multi-path if all dialogues that conform to it can construct argument graphs that are trees.
But, on reflection, since any graph with only one branch is also a tree: Proposition 4.4. Any single-path protocol is an instance of a multi-path protocol. and, furthermore: Proposition 4.5. Any basic protocol P is multi-path.
Proof. Immediate from Proposition 4.1 So the notion of a multi-path protocol does not have much traction. As a result we distinguish multi-path protocols that permit dialogues that can construct trees that have more than one branch as bushy protocols. We then have: Proposition 4.6. A basic protocol P is bushy if, for some i, the set of permitted moves Mi at move i are all R1- or R2-relevant.
Proof. From Proposition 4.3 we know that if all moves are R3-relevant then we"ll get a tree with one branch, and from Proposition 4.1 we know that all basic protocols will build an argument graph that is a tree, so providing we exclude R3-relevant moves, we will get protocols that can build multi-branch trees.
Of course, since, by Proposition 4.2, any move that is R3relevant is R2-relevant and can quite possibly be R1-relevant (all that Proposition 4.2 tells us is that there is no guarantee that it will be), all that Proposition 4.6 tells us is that dialogues that conform to bushy protocols may have more than one branch. All we can do is to identify a bound on the number of branches: Proposition 4.7. Consider a basic dialogue D that includes m moves that are not R3-relevant, and has a corresponding argumentation graph AG. The number of branches in AG is less than or equal to m + 1.
Proof. Since it must connect a node to the last node added to AG, an R3-relevant move can only extend an existing branch. Since they do not have the same restriction,
R1 and R2-relevant moves may create a new branch by connecting to a node that is not the last node added. Every such move could create a new branch, and if they do, we will have m branches. If there were R3-relevant moves before any of these new-branch-creating moves, then these m branches are in addition to the initial branch created by the R3-relevant moves, and we have a maximum of m + 1 possible branches.
We distinguish bushy protocols from multi-path protocols, and hence R1- and R2-relevance from R3-relevance, because of the kinds of dialogue that R3-relevance enforces. In a dialogue in which all moves must be R3-relevant, the argumentation graph has a single branch - the dialogue consists of a sequence of arguments each of which undercuts the previous one and the last move to be made is the one that settles the dialogue. This, as we will see next, means that such a dialogue only allows a subset of all the moves that would otherwise be possible.
The above discussion of the difference between dialogues carried out under single-path and bushy protocols brings us to the consideration of what [18] called predeterminism, but we now prefer to describe using the term completeness. The idea of predeterminism, as described in [18], captures the notion that, under some circumstances, the result of a dialogue can be established without actually having the dialogue - the agents have sufficiently little room for rhetorical manoeuver that were one able to see the contents of all the Σi of all the αi ∈ A, one would be able to identify the outcome of any dialogue on a given subject4 . We develop this idea by considering how the argument graphs constructed by dialogues under different protocols compare to benchmark complete dialogues. We start by developing ideas of what complete might mean. One reasonable definition is that: Definition 5.1. A basic dialogue D between the set of agents A with a corresponding argumentation graph AG is topic-complete if no agent can construct an argument A that undercuts any argument A represented by a node in AG.
The argumentation graph constructed by a topic-complete dialogue is called a topic-complete argumentation graph and is denoted AG(D)T . 4 Assuming that the Σi do not change during the dialogue, which is the usual assumption in this kind of dialogue.
A dialogue is topic-complete when no agent can add anything that is directly connected to the subject of the dialogue. Some protocols will prevent agents from making moves even though the dialogue is not topic-complete. To distinguish such cases we have: Definition 5.2. A basic dialogue D between the set of agents A with a corresponding argumentation graph AG is protocol-complete under a protocol P if no agent can make a move that adds a node to the argumentation graph that is permitted by P.
The argumentation graph constructed by a protocol-complete dialogue is called a protocol-complete argumentation graph and is denoted AG(D)P . Clearly: Proposition 5.1. Any dialogue D under a basic protocol P is protocol-complete if it is topic-complete. The converse does not hold in general.
Proof. If D is topic-complete, no agent can make a move that will extend the argumentation graph. This means that no agent can make a move that is permitted by a basic protocol, and so D is also protocol complete.
The converse does not hold since some basic dialogues (under a protocol that only permits R3-relevant moves, for example) will not permit certain moves (like the addition of a node that connects to the root of the argumentation graph after more than two moves) that would be allowed in a topiccomplete dialogue.
Corollary 5.1. For a basic dialogue D, AG(D)P is a sub-graph of AG(D)T .
Obviously, from the definition of a sub-graph, the converse of Corollary 5.1 does not hold in general.
The important distinction between topic- and protocolcompleteness is that the former is determined purely by the state of the dialogue - as captured by the argumentation graph - and is thus independent of the protocol, while the latter is determined entirely by the protocol. Any time that a dialogue ends in a state of protocol-completeness rather than topic completeness, it is ending when agents still have things to say but can"t because the protocol won"t allow them to.
With these definitions of completeness, our task is to relate topic-completeness - the property that ensures that agents can say everything that they have to say in a dialogue that is, in some sense, important - to the notions of relevance we have developed - which determine what agents are allowed to say. When we need very specific conditions to make protocol-complete dialogues topic-complete, it means that agents have lots of room for rhetorical maneouver when those conditions are not in force. That is there are many ways they can bring dialogues to a close before everything that can be said has been said. Where few conditions are required, or conditions are absent, then dialogues between agents with the same knowledge will always play out the same way, and rhetoric has no place. We have: Proposition 5.2. A protocol-complete basic dialogue D under a protocol which only allows R3-relevant moves will be topic-complete only when AG(D)T has a single branch in which the nodes are labelled in increasing order from the root.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1011 Proof. Given what we know about R3-relevance, the condition on AG(D)P having a single branch is obvious. This is not a sufficient condition on its own because certain protocols may prevent - through additional restrictions, like strict turn-taking in a multi-party dialogue - all the nodes in AG(D)T , which is not subject to such restrictions, being added to the graph. Only when AG(D)T includes the nodes in the exact order that the corresponding arguments are put forward is it necessary that a topic-complete argumentation graph be constructed.
Given Proposition 5.1, these are the conditions under which dialogues conducted under the notion of R3-relevance will always be predetermined, and given how restrictive the conditions are, such dialogues seem to have plenty of room for rhetoric to play a part.
To find similar conditions for dialogues composed of R1and R2-relevant moves, we first need to distinguish between them. We can do this in terms of the structure of the argumentation graph: Proposition 5.3. Consider a basic dialogue D, with argumentation graph AG which has root r denoting an argument A. If argument A , denoted by node v is an an R2relevant move m, m is not R1-relevant if and only if:
and r, and the argument denoted by v defends itself against the argument denoted by v ; or
affects the status of A, and the path from v to r has one or more nodes in common with the path from v to r.
Proof. For the first condition, consider that since AG is a tree, v is connected to r. Thus there is a series of undercut relations between A and A , and this corrresponds to a path through AG. If this path is the only branch in the tree, then A will affect the status of A unless the chain of affect is broken by an undercut that can"t change the status of the undercut argument because the latter defends itself.
For the second condition, as for the first, the only way that A cannot affect the status of A is if something is blocking its influence. If this is not due to defending against, it must be because there is some node u on the path that represents an argument whose status is fixed somehow, and that must mean that there is another chain of undercut relations, another branch of the tree, that is incident at u. Since this second branch denotes another chain of arguments, and these affect the status of the argument denoted by u, they must also affect the status of A. Any of these are the A in the condition.
So an R2-relevant move m is not R1-relevant if either its effect is blocked because an argument upstream is not strong enough, or because there is another line of argument that is currently determining the status of the argument at the root. This, in turn, means that if the effect is not due to defending against, then there is an alternative move that is R1-relevant - a move that undercuts A in the second condition above5 . We can now show 5 Though whether the agent in question can make such a move is another question.
Proposition 5.4. A protocol-complete basic dialogue D will always be topic-complete under a protocol which only includes R2-relevant moves and allows every R2-relevant move to be made.
The restriction on R2-relevant rules is exactly that for topiccompleteness, so a dialogue that has only R2-relevant moves will continue until every argument that any agent can make has been put forward. Given this, and what we revealed about R1-relevance in Proposition 5.3, we can see that: Proposition 5.5. A protocol-complete basic dialogue D under a protocol which only includes R1-relevant moves will be topic-complete if AG(D)T :
and v , denoting A , such that A undercuts A and A is stronger that A; and
indices and no node with degree greater than two is an odd number of arcs from a leaf node.
Proof. The first condition rules out the first condition in Proposition 5.3, and the second deals with the situation that leads to the second condition in Proposition 5.3. The second condition ensures that each branch is constructed in full before any new branch is added, and when a new branch is added, the argument that is undercut as part of the addition will be acceptable, and so the addition will change the status of the argument denoted by that node, and hence the root. With these conditions, every move required to construct AG(D)T will be permitted and so the dialogue will be topic-complete when every move has been completed.
The second part of this result only identifies one possible way to ensure that the second condition in Proposition 5.3 is met, so the converse of this result does not hold.
However, what we have is sufficient to answer the question about predetermination that we started with. For dialogues to be predetermined, every move that is R2-relevant must be made. In such cases every dialogue is topic complete. If we do not require that all R2-relevant moves are made, then there is some room for rhetoric - the way in which alternative lines of argument are presented becomes an issue. If moves are forced to be R3-relevant, then there is considerable room for rhetorical play.
This paper has studied the different ideas of relevance in argumentation-based dialogue, identifying the relationship between these ideas, and showing how they can impact the extent to which the way that agents choose moves in a dialogue - what some authors have called the strategy and tactics of a dialogue. This extends existing work on relvance, such as [3, 15] by showing how different notions of relevance can have an effect on the outcome of a dialogue, in particular when they render the outcome predetermined.
This connection extends the work of [18] which considered dialogue outcome, but stopped short of identifying the conditions under which it is predetermined.
There are two ways we are currently trying to extend this work, both of which will generalise the results and extend its applicability. First, we want to relax the restrictions that
we have imposed, the exclusion of moves that attack several arguments (without which the argument graph can be mulitply-connected) and the exclusion of pre-empted moves, without which the argument graph can have cycles.
Second, we want to extend the ideas of relevance to cope with moves that do not only add undercutting arguments, but also supporting arguments, thus taking account of bipolar argumentation frameworks [5].
Acknowledgments The authors are grateful for financial support received from the EC, through project IST-FP6-002307, and from the NSF under grants REC-02-19347 and NSF IIS-0329037. They are also grateful to Peter Stone for a question, now several years old, which this paper has finally answered.

We consider a multiagent system where each (distributed) agent locally perceives its environment, and we assume that some unexpected event occurs in that system. If each agent computes only locally its favoured hypothesis, it is only natural to assume that agents will seek to coordinate and refine their hypotheses by confronting their observations with other agents. If, in addition, the communication opportunities are severely constrained (for instance, agents can only communicate when they are close enough to some other agent), and dynamically changing (for instance, agents may change their locations), it becomes crucial to carefully design protocols that will allow agents to converge to some desired state of global consistency. In this paper we exhibit some sufficient conditions on the system dynamics and on the protocol/strategy structures that allow to guarantee that property, and we experimentally study some contexts where (some of) these assumptions are relaxed.
While problems of diagnosis are among the venerable classics in the AI tradition, their multiagent counterparts have much more recently attracted some attention. Roos and colleagues [8, 9] in particular study a situation where a number of distributed entities try to come up with a satisfying global diagnosis of the whole system. They show in particular that the number of messages required to establish this global diagnosis is bound to be prohibitive, unless the communication is enhanced with some suitable protocol. However, they do not put any restrictions on agents" communication options, and do not assume either that the system is dynamic.
The benefits of enhancing communication with supporting information to make convergence to a desired global state of a system more efficient has often been put forward in the literature. This is for instance one of the main idea underlying the argumentation-based negotiation approach [7], where the desired state is a compromise between agents with conflicting preferences. Many of these works however make the assumption that this approach is beneficial to start with, and study the technical facets of the problem (or instead emphasize other advantages of using argumentation).
Notable exceptions are the works of [3, 4, 2, 5], which studied in contexts different from ours the efficiency of argumentation.
The rest of the paper is as follows. Section 2 specifies the basic elements of our model, and Section 3 goes on to presenting the different protocols and strategies used by the agents to exchange hypotheses and observations. We put special attention at clearly emphasizing the conditions on the system dynamics and protocols/strategies that will be exploited in the rest of the paper. Section 4 details one of 998 978-81-904262-7-5 (RPS) c 2007 IFAAMAS the main results of the paper, namely the fact that under the aforementioned conditions, the constraints that we put on the topology will not prevent the convergence of the system towards global consistency, at the condition that no agent ever gets completely lost forever in the system, and that unlimited time is allowed for computation and argument exchange. While the conditions on protocols and strategies are fairly mild, it is also clear that these system requirements look much more problematic, even frankly unrealistic in critical situations where distributed approaches are precisely advocated. To get a clearer picture of the situation induced when time is a critical factor, we have set up an experimental framework that we introduce and discuss in Section 5.
The critical situation involves a number of agents aiming at escaping from a burning building. The results reported here show that the effectiveness of argument exchange crucially depends upon the nature of the building, and provide some insights regarding the design of optimal protocol for hypotheses refinement in this context.
We start by defining the basic elements of our system.
Environment Let O be the (potentially infinite) set of possible observations. We assume the sensors of our agents to be perfect, hence the observations to be certain. Let H be the set of hypotheses, uncertain and revisable. Let Cons(h, O) be the consistency relation, a binary relation between a hypothesis h ∈ H and a set of observations O ⊆ O. In most cases, Cons will refer to classical consistency relation, however, we may overload its meaning and add some additional properties to that relation (in which case we will mention it).
The environment may include some dynamics, and change over the course of time. We define below sequences of time points to deal with it: Definition 1 (Sequence of time points). A sequence of time points t1, t2, . . . , tn from t is an ordered set of time points t1, t2, . . . , tn such that t1 ≥ t and ∀i ∈ [1, n − 1], ti+1 ≥ ti.
Agent We take a system populated by n agents a1, . . . , an. Each agent is defined as a tuple F, Oi, hi , where: • F, the set of facts, common knowledge to all agents. • Oi ∈ 2O , the set of observations made by the agent so far. We assume a perfect memory, hence this set grows monotonically. • hi ∈ H, the favourite hypothesis of the agent.
A key notion governing the formation of hypotheses is that of consistency, defined below: Definition 2 (Consistency). We say that: • An agent is consistent (Cons(ai)) iff Cons(hi, Oi) (that is, its hypothesis is consistent with its observation set). • An agent ai consistent with a partner agent aj iff Cons(ai) and Cons(hi, Oj) (that is, this agent is consistent and its hypothesis can explain the observation set of the other agent). • Two agents ai and aj are mutually consistent (MCons(ai, aj)) iff Cons(ai, aj) and Cons(aj, ai). • A system is consistent iff ∀(i, j)∈[1, n]2 it is the case that MCons(ai, aj).
To ensure its consistency, each agent is equipped with an abstract reasoning machinery that we shall call the explanation function Eh. This (deterministic) function takes a set of observation and returns a single prefered hypothesis (2O → H). We assume h = Eh(O) to be consistent with O by definition of Eh, so using this function on its observation set to determine its favourite hypothesis is a sure way for the agent to achieve consistency. Note however that an hypothesis does not need to be generated by Eh to be consistent with an observation set. As a concrete example of such a function, and one of the main inspiration of this work, one can cite the Theorist reasoning system [6] -as long as it is coupled with a filter selecting a single prefered theory among the ones initially selected by Theorist.
Note also that hi may only be modified as a consequence of the application Eh. We refer to this as the autonomy of the agent: no other agent can directly impose a given hypothesis to an agent. As a consequence, only a new observation (being it a new perception, or an observation communicated by a fellow agent) can result in a modification of its prefered hypothesis hi (but not necessarily of course).
We finally define a property of the system that we shall use in the rest of the paper: Definition 3 (Bounded Perceptions). A system involves a bounded perception for agents iff ∃n0 s.t. ∀t| ∪N i=1 Oi| ≤ n0. (That is, the number of observations to be made by the agents in the system is not infinite.) Agent Cycle Now we need to see how these agents will evolve and interact in their environment. In our context, agents evolve in a dynamic environment, and we classicaly assume the following system cycle:
according to the defined rules of the system dynamics.
environment. These perceptions are typically partial (e.g. the agent can only see a portion of a map).
predictions, seek explanations for (potential) difference(s), refine their hypothesis, draw new conclusions.
hypotheses and observations with other agents through a defined protocol. Any agent can only be involved in one communication with another agent by step.
the models obtained from the previous steps and select an action. They can then modify the environment by executing it.
The communication of the agents will be further constrained by topological consideration. At a given time, an agent will only be able to communicate with a number of neighbours. Its connexions with these others agents may The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 999 evolve with its situation in the environment. Typically, an agent can only communicate with agents that it can sense, but one could imagine evolving topological constraints on communication based on a network of communications between agents where the links are not always active.
Communication In our system, agents will be able to communicate with each other. However, due to the aforementionned topological constraints, they will not be able to communicate with any agents at anytime. Who an agent can communicate with will be defined dynamically (for instance, this can be a consequence of the agents being close enough to get in touch).
We will abstractly denote by C(ai, aj, t) the communication property, in other words, the fact that agents ai and aj can communicate at time t (note that this relation is assumed to be symetric, but of course not transitive). We are now in a position to define two essential properties of our system.
Definition 4 (Temporal Path). There exists a temporal communication path at horizon tf (noted Ltf (aI , aJ )) between ai and aj iff there exists a sequence of time points t1, t2, . . . , tn from tf and a sequence of agents k1, k2, . . . , kn s.t. (i) C(aI , ak1 , t1), (ii) C(akn , aJ , tn+1), (iii) ∀i ∈ [1, n],
C(aki , aki+1 , ti) Intuitively, what this property says is that it is possible to find a temporal path in the future that would allow to link agent ai and aj via a sequence of intermediary agents. Note that the time points are not necessarily successive, and that the sequence of agents may involve the same agents several times.
Definition 5 (Temporal Connexity). A system is temporaly connex iff ∀t ∀(i, j)∈[1, n]2 Lt(ai, aj) In short, a temporaly connex system guarantees that any agent will be able to communicate with any other agents, no matter how long it might take to do so, at any time. To put it another way, it is never the case that an agent will be isolated for ever from another agent of the system.
We will next discuss the detail of how communication concretely takes place in our system. Remember that in this paper, we only consider the case of bilateral exchanges (an agent can only speak to a single other agent), and that we also assume that any agent can only engage in a single exchange in a given round.
In this section, we discuss the requirements of the interaction protocols that govern the exchange of messages between agents, and provide some example instantiation of such protocols. To clarify the presentation, we distinguish two levels: the local level, which is concerned with the regulation of bilateral exchanges; and the global level,which essentially regulates the way agents can actually engage into a conversation. At each level, we separate what is specified by the protocol, and what is left to agents" strategies.
Local Protocol and Strategies We start by inspecting local protocols and strategies that will regulate the communication between the agents of the system. As we limit ourselves to bilateral communication, these protocols will simply involve two agents. Such protocol will have to meet one basic requirement to be satisfying. • consistency (CONS)- a local protocol has to guarantee the mutual consistency of agents upon termination (which implies termination of course).
Figure 1: A Hypotheses Exchange Protocol [1] One example such protocol is the protocol described in [1] that is pictured in Fig. 1. To further illustrate how such protocol can be used by agents, we give some details on a possible strategy: upon receiving a hypothesis h1 (propose(h1) or counterpropose(h1)) from a1, agent a2 is in state 2 and has the following possible replies: counterexample (if the agent knows an example contradicting the hypothesis, or not explained by this hypothesis), challenge (if the agents lacks evidence to accept this hypothesis), counterpropose (if the agent agrees with the hypothesis but prefers another one), or accept (if it is indeed as good as its favourite hypothesis). This strategy guarantees, among other properties, the eventual mutual logical consistency of the involved agents [1].
Global Protocol The global protocol regulates the way bilateral exchanges will be initiated between agents. At each turn, agents will concurrently send one weighted request to communicate to other agents. This weight is a value measuring the agent"s willingness to converse with the targeted agent (in practice, this can be based on different heuristics, but we shall make some assumptions on agents" strategies, see below). Sending such a request is a kind of conditional commitment for the agent. An agent sending a weighted request commits to engage in conversation with the target if he does not receive and accept himself another request. Once all request have been received, each agent replies with either an acccept or a reject. By answering with an accept, an agent makes a full commitment to engage in conversation with the sender.
Therefore, it can only send one accept in a given round, as an agent can only participate in one conversation per time step.
When all response have been received, each agent receiving an accept can either initiate a conversation using the local protocol or send a cancel if it has accepted another request.
At the end of all the bilateral exchanges, the agents engaged in conversation are discarded from the protocol. Then each of the remaining agents resends a request and the process iterates until no more requests are sent.
Global Strategy We now define four requirements for the strategies used by agents, depending on their role in the protocol: two are concerned with the requestee role (how to decide who the
agent wishes to communicate with?), the other two with the responder role (how to decide which communication request to accept or not?). • Willingness to solve inconsistancies (SOLVE)-agents want to communicate with any other agents unless they know they are mutually consistent. • Focus on solving inconsistencies (FOCUS)-agents do not request communication with an agent with whom they know they are mutually consistent. • Willingness to communicate (COMM)-agents cannot refuse a weighted communication request, unless they have just received or send a request with a greater weight. • Commitment to communication request (REQU)agents cannot accept a weighted communication request if they have themselves sent a communication request with a greater weight. Therefore, they will not cancel their request unless they have received a communicational request with greater weight.
Now the protocol structure, together with the properties COMM+REQU, ensure that a request can only be rejected if its target agent engages in communication with another agent. Suppose indeed that agent ai wants to communicate with aj by sending a request with weight w. COMM guarantees that an agent receiving a weighted request will either accept this communication, accept a communication with a greater weight or wait for the answer to a request with a greater weight. This ensures that the request with maximal weight will be accepted and not cancelled (as REQU ensures that an agent sending a request can only cancel it if he accepts another request with greater weight). Therefore at least two agents will engage in conversation per round of the global protocol. As the protocol ensures that ai can resend its request while aj is not engaged in a conversation, there will be a turn in which aj must engage in a conversation, either with ai or another agent.
These requirements concern request sending and acceptation, but agents also need some strategy of weight attribution. We describe below an altruist strategy, used in our experiments. Being cooperative, an agent may want to know more of the communication wishes of other agents in order to improve the overall allocation of exchanges to agents. A context request step is then added to the global protocol. Before sending their chosen weighted request, agents attribute a weight to all agents they are prepared to communicate with, according to some internal factors. In the simplest case, this weight will be 1 for all agent with whom the agent is not sure of being mutually consistent (ensuring SOLVE), other agent being not considered for communication (ensuring FOCUS). The agent then sends a context request to all agents with whom communication is considered. This request also provides information about the sender (list of considered communications along with their weight). After reception of all the context requests, agents will either reply with a deny, iff they are already engaged in a conversation (in which case, the requesting agent will not consider communication with them anymore in this turn), or an inform giving the requester information about the requests it has sent and received. When all replies have been received, each agent can calculate the weight of all requests concerning it. It does so by substracting from the weight of its request the weight of all requests concerning either it or its target (that is, the final weight of the request from ai to aj is Wi,j = wi,j +wj,i − ( P k∈R(i)−{j} wi,k + P k∈S(i)−{j} wk,i + P k∈R(j)−{i} wj,k + P k∈S(j)−{i} wk,j) where wi,j is the weight of the request of ai to aj, R(i) is the set of indice of agents having received a request from ai and S(i) is the set of indice of agents having send a request to ai). It then finally sends a weighted request to the agents who maximise this weight (or wait for a request) as described in the global protocol.
GLOBAL CONSISTENCY In this section we will show that the requirements regarding protocols and strategies just discussed will be sufficient to ensure that the system will eventually converge towards global consistency, under some conditions. We first show that, if two agents are not mutually consistent at some time, then there will be necessarily a time in the future such that an agent will learn a new observation, being it because it is new for the system, or by learning it from another agent.
Lemma 1. Let S be a system populated by n agents a1, a2, ..., an, temporaly connex, and involving bounded perceptions for these agents. Let n1 be the sum of cardinalities of the intersection of pairwise observation sets. (n1 = P (i,j)∈[1,n]2 |Oi ∩ Oj|) Let n2 be the cardinality of the union of all agents" observations sets. (n2 = | ∪N i=1 Oi|).
If ¬MCons(ai, aj) at time t0, there is necessarily a time t > t0 s.t. either n1 or n2 will increase.
Proof. Suppose that there exist a time t0 and indices (i, j) s.t. ¬MCons(ai, aj). We will use mt0 = P (k,l)∈[1,n]2 εComm(ak, al, t0) where εComm(ak, al, t0) = 1 if ak and al have communicated at least once since t0, and 0 otherwise.
Temporal connexity guarantees that there exist t1, ..., tm+1 and k1, ..., km s.t. C(ai, ak1 , t1), C(akm , aj, tm+1), and ∀p ∈ [1, m], C(akp , akp+1 , tp). Clearly, if MCons(ai, ak1 ),
MCons(akm , aj) and ∀p, MCons(akp , akp+1 ), we have MCons(ai, aj) which contradicts our hypothesis (MCons being transitive, MCons(ai, ak1 )∧MCons(ak1 , ak2 ) implies that MCons(ai, ak2 ) and so on till MCons(ai, akm )∧ MCons(akm , aj) which implies MCons(ai, aj) ).
At least two agents are then necessarily inconsistent (¬MCons(ai, ak1 ), or ¬MCons(akm , aj), or ∃p0 t.q. ¬MCons(akp0 , akp0+1 )). Let ak and al be these two neighbours at a time t > t0 1 . The SOLVE property ensures that either ak or al will send a communication request to the other agent at time t . As shown before, this in turn ensures that at least one of these agents will be involved in a communication. Then there are two possibilities: (case i) ak and al communicate at time t . In this case, we know that ¬MCons(ak, al). This and the CONS property ensures that at least one of the agents must change its 1 Strictly speaking, the transitivity of MCons only ensure that ak and al are inconsistent at a time t ≥ t0 that can be different from the time t at which they can communicate. But if they become consistent between t and t (or inconsistent between t and t ), it means that at least one of them have changed its hypothesis between t and t , that is, after t0. We can then apply the reasoning of case iib.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1001 hypothesis, which in turn, since agents are autonomous, implies at least one exchange of observation. But then |Ok ∩Ol| is bound to increase: n1(t ) > n1(t0). (case ii) ak communicates with ap at time t . We then have again two possibilities: (case iia) ak and ap did not communicate since t0. But then εComm(ak, ap, t0) had value 0 and takes value 1. Hence mt0 increases. (case iib) ak and ap did communicate at some time t0 > t0. The CONS property of the protocol ensures that MCons(ak, ap) at that time. Now the fact that they communicate and FOCUS implies that at least one of them did change its hypothesis in the meantime. The fact that agents are autonomous implies in turn that a new observation (perceived or received from another agent) necessarily provoked this change. The latter case would ensure the existence of a time t > t0 and an agent aq s.t. either |Op ∩Oq| or |Ok ∩Oq| increases of 1 at that time (implying n1(t ) > n1(t0)). The former case means that the agent gets a new perception o at time t . If that observation was unknown in the system before, then n2(t ) > n2(t0). If some agent aq already knew this observation before, then either Op ∩ Oq or Ok ∩ Oq increases of 1 at time t (which implies that n1(t ) > n1(t0)).
Hence, ¬MCons(ai, aj) at time t0 guarantees that, either: −∃t > t0 t.q. n1(t ) > n1(t0); or −∃t > t0 t.q. n2(t ) > n2(t0); or −∃t > t0 t.q. mt0 increases of 1 at time t .
By iterating the reasoning with t (but keeping t0 as the time reference for mt0 ), we can eliminate the third case (mt0 is integer and bounded by n2 , which means that after a maximum of n2 iterations, we necessarily will be in one of two other cases.) As a result, we have proven that if ¬MCons(ai, aj) at time t0, there is necessarily a time t s.t. either n1 or n2 will increase.
Theorem 1 (Global consistency). Let S be a system populated by n agents a1, a2, ..., an, temporaly connex, and involving bounded perceptions for these agents. Let Cons(ai, aj) be a transitive consistency property. Then any protocol and strategies satisfying properties CONS, SOLVE,
FOCUS, COMM and REQU guarantees that the system will converge towards global consistency.
Proof. For the sake of contradiction, let us assume ∃I, J ∈ [1, N] s.t. ∀t, ∃t0 > t, t.q. ¬Cons(aI , aJ , t0).
Using the lemma, this implies that ∃t > t0 s.t. either n1(t ) > n1(t0) or n2(t ) > n2(t0). But we can apply the same reasoning taking t = t , which would give us t1 > t > t0 s.t. ¬Cons(aI , aJ , t1), which gives us t > t1 s.t. either n1(t ) > n1(t1) or n2(t ) > n2(t1). By successive iterations we can then construct a sequence t0, t1, ..., tn, which can be divided in two sub-sequences t0, t1, ...tn and t0 , t1 , ..., tn s.t. n1(t0) < n1(t1) < ... < n1(tn) and n2(t0 ) < n2(t1 ) < ... < n2(tn). One of these sub-sequences has to be infinite. However, n1(ti) and n2(ti ) are strictly growing, integer, and bounded, which implies that both are finite. Contradiction.
What the previous result essentially shows is that, in a system where no agent will be isolated from the rest of the agents for ever, only very mild assumptions on the protocols and strategies used by agents suffice to guarantee convergence towards system consistency in a finite amount of time (although it might take very long). Unfortunately, in many critical situations, it will not be possible to assume this temporal connexity. As distributed approaches as the one advocated in this paper are precisely often presented as a good way to tackle problems of reliability or problems of dependence to a center that are of utmost importance in these critical applications, it is certainly interesting to further explore how such a system would behave when we relax this assumption.
This experiment involves agents trying to escape from a burning building. The environment is described as a spatial grid with a set of walls and (thankfully) some exits. Time and space are considered discrete. Time is divided in rounds.
Agents are localised by their position on the spatial grid.
These agents can move and communicate with other agents.
In a round, an agent can move of one cell in any of the four cardinal directions, provided it is not blocked by a wall. In this application, agents communicate with any other agent (but, recall, a single one) given that this agent is in view, and that they have not yet exchanged their current favoured hypothesis. Suddenly, a fire erupts in these premises. From this moment, the fire propagates. Each round, for each cases where there is fire, the fire propagates in the four directions.
However, the fire cannot propagate through a wall. If the fire propagates in a case where an agent is positioned, that agent burns and is considered dead. It can of course no longer move nor communicate. If an agent gets to an exit, it is considered saved, and can no longer be burned. Agents know the environment and the rules governing the dynamics of this environment, that is, they know the map as well as the rules of fire propagation previously described. They also locally perceive this environment, but cannot see further than 3 cases away, in any direction. Walls also block the line of view, preventing agents from seeing behind them.
Within their sight, they can see other agents and whether or not the cases they see are on fire. All these perceptions are memorised.
We now show how this instantiates the abstract framework presented the paper. • O = {Fire(x, y, t), NoFire(x, y, t), Agent(ai, x, y, t)} Observations can then be positive (o ∈ P(O) iff ∃h ∈ H s.t. h |= o) or negative (o ∈ N(O) iff ∃h ∈ H s.t. h |= ¬o). • H={FireOrigin(x1, y1, t1)∧...∧FireOrigin(xl, yl, tl)} Hypotheses are conjunctions of FireOrigins. • Cons(h, O) consistency relation satisfies: - coherence : ∀o ∈ N(O), h |= ¬o. - completeness : ∀o ∈ P(O), h |= o. - minimality : For all h ∈ H, if h is coherent and complete for O, then h is prefered to h according to the preference relation (h ≤p h ).2 2 Selects first the minimal number of origins, then the most recent (least preemptive strategy [6]), then uses some arbitrary fixed ranking to discriminate ex-aequo. The resulting relation is a total order, hence minimality implies that there will be a single h s.t.Cons(O, h) for a given O. This in turn means that MCons(ai, aj) iff Cons(ai), Cons(aj), and hi = hj. This relation is then transitive and symmetric.
• Eh takes O as argument and returns min≤p of the coherent and complete hypothesis for O
We will classically (see e.g. [3, 4]) assess the effectiveness and efficiency of different interaction protocols.
Effectiveness of a protocol The proportion of agents surviving the fire over the initial number of agents involved in the experiment will determine the effectiveness of a given protocol. If this value is high, the protocol has been effective to propagate the information and/or for the agents to refine their hypotheses and determine the best way to the exit.
Efficiency of a protocol Typically, the use of supporting information will involve a communication overhead. We will assume here that the efficiency of a given protocol is characterised by the data flow induced by this protocol. In this paper we will only discuss this aspect wrt. local protocols. The main measure that we shall then use here is the mean total size of messages that are exchanged by agents per exchange (hence taking into account both the number of messages and the actual size of the messages, because it could be that messages happen to be very big, containing e.g. a large number of observations, which could counter-balance a low number of messages).
The chosen experimental settings are the following: • Environmental topology- Performances of information propagation are highly constrained by the environment topology. The perception skills of the agents depend on the openness of the environment. With a large number of walls the perceptions of agents are limited, and also the number of possible inter-agent communications, whereas an open environment will provide optimal possibilities of perception and information propagation. Thus, we propose a topological index (see below) as a common basis to charaterize the environments (maps) used during experimentations.
The topological index (TI) is the ratio of the number of cells that can be perceived by agents summed up from all possible positions, divided by the number of cells that would be perceived from the same positions but without any walls. (The closer to 1, the more open the environment). We shall also use two additional, more classical [10], measures: the characteristic path length3 (CPL) and the clustering coefficient4 (CC). • Number of agents- The propagation of information also depends on the initial number of agents involved during an experimentation. For instance, the more agents, the more potential communications there is.
This means that there will be more potential for propagation, but also that the bilateral exchange restriction will be more crucial. 3 The CPL is the median of the means of the shortest path lengths connecting each node to all other nodes. 4 characterising the isolation degree of a region of an environment in terms of acessibility (number of roads still usable to reach this region).
Map T.I. (%) C.P.L. C.C. 69-1 69,23 4,5 0,69 69-2 68,88 4,38 0,65 69-3 69,80 4,25 0,67 53-1 53,19 5,6 0,59 53-2 53,53 6,38 0,54 53-3 53,92 6,08 0,61 38-1 38,56 8,19 0,50 38-2 38,56 7,3 0,50 38-3 38,23 8,13 0,50 Table 1: Topological Characteristics of the Maps • Initial positions of the agents- Initial positions of the agents have a significant influence on the overall behavior of an instance of our system: being close from an exit will (in general) ease the escape.
We choose to realize experiments on three very different topological indexes (69% for open environments, 53% for mixed environments, and 38% for labyrinth-like environments).
Figure 2: Two maps (left: TI=69%, right TI=38%) We designed three different maps for each index (Fig. 2 shows two of them), containing the same maximum number of agents (36 agents max.) with a maximum density of one agent per cell, the same number of exits and a similar fire origin (e.g. starting time and position). The three differents maps of a given index are designed as follows. The first map is a model of an existing building floor. The second map has the same enclosure, exits and fire origin as the first one, but the number and location of walls are different (wall locations are designed by an heuristic which randomly creates walls on the spatial grid such that no fully closed rooms are created and that no exit is closed). The third map is characterised by geometrical enclosure in wich walls location is also designed with the aforementioned heuristic. Table 1 summarizes the different topological measures characterizing these different maps. It is worth pointing out that the values confirm the relevance of TI (maps with a high TI have a low CPL and a high CC. However the CPL and CC allows to further refine the difference between the maps, e.g. between 53-1 and 53-2).
For each triple of maps defined as above we conduct the same experiments. In each experiment, the society differs in terms of its initial proportion of involved agents, from 1% to 100%. This initial proportion represents the percentage of involved agents with regards to the possible maximum number of agents. For each map and each initial proportion,
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1003 we select randomly 100 different initial agents" locations.
For each of those different locations we execute the system one time for each different interaction protocol.
Effectiveness of Communication and Argumentation The first experiment that we set up aims at testing how effective is hypotheses exchange (HE), and in particular how the topological aspects will affect this effectiveness. In order to do so, we have computed the ratio of improvement offered by that protocol over a situation where agents could simply not communicate (no comm). To get further insights as to what extent the hypotheses exchange was really crucial, we also tested a much less elaborated protocol consisting of mere observation exchanges (OE). More precisely, this protocol requires that each agent stores any unexpected observation that it perceives, and agents simply exchange their respective lists of observations when they discuss. In this case, the local protocol is different (note in particular that it does not guarantee mutual consistency), but the global protocol remains the same (at the only exception that agents" motivation to communicate is to synchronise their list of observations, not their hypothesis). If this protocol is at best as effective as HE, it has the advantage of being more efficient (this is obvious wrt the number of messages which will be limited to 2, less straightforward as far as the size of messages is concerned, but the rough observation that the exchange of observations can be viewed as a flat version of the challenge is helpful to see this). The results of these experiments are reported in Fig. 3.
Figure 3: Comparative effectiveness ratio gain of protocols when the proportion of agents augments The first observation that needs to be made is that communication improves the effectiveness of the process, and this ratio increases as the number of agents grows in the system. The second lesson that we learn here is that closeness relatively makes communication more effective over non communication. Maps exhibiting a T.I. of 38% are constantly above the two others, and 53% are still slightly but significantly better than 69%. However, these curves also suggest, perhaps surprisingly, that HE outperforms OE in precisely those situations where the ratio gain is less important (the only noticeable difference occurs for rather open maps where T.I. is 69%). This may be explained as follows: when a map is open, agents have many potential explanation candidates, and argumentation becomes useful to discriminate between those. When a map is labyrinth-like, there are fewer possible explanations to an unexpected event.
Importance of the Global Protocol The second set of experiments seeks to evaluate the importance of the design of the global protocol. We tested our protocol against a local broadcast (LB) protocol. Local broadcast means that all the neighbours agents perceived by an agent will be involved in a communication with that agent in a given round -we alleviate the constraint of a single communication by agent. This gives us a rough upper bound upon the possible ratio gain in the system (for a given local protocol). Again, we evaluated the ratio gain induced by that LB over our classical HE, for the three different classes of maps. The results are reported in Fig. 4.
Figure 4: Ratio gain of local broadcast over hypotheses exchange Note to begin with that the ratio gain is 0 when the proportion of agents is 5%, which is easily explained by the fact that it corresponds to situations involving only two agents.
We first observe that all classes of maps witness a ratio gain increasing when the proportion of agents augments: the gain reaches 10 to 20%, depending on the class of maps considered. If one compares this with the improvement reported in the previous experiment, it appears to be of the same magnitude. This illustrates that the design of the global protocol cannot be ignored, especially when the proportion of agents is high. However, we also note that the effectiveness ratio gain curves have very different shapes in both cases: the gain induced by the accuracy of the local protocol increases very quickly with the proportion of agents, while the curve is really smooth for the global one.
Now let us observe more carefully the results reported here: the curve corresponding to a TI of 53% is above that corresponding to 38%. This is so because the more open a map, the more opportunities to communicate with more than one agent (and hence benefits from broadcast).
However, we also observe that curve for 69% is below that for 53%. This is explained as follows: in the case of 69%, the potential gain to be made in terms of surviving agents is much lower, because our protocols already give rather efficient outcomes anyway (quickly reaching 90%, see Fig. 3).
A simple rule of thumb could be that when the number of agents is small, special attention should be put on the local
protocol, whereas when that number is large, one should carefully design the global one (unless the map is so open that the protocol is already almost optimally efficient).
Efficiency of the Protocols The final experiment reported here is concerned with the analysis of the efficiency of the protocols. We analysis here the mean size of the totality of the messages that are exchanged by agents (mean size of exchanges, for short) using the following protocols: HE, OE, and two variant protocols. The first one is an intermediary restricted hypotheses exchange protocol (RHE). RHE is as follows: it does not involve any challenge nor counter-propose, which means that agents cannot switch their role during the protocol (this differs from RE in that respect). In short, RHE allows an agent to exhaust its partner"s criticism, and eventually this partner will come to adopt the agent"s hypothesis. Note that this means that the autonomy of the agent is not preserved here (as an agent will essentially accept any hypothesis it cannot undermine), with the hope that the gain in efficiency will be significant enough to compensate a loss in effectiveness. The second variant protocol is a complete observation exchange protocol (COE). COE uses the same principles as OE, but includes in addition all critical negative examples (nofire) in the exchange (thus giving all examples used as arguments by the hypotheses exchanges protocol), hence improving effectiveness. Results for map 69-1 are shown on Fig. 5.
Figure 5: Mean size of exchanges First we can observe the fact that the ordering of the protocols, from the least efficient to the most efficient, is COE,
HE, RHE and then OE. HE being more efficient than COE proves that the argumentation process gains efficiency by selecting when it is needed to provide negative example, which have less impact that positive ones in our specific testbed.
However, by communicating hypotheses before eventually giving observation to support it (HE) instead of directly giving the most crucial observations (OE), the argumentation process doubles the size of data exchanges. It is the cost for ensuring consistency at the end of the exchange (a property that OE does not support). Also significant is the fact the the mean size of exchanges is slightly higher when the number of agents is small. This is explained by the fact that in these cases only a very few agents have relevant informations in their possession, and that they will need to communicate a lot in order to come up with a common view of the situation. When the number of agents increases, this knowledge is distributed over more agents which need shorter discussions to get to mutual consistency. As a consequence, the relative gain in efficiency of using RHE appears to be better when the number of agents is small: when it is high, they will hardly argue anyway. Finally, it is worth noticing that the standard deviation for these experiments is rather high, which means that the conversation do not converge to any stereotypic pattern.
This paper has investigated the properties of a multiagent system where each (distributed) agent locally perceives its environment, and tries to reach consistency with other agents despite severe communication restrictions. In particular we have exhibited conditions allowing convergence, and experimentally investigated a typical situation where those conditions cannot hold. There are many possible extensions to this work, the first being to further investigate the properties of different global protocols belonging to the class we identified, and their influence on the outcome. There are in particular many heuristics, highly dependent on the context of the study, that could intuitively yield interesting results (in our study, selecting the recipient on the basis of what can be inferred from his observed actions could be such a heuristic). One obvious candidate for longer term issues concern the relaxation of the assumption of perfect sensing.

Automated negotiation has been received increasing attention in multi-agent systems, and a number of frameworks have been proposed in different contexts ([1, 2, 3, 5, 10, 11, 13, 14], for instance). Negotiation usually proceeds in a series of rounds and each agent makes a proposal at every round. An agent that received a proposal responds in two ways. One is a critique which is a remark as to whether or not (parts of) the proposal is accepted. The other is a counter-proposal which is an alternative proposal made in response to a previous proposal [13].
To see these proposals in one-to-one negotiation, suppose the following negotiation dialogue between a buyer agent B and a seller agent S. (Bi (or Si) represents an utterance of B (or S) in the i-th round.) B1: I want to buy a personal computer of the brand b1, with the specification of CPU:1GHz, Memory:512MB,
HDD: 80GB, and a DVD-RW driver. I want to get it at the price under 1200 USD.
S1: We can provide a PC with the requested specification if you pay for it by cash. In this case, however, service points are not added for this special discount.
B2: I cannot pay it by cash.
S2: In a normal price, the requested PC costs 1300 USD.
B3: I cannot accept the price. My budget is under 1200 USD.
S3: We can provide another computer with the requested specification, except that it is made by the brand b2.
The price is exactly 1200 USD.
B4: I do not want a PC of the brand b2. Instead, I can downgrade a driver from DVD-RW to CD-RW in my initial proposal.
S4: Ok, I accept your offer.
In this dialogue, in response to the opening proposal B1, the counter-proposal S1 is returned. In the rest of the dialogue, B2, B3, S4 are critiques, while S2, S3, B4 are counterproposals.
Critiques are produced by evaluating a proposal in a knowledge base of an agent. In contrast, making counter-proposals involves generating an alternative proposal which is more favorable to the responding agent than the original one.
It is known that there are two ways of producing counterproposals: extending the initial proposal or amending part of the initial proposal. According to [13], the first type appears in the dialogue: A: I propose that you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. The second type is in the dialogue: A: I propose that I provide you with service Y if you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. A negotiation proceeds by iterating such give-andtake dialogues until it reaches an agreement/disagreement.
In those dialogues, agents generate (counter-)proposals by reasoning on their own goals or objectives. The objective of the agent A in the above dialogues is to obtain service X. The agent B proposes conditions to provide the service. In the process of negotiation, however, it may happen that agents are obliged to weaken or change their initial goals to reach a negotiated compromise. In the dialogue of 1022 978-81-904262-7-5 (RPS) c 2007 IFAAMAS a buyer agent and a seller agent presented above, a buyer agent changes its initial goal by downgrading a driver from DVD-RW to CD-RW. Such behavior is usually represented as specific meta-knowledge of an agent or specified as negotiation protocols in particular problems. Currently, there is no computational logic for automated negotiation which has general inference rules for producing (counter-)proposals.
The purpose of this paper is to mechanize a process of building (counter-)proposals in one-to-one negotiation dialogues. We suppose an agent who has a knowledge base represented by a logic program. We then introduce methods for generating three different types of proposals. First, we use the technique of extended abduction in artificial intelligence [8, 15] to construct a conditional proposal as an extension of the original one. Second, we use the technique of relaxation in cooperative query answering for databases [4, 6] to construct a neighborhood proposal as an amendment of the original one. Third, combining extended abduction and relaxation, conditional neighborhood proposals are constructed as amended extensions of the original proposal. We develop a negotiation protocol between two agents based on the exchange of these counter-proposals and critiques. We also provide procedures for computing proposals in logic programming.
This paper is organized as follows. Section 2 introduces a logical framework used in this paper. Section 3 presents methods for constructing proposals, and provides a negotiation protocol. Section 4 provides methods for computing proposals in logic programming. Section 5 discusses related works, and Section 6 concludes the paper.
Logic programs considered in this paper are extended disjunctive programs (EDP) [7]. An EDP (or simply a program) is a set of rules of the form: L1 ; · · · ; Ll ← Ll+1 , . . . , Lm, not Lm+1 , . . . , not Ln (n ≥ m ≥ l ≥ 0) where each Li is a positive/negative literal, i.e., A or ¬A for an atom A, and not is negation as failure (NAF). not L is called an NAF-literal. The symbol ; represents disjunction. The left-hand side of the rule is the head, and the right-hand side is the body. For each rule r of the above form, head(r), body+ (r) and body− (r) denote the sets of literals {L1, . . . , Ll}, {Ll+1, . . . , Lm}, and {Lm+1, . . . , Ln}, respectively. Also, not body− (r) denotes the set of NAF-literals {not Lm+1, . . . , not Ln}. A disjunction of literals and a conjunction of (NAF-)literals in a rule are identified with its corresponding sets of literals. A rule r is often written as head(r) ← body+ (r), not body− (r) or head(r) ← body(r) where body(r) = body+ (r)∪not body− (r).
A rule r is disjunctive if head(r) contains more than one literal. A rule r is an integrity constraint if head(r) = ∅; and r is a fact if body(r) = ∅. A program is NAF-free if no rule contains NAF-literals. Two rules/literals are identified with respect to variable renaming. A substitution is a mapping from variables to terms θ = {x1/t1, . . . , xn/tn}, where x1, . . . , xn are distinct variables and each ti is a term distinct from xi. Given a conjunction G of (NAF-)literals, Gθ denotes the conjunction obtained by applying θ to G. A program, rule, or literal is ground if it contains no variable.
A program P with variables is a shorthand of its ground instantiation Ground(P), the set of ground rules obtained from P by substituting variables in P by elements of its Herbrand universe in every possible way.
The semantics of an EDP is defined by the answer set semantics [7]. Let Lit be the set of all ground literals in the language of a program. Suppose a program P and a set of literals S(⊆ Lit). Then, the reduct P S is the program which contains the ground rule head(r) ← body+ (r) iff there is a rule r in Ground(P) such that body− (r)∩S = ∅.
Given an NAF-free EDP P, Cn(P) denotes the smallest set of ground literals which is (i) closed under P, i.e., for every ground rule r in Ground(P), body(r) ⊆ Cn(P) implies head(r) ∩ Cn(P) = ∅; and (ii) logically closed, i.e., it is either consistent or equal to Lit. Given an EDP P and a set S of literals, S is an answer set of P if S = Cn(P S ). A program has none, one, or multiple answer sets in general.
An answer set is consistent if it is not Lit. A program P is consistent if it has a consistent answer set; otherwise, P is inconsistent.
Abductive logic programming [9] introduces a mechanism of hypothetical reasoning to logic programming. An abductive framework used in this paper is the extended abduction introduced by Inoue and Sakama [8, 15]. An abductive program is a pair P, H where P is an EDP and H is a set of literals called abducibles. When a literal L ∈ H contains variables, any instance of L is also an abducible.
An abductive program P, H is consistent if P is consistent. Throughout the paper, abductive programs are assumed to be consistent unless stated otherwise. Let G = L1, . . . , Lm, not Lm+1, . . . , not Ln be a conjunction, where all variables in G are existentially quantified at the front and range-restricted, i.e., every variable in Lm+1, . . . , Ln appears in L1, . . . , Lm. A set S of ground literals satisfies the conjunction G if { L1θ, . . . , Lmθ } ⊆ S and { Lm+1θ, . . . , Lnθ }∩ S = ∅ for some ground instance Gθ with a substitution θ.
Let P, H be an abductive program and G a conjunction as above. A pair (E, F) is an explanation of an observation G in P, H if1
and F ⊆ H ∩ P.
When (P \ F) ∪ E has an answer set S satisfying the above three conditions, S is called a belief set of an abductive program P, H satisfying G (with respect to (E, F)). Note that if P has a consistent answer set S satisfying G, S is also a belief set of P, H satisfying G with respect to (E, F) = (∅, ∅). Extended abduction introduces/removes hypotheses to/from a program to explain an observation.
Note that normal abduction (as in [9]) considers only introducing hypotheses to explain an observation. An explanation (E, F) of an observation G is called minimal if for any explanation (E , F ) of G, E ⊆ E and F ⊆ F imply E = E and F = F.
Example 2.1. Consider the abductive program P, H : P : flies(x) ← bird(x), not ab(x) , ab(x) ← broken-wing(x) , bird(tweety) ← , bird(opus) ← , broken-wing(tweety) ← .
H : broken-wing(x) .
The observation G = flies(tweety) has the minimal explanation (E, F) = (∅, {broken-wing(tweety)}). 1 This defines credulous explanations [15]. Skeptical explanations are used in [8].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1023
We suppose an agent who has a knowledge base represented by an abductive program P, H . A program P consists of two types of knowledge, belief B and desire D, where B represents objective knowledge of an agent, while D represents subjective knowledge in general. We define P = B ∪ D, but do not distinguish B and D if such distinction is not important in the context. In contrast, abducibles H are used for representing permissible conditions to make a compromise in the process of negotiation.
Definition 3.1. A proposal G is a conjunction of literals and NAF-literals: L1, . . . , Lm, not Lm+1, . . . , not Ln where every variable in G is existentially quantified at the front and range-restricted. In particular, G is called a critique if G = accept or G = reject where accept and reject are the reserved propositions. A counter-proposal is a proposal made in response to a proposal.
Definition 3.2. A proposal G is accepted in an abductive program P, H if P has an answer set satisfying G.
When a proposal is not accepted, abduction is used for seeking conditions to make it acceptable.
Definition 3.3. Let P, H be an abductive program and G a proposal. If (E, F) is a minimal explanation of Gθ for some substitution θ in P, H , the conjunction G : Gθ, E, not F is called a conditional proposal (for G), where E, not F represents the conjunction: A1, . . . , Ak, not Ak+1, . . . , not Al for E = {A1, . . . , Ak} and F = { Ak+1, . . . , Al }.
Proposition 3.1. Let P, H be an abductive program and G a proposal. If G is a conditional proposal, there is a belief set S of P, H satisfying G .
Proof. When G = Gθ, E, not F, (P \ F) ∪ E has a consistent answer set S satisfying Gθ and E ∩ F = ∅. In this case, S satisfies Gθ, E, not F.
A conditional proposal G provides a minimal requirement for accepting the proposal G. If Gθ has multiple minimal explanations, several conditional proposals exist accordingly.
When (E, F) = (∅, ∅), a conditional proposal is used as a new proposal made in response to the proposal G.
Example 3.1. An agent seeks a position of a research assistant at the computer department of a university with the condition that the salary is at least 50,000 USD per year.
The agent makes his/her request as the proposal:2 G = assist(compt dept), salary(x), x ≥ 50, 000.
The university has the abductive program P, H : P : salary(40, 000) ← assist(compt dept), not has PhD, salary(60, 000) ← assist(compt dept), has PhD, salary(50, 000) ← assist(math dept), salary(55, 000) ← system admin(compt dept), 2 For notational convenience, we often include mathematical (in)equations in proposals/programs. They are written by literals, for instance, x ≥ y by geq(x, y) with a suitable definition of the predicate geq. employee(x) ← assist(x), employee(x) ← system admin(x), assist(compt dept); assist(math dept) ; system admin(compt dept) ←,
H : has PhD, where available positions are represented by disjunction.
According to P, the base salary of a research assistant at the computer department is 40,000 USD, but if he/she has PhD, it is 60,000 USD. In this case, (E, F) = ({has PhD}, ∅) becomes the minimal explanation of Gθ = assist(compt dept), salary(60, 000) with θ = { x/60, 000 }. Then, the conditional proposal made by the university becomes assist(compt dept), salary(60, 000), has PhD .
When a proposal is unacceptable, an agent tries to construct a new counter-proposal by weakening constraints in the initial proposal. We use techniques of relaxation for this purpose. Relaxation is used as a technique of cooperative query answering in databases [4, 6]. When an original query fails in a database, relaxation expands the scope of the query by relaxing the constraints in the query. This allows the database to return neighborhood answers which are related to the original query. We use the technique for producing proposals in the process of negotiation.
Definition 3.4. Let P, H be an abductive program and G a proposal. Then, G is relaxed to G in the following three ways: Anti-instantiation: Construct G such that G θ = G for some substitution θ.
Dropping conditions: Construct G such that G ⊂ G.
Goal replacement: If G is a conjunction G1, G2, where G1 and G2 are conjunctions, and there is a rule L ← G1 in P such that G1θ = G1 for some substitution θ, then build G as Lθ, G2. Here, Lθ is called a replaced literal.
In each case, every variable in G is existentially quantified at the front and range-restricted.
Anti-instantiation replaces constants (or terms) with fresh variables. Dropping conditions eliminates some conditions in a proposal. Goal replacement replaces the condition G1 in G with a literal Lθ in the presence of a rule L ← G1 in P under the condition G1θ = G1. All these operations generalize proposals in different ways. Each G obtained by these operations is called a relaxation of G. It is worth noting that these operations are also used in the context of inductive generalization [12]. The relaxed proposal can produce new offers which are neighbor to the original proposal.
Definition 3.5. Let P, H be an abductive program and G a proposal.
P has an answer set S which satisfies G θ for some substitution θ and G θ = G, G θ is called a neighborhood proposal by anti-instantiation.
If P has an answer set S which satisfies G θ for some substitution θ, G θ is called a neighborhood proposal by dropping conditions.
For a replaced literal L ∈ G and a rule H ← B in P such that L = Hσ and (G \ {L}) ∪ Bσ = G for some substitution σ, put G = (G \ {L}) ∪ Bσ. If P has an answer set S which satisfies G θ for some substitution θ, G θ is called a neighborhood proposal by goal replacement.
Example 3.2. (cont. Example 3.1) Given the proposal G = assist(compt dept), salary(x), x ≥ 50, 000, • G1 = assist(w), salary(x), x ≥ 50, 000 is produced by substituting compt dept with a variable w. As G1θ1 = assist(math dept), salary(50, 000) with θ1 = { w/math dept } is satisfied by an answer set of P, G1θ1 becomes a neighborhood proposal by anti-instantiation. • G2 = assist(compt dept), salary(x) is produced by dropping the salary condition x ≥ 50, 000. As G2θ2 = assist(compt dept), salary(40, 000) with θ2 = { x/40, 000 } is satisfied by an answer set of P, G2θ2 becomes a neighborhood proposal by dropping conditions. • G3 = employee(compt dept), salary(x), x ≥ 50, 000 is produced by replacing assist(compt dept) with employee(compt dept) using the rule employee(x) ← assist(x) in P. By G3 and the rule employee(x) ← system admin(x) in P, G3 = sys admin(compt dept), salary(x), x ≥ 50, 000 is produced. As G3 θ3 = sys admin(compt dept), salary(55, 000) with θ3 = { x/55, 000 } is satisfied by an answer set of P, G3 θ3 becomes a neighborhood proposal by goal replacement.
Finally, extended abduction and relaxation are combined to produce conditional neighborhood proposals.
Definition 3.6. Let P, H be an abductive program and G a proposal.
or dropping conditions. If (E, F) is a minimal explanation of G θ(= G) for some substitution θ, the conjunction G θ, E, not F is called a conditional neighborhood proposal by anti-instantiation/dropping conditions.
Suppose G as in Definition 3.5(3). If (E, F) is a minimal explanation of G θ for some substitution θ, the conjunction G θ, E, not F is called a conditional neighborhood proposal by goal replacement.
A conditional neighborhood proposal reduces to a neighborhood proposal when (E, F) = (∅, ∅).
A negotiation protocol defines how to exchange proposals in the process of negotiation. This section presents a negotiation protocol in our framework. We suppose one-to-one negotiation between two agents who have a common ontology and the same language for successful communication.
Definition 3.7. A proposal L1, ..., Lm, not Lm+1, ..., not Ln violates an integrity constraint ← body+ (r), not body− (r) if for any substitution θ, there is a substitution σ such that body+ (r)σ ⊆ { L1θ, . . . , Lmθ }, body− (r)σ∩{ L1θ, . . . , Lmθ } = ∅, and body− (r)σ ⊆ { Lm+1θ, . . . , Lnθ }.
Integrity constraints are conditions which an agent should satisfy, so that they are used to explain why an agent does not accept a proposal.
A negotiation proceeds in a series of rounds. Each i-th round (i ≥ 1) consists of a proposal Gi
Ag1 and another proposal Gi
Definition 3.8. Let P1, H1 be an abductive program of an agent Ag1 and Gi
round. A critique set of Ag1 (at the i-th round) is a set CSi 1(P1, Gj 2) = CSi−1
constraint in P1 and Gj
where j = i − 1 or i, and CS0
2) = CS1
2) = ∅.
A critique set of an agent Ag1 accumulates integrity constraints which are violated by proposals made by another agent Ag2. CSi 2(P2, Gj 1) is defined in the same manner.
Definition 3.9. Let Pk, Hk be an abductive program of an agent Agk and Gj a proposal, which is not a critique, made by any agent at the j(≤ i)-th round. A negotiation set of Agk (at the i-th round) is a triple NSi k = (Si c, Si n, Si cn), where Si c is the set of conditional proposals, Si n is the set of neighborhood proposals, and Si cn is the set of conditional neighborhood proposals, produced by Gj and Pk, Hk .
A negotiation set represents the space of possible proposals made by an agent. Si x (x ∈ {c, n, cn}) accumulates proposals produced by Gj (1 ≤ j ≤ i) according to Definitions 3.3, 3.5, and 3.6. Note that an agent can construct counter-proposals by modifying its own previous proposals or another agent"s proposals. An agent Agk accumulates proposals that are made by Agk but are rejected by another agent, in the failed proposal set FP i k (at the i-th round), where FP 0 k = ∅.
Suppose two agents Ag1 and Ag2 who have abductive programs P1, H1 and P2, H2 , respectively. Given a proposal G1
negotiation starts. In response to the proposal Gi
at the i-th round, Ag2 behaves as follows.
negotiation ends in success.
{G0 2} = ∅. Proceed to the step 4(b).
1, Ag2 returns Gi
(a) If Gi
the critique Gi
critique set CSi 2(P2, Gi 1). (b) Otherwise, construct NSi
(i) Produce Si c. Let μ(Si c) = { p | p ∈ Si c \ FPi
p satisfies the constraints in CSi 1(P1, Gi−1
If μ(Si c) = ∅, select one from μ(Si c) and propose it as Gi
(ii) Produce Si n. If μ(Si n) = ∅, select one from μ(Si n) and propose it as Gi
(iii) Produce Si cn. If μ(Si cn) = ∅, select one from μ(Si cn) and propose it as Gi
negotiation ends in failure. This means that Ag2 can make no counter-proposal or every counterproposal made by Ag2 is rejected by Ag1.
In the step 4(a), Ag2 rejects the proposal Gi
the reason of rejection as a critique set. This helps for Ag1 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1025 in preparing a next counter-proposal. In the step 4(b), Ag2 constructs a new proposal. In its construction, Ag2 should take care of the critique set CSi 1(P1, Gi−1
represents integrity constraints, if any, accumulated in previous rounds, that Ag1 must satisfy. Also, FP i
removing proposals which have been rejected. Construction of Si x (x ∈ {c, n, cn}) in NSi
new counter-proposals produced by Gi
x . For instance, Si n in NSi
Si n = Si−1 n ∪{ p | p is a neighborhood proposal made by Gi
∪ { p | p is a neighborhood proposal made by Gi−1
where S0 n = ∅. That is, Si n is constructed from Si−1 n by adding new proposals which are obtained by modifying the proposal Gi
the proposal Gi−1
c and Si cn are obtained as well.
In the above protocol, an agent produces Si c at first, secondly Si n, and finally Si cn. This strategy seeks conditions which satisfy the given proposal, prior to neighborhood proposals which change the original one. Another strategy, which prefers neighborhood proposals to conditional ones, is also considered. Conditional neighborhood proposals are to be considered in the last place, since they differ from the original one to the maximal extent. The above protocol produces the candidate proposals in Si x for each x ∈ {c, n, cn} at once. We can consider a variant of the protocol in which each proposal in Si x is constructed one by one (see Example 3.3).
The above protocol is repeatedly applied to each one of the two negotiating agents until a negotiation ends in success/failure. Formally, the above negotiation protocol has the following properties.
Theorem 3.2. Let Ag1 and Ag2 be two agents having abductive programs P1, H1 and P2, H2 , respectively.
Pi and Hi contain no function symbol), any negotiation will terminate.
proposal G, both P1, H1 and P2, H2 have belief sets satisfying G.
Proof. 1. When an abductive program is function-free, abducibles and negotiation sets are both finite. Moreover, if a proposal is once rejected, it is not proposed again by the function μ. Thus, negotiation will terminate in finite steps.
belief set satisfying G. If the agent Ag2 accepts the proposal G, it is satisfied by an answer set of P2 which is also a belief set of P2, H2 .
Example 3.3. Suppose a buying-selling situation in the introduction. A seller agent has the abductive program Ps, Hs in which Ps consists of belief Bs and desire Ds: Bs : pc(b1, 1G, 512M, 80G) ; pc(b2, 1G, 512M, 80G) ←,(1) dvd-rw ; cd-rw ←, (2) Ds : normal price(1300) ← pc(b1, 1G, 512M, 80G), dvd-rw, (3) normal price(1200) ← pc(b1, 1G, 512M, 80G), cd-rw, (4) normal price(1200) ← pc(b2, 1G, 512M, 80G), dvd-rw, (5) price(x) ← normal price(x), add point, (6) price(x ∗ 0.9) ← normal price(x), pay cash, not add point,(7) add point ←, (8) Hs : add point, pay cash.
Here, (1) and (2) represent selection of products. The atom pc(b1, 1G, 512M, 80G) represents that the seller agent has a PC of the brand b1 such that CPU is 1GHz, memory is 512MB, and HDD is 80GB. Prices of products are represented as desire of the seller. The rules (3) - (5) are normal prices of products. A normal price is a selling price on the condition that service points are added (6). On the other hand, a discount price is applied if the paying method is cash and no service point is added (7). The fact (8) represents the addition of service points. This service would be withdrawn in case of discount prices, so add point is specified as an abducible.
A buyer agent has the abductive program Pb, Hb in which Pb consists of belief Bb and desire Db: Bb : drive ← dvd-rw, (9) drive ← cd-rw, (10) price(x) ←, (11) Db : pc(b1, 1G, 512M, 80G) ←, (12) dvd-rw ←, (13) cd-rw ← not dvd-rw, (14) ← pay cash, (15) ← price(x), x > 1200, (16) Hb : dvd-rw.
Rules (12) - (16) are the buyer"s desire. Among them, (15) and (16) impose constraints for buying a PC. A DVD-RW is specified as an abducible which is subject to concession. (1st round) First, the following proposal is given by the buyer agent: G1 b : pc(b1, 1G, 512M, 80G), dvd-rw, price(x), x ≤ 1200.
As Ps has no answer set which satisfies G1 b , the seller agent cannot accept the proposal. The seller takes an action of making a counter-proposal and performs abduction. As a result, the seller finds the minimal explanation (E, F) = ({ pay cash }, { add point }) which explains G1 b θ1 with θ1 = { x/1170 }. The seller constructs the conditional proposal: G1 s : pc(b1, 1G, 512M, 80G), dvd-rw, price(1170), pay cash, not add point and offers it to the buyer. (2nd round) The buyer does not accept G1 s because he/she cannot pay it by cash (15). The buyer then returns the critique G2 b = reject to the seller, together with the critique set CS2 b (Pb, G1 s) = {(15)}. In response to this, the seller tries to make another proposal which satisfies the constraint in this critique set. As G1 s is stored in FP 2 s and no other conditional proposal satisfying the buyer"s requirement exists, the seller produces neighborhood proposals. He/she relaxes G1 b by dropping x ≤ 1200 in the condition, and produces pc(b1, 1G, 512M, 80G), dvd-rw, price(x).
As Ps has an answer set which satisfies G2 s : pc(b1, 1G, 512M, 80G), dvd-rw, price(1300),
the seller offers G2 s as a new counter-proposal. (3rd round) The buyer does not accept G2 s because he/she cannot pay more than 1200USD (16). The buyer again returns the critique G3 b = reject to the seller, together with the critique set CS3 b (Pb, G2 s) = CS2 b (Pb, G1 s) ∪ {(16)}. The seller then considers another proposal by replacing b1 with a variable w, G1 b now becomes pc(w, 1G, 512M, 80G), dvd-rw, price(x), x ≤ 1200.
As Ps has an answer set which satisfies G3 s : pc(b2, 1G, 512M, 80G), dvd-rw, price(1200), the seller offers G3 s as a new counter-proposal. (4th round) The buyer does not accept G3 s because a PC of the brand b2 is out of his/her interest and Pb has no answer set satisfying G3 s. Then, the buyer makes a concession by changing his/her original goal. The buyer relaxes G1 b by goal replacement using the rule (9) in Pb, and produces pc(b1, 1G, 512M, 80G), drive, price(x), x ≤ 1200.
Using (10), the following proposal is produced: pc(b1, 1G, 512M, 80G), cd-rw, price(x), x ≤ 1200.
As Pb \ { dvd-rw } has a consistent answer set satisfying the above proposal, the buyer proposes the conditional neighborhood proposal G4 b : pc(b1, 1G, 512M, 80G), cd-rw, not dvd-rw, price(x), x ≤ 1200 to the seller agent. Since Ps also has an answer set satisfying G4 b , the seller accepts it and sends the message G4 s = accept to the buyer. Thus, the negotiation ends in success.
In this section, we provide methods of computing proposals in terms of answer sets of programs. We first introduce some definitions from [15].
Definition 4.1. Given an abductive program P, H , the set UR of update rules is defined as: UR = { L ← not L, L ← not L | L ∈ H } ∪ { +L ← L | L ∈ H \ P } ∪ { −L ← not L | L ∈ H ∩ P } , where L, +L, and −L are new atoms uniquely associated with every L ∈ H. The atoms +L and −L are called update atoms.
By the definition, the atom L becomes true iff L is not true. The pair of rules L ← not L and L ← not L specify the situation that an abducible L is true or not. When p(x) ∈ H and p(a) ∈ P but p(t) ∈ P for t = a, the rule +L ← L precisely becomes +p(t) ← p(t) for any t = a. In this case, the rule is shortly written as +p(x) ← p(x), x = a.
Generally, the rule becomes +p(x) ← p(x), x = t1, . . . , x = tn for n such instances. The rule +L ← L derives the atom +L if an abducible L which is not in P is to be true. In contrast, the rule −L ← not L derives the atom −L if an abducible L which is in P is not to be true. Thus, update atoms represent the change of truth values of abducibles in a program. That is, +L means the introduction of L, while −L means the deletion of L. When an abducible L contains variables, the associated update atom +L or −L is supposed to have exactly the same variables. In this case, an update atom is semantically identified with its ground instances.
The set of all update atoms associated with the abducibles in H is denoted by UH, and UH = UH+ ∪ UH− where UH+ (resp. UH− ) is the set of update atoms of the form +L (resp. −L).
Definition 4.2. Given an abductive program P, H , its update program UP is defined as the program UP = (P \ H) ∪ UR .
An answer set S of UP is called U-minimal if there is no answer set T of UP such that T ∩ UH ⊂ S ∩ UH.
By the definition, U-minimal answer sets exist whenever UP has answer sets. Update programs are used for computing (minimal) explanations of an observation. Given an observation G as a conjunction of literals and NAF-literals possibly containing variables, we introduce a new ground literal O together with the rule O ← G. In this case, O has an explanation (E, F) iff G has the same explanation.
With this replacement, an observation is assumed to be a ground literal without loss of generality. In what follows,
E+ = { +L | L ∈ E } and F − = { −L | L ∈ F } for E ⊆ H and F ⊆ H.
Proposition 4.1. ([15]) Let P, H be an abductive program, UP its update program, and G a ground literal representing an observation. Then, a pair (E, F) is an explanation of G iff UP ∪ { ← not G } has a consistent answer set S such that E+ = S ∩ UH+ and F− = S ∩ UH− . In particular, (E, F) is a minimal explanation iff S is a U-minimal answer set.
Example 4.1. To explain the observation G = flies(t) in the program P of Example 2.1, first construct the update program UP of P:3 UP : flies(x) ← bird(x), not ab(x), ab(x) ← broken-wing(x) , bird(t) ← , bird(o) ← , broken-wing(x) ← not broken-wing(x), broken-wing(x) ← not broken-wing(x), +broken-wing(x) ← broken-wing(x), x = t , −broken-wing(t) ← not broken-wing(t) .
Next, consider the program UP ∪ { ← not flies(t) }. It has the single U-minimal answer set: S = { bird(t), bird(o), flies(t), flies(o), broken-wing(t), broken-wing(o), −broken-wing(t) }.
The unique minimal explanation (E, F) = (∅, {broken-wing(t)}) of G is expressed by the update atom −broken-wing(t) in S ∩ UH− .
Proposition 4.2. Let P, H be an abductive program and G a ground literal representing an observation. If P ∪ { ← not G } has a consistent answer set S, G has the minimal explanation (E, F) = (∅, ∅) and S satisfies G.
Now we provide methods for computing (counter-)proposals.
First, conditional proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sc of proposals.
If G is a ground literal, compute its minimal explanation (E, F) in P, H using the update program. Put G, E, not F in Sc. Else if G is a conjunction possibly containing variables, consider the abductive program 3 t represents tweety and o represents opus.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1027 P ∪{ O ← G }, H with a ground literal O. Compute a minimal explanation of O in P ∪ { O ← G }, H using its update program. If O has a minimal explanation (E, F) with a substitution θ for variables in G, put Gθ, E, not F in Sc.
Next, neighborhood proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sn of proposals. % neighborhood proposals by anti-instantiation; Construct G by anti-instantiation. For a ground literal O, if P ∪ { O ← G } ∪ { ← not O } has a consistent answer set satisfying G θ with a substitution θ and G θ = G, put G θ in Sn. % neighborhood proposals by dropping conditions; Construct G by dropping conditions. If G is a ground literal and the program P ∪ { ← not G } has a consistent answer set, put G in Sn. Else if G is a conjunction possibly containing variables, do the following.
For a ground literal O, if P ∪{ O ← G }∪{ ← not O } has a consistent answer set satisfying G θ with a substitution θ, put G θ in Sn. % neighborhood proposals by goal replacement; Construct G by goal replacement. If G is a ground literal and there is a rule H ← B in P such that G = Hσ and Bσ = G for some substitution σ, put G = Bσ.
If P ∪ { ← not G } has a consistent answer set satisfying G θ with a substitution θ, put G θ in Sn. Else if G is a conjunction possibly containing variables, do the following. For a replaced literal L ∈ G , if there is a rule H ← B in P such that L = Hσ and (G \ {L}) ∪ Bσ = G for some substitution σ, put G = (G \ {L}) ∪ Bσ. For a ground literal O, if P ∪ { O ← G } ∪ { ← not O } has a consistent answer set satisfying G θ with a substitution θ, put G θ in Sn.
Theorem 4.3. The set Sc (resp. Sn) computed above coincides with the set of conditional proposals (resp. neighborhood proposals).
Proof. The result for Sc follows from Definition 3.3 and Proposition 4.1. The result for Sn follows from Definition 3.5 and Proposition 4.2.
Conditional neighborhood proposals are computed by combining the above two procedures. Those proposals are computed at each round. Note that the procedure for computing Sn contains some nondeterministic choices. For instance, there are generally several candidates of literals to relax in a proposal. Also, there might be several rules in a program for the usage of goal replacement. In practice, an agent can prespecify literals in a proposal for possible relaxation or rules in a program for the usage of goal replacement.
As there are a number of literature on automated negotiation, this section focuses on comparison with negotiation frameworks based on logic and argumentation.
Sadri et al. [14] use abductive logic programming as a representation language of negotiating agents. Agents negotiate using common dialogue primitives, called dialogue moves.
Each agent has an abductive logic program in which a sequence of dialogues are specified by a program, a dialogue protocol is specified as constraints, and dialogue moves are specified as abducibles. The behavior of agents is regulated by an observe-think-act cycle. Once a dialogue move is uttered by an agent, another agent that observed the utterance thinks and acts using a proof procedure. Their approach and ours both employ abductive logic programming as a platform of agent reasoning, but the use of it is quite different. First, they use abducibles to specify dialogue primitives of the form tell(utterer, receiver, subject, identifier, time), while we use abducibles to specify arbitrary permissible hypotheses to construct conditional proposals. Second, a program pre-specifies a plan to carry out in order to achieve a goal, together with available/missing resources in the context of resource-exchanging problems. This is in contrast with our method in which possible counter-proposals are newly constructed in response to a proposal made by an agent. Third, they specify a negotiation policy inside a program (as integrity constraints), while we give a protocol independent of individual agents. They provide an operational model that completely specifies the behavior of agents in terms of agent cycle. We do not provide such a complete specification of the behavior of agents. Our primary interest is to mechanize construction of proposals.
Bracciali and Torroni [2] formulate abductive agents that have knowledge in abductive logic programs. To explain an observation, two agents communicate by exchanging integrity constraints. In the process of communication, an agent can revise its own integrity constraints according to the information provided by the other agent. A set IC of integrity constraints relaxes a set IC (or IC tightens IC ) if any observation that can be proved with respect to IC can also be proved with respect to IC . For instance,
IC : ← a, b, c relaxes IC : ← a, b. Thus, they use relaxation for weakening the constraints in an abductive logic program.
In contrast, we use relaxation for weakening proposals and three different relaxation methods, anti-instantiation, dropping conditions, and goal replacement, are considered. Their goal is to explain an observation by revising integrity constraints of an agent through communication, while we use integrity constraints for communication to explain critiques and help other agents in making counter-proposals.
Meyer et al. [11] introduce a logical framework for negotiating agents. They introduce two different modes of negotiation: concession and adaptation. They provide rational postulates to characterize negotiated outcomes between two agents, and describe methods for constructing outcomes.
They provide logical conditions for negotiated outcomes to satisfy, but they do not describe a process of negotiation nor negotiation protocols. Moreover, they represent agents by classical propositional theories, which is different from our abductive logic programming framework.
Foo et al. [5] model one-to-one negotiation as a one-time encounter between two extended logic programs. An agent offers an answer set of its program, and their mutual deal is regarded as a trade on their answer sets. Starting from the initial agreement set S∩T for an answer set S of an agent and an answer set T of another agent, each agent extends this set to reflect its own demand while keeping consistency with demand of the other agent. Their algorithm returns new programs having answer sets which are consistent with each other and keep the agreement set. The work is extended to repeated encounters in [3]. In their framework, two agents exchange answer sets to produce a common belief set, which is different from our framework of exchanging proposals.
There are a number of proposals for negotiation based
on argumentation. An advantage of argumentation-based negotiation is that it constructs a proposal with arguments supporting the proposal [1]. The existence of arguments is useful to convince other agents of reasons why an agent offers (counter-)proposals or returns critiques. Parsons et al. [13] develop a logic of argumentation-based negotiation among BDI agents. In one-to-one negotiation, an agent A generates a proposal together with its arguments, and passes it to another agent B. The proposal is evaluated by B which attempts to build arguments against it. If it conflicts with B"s interest, B informs A of its objection by sending back its attacking argument. In response to this, A tries to find an alternative way of achieving its original objective, or a way of persuading B to drop its objection. If either type of argument can be found, A will submit it to B. If B finds no reason to reject the new proposal, it will be accepted and the negotiation ends in success. Otherwise, the process is iterated. In this negotiation processes, the agent A never changes its original objective, so that negotiation ends in failure if A fails to find an alternative way of achieving the original objective. In our framework, when a proposal is rejected by another agent, an agent can weaken or change its objective by abduction and relaxation. Our framework does not have a mechanism of argumentation, but reasons for critiques can be informed by responding critique sets.
Kakas and Moraitis [10] propose a negotiation protocol which integrates abduction within an argumentation framework. A proposal contains an offer corresponding to the negotiation object, together with supporting information representing conditions under which this offer is made.
Supporting information is computed by abduction and is used for constructing conditional arguments during the process of negotiation. In their negotiation protocol, when an agent cannot satisfy its own goal, the agent considers the other agent"s goal and searches for conditions under which the goal is acceptable. Our present approach differs from theirs in the following points. First, they use abduction to seek conditions to support arguments, while we use abduction to seek conditions for proposals to accept. Second, in their negotiation protocol, counter-proposals are chosen among candidates based on preference knowledge of an agent at meta-level, which represents policy under which an agent uses its object-level decision rules according to situations.
In our framework, counter-proposals are newly constructed using abduction and relaxation. The method of construction is independent of particular negotiation protocols. As [2, 10, 14], abduction or abductive logic programming used in negotiation is mostly based on normal abduction. In contrast, our approach is based on extended abduction which can not only introduce hypotheses but remove them from a program. This is another important difference.
Relaxation and neighborhood query answering are devised to make databases cooperative with their users [4, 6]. In this sense, those techniques have the spirit similar to cooperative problem solving in multi-agent systems. As far as the authors know, however, there is no study which applies those technique to agent negotiation.
In this paper we proposed a logical framework for negotiating agents. To construct proposals in the process of negotiation, we combined the techniques of extended abduction and relaxation. It was shown that these two operations are used for general inference rules in producing proposals. We developed a negotiation protocol between two agents based on exchange of proposals and critiques, and provided procedures for computing proposals in abductive logic programming. This enables us to realize automated negotiation on top of the existing answer set solvers. The present framework does not have a mechanism of selecting an optimal (counter-)proposal among different alternatives. To compare and evaluate proposals, an agent must have preference knowledge of candidate proposals. Further elaboration to maximize the utility of agents is left for future study.

In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters. The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14]. We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical. Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results. In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them. These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels). These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows. We don"t follow the "Co-Opetition" aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process. Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level. It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension. In this sense, humans aim at a general sense of fairness in an interaction.
In Section 2 we outline the aspects of human negotiation modelling that we cover in this work. Then, in Section 3 we introduce the negotiation language. Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation. Section 5 contains a description of the different metrics used in the agent model including intimacy. Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance.
Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy. What information is relevant to the negotiation process? What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options. What are the possible agreements we can accept? • Goals. What are the underlying things we need or care about? What are our goals? • Independence. What will we do if the negotiation fails?
What alternatives have we got? • Commitment. What outstanding commitments do we have?
Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . . A key part of any negotiation process is to build a model of our opponent(s) along these dimensions. All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments. Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them. For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain. These two views: informationbased and utility-based, are central in the model proposed in this paper.
There is evidence from psychological studies that humans seek a balance in their negotiation relationships. The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).
However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need. Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby). For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.
The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level). In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.
We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved. According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used. This might be part of our social evolution. There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.
In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity). In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.
See Table 1 for some examples of desired balances along the LOGIC dimensions.
The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators. For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close. Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue.
In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal,
Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where:
data types);
on C (a partial order)
is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found. This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure. Usually, food is accumulated at the shelter for future use. Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required. We will omit this here.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy. To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on). R contains relations between the concepts in the hierarchy, this is needed to define ‘objects" (e.g. deals) that are defined as a tuple of issues.
The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation. Semantic distance plays a fundamental role in strategies for information-based agency. How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships. A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts). Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively.
The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions. First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated. Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts. A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively. Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed. Contracts and Information can thus be thought of as normative statements that restrict an agent"s behaviour.
Norms, contracts, and information have an obvious temporal dimension. Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time. The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. α"s communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place. In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions. C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ‘;" means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a. Representing an ontology as a set predicates in Prolog is simple. The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r.
Figure 1: The LOGIC agent architecture
A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2]. The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment. In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.
The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents. Agents have a probabilistic first-order internal language L used to represent a world model, Mt . A generic information-based architecture is described in detail in [15].
The LOGIC agent architecture is shown in Figure 1. Agent α acts in response to a need that is expressed in terms of the ontology. A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires. Needs trigger α"s goal/plan proactive reasoning, while other messages are dealt with by α"s reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a ‘LOGIC briefcase" that the agent ‘carries" into the negotiation7 . The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here. For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.
Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of α"s plans and reactions contain constructors for an initial world model Mt . Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer. We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2. Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2. The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6. We now describe two of the distributions in Mt that support offer exchange.
Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c.
This estimate is compounded from subjective and objective views of acceptability. The subjective estimate takes account of: the extent to which the enactment of δ will satisfy α"s need χ, how much δ is ‘worth" to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15]. Sα(β, a) is a random variable denoting α"s estimate of β"s subjective valuation of a over some finite, numerical evaluation space. The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes α"s open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space. We also consider needs, the variable Tα(β, a) denotes α"s estimate of the strength of β"s motivating need for the enactment of commitment a over a valuation space.
Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is α"s greed, h ∈ [0, 1] is α"s degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6. The parameters g and h are independent. We can imagine a relationship that begins with g = 1 and h = 0.
Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1. The basis for the acceptance criterion has thus developed from equity to equality, and then to need.
Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing β"s responses. For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of α"s, δ0, then: {Pt (acc(β, α, δ0)) = 0}. In the not-atypical special case of multi-issue bargaining where the agents" preferences over the individual issues only are known and are complementary to each other"s, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent β"s limit of acceptability [6].
α"s world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible. This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed. All observations about the world are received as utterances from an all-truthful institution agent ξ. For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to α"s account) then α may conclude that the goal that β chose to satisfy was something other than hunger. So, α"s world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.
We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility. For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise β"s possible actions.
In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data. In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi). For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the i"th distribution.
Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).
Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of α"s level of personal caution. We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ.
(ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations. We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows. Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj . This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g.
Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].
Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ.
(φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution. The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation. The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t.
The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is α"s expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then α"s expectation of the value of a commitment, φ, to supply wine should decrease. We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk))
A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context. A relationship, Ψ∗t , is a sequence of dialogues. We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what
subsequently occurs (the observation). Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.
Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues.
Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories. In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].
Ideal observations. Consider a distribution of observations that represent α"s ideal in the sense that it is the best that α could reasonably expect to observe. This distribution will be a function of α"s context with β denoted by e, and is Pt I (ϕ |ϕ, e). Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ). That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have. This equation measures confidence for a single statement ϕ. It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.
Similarly, for an overall estimate of β"s confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations. The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ. Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ. Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation. Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement. If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ),
B∗ = (
log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.
Computational Note. The various measures given above involve extensive calculations. For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ . We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to:
X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ). The extent of this calculation is controlled by the parameter η. An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ.
Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β. This negotiation dialogue is evaluated by α in the context of α"s world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}. Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that β"s reasoning apparatus mirrors its own.
In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated. The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define. This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach. For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain. Attaching a utilitarian measure to this utterance may not be so simple.
We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agent"s requirements. Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t.
In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system.
The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).
The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates,
I∗t αβ = ` V ∗t α , V ∗t β ´ . If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally,
V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 ×
agent x of the intimacy of the relationship in the absence of any interaction. Dx is the reputation of agent x. The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β . In particular, the intimacy determines values for the parameters g and h in Equation 1. As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.
The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues. In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework.
Each negotiation has to achieve two goals. First it may be intended to achieve some contractual outcome. Second it will aim to contribute to the growth, or decline, of the relationship intimacy.
We now describe in greater detail the contents of the Negotiation box in Figure 1. The negotiation literature consistently advises that an agent"s behaviour should not be predictable even in close, intimate relationships. The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy. The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases. The stance, St αβ, is a
perturbs α"s actions. The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.
The negotiation strategy is concerned with maintaining a working set of Options. If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position. In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options. This is achieved by increasing the value of s in Equation 1. The following strategy uses the machinery described in Section 4.
Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to β"s history of Propose and Reject utterances.
Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation. Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ . Suppose that the relationship target is (T∗t α , T∗t β ). Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents α"s desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g. I"ll pass a bit more information on options than usual, I"ll be stronger in concessions on options, etc.). It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.
Nα(Ψt ) is what α hopes to receive from β during Ψt . This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.
A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance. A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue. If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above.
In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies. It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic. Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals,
Independence, and Commitment. Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.
The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next. We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors.
IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).) Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project.
[1] Adams, J. S. Inequity in social exchange. In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2. New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P.,
Rodr´ıguez, J. A., and Sierra, C. Environment engineering for multiagent systems. Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B. Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives. Administration Science Quarterly,
[4] Brandenburger, A., and Nalebuff, B.
Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation. Doubleday, New York,
[5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering. American Institute of Physics, Melville,
NY, USA, 2004, ch. On The Relationship between Bayesian and Maximum Entropy Inference, pp.
[6] Debenham, J. Bargaining with information. In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra,
L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B. Getting to Yes: Negotiating agreements without giving in.
Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory. In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science. Springer-Verlag: Heidelberg,
Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton,
J. W. Essentials of Negotiation. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources. IEEE Transactions on Knowledge and Data Engineering 15,
[11] MacKay, D. Information Theory, Inference and Learning Algorithms. Cambridge University Press,
[12] Paris, J. Common sense and maximum entropy.
Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J. An information-based model for trust. In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum,
V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J. Trust and honour in information-based agency. In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan,
May 2006), P. Stone and G. Weiss, Eds., ACM Press,
New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency. In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship. Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.
Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations. In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard,
Eds., vol. 5. JAI Press, 1995, pp. 65-94.

Recently, software development has evolved toward the development of intelligent, interconnected systems working in a distributed manner. The agent paradigm has become well suited as a design metaphor to deal with complex systems comprising many components each having their own thread of control and purposes and involved in dynamic and complex interactions.
In multi-agent environments, agents often need to interact with each other to fulfill their goals. Protocols are used to regulate interactions. In traditional approaches to protocol specification, like those using Finite State Machines or Petri Nets, protocols are often predetermined legal sequences of interactive behaviors. In frequently changing environments such as the Internet, such fixed sequences can quickly become outdated and are prone to failure. Therefore, agents are required to adapt their interactive behaviors to succeed and interactions among agents should not be constructed rigidly.
To achieve flexibility, as characterized by Yolum and Singh in [11], interaction protocols should ensure that agents have autonomy over their interactive behaviors, and be free from any unnecessary constraints. Also, agents should be allowed to adjust their interactive actions to take advantages of opportunities or handle exceptions that arise during interaction.
For example, consider the scenario below for online sales.
A merchant Mer has 200 cricket bats available for sale with a unit price of 10 dollars. A customer Cus has $50. Cus has a goal of obtaining from Mer a cricket bat at some time.
There are two options for Cus to pay. If Cus uses credit payment, Mer needs a bank Ebank to check Cus"s credit. If Cus"s credit is approved, Ebank will arrange the credit payment. Otherwise, Cus may then take the option to pay via PayPal. The interaction ends when goods are delivered and payment is arranged.
A flexible approach to this example should include several features. Firstly, the payment method used by Cus should be at Cus"s own choice and have the property that if Cus"s credit check results in a disapproval, this exception should also be handled automatically by Cus"s switching to PayPal.
Secondly, there should be no unnecessary constraint on the order in which actions are performed, such as which of making payments and sending the cricket bat should come first.
Thirdly, choosing a sequence of interactive actions should be based on reasoning about the intrinsic meanings of protocol actions, which are based on the notion of commitment, i.e. which refers to a strong promise to other agent(s) to undertake some courses of action.
Current approaches [11, 12, 10, 1] to achieve flexibilities using the notion of commitment make use of an abstract layer of commitments. However, in these approaches, a mapping from protocol actions onto operations on commitments 124 978-81-904262-7-5 (RPS) c 2007 IFAAMAS as well as handling and enforcement mechanisms of commitments must be externally provided. Execution of protocol actions also requires concurrent execution of operations on related commitments. As a result, the overhead of processing the commitment layer makes specification and execution of protocols more complicated and error prone. There is also a lack of a logic to naturally express aspects of resources, internal and external choices as well as time of protocols.
Rather than creating another layer of commitment outside protocol actions, we try to achieve a modeling of commitments that is integrated with protocol actions. Both commitments and protocol actions can then be reasoned about in one consistent system. In order to achieve that, we specify protocols in a declarative manner, i.e. what is to be achieved rather then how agents should interact. A key to this is using logic. Temporal logic, in particular, is suitable for describing and reasoning about temporal constraints while linear logic [3] is quite suitable for modeling resources. We suggest using a combination of linear logic and temporal logic to construct a commitment based interaction framework which allows both temporal and resource-related reasoning for interaction protocols. This provides a natural manipulation and reasoning mechanism as well as internal enforcement mechanisms for commitments based on proof search.
This paper is organized as follows. Section 2 discusses the background material of linear logic, temporal linear logic and commitments. Section 3 introduces our modeling framework and specification of protocols. Section 4 discusses how our framework can be used for an example of online sale interactions between a merchant, a bank and a customer.
We then discuss the advantages and limitations of using our framework to model interaction protocols and achieve flexibility in Section 5. Section 6 presents our conclusions and items of further work.
In order to increase the agents" autonomy over their interactive behaviors, protocols should be specified in terms of what is to be achieved rather than how the agents should act.
In other words, protocols should be specified in a declarative manner. Using logic is central to this specification process.
Logic has been used as formalism to model and reason about agent systems. Linear logic [3] is well-known for modeling resources as well as updating processes. It has been considered in agent systems to support agent negotiation and planning by means of proof search [5, 8].
In real life, resources are consumed and new resources are created. In such logic as classical or temporal logic, however, a direct mapping of resources onto formulas is troublesome.
If we model resources like A as one dollar and B as a chocolate bar, then A ⇒ B in classical logic is read as from one dollar we can get a chocolate bar. This causes problems as the implication allows to get a chocolate bar (B is true) while retaining one dollar (A remains true).
In order to resolve such resource - formula mapping issues,
Girard proposed the constraints on which formulas will be used exactly once and can no longer be freely added or removed in derivations and hence treating linear logic formulas as resources. In linear logic, a linear implication A B, however, allows A to be removed after deriving B, which means the dollar is gone after using one dollar to buy a chocolate bar.
Classical conjunction (and) and disjunction (or) are recast over different uses of contexts - multiplicative as combining and additive as sharing to come up with four connectives.
A ⊗ (multiplicative conjunction) A, means that one has two As at the same time, which is different from A ∧ A = A.
Hence, ⊗ allows a natural expression of proportion. A ℘ (multiplicative disjunction) B, means that if not A then B or vice versa but not both A and B.
The ability to specify choices via the additive connectives is a particularly useful feature of linear logic. A (additive conjunction) B, stands for one own choice, either of A or B but not both. A ⊕ (additive disjunction) B, stands for the possibility of either A or B, but we don"t know which.
As remarked in [5], and ⊕ allow choices to be made clear between internal choices (one"s own), and external choices (others" choice). For instance, to specify that the choice of places A or B for goods" delivery is ours as the supplier, we use A B, or is the client"s, we use A ⊕ B.
In agent systems, this duality between inner and outer choices is manifested by one agent having the power to choose between alternatives and the other having to react to whatever choice is made.
Moreover, during interaction, the ability to match consumption and supply of resources among agents can simplify the specification of resource allocations. Linear logic is a natural mechanism to provide this ability [5]. In addition, it is emphasized in [8] that linear logic is used to model agent states as sets of consumable resources and particularly, linear implication is used to model transitions among states and capabilities of agents.
While linear logic provides advantages to modeling and reasoning about resources, it does not deal naturally with time constraints. Temporal logic, on the other hand, is a formal system which addresses the description and reasoning about the changes of truth values of logic expressions over time [2]. Temporal logic can be used for specification and verification of concurrent and reactive programs [2].
Temporal Linear Logic (TLL) [6] is the result of introducing temporal logic into linear logic and hence is resourceconscious as well as deals with time. The temporal operators used are (next), (anytime), and (sometime) [6].
Formulas with no temporal operators can be considered as being available only at present. Adding to a formula A, i.e. A, means that A can be used only at the next time and exactly once. Similarly, A means that A can be used at any time and exactly once. A means that A can be used once at some time.
Though both and refer to a point in time, the choice of which time is different. Regarding , the choice is an internal choice, as appropriate to one"s own capability. With , the choice is externally decided by others.
The concept of social commitment has been recognized as fundamental to agent interaction. Indeed, social commitment provides intrinsic meanings of protocol actions and states [11]. In particular, persistence in commitments introduces into agents" consideration a certain level of predictability of other agents" actions, which is important when agents deal with issues of inter-dependencies, global constraints or The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 125 resources sharing [7].
Commitment based approaches associate protocols actions with operations on commitments and protocol states with the set of effective commitments [11]. Completing the protocol is done via means-end reasoning on commitment operations to bring the current state to final states where all commitments are resolved. From then, the corresponding legal sequences of interactive actions are determined. Hence, the approaches systematically enhance a variety of legal computations [11].
Commitments can be reduced to a more fundamental form known as pre-commitments. A pre-commitment here refers to a potential commitment that specifies what the owner agent is willing to commit [4], like performing some actions or achieving a particular state. Agents can negotiate about pre-commitments by sending proposals of them to others.
The others can respond by agreeing or disagreeing with the proposal or proposing another pre-commitment. Once a precommitment is agreed, it then becomes a commitment and the process moves from negotiation phase to commitment phase, in which the agents act to fulfill their commitments.
Protocols are normally viewed external to agents and are essentially a set of commitments externally imposed on participating agents. We take an internal view of protocols, i.e. from the view of participating agents by putting the specification of commitments locally at the respective agents according to their roles.
Such an approach enables agents to manage their own protocol commitments. Indeed, agents no longer accept and follow a given set of commitments but can reason about which commitments of theirs to offer and which commitments of others to take, while considering the current needs and the environment. Protocols arise as commitments are then linked together via agents" reasoning based on proof search during the interaction. Also, ongoing changes in the environment are taken as input into the generation of protocols by agent reasoning. This is the reverse of other approaches which try to make the specification flexible to accommodate changes in the environment. Hence, it is a step closer to enabling emergent protocols, which makes protocols more dynamic and flexible to the context.
In a nutshell, services are what agents are capable of providing to other agents. Commitments can then be seen to arise from combinations of services, i.e. an agent"s capabiliA unit of consumable resources is modeled as a proposition in linear logic. Numeric figures can be used to abbreviate a multiplicative conjunction of the same instances. For example, 2 dollar = dollar ⊗ dollar. Moreover, such 3 A is a shorthand for A.
In order to address the dynamic manipulation of resources, we also include information about the location and ownership in the encoding of resources to address the relocation and changes in possession of resources during agent interaction. That resource A is located at agent α and owned by agent β is expressed via a shorthand notation as A@αβ , which is treated as a logic proposition in our framework.
This notation can be later extended to a more complex logic construct to reason about changes in location and ownership.
In our running example, a cricket bat cricket b being located at and owned by agent Mer is denoted as cricket b@M .M After a successful sale to the agent customer Cus, the cricket bat will be relocated to and owned by agent Cus. The formula cricket b@CC will replace the formula cricket b@MM to reflect the changes.
Our treatment of unlimited resources is to model it as a number σ of copies of the resource"s formula such that the number σ is chosen to be extremely large, relative to the context. For instance, to indicate that the merchant Mer can issue an unlimited number of sale quotes at any time, we use σ sale quote@M .M Declaration of actions is also modeled in a similar manner as of resources.
The capabilities of agents refer to producing, consuming, relocating and changing ownership of resources. Capabilities are represented by describing the state before and after performing them. The general representation form is Γ Δ, in which Γ describes the conditions before and Δ describes the conditions after. The linear implication in linear logic indeed ensures that the conditions before will be transformed into the conditions after. Moreover, some capabilities can be applied at any number of times in the interaction context and their formulas are also preceded by the number σ.
To take an example, we consider the capability of agent Mer of selling a cricket bat for 10 dollars. The conditions before are 10 dollars and a payment method from agent Cus: 10$@CC ⊗ pay m@C . Given these, by applying theC capability, Mer will gain 10 dollars (10$@MM ) and com⊥ ) so that CusM mit to providing a cricket bat (cricket b@M will get a cricket bat (cricket b@CC ). Together, the capability is encoded as 10$@C ⊗ pay m@C 10$@MC C ⊗ cricket b@C M ⊗ ties. Hence, our approach shifts specifying a set of protocol commitments to specifying sets of pre-commitments as capabilities for each agent. Commitments are then can be ⊥ M cricket b@M .C
mechanism as is used for the agents" actions, resources and goals, which greatly simplifies the system.
Our framework uses TLL as a means of specifying interaction protocols. We encode various concepts such as resource, capability and commitment in TLL. The symmetry between a formula and its negation in TLL is explored as a way to model resources and commitments. We then discuss the central role of pre-commitments, and how they are specified at each participating agent. It then remains for agents to reason about pre-commitments to form protocol commitments,
We discuss the modeling of various types of commitments, their fulfillments and enforcement mechanisms.
Due to duality in linear logic, positive formulas can be regarded as formulas in supply and negative formulas can be regarded as formulas in demand. Hence, we take an approach of modeling non-conditional or base commitments as negative formulas. In particular, by turning a formula into its negative form, a base commitment to derive the resources or carry out the actions associated with the formula is created. In the above example, a commitment of agent Mer to ⊥ M .which are subsequently discharged. provide a cricket bat (cricket b@MM ) is cricket b@M A base commitment is fulfilled (discharged) whenever the
126 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) resources or carries out the actions as required by the commitment. In TLL modeling, this means that the corresponding positive formula is derived. Resolution of commitments can then be naturally carried out by inference in TLL. For example, cricket b@M will fulfil the commit-M ment cricket b@M⊥ and both formulas are automaticallyM removed as cricket b@MM ⊗ cricket b@M⊥ .M ⊥ Under a further assumption that agents are expected to resolve all formulas in demand (removing negative formulas), this creates a driving pressure on agents to resolve base commitments. This pressure then becomes a natural and internal enforcement mechanism for base commitments.
A commitment with conditions (or conditional commitment) can be modeled by connecting the conditions to base commitments via a linear implication. A general form is Γ Δ where Γ is the condition part and Δ includes base commitments. If the condition Γ is derived, by consuming Γ, the linear implication will ensure that Δ results, which means the base commitments in Δ become effective. If the conditions can not be achieved, the linear implication can not be applied and hence commitment part in the conditional commitment is still inactive.
In our approach, conditional commitments are specified in their potential form as pre-commitments of participating agents. Pre-commitments are negotiated among agents via proposals and upon being accepted, will form conditional commitments among the engaged agents. Conditional commitments are interpreted as that the condition Γ is required of the proposed agent and the commitment part Δ is the responsibility of the owner (proposing) agent. Indeed, such interpretation and the encoding of realize the notion of a conditional commitment that owner agent is willing to commit to deriving Δ given the proposed agent satisfies the conditions Γ.
Conditional commitments, pre-commitments and capabilities all have similar encodings. However, their differences lie in the phases of commitment that they are in. Capabilities are used internally by the owner agent and do not involve any commitment. Pre-commitments can be regarded as capabilities intended for forming conditional commitments.
Upon being accepted, pre-commitments will turn into conditional commitments and bring the two engaged agents into a commitment phase. As an example, consider that Mer has a capability of selling cricket bats: (10$@CC ⊗pay m@CC ) (10$@M ⊗ cricket b@M⊥ ⊗ cricket b@CC ). When MerM M proposes its capability to Cus, the capability acts as a precommitment. When the proposal gets accepted, that precommitment will turn into a conditional commitment in which Mer commits to fulfilling the base commitment cricket b@M⊥ (which leads to having cricket b@CC ) uponM the condition that Cus derives 10$@CC ⊗pay m@C (whichC leads to having 10$@MM ).
Breakable commitments which are in place to provide agents with the desired flexibility to remove itself from its commitments (cancel commitments) are also modeled naturally in our framework. A base commitment Com⊥ is turned into a breakable base commitment (cond ⊕ Com)⊥ . The extra token cond reflects the agent"s internal deliberation about when the commitment to derive Com is broken. Once cond is produced, due to the logic deduction cond ⊗ (cond ⊕ Com)⊥ ⊥, the commitment (cond ⊕ Com)⊥ is removed, and hence breaking the commitment of deriving Com.
Moreover, a breakable conditional commitment is modeled as A (1 B), instead of A B. When the condition A is provided, the linear implication brings about (1 B) and it is now up to the owner agent"s internal choice whether 1 or B is resulted. If the agent chooses 1, which practically means nothing is derived, then the conditional commitment is deliberately broken.
Given the modeling of various interaction concepts like resource, action, capability, and commitment, we will discuss how protocols can be specified.
In our framework, each agent is encoded with the resources, actions, capabilities, pre-commitments, any pending commitments that it has. Pre-commitments, which stem from services the agents are capable of providing, are designated to be fair exchanges. In a pre-commitment, all the requirements of the other party are put in the condition part and all the effects to be provided by the owner agent are put on the commitment part to make up a trade-off. Such a design allows agents to freely propose pre-commitments to any interested parties.
An example of pre-commitments is that of an agent Merchant regarding a sale of a cricket bat: [10$@CC ⊗pay m@C 10 $@MM ⊗ cricket b@CC ⊗cricket b@M⊥ M ]. The condition is the requirement that the customer agent provides 10 dollars, which is assumed to be the price of a cricket bat, via a payment method. The exchange is the cricket bat for the customer ( cricket b@CC ) and hence is fair to the merchant.
Protocols are specified in terms of sets of pre-commitments at participating agents. Given some initial interaction commitments, a protocol emerges as agents are reasoning about which pre-commitments to offer and accept in order to fulfill these commitments.
Given a such a protocol specification, we then discuss how interaction might take place. An interaction can start with a request or a proposal. When an agent can not achieve some commitments by itself, it can make a request of them or propose a relevant pre-commitment to an appropriate agent to fulfill them. The choice of which pre-commitments depends on if such pre-commitments can produce the formulas to fulfill the agent"s pending commitments.
When an agent receives a request, it searches for precommitments that can together produce the required formulas of the requests. Those pre-commitments found will be used as proposals to the requesting agents. Otherwise, a failure notice will be returned.
When a proposal is received, the recipient agent also performs a search with the inclusion of the proposal for a proof of those formulas that can resolve its commitments. If the search result is positive, the proposal is accepted and becomes a commitment. The recipient then attempts to fulfill conditions of the commitments. Otherwise, the proposal is refused and no commitment is formed.
Throughout the interaction, proof search has played a vital role in protocol construction. Proof search reveals that some commitments can not be resolved locally or some pre-commitments can be used to resolve pending commitments, which prompts the agent to make a request or proposal respectively. Proof search also determines which precommitments are relevant to fulfillment of a request, which helps agents to decide which pre-commitments to propose to answer the request. Moreover, whether a received proposal C The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 127 is relevant to any pending commitments or not is also determined by a search for proof of these commitments with an inclusion of the proposal. Conditions of proposals can be resolved by proof search as it links them with the agents" current resources and capabilities as well as any relevant precommitments. Therefore, it can be seen that proof search performed by participating agents can link up their respective pre-commitments and turn them into commitments as appropriate, which give rise to protocol formation. We will demonstrate this via our running example in section 4.
Agents interact by sending messages. We address agent interaction in a simple model which contains messages of type requests, proposals, acceptance, refusal and failure notice.
We denote Source to Destination: prior to each message to indicate the source and destination of the message. For example Cust to Mer: denotes that the message is sent from agent Cust to agent Mer.
Request messages start with the key word REQUEST: REQUEST + formula. Formulas in request messages are of commitments.
Proposal messages are preceded with PROPOSE.
Formulas are of capabilities. For example, α to β: PROPOSE Γ Δ is a proposal from agent α to agent β.
There are messages that agents use to response to a proposal. Agents can indicate an acceptance: ACCEPT, or a refusal: REFUSE. To notice a failure in fulfilling a request or proposal, agents reply with that request or proposal message appended with FAIL.
As we have seen, temporal linear logic provides an elegant means for encoding the various concepts of agent interaction in a commitment based specification framework.
Appropriate interaction is generated as agents negotiate their specified pre-commitments to fulfill their goals. The association among pre-commitments at participating agents and the monitoring of commitments to ensure that all are discharged are performed by proof search. In the next section, we will demonstrate how specification and generation of interactions in our framework might work.
We return to the online sales scenario introduced in Section 1.
We design a set of pre-commitments and capabilities to implement the above scenario. For simplicity, we refer to them as rules.
Rules at agent Mer Mer has available at any time 200 cricket bats for sale and can issue sale quotes at any time: 200 cricket b@M ⊗ σ sale quote@M .M M Rule 1: Mer commits to offering a cricket bat (cricket b@M⊥ M ) to Cus ( cricket b@CC ) if Cus pays 10 dollars (10$@CC ) either via Paypal or credit card. The choice is at Cus. σ [10$@C ⊗ (Paypal paid@M ⊕ credit paid@MM )C M 10 $@M ⊗ cricket b@C ⊗ cricket b@M⊥ M C M ] Rule 2: If EBank carries out the credit payment to Mer then the requirement of credit payment at Mer is fulfilled: σ [credit paym@M credit paid@MM ]B Rule 3: If Ebank informs Mer of its disapproval of Cus"s credit then Mer will also let Cus know. σ [credit not appr@M credit not appr@CB ]B Rules at agent Ebank Rule 4: Upon receiving a sale quote from Mer, at the next time point, Ebank commits to either informing Mer that Cus"s credit is not approved ( credit not appr@MB ) or arranging a credit payment to Mer ( credit paym@MB ).
The decision is dependent on the credibility of Cus and hence is external (⊕) to Ebank and Mer: σ [sale quote@M ( credit not appr@MB ) ⊕M credit paym@MB ] Rules at agent Cus Cus has an amount of 50 dollars available at any time, can be used for credit payment or cash payment: $50@C.
Cus has a commitment of obtaining a cricket bat at some time: [ cricket b@CC ]⊥ .
Rule 5: Cus will pay Mer via Paypal if there is an indication from EBank that Cus"s credit is not approved: σ [credit not appr@C Paypal paid@MM ]B
Cus requests from Mer a cricket bat at some time. Mer replies with a proposal in which each cricket bat costs 10 dollars. Cus needs to prepare 10 dollars and payment can be made by credit or via Paypal.
Assuming that Cus only pays via Paypal if credit payment fails, Cus will let Mer charges by credit. Mer will then ask EBank to arrange a credit payment. EBank proposes that Mer gives a quote of sale and depending on Cus"s credibility, at the next time point, either credit payment will be arranged or a disapproval of Cus"s credit will be informed.
Mer accepts and fulfills the conditions. If the first case happens, credit payment is done. If the second case happens, credit payment is failed, Cus may back track to take the option paying via Paypal.
Once payment is arranged, Mer will apply its original proposal to satisfy the Cus"s request of a cricket bat and hence removing one cricket bat and adding 10 dollars into its set of resources.
and hence, makes a request to Merchant: C to M: REQUEST [ cricket b@CC ]⊥
One applications of rule 1 can derive cricket b@C and cricket b@C cricket b@C . Mer will propose rule 1C C at a time instance n1 to Cus as a pre-commitment.
M to C: PROPOSE n1 [10$@C ⊗ (Paypal paid@M ⊕ credit paid@MM )C M 10 $@M ⊗ cricket b@C ⊗ cricket b@M⊥ M ]M C With similar analysis, Cus determines that given the conC 128 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ditions can be satisfied, the proposals can help to derive its request. Hence,
C to M: ACCEPT Cus analyzes the conditions of the accepted proposal by proof search. n110$@CC ; n1Paypal paid@M or n1credit paid@MM ; -(*)-M n110$@C ⊗ ( n1Paypal paid@M ⊕ n1credit paid@MM )C M n1(10$@C ⊗ (Paypal paid@M ⊕ credit paid@MM ))C M From (*), one way to satisfy the conditions is for Cus to derive, at the next n1 time points, 10 dollars ( n1 10$@CC ); and to choose paying via Paypal ( n1 Paypal paid@MM ) OR by credit payment ( n1 credit paid@MM ).
10$@C : as Cus has 50 dollars, it canC make use of 10 dollars: 10 $@C 10$@C n1 10$@C .C C C There are two options for payment method, the choice is at agent Cus. We assume that Cus prefers credit payment.
credit paid@M : Cus can not deriveM this formula by itself, hence, it will make a request to Mer: C to M: REQUEST [ n1 credit paid@MM ]⊥ .
condition ( n1 credit paym@MB ). Hence, Mer will further make a request to EBank.
M to E: REQUEST [ n1 credit paym@MB ]⊥ Ebank searches and finds rule 4 applicable. Because credit paym@M will be available one time point after theB rule"s application time, Ebank proposes to Mer an instance of rule 4 at the next n1-1 time points.
[quote@MM ( credit not appr@M ⊕ credit paym@MB )]B With similar analysis, Mer accepts the proposal.
M to B: ACCEPT The rule condition is fulfilled by Mer as quote@MM n1−1 quote@M . Hence, Ebank then applies the proposalM to derive: n1−1 ( credit not appr@M ⊕ credit paym@MB ).B ⊕ indicates the choice is external to both agents. There are two cases, Cus"s credit is approved or disapproved.
For simplicity, we show only the case where Cus"s credit is approved. At the next (n1-1) time point, n1−1 ( credit not appr@MB ⊕ credit paym@MB ) becomes n1−1 credit paym@M n1 credit paym@M .B B As a result, at the next n1 time points, Ebank will arrange the credit payment.
When any of n1 Paypal paid@M (if Cus pays via Pay-M pal) or n1 credit paid@M (if Cus pays by credit card)M is derived, n1 (credit paym@M ⊕ Paypal paid@MM ) isM also derived, hence the payment method is arranged.
Together with the other condition 10$@C being satisfied,C this allows the initial proposal to be applied by Mer to derive n1 cricket b@CC and a commitment of n1 cricket b@M⊥ M for Mer, which is also resolved by the resource cricket b@MM available at Mer.
Any values of n1 such that n1 − 1 ≥ 0 ⇔ n1 ≥ 1 will allow Mer to fulfill Cus"s initial request of [ cricket b@CC ]⊥ .
The interaction ends as all commitments are resolved.
The desired flexibility has been achieved in the example.
It is Cus"s own decision to proceed with the preferred payment method. Also, non-determinism that whether Cus"s credit is disapproved or credit payment is made to Mer is faithfully represented. If an exception happens that Cus"s credit is not approved, credit not appr@C is produced andB Cus can backtrack to paying via Paypal. Rule 5 will then be utilized to allow Cus to handle the exception by paying via Paypal.
Moreover, in order to specify that making payments and sending cricket bats can be in any order, we can add in front of payment method in rule 1 as follows: σ [10$@C ⊗ (Paypal paid@M ⊕ credit paid@MM )C M 10 $@M ⊗ cricket b@C ⊗ cricket b@M⊥ M ].M C This addition in the condition of the rule means that the time of payment can be any time up to Cus"s choice, as long as Cus pays and hence, the time order between making payments and sending goods becomes flexible.
Our TLL framework has demonstrated natural and expressive specification of agent interaction protocols. Linear implication ( ) expresses causal relationship, which makes it natural to model a removal or a consumption, especially of resources, together with its consequences. Hence, in our framework, resource transformation is modeled as a linear implication of the consumed resources to the produced resources. Resource relocation is modeled as a linear implication from a resource at one agent to that resource at the other agent. Linear implication also ensures that fulfillment of the conditions of a conditional commitment will cause the commitments to happen. Moreover, state updates of agents are resulted from a linear implication from the old state to the current state.
Temporal operators ( , and ) and their combinations help to specify the time of actions, of resource availability and express the time order of events. Particularly, precise time points are described by the use of operator or multiple copies of it. Based on this ability to specify correct time points for actions or events, time order or sequencing of them can also be captured. Also, a sense of duration is simulated by spreading copies of the resources or actions" formulas across multiple adjacent time points. Moreover, uncertainty in time can represented and reasoned about by the use of and and their combinations with . can be used to express outer non-determinism while expresses inner non-determinism. These time properties of resources, actions and events are correctly respected through out the agent reasoning process based on sequent calculus rules.
Furthermore, the centrality of the notion of commitment in agent interaction has been recognized in many frameworks [11, 12, 1, 10, 4]. However, to the best of our knowledge, modeling commitments directly at the propositional level of such a resource conscious and time aware logic as TLL is The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 129 firstly investigated in our framework. Our framework models base commitments as negative formulas and conditional commitments via the use of linear implication and/or negative formulas. The modeling of commitments has a number of advantages: • Commitments are represented directly at the propositional logic level or via a logic connective rather than a non-logical construct like [11], which makes treatment of commitments more natural and simple and allows to make use of readily available proof search systems like using sequent calculus for handling commitments.
Existing logic connectives like ⊗, , ⊕, are also readily available for describing the relationships among commitments. • Fulfillment of commitments then becomes deriving the corresponding positive formulas or condition formulas, which then simply reduces to a proof search task.
Also, given the required formulas, fulfillment of commitments can be implemented easily and automatically as deduction (com ⊗ com⊥ ⊥).
The enforcement of commitments is also internal and• simply implemented via the assumption that agents are driven to remove all negative formulas for base commitments and via the use of linear implication for conditional commitments.
Regarding making protocol specification more flexible, our approach has marked a number of significant points.
Firstly, flexibility of protocol specifications in our framework comes from the expressive power of the connectives of TLL. and ⊕ refer to internal and external choices of agents on resources and actions while and refer to internal choices and external choices in time domain. Given that flexibility includes the ability to make a sensible choice, having the choices expressed explicitly in the specification of interaction protocols provides agents with an opportunity to reason about the right choices during interaction and hence explore the flexibility in them.
Secondly, instead of being sequences of interactive actions, protocols are structured on commitments, which are more abstract than protocol actions. Execution of protocols is then based on fulfilling commitments. Hence, unnecessary constraints on which particular interactive actions to execute by which agents and on the order among them are now removed, which is a step forward to flexibility as compared to traditional approaches. On the other hand, in the presence of changes introduced externally, agents have the freedom to explore new sets of interactive actions or skip some interactive actions ahead as long as they still fulfill the protocol"s commitments. This brings more flexibility to the overall level of agents" interactive behaviors, and thus the protocol.
Thirdly, the protocol is specified in a declarative manner essentially as sets of pre-commitments at each participating agents. To achieve goals, agents use reasoning based on TLL sequent calculus to construct proofs of goals from pre-commitments and state formulas. This essentially gives agents an autonomy in utilization of pre-commitments and hence agents can adapt the ways they use these to flexibly deal with changing environments.
In particular, as proof construction by agents selects a sequence of pre-commitments for interaction, being able to select from all the possible combinations of pre-commitments in proof search gives more chances and flexibility than selecting from only a few fixed and predefined sequences. It is then also more likely to allow agents to handle exceptions or explore opportunities that arise. Moreover, as the actual order of pre-commitments is determined by the proof construction process rather than predefined, agents can flexibly change the order to suit new situations.
Fourthly, changes in the environment can be regarded as removing or adding formulas onto the state formulas.
Because the proof construction by agents takes into account the current state formulas when it picks up pre-commitments, changes in the state formulas will be reflected in the choice of which relevant pre-commitments to proceed. Hence, the agents have the flexibility in deciding what to do to deal with changes.
Lastly, specifying protocols in our framework has a modular approach which adds ease and flexibility to the designing process of protocols. Protocols are specified by placing a set of pre-commitments at each participating agent according to their roles. Each pre-commitment can indeed be specified as a process in its own with condition formulas as its input and commitment part"s formulas as its output. Execution of each conditional commitment is a relatively independent thread and they are linked together by the proof search to fulfill agents" commitments. As a results, with such a design of pre-commitments, one pre-commitment can be added or removed without interfering the others and hence, achieving a modular design of the protocols.
Modeling As all the temporal operators in TLL refer to concrete time points, we can not express durations in time faithfully.
One major disadvantage of simulating a duration of an event by spreading copies of that event over adjacent time points A⊗ 10 continuously (like A⊗ 2 . . . A) is that it requires the time range to be provided explicitly. Hence, such notion like until can not be naturally expressed in TLL.
Commitments of agents can be in conflict, especially when resolving all of them requires more resources or actions than what agents have. Our work has not covered handling commitments that are in conflict.
Another troublesome aspect of this approach is that the rules for interaction require some detailed knowledge of the formulas of temporal linear logic. Clearly it would be beneficial to have a visually-based tool similar to UML diagrams which would allow non-experts to specify the appropriate rules without having to learn the details of the formulas themselves.
This paper uses TLL for specifying interaction protocols.
In particular, TLL is used to model the concept of resource, capability, pre-commitment and commitment with tight integration as well as their manipulations with respect to time.
Agents then make use of proof search techniques to perform the desired interactions.
In particular, the approach allows protocol specifications to capture the meaning of interactive actions via commitments, to capture the internal choices and external choices of agents about resources, commitments and about time as well as updating processes. The proof construction mechanism 130 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) provides agents with the ability to dynamically select appropriate pre-commitments, and hence, help agents to gain the flexibility in choosing the interactive actions that are most suitable and the flexibility in the order of them, taking into consideration on-going changes in the environment.
Many other approaches to modeling protocols also use the commitment concept to bring more meaning into agents" interactive actions. Approaches based on commitment machines [11, 12, 10, 1] endure a number of issues. These approaches use logic systems that are limited in their expressiveness to model resources. Also, as an extra abstract layer of commitments is created, more tasks are created accordingly. In particular, there must be a human-designed mapping between protocol actions and operations on commitments as well as between control variables (fluent) and phases of commitment achievement. Moreover, external mechanisms must be in place to comprehend and handle operations and resolution of commitments as well as enforcement of the notion of commitment on its abstract data type representations. This requires another execution in the commitment layer in conjunction with the actual execution of the protocol. Not only these extra tasks create an overhead but also makes the specification and execution of protocols more error prone.
Similar works in [8] and [9] explore the advantages of linear logic and TLL respectively by using partial deduction techniques to help agents to figure out the missing capabilities or resources and based on that, to negotiate with other agents about cooperation strategies. Our approach differs in bringing the concept of commitment into the modeling of interaction, and providing a more natural and detailed map for specifying interaction, especially about choices, time and updating using the full propositional TLL. Moreover, we emphasize on the use of pre-commitments as interaction rules with a full set of TLL inference rules to provide the advantages of proof construction in achieving flexible interaction.
Our further work will include using TLL to verify various properties of interaction protocols such as liveness and safety. Also, we will investigate developing an execution mechanism for such TLL specifications in our framework.
Acknowledgments We are very thankful to Michael Winikoff for many stimulating and helpful discussions of this material. We also would like to acknowledge the support of the Australian Research Council under grant DP0663147.

