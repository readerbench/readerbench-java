Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies. As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword-matching based search to broad information browsing such as what are the major international events in recent months. Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e. they usually provide specified search for documents matching the user"s query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed. In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful.
Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods. The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches. For example, hierarchical agglomerative clustering (HAC) [13] is a typical bottom-up hierarchical clustering method. It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster. On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions. For instance, K-means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers. In this paper, we will focus on the partitioning methods.
As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non-convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations. In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28].
Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community.
The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points. Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph. For example Spectral Clustering is one kind of the most representative graph-based clustering approaches, it generally aims to optimize some cut value (e.g. Normalized Cut [22], Ratio Cut [7],
Min-Max Cut [11]) defined on an undirected graph. After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal. In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph.
In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer. So we call our method Clustering with Local and Global Regularization (CLGR). The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods.
The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail. The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4.
In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail. First let"s see the how the documents are represented throughout this paper.
In our work, all the documents are represented by the weighted term-frequency vectors. Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations). The term-frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk. In this way, xi is also called the TFIDF representation of document di. Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector.
As its name suggests, CLGR is composed of two parts: local regularization and global regularization. In this subsection we will introduce the local regularization part in detail.
As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way.
Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24]. For example, in the two-class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi. By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector. Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.
A natural problem in Eq.(3) is that the matrix XXT may be singular and thus not invertable (e.g. when m n). To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter. Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix. It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].
However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e. w∗ is estimated using the whole training set. According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space. In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points. For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes. The generalizations of our method to multi-class cases will be discussed in section
of it. Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6].
Inspired by their success, we proposed to apply the local learning algorithms for clustering. The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi. Finally we will combine all those local predictors by minimizing the sum of their prediction errors. In this subsection we will introduce how to construct those local predictors.
Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj. Then using Eq.(6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik. The problem here is that XiXT i is an m × m matrix with m ni, i.e. we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited.
Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq.(8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.
Proof. Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .
Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.
Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor. Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.
This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function.
After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors. Combining Eq.(10) with Eq.(6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way. Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .
Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques. However, the results may not be good enough since we only exploit the local informations of the dataset. In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way.
In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g. submanifold or cluster) tend to have the same cluster assignments [31].
Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e. 2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x. The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.
Generally, the graph can be viewed as the discretized form of manifold. We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points. Then it can be shown that minimizing Eq.(13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj. If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter. It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].
In summary, using Eq.(15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold. Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments.
Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t. qi ∈ {−1, +1}, (18) where P is defined as in Eq.(12), and λ is a regularization parameter to trade off Jl and Jg. However, the discrete fill in the whole high-dimensional sample space. And it has been shown that the manifold based methods can achieve good results on text classification tasks [31]. 3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi). constraint of pi makes the problem an NP hard integer programming problem. A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q. Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t. qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e. xi will be classified as class one if qi > 0, otherwise it will be classified as class 2.
In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.
First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}. qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.
Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).
Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 . (23) As in Eq.(11), we can define an n×n matrix P (see Eq.(12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 . (24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc . (25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix. The same as in Eq.(20), we also add the constraint that QT Q = I to restrict the scale of Q. Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t. QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix. Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points. There are mainly two approaches to achieve this goal:
embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters.
is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .
The detailed algorithm can be referred to [26].
The detailed algorithm procedure for CLGR is summarized in table 1.
In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets.
First we will introduce the basic informations of those datasets.
We use a variety of datasets, most of which are frequently used in the information retrieval research. Table 2 summarizes the characteristics of the datasets. 4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1. Then the xi can be assigned to the j-th cluster such that j = argjQ∗ ij = 1.
Table 1: Clustering with Local and Global Regularization (CLGR) Input:
i=1;
i=1;
Output: The cluster membership of each data point.
Procedure:
data point;
(P − I) + λL;
the matrix Q∗ according to Eq.(28);
by properly discretize Q∗ .
Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR. This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university. The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory.
WebKB. The WebKB dataset contains webpages gathered from university computer science departments. There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other. The raw text is about 27MB. Among these 7 categories, student, faculty, course and project are four most populous entity-representing categories. The associated subset is typically called WebKB4.
Reuters. The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987. It is a standard text categorization benchmark and contains 135 categories. In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10.
WebACE. The WebACE dataset was from WebACE project and has been used for document clustering [17][5]. The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997.
These documents are divided into 20 classes.
News4. The News4 dataset used in our experiments are selected from the famous 20-newsgroups dataset5 . The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news-18828. The News4 dataset contains 3970 document vectors. 5 http://people.csail.mit.edu/jrennie/20Newsgroups/ To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels.
In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms. To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.
Clustering Accuracy (Acc). The first performance measure is the Clustering Accuracy, which discovers the one-toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. It sums up the whole matching degree between all pair class-clusters. Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k-th cluster in the final results, and Lm is the true m-th class. T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k.
Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps.
The greater clustering accuracy means the better clustering performance.
Normalized Mutual Information (NMI). Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters. For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively. One can see that NMI(X, X) = 1, which is the maximal possible value of NMI. Given a clustering result, the NMI in Eq.(30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m-th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m-th class. The value calculated in Eq.(31) is used as a performance measure for the given clustering result. The larger this value, the better the clustering performance.
We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora. The algorithms that we evaluated are listed below.
on [9].
is based on [16].
implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30]. Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian.
In this method we just minimize Jl (defined in Eq.(24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods.
implementation is based on [14].
implementation is based on [27].
(TNMF) [12]. The implementation is based on [15].
For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10}. The size of the k-nearest neighborhoods is set by grid search from {20, 40, 80}. For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10}. When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix. The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20].
The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4. From the two tables we mainly observe that:
clustering methods in most of the datasets;
usually outperforms the traditional k-means clustering method, and the GMM method can achieve competitive results compared to the Spherical k-means method;
algorithms are usually worse than the results achieved from Spectral Clustering. Since Spectral Clustering can be viewed as a weighted version of kernel k-means, it can obtain good results the data clusters are arbitrarily shaped. This corroborates that the documents vectors are not regularly distributed (spherical or elliptical).
equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10]. It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results.
can usually achieve better results than traditional purely document vector based methods. Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results.
than the results achieved from Spectral Clustering, which supports Vapnik"s theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms.
Besides the above comparison experiments, we also test the parameter sensibility of our method. There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λi"s to be identical to λ∗ in our experiments), and the size of the neighborhoods.
Therefore we have also done two sets of experiments:
clustering performance with varying λ∗ and λ. In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small.
Typically our method can achieve good results when λ∗ and λ are around 0.1. Figure 1 shows us such a testing example on the WebACE dataset.
and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3
local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x-axis and y-axis represents the log2 value of λ∗ and λ. sizes of neighborhoods. In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results. This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics. Figure 2 shows us a testing example on the WebACE dataset.
Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications.
In this paper, we derived a new clustering algorithm called clustering with local and global regularization. Our method preserves the merit of local learning algorithms and spectral clustering. Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm.
[1] L. Baker and A. McCallum. Distributional Clustering of Words for Text Classification. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998. [2] M. Belkin and P. Niyogi. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.
Neural Computation, 15 (6):1373-1396. June 2003. [3] M. Belkin and P. Niyogi. Towards a Theoretical Foundation for Laplacian-Based Manifold Methods. In Proceedings of the 18th Conference on Learning Theory (COLT). 2005. 10 20 30 40 50 60 70 80 90 100
size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindhwani. Manifold Regularization: a Geometric Framework for Learning from Examples. Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley. Principal Direction Divisive Partitioning.
Data mining and knowledge discovery, 2:325-344, 1998. [6] L. Bottou and V. Vapnik. Local learning algorithms.
Neural Computation, 4:888-900, 1992. [7] P. K. Chan, D. F. Schlag and J. Y. Zien. Spectral K-way Ratio-Cut Partitioning and Clustering. IEEE Trans. Computer-Aided Design, 13:1088-1096, Sep.
[8] D. R. Cutting, D. R. Karger, J. O. Pederson and J.
W. Tukey. Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992. [9] I. S. Dhillon and D. S. Modha. Concept Decompositions for Large Sparse Text Data using Clustering. Machine Learning, vol. 42(1), pages 143-175, January 2001. [10] C. Ding, X. He, and H. Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In Proceedings of the SIAM Data Mining Conference, 2005. [11] C. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A min-max cut algorithm for graph partitioning and data clustering. In Proc. of the 1st International Conference on Data Mining (ICDM), pages 107-114,
[12] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering.
In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. [13] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma, and M. Ogihara. Document Clustering via Adaptive Subspace Iteration. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. [15] T. Li and C. Ding. The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering. In Proceedings of the 6th International Conference on Data Mining (ICDM). 2006. [16] X. Liu and Y. Gong. Document Clustering with Cluster Refinement and Model Selection Capabilities.
In Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G.
Karypis, V. Kumar, B. Mobasher, and J. Moore.
WebACE: A Web Agent for Document Categorization and Exploration. In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert, and U. von Luxburg. From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians. In Proceedings of the 18th Conference on Learning Theory (COLT), 470-485. 2005. [19] J. He, M. Lan, C.-L. Tan, S.-Y. Sung, and H.-B. Low.
Initialization of Cluster Refinement Algorithms: A Review and Comparative Study. In Proc. of Inter.
Joint Conference on Neural Networks, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. On Spectral Clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14. 2002. [21] B. Sch¨olkopf and A. Smola. Learning with Kernels.
The MIT Press. Cambridge, Massachusetts. 2002. [22] J. Shi and J. Malik. Normalized Cuts and Image Segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. [23] A. Strehl and J. Ghosh. Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions. Journal of Machine Learning Research, 3:583-617, 2002. [24] V. N. Vapnik. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag, 1995. [25] Wu, M. and Sch¨olkopf, B. A Local Learning Approach for Clustering. In Advances in Neural Information Processing Systems 18. 2006. [26] S. X. Yu, J. Shi. Multiclass Spectral Clustering. In Proceedings of the International Conference on Computer Vision, 2003. [27] W. Xu, X. Liu and Y. Gong. Document Clustering Based On Non-Negative Matrix Factorization. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003. [28] H. Zha, X. He, C. Ding, M. Gu and H. Simon. Spectral Relaxation for K-means Clustering. In NIPS 14. 2001. [29] T. Zhang and F. J. Oles. Text Categorization Based on Regularized Linear Classification Methods. Journal of Information Retrieval, 4:5-31, 2001. [30] L. Zelnik-Manor and P. Perona. Self-Tuning Spectral Clustering. In NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B.

In many machine learning and information retrieval tasks, there is no shortage of unlabeled data but labels are expensive. The challenge is thus to determine which unlabeled samples would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. This problem is typically called active learning [4].
Here the task is to minimize an overall cost, which depends both on the classifier accuracy and the cost of data collection. Many real world applications can be casted into active learning framework. Particularly, we consider the problem of relevance feedback driven Content-Based Image Retrieval (CBIR) [13].
Content-Based Image Retrieval has attracted substantial interests in the last decade [13]. It is motivated by the fast growth of digital image databases which, in turn, require efficient search schemes. Rather than describe an image using text, in these systems an image query is described using one or more example images. The low level visual features (color, texture, shape, etc.) are automatically extracted to represent the images. However, the low level features may not accurately characterize the high level semantic concepts.
To narrow down the semantic gap, relevance feedback is introduced into CBIR [12].
In many of the current relevance feedback driven CBIR systems, the user is required to provide his/her relevance judgments on the top images returned by the system. The labeled images are then used to train a classifier to separate images that match the query concept from those that do not. However, in general the top returned images may not be the most informative ones. In the worst case, all the top images labeled by the user may be positive and thus the standard classification techniques can not be applied due to the lack of negative examples. Unlike the standard classification problems where the labeled samples are pregiven, in relevance feedback image retrieval the system can actively select the images to label. Thus active learning can be naturally introduced into image retrieval.
Despite many existing active learning techniques, Support Vector Machine (SVM) active learning [14] and regression based active learning [1] have received the most interests.
Based on the observation that the closer to the SVM boundary an image is, the less reliable its classification is, SVM active learning selects those unlabeled images closest to the boundary to solicit user feedback so as to achieve maximal refinement on the hyperplane between the two classes. The major disadvantage of SVM active learning is that the estimated boundary may not be accurate enough. Moreover, it may not be applied at the beginning of the retrieval when there is no labeled images. Some other SVM based active learning algorithms can be found in [7], [9].
In statistics, the problem of selecting samples to label is typically referred to as experimental design. The sample x is referred to as experiment, and its label y is referred to as measurement. The study of optimal experimental design (OED) [1] is concerned with the design of experiments that are expected to minimize variances of a parameterized model. The intent of optimal experimental design is usually to maximize confidence in a given model, minimize parameter variances for system identification, or minimize the model"s output variance. Classical experimental design approaches include A-Optimal Design, D-Optimal Design, and E-Optimal Design. All of these approaches are based on a least squares regression model. Comparing to SVM based active learning algorithms, experimental design approaches are much more efficient in computation. However, this kind of approaches takes only measured (or, labeled) data into account in their objective function, while the unmeasured (or, unlabeled) data is ignored.
Benefit from recent progresses on optimal experimental design and semi-supervised learning, in this paper we propose a novel active learning algorithm for image retrieval, called Laplacian Optimal Design (LOD). Unlike traditional experimental design methods whose loss functions are only defined on the measured points, the loss function of our proposed LOD algorithm is defined on both measured and unmeasured points. Specifically, we introduce a locality preserving regularizer into the standard least-square-error based loss function. The new loss function aims to find a classifier which is locally as smooth as possible. In other words, if two points are sufficiently close to each other in the input space, then they are expected to share the same label. Once the loss function is defined, we can select the most informative data points which are presented to the user for labeling. It would be important to note that the most informative images may not be the top returned images.
The rest of the paper is organized as follows. In Section 2, we provide a brief description of the related work. Our proposed Laplacian Optimal Design algorithm is introduced in Section 3. In Section 4, we compare our algorithm with the state-or-the-art algorithms and present the experimental results on image retrieval. Finally, we provide some concluding remarks and suggestions for future work in Section 5.
Since our proposed algorithm is based on regression framework. The most related work is optimal experimental design [1], including A-Optimal Design, D-Optimal Design, and EOptimal Design. In this Section, we give a brief description of these approaches.
The generic problem of active learning is the following.
Given a set of points A = {x1, x2, · · · , xm} in Rd , find a subset B = {z1, z2, · · · , zk} ⊂ A which contains the most informative points. In other words, the points zi(i = 1, · · · , k) can improve the classifier the most if they are labeled and used as training points.
We consider a linear regression model y = wT x + (1) where y is the observation, x is the independent variable, w is the weight vector and is an unknown error with zero mean. Different observations have errors that are independent, but with equal variances σ2 . We define f(x) = wT x to be the learner"s output given input x and the weight vector w. Suppose we have a set of labeled sample points (z1, y1), · · · , (zk, yk), where yi is the label of zi. Thus, the maximum likelihood estimate for the weight vector, ˆw, is that which minimizes the sum squared error Jsse(w) = k i=1 wT zi − yi 2 (2) The estimate ˆw gives us an estimate of the output at a novel input: ˆy = ˆwT x.
By Gauss-Markov theorem, we know that ˆw − w has a zero mean and a covariance matrix given by σ2 H−1 sse, where Hsse is the Hessian of Jsse(w) Hsse = ∂2 Jsse ∂w2 = k i=1 zizT i = ZZT where Z = (z1, z2, · · · , zk).
The three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are: • D-optimal design: determinant of Hsse. • A-optimal design: trace of Hsse. • E-optimal design: maximum eigenvalue of Hsse.
Since the computation of the determinant and eigenvalues of a matrix is much more expensive than the computation of matrix trace, A-optimal design is more efficient than the other two. Some recent work on experimental design can be found in [6], [16].
Since the covariance matrix Hsse used in traditional approaches is only dependent on the measured samples, i.e. zi"s, these approaches fail to evaluate the expected errors on the unmeasured samples. In this Section, we introduce a novel active learning algorithm called Laplacian Optimal Design (LOD) which makes efficient use of both measured (labeled) and unmeasured (unlabeled) samples.
In many machine learning problems, it is natural to assume that if two points xi, xj are sufficiently close to each other, then their measurements (f(xi), f(xj)) are close as well. Let S be a similarity matrix. Thus, a new loss function which respects the geometrical structure of the data space can be defined as follows: J0(w) = k i=1 f(zi)−yi 2 + λ 2 m i,j=1 f(xi)−f(xj) 2 Sij (3) where yi is the measurement (or, label) of zi. Note that, the loss function (3) is essentially the same as the one used in Laplacian Regularized Regression (LRR, [2]). However,
LRR is a passive learning algorithm where the training data is given. In this paper, we are focused on how to select the most informative data for training. The loss function with our choice of symmetric weights Sij (Sij = Sji) incurs a heavy penalty if neighboring points xi and xj are mapped far apart. Therefore, minimizing J0(w) is an attempt to ensure that if xi and xj are close then f(xi) and f(xj) are close as well. There are many choices of the similarity matrix S. A simple definition is as follows: Sij = ⎧ ⎨ ⎩ 1, if xi is among the p nearest neighbors of xj, or xj is among the p nearest neighbors of xi; 0, otherwise. (4) Let D be a diagonal matrix, Dii = j Sij, and L = D−S.
The matrix L is called graph Laplacian in spectral graph theory [3]. Let y = (y1, · · · , yk)T and X = (x1, · · · , xm).
Following some simple algebraic steps, we see that: J0(w) = k i=1 wT zi − yi 2 + λ 2 m i,j=1 wT xi − wT xj 2 Sij = y − ZT w T y − ZT w + λwT m i=1 DiixixT i − m i,j=1 SijxixT j w = yT y − 2wT Zy + wT ZZT w +λwT XDXT − XSXT w = yT y − 2wT Zy + wT ZZT + λXLXT w The Hessian of J0(w) can be computed as follows: H0 = ∂2 J0 ∂w2 = ZZT + λXLXT In some cases, the matrix ZZT +λXLXT is singular (e.g. if m < d). Thus, there is no stable solution to the optimization problem Eq. (3). A common way to deal with this ill-posed problem is to introduce a Tikhonov regularizer into our loss function: J(w) = k i=1 wT zi − yi 2 + λ1 2 m i,j=1 wT xi − wT xj 2 Sij +λ2 w 2 (5) The Hessian of the new loss function is given by: H = ∂2 J ∂w2 = ZZT + λ1XLXT + λ2I := ZZT + Λ where I is an identity matrix and Λ = λ1XLXT + λ2I.
Clearly, H is of full rank. Requiring that the gradient of J(w) with respect to w vanish gives the optimal estimate ˆw: ˆw = H−1 Zy The following proposition states the bias and variance properties of the estimator for the coefficient vector w.
Proposition 3.1. E( ˆw − w) = −H−1 Λw, Cov( ˆw) = σ2 (H−1 − H−1 ΛH−1 ) Proof. Since y = ZT w + and E( ) = 0, it follows that E( ˆw − w) (6) = H−1 ZZT w − w = H−1 (ZZT + Λ − Λ)w − w = (I − H−1 Λ)w − w = −H−1 Λw (7) Notice Cov(y) = σ2 I, the covariance matrix of ˆw has the expression: Cov( ˆw) = H−1 ZCov(y)ZT H−1 = σ2 H−1 ZZT H−1 = σ2 H−1 (H − Λ)H−1 = σ2 (H−1 − H−1 ΛH−1 ) (8) Therefore mean squared error matrix for the coefficients w is E(w − ˆw)(w − ˆw)T (9) = H−1 ΛwwT ΛH−1 + σ2 (H−1 − H−1 ΛH−1 ) (10) For any x, let ˆy = ˆwT x be its predicted observation. The expected squared prediction error is E(y − ˆy)2 = E( + wT x − ˆwT x)2 = σ2 + xT [E(w − ˆw)(w − ˆw)T ]x = σ2 + xT [H−1 ΛwwT ΛH−1 + σ2 H−1 − σ2 H−1 ΛH−1 ]x Clearly the expected square prediction error depends on the explanatory variable x, therefore average expected square predictive error over the complete data set A is 1 m m i=1 E(yi − ˆwT xi)2 = 1 m m i=1 xT i [H−1 ΛwwT ΛH−1 + σ2 H−1 − σ2 H−1 ΛH−1 ]xi +σ2 = 1 m Tr(XT [σ2 H−1 + H−1 ΛwwT ΛH−1 − σ2 H−1 ΛH−1 ]X) +σ2 Since Tr(XT [H−1 ΛwwT ΛH−1 − σ2 H−1 ΛH−1 ]X) Tr(σ2 XT H−1 X),
Our Laplacian optimality criterion is thus formulated by minimizing the trace of XT H−1 X.
Definition 1. Laplacian Optimal Design min Z=(z1,··· ,zk) Tr XT ZZT + λ1XLXT + λ2I −1 X (11) where z1, · · · , zk are selected from {x1, · · · , xm}.
Canonical experimental design approaches (e.g. A-Optimal Design, D-Optimal Design, and E-Optimal) only consider linear functions. They fail to discover the intrinsic geometry in the data when the data space is highly nonlinear. In this section, we describe how to perform Laplacian Experimental Design in Reproducing Kernel Hilbert Space (RKHS) which gives rise to Kernel Laplacian Experimental Design (KLOD).
For given data points x1, · · · , xm ∈ X with a positive definite mercer kernel K : X ×X → R, there exists a unique RKHS HK of real valued functions on X. Let Kt(s) be the function of s obtained by fixing t and letting Kt(s) . = K(s, t).
HK consists of all finite linear combinations of the form l i=1 αiKti with ti ∈ X and limits of such functions as the ti become dense in X. We have Ks, Kt HK = K(s, t).
Kernel Hilbert Space Consider the optimization problem (5) in RKHS. Thus, we seek a function f ∈ HK such that the following objective function is minimized: min f∈HK k i=1 f(zi)−yi 2 + λ1 2 m i,j=1 f(xi)−f(xj) 2 Sij+λ2 f 2 HK (12) We have the following proposition.
Proposition 4.1. Let H = { m i=1 αiK(·, xi)|αi ∈ R} be a subspace of HK , the solution to the problem (12) is in H.
Proof. Let H⊥ be the orthogonal complement of H, i.e.
HK = H ⊕ H⊥ . Thus, for any function f ∈ HK , it has orthogonal decomposition as follows: f = fH + fH⊥ Now, let"s evaluate f at xi: f(xi) = f, Kxi HK = fH + fH⊥ , Kxi HK = fH, Kxi HK + fH⊥ , Kxi HK Notice that Kxi ∈ H while fH⊥ ∈ H⊥ . This implies that fH⊥ , Kxi HK = 0. Therefore, f(xi) = fH, Kxi HK = fH(xi) This completes the proof.
Proposition 4.1 tells us the minimizer of problem (12) admits a representation f∗ = m i=1 αiK(·, xi). Please see [2] for the details.
Let φ : Rd → H be a feature map from the input space Rd to H, and K(xi, xj) =< φ(xi), φ(xj) >. Let X denote the data matrix in RKHS, X = (φ(x1), φ(x2), · · · , φ(xm)).
Similarly, we define Z = (φ(z1), φ(z2), · · · , φ(zk)). Thus, the optimization problem in RKHS can be written as follows: min Z Tr XT ZZT + λ1XLXT + λ2I −1 X (13) Since the mapping function φ is generally unknown, there is no direct way to solve problem (13). In the following, we apply kernel tricks to solve this optimization problem. Let X−1 be the Moore-Penrose inverse (also known as pseudo inverse) of X. Thus, we have: XT ZZT + λ1XLXT + λ2I −1 X = XT XX−1 ZZT + λ1XLXT + λ2I −1 (XT )−1 XT X = XT X ZZT X + λ1XLXT X + λ2X −1 (XT )−1 XT X = XT X XT ZZT X + λ1XT XLXT X + λ2XT X −1 XT X = KXX KXZKZX + λ1KXXLKXX + λ2KXX −1 KXX where KXX is a m × m matrix (KXX,ij = K(xi, xj)), KXZ is a m×k matrix (KXZ,ij = K(xi, zj)), and KZX is a k×m matrix (KZX,ij = K(zi, xj)). Thus, the Kernel Laplacian Optimal Design can be defined as follows: Definition 2. Kernel Laplacian Optimal Design minZ=(z1,··· ,zk) Tr KXX KXZKZX + λ1KXXLKXX λ2KXX −1 KXX (14)
In this subsection, we discuss how to solve the optimization problems (11) and (14). Particularly, if we select a linear kernel for KLOD, then it reduces to LOD. Therefore, we will focus on problem (14) in the following.
It can be shown that the optimization problem (14) is NP-hard. In this subsection, we develop a simple sequential greedy approach to solve (14). Suppose n points have been selected, denoted by a matrix Zn = (z1, · · · , zn). The (n + 1)-th point zn+1 can be selected by solving the following optimization problem: max Zn+1=(Zn,zn+1) Tr KXX KXZn+1 KZn+1X + λ1KXXLKXX + λ2KXX −1 KXX (15) The kernel matrices KXZn+1 and KZn+1X can be rewritten as follows: KXZn+1 = KXZn , KXzn+1 , KZn+1X = KZnX Kzn+1X Thus, we have: KXZn+1 KZn+1X = KXZn KZnX + KXzn+1 Kzn+1X We define: A = KXZn KZnX + λ1KXXLKXX + λ2KXX A is only dependent on X and Zn . Thus, the (n + 1)-th point zn+1 is given by: zn+1 = arg min zn+1 Tr KXX A + KXzn+1 Kzn+1X −1 KXX (16) Each time we select a new point zn+1, the matrix A is updated by: A ← A + KXzn+1 Kzn+1X If the kernel function is chosen as inner product K(x, y) = x, y , then HK is a linear functional space and the algorithm reduces to LOD.
USING LAPLACIAN OPTIMAL DESIGN In this section, we describe how to apply Laplacian Optimal Design to CBIR. We begin with a brief description of image representation using low level visual features.
Low-level image representation is a crucial problem in CBIR. General visual features includes color, texture, shape, etc. Color and texture features are the most extensively used visual features in CBIR. Compared with color and texture features, shape features are usually described after images have been segmented into regions or objects. Since robust and accurate image segmentation is difficult to achieve, the use of shape features for image retrieval has been limited to special applications where objects or regions are readily available.
In this work, We combine 64-dimensional color histogram and 64-dimensional Color Texture Moment (CTM, [15]) to represent the images. The color histogram is calculated using 4 × 4 × 4 bins in HSV space. The Color Texture Moment is proposed by Yu et al. [15], which integrates the color and texture characteristics of the image in a compact form. CTM adopts local Fourier transform as a texture representation scheme and derives eight characteristic maps to describe different aspects of co-occurrence relations of image pixels in each channel of the (SVcosH, SVsinH, V) color space. Then CTM calculates the first and second moment of these maps as a representation of the natural color image pixel distribution. Please see [15] for details.
Relevance feedback is one of the most important techniques to narrow down the gap between low level visual features and high level semantic concepts [12].
Traditionally, the user"s relevance feedbacks are used to update the query vector or adjust the weighting of different dimensions.
This process can be viewed as an on-line learning process in which the image retrieval system acts as a learner and the user acts as a teacher. They typical retrieval process is outlined as follows:
system. The system ranks the images in database according to some pre-defined distance metric and presents to the user the top ranked images.
request the user to label them as relevant or irrelevant.
rerank the images in database and returns to the user the top images. Go to step 2 until the user is satisfied.
Our Laplacian Optimal Design algorithm is applied in the second step for selecting the most informative images. Once we get the labels for the images selected by LOD, we apply Laplacian Regularized Regression (LRR, [2]) to solve the optimization problem (3) and build the classifier. The classifier is then used to re-rank the images in database. Note that, in order to reduce the computational complexity, we do not use all the unlabeled images in the database but only those within top 500 returns of previous iteration.
In this section, we evaluate the performance of our proposed algorithm on a large image database. To demonstrate the effectiveness of our proposed LOD algorithm, we compare it with Laplacian Regularized Regression (LRR, [2]),
Support Vector Machine (SVM), Support Vector Machine Active Learning (SVMactive) [14], and A-Optimal Design (AOD). Both SVMactive, AOD, and LOD are active learning algorithms, while LRR and SVM are standard classification algorithms. SVM only makes use of the labeled images, while LRR is a semi-supervised learning algorithm which makes use of both labeled and unlabeled images. For SVMactive, AOD, and LOD, 10 training images are selected by the algorithms themselves at each iteration. While for LRR and SVM, we use the top 10 images as training data.
It would be important to note that SVMactive is based on the ordinary SVM, LOD is based on LRR, and AOD is based on the ordinary regression. The parameters λ1 and λ2 in our LOD algorithm are empirically set to be 0.001 and 0.00001.
For both LRR and LOD algorithms, we use the same graph structure (see Eq. 4) and set the value of p (number of nearest neighbors) to be 5. We begin with a simple synthetic example to give some intuition about how LOD works.
A simple synthetic example is given in Figure 1. The data set contains two circles. Eight points are selected by AOD and LOD. As can be seen, all the points selected by AOD are from the big circle, while LOD selects four points from the big circle and four from the small circle. The numbers beside the selected points denote their orders to be selected.
Clearly, the points selected by our LOD algorithm can better represent the original data set. We did not compare our algorithm with SVMactive because SVMactive can not be applied in this case due to the lack of the labeled points.
The image database we used consists of 7,900 images of 79 semantic categories, from COREL data set. It is a large and heterogeneous image set. Each image is represented as a 128-dimensional vector as described in Section 5.1. Figure 2 shows some sample images.
To exhibit the advantages of using our algorithm, we need a reliable way of evaluating the retrieval performance and the comparisons with other algorithms. We list different aspects of the experimental design below.
We use precision-scope curve and precision rate [10] to evaluate the effectiveness of the image retrieval algorithms.
The scope is specified by the number (N) of top-ranked images presented to the user. The precision is the ratio of the number of relevant images presented to the user to the (a) Data set 1 2 3 4 5 6 7 8 (b) AOD 1 2 3 4 5 6 7 8 (c) LOD Figure 1: Data selection by active learning algorithms. The numbers beside the selected points denote their orders to be selected. Clearly, the points selected by our LOD algorithm can better represent the original data set. Note that, the SVMactive algorithm can not be applied in this case due to the lack of labeled points. (a) (b) (c) Figure 2: Sample images from category bead, elephant, and ship. scope N. The precision-scope curve describes the precision with various scopes and thus gives an overall performance evaluation of the algorithms. On the other hand, the precision rate emphasizes the precision at a particular value of scope. In general, it is appropriate to present 20 images on a screen. Putting more images on a screen may affect the quality of the presented images. Therefore, the precision at top 20 (N = 20) is especially important.
In real world image retrieval systems, the query image is usually not in the image database. To simulate such environment, we use five-fold cross validation to evaluate the algorithms. More precisely, we divide the whole image database into five subsets with equal size. Thus, there are 20 images per category in each subset. At each run of cross validation, one subset is selected as the query set, and the other four subsets are used as the database for retrieval. The precisionscope curve and precision rate are computed by averaging the results from the five-fold cross validation.
We designed an automatic feedback scheme to model the retrieval process. For each submitted query, our system retrieves and ranks the images in the database. 10 images were selected from the database for user labeling and the label information is used by the system for re-ranking. Note that, the images which have been selected at previous iterations are excluded from later selections. For each query, the automatic relevance feedback mechanism is performed for four iterations.
It is important to note that the automatic relevance feedback scheme used here is different from the ones described in [8], [11]. In [8], [11], the top four relevant and irrelevant images were selected as the feedback images. However, this may not be practical. In real world image retrieval systems, it is possible that most of the top-ranked images are relevant (or, irrelevant). Thus, it is difficult for the user to find both four relevant and irrelevant images. It is more reasonable for the users to provide feedback information only on the 10 images selected by the system.
In real world, it is not practical to require the user to provide many rounds of feedbacks. The retrieval performance after the first two rounds of feedbacks (especially the first round) is more important. Figure 3 shows the average precision-scope curves of the different algorithms for the first two feedback iterations. At the beginning of retrieval, the Euclidean distances in the original 128-dimensional space are used to rank the images in database. After the user provides relevance feedbacks, the LRR, SVM, SVMactive,
AOD, and LOD algorithms are then applied to re-rank the images. In order to reduce the time complexity of active learning algorithms, we didn"t select the most informative images from the whole database but from the top 500 images. For LRR and SVM, the user is required to label the top 10 images. For SVMactive, AOD, and LOD, the user is required to label 10 most informative images selected by these algorithms. Note that, SVMactive can only be ap(a) Feedback Iteration 1 (b) Feedback Iteration 2 Figure 3: The average precision-scope curves of different algorithms for the first two feedback iterations. The LOD algorithm performs the best on the entire scope. Note that, at the first round of feedback, the SVMactive algorithm can not be applied. It applies the ordinary SVM to build the initial classifier. (a) Precision at Top 10 (b) Precision at Top 20 (c) Precision at Top 30 Figure 4: Performance evaluation of the five learning algorithms for relevance feedback image retrieval. (a) Precision at top 10, (b) Precision at top 20, and (c) Precision at top 30. As can be seen, our LOD algorithm consistently outperforms the other four algorithms. plied when the classifier is already built. Therefore, it can not be applied at the first round and we use the standard SVM to build the initial classifier. As can be seen, our LOD algorithm outperforms the other four algorithms on the entire scope. Also, the LRR algorithm performs better than SVM. This is because that the LRR algorithm makes efficient use of the unlabeled images by incorporating a locality preserving regularizer into the ordinary regression objective function. The AOD algorithm performs the worst. As the scope gets larger, the performance difference between these algorithms gets smaller.
By iteratively adding the user"s feedbacks, the corresponding precision results (at top 10, top 20, and top 30) of the five algorithms are respectively shown in Figure 4. As can be seen, our LOD algorithm performs the best in all the cases and the LRR algorithm performs the second best. Both of these two algorithms make use of the unlabeled images. This shows that the unlabeled images are helpful for discovering the intrinsic geometrical structure of the image space and therefore enhance the retrieval performance. In real world, the user may not be willing to provide too many relevance feedbacks. Therefore, the retrieval performance at the first two rounds are especially important. As can be seen, our LOD algorithm achieves 6.8% performance improvement for top 10 results, 5.2% for top 20 results, and 4.1% for top 30 results, comparing to the second best algorithm (LRR) after the first two rounds of relevance feedbacks.
Several experiments on Corel database have been systematically performed. We would like to highlight several interesting points:
in the image retrieval domain. There is a significant increase in performance from using the active learning methods. Especially, out of the three active learning methods (SVMactive, AOD, LOD), our proposed LOD algorithm performs the best.
feedback image retrieval, there are generally two ways of reducing labor-intensive manual labeling task. One is active learning which selects the most informative samples to label, and the other is semi-supervised learning which makes use of the unlabeled samples to enhance the learning performance. Both of these two strategies have been studied extensively in the past [14], [7], [5], [8]. The work presented in this paper is focused on active learning, but it also takes advantage of the recent progresses on semi-supervised learning [2].
Specifically, we incorporate a locality preserving regularizer into the standard regression framework and find the most informative samples with respect to the new objective function. In this way, the active learning and semi-supervised learning techniques are seamlessly unified for learning an optimal classifier.
retrieval. For all the five algorithms, the retrieval performance improves with more feedbacks provided by the user.
This paper describes a novel active learning algorithm, called Laplacian Optimal Design, to enable more effective relevance feedback image retrieval. Our algorithm is based on an objective function which simultaneously minimizes the empirical error and preserves the local geometrical structure of the data space. Using techniques from experimental design, our algorithm finds the most informative images to label. These labeled images and the unlabeled images in the database are used to learn a classifier. The experimental results on Corel database show that both active learning and semi-supervised learning can significantly improve the retrieval performance.
In this paper, we consider the image retrieval problem on a small, static, and closed-domain image data. A much more challenging domain is the World Wide Web (WWW). For Web image search, it is possible to collect a large amount of user click information. This information can be naturally used to construct the affinity graph in our algorithm.
However, the computational complexity in Web scenario may become a crucial issue. Also, although our primary interest in this paper is focused on relevance feedback image retrieval, our results may also be of interest to researchers in patten recognition and machine learning, especially when a large amount of data is available but only a limited samples can be labeled.
[1] A. C. Atkinson and A. N. Donev. Optimum Experimental Designs. Oxford University Press, 2002. [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from examples. Journal of Machine Learning Research, 7:2399-2434, 2006. [3] F. R. K. Chung. Spectral Graph Theory, volume 92 of Regional Conference Series in Mathematics. AMS,
[4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129-145, 1996. [5] A. Dong and B. Bhanu. A new semi-supervised em algorithm for image retrieval. In IEEE Conf. on Computer Vision and Pattern Recognition, Madison,
WI, 2003. [6] P. Flaherty, M. I. Jordan, and A. P. Arkin. Robust design of biological experiments. In Advances in Neural Information Processing Systems 18, Vancouver,
Canada, 2005. [7] K.-S. Goh, E. Y. Chang, and W.-C. Lai. Multimodal concept-dependent active learning for image retrieval.
In Proceedings of the ACM Conference on Multimedia,
New York, October 2004. [8] X. He. Incremental semi-supervised subspace learning for image retrieval. In Proceedings of the ACM Conference on Multimedia, New York, October 2004. [9] S. C. Hoi and M. R. Lyu. A semi-supervised active learning framework for image retrieval. In IEEE International Conference on Computer Vision and Pattern Recognition, San Diego, CA, 2005. [10] D. P. Huijsmans and N. Sebe. How to complete performance graphs in content-based image retrieval: Add generality and normalize scope. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(2):245-251, 2005. [11] Y.-Y. Lin, T.-L. Liu, and H.-T. Chen. Semantic manifold learning for image retrieval. In Proceedings of the ACM Conference on Multimedia, Singapore,
November 2005. [12] Y. Rui, T. S. Huang, M. Ortega, and S. Mehrotra.
Relevance feedback: A power tool for interative content-based image retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 8(5), 1998. [13] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain. Content-based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(12):1349-1380,

Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.
In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.
Snippets are short fragments of text extracted from the document content (or its metadata). They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20]. A query-biased snippet is one selectively extracted on the basis of its relation to the searcher"s query.
The addition of informative snippets to search results may substantially increase their value to searchers. Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.
In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searcher"s real information need, such as the contact details of a person or an organization.
Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets). The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required. The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load. Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems. Special-purpose filesystems have been built to address these problems [6].
Note that the utility of snippets is by no means restricted to whole-of-Web search applications. Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages). Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.
We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.
We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.
As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required. However, this is only true if there is no caching of documents in RAM.
Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation. In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].
As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document. This leads to further time savings, with only marginal impact on the quality of the snippets returned.
Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query. This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.
Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document. Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms. To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.
Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries. Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature. Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm. White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper.
A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1. Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation. Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine. Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources. Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram. For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted. Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.
In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.
The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.
There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries. When one of the popular queries is issued, the result page is fetched straight from the query cache. If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page.
term, using them to get a ranked list of documents.
query term numbers to generate snippets.
each document to construct the results page.
IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.
OUT Remove the number of sentences required from the heap to form the summary.
Figure 2: Simple sentence ranker that operates on raw text with one sentence per line.
For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.
Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12]. Accordingly, we also assume a web snippet extraction process will extract sentences from documents. In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet. The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.
Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document. The final score of a sentence, assigned in Step 7, can be derived in many different ways. In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components.
Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.
We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens. We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.
Both are limited to a maximum of 50 characters. Adjacent, repeating characters are removed from the punctuation.
Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ?!.. Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences. In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].
Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.
A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content. For example, The most wonderful company on earth. Products. Service. About us. Contact us. Try before you buy. Similar, but often not identical, navigational material is typically presented on every page within a site. This material tends to lower the quality of summaries and slow down summary generation.
In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing. In wt100g, the average web page size is more than half the current Web average [11].
Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.
Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison.
An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2. Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.
Each document is stored in a single file. While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].
For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed. The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching.
Several optimizations over the baseline are possible. The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24]. Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.
Words and non-words strictly alternate in the compressed file, which always begins with a word.
In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency. We use the vbyte coding scheme to code the word tokens [22]. The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code. The remaining 2 bits of each punctuation symbol are used to store capitalization information.
The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high. If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23]. Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model. Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.
During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words. Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue. In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one. If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.
The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.
Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model. We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.
We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.
While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file. That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.
The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file. Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings. The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping. This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine.
All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10. The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No. Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60
Time(seconds) 0 20 40 60
Time(seconds) 0 20 40 60
Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.
RAM. All source code was compiled using gcc4.1.1 with -O9 optimisation. Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.
In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.
Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable. Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy. We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period. Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g. The first two rows of Table 1 give the number of documents and the size in Mb of these collections.
The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes. As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size. Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM. It is 1024 Mb in these experiments.
The Zettair search engine [25] was used to produce a list of documents to summarize for each query. For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings. For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query. This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine. In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.
Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log. Only the data for wt10g is shown, but the other collections showed similar patterns. The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100. Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed. All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests. This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.
Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching. But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure). This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times. This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris). With disk caching turned off, the average time to generate snippets varies little.
The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2. Once the system has stabilized, CTS is over 50% faster than the Baseline system. This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.
Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed. As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents. Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet. As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching.
In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection. For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM. In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available. In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.
We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted. Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.
The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.
All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically. This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.
Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g. The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.
From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation. With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided. The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles). In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.
The reason for the large impact of the document cache is
020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection. Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others. This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17]. Documents with high static scores are much more likely to be retrieved than others.
In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here). This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.
For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).
In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times. This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols. Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1. With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size. Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols). Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models. Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.
Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache. Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk. This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section.
Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document. Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored. Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk. Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.
We now introduce four sentence reordering approaches.
document usually best describe the document content [12].
Thus simply processing a document in order should yield a quality snippet. Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit. More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document.
of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20]. Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms. The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence. The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence. We then sort the sentences by this score.
and a small number of queries make up a large volume of total searches [9]. In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted. In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log. To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1.
but repeated terms in the sentence are only counted once.
By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k). Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.
Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method. It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components. The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered. The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.
Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.
Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST).
In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines. Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem. Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation. Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided. Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12]. Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.
Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created. We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.
In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.
We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine. We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice. Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]). Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents. Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.
The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.
Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine. Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services. These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.
In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.
Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20]. The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words. The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.
A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.) As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper. We will explore alternate compression schemes in future work.
Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT). Thanks to Nick Lester and Justin Zobel for valuable discussions.
[1] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak,
D. Sivakumar, J. A. Tomlin, and D. P. Williamson.
Searching the workplace web. In WWW2003,
Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.
Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler. Zlib Compression Library. www.zlib.net. Accessed January 2007. [5] S. Garcia, H.E. Williams, and A. Cannane.
Access-ordered indexes. In V. Estivill-Castro, editor,
Proc. Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung. The google file system. In SOSP "03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003. ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. Summarizing text documents: sentence selection and evaluation metrics. In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.
Overview of TREC-7 Very Large Collection Track. In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen. A temporal comparison of altavista web searching. J. Am. Soc.
Inf. Sci. Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L. Giles. Accessibility of information on the web. Nature, 400:107-109, July
[12] H.P. Luhn. The automatic creation of literature abstracts. IBM Journal, pages 159-165, April 1958. [13] I. Mani. Automatic Summarization, volume 3 of Natural Language Processing. John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman. Text compression for dynamic document databases.
Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen. Compressed full text indexes. ACM Computing Surveys, 2007. To appear. [16] D. R. Radev, E. Hovy, and K. McKeown. Introduction to the special issue on summarization. Comput.

The major commercial Web search engines all present their results in much the same way. Each search result is described by a brief caption, comprising the URL of the associated Web page, a title, and a brief summary (or snippet) describing the contents of the page. Often the snippet is extracted from the Web page itself, but it may also be taken from external sources, such as the human-generated summaries found in Web directories.
Figure 1 shows a typical Web search, with captions for the top three results. While the three captions share the same basic structure, their content differs in several respects. The snippet of the third caption is nearly twice as long as that of the first, while the snippet is missing entirely from the second caption. The title of the third caption contains all of the query terms in order, while the titles of the first and second captions contain only two of the three terms. One of the query terms is repeated in the first caption. All of the query terms appear in the URL of the third caption, while none appear in the URL of the first caption. The snippet of the first caption consists of a complete sentence that concisely describes the associated page, while the snippet of the third caption consists of two incomplete sentences that are largely unrelated to the overall contents of the associated page and to the apparent intent of the query.
While these differences may seem minor, they may also have a substantial impact on user behavior. A principal motivation for providing a caption is to assist the user in determining the relevance of the associated page without actually having to click through to the result. In the case of a navigational query - particularly when the destination is well known - the URL alone may be sufficient to identify the desired page. But in the case of an informational query, the title and snippet may be necessary to guide the user in selecting a page for further study, and she may judge the relevance of a page on the basis of the caption alone.
When this judgment is correct, it can speed the search process by allowing the user to avoid unwanted material.
When it fails, the user may waste her time clicking through to an inappropriate result and scanning a page containing little or nothing of interest. Even worse, the user may be misled into skipping a page that contains desired information.
All three of the results in figure 1 are relevant, with some limitations. The first result links to the main Yahoo Kids! homepage, but it is then necessary to follow a link in a menu to find the main page for games. Despite appearances, the second result links to a surprisingly large collection of online games, primarily with environmental themes. The third result might be somewhat disappointing to a user, since it leads to only a single game, hosted at the Centers for Disease Control, that could not reasonably be described as online.
Unfortunately, these page characteristics are not entirely reflected in the captions.
In this paper, we examine the influence of caption features on user"s Web search behavior, using clickthroughs extracted from search engines logs as our primary investigative tool. Understanding this influence may help to validate algorithms and guidelines for the improved generation of the Figure 1: Top three results for the query: kids online games. captions themselves. In addition, these features can play a role in the process of inferring relevance judgments from user behavior [1]. By better understanding their influence, better judgments may result.
Different caption generation algorithms might select snippets of different lengths from different areas of a page.
Snippets may be generated in a query-independent fashion, providing a summary of the page as a whole, or in a querydependent fashion, providing a summary of how the page relates to the query terms. The correct choice of snippet may depend on aspects of both the query and the result page. The title may be taken from the HTML header or extracted from the body of the document [8]. For links that re-direct, it may be possible to display alternative URLs.
Moreover, for pages listed in human-edited Web directories such as the Open Directory Project1 , it may be possible to display alternative titles and snippets derived from these listings.
When these alternative snippets, titles and URLs are available, the selection of an appropriate combination for display may be guided by their features. A snippet from a Web directory may consist of complete sentences and be less fragmentary than an extracted snippet. A title extracted from the body may provide greater coverage of the query terms.
A URL before re-direction may be shorter and provide a clearer idea of the final destination.
The work reported in this paper was undertaken in the context of the Windows Live search engine. The image in figure 1 was captured from Windows Live and cropped to eliminate branding, advertising and navigational elements. The experiments reported in later sections are based on Windows Live query logs, result pages and relevance judgments collected as part of ongoing research into search engine performance [1,2]. Nonetheless, given the similarity of caption formats across the major Web search engines we believe the results are applicable to these other engines. The query in 1 www.dmoz.org figure 1 produces results with similar relevance on the other major search engines. This and other queries produce captions that exhibit similar variations. In addition, we believe our methodology may be generalized to other search applications when sufficient clickthrough data is available.
While commercial Web search engines have followed similar approaches to caption display since their genesis, relatively little research has been published about methods for generating these captions and evaluating their impact on user behavior. Most related research in the area of document summarization has focused on newspaper articles and similar material, rather than Web pages, and has conducted evaluations by comparing automatically generated summaries with manually generated summaries. Most research on the display of Web results has proposed substantial interface changes, rather than addressing details of the existing interfaces.
Varadarajan and Hristidis [16] are among the few who have attempted to improve directly upon the snippets generated by commercial search systems, without introducing additional changes to the interface. They generated snippets from spanning trees of document graphs and experimentally compared these snippets against the snippets generated for the same documents by the Google desktop search system and MSN desktop search system. They evaluated their method by asking users to compare snippets from the various sources.
Cutrell and Guan [4] conducted an eye-tracking study to investigate the influence of snippet length on Web search performance and found that the optimal snippet length varied according to the task type, with longer snippets leading to improved performance for informational tasks and shorter snippets for navigational tasks.
Many researchers have explored alternative methods for displaying Web search results. Dumais et al. [5] compared an interface typical of those used by major Web search engines with one that groups results by category, finding that users perform search tasks faster with the category interface. Paek et al. [12] propose an interface based on a fisheye lens, in which mouse hovers and other events cause captions to zoom and snippets to expand with additional text.
White et al. [17] evaluated three alternatives to the standard Web search interface: one that displays expanded summaries on mouse hovers, one that displays a list of top ranking sentences extracted from the results taken as a group, and one that updates this list automatically through implicit feedback. They treat the length of time that a user spends viewing a summary as an implicit indicator of relevance. Their goal was to improve the ability of users to interact with a given result set, helping them to look beyond the first page of results and to reduce the burden of query re-formulation.
Outside the narrow context of Web search considerable related research has been undertaken on the problem of document summarization. The basic idea of extractive summarization - creating a summary by selecting sentences or fragments - goes back to the foundational work of Luhn [11].
Luhn"s approach uses term frequencies to identify significant words within a document and then selects and extracts sentences that contain significant words in close proximity.
A considerable fraction of later work may be viewed as extending and tuning this basic approach, developing improved methods for identifying significant words and selecting sentences. For example, a recent paper by Sun et al. [14] describes a variant of Luhn"s algorithm that uses clickthrough data to identify significant words. At its simplest, snippet generation for Web captions might also be viewed as following this approach, with query terms taking on the role of significant words.
Since 2000, the annual Document Understanding Conference (DUC) series, conducted by the US National Institute of Standards and Technology, has provided a vehicle for evaluating much of the research in document summarization2 . Each year DUC defines a methodology for one or more experimental tasks, and supplies the necessary test documents, human-created summaries, and automatically extracted baseline summaries. The majority of participating systems use extractive summarization, but a number attempt natural language generation and other approaches.
Evaluation at DUC is achieved through comparison with manually generated summaries. Over the years DUC has included both single-document summarization and multidocument summarization tasks. The main DUC 2007 task is posed as taking place in a question answering context.
Given a topic and 25 documents, participants were asked to generate a 250-word summary satisfying the information need enbodied in the topic. We view our approach of evaluating summarization through the analysis of Web logs as complementing the approach taken at DUC.
A number of other researchers have examined the value of query-dependent summarization in a non-Web context.
Tombros and Sanderson [15] compared the performance of 20 subjects searching a collection of newspaper articles when 2 duc.nist.gov guided by query-independent vs. query-dependent snippets.
The query-independent snippets were created by extracting the first few sentences of the articles; the query-dependent snippets were created by selecting the highest scoring sentences under a measure biased towards sentences containing query terms. When query-dependent summaries were presented, subjects were better able to identify relevant documents without clicking through to the full text.
Goldstein et al. [6] describe another extractive system for generating query-dependent summaries from newspaper articles. In their system, sentences are ranked by combining statistical and linguistic features. They introduce normalized measures of recall and precision to facilitate evaluation.
Queries and clickthroughs taken from the logs of commercial Web search engines have been widely used to improve the performance of these systems and to better understand how users interact with them. In early work, Broder [3] examined the logs of the AltaVista search engine and identified three broad categories of Web queries: informational, navigational and transactional. Rose and Levinson [13] conducted a similar study, developing a hierarchy of query goals with three top-level categories: informational, navigational and resource. Under their taxonomy, a transactional query as defined by Broder might fall under either of their three categories, depending on details of the desired transaction.
Lee et al. [10] used clickthrough patterns to automatically categorize queries into one of two categories: informational - for which multiple Websites may satisfy all or part of the user"s need - and navigational - for which users have a particular Website in mind. Under their taxonomy, a transactional or resource query would be subsumed under one of these two categories.
Agichtein et al. interpreted caption features, clickthroughs and other user behavior as implicit feedback to learn preferences [2] and improve ranking [1] in Web search. Xue et al. [18] present several methods for associating queries with documents by analyzing clickthrough patterns and links between documents. Queries associated with documents in this way are treated as meta-data. In effect, they are added to the document content for indexing and ranking purposes.
Of particular interest to us is the work of Joachims et al. [9] and Granka et al. [7]. They conducted eye-tracking studies and analyzed log data to determine the extent to which clickthrough data may be treated as implicit relevance judgments. They identified a trust bias, which leads users to prefer the higher ranking result when all other factors are equal. In addition, they explored techniques that treat clicks as pairwise preferences. For example, a click at position N + 1 - after skipping the result at position N - may be viewed as a preference for the result at position N+1 relative to the result at position N. These findings form the basis of the clickthrough inversion methodology we use to interpret user interactions with search results. Our examination of large search logs compliments their detailed analysis of a smaller number of participants.
While other researchers have evaluated the display of Web search results through user studies - presenting users with a small number of different techniques and asking them to complete experimental tasks - we approach the problem by extracting implicit feedback from search engine logs.
Examining user behavior in situ allows us to consider many more queries and caption characteristics, with the volume of available data compensating for the lack of a controlled lab environment.
The problem remains of interpreting the information in these logs as implicit indicators of user preferences, and in this matter we are guided by the work of Joachims et al. [9].
We consider caption pairs, which appear adjacent to one another in the result list.
Our primary tool for examining the influence of caption features is a type of pattern observed with respect to these caption pairs, which we call a clickthrough inversion. A clickthrough inversion occurs at position N when the result at position N receives fewer clicks than the result at position N + 1. Following Joachims et al. [9], we interpret a clickthrough inversion as indicating a preference for the lower ranking result, overcoming any trust bias. For simplicity, in the remainder of this paper we refer to the higher ranking caption in a pair as caption A and the lower ranking caption as caption B.
For the experiments reported in this paper, we sampled a subset of the queries and clickthroughs from the logs of the Windows Live search engine over a period of 3-4 days on three separate occasions: once for results reported in section 3.3, once for a pilot of our main experiment, and once for the experiment itself (sections 4 and 5). For simplicity we restricted our sample to queries submitted to the US English interface and ignored any queries containing complex or non-alphanumeric terms (e.g. operators and phrases). At the end of each sampling period, we downloaded captions for the queries associated with the clickthrough sample.
When identifying clickthroughs in search engine logs, we consider only the first clickthrough action taken by a user after entering a query and viewing the result page. Users are identified by IP address, which is a reasonably reliable method of eliminating multiple results from a single user, at the cost of falsely eliminating results from multiple users sharing the same address.
By focusing on the initial clickthrough, we hope to capture a user"s impression of the relative relevance within a caption pair when first encountered. If the user later clicks on other results or re-issues the same query, we ignore these actions. Any preference captured by a clickthrough inversion is therefore a preference among a group of users issuing a particular query, rather than a preference on the part of a single user. In the remainder of the paper, we use the term clickthrough to refer only to this initial action.
Given the dynamic nature of the Web and the volumes of data involved, search engine logs are bound to contain considerable noise. For example, even over a period of hours or minutes the order of results for a given query can change, with some results dropping out of the top ten and new ones appearing. For this reason, we retained clickthroughs for a specific combination of a query and a result only if this result appears in a consistent position for at least 50% of the clickthroughs. Clickthroughs for the same result when it appeared at other positions were discarded. For similar reasons, if we did not detect at least ten clickthroughs for a particular query during the sampling period, no clickthroughs for that query were retained. 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position a) craigslist 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position b) periodic table of elements 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position c) kids online games Figure 2: Clickthrough curves for three queries: a) a stereotypical navigational query, b) a stereotypical informational query, and c) a query exhibiting clickthrough inversions.
The outcome at the end of each sampling period is a set of records, with each record describing the clickthroughs for a given query/result combination. Each record includes a query, a result position, a title, a snippet, a URL, the number of clickthroughs for this result, and the total number of clickthroughs for this query. We then processed this set to generate clickthrough curves and identify inversions.
It could be argued that under ideal circumstances, clickthrough inversions would not be present in search engine logs. A hypothetical perfect search engine would respond to a query by placing the result most likely to be relevant first in the result list. Each caption would appropriately summarize the content of the linked page and its relationship to the query, allowing users to make accurate judgments. Later results would complement earlier ones, linking to novel or supplementary material, and ordered by their interest to the greatest number of users.
Figure 2 provides clickthrough curves for three example queries. For each example, we plot the percentage of clickthroughs against position for the top ten results. The first query (craigslist) is stereotypically navigational, showing a spike at the correct answer (www.craigslist.org). The second query is informational in the sense of Lee et al. [10] (periodic table of elements). Its curve is flatter and less skewed toward a single result. For both queries, the number of clickthroughs is consistent with the result positions, with the percentage of clickthroughs decreasing monotonically as position increases, the ideal behavior.
Regrettably, no search engine is perfect, and clickthrough inversions are seen for many queries. For example, for the third query (kids online games) the clickthrough curve exhibits a number of clickthrough inversions, with an apparent preference for the result at position 4.
Several causes may be enlisted to explain the presence of an inversion in a clickthrough curve. The search engine may have failed in its primary goal, ranking more relevant results below less relevant results. Even when the relative ranking is appropriate, a caption may fail to reflect the content of the underlying page with respect to the query, leading the user to make an incorrect judgment. Before turning to the second case, we address the first, and examine the extent to which relevance alone may explain these inversions.
The simplest explanation for the presence of a clickthrough inversion is a relevance difference between the higher ranking member of caption pair and the lower ranking member.
In order to examine the extent to which relevance plays a role in clickthrough inversions, we conducted an initial experiment using a set of 1,811 queries with associated judgments created as part of on-going work. Over a four-day period, we sampled the search engine logs and extracted over one hundred thousand clicks involving these queries. From these clicks we identified 355 clickthrough inversions, satisfying the criteria of section 3.1, where relevance judgments existed for both pages.
The relevance judgments were made by independent assessors viewing the pages themselves, rather than the captions.
Relevance was assessed on a 6-point scale. The outcome is presented in figure 3, which shows the explicit judgments for the 355 clickthrough inversions. In all of these cases, there were more clicks on the lower ranked member of the Relationship Number Percent rel(A) < rel(B) 119 33.5% rel(A) = rel(B) 134 37.7% rel(A) > rel(B) 102 28.7% Figure 3: Relevance relationships at clickthrough inversions. Compares relevance between the higher ranking member of a caption pair (rel(A)) to the relevance of the lower ranking member (rel(B)), where caption A received fewer clicks than caption B. pair (B). The figure shows the corresponding relevance judgments. For example, the first row rel(A) < rel(B), indicates that the higher ranking member of pair (A) was rated as less relevant than the lower ranking member of the pair (B).
As we see in the figure, relevance alone appears inadequate to explain the majority of clickthrough inversions. For twothirds of the inversions (236), the page associated with caption A is at least as relevant as the page associated with caption B. For 28.7% of the inversions, A has greater relevance than B, which received the greater number of clickthroughs.
Having demonstrated that clickthrough inversions cannot always be explained by relevance differences, we explore what features of caption pairs, if any, lead users to prefer one caption over another. For example, we may hypothesize that the absence of a snippet in caption A and the presence of a snippet in caption B (e.g. captions 2 and 3 in figure 1) leads users to prefer caption A. Nonetheless, due to competing factors, a large set of clickthrough inversions may also include pairs where the snippet is missing in caption B and not in caption A. However, if we compare a large set of clickthrough inversions to a similar set of pairs for which the clickthroughs are consistent with their ranking, we would expect to see relatively more pairs where the snippet was missing in caption A.
Following this line of reasoning, we extracted two sets of caption pairs from search logs over a three day period.
The first is a set of nearly five thousand clickthrough inversions, extracted according to the procedure described in section 3.1. The second is a corresponding set of caption pairs that do not exhibit clickthrough inversions. In other words, for pairs in this set, the result at the higher rank (caption A) received more clickthroughs than the result at the lower rank (caption B). To the greatest extent possible, each pair in the second set was selected to correspond to a pair in the first set, in terms of result position and number of clicks on each result. We refer to the first set, containing clickthrough inversions, as the INV set; we refer to the second set, containing caption pairs for which the clickthroughs are consistent with their rank order, as the CON set.
We extract a number of features characterizing snippets (described in detail in the next section) and compare the presence of each feature in the INV and CON sets. We describe the features as a hypothesized preference (e.g., a preference for captions containing a snippet). Thus, in either set, a given feature may be present in one of two forms: favoring the higher ranked caption (caption A) or favoring the lower ranked caption (caption B). For example, the abFeature Tag Description MissingSnippet snippet missing in caption A and present in caption B SnippetShort short snippet in caption A (< 25 characters) with long snippet (> 100 characters) in caption B TermMatchTitle title of caption A contains matches to fewer query terms than the title of caption B TermMatchTS title+snippet of caption A contains matches to fewer query terms than the title+snippet of caption B TermMatchTSU title+snippet+URL of caption A contains matches to fewer query terms than caption B TitleStartQuery title of caption B (but not A) starts with a phrase match to the query QueryPhraseMatch title+snippet+url contains the query as a phrase match MatchAll caption B contains one match to each term; caption A contains more matches with missing terms URLQuery caption B URL is of the form www.query.com where the query matches exactly with spaces removed URLSlashes caption A URL contains more slashes (i.e. a longer path length) than the caption B URL URLLenDIff caption A URL is longer than the caption B URL Official title or snippet of caption B (but not A) contains the term official (with stemming) Home title or snippet of caption B (but not A) contains the phrase home page Image title or snippet of caption B (but not A) contains a term suggesting the presence of an image gallery Readable caption B (but not A) passes a simple readability test Figure 4: Features measured in caption pairs (caption A and caption B), with caption A as the higher ranked result. These features are expressed from the perspective of the prevalent relationship predicted for clickthrough inversions. sence of a snippet in caption A favors caption B, and the absence of a snippet in caption B favors caption A. When the feature favors caption B (consistent with a clickthrough inversion) we refer to the caption pair as a positive pair.
When the feature favors caption A, we refer to it as a negative pair. For missing snippets, a positive pair has the caption missing in caption A (but not B) and a negative pair has the caption missing in B (but not A).
Thus, for a specific feature, we can construct four subsets: 1) INV+, the set of positive pairs from INV; 2) INV−, the set of negative pairs from INV; 3) CON+; the set of positive pairs from CON; and 4) CON− the set of negative pairs from CON. The sets INV+, INV−, CON+, and CON− will contain different subsets of INV and CON for each feature.
When stating a feature corresponding to a hypothesized user preference, we follow the practice of stating the feature with the expectation that the size of INV+ relative to the size of INV− should be greater than the size of CON+ relative to the size of CON−. For example, we state the missing snippet feature as snippet missing in caption A and present in caption B.
This evaluation methodology allows us to construct a contingency table for each feature, with INV essentially forming the experimental group and CON the control group. We can then apply Pearson"s chi-square test for significance.
Figure 4 lists the features tested. Many of the features on this list correspond to our own assumptions regarding the importance of certain caption characteristics: the presence of query terms, the inclusion of a snippet, and the importance of query term matches in the title. Other features suggested themselves during the examination of the snippets collected as part of the study described in section 3.3 and during a pilot of the evaluation methodology (section 4.1).
For this pilot we collected INV and CON sets of similar sizes, and used these sets to evaluate a preliminary list of features and to establish appropriate parameters for the SnippetShort and Readable features. In the pilot, all of the features list in figure 4 were significant at the 95% level. A small number of other features were dropped after the pilot.
These features all capture simple aspects of the captions.
The first feature concerns the existence of a snippet and the second concerns the relative size of snippets. Apart from this first feature, we ignore pairs where one caption has a missing snippet. These pairs are not included in the sets constructed for the remaining features, since captions with missing snippets do not contain all the elements of a standard caption and we wanted to avoid their influence.
The next six features concern the location and number of matching query terms. For the first five, a match for each query term is counted only once, additional matches for the same term are ignored. The MatchAll feature tests the idea that matching all the query terms exactly once is preferable to matching a subset of the terms many times with a least one query term unmatched.
The next three features concern the URLs, capturing aspects of their length and complexity, and the last four features concern caption content. The first two of these content features (Official and Home) suggest claims about the importance or significance of the associated page. The third content feature (Image) suggests the presence of an image gallery, a popular genre of Web page. Terms represented by this feature include pictures, pics, and gallery.
The last content feature (Readable) applies an ad-hoc readability metric to each snippet. Regular users of Web search engines may notice occasional snippets that consist of little more than lists of words and phrases, rather than a coherent description. We define our own metric, since the Flesch-Kincaid readability score and similar measures are intended for entire documents not text fragments. While the metric has not been experimentally validated, it does reflect our intuitions and observations regarding result snippets. In English, the 100 most frequent words represent about 48% of text, and we would expect readable prose, as opposed to a disjointed list of words, to contain these words in roughly this proportion. The Readable feature computes the percentage of these top-100 words appearing in each caption.
If these words represent more than 40% of one caption and less than 10% of the other, the pair is included in the appropriate set.
Feature Tag INV+ INV− %+ CON+ CON− %+ χ2 p-value MissingSnippet 185 121 60.4 144 133 51.9 4.2443 0.0393 SnippetShort 20 6 76.9 12 16 42.8 6.4803 0.0109 TermMatchTitle 800 559 58.8 660 700 48.5 29.2154 <.0001 TermMatchTS 310 213 59.2 269 216 55.4 1.4938 0.2216 TermMatchTSU 236 138 63.1 189 149 55.9 3.8088 0.0509 TitleStartQuery 1058 933 53.1 916 1096 45.5 23.1999 <.0001 QueryPhraseMatch 465 346 57.3 427 422 50.2 8.2741 0.0040 MatchAll 8 2 80.0 1 4 20.0 0.0470 URLQuery 277 188 59.5 159 315 33.5 63.9210 <.0001 URLSlashes 1715 1388 55.2 1380 1758 43.9 79.5819 <.0001 URLLenDiff 2288 2233 50.6 2062 2649 43.7 43.2974 <.0001 Official 215 142 60.2 133 215 38.2 34.1397 <.0001 Home 62 49 55.8 64 82 43.8 3.6458 0.0562 Image 391 270 59.1 315 335 48.4 15.0735 <.0001 Readable 52 43 54.7 31 48 39.2 4.1518 0.0415 Figure 5: Results corresponding to the features listed in figure 4 with χ2 and p-values (df = 1). Features supported at the 95% confidence level are bolded. The p-value for the MatchAll feature is computed using Fisher"s Exact Test.
Figure 5 presents the results. Each row lists the size of the four sets (INV+, INV−, CON+, and CON−) for a given feature and indicates the percentage of positive pairs (%+) for INV and CON. In order to reject the null hypothesis, this percentage should be significantly greater for INV than CON. Except in one case, we applied the chi-squared test of independence to these sizes, with p-values shown in the last column. For the MatchAll feature, where the sum of the set sizes is 15, we applied Fisher"s exact test. Features supported at the 95% confidence level are bolded.
The results support claims that missing snippets, short snippets, missing query terms and complex URLs negatively impact clickthroughs. While this outcome may not be surprising, we are aware of no other work that can provide support for claims of this type in the context of a commercial Web search engine.
This work was originally motivated by our desire to validate some simple guidelines for the generation of captionssummarizing opinions that we formulated while working on related issues. While our results do not direct address all of the many variables that influence users understanding of captions, they are consistent with the major guidelines.
Further work is needed to provide additional support for the guidelines and to understand the relationships among variables.
The first of these guidelines underscores the importance of displaying query terms in context: Whenever possible all of the query terms should appear in the caption, reflecting their relationship to the associated page. If a query term is missing from a caption, the user may have no idea why the result was returned. The results for the MatchAll feature directly support this guideline. The results for TermMatchTitle and TermMatchTSU confirm that matching more terms is desirable. Other features provide additional indirect support for this guideline, and none of the results are inconsistent with it.
A second guideline speaks to the desirability of presenting the user with a readable snippet: When query terms are present in the title, they need not be repeated in the snippet. In particular, when a high-quality query-independent summary is available from an external source, such as a Web directory, it may be more appropriate to display this summary than a lower-quality query-dependent fragment selected on-the-fly. When titles are available from multiple sources -the header, the body, Web directories - a caption generation algorithm might a select a combination of title, snippet and URL that includes as many of the query terms as possible. When a title containing all query terms can be found, the algorithm might select a query-independent snippet. The MatchAll and Readable features directly support this guideline. Once again, other features provide indirect support, and none of the results are inconsistent with it.
Finally, the length and complexity of a URL influences user behavior. When query terms appear in the URL they should highlighted or otherwise distinguished. When multiple URLs reference the same page (due to re-directions, etc.) the shortest URL should be preferred, provided that all query terms will still appear in the caption. In other words, URLs should be selected and displayed in a manner that emphasizes their relationship to the query. The three URL features, as well as TermMatchTSU, directly support this guideline.
The influence of the Official and Image features led us to wonder what other terms are prevalent in the captions of clickthrough inversions. As an additional experiment, we treated each of the terms appearing in the INV and CON sets as a separate feature (case normalized), ranking them by their χ2 values. The results are presented in figure 6. Since we use the χ2 statistic as a divergence measure, rather than a significance test, no p-values are given. The final column of the table indicates the direction of the influence, whether the presence of the terms positively or negatively influence clickthroughs.
The positive influence of official has already been observed (the difference in the χ2 value from that of figure 5 is due to stemming). None of the terms included in the Image Rank Term χ2 influence 1 encyclopedia 114.6891 ↓ 2 wikipedia 94.0033 ↓ 3 official 36.5566 ↑ 4 and 28.3349 ↑ 5 tourism 25.2003 ↑ 6 attractions 24.7283 ↑ 7 free 23.6529 ↓ 8 sexy 21.9773 ↑ 9 medlineplus 19.9726 ↓ 10 information 19.9115 ↑ Figure 6: Words exhibiting the greatest positive (↑) and negative (↓) influence on clickthrough patterns. feature appear in the top ten, but pictures and photos appear at positions 21 and 22. The high rank given to and may be related to readability (the term the appears in position 20).
Most surprising to us is the negative influence of the terms: encyclopedia, wikipedia, free, and medlineplus. The first three terms appear in the title of Wikipedia articles3 and the last appears in the title of MedlinePlus articles4 .
These individual word-level features provide hints about issues. More detailed analyses and further experiments will be required to understand these features.
Clickthrough inversions form an appropriate tool for assessing the influence of caption features. Using clickthrough inversions, we have demonstrated that relatively simple caption features can significantly influence user behavior. To our knowledge, this is first methodology validated for assessing the quality of Web captions through implicit feedback. In the future, we hope to substantially expand this work, considering more features over larger datasets. We also hope to directly address the goal of predicting relevance from clickthoughs and other information present in search engine logs.
This work was conducted while the first author was visiting Microsoft Research. The authors thank members of the Windows Live team for their comments and assistance, particularly Girish Kumar, Luke DeLorme, Rohit Wad and Ramez Naam.
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In 29th ACM SIGIR, pages 19-26,
Seattle, August 2006. [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.
Learning user interaction models for predicting Web search result preferences. In 29th ACM SIGIR, pages 3-10, Seattle, August 2006. [3] A. Broder. A taxonomy of Web search. SIGIR Forum, 36(2):3-10, 2002. 3 www.wikipedia.org 4 www.nlm.nih.gov/medlineplus/ [4] E. Cutrell and Z. Guan. What are you looking for?
An eye-tracking study of information usage in Web search. In SIGCHI Conference on Human Factors in Computing Systems, pages 407-416, San Jose,
California, April-May 2007. [5] S. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In SIGCHI Conference on Human Factors in Computing Systems, pages 277-284, Seattle, March-April 2001. [6] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. Summarizing text documents: Sentence selection and evaluation metrics. In 22nd ACM SIGIR, pages 121-128, Berkeley, August 1999. [7] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in WWW search. In 27th ACM SIGIR, pages 478-479, Sheffield, July 2004. [8] Y. Hu, G. Xin, R. Song, G. Hu, S. Shi, Y. Cao, and H. Li. Title extraction from bodies of HTML documents and its application to Web page retrieval.
In 28th ACM SIGIR, pages 250-257, Salvador, Brazil,
August 2005. [9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In 28th ACM SIGIR, pages 154-161, Salvador, Brazil, August 2005. [10] U. Lee, Z. Liu, and J. Cho. Automatic identification of user goals in Web search. In 14th International World Wide Web Conference, pages 391-400, Edinburgh,
May 2005. [11] H. P. Luhn. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2):159-165, April 1958. [12] T. Paek, S. Dumais, and R. Logan. WaveLens: A new view onto Internet search results. In SIGCHI Conference on Human Factors in Computing Systems, pages 727-734, Vienna, Austria, April 2004. [13] D. Rose and D. Levinson. Understanding user goals in Web search. In 13th International World Wide Web Conference, pages 13-19, New York, May 2004. [14] J.-T. Sun, D. Shen, H.-J. Zeng, Q. Yang, Y. Lu, and Z. Chen. Web-page summarization using clickthrough data. In 28th ACM SIGIR, pages 194-201, Salvador,
Brazil, August 2005. [15] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In 21st ACM SIGIR, pages 2-10, Melbourne, Australia,
August 1998. [16] R. Varadarajan and V. Hristidis. A system for query-specific document summarization. In 15th ACM international conference on Information and knowledge management (CIKM), pages 622-631,
Arlington, Virginia, November 2006. [17] R. W. White, I. Ruthven, and J. M. Jose. Finding relevant documents using top ranking sentences: An evaluation of two alternative schemes. In 25th ACM SIGIR, pages 57-64, Tampere, Finland, August 2002. [18] G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma,

The problem of improving queries sent to Information Retrieval (IR) systems has been studied extensively in IR research [4][11].
Alternative query formulations, known as query suggestions, can be offered to users following an initial query, allowing them to modify the specification of their needs provided to the system, leading to improved retrieval performance. Recent popularity of Web search engines has enabled query suggestions that draw upon the query reformulation behavior of many users to make query recommendations based on previous user interactions [10].
Leveraging the decision-making processes of many users for query reformulation has its roots in adaptive indexing [8]. In recent years, applying such techniques has become possible at a much larger scale and in a different context than what was proposed in early work. However, interaction-based approaches to query suggestion may be less potent when the information need is exploratory, since a large proportion of user activity for such information needs may occur beyond search engine interactions. In cases where directed searching is only a fraction of users" information-seeking behavior, the utility of other users" clicks over the space of top-ranked results may be limited, as it does not cover the subsequent browsing behavior. At the same time, user navigation that follows search engine interactions provides implicit endorsement of Web resources preferred by users, which may be particularly valuable for exploratory search tasks. Thus, we propose exploiting a combination of past searching and browsing user behavior to enhance users" Web search interactions.
Browser plugins and proxy server logs provide access to the browsing patterns of users that transcend search engine interactions.
In previous work, such data have been used to improve search result ranking by Agichtein et al. [1]. However, this approach only considers page visitation statistics independently of each other, not taking into account the pages" relative positions on post-query browsing paths. Radlinski and Joachims [13] have utilized such collective user intelligence to improve retrieval accuracy by using sequences of consecutive query reformulations, yet their approach does not consider users" interactions beyond the search result page.
In this paper, we present a user study of a technique that exploits the searching and browsing behavior of many users to suggest popular Web pages, referred to as destinations henceforth, in addition to the regular search results. The destinations may not be among the topranked results, may not contain the queried terms, or may not even be indexed by the search engine. Instead, they are pages at which other users end up frequently after submitting same or similar queries and then browsing away from initially clicked search results. We conjecture that destinations popular across a large number of users can capture the collective user experience for information needs, and our results support this hypothesis.
In prior work, O"Day and Jeffries [12] identified teleportation as an information-seeking strategy employed by users jumping to their previously-visited information targets, while Anderson et al. [2] applied similar principles to support the rapid navigation of Web sites on mobile devices. In [19], Wexelblat and Maes describe a system to support within-domain navigation based on the browse trails of other users. However, we are not aware of such principles being applied to Web search. Research in the area of recommender systems has also addressed similar issues, but in areas such as question-answering [9] and relatively small online communities [16]. Perhaps the nearest instantiation of teleportation is search engines" offering of several within-domain shortcuts below the title of a search result. While these may be based on user behavior and possibly site structure, the user saves at most one click from this feature. In contrast, our proposed approach can transport users to locations many clicks beyond the search result, saving time and giving them a broader perspective on the available related information.
The conducted user study investigates the effectiveness of including links to popular destinations as an additional interface feature on search engine result pages. We compare two variants of this approach against the suggestion of related queries and unaided Web search, and seek answers to questions on: (i) user preference and search effectiveness for known-item and exploratory search tasks, and (ii) the preferred distance between query and destination used to identify popular destinations from past behavior logs. The results indicate that suggesting popular destinations to users attempting exploratory tasks provides best results in key aspects of the information-seeking experience, while providing query refinement suggestions is most desirable for known-item tasks.
The remainder of the paper is structured as follows. In Section 2 we describe the extraction of search and browsing trails from user activity logs, and their use in identifying top destinations for new queries. Section 3 describes the design of the user study, while Sections 4 and 5 present the study findings and their discussion, respectively. We conclude in Section 6 with a summary.
We used Web activity logs containing searching and browsing activity collected with permission from hundreds of thousands of users over a five-month period between December 2005 and April
timestamp, a unique browser window identifier, and the URL of a visited Web page. This information was sufficient to reconstruct temporally ordered sequences of viewed pages that we refer to as trails. In this section, we summarize the extraction of trails, their features, and destinations (trail end-points). In-depth description and analysis of trail extraction are presented in [20].
For each user, interaction logs were grouped based on browser identifier information. Within each browser instance, participant navigation was summarized as a path known as a browser trail, from the first to the last Web page visited in that browser. Located within some of these trails were search trails that originated with a query submission to a commercial search engine such as Google,
Yahoo!, Windows Live Search, and Ask. It is these search trails that we use to identify popular destinations.
After originating with a query submission to a search engine, trails proceed until a point of termination where it is assumed that the user has completed their information-seeking activity. Trails must contain pages that are either: search result pages, search engine homepages, or pages connected to a search result page via a sequence of clicked hyperlinks. Extracting search trails using this methodology also goes some way toward handling multi-tasking, where users run multiple searches concurrently. Since users may open a new browser window (or tab) for each task [18], each task has its own browser trail, and a corresponding distinct search trail.
To reduce the amount of noise from pages unrelated to the active search task that may pollute our data, search trails are terminated when one of the following events occurs: (1) a user returns to their homepage, checks e-mail, logs in to an online service (e.g.,
MySpace or del.ico.us), types a URL or visits a bookmarked page; (2) a page is viewed for more than 30 minutes with no activity; (3) the user closes the active browser window. If a page (at step i) meets any of these criteria, the trail is assumed to terminate on the previous page (i.e., step i - 1).
There are two types of search trails we consider: session trails and query trails. Session trails transcend multiple queries and terminate only when one of the three termination criteria above are satisfied.
Query trails use the same termination criteria as session trails, but also terminate upon submission of a new query to a search engine.
Approximately 14 million query trails and 4 million session trails were extracted from the logs. We now describe some trail features.
Table 1 presents summary statistics for the query and session trails.
Differences in user interaction between the last domain on the trail (Domain n) and all domains visited earlier (Domains 1 to (n - 1)) are particularly important, because they highlight the wealth of user behavior data not captured by logs of search engine interactions.
Statistics are averages for all trails with two or more steps (i.e., those trails where at least one search result was clicked).
Table 1. Summary statistics (mean averages) for search trails.
Measure Query trails Session trails Number of unique domains 2.0 4.3 Total page views All domains 4.8 16.2 Domains 1 to (n - 1) 1.4 10.1 Domain n (destination) 3.4 6.2 Total time spent (secs) All domains 172.6 621.8 Domains 1 to (n - 1) 70.4 397.6 Domain n (destination) 102.3 224.1 The statistics suggest that users generally browse far from the search results page (i.e., around 5 steps), and visit a range of domains during the course of their search. On average, users visit 2 unique (non search-engine) domains per query trail, and just over 4 unique domains per session trail. This suggests that users often do not find all the information they seek on the first domain they visit.
For query trails, users also visit more pages, and spend significantly longer, on the last domain in the trail compared to all previous domains combined.1 These distinctions of the last domains in the trails may indicate user interest, page utility, or page relevance.2
For frequent queries, most popular destinations identified from Web activity logs could be simply stored for future lookup at search time.
However, we have found that over the six-month period covered by our dataset, 56.9% of queries are unique, and 97% queries occur 10 or fewer times, accounting for 19.8% and 66.3% of all searches respectively (these numbers are comparable to those reported in previous studies of search engine query logs [15,17]). Therefore, a lookup-based approach would prevent us from reliably suggesting destinations for a large fraction of searches. To overcome this problem, we utilize a simple term-based prediction model.
As discussed above, we extract two types of destinations: query destinations and session destinations. For both destination types, we obtain a corpus of query-destination pairs and use it to construct term-vector representation of destinations that is analogous to the classic tf.idf document representation in traditional IR [14].
Then, given a new query q consisting of k terms t1…tk, we identify highest-scoring destinations using the following similarity function: 1 Independent measures t-test: t(~60M) = 3.89, p < .001 2 The topical relevance of the destinations was tested for a subset of around ten thousand queries for which we had human judgments. The average rating of most of the destinations lay between good and excellent.
Visual inspection of those that did not lie in this range revealed that many were either relevant but had no judgments, or were related but had indirect query association (e.g., petfooddirect.com for query [dogs]). , : Where query and destination term weights, an computed using standard tf.idf weighting and que session-normalized smoothed tf.idf weighting, respec exploring alternative algorithms for the destination p remains an interesting challenge for future work, resu study described in subsequent sections demonstrate th approach provides robust, effective results.
To examine the usefulness of destinations, we con study investigating the perceptions and performance on four Web search systems, two with destination sug
Four systems were used in this study: a baseline Web with no explicit support for query refinement (Base system with a query suggestion method that recomme queries (QuerySuggestion), and two systems that aug Web search with destination suggestions using either query trails (QueryDestination), or end-points of (SessionDestination).
To establish baseline performance against which othe be compared, we developed a masked interface to a p engine without additional support in formulating q system presented the user-constructed query to the and returned ten top-ranking documents retrieved by t remove potential bias that may have been caused by perceptions, we removed all identifying information engine logos and distinguishing interface features.
In addition to the basic search functionality offered QuerySuggestion provides suggestions about f refinements that searchers can make following an submission. These suggestions are computed usin engine query log over the timeframe used for trail ge each target query, we retrieve two sets of candidate su contain the target query as a substring. One set is com most frequent such queries, while the second set cont frequent queries that followed the target query in que candidate query is then scored by multiplying its sm frequency by its smoothed frequency of following th in past search sessions, using Laplacian smoothing. B scores, six top-ranked query suggestions are returned. six suggestions are found, iterative backoff is per progressively longer suffixes of the target query; a si is described in [10].
Suggestions were offered in a box positioned on the t result page, adjacent to the search results. Figure position of the suggestions on the page. Figure 1b sh view of the portion of the results page containing th offered for the query [hubble telescope]. To the left o nd , are ery- and userctively. While prediction task ults of the user hat this simple nducted a user of 36 subjects ggestions. search system line), a search ends additional gment baseline r end-points of session trails er systems can popular search queries. This search engine the engine. To subjects" prior such as search d by Baseline, further query n initial query ng the search eneration. For uggestions that mposed of 100 tains 100 most ery logs. Each moothed overall he target query Based on these . If fewer than rformed using imilar strategy top-right of the 1a shows the hows a zoomed he suggestions of each query (a) Position of suggestions (b) Zoo Figure 1. Query suggestion presentation in suggestion is an icon similar to a progress b normalized popularity. Clicking a suggestion r results for that query.
QueryDestination uses an interface similar t However, instead of showing query refinemen query, QueryDestination suggests up to six des visited by other users who submitted queries s one, and computed as described in the previous shows the position of the destination suggestio page. Figure 2b shows a zoomed view of the p page destinations suggested for the query [hubb (a) Position of destinations (b) Zoo Figure 2. Destination presentation in Que To keep the interface uncluttered, the page title is shown on hover over the page URL (shown to the destination name, there is a clickable icon to execute a search for the current query wi domain displayed. We show destinations as a than increasing their search result rank, since deviate from the original query (e.g., those topics or not containing the original query terms
The interface functionality in SessionDestinat QueryDestination. The only difference between the definition of trail end-points for queries use destinations. QueryDestination directs users to end up at for the active or similar que SessionDestination directs users to the domains the end of the search session that follows th queries. This downgrades the effect of multi (i.e., we only care where users end up after sub rather than directing searchers to potentially irre may precede a query reformulation.
We were interested in determining the value of p To do this we attempt to answer the following re 3 To improve reliability, in a similar way to QueryS are only shown if their popularity exceeds a frequen med suggestions QuerySuggestion. bar that encodes its retrieves new search to QuerySuggestion. nts for the submitted stinations frequently imilar to the current s section.3 Figure 2a ons on search results portion of the results le telescope]. med destinations eryDestination. e of each destination in Figure 2b). Next n that allows the user ithin the destination a separate list, rather they may topically focusing on related s). tion is analogous to n the two systems is ed in computing top the domains others ries. In contrast, s other users visit at he active or similar iple query iterations bmitting all queries), elevant domains that popular destinations. esearch questions: Suggestion, destinations ncy threshold.
RQ1: Are popular destinations preferable and more effective than query refinement suggestions and unaided Web search for: a. Searches that are well-defined (known-item tasks)? b. Searches that are ill-defined (exploratory tasks)?
RQ2: Should popular destinations be taken from the end of query trails or the end of session trails?
36 subjects (26 males and 10 females) participated in our study.
They were recruited through an email announcement within our organization where they hold a range of positions in different divisions. The average age of subjects was 34.9 years (max=62, min=27, SD=6.2). All are familiar with Web search, and conduct
(86.1%) reported general awareness of the query refinements offered by commercial Web search engines.
Since the search task may influence information-seeking behavior [4], we made task type an independent variable in the study. We constructed six known-item tasks and six open-ended, exploratory tasks that were rotated between systems and subjects as described in the next section. Figure 3 shows examples of the two task types.
Known-item task Identify three tropical storms (hurricanes and typhoons) that have caused property damage and/or loss of life.
Exploratory task You are considering purchasing a Voice Over Internet Protocol (VoIP) telephone. You want to learn more about VoIP technology and providers that offer the service, and select the provider and telephone that best suits you.
Figure 3. Examples of known-item and exploratory tasks.
Exploratory tasks were phrased as simulated work task situations [5], i.e., short search scenarios that were designed to reflect real-life information needs. These tasks generally required subjects to gather background information on a topic or gather sufficient information to make an informed decision. The known-item search tasks required search for particular items of information (e.g., activities, discoveries, names) for which the target was welldefined. A similar task classification has been used successfully in previous work [21]. Tasks were taken and adapted from the Text Retrieval Conference (TREC) Interactive Track [7], and questions posed on question-answering communities (Yahoo! Answers,
Google Answers, and Windows Live QnA). To motivate the subjects during their searches, we allowed them to select two known-item and two exploratory tasks at the beginning of the experiment from the six possibilities for each category, before seeing any of the systems or having the study described to them.
Prior to the experiment all tasks were pilot tested with a small number of different subjects to help ensure that they were comparable in difficulty and selectability (i.e., the likelihood that a task would be chosen given the alternatives). Post-hoc analysis of the distribution of tasks selected by subjects during the full study showed no preference for any task in either category.
The study used a within-subjects experimental design. System had four levels (corresponding to the four experimental systems) and search tasks had two levels (corresponding to the two task types).
System and task-type order were counterbalanced according to a Graeco-Latin square design.
Subjects were tested independently and each experimental session lasted for up to one hour. We adhered to the following procedure:
two exploratory tasks from the six tasks of each type.
was read aloud to them by the experimenter.
aspects of search experience.
a. Subjects were given an explanation of interface functionality lasting around 2 minutes. b. Subjects were instructed to attempt the task on the assigned system searching the Web, and were allotted up to 10 minutes to do so. c. Upon completion of the task, subjects were asked to complete a post-search questionnaire.
a final questionnaire comparing their experiences on the systems.
In the next section we present the findings of this study.
In this section we use the data derived from the experiment to address our hypotheses about query suggestions and destinations, providing information on the effect of task type and topic familiarity where appropriate. Parametric statistical testing is used in this analysis and the level of significance is set to < 0.05, unless otherwise stated. All Likert scales and semantic differentials used a 5-point scale where a rating closer to one signifies more agreement with the attitude statement.
In this section we present findings on how subjects perceived the systems that they used. Responses to post-search (per-system) and final questionnaires are used as the basis for our analysis.
To address the first research question wanted insight into subjects" perceptions of the search experience on each of the four systems. In the post-search questionnaires, we asked subjects to complete four 5-point semantic differentials indicating their responses to the attitude statement: The search we asked you to perform was. The paired stimuli offered as responses were: relaxing/stressful, interesting/ boring, restful/tiring, and easy/difficult.
The average obtained differential values are shown in Table 1 for each system and each task type. The value corresponding to the differential All represents the mean of all three differentials, providing an overall measure of subjects" feelings.
Table 1. Perceptions of search process (lower = better).
Differential Known-item Exploratory B QS QD SD B QS QD SD Easy 2.6 1.6 1.7 2.3 2.5 2.6 1.9 2.9 Restful 2.8 2.3 2.4 2.6 2.8 2.8 2.4 2.8 Interesting 2.4 2.2 1.7 2.2 2.2 1.8 1.8 2 Relaxing 2.6 1.9 2 2.2 2.5 2.8 2.3 2.9 All 2.6 2 1.9 2.3 2.5 2.5 2.1 2.7 Each cell in Table 1 summarizes subject responses for 18 tasksystem pairs (18 subjects who ran a known-item task on Baseline (B), 18 subjects who ran an exploratory task on QuerySuggestion (QS), etc.). The most positive response across all systems for each differential-task pair is shown in bold. We applied two-way analysis of variance (ANOVA) to each differential across all four systems and two task types. Subjects found the search easier on QuerySuggestion and QueryDestination than the other systems for known-item tasks.4 For exploratory tasks, only searches conducted on QueryDestination were easier than on the other systems.5 Subjects indicated that exploratory tasks on the three non-baseline systems were more stressful (i.e., less relaxing) than the knownitem tasks.6 As we will discuss in more detail in Section 4.1.3, subjects regarded the familiarity of Baseline as a strength, and may have struggled to attempt a more complex task while learning a new interface feature such as query or destination suggestions.
We solicited subjects" opinions on the search support offered by QuerySuggestion, QueryDestination, and SessionDestination. The following Likert scales and semantic differentials were used: • Likert scale A: Using this system enhances my effectiveness in finding relevant information. (Effectiveness)7 • Likert scale B: The queries/destinations suggested helped me get closer to my information goal. (CloseToGoal) • Likert scale C: I would re-use the queries/destinations suggested if I encountered a similar task in the future (Re-use) • Semantic differential A: The queries/destinations suggested by the system were: relevant/irrelevant, useful/useless, appropriate/inappropriate.
We did not include these in the post-search questionnaire when subjects used the Baseline system as they refer to interface support options that Baseline did not offer. Table 2 presents the average responses for each of these scales and differentials, using the labels after each of the first three Likert scales in the bulleted list above.
The values for the three semantic differentials are included at the bottom of the table, as is their overall average under All.
Table 2. Perceptions of system support (lower = better).
Scale / Differential Known-item Exploratory QS QD SD QS QD SD Effectiveness 2.7 2.5 2.6 2.8 2.3 2.8 CloseToGoal 2.9 2.7 2.8 2.7 2.2 3.1 Re-use 2.9 3 2.4 2.5 2.5 3.2 1 Relevant 2.6 2.5 2.8 2.4 2 3.1 2 Useful 2.6 2.7 2.8 2.7 2.1 3.1 3 Appropriate 2.6 2.4 2.5 2.4 2.4 2.6 All {1,2,3} 2.6 2.6 2.6 2.6 2.3 2.9 The results show that all three experimental systems improved subjects" perceptions of their search effectiveness over Baseline, although only QueryDestination did so significantly.8 Further examination of the effect size (measured using Cohen"s d) revealed that QueryDestination affects search effectiveness most positively.9 QueryDestination also appears to get subjects closer to their information goal (CloseToGoal) than QuerySuggestion or 4 easy: F(3,136) = 4.71, p = .0037; Tukey post-hoc tests: all p ≤ .008 5 easy: F(3,136) = 3.93, p = .01; Tukey post-hoc tests: all p ≤ .012 6 relaxing: F(1,136) = 6.47, p = .011 7 This question was conditioned on subjects" use of Baseline and their previous Web search experiences. 8 F(3,136) = 4.07, p = .008; Tukey post-hoc tests: all p ≤ .002 9 QS: d(K,E) = (.26, .52); QD: d(K,E) = (.77, 1.50); SD: d(K,E) = (.48, .28) SessionDestination, although only for exploratory search tasks.10 Additional comments on QuerySuggestion conveyed that subjects saw it as a convenience (to save them typing a reformulation) rather than a way to dramatically influence the outcome of their search.
For exploratory searches, users benefited more from being pointed to alternative information sources than from suggestions for iterative refinements of their queries. Our findings also show that our subjects felt that QueryDestination produced more relevant and useful suggestions for exploratory tasks than the other systems.11 All other observed differences between the systems were not statistically significant.12 The difference between performance of QueryDestination and SessionDestination is explained by the approach used to generate destinations (described in Section 2).
SessionDestination"s recommendations came from the end of users" session trails that often transcend multiple queries. This increases the likelihood that topic shifts adversely affect their relevance.
In the final questionnaire that followed completion of all tasks on all systems, subjects were asked to rank the four systems in descending order based on their preferences. Table 3 presents the mean average rank assigned to each of the systems.
Table 3. Relative ranking of systems (lower = better).
Systems Baseline QSuggest QDest SDest Ranking 2.47 2.14 1.92 2.31 These results indicate that subjects preferred QuerySuggestion and QueryDestination overall. However, none of the differences between systems" ratings are significant.13 One possible explanation for these systems being rated higher could be that although the popular destination systems performed well for exploratory searches while QuerySuggestion performed well for known-item searches, an overall ranking merges these two performances. This relative ranking reflects subjects" overall perceptions, but does not separate them for each task category. Over all tasks there appeared to be a slight preference for QueryDestination, but as other results show, the effect of task type on subjects" perceptions is significant.
The final questionnaire also included open-ended questions that asked subjects to explain their system ranking, and describe what they liked and disliked about each system: Baseline: Subjects who preferred Baseline commented on the familiarity of the system (e.g., was familiar and I didn"t end up using suggestions (S36)). Those who did not prefer this system disliked the lack of support for query formulation (Can be difficult if you don"t pick good search terms (S20)) and difficulty locating relevant documents (e.g., Difficult to find what I was looking for (S13); Clunky current technology (S30)).
QuerySuggestion: Subjects who rated QuerySuggestion highest commented on rapid support for query formulation (e.g., was useful in (1) saving typing (2) coming up with new ideas for query expansion (S12); helps me better phrase the search term (S24); made my next query easier (S21)). Those who did not prefer this system criticized suggestion quality (e.g., Not relevant (S11); Popular 10 F(2,102) = 5.00, p = .009; Tukey post-hoc tests: all p ≤ .012 11 F(2,102) = 4.01, p = .01; α = .0167 12 Tukey post-hoc tests: all p ≥ .143 13 One-way repeated measures ANOVA: F(3,105) = 1.50, p = .22 queries weren"t what I was looking for (S18)) and the quality of results they led to (e.g., Results (after clicking on suggestions) were of low quality (S35); Ultimately unhelpful (S1)).
QueryDestination: Subjects who preferred this system commented mainly on support for accessing new information sources (e.g., provided potentially helpful and new areas / domains to look at (S27)) and bypassing the need to browse to these pages (Useful to try to ‘cut to the chase" and go where others may have found answers to the topic (S3)). Those who did not prefer this system commented on the lack of specificity in the suggested domains (Should just link to site-specific query, not site itself (S16); Sites were not very specific (S24); Too general/vague (S28)14 ), and the quality of the suggestions (Not relevant (S11); Irrelevant (S6)).
SessionDestination: Subjects who preferred this system commented on the utility of the suggested domains (suggestions make an awful lot of sense in providing search assistance, and seemed to help very nicely (S5)). However, more subjects commented on the irrelevance of the suggestions (e.g., did not seem reliable, not much help (S30); Irrelevant, not my style (S21), and the related need to include explanations about why the suggestions were offered (e.g.,
Low-quality results, not enough information presented (S35)).
These comments demonstrate a diverse range of perspectives on different aspects of the experimental systems. Work is obviously needed in improving the quality of the suggestions in all systems, but subjects seemed to distinguish the settings when each of these systems may be useful. Even though all systems can at times offer irrelevant suggestions, subjects appeared to prefer having them rather than not (e.g., one subject remarked suggestions were helpful in some cases and harmless in all (S15)).
The findings obtained from our study on subjects" perceptions of the four systems indicate that subjects tend to prefer QueryDestination for the exploratory tasks and QuerySuggestion for the known-item searches. Suggestions to incrementally refine the current query may be preferred by searchers on known-item tasks when they may have just missed their information target.
However, when the task is more demanding, searchers appreciate suggestions that have the potential to dramatically influence the direction of a search or greatly improve topic coverage.
To gain a better understanding of how subjects performed during the study, we analyze data captured on their perceptions of task completeness and the time that it took them to complete each task.
In the post-search questionnaire, subjects were asked to indicate on a 5-point Likert scale the extent to which they agreed with the following attitude statement: I believe I have succeeded in my performance of this task (Success). In addition, they were asked to complete three 5-point semantic differentials indicating their response to the attitude statement: The task we asked you to perform was: The paired stimuli offered as possible responses were clear/unclear, simple/complex, and familiar/ unfamiliar. Table 4 presents the mean average response to these statements for each system and task type. 14 Although the destination systems provided support for search within a domain, subjects mainly chose to ignore this.
Table 4. Perceptions of task and task success (lower = better).
Scale Known-item Exploratory B QS QD SD B QS QD SD Success 2.0 1.3 1.4 1.4 2.8 2.3 1.4 2.6 1 Clear 1.2 1.1 1.1 1.1 1.6 1.5 1.5 1.6 2 Simple 1.9 1.4 1.8 1.8 2.4 2.9 2.4 3 3 Familiar 2.2 1.9 2.0 2.2 2.6 2.5 2.7 2.7 All {1,2,3} 1.8 1.4 1.6 1.8 2.2 2.2 2.2 2.3 Subject responses demonstrate that users felt that their searches had been more successful using QueryDestination for exploratory tasks than with the other three systems (i.e., there was a two-way interaction between these two variables).15 In addition, subjects perceived a significantly greater sense of completion with knownitem tasks than with exploratory tasks.16 Subjects also found known-item tasks to be more simple, clear, and familiar. 17 These responses confirm differences in the nature of the tasks we had envisaged when planning the study. As illustrated by the examples in Figure 3, the known-item tasks required subjects to retrieve a finite set of answers (e.g., find three interesting things to do during a weekend visit to Kyoto, Japan). In contrast, the exploratory tasks were multi-faceted, and required subjects to find out more about a topic or to find sufficient information to make a decision. The end-point in such tasks was less well-defined and may have affected subjects" perceptions of when they had completed the task. Given that there was no difference in the tasks attempted on each system, theoretically the perception of the tasks" simplicity, clarity, and familiarity should have been the same for all systems. However, we observe a clear interaction effect between the system and subjects" perception of the actual tasks.
In addition to asking subjects to indicate the extent to which they felt the task was completed, we also monitored the time that it took them to indicate to the experimenter that they had finished. The elapsed time from when the subject began issuing their first query until when they indicated that they were done was monitored using a stopwatch and recorded for later analysis. A stopwatch rather than system logging was used for this since we wanted to record the time regardless of system interactions. Figure 4 shows the average task completion time for each system and each task type.
Figure 4. Mean average task completion time (± SEM). 15 F(3,136) = 6.34, p = .001 16 F(1,136) = 18.95, p < .001 17 F(1,136) = 6.82, p = .028; Known-item tasks were also more simple on QS (F(3,136) = 3.93, p = .01; Tukey post-hoc test: p = .01); α = .167 Known-item Exploratory 0 100 200 300 400 500 600 Task categories Baseline QSuggest Time(seconds) Systems
QDestination SDestination As can be seen in the figure above, the task completion times for the known-item tasks differ greatly between systems.18 Subjects attempting these tasks on QueryDestination and QuerySuggestion complete them in less time than subjects on Baseline and SessionDestination.19 As discussed in the previous section, subjects were more familiar with the known-item tasks, and felt they were simpler and clearer. Baseline may have taken longer than the other systems since users had no additional support and had to formulate their own queries. Subjects generally felt that the recommendations offered by SessionDestination were of low relevance and usefulness. Consequently, the completion time increased slightly between these two systems perhaps as the subjects assessed the value of the proposed suggestions, but reaped little benefit from them. The task completion times for the exploratory tasks were approximately equal on all four systems20 , although the time on Baseline was slightly higher. Since these tasks had no clearly defined termination criteria (i.e., the subject decided when they had gathered sufficient information), subjects generally spent longer searching, and consulted a broader range of information sources than in the known-item tasks.
Analysis of subjects" perception of the search tasks and aspects of task completion shows that the QuerySuggestion system made subjects feel more successful (and the task more simple, clear, and familiar) for the known-item tasks. On the other hand,
QueryDestination was shown to lead to heightened perceptions of search success and task ease, clarity, and familiarity for the exploratory tasks. Task completion times on both systems were significantly lower than on the other systems for known-item tasks.
We now focus our analysis on the observed interactions between searchers and systems. As well as eliciting feedback on each system from our subjects, we also recorded several aspects of their interaction with each system in log files. In this section, we analyze three interaction aspects: query iterations, search-result clicks, and subject engagement with the additional interface features offered by the three non-baseline systems.
Searchers typically interact with search systems by submitting queries and clicking on search results. Although our system offers additional interface affordances, we begin this section by analyzing querying and clickthrough behavior of our subjects to better understand how they conducted core search activities. Table 5 shows the average number of query iterations and search results clicked for each system-task pair. The average value in each cell is computed for 18 subjects on each task type and system.
Table 5. Average query iterations and result clicks (per task).
Scale Known-item Exploratory B QS QD SD B QS QD SD Queries 1.9 4.2 1.5 2.4 3.1 5.7 2.7 3.5 Result clicks 2.6 2 1.7 2.4 3.4 4.3 2.3 5.1 Subjects submitted fewer queries and clicked on fewer search results in QueryDestination than in any of the other systems.21 As 18 F(3,136) = 4.56, p = .004 19 Tukey post-hoc tests: all p ≤ .021 20 F(3,136) = 1.06, p = .37 21 Queries: F(3,443) = 3.99; p = .008; Tukey post-hoc tests: all p ≤ .004; Systems: F(3,431) = 3.63, p = .013; Tukey post-hoc tests: all p ≤ .011 discussed in the previous section, subjects using this system felt more successful in their searches yet they exhibited less of the traditional query and result-click interactions required for search success on traditional search systems. It may be the case that subjects" queries on this system were more effective, but it is more likely that they interacted less with the system through these means and elected to use the popular destinations instead. Overall, subjects submitted most queries in QuerySuggestion, which is not surprising as this system actively encourages searchers to iteratively re-submit refined queries. Subjects interacted similarly with Baseline and SessionDestination systems, perhaps due to the low quality of the popular destinations in the latter. To investigate this and related issues, we will next analyze usage of the suggestions on the three non-baseline systems.
To determine whether subjects found additional features useful, we measure the extent to which they were used when they were provided. Suggestion usage is defined as the proportion of submitted queries for which suggestions were offered and at least one suggestion was clicked. Table 6 shows the average usage for each system and task category.
Table 6. Suggestion uptake (values are percentages).
Measure Known-item Exploratory QS QD SD QS QD SD Usage 35.7 33.5 23.4 30.0 35.2 25.3 Results indicate that QuerySuggestion was used more for knownitem tasks than SessionDestination22 , and QueryDestination was used more than all other systems for the exploratory tasks.23 For well-specified targets in known-item search, subjects appeared to use query refinement most heavily. In contrast, when subjects were exploring, they seemed to benefit most from the recommendation of additional information sources. Subjects selected almost twice as many destinations per query when using QueryDestination compared to SessionDestination.24 As discussed earlier, this may be explained by the lower perceived relevance and usefulness of destinations recommended by SessionDestination.
Analysis of log interaction data gathered during the study indicates that although subjects submitted fewer queries and clicked fewer search results on QueryDestination, their engagement with suggestions was highest on this system, particularly for exploratory search tasks. The refined queries proposed by QuerySuggestion were used the most for the known-item tasks. There appears to be a clear division between the systems: QuerySuggestion was preferred for known-item tasks, while QueryDestination provided most-used support for exploratory tasks.
The promising findings of our study suggest that systems offering popular destinations lead to more successful and efficient searching compared to query suggestion and unaided Web search.
Subjects seemed to prefer QuerySuggestion for the known-item tasks where the information-seeking goal was well-defined. If the initial query does not retrieve relevant information, then subjects 22 F(2,355) = 4.67, p = .01; Tukey post-hoc tests: p = .006 23 Tukey"s post-hoc tests: all p ≤ .027 24 QD: MK = 1.8, ME = 2.1; SD: MK = 1.1, ME = 1.2; F(1,231) = 5.49, p = .02; Tukey post-hoc tests: all p ≤ .003; (M represents mean average). appreciate support in deciding what refinements to make to the query. From examination of the queries that subjects entered for the known-item searches across all systems, they appeared to use the initial query as a starting point, and add or subtract individual terms depending on search results. The post-search questionnaire asked subjects to select from a list of proposed explanations (or offer their own explanations) as to why they used recommended query refinements. For both known-item tasks and the exploratory tasks, around 40% of subjects indicated that they selected a query suggestion because they wanted to save time typing a query, while less than 10% of subjects did so because the suggestions represented new ideas. Thus, subjects seemed to view QuerySuggestion as a time-saving convenience, rather than a way to dramatically impact search effectiveness.
The two variants of recommending destinations that we considered,
QueryDestination and SessionDestination, offered suggestions that differed in their temporal proximity to the current query. The quality of the destinations appeared to affect subjects" perceptions of them and their task performance. As discussed earlier, domains residing at the end of a complete search session (as in SessionDestination) are more likely to be unrelated to the current query, and thus are less likely to constitute valuable suggestions.
Destination systems, in particular QueryDestination, performed best for the exploratory search tasks, where subjects may have benefited from exposure to additional information sources whose topical relevance to the search query is indirect. As with QuerySuggestion, subjects were asked to offer explanations for why they selected destinations. Over both task types they suggested that destinations were clicked because they grabbed their attention (40%), represented new ideas (25%), or users couldn"t find what they were looking for (20%). The least popular responses were wanted to save time typing the address (7%) and the destination was popular (3%).
The positive response to destination suggestions from the study subjects provides interesting directions for design refinements. We were surprised to learn that subjects did not find the popularity bars useful, or hardly used the within-site search functionality, inviting re-design of these components. Subjects also remarked that they would like to see query-based summaries for each suggested destination to support more informed selection, as well as categorization of destinations with capability of drill-down for each category. Since QuerySuggestion and QueryDestination perform well in distinct task scenarios, integrating both in a single system is an interesting future direction. We hope to deploy some of these ideas on Web scale in future systems, which will allow log-based evaluation across large user pools.
We presented a novel approach for enhancing users" Web search interaction by providing links to websites frequently visited by past searchers with similar information needs. A user study was conducted in which we evaluated the effectiveness of the proposed technique compared with a query refinement system and unaided Web search. Results of our study revealed that: (i) systems suggesting query refinements were preferred for known-item tasks, (ii) systems offering popular destinations were preferred for exploratory search tasks, and (iii) destinations should be mined from the end of query trails, not session trails. Overall, popular destination suggestions strategically influenced searches in a way not achievable by query suggestion approaches by offering a new way to resolve information problems, and enhance the informationseeking experience for many Web searchers.

Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.
The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal. Caching can be applied at different levels with increasing response latencies or processing requirements. For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.
The decision of what to cache is either off-line (static) or online (dynamic). A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.
Such online decisions are based on a cache policy, and several different policies have been studied in the past.
For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.
Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.
Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.
Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.
Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.
Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.
In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.
More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.
The remainder of this paper is organized as follows.
Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks.
There is a large body of work devoted to query optimization. Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15]. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. They may be considered separate and complementary to a cache-based approach.
Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries. Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies. Based on the observations of Markatos,
Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8]. Fagni et al. follow Markatos" work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7]. Different from our work, they consider caching and prefetching of pages of results.
As systems are often hierarchical, there has also been some effort on multi-level architectures. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13]. Their goal for such systems has been to improve response time for hierarchical engines.
In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.
Baeza-Yates and Saint-Jean propose a three-level index organization [2]. Long and Suel propose a caching system structured according to three different levels [9]. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.
Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists.
Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November
unique. The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04
1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.
The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).
Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve). The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.) The y-axis is Table 1: Statistics of the UK-2006 sample.
UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term). As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively. In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space. The query terms (middle curve) have been normalized for case, as have the terms in the document collection.
The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB. The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2). The statistics of the collection are shown in Table 1. We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424. A scatter plot for a random sample of terms is shown in Figure 3. In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04
1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies.
Caching relies upon the assumption that there is locality in the stream of requests. That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective. In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume. Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%. This is because only 56% of all the queries comprise queries that have multiple occurrences. It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses. A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/. URL retrieved 05/2007. 0
1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.
This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.
If we consider a cache with infinite memory, then the hit ratio is 50%. Note that for an infinite cache there are no capacity misses.
As we mentioned before, another possibility is to cache the posting lists of terms. Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query. On the other hand, they need more space.
As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller. In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms. We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.
Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.
That is, we plot the normalized number of elements that appear in a day. This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.
Total queries and Total terms correspond to the total volume of queries and terms, respectively. Unique queries and Unique terms correspond to the arrival rate of unique queries and terms. Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.
In Figure 4, as expected, the volume of terms is much higher than the volume of queries. The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries. This observation implies that terms repeat significantly more than queries. If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.
We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine. We found that it follows closely the arrival rate for terms shown in Figure 4.
To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache. On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries. In this graph, the most frequent queries are not the same queries that were most frequent before the cache. It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.
The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries. If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.
When discussing the effectiveness of dynamically caching, an important metric is cache miss rate. To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14]. A working set, informally, is the set of references that an application or an operating system is currently working with. The model uses such sets in a strategy that tries to capture the temporal locality of references.
The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.
Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons. First, it captures the amount of locality of queries and terms in a sequence of queries. Locality in this case refers to the frequency of queries and terms in a window of time. If many queries appear multiple times in a window, then locality is high. Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.
Third, working sets capture aspects of efficient caching algorithms such as LRU. LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].
Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms. The working set sizes are normalized against the total number of queries in the query log. In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01. Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes.
1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.
Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller. The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values. The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries. This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries. This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query. We analyze these issues more carefully later in this paper.
It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph. It reports the distribution of distances between repetitions of the same frequent query. The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times. From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.
Thus, caching the posting lists of terms has the potential to improve the hit ratio. This is what we explore next.
The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.
In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available. The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists. We consider both dynamic and static caching. For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.
Before discussing the static caching strategies, we introduce some notation. We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.
The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t). We call this algorithm Qtf.
We observe that there is a trade-off between fq(t) and fd(t). Terms with high fq(t) are useful to keep in the cache because they are queried often. On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space. In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value. In our case, value corresponds to fq(t) and size corresponds to fd(t). Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) . We call this algorithm QtfDf. We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.
In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.
The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.
Performance is measured with hit rate. The cache size is measured as a fraction of the total space required to store the posting lists of all terms.
For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries. For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream. As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest.
1
Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.
The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms. An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries. However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate.
In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists. Our analysis takes into account the impact of caching between two levels of the data-access hierarchy. It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.
Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists.
Let M be the size of the cache measured in answer units (the cache can store M query answers). Assume that all posting lists are of the same length L, measured in answer units. We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists. In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache. Thus, Np = Nc/L. Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).
For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit. For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units. Of course TR2 > TR1.
Now we want to compare the time to answer a stream of Q queries in both cases. Let Vc(Nc) be the volume of the most frequent Nc queries. Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).
Similarly, for case (B), let Vp(Np) be the number of computable queries. Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).
We want to check under which conditions we have TP L < TCA. We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.
Figure 9 shows the values of Vp and Vc for our data. We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.
As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα . Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).
We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .
In the worst case, for a large cache, β → 1. That is, both techniques will cache a constant fraction of the overall query volume. Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.
If we use compression, we have L < L and TR1 > TR1.
According to the experiments that we show later, compression is always better.
For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data. In this case there will always be a point where TP L > TCA for a large number of queries.
In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists. In such a case, there will be some queries that could be answered by both parts of the cache. As the answer cache is faster, it will be the first choice for answering those queries. Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively. Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L. Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically. In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples.
We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section. We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.
We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming. The posting lists in the inverted file consist of pairs of document identifier and term frequency. We compress the document identifier gaps using Elias gamma encoding, and the
1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.
Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).
Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].
The size of the inverted file is 1,189Mb. A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes. From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.
We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way. Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory. The average time is Tc = 0.069ms. T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system). For each query, we remove stop words, if there are at least three remaining terms. The stop words correspond to the terms with a frequency higher than the number of documents in the index. We use a document-at-a-time approach to retrieve documents containing all query terms. The only disk access required during query processing is for reading compressed posting lists from the inverted file. We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.
In the partial evaluation of queries, we terminate the processing after matching 10,000 documents. The estimated ratios TR are presented in Table 2.
Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists. The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t). Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0
1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.
The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites. Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site. The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker. Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible). The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors. Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms. Hence,
TRL = TR + 0.615ms/0.069ms = TR + 9.
In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms. Hence, TRW = TR + 329ms/0.069ms = TR + 4768.
We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.
To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.
We perform simulations and compute the average response time as a function of x. Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists. For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.
In Figure 11, we plot the simulated response time for a centralized system as a function of x. For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB. In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the
obtained similar trends in the results for the LAN setting.
Figure 12 shows the simulated workload for a distributed system across a WAN. In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists. According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially. When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers. This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker.
For our query log, the query distribution and query-term distribution change slowly over time. To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June. We found that a very small percentage of queries are new queries. The majority of queries that appear in a given week repeat in the following weeks for the next six months.
We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13). We report hit rate hourly for 7 days, starting from 5pm. We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum. After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period.
0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.
The static cache of posting lists can be periodically recomputed. To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.
We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14). We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream. This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.
To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate. As Figure 14 shows, the hit rate decreases by less than 2%. The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.
Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%.
Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching.
Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12%. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.
We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results.
Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists.
[1] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean. A three level search engine index based in query log distribution. In SPIRE,
[3] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems.
In ACM CIKM, 2006. [5] P. Cao and S. Irani. Cost-aware WWW proxy caching algorithms. In USITS, 1997. [6] P. Denning. Working sets past and present. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran. Predictive caching and prefetching of query results in search engines. In WWW,
[9] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. In WWW, 2005. [10] E. P. Markatos. On caching search engine query results.
Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever. On the reuse of past optimal queries. In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira,

The amount of information on the Web is growing at a prodigious rate [24]. According to a recent study [13], it is estimated that the Web currently consists of more than 11 billion pages. Due to this immense amount of available information, the users are becoming more and more dependent on the Web search engines for locating relevant information on the Web. Typically, the Web search engines, similar to other information retrieval applications, utilize a data structure called inverted index. An inverted index provides for the efficient retrieval of the documents (or Web pages) that contain a particular keyword.
In most cases, a query that the user issues may have thousands or even millions of matching documents. In order to avoid overwhelming the users with a huge amount of results, the search engines present the results in batches of 10 to 20 relevant documents.
The user then looks through the first batch of results and, if she doesn"t find the answer she is looking for, she may potentially request to view the next batch or decide to issue a new query.
A recent study [16] indicated that approximately 80% of the users examine at most the first 3 batches of the results. That is, 80% of the users typically view at most 30 to 60 results for every query that they issue to a search engine. At the same time, given the size of the Web, the inverted index that the search engines maintain can grow very large. Since the users are interested in a small number of results (and thus are viewing a small portion of the index for every query that they issue), using an index that is capable of returning all the results for a query may constitute a significant waste in terms of time, storage space and computational resources, which is bound to get worse as the Web grows larger over time [24].
One natural solution to this problem is to create a small index on a subset of the documents that are likely to be returned as the top results (by using, for example, the pruning techniques in [7, 20]) and compute the first batch of answers using the pruned index. While this approach has been shown to give significant improvement in performance, it also leads to noticeable degradation in the quality of the search results, because the top answers are computed only from the pruned index [7, 20]. That is, even if a page should be placed as the top-matching page according to a search engine"s ranking metric, the page may be placed behind the ones contained in the pruned index if the page did not become part of the pruned index for various reasons [7, 20]. Given the fierce competition among search engines today this degradation is clearly undesirable and needs to be addressed if possible.
In this paper, we study how we can avoid any degradation of search quality due to the above performance optimization while still realizing most of its benefit. That is, we present a number of simple (yet important) changes in the pruning techniques for creating the pruned index. Our main contribution is a new answer computation algorithm that guarantees that the top-matching pages (according to the search-engine"s ranking metric) are always placed at the top of search results, even though we are computing the first batch of answers from the pruned index most of the time. These enhanced pruning techniques and answer-computation algorithms are explored in the context of the cluster architecture commonly employed by today"s search engines. Finally, we study and present how search engines can minimize the operational cost of answering queries while providing high quality search results.
IF IF IF IF IF IF IF Ip Ip Ip Ip Ip Ip 5000 queries/sec 5000 queries/sec : 1000 queries/sec : 1000 queries/sec 2nd tier 1st tier (a) (b) Figure 1: (a) Search engine replicates its full index IF to increase query-answering capacity. (b) In the 1st tier, small pindexes IP handle most of the queries. When IP cannot answer a query, it is redirected to the 2nd tier, where the full index IF is used to compute the answer.
SAVINGS FROM A PRUNED INDEX Typically, a search engine downloads documents from the Web and maintains a local inverted index that is used to answer queries quickly.
Inverted indexes. Assume that we have collected a set of documents D = {D1, . . . , DM } and that we have extracted all the terms T = {t1, . . . , tn} from the documents. For every single term ti ∈ T we maintain a list I(ti) of document IDs that contain ti. Every entry in I(ti) is called a posting and can be extended to include additional information, such as how many times ti appears in a document, the positions of ti in the document, whether ti is bold/italic, etc. The set of all the lists I = {I(t1), . . . , I(tn)} is our inverted index.
Search engines are accepting an enormous number of queries every day from eager users searching for relevant information. For example, Google is estimated to answer more than 250 million user queries per day. In order to cope with this huge query load, search engines typically replicate their index across a large cluster of machines as the following example illustrates: Example 1 Consider a search engine that maintains a cluster of machines as in Figure 1(a). The size of its full inverted index IF is larger than what can be stored in a single machine, so each copy of IF is stored across four different machines. We also suppose that one copy of IF can handle the query load of 1000 queries/sec.
Assuming that the search engine gets 5000 queries/sec, it needs to replicate IF five times to handle the load. Overall, the search engine needs to maintain 4 × 5 = 20 machines in its cluster. 2 While fully replicating the entire index IF multiple times is a straightforward way to scale to a large number of queries, typical query loads at search engines exhibit certain localities, allowing for significant reduction in cost by replicating only a small portion of the full index. In principle, this is typically done by pruning a full index IF to create a smaller, pruned index (or p-index) IP , which contains a subset of the documents that are likely to be returned as top results.
Given the p-index, search engines operate by employing a twotier index architecture as we show in Figure 1(b): All incoming queries are first directed to one of the p-indexes kept in the 1st tier.
In the cases where a p-index cannot compute the answer (e.g. was unable to find enough documents to return to the user) the query is answered by redirecting it to the 2nd tier, where we maintain a full index IF . The following example illustrates the potential reduction in the query-processing cost by employing this two-tier index architecture.
Example 2 Assume the same parameter settings as in Example 1.
That is, the search engine gets a query load of 5000 queries/sec Algorithm 2.1 Computation of answer with correctness guarantee Input q = ({t1, . . . , tn}, [i, i + k]) where {t1, . . . , tn}: keywords in the query [i, i + k]: range of the answer to return Procedure (1) (A, C) = ComputeAnswer(q, IP ) (2) If (C = 1) Then (3) Return A (4) Else (5) A = ComputeAnswer(q, IF ) (6) Return A Figure 2: Computing the answer under the two-tier architecture with the result correctness guarantee. and every copy of an index (both the full IF and p-index IP ) can handle up to 1000 queries/sec. Also assume that the size of IP is one fourth of IF and thus can be stored on a single machine.
Finally, suppose that the p-indexes can handle 80% of the user queries by themselves and only forward the remaining 20% queries to IF .
Under this setting, since all 5000/sec user queries are first directed to a p-index, five copies of IP are needed in the 1st tier. For the 2nd tier, since 20% (or 1000 queries/sec) are forwarded, we need to maintain one copy of IF to handle the load. Overall we need a total of 9 machines (five machines for the five copies of IP and four machines for one copy of IF ). Compared to Example 1, this is more than 50% reduction in the number of machines. 2 The above example demonstrates the potential cost saving achieved by using a p-index. However, the two-tier architecture may have a significant drawback in terms of its result quality compared to the full replication of IF ; given the fact that the p-index contains only a subset of the data of the full index, it is possible that, for some queries, the p-index may not contain the top-ranked document according to the particular ranking criteria used by the search engine and fail to return it as the top page, leading to noticeable quality degradation in search results. Given the fierce competition in the online search market, search engine operators desperately try to avoid any reduction in search quality in order to maximize user satisfaction.
architecture How can we avoid the potential degradation of search quality under the two-tier architecture? Our basic idea is straightforward: We use the top-k result from the p-index only if we know for sure that the result is the same as the top-k result from the full index.
The algorithm in Figure 2 formalizes this idea. In the algorithm, when we compute the result from IP (Step 1), we compute not only the top-k result A, but also the correctness indicator function C defined as follows: Definition 1 (Correctness indicator function) Given a query q, the p-index IP returns the answer A together with a correctness indicator function C. C is set to 1 if A is guaranteed to be identical (i.e. same results in the same order) to the result computed from the full index IF . If it is possible that A is different, C is set to 0. 2 Note that the algorithm returns the result from IP (Step 3) only when it is identical to the result from IF (condition C = 1 in Step 2). Otherwise, the algorithm recomputes and returns the result from the full index IF (Step 5). Therefore, the algorithm is guaranteed to return the same result as the full replication of IF all the time.
Now, the real challenge is to find out (1) how we can compute the correctness indicator function C and (2) how we should prune the index to make sure that the majority of queries are handled by IP alone.
Question 1 How can we compute the correctness indicator function C?
A straightforward way to calculate C is to compute the top-k answer both from IP and IF and compare them. This naive solution, however, incurs a cost even higher than the full replication of IF because the answers are computed twice: once from IP and once from IF . Is there any way to compute the correctness indicator function C only from IP without computing the answer from IF ?
Question 2 How should we prune IF to IP to realize the maximum cost saving?
The effectiveness of Algorithm 2.1 critically depends on how often the correctness indicator function C is evaluated to be 1. If C = 0 for all queries, for example, the answers to all queries will be computed twice, once from IP (Step 1) and once from IF (Step 5), so the performance will be worse than the full replication of IF .
What will be the optimal way to prune IF to IP , such that C = 1 for a large fraction of queries? In the next few sections, we try to address these questions.
Intuitively, there exists a clear tradeoff between the size of IP and the fraction of queries that IP can handle: When IP is large and has more information, it will be able to handle more queries, but the cost for maintaining and looking up IP will be higher. When IP is small, on the other hand, the cost for IP will be smaller, but more queries will be forwarded to IF , requiring us to maintain more copies of IF . Given this tradeoff, how should we determine the optimal size of IP in order to maximize the cost saving? To find the answer, we start with a simple example.
Example 3 Again, consider a scenario similar to Example 1, where the query load is 5000 queries/sec, each copy of an index can handle 1000 queries/sec, and the full index spans across 4 machines. But now, suppose that if we prune IF by 75% to IP 1 (i.e., the size of IP 1 is 25% of IF ), IP 1 can handle 40% of the queries (i.e., C = 1 for 40% of the queries). Also suppose that if IF is pruned by 50% to IP 2, IP 2 can handle 80% of the queries. Which one of the IP 1, IP 2 is preferable for the 1st -tier index?
To find out the answer, we first compute the number of machines needed when we use IP 1 for the 1st tier. At the 1st tier, we need 5 copies of IP 1 to handle the query load of 5000 queries/sec. Since the size of IP 1 is 25% of IF (that requires 4 machines), one copy of IP 1 requires one machine. Therefore, the total number of machines required for the 1st tier is 5×1 = 5 (5 copies of IP 1 with 1 machine per copy). Also, since IP 1 can handle 40% of the queries, the 2nd tier has to handle 3000 queries/sec (60% of the 5000 queries/sec), so we need a total of 3×4 = 12 machines for the 2nd tier (3 copies of IF with 4 machines per copy). Overall, when we use IP 1 for the 1st tier, we need 5 + 12 = 17 machines to handle the load. We can do similar analysis when we use IP 2 and see that a total of 14 machines are needed when IP 2 is used. Given this result, we can conclude that using IP 2 is preferable. 2 The above example shows that the cost of the two-tier architecture depends on two important parameters: the size of the p-index and the fraction of the queries that can be handled by the 1st tier index alone. We use s to denote the size of the p-index relative to IF (i.e., if s = 0.2, for example, the p-index is 20% of the size of IF ). We use f(s) to denote the fraction of the queries that a p-index of size s can handle (i.e., if f(s) = 0.3, 30% of the queries return the value C = 1 from IP ). In general, we can expect that f(s) will increase as s gets larger because IP can handle more queries as its size grows. In Figure 3, we show an example graph of f(s) over s.
Given the notation, we can state the problem of p-index-size optimization as follows. In formulating the problem, we assume that the number of machines required to operate a two-tier architecture 0
1 0 0.2 0.4 0.6 0.8 1 Fractionofqueriesguaranteed-f(s) Fraction of index - s Fraction of queries guaranteed per fraction of index Optimal size s=0.16 Figure 3: Example function showing the fraction of guaranteed queries f(s) at a given size s of the p-index. is roughly proportional to the total size of the indexes necessary to handle the query load.
Problem 1 (Optimal index size) Given a query load Q and the function f(s), find the optimal p-index size s that minimizes the total size of the indexes necessary to handle the load Q. 2 The following theorem shows how we can determine the optimal index size.
Theorem 1 The cost for handling the query load Q is minimal when the size of the p-index, s, satisfies d f(s) d s = 1. 2 Proof The proof of this and the following theorems is omitted due to space constraints.
This theorem shows that the optimal point is when the slope of the f(s) curve is 1. For example, in Figure 3, the optimal size is when s = 0.16. Note that the exact shape of the f(s) graph may vary depending on the query load and the pruning policy. For example, even for the same p-index, if the query load changes significantly, fewer (or more) queries may be handled by the p-index, decreasing (or increasing)f(s). Similarly, if we use an effective pruning policy, more queries will be handled by IP than when we use an ineffective pruning policy, increasing f(s). Therefore, the function f(s) and the optimal-index size may change significantly depending on the query load and the pruning policy. In our later experiments, however, we find that even though the shape of the f(s) graph changes noticeably between experiments, the optimal index size consistently lies between 10%-30% in most experiments.
In this section, we show how we should prune the full index IF to IP , so that (1) we can compute the correctness indicator function C from IP itself and (2) we can handle a large fraction of queries by IP . In designing the pruning policies, we note the following two localities in the users" search behavior:
in the document collection that the search engine indexes, a few popular keywords constitute the majority of the query loads. This keyword locality implies that the search engine will be able to answer a significant fraction of user queries even if it can handle only these few popular keywords.
matching documents, users typically look at only the first few results [16]. Thus, as long as search engines can compute the first few top-k answers correctly, users often will not notice that the search engine actually has not computed the correct answer for the remaining results (unless the users explicitly request them).
Based on the above two localities, we now investigate two different types of pruning policies: (1) a keyword pruning policy, which takes advantage of the keyword locality by pruning the whole inverted list I(ti) for unpopular keywords ti"s and (2) a document pruning policy, which takes advantage of the document locality by keeping only a few postings in each list I(ti), which are likely to be included in the top-k results.
As we discussed before, we need to be able to compute the correctness indicator function from the pruned index alone in order to provide the correctness guarantee. Since the computation of correctness indicator function may critically depend on the particular ranking function used by a search engine, we first clarify our assumptions on the ranking function.
Consider a query q = {t1, t2, . . . , tw} that contains a subset of the index terms. The goal of the search engine is to return the documents that are most relevant to query q. This is done in two steps: first we use the inverted index to find all the documents that contain the terms in the query. Second, once we have the relevant documents, we calculate the rank (or score) of each one of the documents with respect to the query and we return to the user the documents that rank the highest.
Most of the major search engines today return documents containing all query terms (i.e. they use AND-semantics). In order to make our discussions more concise, we will also assume the popular AND-semantics while answering a query. It is straightforward to extend our results to OR-semantics as well. The exact ranking function that search engines employ is a closely guarded secret.
What is known, however, is that the factors in determining the document ranking can be roughly categorized into two classes: Query-dependent relevance. This particular factor of relevance captures how relevant the query is to every document. At a high level, given a document D, for every term ti a search engine assigns a term relevance score tr(D, ti) to D. Given the tr(D, ti) scores for every ti, then the query-dependent relevance of D to the query, noted as tr(D, q), can be computed by combining the individual term relevance values. One popular way for calculating the querydependent relevance is to represent both the document D and the query q using the TF.IDF vector space model [29] and employ a cosine distance metric.
Since the exact form of tr(D, ti) and tr(D, q) differs depending on the search engine, we will not restrict to any particular form; instead, in order to make our work applicable in the general case, we will make the generic assumption that the query-dependent relevance is computed as a function of the individual term relevance values in the query: tr(D, q) = ftr(tr(D, t1), . . . , tr(D, tw)) (1) Query-independent document quality. This is a factor that measures the overall quality of a document D independent of the particular query issued by the user. Popular techniques that compute the general quality of a page include PageRank [26], HITS [17] and the likelihood that the page is a spam page [25, 15]. Here, we will use pr(D) to denote this query-independent part of the final ranking function for document D.
The final ranking score r(D, q) of a document will depend on both the query-dependent and query-independent parts of the ranking function. The exact combination of these parts may be done in a variety of ways. In general, we can assume that the final ranking score of a document is a function of its query-dependent and query-independent relevance scores. More formally: r(D, q) = fr(tr(D, q), pr(D)) (2) For example, fr(tr(D, q), pr(D)) may take the form fr(tr(D, q), pr(D)) = α · tr(D, q) + (1 − α) · pr(D), thus giving weight α to the query-dependent part and the weight 1 − α to the query-independent part.
In Equations 1 and 2 the exact form of fr and ftr can vary depending on the search engine. Therefore, to make our discussion applicable independent of the particular ranking function used by search engines, in this paper, we will make only the generic assumption that the ranking function r(D, q) is monotonic on its parameters tr(D, t1), . . . , tr(D, tw) and pr(D). t1 → D1 D2 D3 D4 D5 D6 t2 → D1 D2 D3 t3 → D3 D5 D7 D8 t4 → D4 D10 t5 → D6 D8 D9 Figure 4: Keyword and document pruning.
Algorithm 4.1 Computation of C for keyword pruning Procedure (1) C = 1 (2) Foreach ti ∈ q (3) If (I(ti) /∈ IP ) Then C = 0 (4) Return C Figure 5: Result guarantee in keyword pruning.
Definition 2 A function f(α, β, . . . , ω) is monotonic if ∀α1 ≥ α2, ∀β1 ≥ β2, . . . ∀ω1 ≥ ω2 it holds that: f(α1, β1, . . . , ω1) ≥ f(α2, β2, . . . , ω2).
Roughly, the monotonicity of the ranking function implies that, between two documents D1 and D2, if D1 has higher querydependent relevance than D2 and also a higher query-independent score than D2, then D1 should be ranked higher than D2, which we believe is a reasonable assumption in most practical settings.
Given our assumptions on the ranking function, we now investigate the keyword pruning policy, which prunes the inverted index IF horizontally by removing the whole I(ti)"s corresponding to the least frequent terms. In Figure 4 we show a graphical representation of keyword pruning, where we remove the inverted lists for t3 and t5, assuming that they do not appear often in the query load.
Note that after keyword pruning, if all keywords {t1, . . . , tn} in the query q appear in IP , the p-index has the same information as IF as long as q is concerned. In other words, if all keywords in q appear in IP , the answer computed from IP is guaranteed to be the same as the answer computed from IF . Figure 5 formalizes this observation and computes the correctness indicator function C for a keyword-pruned index IP . It is straightforward to prove that the answer from IP is identical to that from IF if C = 1 in the above algorithm.
We now consider the issue of optimizing the IP such that it can handle the largest fraction of queries. This problem can be formally stated as follows: Problem 2 (Optimal keyword pruning) Given the query load Q and a goal index size s · |IF | for the pruned index, select the inverted lists IP = {I(t1), . . . , I(th)} such that |IP | ≤ s · |IF | and the fraction of queries that IP can answer (expressed by f(s)) is maximized. 2 Unfortunately, the optimal solution to the above problem is intractable as we can show by reducing from knapsack (we omit the complete proof).
Theorem 2 The problem of calculating the optimal keyword pruning is NP-hard. 2 Given the intractability of the optimal solution, we need to resort to an approximate solution. A common approach for similar knapsack problems is to adopt a greedy policy by keeping the items with the maximum benefit per unit cost [9]. In our context, the potential benefit of an inverted list I(ti) is the number of queries that can be answered by IP when I(ti) is included in IP . We approximate this number by the fraction of queries in the query load Q that include the term ti and represent it as P(ti). For example, if 100 out of 1000 queries contain the term computer,
Algorithm 4.2 Greedy keyword pruning HS Procedure (1) ∀ti, calculate HS(ti) = P (ti) |I(ti)| . (2) Include the inverted lists with the highest HS(ti) values such that |IP | ≤ s · |IF |.
Figure 6: Approximation algorithm for the optimal keyword pruning.
Algorithm 4.3 Global document pruning V SG Procedure (1) Sort all documents Di based on pr(Di) (2) Find the threshold value τp, such that only s fraction of the documents have pr(Di) > τp (4) Keep Di in the inverted lists if pr(Di) > τp Figure 7: Global document pruning based on pr. then P(computer) = 0.1. The cost of including I(ti) in the pindex is its size |I(ti)|. Thus, in our greedy approach in Figure 6, we include I(ti)"s in the decreasing order of P(ti)/|I(ti)| as long as |IP | ≤ s · |IF |. Later in our experiment section, we evaluate what fraction of queries can be handled by IP when we employ this greedy keyword-pruning policy.
At a high level, document pruning tries to take advantage of the observation that most users are mainly interested in viewing the top few answers to a query. Given this, it is unnecessary to keep all postings in an inverted list I(ti), because users will not look at most of the documents in the list anyway. We depict the conceptual diagram of the document pruning policy in Figure 4. In the figure, we vertically prune postings corresponding to D4, D5 and D6 of t1 and D8 of t3, assuming that these documents are unlikely to be part of top-k answers to user queries. Again, our goal is to develop a pruning policy such that (1) we can compute the correctness indicator function C from IP alone and (2) we can handle the largest fraction of queries with IP . In the next few sections, we discuss a few alternative approaches for document pruning.
We first investigate the pruning policy that is commonly used by existing search engines. The basic idea for this pruning policy is that the query-independent quality score pr(D) is a very important factor in computing the final ranking of the document (e.g.
PageRank is known to be one of the most important factors determining the overall ranking in the search results), so we build the p-index by keeping only those documents whose pr values are high (i.e., pr(D) > τp for a threshold value τp). The hope is that most of the top-ranked results are likely to have high pr(D) values, so the answer computed from this p-index is likely to be similar to the answer computed from the full index. Figure 7 describes this pruning policy more formally, where we sort all documents Di"s by their respective pr(Di) values and keep a Di in the p-index when its Algorithm 4.4 Local document pruning V SL N: maximum size of a single posting list Procedure (1) Foreach I(ti) ∈ IF (2) Sort Di"s in I(ti) based on pr(Di) (3) If |I(ti)| ≤ N Then keep all Di"s (4) Else keep the top-N Di"s with the highest pr(Di) Figure 8: Local document pruning based on pr.
Algorithm 4.5 Extended keyword-specific document pruning Procedure (1) For each I(ti) (2) Keep D ∈ I(ti) if pr(D) > τpi or tr(D, ti) > τti Figure 9: Extended keyword-specific document pruning based on pr and tr. pr(Di) value is higher than the global threshold value τp. We refer to this pruning policy as global PR-based pruning (GPR).
Variations of this pruning policy are possible. For example, we may adjust the threshold value τp locally for each inverted list I(ti), so that we maintain at least a certain number of postings for each inverted list I(ti). This policy is shown in Figure 8. We refer to this pruning policy as local PR-based pruning (LPR).
Unfortunately, the biggest shortcoming of this policy is that we can prove that we cannot compute the correctness function C from IP alone when IP is constructed this way.
Theorem 3 No PR-based document pruning can provide the result guarantee. 2 Proof Assume we create IP based on the GPR policy (generalizing the proof to LPR is straightforward) and that every document D with pr(D) > τp is included in IP . Assume that the kth entry in the top-k results, has a ranking score of r(Dk, q) = fr(tr(Dk, q), pr(Dk)). Now consider another document Dj that was pruned from IP because pr(Dj) < τp. Even so, it is still possible that the document"s tr(Dj, q) value is very high such that r(Dj, q) = fr(tr(Dj, q), pr(Dj)) > r(Dk, q).
Therefore, under a PR-based pruning policy, the quality of the answer computed from IP can be significantly worse than that from IF and it is not possible to detect this degradation without computing the answer from IF . In the next section, we propose simple yet essential changes to this pruning policy that allows us to compute the correctness function C from IP alone.
The main problem of global PR-based document pruning policies is that we do not know the term-relevance score tr(D, ti) of the pruned documents, so a document not in IP may have a higher ranking score than the ones returned from IP because of their high tr scores.
Here, we propose a new pruning policy, called extended keyword-specific document pruning (EKS), which avoids this problem by pruning not just based on the query-independent pr(D) score but also based on the term-relevance tr(D, ti) score. That is, for every inverted list I(ti), we pick two threshold values, τpi for pr and τti for tr, such that if a document D ∈ I(ti) satisfies pr(D) > τpi or tr(D, ti) > τti, we include it in I(ti) of IP .
Otherwise, we prune it from IP . Figure 9 formally describes this algorithm. The threshold values, τpi and τti, may be selected in a number of different ways. For example, if pr and tr have equal weight in the final ranking and if we want to keep at most N postings in each inverted list I(ti), we may want to set the two threshold values equal to τi (τpi = τti = τi) and adjust τi such that N postings remain in I(ti).
This new pruning policy, when combined with a monotonic scoring function, enables us to compute the correctness indicator function C from the pruned index. We use the following example to explain how we may compute C.
Example 4 Consider the query q = {t1, t2} and a monotonic ranking function, f(pr(D), tr(D, t1), tr(D, t2)). There are three possible scenarios on how a document D appears in the pruned index IP .
information of D appears in IP , we can compute the exact Algorithm 4.6 Computing Answer from IP Input Query q = {t1, . . . , tw} Output A: top-k result, C: correctness indicator function Procedure (1) For each Di ∈ I(t1) ∪ · · · ∪ I(tw) (2) For each tm ∈ q (3) If Di ∈ I(tm) (4) tr∗(Di, tm) = tr(Di, tm) (5) Else (6) tr∗(Di, tm) = τtm (7) f(Di) = f(pr(Di), tr∗(Di, t1), . . . , tr∗(Di, tn)) (8) A = top-k Di"s with highest f(Di) values (9) C = j 1 if all Di ∈ A appear in all I(ti), ti ∈ q 0 otherwise Figure 10: Ranking based on thresholds trτ (ti) and prτ (ti). score of D based on pr(D), tr(D, t1) and tr(D, t2) values in IP : f(pr(D), tr(D, t1), tr(D, t2)).
not appear in I(t2), we do not know tr(D, t2), so we cannot compute its exact ranking score. However, from our pruning criteria, we know that tr(D, t2) cannot be larger than the threshold value τt2. Therefore, from the monotonicity of f (Definition 2), we know that the ranking score of D, f(pr(D), tr(D, t1), tr(D, t2)), cannot be larger than f(pr(D), tr(D, t1), τt2).
at all in IP , we do not know any of the pr(D), tr(D, t1), tr(D, t2) values. However, from our pruning criteria, we know that pr(D) ≤ τp1 and ≤ τp2 and that tr(D, t1) ≤ τt1 and tr(D, t2) ≤ τt2. Therefore, from the monotonicity of f, we know that the ranking score of D, cannot be larger than f(min(τp1, τp2), τt1, τt2). 2 The above example shows that when a document does not appear in one of the inverted lists I(ti) with ti ∈ q, we cannot compute its exact ranking score, but we can still compute its upper bound score by using the threshold value τti for the missing values. This suggests the algorithm in Figure 10 that computes the top-k result A from IP together with the correctness indicator function C. In the algorithm, the correctness indicator function C is set to one only if all documents in the top-k result A appear in all inverted lists I(ti) with ti ∈ q, so we know their exact score. In this case, because these documents have scores higher than the upper bound scores of any other documents, we know that no other documents can appear in the top-k. The following theorem formally proves the correctness of the algorithm. In [11] Fagin et al., provides a similar proof in the context of multimedia middleware.
Theorem 4 Given an inverted index IP pruned by the algorithm in Figure 9, a query q = {t1, . . . , tw} and a monotonic ranking function, the top-k result from IP computed by Algorithm 4.6 is the same as the top-k result from IF if C = 1. 2 Proof Let us assume Dk is the kth ranked document computed from IP according to Algorithm 4.6. For every document Di ∈ IF that is not in the top-k result from IP , there are two possible scenarios: First, Di is not in the final answer because it was pruned from all inverted lists I(tj), 1 ≤ j ≤ w, in IP . In this case, we know that pr(Di) ≤ min1≤j≤wτpj < pr(Dk) and that tr(Di, tj) ≤ τtj < tr(Dk, tj), 1 ≤ j ≤ w. From the monotonicity assumption, it follows that the ranking score of DI is r(Di) < r(Dk). That is,
Di"s score can never be larger than that of Dk.
Second, Di is not in the answer because Di is pruned from some inverted lists, say, I(t1), . . . , I(tm), in IP . Let us assume ¯r(Di) = f(pr(Di),τt1,. . . ,τtm,tr(Di, tm+1),. . . ,tr(Di, tw)). Then, from tr(Di, tj) ≤ τtj(1 ≤ j ≤ m) and the monotonicity assumption, 0
1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
1 Fractionofqueriesguaranteed−f(s) Fraction of index − s Fraction of queries guaranteed per fraction of index queries guaranteed Figure 11: Fraction of guaranteed queries f(s) answered in a keyword-pruned p-index of size s. we know that r(Di) ≤ ¯r(Di). Also, Algorithm 4.6 sets C = 1 only when the top-k documents have scores larger than ¯r(Di).
Therefore, r(Di) cannot be larger than r(Dk).
In order to perform realistic tests for our pruning policies, we implemented a search engine prototype. For the experiments in this paper, our search engine indexed about 130 million pages, crawled from the Web during March of 2004. The crawl started from the Open Directory"s [10] homepage and proceeded in a breadth-first manner. Overall, the total uncompressed size of our crawled Web pages is approximately 1.9 TB, yielding a full inverted index IF of approximately 1.2 TB.
For the experiments reported in this section we used a real set of queries issued to Looksmart [22] on a daily basis during April of 2003. After keeping only the queries containing keywords that were present in our inverted index, we were left with a set of about 462 million queries. Within our query set, the average number of terms per query is 2 and 98% of the queries contain at most 5 terms.
Some experiments require us to use a particular ranking function. For these, we use the ranking function similar to the one used in [20]. More precisely, our ranking function r(D, q) is r(D, q) = prnorm(D) + trnorm(D, q) (3) where prnorm(D) is the normalized PageRank of D computed from the downloaded pages and trnorm(D, q) is the normalized TF.IDF cosine distance of D to q. This function is clearly simpler than the real functions employed by commercial search engines, but we believe for our evaluation this simple function is adequate, because we are not studying the effectiveness of a ranking function, but the effectiveness of pruning policies.
In our first experiment we study the performance of the keyword pruning, described in Section 4.2. More specifically, we apply the algorithm HS of Figure 6 to our full index IF and create a keyword-pruned p-index IP of size s. For the construction of our keyword-pruned p-index we used the query frequencies observed during the first 10 days of our data set. Then, using the remaining 20-day query load, we measured f(s), the fraction of queries handled by IP . According to the algorithm of Figure 5, a query can be handled by IP (i.e., C = 1) if IP includes the inverted lists for all of the query"s keywords.
We have repeated the experiment for varying values of s, picking the keywords greedily as discussed in Section 4.2.The result is shown in Figure 11. The horizontal axis denotes the size s of the p-index as a fraction of the size of IF . The vertical axis shows the fraction f(s) of the queries that the p-index of size s can answer.
The results of Figure 11, are very encouraging: we can answer a significant fraction of the queries with a small fraction of the original index. For example, approximately 73% of the queries can be answered using 30% of the original index. Also, we find that when we use the keyword pruning policy only, the optimal index size is s = 0.17. 0
1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fractionofqueriesguaranteed-f(s) Fraction of index - s Fraction of queries guaranteed for top-20 per fraction of index fraction of queries guaranteed (EKS) Figure 12: Fraction of guaranteed queries f(s) answered in a document-pruned p-index of size s. 0
1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fractionofqueriesanswered index size - s Fraction of queries answered for top-20 per fraction of index GPR LPR EKS Figure 13: Fraction of queries answered in a document-pruned p-index of size s.
We continue our experimental evaluation by studying the performance of the various document pruning policies described in Section 4.3. For the experiments on document pruning reported here we worked with a 5.5% sample of the whole query set. The reason behind this is merely practical: since we have much less machines compared to a commercial search engine it would take us about a year of computation to process all 462 million queries.
For our first experiment, we generate a document-pruned p-index of size s by using the Extended Keyword-Specific pruning (EKS) in Section 4. Within the p-index we measure the fraction of queries that can be guaranteed (according to Theorem 4) to be correct. We have performed the experiment for varying index sizes s and the result is shown in Figure 12. Based on this figure, we can see that our document pruning algorithm performs well across the scale of index sizes s: for all index sizes larger than 40%, we can guarantee the correct answer for about 70% of the queries. This implies that our EKS algorithm can successfully identify the necessary postings for calculating the top-20 results for 70% of the queries by using at least 40% of the full index size. From the figure, we can see that the optimal index size s = 0.20 when we use EKS as our pruning policy.
We can compare the two pruning schemes, namely the keyword pruning and EKS, by contrasting Figures 11 and 12. Our observation is that, if we would have to pick one of the two pruning policies, then the two policies seem to be more or less equivalent for the p-index sizes s ≤ 20%. For the p-index sizes s > 20%, keyword pruning does a much better job as it provides a higher number of guarantees at any given index size. Later in Section 5.3, we discuss the combination of the two policies.
In our next experiment, we are interested in comparing EKS with the PR-based pruning policies described in Section 4.3. To this end, apart from EKS, we also generated document-pruned pindexes for the Global pr-based pruning (GPR) and the Local prbased pruning (LPR) policies. For each of the polices we created document-pruned p-indexes of varying sizes s. Since GPR and LPR cannot provide a correctness guarantee, we will compare the fraction of queries from each policy that are identical (i.e. the same results in the same order) to the top-k results calculated from the full index. Here, we will report our results for k = 20; the results are similar for other values of k. The results are shown in Figure 13. 0
1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Averagefractionofdocsinanswer index size - s Average fraction of docs in answer for top-20 per fraction of index GPR LPR EKS Figure 14: Average fraction of the top-20 results of p-index with size s contained in top-20 results of the full index.
Fraction of queries guaranteed for top-20 per fraction of index, using keyword and document 0
1 Keyword fraction of index - sh 0
1 Document fraction of index - sv 0
1 Fraction of queries guaranteed - f(s) Figure 15: Combining keyword and document pruning.
The horizontal axis shows the size s of the p-index; the vertical axis shows the fraction f(s) of the queries whose top-20 results are identical to the top-20 results of the full index, for a given size s.
By observing Figure 13, we can see that GPR performs the worst of the three policies. On the other hand EKS, picks up early, by answering a great fraction of queries (about 62%) correctly with only 10% of the index size. The fraction of queries that LPR can answer remains below that of EKS until about s = 37%. For any index size larger than 37%, LPR performs the best.
In the experiment of Figure 13, we applied the strict definition that the results of the p-index have to be in the same order as the ones of the full index. However, in a practical scenario, it may be acceptable to have some of the results out of order. Therefore, in our next experiment we will measure the fraction of the results coming from an p-index that are contained within the results of the full index. The result of the experiment is shown on Figure 14. The horizontal axis is, again, the size s of the p-index; the vertical axis shows the average fraction of the top-20 results common with the top-20 results from the full index. Overall, Figure 14 depicts that EKS and LPR identify the same high (≈ 96%) fraction of results on average for any size s ≥ 30%, with GPR not too far behind.
pruning In Sections 5.1 and 5.2 we studied the individual performance of our keyword and document pruning schemes. One interesting question however is how do these policies perform in combination? What fraction of queries can we guarantee if we apply both keyword and document pruning in our full index IF ?
To answer this question, we performed the following experiment.
We started with the full index IF and we applied keyword pruning to create an index Ih P of size sh · 100% of IF . After that, we further applied document pruning to Ih P , and created our final pindex IP of size sv ·100% of Ih P . We then calculated the fraction of guaranteed queries in IP . We repeated the experiment for different values of sh and sv. The result is shown on Figure 15. The x-axis shows the index size sh after applying keyword pruning; the y-axis shows the index size sv after applying document pruning; the z-axis shows the fraction of guaranteed queries after the two prunings. For example the point (0.2, 0.3, 0.4) means that if we apply keyword pruning and keep 20% of IF , and subsequently on the resulting index we apply document pruning keeping 30% (thus creating a pindex of size 20%·30% = 6% of IF ) we can guarantee 40% of the queries. By observing Figure 15, we can see that for p-index sizes smaller than 50%, our combined pruning does relatively well. For example, by performing 40% keyword and 40% document pruning (which translates to a pruned index with s = 0.16) we can provide a guarantee for about 60% of the queries. In Figure 15, we also observe a plateau for sh > 0.5 and sv > 0.5. For this combined pruning policy, the optimal index size is at s = 0.13, with sh =
[3, 30] provide a good overview of inverted indexing in Web search engines and IR systems. Experimental studies and analyses of various partitioning schemes for an inverted index are presented in [6, 23, 33]. The pruning algorithms that we have presented in this paper are independent of the partitioning scheme used.
The works in [1, 5, 7, 20, 27] are the most related to ours, as they describe pruning techniques based on the idea of keeping the postings that contribute the most in the final ranking. However, [1, 5, 7, 27] do not consider any query-independent quality (such as PageRank) in the ranking function. [32] presents a generic framework for computing approximate top-k answers with some probabilistic bounds on the quality of results. Our work essentially extends [1, 2, 4, 7, 20, 27, 31] by proposing mechanisms for providing the correctness guarantee to the computed top-k results.
Search engines use various methods of caching as a means of reducing the cost associated with queries [18, 19, 21, 31]. This thread of work is also orthogonal to ours because a caching scheme may operate on top of our p-index in order to minimize the answer computation cost. The exact ranking functions employed by current search engines are closely guarded secrets. In general, however, the rankings are based on query-dependent relevance and queryindependent document quality. Query-dependent relevance can be calculated in a variety of ways (see [3, 30]). Similarly, there are a number of works that measure the quality of the documents, typically as captured through link-based analysis [17, 28, 26]. Since our work does not assume a particular form of ranking function, it is complementary to this body of work.
There has been a great body of work on top-k result calculation.
The main idea is to either stop the traversal of the inverted lists early, or to shrink the lists by pruning postings from the lists [14, 4, 11, 8]. Our proof for the correctness indicator function was primarily inspired by [12].
Web search engines typically prune their large-scale inverted indexes in order to scale to enormous query loads. While this approach may improve performance, by computing the top results from a pruned index we may notice a significant degradation in the result quality. In this paper, we provided a framework for new pruning techniques and answer computation algorithms that guarantee that the top matching pages are always placed at the top of search results in the correct order. We studied two pruning techniques, namely keyword-based and document-based pruning as well as their combination. Our experimental results demonstrated that our algorithms can effectively be used to prune an inverted index without degradation in the quality of results. In particular, a keyword-pruned index can guarantee 73% of the queries with a size of 30% of the full index, while a document-pruned index can guarantee 68% of the queries with the same size. When we combine the two pruning algorithms we can guarantee 60% of the queries with an index size of 16%. It is our hope that our work will help search engines develop better, faster and more efficient indexes and thus provide for a better user search experience on the Web.
[1] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In SIGIR, 2001. [2] V. N. Anh and A. Moffat. Pruning strategies for mixed-mode querying. In CIKM, 2006. [3] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press / Addison-Wesley, 1999. [4] N. Bruno, L. Gravano, and A. Marian. Evaluating top-k queries over web-accessible databases. In ICDE, 2002. [5] S. B¨uttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In CIKM, 2006. [6] B. Cahoon, K. S. McKinley, and Z. Lu. Evaluating the performance of distributed architectures for information retrieval using a variety of workloads. ACM TOIS, 18(1), 2000. [7] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. Maarek, and A. Soffer. Static index pruning for information retrieval systems.
In SIGIR, 2001. [8] S. Chaudhuri and L. Gravano. Optimizing queries over multimedia repositories. In SIGMOD, 1996. [9] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms, 2nd Edition. MIT Press/McGraw Hill, 2001. [10] Open directory. http://www.dmoz.org. [11] R. Fagin. Combining fuzzy information: an overview. In SIGMOD Record, 31(2), 2002. [12] R. Fagin, A. Lotem, and M. Naor. Optimal aggregation algorithms for middleware. In PODS, 2001. [13] A. Gulli and A. Signorini. The indexable web is more than 11.5 billion pages. In WWW, 2005. [14] U. Guntzer, G. Balke, and W. Kiessling. Towards efficient multi-feature queries in heterogeneous environments. In ITCC, 2001. [15] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. Combating web spam with trustrank. In VLDB, 2004. [16] B. J. Jansen and A. Spink. An analysis of web documents retrieved and viewed. In International Conf. on Internet Computing, 2003. [17] J. Kleinberg. Authoritative sources in a hyperlinked environment.
Journal of the ACM, 46(5):604-632, September 1999. [18] R. Lempel and S. Moran. Predictive caching and prefetching of query results in search engines. In WWW, 2003. [19] R. Lempel and S. Moran. Optimizing result prefetching in web search engines with segmented indices. ACM Trans. Inter. Tech., 4(1), 2004. [20] X. Long and T. Suel. Optimized query execution in large search engines with global page ordering. In VLDB, 2003. [21] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. In WWW, 2005. [22] Looksmart inc. http://www.looksmart.com. [23] S. Melnik, S. Raghavan, B. Yang, and H. Garcia-Molina. Building a distributed full-text index for the web. ACM TOIS, 19(3):217-241,
[24] A. Ntoulas, J. Cho, C. Olston. What"s new on the web? The evolution of the web from a search engine perspective. In WWW, 2004. [25] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. Detecting spam web pages through content analysis. In WWW, 2006. [26] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. Technical report,

Many researchers have worked on topic detection and tracking (TDT) [26] and topic segmentation during the past decade.
Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure. Topic segmentation tasks usually fall into two categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics. The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents. Previous research in connection with TDT falls into the former category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.
Traditional approaches perform topic segmentation on documents one at a time [15, 25, 6]. Most of them perform badly in subtle tasks like coherent document segmentation [15]. Often, end-users seek documents that have the similar content. Search engines, like, Google, provide links to obtain similar pages. At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest. Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.
Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6]. However, they usually suffer the issue of identifying stop words. For example, additional document-dependent stop words are removed together with the generic stop words in [15]. There are two reasons that we do not remove stop words directly. First, identifying stop words is another issue [12] that requires estimation in each domain. Removing common stop words may result in the loss of useful information in a specific domain. Second, even though stop words can be identified, hard classification of stop words and nonstop words cannot represent the gradually changing amount of information content of each word. We employ a soft classification using term weights.
In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment. This is equal to maximizing the MI (or WMI). The MI focuses on measuring the difference among segments whereas previous research focused on finding the similarity (e.g. cosine distance) of segments [15, 25, 6]. Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster. Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem. Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).
Usually, human readers can identify topic transition based on cue words, and can ignore stop words. Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of documents. Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.
These words are common in a document. Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].
Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.
The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria. Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents. Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly. Obviously, our approach can handle single documents as a special case when multiple documents are unavailable. It can detect shared topics among documents to judge if they are multiple documents on the same topic. We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further. We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice. Some of our prior work is in [24].
The rest of this paper is organized as follows: In Section 2, we review related work. Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI. In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by usFigure 1: Illustration of multi-document segmentation and alignment. ing dynamic programming. In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm. Conclusions and some future directions of the research work are discussed in Section 6.
Generally, the existing approaches to text segmentation fall into two categories: supervised learning [19, 17, 23] and unsupervised learning [3, 27, 5, 6, 15, 25, 21].
Supervised learning usually has good performance, since it learns functions from labelled training sets. However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired. Some models consider dependence between sentences and sections, such as Hidden Markov Model [3, 27], Maximum Entropy Markov Model [19], and Conditional Random Fields [17], while many other approaches are based on lexical cohesion or similarity of sentences [5, 6, 15, 25, 21]. Some approaches also focus on cue words as hints of topic transitions [11]. While some existing methods only consider information in single documents [6, 15], others utilize multiple documents [16, 14]. There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents. Previous research studied methods to find shared topics [16] and topic segmentation and summarization between just a pair of documents [14].
Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods. Topical classification or clustering is an important direction in this area, especially co-clustering of documents and terms, such as LSA [9], PLSA [13], and approaches based on distances and bipartite graph partitioning [28] or maximum MI [2, 10], or maximum entropy [1, 18]. Criteria of these approaches can be utilized in the issue of topic segmentation. Some of those methods have been extended into the area of topic segmentation, such as PLSA [5] and maximum entropy [7], but to our best knowledge, using MI for topic segmentation has not been studied.
Our goal is to segment documents and align the segments across documents (Figure 1). Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm}. Let Sd be the set of sentences for document d ∈ D, i.e.{s1, s2, ..., snd }. We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T. Sd actually is a random vector including a random variable for each d ∈ D. The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND, where T(t, d, s) is the number of t in d"s sentence s and ND is the total number of terms in D. ˆS represents the set of segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment among multiple documents, where the number of segments | ˆS| = p. A segment ˆsi of document d is a sequence of adjacent sentences in d. Since for different documents si may discuss different sub-topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ˆsi is about the same sub-topic. The goal is to find the optimal topic segmentation and alignment mapping Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp} and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈ D, where ˆsi is ith segment with the constraint that only adjacent sentences can be mapped to the same segment, i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p}, where p is the segment number, and if i > j, then for d, ˆsq is missing. After segmentation and alignment, random vector Sd becomes an aligned random variable ˆS. Thus,
P(D, Sd, T) becomes P(D, ˆS, T).
Term co-clustering is a technique that has been employed [10] to improve the accuracy of document clustering. We evaluate the effect of it for topic segmentation. A term t is mapped to exactly one term cluster. Term co-clustering involves simultaneously finding the optimal term clustering mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤ l, l is the total number of words in all the documents, and k is the number of clusters.
We now describe a novel algorithm which can handle singledocument segmentation, shared topic detection, and multidocument segmentation and alignment based on MI or WMI.
MI I(X; Y ) is a quantity to measure the amount of information which is contained in two or more random variables [8, 10]. For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are independent, I(X; Y ) = 0. Thus, intuitively, the value of MI depends on how random variables are dependent on each other.
The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ). This is the criterion of MI for clustering.
In the case of topic segmentation, the two random variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are. However, I(T; S) cannot be computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents. Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents. The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS).
In topic segmentation and alignment of multiple documents, if P(D, ˆS, T) is known, based on the marginal distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the dimensions of documents and segments. • Document-dependent stop words that depends on the personal writing style are common only along the dimension of segments for some documents. • Cue words are the most important elements for segmentation. They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments. • Noisy words are other words which are not common along both dimensions.
Entropy based on P(D|T) and P( ˆS|T) can be used to identify different types of terms. To reinforce the contribution of cue words in the MI computation, and simultaneously reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e. ED(ˆt) and EˆS(ˆt), to compute the weight. A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).
We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) ,
EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights. Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values. Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).
However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown. Thus, an iterative algorithm is required to estimate term weights wˆt and find the best segmentation and alignment to optimize the objective function Iw concurrently. After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.
Output: Mapping Clu, Seg, Ali, and term weights wˆt.
Initialization:
(0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1:
segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2:
Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i);
segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l;
if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3:
(i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3);
segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ;
and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and alignment based on MI or WMI. and each sentence is segmented into words, each word is stemmed. Then the joint probability distribution P(D, Sd, T) can be estimated. Finally, this distribution can be used to compute MI in our algorithm.
Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term occurrences in different segments. Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.
Moreover, this problem is NP-hard [10], even though if we know the term weights. Thus, an iterative greedy algorithm is desired to find the best solution, even though probably only local maxima are reached. We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation. This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering. Otherwise, since it is just a one step algorithm to solve the task of single-document segmentation [6, 15, 25], the global maximum of MI is guaranteed.
We will show later that term co-clustering reduces the accuracy of the results and is not necessary, and for singledocument segmentation, term weights are also not required.
In Step 0, the initial term clustering Clut and topic segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.
First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t.
Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences. Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt . For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.
For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained.
After initialization, there are three stages for different cases. Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1. Single document segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1). If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively. If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is executed iteratively. If both are required (k < l, w = 1), Stage 2 and 3 run one after the other. For multi-document segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.
At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.
Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster. Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.
At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation. This cycle is repeated to find a local maximum based on MI I until it converges. The two steps are: (1) based on current term clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid. After finding a good term clustering, term weights are estimated if w = 1.
At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.
They are repeated to find a local maximum based on WMI Iw until it converges. However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result. Finally, at Step 3.3, this algorithm converges and return the output. This algorithm can handle both single-document and multi-document segmentation. It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2.
In many previous works on segmentation, dynamic programming is a technique used to maximize the objective function. Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming. For Stage 1, using dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a document. Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw). There are two cases that are not shown in the algorithm in Figure 2: (a) single-document segmentation or multi-document segmentation with the same sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different sequential orders of segments, where alignment is necessary. The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter one"s alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different. The computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs) without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j. (2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)). Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document segmentation). (3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1. The optimal Iw is found and the corresponding segmentation is the best.
Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1. (2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}. Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p! L!(p−L)! combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j. (3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.
The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation. First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set. Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found. Finally, the maximal WMI is found among different sums of M(sm, p − 1) and PIw for Segment p.
In this section, single-document segmentation, shared topic detection, and multi-document segmentation will be tested.
Different hyper parameters of our method are studied. For convenience, we refer to the method using I as MIk if w = 0, and Iw as WMIk if w = 2 or as WMIk if w = 1, where k is the number of term clusters, and if k = l, where l is the total number of terms, then no term clustering is required, i.e. MIl and WMIl.
The first data set we tested is a synthetic one used in previous research [6, 15, 25] and many other papers. It has 700 samples. Each is a concatenation of ten segments. Each segment is the first n sentence selected randomly from the Brown corpus, which is supposed to have a different topic from each other. Currently, the best results on this data set is achieved by Ji et.al. [15]. To compare the performance of our methods, the criterion used widely in previous research is applied, instead of the unbiased criterion introduced in [20]. It chooses a pair of words randomly. If they are in different segments (different) for the real segmentation (real), but predicted (pred) as in the same segment, it is a miss. If they are in the same segment (same), but predicted as in different segments, it is a false alarm. Thus, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, diff)p(diff|real) +p(false alarm|real, pred, same)p(same|real).
We tested the case when the number of segments is known.
Table 1 shows the results of our methods with different hyper parameter values and three previous approaches, C99[25],
U00[6], and ADDP03[15], on this data set when the segment number is known. In WMI for single-document segmentation, the term weights are computed as follows: wˆt = 1−EˆS(ˆt)/maxˆt ∈ ˆT (EˆS(ˆt )). For this case, our methods MIl and WMIl both outperform all the previous approaches.
We compared our methods with ADDP03using one-sample one-sided t-test and p-values are shown in Table 2. From the p-values, we can see that mostly the differences are very Table 1: Average Error Rates of Single-document Segmentation Given Segment Numbers Known Range of n 3-11 3-5 6-8 9-11 Sample size 400 100 100 100 C99 12% 11% 10% 9% U00 10% 9% 7% 5% ADDP03 6.0% 6.8% 5.2% 4.3% MIl 4.68% 5.57% 2.59% 1.59% WMIl 4.94% 6.33% 2.76% 1.62% MI100 9.62% 12.92% 8.66% 6.67% Table 2: Single-document Segmentation: P-values of T-test on Error Rates Range of n 3-11 3-5 6-8 9-11 ADDP03, MIl 0.000 0.000 0.000 0.000 ADDP03, WMIl 0.000 0.099 0.000 0.000 MIl, WMIl 0.061 0.132 0.526 0.898 significant. We also compare the error rates between our two methods using two-sample two-sided t-test to check the hypothesis that they are equal. We cannot reject the hypothesis that they are equal, so the difference are not significant, even though all the error rates for MIl are smaller than WMIl. However, we can conclude that term weights contribute little in single-document segmentation. The results also show that MI using term co-clustering (k = 100) decreases the performance. We tested different number of term clusters, and found that the performance becomes better when the cluster number increases to reach l. WMIk<l has similar results that we did not show in the table.
As mentioned before, using MI may be inconsistent on optimal boundaries given different numbers of segments. This situation occurs especially when the similarities among segments are quite different, i.e. some transitions are very obvious, while others are not. This is because usually a document is a hierarchical structure instead of only a sequential structure. When the segments are not at the same level, this situation may occur. Thus, a hierarchical topic segmentation approach is desired, and the structure highly depends on the number of segments for each internal node and the stop criteria of splitting. For this data set of singledocument segmentation, since it is just a synthetic set, which is just a concatenation of several segments about different topics, it is reasonable that approaches simply based on term frequency have a good performance. Usually for the tasks of segmenting coherent documents for sub-topics, the effectiveness decreases much.
The second data set contains 80 news articles from Google News. There are eight topics and each has 10 articles. We randomly split the set into subsets with different document numbers and each subset has all eight topics. We compare our approach MIl and WMIl with LDA [4]. LDA treats a document in the data set as a bag of words, finds its distribution on topics, and its major topic. MIl and WMIl views each sentence as a bag of words and tag it with a topic label. Then for each pair of documents, LDA determines if they are on the same topic, while MIl and Table 3: Shared Topic Detection: Average Error Rates for Different Numbers of Documents in Each Subset #Doc 10 20 40 80 LDA 8.89% 16.33% 1.35% 0.60% MIl, θ = 0.6 4.17% 1.71% 1.47% 0.0% WMIl, θ = 0.8 18.6% 3.16% 1.92% 0.0% WMIl check whether the proportion overlapped sentences on the same topic is larger than the adjustable threshold θ.
That is, in MIl and WMIl, for a pair of documents d, d , if [ s∈Sd,s ∈Sd 1(topics=topics )/min(|Sd|, |Sd|)] > θ, where Sd is the set of sentences of d, and |Sd| is the number of sentences of d, then d and d have the shared topic.
For a pair of documents selected randomly, the error rate is computed using the following equation: p(err|real, pred) = p(miss|real, pred, same)p(same|real) +p(false alarm|real, pred, diff)p(diff|real), where a miss means if they have the same topic (same) for the real case (real), but predicted (pred) as on the same topic. If they are on different topics (diff), but predicted as on the same topic, it is a false alarm.
The results are shown in Table 3. If most documents have different topics, in WMIl, the estimation of term weights in Equation (3) is not correct. Thus, WMIl is not expected to have a better performance than MIl, when most documents have different topics. When there are fewer documents in a subset with the same number of topics, more documents have different topics, so WMIl is more worse than MIl. We can see that for most cases MIl has a better (or at least similar) performance than LDA. After shared topic detection, multi-document segmentation of documents with the shared topics is able to be executed.
For multi-document segmentation and alignment, our goal is to identify these segments about the same topic among multiple similar documents with shared topics. Using Iw is expected to perform better than I, since without term weights the result is affected seriously by document-dependent stop words and noisy words which depends on the personal writing style. It is more likely to treat the same segments of different documents as different segments under the effect of document-dependent stop words and noisy words. Term weights can reduce the effect of document-dependent stop words and noisy words by giving cue terms more weights.
The data set for multi-document segmentation and alignment has 102 samples and 2264 sentences totally. Each is the introduction part of a lab report selected from the course of Biol 240W, Pennsylvania State University. Each sample has two segments, introduction of plant hormones and the content in the lab. The length range of samples is from two to 56 sentences. Some samples only have one part and some have a reverse order the these two segments. It is not hard to identify the boundary between two segments for human. We labelled each sentence manually for evaluation.
The criterion of evaluation is just using the proportion of the number of sentences with wrong predicted segment labels in the total number of sentences in the whole training Table 4: Average Error Rates of Multi-document Segmentation Given Segment Numbers Known #Doc MIl WMIl k MIk WMIk 102 3.14% 2.78% 300 4.68% 6.58% 51 4.17% 3.63% 300 17.83% 22.84% 34 5.06% 4.12% 300 18.75% 20.95% 20 7.08% 5.42% 250 20.40% 21.83% 10 10.38% 7.89% 250 21.42% 21.91% 5 15.77% 11.64% 250 21.89% 22.59% 2 25.90% 23.18% 50 25.44% 25.49% 1 23.90% 24.82% 25 25.75% 26.15% Table 5: Multi-document Segmentation: P-values of T-test on Error Rates for MIl and WMIl #Doc 51 34 20 10 5 2 P-value 0.19 0.101 0.025 0.001 0.000 0.002 set as the error rate: p(error|predicted, real) = d∈D s∈Sd 1(predicteds=reals)/ d∈D nd.
In order to show the benefits of multi-document segmentation and alignment, we compared our method with different parameters on different partitions of the same training set.
Except the cases that the number of documents is 102 and one (they are special cases of using the whole set and the pure single-document segmentation), we randomly divided the training set into m partitions, and each has 51, 34, 20, 10, 5, and 2 document samples. Then we applied our methods on each partition and calculated the error rate of the whole training set. Each case was repeated for 10 times for computing the average error rates. For different partitions of the training set, different k values are used, since the number of terms increases when the document number in each partition increases.
From the experiment results in Table 4, we can see the following observations: (1) When the number of documents increases, all methods have better performances. Only from one to two documents, MIl has decrease a little. We can observe this from Figure 3 at the point of document number = 2. Most curves even have the worst results at this point. There are two reasons. First, because samples vote for the best multi-document segmentation and alignment, but if only two documents are compared with each other, the one with missing segments or a totally different sequence will affect the correct segmentation and alignment of the other.
Second, as noted at the beginning of this section, if two documents have more document-dependent stop words or noisy words than cue words, then the algorithm may view them as two different segments and the other segment is missing.
Generally, we can only expect a better performance when the number of documents is larger than the number of segments. (2) Except single-document segmentation, WMIl is always better than MIl, and when the number of documents is reaching one or increases to a very large number, their performances become closer. Table 5 shows p-values of twosample one-sided t-test between MIl and WMIl. We also can see this trend from p-values. When document number = 5, we reached the smallest p-value and the largest difference between error rates of MIl and WMIl. For single-document Table 6: Multi-document Segmentation: Average Error Rate for Document Number = 5 in Each Subset with Different Number of Term Clusters #Cluster 75 100 150 250 l MIk 24.67% 24.54% 23.91% 22.59% 15.77% segmentation, WMIl is even a little bit worse than MIl, which is similar as the results of the single-document segmentation on the first data set. The reason is that for singledocument segmentation, we cannot estimate term weights accurately, since multiple documents are unavailable. (3) Using term clustering usually gets worse results than MIl and WMIl.(4) Using term clustering in WMIk is even worse than in MIk, since in WMIk term clusters are found first using I before using Iw. If the term clusters are not correct, then the term weights are estimated worse, which may mislead the algorithm to reach even worse results. From the results we also found that in multi-document segmentation and alignment, most documents with missing segments and a reverse order are identified correctly.
Table 6 illustrates the experiment results for the case of 20 partitions (each has five document samples) of the training set and topic segmentation and alignment using MIk with different numbers of term clusters k. Notice that when the number of term clusters increases, the error rate becomes smaller. Without term clustering, we have the best result.
We did not show results for WMIk with term clustering, but the results are similar.
We also tested WMIl with different hyper parameters of a and b to adjust term weights. The results are presented in Figure 3. It was shown that the default case WMIl : a = 1, b = 1 gave the best results for different partitions of the training set. We can see the trend that when the document number is very small or large, the difference between MIl : a = 0, b = 0 and WMIl : a = 1, b = 1 becomes quite small. When the document number is not large (about from 2 to 10), all the cases using term weights have better performances than MIl : a = 0, b = 0 without term weights, but when the document number becomes larger, the cases WMIl : a = 1, b = 0 and WMIl : a = 2, b = 1 become worse than MIl : a = 0, b = 0. When the document number becomes very large, they are even worse than cases with small document numbers. This means that a proper way to estimate term weights for the criterion of WMI is very important. Figure 4 shows the term weights learned from the whole training set. Four types of words are categorized roughly even though the transition among them are subtle. Figure 5 illustrates the change in (weighted) mutual information for MIl and WMIl. As expected, mutual information for MIl increases monotonically with the number of steps, while WMIl does not. Finally, MIl and WMIl are scalable, with computational complexity shown in Figure 6.
One advantage for our approach based on MI is that removing stop words is not required. Another important advantage is that there are no necessary hyper parameters to adjust. In single-document segmentation, the performance based on MI is even better for that based on WMI, so no extra hyper parameter is required. In multi-document segmentation, we show in the experiment, a = 1 and b = 1 is the best. Our method gives more weights to cue terms.
However, usually cue terms or sentences appear at the beginning of a segment, while the end of the segment may be 1 2 5 10 20 34 51 102 0
Document Number ErrorRate MIl :a=0,b=0 WMI l :a=1,b=1 WMI l :a=1,b=0 WMI l :a=2,b=1 Figure 3: Error rates for different hyper parameters of term weights. 0 0.2 0.4 0.6 0.8 1 0
1 Normalized Document Entropy NormalizedSegmentEntropy Noisy words Cue words Common stop words Doc−dep stop words Figure 4: Term weights learned from the whole training set. 0 100 200 300 400 500 600
Number of Steps (Weighted)MutualInformation MI l WMI l Figure 5: Change in (weighted) MI for MIl and WMIl. 0 20 40 60 80 100 120 0 200 400 600 800 1000 1200 1400 1600 1800 2000 Document Number TimetoConverge(sec) MI l WMI l Figure 6: Time to converge for MIl and WMIl. much noisy. One possible solution is giving more weights to terms at the beginning of each segment. Moreover, when the length of segments are quite different, long segments have much higher term frequencies, so they may dominate the segmentation boundaries. Normalization of term frequencies versus the segment length may be useful.
We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases. We used dynamic programming to optimize our algorithm. Our approach outperforms all the previous methods on singledocument cases. Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously. Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance.
We only tested our method on limited data sets. More data sets especially complicated ones should be tested. More previous methods should be compared with. Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries. Supervised learning also can be considered.
The authors want to thank Xiang Ji, and Prof. J. Scott Payne for their help.
[1] A. Banerjee, I. Ghillon, J. Ghosh, S. Merugu, and D. Modha. A generalized maximum entropy approach to bregman co-clustering and matrix approximation. In Proceedings of SIGKDD, 2004. [2] R. Bekkerman, R. El-Yaniv, and A. McCallum. Multi-way distributional clustering via pairwise interactions. In Proceedings of ICML, 2005. [3] D. M. Blei and P. J. Moreno. Topic segmentation with an aspect hidden markov model. In Proceedings of SIGIR,
[4] D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [5] T. Brants, F. Chen, and I. Tsochantaridis. Topic-based document segmentation with probabilistic latent semantic analysis. In Proceedings of CIKM, 2002. [6] F. Choi. Advances in domain indepedent linear text segmentation. In Proceedings of the NAACL, 2000. [7] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
Maximum entropy segmentation of broadcast news. In Proceedings of ICASSP, 2005. [8] T. Cover and J. Thomas. Elements of Information Theory.
John Wiley and Sons, New York, USA, 1991. [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Systems, 1990. [10] I. Dhillon, S. Mallela, and D. Modha. Information-theoretic co-clustering. In Proceedings of SIGKDD, 2003. [11] M. Hajime, H. Takeo, and O. Manabu. Text segmentation with multiple surface linguistic cues. In Proceedings of COLING-ACL, 1998. [12] T. K. Ho. Stop word location and identification for adaptive text recognition. International Journal of Document Analysis and Recognition, 3(1), August 2000. [13] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of the UAI"99, 1999. [14] X. Ji and H. Zha. Correlating summarization of a pair of multilingual documents. In Proceedings of RIDE, 2003. [15] X. Ji and H. Zha. Domain-independent text segmentation using anisotropic diffusion and dynamic programming. In Proceedings of SIGIR, 2003. [16] X. Ji and H. Zha. Extracting shared topics of multiple documents. In Proceedings of the 7th PAKDD, 2003. [17] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, 2001. [18] T. Li, S. Ma, and M. Ogihara. Entropy-based criterion in categorical clustering. In Proceedings of ICML, 2004. [19] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy markov models for information extraction and segmentation. In Proceedings of ICML, 2000. [20] L. Pevzner and M. Hearst. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistic, 28(1):19-36, 2002. [21] J. C. Reynar. Statistical models for topic segmentation. In Proceedings of ACL, 1999. [22] G. Salton and M. McGill. Introduction to Modern Information Retrieval. McGraw Hill, 1983. [23] B. Sun, Q. Tan, P. Mitra, and C. L. Giles. Extraction and search of chemical formulae in text documents on the web.

There are more than 4,000 online news sources in the world. Manually monitoring all of them for important events has become difficult or practically impossible. In fact, the topic detection and tracking (TDT) community has for many years been trying to come up with a practical solution to help people monitor news effectively. Unfortunately, the holy grail is still elusive, because the vast majority of TDT solutions proposed for event detection [20, 5, 17, 4, 21, 7, 14, 10] are either too simplistic (based on cosine similarity [5]) or impractical due to the need to tune a large number of parameters [9]. The ineffectiveness of current TDT technologies can be easily illustrated by subscribing to any of the many online news alerts services such as the industryleading Google News Alerts [2], which generates more than 50% false alarms [10]. As further proof, portals like Yahoo take a more pragmatic approach by requiring all machine generated news alerts to go through a human operator for confirmation before sending them out to subscribers.
Instead of attacking the problem with variations of the same hammer (cosine similarity and TFIDF), a fundamental understanding of the characteristics of news stream data is necessary before any major breakthroughs can be made in TDT. Thus in this paper, we look at news stories and feature trends from the perspective of analyzing a time-series word signal. Previous work like [9] has attempted to reconstruct an event with its representative features. However, in many predictive event detection tasks (i.e., retrospective event detection), there is a vast set of potential features only for a fixed set of observations (i.e., the obvious bursts).
Of these features, often only a small number are expected to be useful. In particular, we study the novel problem of analyzing feature trajectories for event detection, borrowing a well-known technique from signal processing: identifying distributional correlations among all features by spectral analysis. To evaluate our method, we subsequently propose an unsupervised event detection algorithm for news streams. 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 Easter April (a) aperiodic event 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 Unaudited Ended (b) periodic event Figure 1: Feature correlation (DFIDF:time) between a) Easter and April b) Unaudited and Ended.
As an illustrative example, consider the correlation between the words Easter and April from the Reuters Corpus1 . From the plot of their normalized DFIDF in Figure 1(a), we observe the heavy overlap between the two words circa 04/1997, which means they probably both belong to the same event during that time (Easter feast). In this example, the hidden event Easter feast is a typical important aperiodic event over 1-year data. Another example is given by Figure 1(b), where both the words Unaudited and Ended 1 Reuters Corpus is the default dataset for all examples. exhibit similar behaviour over periods of 3 months. These two words actually originated from the same periodic event, net income-loss reports, which are released quarterly by publicly listed companies.
Other observations drawn from Figure 1 are: 1) the bursty period of April is much longer than Easter, which suggests that April may exist in other events during the same period; 2) Unaudited has a higher average DFIDF value than Ended, which indicates Unaudited to be more representative for the underlying event. These two examples are but the tip of the iceberg among all word trends and correlations hidden in a news stream like Reuters. If a large number of them can be uncovered, it could significantly aid TDT tasks. In particular, it indicates the significance of mining correlating features for detecting corresponding events. To summarize, we postulate that: 1) An event is described by its representative features. A periodic event has a list of periodic features and an aperiodic event has a list of aperiodic features; 2) Representative features from the same event share similar distributions over time and are highly correlated; 3) An important event has a set of active (largely reported) representative features, whereas an unimportant event has a set of inactive (less-reported) representative features; 4) A feature may be included by several events with overlaps in time frames. Based on these observations, we can either mine representative features given an event or detect an event from a list of highly correlated features. In this paper, we focus on the latter, i.e., how correlated features can be uncovered to form an event in an unsupervised manner.
This paper has three main contributions: • To the best of our knowledge, our approach is the first to categorize word features for heterogenous events.
Specifically, every word feature is categorized into one of the following five feature types based on its power spectrum strength and periodicity: 1) HH (high power and high/long periodicity): important aperiodic events, 2) HL (high power and low periodicity): important periodic events, 3) LH (low power and high periodicity): unimportant aperiodic events, 4) LL (low power and low periodicity): non-events, and 5) SW (stopwords), a higher power and periodicity subset of LL comprising stopwords, which contains no information. • We propose a simple and effective mixture densitybased approach to model and detect feature bursts. • We come up with an unsupervised event detection algorithm to detect both aperiodic and periodic events.
Our algorithm has been evaluated on a real news stream to show its effectiveness.
This work is largely motivated by a broader family of problems collectively known as Topic Detection and Tracking (TDT) [20, 5, 17, 4, 21, 7, 14, 10]. Moreover, most TDT research so far has been concerned with clustering/classifying documents into topic types, identifying novel sentences [6] for new events, etc., without much regard to analyzing the word trajectory with respect to time. Swan and Allan [18] first attempted using co-occuring terms to construct an event.
However, they only considered named entities and noun phrase pairs, without considering their periodicities. On the contrary, our paper considers all of the above.
Recently, there has been significant interest in modeling an event in text streams as a burst of activities by incorporating temporal information. Kleinberg"s seminal work described how bursty features can be extracted from text streams using an infinite automaton model [12], which inspired a whole series of applications such as Kumar"s identification of bursty communities from Weblog graphs [13], Mei"s summarization of evolutionary themes in text streams [15],
He"s clustering of text streams using bursty features [11], etc.
Nevertheless, none of the existing work specifically identified features for events, except for Fung et al. [9], who clustered busty features to identify various bursty events. Our work differs from [9] in several ways: 1) we analyze every single feature, not only bursty features; 2) we classify features along two categorical dimensions (periodicity and power), yielding altogether five primary feature types; 3) we do not restrict each feature to exclusively belong to only one event.
Spectral analysis techniques have previously been used by Vlachos et al. [19] to identify periodicities and bursts from query logs. Their focus was on detecting multiple periodicities from the power spectrum graph, which were then used to index words for query-by-burst search. In this paper, we use spectral analysis to classify word features along two dimensions, namely periodicity and power spectrum, with the ultimate goal of identifying both periodic and aperiodic bursty events.
Let T be the duration/period (in days) of a news stream, and F represents the complete word feature space in the classical static Vector Space Model (VSM).
Within T, there may exist certain events that occur only once, e.g., Tony Blair elected as Prime Minister of U.K., and other recurring events of various periodicities, e.g., weekly soccer matches. We thus categorize all events into two types: aperiodic and periodic, defined as follows.
Definition 1. (Aperiodic Event) An event is aperiodic within T if it only happens once.
Definition 2. (Periodic Event) If events of a certain event genre occur regularly with a fixed periodicity P ≤ T/2 , we say that this particular event genre is periodic, with each member event qualified as a periodic event.
Note that the definition of aperiodic is relative, i.e., it is true only for a given T, and may be invalid for any other T > T. For example, the event Christmas feast is aperiodic for T ≤ 365 but periodic for T ≥ 730.
Intuitively, an event can be described very concisely by a few discriminative and representative word features and vice-versa, e.g., hurricane, sweep, and strike could be representative features of a Hurricane genre event. Likewise, a set of strongly correlated features could be used to reconstruct an event description, assuming that strongly correlated features are representative. The representation vector of a word feature is defined as follows: Definition 3. (Feature Trajectory) The trajectory of a word feature f can be written as the sequence yf = [yf (1), yf (2), . . . , yf (T)], where each element yf (t) is a measure of feature f at time t, which could be defined using the normalized DFIDF score2 yf (t) = DFf (t) N(t) × log( N DFf ), where DFf (t) is the number of documents (local DF) containing feature f at day t, DFf is the total number of documents (global DF) containing feature f over T, N(t) is the number of documents for day t, and N is the total number of documents over T.
In this section, we show how representative features can be extracted for (un)important or (a)periodic events.
Given a feature f, we decompose its feature trajectory yf = [yf (1), yf (2), ..., yf (T)] into the sequence of T complex numbers [X1, . . . , XT ] via the discrete Fourier transform (DFT): Xk = T t=1 yf (t)e− 2πi T (k−1)t , k = 1, 2, . . . , T.
DFT can represent the original time series as a linear combination of complex sinusoids, which is illustrated by the inverse discrete Fourier transform (IDFT): yf (t) = 1 T T k=1 Xke 2πi T (k−1)t , t = 1, 2, . . . , T, where the Fourier coefficient Xk denotes the amplitude of the sinusoid with frequency k/T.
The original trajectory can be reconstructed with just the dominant frequencies, which can be determined from the power spectrum using the popular periodogram estimator.
The periodogram is a sequence of the squared magnitude of the Fourier coefficients, Xk 2 , k = 1, 2, . . . , T/2 , which indicates the signal power at frequency k/T in the spectrum.
From the power spectrum, the dominant period is chosen as the inverse of the frequency with the highest power spectrum, as follows.
Definition 4. (Dominant Period) The dominant period (DP) of a given feature f is Pf = T/ arg max k Xk 2 .
Accordingly, we have Definition 5. (Dominant Power Spectrum) The dominant power spectrum (DPS) of a given feature f is Sf = Xk 2 , with Xk 2 ≥ Xj 2 , ∀j = k.
The DPS of a feature trajectory is a strong indicator of its activeness at the specified frequency; the higher the DPS, the more likely for the feature to be bursty. Combining DPS with DP, we therefore categorize all features into four types: 2 We normalize yf (t) as yf (t) = yf (t)/ T i=1 yf (i) so that it could be interpreted as a probability. • HH: high Sf , aperiodic or long-term periodic (Pf > T/2 ); • HL: high Sf , short-term periodic (Pf ≤ T/2 ); • LH: low Sf , aperiodic or long-term periodic; • LL: low Sf , short-term periodic.
The boundary between long-term and short-term periodic is set to T/2 . However, distinguishing between a high and low DPS is not straightforward, which will be tackled later.
Properties of Different Feature Sets To better understand the properties of HH, HL, LH and LL, we select four features, Christmas, soccer, DBS and your as illustrative examples. Since the boundary between high and low power spectrum is unclear, these chosen examples have relative wide range of power spectrum values. Figure 2(a) shows the DFIDF trajectory for Christmas with a distinct burst around Christmas day. For the 1-year Reuters dataset,
Christmas is classified as a typical aperiodic event with Pf = 365 and Sf = 135.68, as shown in Figure 2(b). Clearly, the value of Sf = 135.68 is reasonable for a well-known bursty event like Christmas. 0
1
8/20/1996 12/8/1996 3/28/1997 7/16/1997 (a) Christmas(DFIDF:time) 0 50 100 150
P=365 S=135.68 (b) Christmas(S:frequency) Figure 2: Feature Christmas with relative high Sf and long-term Pf .
The DFIDF trajectory for soccer is shown in Figure 3(a), from which we can observe that there is a regular burst every 7 days, which is again verified by its computed value of Pf = 7, as shown in Figure 3(b). Using the domain knowledge that soccer games have more matches every Saturday, which makes it a typical and heavily reported periodic event, we thus consider the value of Sf = 155.13 to be high. 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 (a) soccer(DFIDF:time) 0 50 100 150 200
P=7 S=155.13 (b) soccer(S:frequency) Figure 3: Feature soccer with relative high Sf and short-term Pf .
From the DFIDF trajectory for DBS in Figure 4(a), we can immediately deduce DBS to be an infrequent word with a trivial burst on 08/17/1997 corresponding to DBS Land Raﬄes Holdings plans. This is confirmed by the long period of Pf = 365 and low power of Sf = 0.3084 as shown in Figure 4(b). Moreover, since this aperiodic event is only reported in a few news stories over a very short time of few days, we therefore say that its low power value of Sf =
The most confusing example is shown in Figure 5 for the word feature your, which looks very similar to the graph for soccer in Figure 3. At first glance, we may be tempted to group both your and soccer into the same category of HL or LL since both distributions look similar and have the same dominant period of approximately a week. However, further 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 (a) DBS(DFIDF:time) 0
P=365 S=0.3084 (b) DBS(S:frequency) Figure 4: Feature DBS with relative low Sf and long-term Pf . analysis indicates that the periodicity of your is due to the differences in document counts for weekdays (average 2,919 per day) and weekends3 (average 479 per day). One would have expected the periodicity of a stopword like your to be a day. Moreover, despite our DFIDF normalization, the weekday/weekend imbalance still prevailed; stopwords occur 4 times more frequently on weekends than on weekdays.
Thus, the DPS remains the only distinguishing factor between your (Sf = 9.42) and soccer (Sf = 155.13). However, it is very dangerous to simply conclude that a power value of S = 9.42 corresponds to a stopword feature. 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 (a) your(DFIDF:time) 0 5 10
P=7 S=9.42 (b) your(S:frequency) Figure 5: Feature your as an example confusing with feature soccer.
Before introducing our solution to this problem, let"s look at another LL example as shown in Figure 6 for beenb, which is actually a confirmed typo. We therefore classify beenb as a noisy feature that does not contribute to any event. Clearly, the trajectory of your is very different from beenb, which means that the former has to be considered separately. 0
8/20/1996 12/8/1996 3/28/1997 7/16/1997 (a) beenb(DFIDF:time)
P=8 S=1.20E-05 (b) beenb(S:frequency) Figure 6: Feature beenb with relative low Sf and short-term Pf .
Stop Words (SW) Feature Set Based on the above analysis, we realize that there must be another feature set between HL and LL that corresponds to the set of stopwords. Features from this set has moderate DPS and low but known dominant period. Since it is hard to distinguish this feature set from HL and LL only based on DPS, we introduce another factor called average DFIDF (DFIDF). As shown in Figure 5, features like your usually have a lower DPS than a HL feature like soccer, but have a much higher DFIDF than another LL noisy feature such as beenb. Since such properties are usually characteristics of stopwords, we group features like your into the newly defined stopword (SW) feature set.
Since setting the DPS and DFIDF thresholds for identifying stopwords is more of an art than science, we proposed a heuristic HS algorithm, Algorithm 1. The basic idea is to only use news stories from weekdays to identify stopwords. 3 The weekends here also include public holidays falling on weekdays.
The SW set is initially seeded with a small set of 29 popular stopwords utilized by Google search engine.
Algorithm 1 Heuristic Stopwords detection (HS) Input: Seed SW set, weekday trajectories of all words 1: From the seed set SW, compute the maximum DPS as UDPS, maximum DFIDF as UDFIDF, and minimum of DFIDF as LDFIDF. 2: for fi ∈ F do 3: Compute DFT for fi. 4: if Sfi ≤ UDPS and DFIDFfi ∈ [LDFIDF, UDFIDF] then 5: fi → SW 6: F = F − fi 7: end if 8: end for Overview of Feature Categorization After the SW set is generated, all stopwords are removed from F. We then set the boundary between high and low DPS to be the upper bound of the SW set"s DPS. An overview of all five feature sets is shown in Figure 7.
Figure 7: The 5 feature sets for events.
Since only features from HH, HL and LH are meaningful and could potentially be representative to some events, we pruned all other feature classified as LL or SW. In this section, we describe how bursts can be identified from the remaining features. Unlike Kleinberg"s burst identification algorithm [12], we can identify both significant and trivial bursts without the need to set any parameters.
For each feature in HH and HL, we truncate its trajectory by keeping only the bursty period, which is modeled with a Gaussian distribution. For example, Figure 8 shows the word feature Iraq with a burst circa 09/06/1996 being modeled as a Gaussian. Its bursty period is defined by [μf − σf , μf + σf ] as shown in Figure 8(b).
Since we have computed the DP for a periodic feature f, we can easily model its periodic feature trajectory yf using 0
8/20/96 12/8/96 3/28/97 7/16/97 (a) original DFIDF:time 0
8/20/96 12/8/96 3/28/97 7/16/97 burst= [μ-σ, μ+σ] (b) identifying burst Figure 8: Modeling Iraq"s time series as a truncated Gaussian with μ = 09/06/1996 and σ = 6.26. a mixture of K = T/Pf Gaussians: f(yf = yf (t)|θf ) = K k=1 αk 1 2πσ2 k e − 1 2σ2 k (yf (t)−µk)2 , where the parameter set θf = {αk, μk, σk}K k=1 comprises: • αk is the probability of assigning yf into the kth  Gaussian. αk > 0, ∀k ∈ [1, K] and K k=1 αk = 1; • μk/σk is mean/standard deviation of the kth Gaussian.
The well known Expectation Maximization (EM) [8] algorithm is used to compute the mixing proportions αk, as well as the individual Gaussian density parameters μk and σK .
Each Gaussian represents one periodic event, and is modeled similarly as mentioned in Section 5.1.
After identifying and modeling bursts for all features, the next task is to paint a picture of the event with a potential set of representative features.
If two features fi and fj are representative of the same event, they must satisfy the following necessary conditions:
Measuring Feature Distribution Similarity We measure the similarity between two features fi and fj using discrete KL-divergence defined as follows.
Definition 6. (feature similarity) KL(fi, fj ) is given by max(KL(fi|fj ), KL(fj |fi)), where KL(fi|fj ) = T t=1 f(yfi (t)|θfi )log f(yfi (t)|θfi ) f(yfj (t)|θfj ) . (1) Since KL-divergence is not symmetric, we define the similarity between between fi and fj as the maximum of KL(fi|fj ) and KL(fj |fi). Further, the similarity between two aperiodic features can be computed using a closed form of the KL-divergence [16]. The same discrete KL-divergence formula of Eq. 1 is employed to compute the similarity between two periodic features,
Next, we define the overal similarity among a set of features R using the maximum inter-feature KL-Divergence value as follows.
Definition 7. (set"s similarity)KL(R) = max ∀fi,fj ∈R KL(fi, fj ).
Document Overlap Let Mi be the set of all documents containing feature fi.
Given two features fi and fj , the overlapping document set containing both features is Mi ∩ Mj . Intuitively, the higher the |Mi ∩ Mj |, the more likelty that fi and fj will be highly correlated. We define the degree of document overlap between two features fi and fj as follows.
Definition 8. (Feature DF Overlap) d(fi, fj ) = |Mi∩Mj| min(|Mi|,|Mj|) .
Accordingly, the DF Overlap among a set of features R is also defined.
Definition 9. (Set DF Overlap) d(R) = min ∀fi,fj ∈R d(fi, fj).
We use features from HH to detect important aperiodic events, features from LH to detect less-reported/unimportant aperiodic events, and features from HL to detect periodic events. All of them share the same algorithm. Given bursty feature fi ∈ HH, the goal is to find highly correlated features from HH. The set of features similar to fi can then collectively describe an event. Specifically, we need to find a subset Ri of HH that minimizes the following cost function: C(Ri) = KL(Ri) d(Ri) fj ∈Ri Sfj , Ri ⊂ HH. (2) The underlying event e (associated with the burst of fi) can be represented by Ri as y(e) = fj ∈Ri Sfj fu∈Ri Sfu yfj . (3) The burst analysis for event e is exactly the same as the feature trajectory.
The cost in Eq. 2 can be minimized using our unsupervised greedy UG event detection algorithm, which is described in Algorithm 2. The UG algorithm allows a feature Algorithm 2 Unsupervised Greedy event detection (UG).
Input: HH, document index for each feature. 1: Sort and select features in descending DPS order: Sf1 ≥ Sf2 ≥ . . . ≥ Sf|HH| . 2: k = 0. 3: for fi ∈ HH do 4: k = k + 1. 5: Init: Ri ← fi, C(Ri) = 1/Sfi and HH = HH − fi. 6: while HH not empty do 7: m = arg min m C(Ri ∪ fm). 8: if C(Ri ∪ fm) < C(Ri) then 9: Ri ← fm and HH = HH − fm. 10: else 11: break while. 12: end if 13: end while 14: Output ek as Eq. 3. 15: end for to be contained in multiple events so that we can detect several events happening at the same time. Furthermore, trivial events only containing year/month features (i.e., an event only containing 1 feature Aug could be identified over a 1year news stream) could be removed, although such events will have inherent high cost and should already be ranked very low. Note that our UG algorithm only requires one data-dependant parameter, the boundary between high and low power spectrum, to be set once, and this parameter can be easily estimated using the HS algorithm (Algorithm 1).
In this section, we study the performances of our feature categorizing method and event detection algorithm. We first introduce the dataset and experimental setup, then we subjectively evaluate the categorization of features for HH, HL,
LH, LL and SW. Finally, we study the (a)periodic event detection problem with Algorithm 2.
The Reuters Corpus contains 806,791 English news stories from 08/20/1996 to 08/19/1997 at a day resolution. Version 2 of the open source Lucene software [1] was used to tokenize the news text content and generate the document-word vector. In order to preserve the time-sensitive past/present/future tenses of verbs and the differences between lower case nouns and upper case named entities, no stemming was done. Since dynamic stopword removal is one of the functionalities of our method, no stopword was removed. We did remove nonEnglish characters, however, after which the number of word features amounts to 423,433. All experiments were implemented in Java and conducted on a 3.2 GHz Pentium 4 PC running Windows 2003 Server with 1 GB of memory.
We downloaded 34 well-known stopwords utilized by the Google search engine as our seed training features, which includes a, about, an, are, as, at, be, by, de, for, from, how, in, is, it, of, on, or, that, the, this, to, was, what, when, where, who, will, with, la, com, und, en and www. We excluded the last five stopwords as they are uncommon in news stories. By only analyzing news stories over 259 weekdays, we computed the upper bound of the power spectrum for stopwords at 11.18 and corresponding DFIDF ranges from
and 0.1182 <= DFIDFf <= 0.3691 over weekdays will be considered a stopword. In this manner, 470 stopwords were found and removed as visualized in Figure 9. Some detected stopwords are A (P = 65, S = 3.36, DFIDF = 0.3103), At (P = 259, S = 1.86, DFIDF = 0.1551), GMT (P = 130,
S = 6.16, DFIDF = 0.1628) and much (P = 22, S = 0.80,
DFIDF = 0.1865). After the removal of these stopwords, the distribution of weekday and weekend news are more or less matched, and in the ensuing experiments, we shall make use of the full corpus (weekdays and weekends).
The upper bound power spectrum value of 11.18 for stopwords training was selected as the boundary between the high power and low power spectrum. The boundary between high and low periodicity was set to 365/2 = 183.
All 422,963 (423433 − 470) word features were categorized into 4 feature sets: HH (69 features), HL (1,087 features),
LH (83,471 features), and LL (338,806 features) as shown in Figure 10. In Figure 10, each gray level denotes the relative density of features in a square region, measured by log10(1 + Dk), where Dk is the number of features within the k-th square region. From the figure, we can make the 0 2 4 6 8 11.18 12879.82 1 65 100 130 259 S(f) P(f) LH HH LL HL Figure 9: Distribution of SW (stopwords) in the HH,
HL, LH, and LL regions. 0 5 11.18 50 100 200 300 400 500 1000 12879.82 1 2 4 6 8 20 50 100 183 364 365 S(f) P(f) 0
1
2
3
4
LH HH LL HL Figure 10: Distribution of categorized features over the four quadrants (shading in log scale). following observations:
from those features having a much higher S, which allows us to detect important (a)periodic events from trivial events by selecting features with high S.
which are nicely separated (big horizontal gap) from the periodic features. This allows reliably detecting aperiodic events and periodic events independently.
spectrum is not as clearcut and the exact value will be application specific.
By checking the scatter distribution of features from SW on HH, HL, LH, and LL as shown in Figure 9, we found that
LL. The LL classification and high DFIDF scores of stopwords agree with the generally accepted notion that stopwords are equally frequent over all time. Therefore, setting the boundary between high and low power spectrum using the upper bound Sf of SW is a reasonable heuristic.
We shall evaluate our two hypotheses, 1)important aperiodic events can be defined by a set of HH features, and 2)less reported aperiodic events can be defined by a set of LH features. Since no benchmark news streams exist for event detection (TDT datasets are not proper streams), we evaluate the quality of the automatically detected events by comparing them to manually-confirmed events by searching through the corpus.
Among the 69 HH features, we detected 17 important aperiodic events as shown in Table 1 (e1 − e17). Note that the entire identification took less than 1 second, after removing events containing only the month feature. Among the 17 events, other than the overlaps between e3 and e4 (both describes the same hostage event), e11 and e16 (both about company reports), the 14 identified events are extremely accurate and correspond very well to the major events of the period. For example, the defeat of Bob Dole, election of Tony Blair, Missile attack on Iraq, etc. Recall that selecting the features for one event should minimize the cost in Eq. 2 such that 1) the number of features span different events, and 2) not all features relevant to an event will be selected, e.g., the feature Clinton is representative to e12 but since Clinton relates to many other events, its time domain signal is far different from those of other representative features like Dole and Bob. The number of documents of a detected event is roughly estimated by the number of indexed documents containing the representative features. We can see that all 17 important aperiodic events are popularly reported events.
After 742 minutes of computation time, we detected 23, 525 less reported aperiodic events from 83,471 LH features.
Table 1 lists the top 5 detected aperiodic events (e18 − e22) with respect to the cost. We found that these 5 events are actually very trivial events with only a few news reports, and are usually subsumed by some larger topics. For example, e22 is one of the rescue events in an airplane hijack topic. One advantage of our UG Algorithm for discovering less-reported aperiodic events is that we are able to precisely detect the true event period.
Among the 1,087 HL features, 330 important periodic events were detected within 10 minutes of computing time.
Table 1 lists the top 5 detected periodic events with respect to the cost (e23 − e27). All of the detected periodic events are indeed valid, and correspond to real life periodic events.
The GMM model is able to detect and estimate the bursty period nicely although it cannot distinguish the slight difference between every Monday-Friday and all weekdays as shown in e23. We also notice that e26 is actually a subset of e27 (soccer game), which is acceptable since the Sheffield league results are announced independently every weekend.
This paper took a whole new perspective of analyzing feature trajectories as time domain signals. By considering the word document frequencies in both time and frequency domains, we were able to derive many new characteristics about news streams that were previously unknown, e.g., the different distributions of stopwords during weekdays and weekends. For the first time in the area of TDT, we applied a systematic approach to automatically detect important and less-reported, periodic and aperiodic events.
The key idea of our work lies in the observations that (a)periodic events have (a)periodic representative features and (un)important events have (in)active representative features, differentiated by their power spectrums and time periods. To address the real event detection problem, a simple and effective mixture density-based approach was used to identify feature bursts and their associated bursty periods.
We also designed an unsupervised greedy algorithm to detect both aperiodic and periodic events, which was successful in detecting real events as shown in the evaluation on a real news stream.
Although we have not made any benchmark comparison against another approach, simply because there is no previous work in the addressed problem. Future work includes evaluating the recall of detected events for a labeled news stream, and comparing our model against the closest equivalent methods, which currently are limited to the methods of Kleinberg [12] (which can only detect certain type of bursty events depending on parameter settings), Fung et al. [9], and Swan and Allan [18]. Nevertheless, we believe our simple and effective method will be useful for all TDT practitioners, and will be especially useful for the initial exploratory analysis of news streams.
[1] Apache lucene-core 2.0.0, http://lucene.apache.org. [2] Google news alerts, http://www.google.com/alerts. [3] Reuters corpus, http://www.reuters.com/researchandstandards/corpus/. [4] J. Allan. Topic Detection and Tracking. Event-based Information Organization. Kluwer Academic Publishers, 2002. [5] J. Allan, V. Lavrenko, and H. Jin. First story detection in tdt is hard. In CIKM, pages 374-381, 2000. [6] J. Allan, C. Wade, and A. Bolivar. Retrieval and novelty detection at the sentence level. In SIGIR, pages 314-321, 2003. [7] T. Brants, F. Chen, and A. Farahat. A system for new event detection. In SIGIR, pages 330-337, 2003. [8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1-38, 1977. [9] G. P. C. Fung, J. X. Yu, P. S. Yu, and H. Lu. Parameter free bursty events detection in text streams. In VLDB, pages 181-192, 2005. [10] Q. He, K. Chang, and E.-P. Lim. A model for anticipatory event detection. In ER, pages 168-181, 2006. [11] Q. He, K. Chang, E.-P. Lim, and J. Zhang. Bursty feature reprensentation for clustering text streams. In SDM, accepted,
[12] J. Kleinberg. Bursty and hierarchical structure in streams. In SIGKDD, pages 91-101, 2002. [13] R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. On the bursty evolution of blogspace. In WWW, pages 159-178, 2005. [14] G. Kumaran and J. Allan. Text classification and named entities for new event detection. In SIGIR, pages 297-304,
[15] Q. Mei and C. Zhai. Discovering evolutionary theme patterns from text: an exploration of temporal text mining. In SIGKDD, pages 198-207, 2005. [16] W. D. Penny. Kullback-liebler divergences of normal, gamma, dirichlet and wishart densities. Technical report, 2001. [17] N. Stokes and J. Carthy. Combining semantic and syntactic document classifiers to improve first story detection. In SIGIR, pages 424-425, 2001. [18] R. Swan and J. Allan. Automatic generation of overview timelines. In SIGIR, pages 49-56, 2000. [19] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos.
Identifying similarities, periodicities and bursts for online search queries. In SIGMOD, pages 131-142, 2004. [20] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In SIGIR, pages 28-36, 1998. [21] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In SIGKDD, pages 688-693, 2002.
Table 1: All important aperiodic events (e1 − e17), top 5 less-reported aperiodic events (e18 − e22) and top 5 important periodic events (e23 − e27).
Detected Event and Bursty Period Doc # True Event e1(Sali,Berisha,Albania,Albanian,March) 02/02/199705/29/1997 1409 Albanian"s president Sali Berisha lost in an early election and resigned, 12/1996-07/1997. e2(Seko,Mobutu,Sese,Kabila) 03/22/1997-06/09/1997 2273 Zaire"s president Mobutu Sese coordinated the native rebellion and failed on 05/16/1997. e3(Marxist,Peruvian) 11/19/1996-03/05/1997 824 Peru rebels (Tupac Amaru revolutionary Movement) led a hostage siege in Lima in early 1997. e4(Movement,Tupac,Amaru,Lima,hostage,hostages) 11/16/1996-03/20/1997 824 The same as e3. e5(Kinshasa,Kabila,Laurent,Congo) 03/26/199706/15/1997 1378 Zaire was renamed the Democratic Republic of Congo on 05/16/1997. e6(Jospin,Lionel,June) 05/10/1997-07/09/1997 605 Following the early General Elections circa 06/1997,
Lionel Jospin was appointed Prime Minister on 06/02/1997. e7(Iraq,missile) 08/31/1996-09/13/1996 1262 U.S. fired missile at Iraq on 09/03/1996 and 09/04/1996. e8(Kurdish,Baghdad,Iraqi) 08/29/1996-09/09/1996 1132 Iraqi troop fought with Kurdish faction circa 09/1996. e9(May,Blair) 03/24/1997-07/04/1997 1049 Tony Blair became the Primary Minister of the United Kingdom on 05/02/1997. e10(slalom,skiing) 12/05/1996-03/21/1997 253 Slalom Game of Alpine Skiing in 01/1997-02/1997. e11(Interim,months) 09/24/1996-12/31/1996 3063 Tokyo released company interim results for the past several months in 09/1996-12/1996. e12(Dole,Bob) 09/09/1996-11/24/1996 1599 Dole Bob lost the 1996 US presidential election. e13(July,Sen) 06/25/1997-06/25/1997 344 Cambodia"s Prime Minister Hun Sen launched a bloody military coup in 07/1997. e14(Hebron) 10/15/1996-02/14/1997 2098 Hebron was divided into two sectors in early 1997. e15(April,Easter) 02/23/1997-05/04/1997 480 Easter feasts circa 04/1997 (for western and Orthodox). e16(Diluted,Group) 04/27/1997-07/20/1997 1888 Tokyo released all 96/97 group results in 04/199707/1997. e17(December,Christmas) 11/17/1996-01/26/1997 1326 Christmas feast in late 12/1997. e18(Kolaceva,winter,Together,promenades,Zajedno,
Slobodan,Belgrade,Serbian,Serbia,Draskovic,municipal,
Kragujevac) 1/25/1997 3 University students organized a vigil on Kolaceva street against government on 1/25/1997. e19(Tutsi,Luvengi,Burundi,Uvira,fuel,Banyamulenge,
Burundian,Kivu,Kiliba,Runingo,Kagunga,Bwegera) 10/19/1996 6 Fresh fighting erupted around Uvira between Zaire armed forces and Banyamulengs Tutsi rebels on 10/19/1996. e20(Malantacchi,Korea,Guy,Rider,Unions,labour,
Trade,unions,Confederation,rammed,Geneva,stoppages,
Virgin,hire,Myongdong,Metalworkers) 1/11/1997 2 Marcello Malantacchi secretary general of the International Metalworkers Federation and Guy Rider who heads the Geneva office of the International Confederation of Free Trade Unions attacked the new labour law of South Korea on 1/11/1997. e21(DBS,Raﬄes) 8/17/1997 9 The list of the unit of Singapore DBS Land Raﬄes Holdings plans on 8/17/1997. e22(preserver,fuel,Galawa,Huddle,Leul,Beausse) 11/24/1996 3 Rescued a woman and her baby during a hijacked Ethiopian plane that ran out of fuel and crashed into the sea near Le Galawa beach on 11/24/1996. e23(PRICE,LISTING,MLN,MATURITY,COUPON,
MOODY,AMT,FIRST,ISS,TYPE,PAY,BORROWER) Monday-Friday/week 7966 Announce bond price on all weekdays. e24(Unaudited,Ended,Months,Weighted,Provision,Cost,
Selling,Revenues,Loss,Income,except,Shrs,Revs) every season 2264 Net income-loss reports released by companies in every season. e25(rating,Wall,Street,Ian) Monday-Friday/week 21767 Stock reports from Wall Street on all weekdays. e26(Sheffield,league,scoring,goals,striker,games) every Friday, Saturday and Sunday 574 Match results of Sheffield soccer league were published on Friday, Saturday and Sunday 10 times than other 4 days. e27(soccer,matches,Results,season,game,Cup,match, victory,beat,played,play,division) every Friday,

The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are inherently ambiguous. The query canon book for example covers several different areas of interest: religion, photography, literature, and music. Clearly, one would prefer search output to be aligned with user"s topic(s) of interest, rather than displaying a selection of popular URLs from each category. Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.
Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]). This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the user"s Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc. Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably). First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user. Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy. Search engines should not be able to know about a person"s interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).
Our algorithms expand Web queries with keywords extracted from user"s PIR, thus implicitly personalizing the search output.
After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.
We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best. In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri. The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%. In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. This yields an additional improvement of 8.47% over the previously identified best algorithm. We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs. However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side.
This paper brings together two IR areas: Search Personalization and Automatic Query Expansion. There exists a vast amount of algorithms for both domains. However, not much has been done specifically aimed at combining them. In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms.
Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm. This section splits the relevant background according to the focus of each article into either one of these elements.
Approaches focused on the User Profile. Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages. Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile. Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13]. User profiling based on browsing history has the advantage of being rather easy to obtain and process. This is probably why it is also employed by several industrial search engines (e.g., Yahoo! MyWeb2 ).
However, it is definitely not sufficient for gathering a thorough insight into user"s interests. More, it requires to store all personal information at the server side, which raises significant privacy concerns.
Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing user"s interests. Moreover, none of these investigated the adaptive application of personalization.
Approaches focused on the Personalization Algorithm.
Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently. Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics. More recently,
Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings. Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination. As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores.
Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval. It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output. In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.
Some other approaches are also addressed in the end of the section.
Relevance Feedback Techniques. The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query. First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.
Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc. We used some of these as inspiration for our Desktop specific techniques. Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary. RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF). Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query. Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection. Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.
Co-occurrence Based Techniques. Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17]. Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc. We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.
Thesaurus Based Techniques. A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords. Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined. Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36]. Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22]. We also use WordNet based expansion terms. However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.
Other Techniques. There are many other attempts to extract expansion terms. Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.
Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords.
DESKTOP DATA Desktop data represents a very rich repository of profiling information. However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics. In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit user"s PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository. Then, in the second part of the section we empirically analyze the performance of each approach.
This section presents the five generic approaches for analyzing user"s Desktop data in order to provide expansion terms for Web search. In the proposed algorithms we gradually increase the amount of personal information utilized. Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching user"s query best. We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents. In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process.
Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for user"s Web query, rather than from the top ranked Web search results. We distinguish three granularity levels for this process and we investigate each of them separately.
Term and Document Frequency. As the simplest possible measures, TF and DF have the advantage of being very fast to compute.
Previous experiments with small data sets have showed them to yield very good results [11]. We thus independently associate a score with each term, based on each of the two statistics. The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document. This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10]. The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching user"s Web query.
The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request. This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise. Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with. Ties are resolved using the corresponding TF scores.
Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web. For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF. However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.
Lexical Compounds. Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expression"s lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set. Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1]. We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository. Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.
Sentence Selection. This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output. Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences). Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line. We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein. A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details). The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items. This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning. However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant. The final term biases the summary towards the query. It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query. It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query.
In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms. In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.
Term Co-occurrence Statistics. For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query. Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1. Co-occurrence based keyword similarity search.
Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.
The off-line computation needs an initial trimming phase (step 1) for optimization purposes. In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably. During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query. Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b). We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.
DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y. To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms. We set W to be the same as the maximum amount of expansion keywords desired.
Dunning"s Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 . It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other. This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present. Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together. We compare the two binomial processes by using likelihood ratios of their associated hypotheses. First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ . Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k . Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion. Large scale thesauri encapsulate global knowledge about term relationships. Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request. In the end, those suggestions with the highest frequencies are kept. The algorithm is as follows: Algorithm 3.1.2.2. Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.
We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).
As they represent quite different types of association, we investigated them separately. We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs). We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs.
We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education). First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache. Without loss of generality, we focused the experiments on single-user machines. Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001. In order to connect such a query to each user"s interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subject"s Desktop. To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.
The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones. Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations. The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances. Note that the former ones were somewhat farther away from each subject"s interest, thus being also more difficult to personalize on. To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest. The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.
For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1. These results were then shuffled into one set containing usually between 70 and 90 URLs. Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL. Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment. For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant. Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15]. DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.
We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones. As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries. All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.
Algorithmic specific aspects. The main parameter of our algorithms is the number of generated expansion keywords. For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation. In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four). For all algorithms we also investigated bigger limitations. This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected. We therefore chose to experiment with this new approach as well. For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain. We labeled the algorithms we evaluated as follows:
Google API;
one top compound per document) Lexical Compounds;
using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients;
expansion with synonyms, sub-concepts, and super-concepts, respectively.
Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent user"s personal information. This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34]. However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis.
Log Queries. We evaluated all variants of our algorithms using NDCG. For log queries, the best performance was achieved with TF, LC[O], and TC[LR]. The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O]. A summary of all results is depicted in Table 1.
Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task. LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04. Thus, a selection of compounds spanning over several Desktop documents is more informative about user"s interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.
The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR]. Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects. We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering). As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific. NDCG Signific.
Top vs. Google Random vs. Google Google 0.42 -
DF 0.17 -
SS 0.33 -
WN[SYN] 0.42 -
5 results when searching for top (left) and random (right) log queries.
Algorithm NDCG Signific. NDCG Signific.
Clear vs. Google Ambiguous vs. Google Google 0.71 -
DF 0.37 -
LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 -
5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions. Finally, we noticed Google to be very optimized for some top frequent queries.
However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).
Self-selected Queries. The NDCG values obtained with selfselected queries are depicted in Table 2. While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries. In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.
In general, the relative differences between our algorithms were similar to those observed for the log based queries. As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best. Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics. There were no visible differences between the behavior of the three different approaches to cooccurrence calculation. Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements. We thus pursued this idea with the adaptive algorithms presented in the next section.
In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query. However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it. In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process. Then, in the second part we present some initial experiments with one of them, namely query clarity.
Several indicators could assist the algorithm to automatically tune the number of expansion terms. We start by discussing adaptation by analyzing the query clarity level. Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.
Query Clarity. The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic. Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario. Also, the success of IR systems clearly varies across different topics. We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.
The following metrics are available: • The Query Length is expressed simply by the number of words in the user query. The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far. It measures the divergence between the language model associated to the user query and the language model associated to the collection. In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.
Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications. We thus decided to investigate only C1 and C2. First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].
In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web. As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis. As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case. Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.
Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.
Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within user"s PIR. Note that the ambiguity level is related to the number of documents covering a certain query. Thus, to some extent, it has different meanings on the Web and within PIRs. While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop. Take for example the query PageRank. If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous. However, when analyzed against the Web, this is definitely a clear query. Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop. Put another way, queries deemed clear on the Desktop were inherently not well covered within user"s PIR, and thus had fewer keywords appended to them. The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.
Query Formulation Process. Interactive query expansion has a high potential for enhancing search [29]. We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms. For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request. Thus, the newly added terms are more likely to convey information about her search goals. For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords. Within our personalized scenario, the generated expansions can similarly be biased towards these terms. Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.
Other Features. The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.
Only some approaches have been investigated, usually indirectly.
There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc. However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization.
We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm. The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.
The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4). For top frequent queries,
Algorithm NDCG Signific. NDCG Signific.
Top vs. Google Random vs. Google Google 0.51 -
LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 -
A[LCO/WN] 0.55 p = 0.01
results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.
Algorithm NDCG Signific. NDCG Signific.
Clear vs. Google Ambiguous vs. Google Google 0.81 -
LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 -
A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01. They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07). For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms. The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.
Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.
The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5). For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01. Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05). Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.
All results are depicted graphically in Figure 1. We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected. The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization.
In this paper we proposed to expand Web search queries by exploiting the user"s Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to user"s interests, personalizing the search output. In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents. Each of them produces additional query keywords by analyzing user"s Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.
Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios. We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.
We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms. We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data. Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms.
We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented. We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.
Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705).

Topic Detection and Tracking (TDT) program aims to develop techniques which can effectively organize, search and structure news text materials from a variety of newswire and broadcast media [1]. New Event Detection (NED) is one of the five tasks in TDT. It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents. A Topic is defined as a seminal event or activity, along with directly related events and activities [2]. An Event is defined as something (non-trivial) happening in a certain place at a certain time [3]. For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same topic would be those discussing salvaging efforts, the search for perpetrators, arrests and trial and so on. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering.
In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. They are usually in the form of cosine similarity or Hellinger similarity metric. The core problem of NED is to identify whether two stories are on the same topic.
Obviously, these systems cannot take advantage of topic information. Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process. Other systems organize previous stories into clusters (each cluster corresponds to a topic), and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. Nevertheless, it has been proved that this manner is less accurate [4, 5]. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic.
On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13].
However, none of the systems have considered that terms of different types (e.g. Noun, Verb or Person name) have different effects for different classes of stories in determining whether two stories are on the same topic. For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class.
So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story representation by better understanding of named entities.
Driven by these problems, we have proposed three approaches in this paper. (1)To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity. Comparisons between current story and previous clusters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster (topic) information is used properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from training data, we found that terms of different types (e.g. Noun and Verb) have different effects for different classes of stories in determining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. On TDT3 dataset, the new NED model just uses 14.9% comparing times of the basic model, while its minimum normalized cost is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13].
The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2.
Section 3 presents the basic model for NED that most current systems use. Section 4 describes our new detection procedure based on news indexing-tree. In section 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evaluation metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8.
Papka et al. proposed Single-Pass clustering on NED [6]. When a new story was encountered, it was processed immediately to extract term features and a query representation of the story"s content is built up. Then it was compared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7]. In this manner comparisons happen between stories and clusters.
Recent years, most work focus on proposing better methods on comparison of stories and document representation. Brants et al. [8] extended a basic incremental TF-IDF model to include sourcespecific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents. Good improvements on TDT bench-marks were shown. Stokes et al. [9] utilized a combination of evidence from two distinct representations of a document"s content. One of the representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector. Then the two representations are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used.
Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities [10]. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12].
UMass [13] research group split document representation into two parts: named entities and non-named entities. And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both [10] and [13] used text categorization technique to classify news stories in advance. In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In [10] frequent terms for each class are removed from document representation. For example, word election does not help identify different elections. In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated. We use statistical analysis to reveal the fact and use it to improve NED performance.
In this section, we present the basic New Event Detection model which is similar to what most current systems apply. Then, we propose our new model by extending the basic model.
New Event Detection systems use news story stream as input, in which stories are strictly time-ordered. Only previously received stories are available when dealing with current story. The output is a decision for whether the current story is on a new event or not and the confidence of the decision. Usually, a NED model consists of three parts: story representation, similarity calculation and detection procedure.
Preprocessing is needed before generating story representation.
For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story.
We use incremental TF-IDF model for term weight calculation [4]. In a TF-IDF model, term frequency in a news document is weighted by the inverse document frequency, which is generated from training corpus. When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1). The new term receives too low weight in the first solution (0) and too high weight in the second solution. In incremental TF-IDF model, document frequencies are updated dynamically in each time step t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) where Dt represents news story set received in time t, and dfDt(w) means the number of documents that term w occurs in, and dft(w) means the total number of documents that term w occurs in before time t. In this work, each time window includes 50 news stories.
Thus, each story d received in t is represented as follows: 1 2{ ( , , ), ( , , ),..., ( , , )}nd weight d t w weight d t w weight d t w→ where n means the number of distinct terms in story d, and ( , , )weight d t w means the weight of term w in story d at time t: ' log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ') 1) log(( 1) /( ( ') 0.5)) t t t t w d tf d w N df w weight d t w tf d w N df w ∈ + + + = + + +∑ (2) where Nt means the total number of news stories before time t, and tf(d,w) means how many times term w occurs in news story d.
We use Hellinger distance for the calculation of similarity between two stories, for two stories d and d" at time t, their similarity is defined as follows: , ' ( , ', ) ( , , ) * ( ', , ) w d d sim d d t weight d t w weight d t w ∈ = ∑ (3)
For each story d received in time step t, the value ( ') ( ) ( ) ( ( , ', )) time d time d n d max sim d d t < = (4) is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our decision [8]. time(d) means the publication time of story d. If the score exceeds the thresholdθ new, then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story.
Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story. If the highest similarity exceeds thresholdθ new, then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster. Previous work show that the first manner is more accurate than the second one [4][5]. Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic. So using similarities between stories for determining new story is better than using similarities between story and clusters. Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient. We propose a new detection procedure which uses comparisons with previous clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story.
Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods.
The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters. We index similar stories together by their common ancestor (a cluster node). Dissimilar stories are indexed in different clusters. When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision. After the new event decision is made, the current story is inserted to the indexing-tree for the following detection.
The news indexing-tree is defined formally as follows: S-Tree = {r, NC , NS , E} where r is the root of S-Tree, NC is the set of all cluster nodes, NS is the set of all story nodes, and E is the set of all edges in S-Tree.
We define a set of constraints for a S-Tree: ⅰ . , is an non-terminal node in the treeC i i N i∀ ∈ → ⅱ . , is a terminal node in the treeS i i N i∀ ∈ → ⅲ . , out degree of is at least 2C i i N i∀ ∈ → ⅳ . , is represented as the centroid of its desendantsC i i iN∀ ∈ → For a news story di, the comparison procedure and inserting procedure based on indexing-tree are defined as follows. An example is shown by Figure 1 and Figure 2.
Figure 1. Comparison procedure Figure 2. Inserting procedure Comparison procedure: Step 1: compare di to all the direct child nodes of r and select λ nodes with highest similarities, e.g., C1 2 and C1 3 in Figure 1.
Step 2: for each selected node in the last step, e.g. C1 2, compare di to all its direct child nodes, and select λ nodes with highest similarities, e.g. C2 2 and d8. Repeat step 2 for all non-terminal nodes.
Step 3: record the terminal node with the highest similarty to di, e.g. s5, and the similarity value (0.20).
Inserting di to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s, e.g. C1
than θ init+(h-1)δ , then add di to the tree as a direct child of r.
Otherwise, if n is a terminal node, then create a cluster node instead of n, and add both n and di as its direct children; if n is an non-terminal node, then repeat this procedure and insert di to the sub-tree with n as root recursively. Here h is the length between n and the root of S-tree.
The more the stories in a cluster similar to each other, the better the cluster represents the stories in it. Hence we add no constraints on the maximum of tree"s height and degree of a node. Therefore, we cannot give the complexity of this indexing-tree based procedure. But we will give the number of comparing times needed by the new procedure in our experiments in section7.
In this section, two term reweighting methods are proposed to improve NED accuracy. In the first method, a new way is explored for better using of cluster (topic) information. The second one finds a better way to make use of named entities based on news classification.
Distance TF-IDF is the most prevalent model used in information retrieval systems. The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term). Nevertheless, in TDT domain, we need to discriminate documents with regard to topics rather than queries. Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using story vectors. Unfortunately, the experimental results do not support this intuition [4][5]. Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other. Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about criminal trial, therefore stories about trial would have low similarity with the topic vector built on its previous events. This section focuses on how to effectively make use of topic information and at the same time avoid the problem of content decentralization.
At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g., year and people. Terms of this class should be given low weights because they do not help much for topic discrimination.
Term class B: terms that occur frequently within a news category, e.g., election, storm. They are useful to distinguish two stories in different news categories. However, they cannot provide information to determine whether two stories are on the same or different topics. In another words, term election and term storm are not helpful in differentiate two election campaigns and two storm disasters. Therefore, terms of this class should be assigned lower weights.
Term class C: terms that occur frequently in a topic, and infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane. News stories that belong to different topics rarely have overlap terms in this class. The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight.
Term class D: terms that appear in a topic exclusively, but not frequently. For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topics. Terms of this type should receive more weights than in TF-IDF model. However, since they are not popular in the topic, it is not appropriate to give them too high weights.
Term class E: terms with low document frequency, and appear in different topics. Terms of this class should receive lower weights.
Now we analyze whether TF-IDF model can give proper weights to the five classes of terms. Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above. In TF-IDF model, terms of class B are highly dependant with the number of stories in a news class.
TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class. For a term of class C, the more frequently it appears in a topic, the less weight TFIDF model gives to it. This strongly conflicts with the requirement of terms in class C. For terms of class D, TF-IDF model gives them high weights correctly. But for terms of class E, TF-IDF model gives high weights to them which are not conformable with the requirement of low weights. To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model. So, we propose a modified model to resolve this problem.
When θ init andθ new are set closely, we assume that most of the stories in a first-level cluster (a direct child node of root node) are on the same topic. Therefore, we make use of a first-level cluster to capture term distribution (df for all the terms within the cluster) within the topic dynamically. KL divergence of term distribution in a first-level cluster and the whole story set is used to adjust term weights: ' ' ' ( , , ) * (1 * ( || )) ( , , ) ( , , ') * (1 * ( || )) cw tw cw tw w d D weight d t w KL P P weight d t w weight d t w KL P P γ γ ∈ + = +∑ (5) where ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) where dfc(w) is the number of documents containing term w within cluster C, and Nc is the number of documents in cluster C, and Nt is the total number of documents that arrive before time step t. γ is a const parameter, now is manually set 3.
KL divergence is defined as follows [17]: ( ) ( || ) ( ) log ( )x p x KL P Q p x q x = ∑ (8) The basic idea is: for a story in a topic, the more a term occurs within the topic, and the less it occurs in other topics, it should be assigned higher weights. Obviously, modified model can meet all the requirements of the five term classes listed above.
and Story Class Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities. But we find that terms of different types should be given different amount of extra weight for different classes of news stories.
We use open-NLP1 to recognize named entity types and part-ofspeech tags for terms that appear in news stories. Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD). Statistical analysis shows topic-level discriminative terms types for different classes of stories. For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections.
Determining whether two stories are about the same topic is a basic component for NED task. So at first we use 2 χ statistic to compute correlations between terms and topics. For a term t and a topic T, a contingence table is derived: Table 1. A 2×2 Contingence Table Doc Number belong to topic T not belong to topic T include t A B not include t C D The 2 χ statistic for a specific term t with respect to topic T is defined to be [16]: 2 2 ( , ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D χ = + + + − + + + + (9) News topics for the TDT task are further classified into 11 rules of interpretations (ROIs) 2 . The ROI can be seen as a higher level class of stories. The average correlation between a term type and a topic ROI is computed as: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) where K is the number of term types (set 12 constantly in the paper). M is the number news classes (ROIs, set 11 in the paper).
Pk represents the set of all terms of type k, and Rm represents the set of all topics of class m, p(t,T) means the probability that t occurs in topic T. Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the average correlation values between them.
The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are already normalized for convenience in comparison.) The statistics in table 2 indicates the usefulness of different term types in topic discrimination with respect to different news classes. We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War,
Finances. And for three other categories Elections, Legal/Criminal Cases, Science and Discovery, person name is the most discriminative term type. For Scandals/Hearings, date is the most important information for topic discrimination. In addition,
Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Science and Discovery have higher correlation with percentage terms. Non-name terms are more stable for different classes. 1 . http://opennlp.sourceforge.net/ 2 . http://projects.ldc.upenn.edu/TDT3/Guide/label.html From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to. New term weights are reweighted as follows: ( ) ( ) ( ) ( ') ' ( , , ) * ( , , ) ( , , ) *' class d D type w T class d D type w w d weight d t w weight d t w weight d t w α α ∈ = ∑ (11) where type(w) represents the type of term w, and class(d) represents the class of story d, c kα is reweighting parameter for news class c and term type k. In the work, we just simply use statistics in table 2 as the reweighting parameters. Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters. We will try to use machine learning techniques to obtain the best parameters in the future work.
In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs. BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data. We use term weight generated using TF-IDF model as feature for story classification.
We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3. Classification results are used for term reweighting in formula (11). Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here. Thus we do not discuss the effects of classification accuracy to NED performance in the paper.
We used two LDC [18] datasets TDT2 and TDT3 for our experiments. TDT2 contains news stories from January to June
Associated Press, CNN, New York Times, Public Radio International, Voice of America etc. Only English stories in the collection were considered. TDT3 contains approximately 31,000 English stories collected from October to December 1998. In addition to the sources used in TDT2, it also contains stories from NBC and MSNBC TV broadcasts. We used transcribed versions of the TV and radio broadcasts besides textual news.
TDT2 dataset is labeled with about 100 topics, and approximately 12,000 English stories belong to at least one of these topics. TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics. All the topics are classified into 11 Rules of Interpretation: (1)Elections, (2)Scandals/Hearings, (3)Legal/Criminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoing Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC. News.
TDT uses a cost function CDet that combines the probabilities of missing a new story and a false alarm [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Table 2. Average correlation between term types and news classes where CMiss means the cost of missing a new story, PMiss means the probability of missing a new story, and PTarget means the probability of seeing a new story in the data; CFA means the cost of a false alarm, PFA means the probability of a false alarm, and PNontarget means the probability of seeing an old story. The cost CDet is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) New event detection system gives two outputs for each story. The first part is yes or no indicating whether the story triggers a new event or not. The second part is a score indicating confidence of the first decision. Confidence scores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities.
Minimum normalized cost can be determined if optimal threshold on the score were chosen.
To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as baseline. It is implemented based on the basic model described in section 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity. Similarity score normalization is also employed [8]. S-S detection procedure is used.
System-2: this system is the same as system-1 except that S-C detection procedure is used.
System-3: this system is the same as system-1 except that it uses the new detection procedure which is based on indexing-tree.
System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories. The new detection procedure is used.
System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters. The new detection procedure is used.
The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively. And employ Support Vector Machine to predict new or old using the similarity values as features.
System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc.
System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class.
Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively. Since no heldout data set for fine-tuning the threshold θ new was available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3. System-5 outperforms all other systems including system-6, and it performs only
Table 3. NED results on TDT2 Systems Min Norm(CDet) Cmp times System-1 0.5749 2.08e+9 System-2① 0.6673 3.77e+8 System-3② 0.5765 2.81e+8 System-4② 0.5431 2.99e+8 System-5② 0.5089 2.78e+8 System-6 0.5300 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TDT2 data set for all systems.
System-2 reduces comparing times to 1.29e+9 which is just
minimum normalized cost which is 0.0499 higher than system-1.
System-3 uses the new detection procedure based on news indexing-tree. It requires even less comparing times than system-2.
This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined Location Person Date Organization Money Percentage NN JJ CD Elections 0.37 1 0.04 0.58 0.08 0.03 0.32 0.13 0.1 Scandals/Hearings 0.66 0.62 0.28 1 0.11 0.02 0.27 0.13 0.05 Legal/Criminal Cases 0.48 1 0.02 0.62 0.15 0 0.22 0.24 0.09 Natural Disasters 1 0.27 0 0.04 0.04 0 0.25 0.04 0.02 Violence or War 1 0.36 0.02 0.14 0.02 0.04 0.21 0.11 0.02 Science and Discovery 0.11 1 0.01 0.22 0.08 0.12 0.19 0.08 0.03 Finances 1 0.45 0.04 0.98 0.13 0.02 0.29 0.06 0.05 Sports 0.16 0.27 0.01 1 0.02 0 0.11 0.03 0.01 together in system-3. And system-3 is basically equivalent to system-1 in accuracy results. System-4 adjusts term weights based on the distance of term distributions between the whole corpus and cluster story set, yielding a good improvement by 0.0468 compared to system-1. The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13]. Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1.
Table 4. NED results on TDT3 Systems Norm(CDet) Min Norm(CDet) Cmp times System-1 0.6159 0.5809 7.04e+8 System-2① 0.6493 0.6308 1.29e+8 System-3② 0.6197 0.5868 1.03e+8 System-4② 0.5601 0.5341 1.03e+8 System-5② 0.5413 0.5012 1.05e+8 System-7 -- 0.5783 -System-8 -- 0.5229 -① θ new=0.13 ② θ init=0.13, λ =3,δ =0.15 Figure5 shows the five DET curves for our systems on data set TDT3. System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310. We can observe that System4 and System-5 obtain lower miss probability at regions of low false alarm probabilities. The hypothesis is that, more weight value is transferred to key terms of topics from non-key terms.
Similarity score between two stories belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics.
detection Figure 3 shows the minimum normalized costs obtained by system-3 on TDT3 using different parameters. Theθ init parameter is tested on six values spanning from 0.03 to 0.18. And the λ parameter is tested on four values 1, 2, 3 and 4. We can see that, whenθ init is set to 0.12, which is the closest one toθ new, the costs are lower than others. This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to represent the stories in it. When parameter λ is set to 3 or 4, the costs are better than other cases, but there is no much difference between 3 and 4. 0
1 2 3 4
1 θ-initλ MinCost
Figure 3. Min Cost on TDT3 (δ =0.15) 0
1 2 3 4 0
1
2
x 10 8 θ-init λ Comparingtimes
1
2 x 10 8 Figure 4. Comparing times on TDT3 (δ =0.15) Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3. The comparing times are strongly dependent onθ init. Because the greaterθ init is, the less stories combined together, the more comparing times are needed for new event decision.
So we useθ init =0.13,λ =3,δ =0.15 for system-3, 4, and 5. In this parameter setting, we can get both low minimum normalized costs and less comparing times.
We have proposed a news indexing-tree based detection procedure in our model. It reduces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensions to the basic TF-IDF model.
The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set.
And the second extension to basic TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories. Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy.
We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months).
For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.
Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 90604025. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
[1] http://www.nist.gov/speech/tests/tdt/index.htm [2] In Topic Detection and Tracking. Event-based Information Organization. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 False Alarm Probability (in %) MissProbability(in%) SYSTEM1 Topic Weighted Curve SYSTEM1 Min Norm(Cost) SYSTEM2 Topic Weighted Curve SYSTEM2 Min Norm(Cost) SYSTEM3 Topic Weighted Curve SYSTEM3 Min Norm(Cost) SYSTEM4 Topic Weighted Curve SYSTEM4 Min Norm(Cost) SYSTEM5 Topic Weighted Curve SYSTEM5 Min Norm(Cost) Random Performance Figure 5. DET curves on TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald, and X. Liu. Learning Approaches for Detecting and Tracking News Events. In IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval, volume 14 (4), 1999, 32-43. [4] Y. Yang, T. Pierce, and J. Carbonell. A Study on Retrospective and On-line Event Detection. In Proceedings of SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin, and R. Swan. Detections,
Bounds, and Timelines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna,
VA, 2000, 167-174. [6] R. Papka and J. Allan. On-line New Event Detection Using Single Pass Clustering TITLE2:. Technical Report UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong, and J. Yen. Using Contextual Analysis for News Event Detection. International Journal on Intelligent Systems, 2001, 525-546. [8] B. Thorsten, C. Francine, and F. Ayman. A System for New Event Detection. In Proceedings of the 26th Annual International ACM SIGIR Conference, New York, NY, USA.
ACM Press. 2003, 330-337. [9] S. Nicola and C. Joe. Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection. In Proceedings of the 24th Annual International ACM SIGIR Conference, New York, NY, USA. ACM Press. 2001,
[10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.
Topicconditioned Novelty Detection. In Proceedings of the 8th ACM SIGKDD International Conference, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena, and S. Marko. Applying Semantic Classes in Event Detection and Tracking. In Proceedings of International Conference on Natural Language Processing (ICON 2002), 2002, pages 175-183. [12] M. Juha, A.M. Helena, and S. Marko. Simple Semantics in Topic Detection and Tracking. Information Retrieval, 7(3-4): 2004, 347-368. [13] K. Giridhar and J. Allan. Text Classification and Named Entities for New Event Detection. In Proceedings of the 27th Annual International ACM SIGIR Conference, New York,
NY, USA. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, and S. M. Harding. The INQUERY Retrieval System. In Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications, 1992, 78-83. [15] R. Krovetz. Viewing Morphology as An Inference Process.
In Proceedings of ACM SIGIR93, 1993, 61-81. [16] Y. Yang and J. Pedersen. A Comparative Study on Feature Selection in Text Categorization. In J. D. H. Fisher, editor,

In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.
Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.
Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.
At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.
In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.
Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.
Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages.
Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.
Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.
Another important aspect of our work lies in the choice of queries. The volume of queries in today"s search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy.
Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.
Early studies in query interpretation focused on query augmentation through external dictionaries [22]. More recent studies [18, 21] also attempted to gather some additional knowledge from the Web. However, these studies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11]. They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].
The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.
The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5].
Our methodology has two main phases. In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries. However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified. In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification.
In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1). Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.
Given a taxonomy of this size, the computational efficiency of classification is a major issue. Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples. Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers. A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work.
Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. Let"s assume that there is a set of documents D = d1 . . . dm indexed by a search engine. The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.
Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q). Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).
We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query. This is the case for the majority of queries that are unambiguous. Counter examples are queries like "jaguar" (animal and car brand) or "apple" (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words. In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds. Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q). The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1). While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query. This issue is further explored in the next section.
In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q. This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q. In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.
This relevance function is an adaptation of the traditional word-based retrieval rules. For example, we may let categories be the words in the vocabulary. We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.
With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.
If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).
That is, the ads are ranked according to P(q|a). This relevance model has been employed in various statistical language modeling techniques for information retrieval. The intuition can be described as follows. We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj. For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.
It should be mentioned that in our case, each query and ad can have multiple categories. For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj. We use P(Cj|q) to denote the probability of q belonging to category Cj. Here the sum Cj ∈C P(Cj|q) may not equal to one. We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known). Thus, we only need to obtain estimates of P(Cj|q) for each query q.
Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q. In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search. That is, top results ranked by search engines should also be ranked high by this formula. Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.
Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)). Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads. Nor does it affect query classification with appropriately chosen thresholds. In what follows, we consider two methods to compute the classification information P(Cj|q).
We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter. The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).
Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale. In the experiment, we will simply take uniform weights wi. A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.
In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.
For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine. Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques.
We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data. That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K.
In this setting, the classification scoring rule for a document di(q) is linear. Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method. In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level
In this section, we evaluate our methodology that uses Web search results for improving query classification.
Our choice of taxonomy was guided by a Web advertising application. Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.
Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Figure 1 shows the distribution of categories by taxonomy levels. Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category.
To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.
Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query. All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency. These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].
However, many searches do not explicitly use phrases that someone bids on. Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.
These transformations are based on rules and dictionaries.
As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue.
We used two representative sets of 1000 queries. Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).
The first set of queries can be matched to at least one ad using broad match as described above. Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them. In a sense, these are even more rare queries and further away from common queries.
As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.
The queries in the two sets differ in their classification difficulty. In fact, queries in Set 2 are difficult to interpret even for human evaluators. Queries in Set 1 have on average
in Set 2 have on average 4.39 words, with the longest query of 81 words. Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets. As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks.
The two sets of queries were classified into the target taxonomy using the techniques presented in section 2. Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators. These evaluators were trained editorial staff who possessed knowledge about the taxonomy. The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query. About 2.4% queries in Set 1 and
random strings of characters), and were consequently excluded from evaluation. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.
We used standard evaluation metrics: precision, recall and F1. In what follows, we plot precision-recall graphs for all the experiments. For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1). Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables.
We compared our method to a baseline query classifier that does not use any external knowledge. Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2  http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html
1
Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach. This baseline classifier is actually a production version of the query classifier running in a major US search engine.
In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.
In what follows, we start with the general assessment of the effect of using Web search results. We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs. We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena.
Queries by themselves are very short and difficult to classify. We use top search engine results for collecting background knowledge for queries. We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.
Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.
Engine Context Prec. F1 Prec. F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge
There are two major ways to use search results as additional knowledge. First, individual results can be classified separately, with subsequent voting among individual classifications. Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier. Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. However, in the case of summaries, bundling together is found to be consistently better than individual classification. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable.
1
Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling
To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together. The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification. This observation differs from findings by Shen et al. [20], who found summaries to be more useful. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify.
We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes. Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings. As can be readily seen, all three variants produce very similar results. However, the precision-recall curve for the 1-class experiment has higher fluctuations. Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth. Thus, as we increase the number of classes per result, we observe higher stability in query classification.
We also experimented with different numbers of search results per query. Figure 5 and Table 2 present the results of this experiment. In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50). This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.
Using paired t-test, we assessed the statistical significance
1
Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page
1
Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline. We found the results to be highly significant (p <
query classification.
As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. As we have seen, the voting method works quite well. In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine. We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.
The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).
Method B requires a training/testing split. Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods. For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation. The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG. The decaying choice of log2(i + 1) is conventional, which does not have particular importance. The overall DCG of a system is the averaged DCG over queries.
We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.
Therefore as a single metric, it is convenient for comparing the methods. Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.
Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3. The oracle method is the best ranking of categories for each query after seeing human judgments. It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance. The simple voting method performs very well in our experiments. The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5). However, both methods are computationally more costly, and the potential gain is minor enough to be neglected. This means that as a simple method, voting is quite effective.
We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results. It may be possible to improve this method by including other page-features that can differentiate top-ranked search results. However, the effectiveness will require further investigation which we did not test. We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1.
We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries). One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use.
Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information.
One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.
Early work in information retrieval concentrated on manually reviewing the returned results [16, 15]. However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].
More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed,
Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.
Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2]. Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.
The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21]. The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.
The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.
Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.
In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set (ODP) is used. Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set.
Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy.
The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.
Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.
We also experimented with different values of parameters that characterize our method. When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages (Table 1).
These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.
In this study we used two major search engines, A and B.
Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.
We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.
When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.
We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits.
In this work, we first classify search results, and then use their classifications directly to classify the original query.
Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages. We plan to further investigate this direction in our future work.
It is also essential to note that implementing our methodology incurs little overhead. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.
To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.
In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.
[1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis,
A. Chowdhury, and A. Kolcz. Automatic web query classification using labeled and unlabeled training data. In Proceedings of SIGIR"05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz. Improving automatic query classification via semi-supervised learning. In Proceedings of ICDM"05, 2005. [3] R. Duda and P. Hart. Pattern Classification and Scene Analysis. John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron. UCLA-Okapi at TREC-2: Query expansion experiments. In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch. Feature generation for text categorization using world knowledge. In IJCAI"05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.
Categorizing web queries according to geographical locality.
In CIKM"03, 2003. [7] E. Han and G. Karypis. Centroid-based document classification: Analysis and experimental results. In PKDD"00, September 2000. [8] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR"00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi. The ferrety algorithm for the KDD Cup 2005 problem. In SIGKDD Explorations, volume 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann. Analyzing the effect of query class on document retrieval performance.
In Proc. Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: Facing a great challenge. In SIGKDD Explorations, volume 7, pages 91-99. ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In SIGIR"98, pages 206-214, 1998. [13] M. Moran and B. Hunt. Search Engine Marketing, Inc.: Driving Search Traffic to Your Company"s Web Site.
Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In TREC-3, 1995. [15] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323. Prentice Hall, 1971. [16] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy. The Statistical Analysis of Discrete Data. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Q2C@UST: Our winning solution to query classification in KDDCUP 2005. In SIGKDD Explorations, volume 7, pages 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Query enrichment for web-query classification.
ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building bridges for web query classification. In SIGIR"06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen,

The formulation of query statements that capture both the salient aspects of information needs and are meaningful to Information Retrieval (IR) systems poses a challenge for many searchers [3].
Commercial Web search engines such as Google, Yahoo!, and Windows Live Search offer users the ability to improve the quality of their queries using query operators such as quotation marks, plus and minus signs, and modifiers that restrict the search to a particular site or type of file. These techniques can be useful in improving result precision yet, other than via log analyses (e.g., [15][27]), they have generally been overlooked by the research community in attempts to improve the quality of search results.
IR research has generally focused on alternative ways for users to specify their needs rather than increasing the uptake of advanced syntax. Research on practical techniques to supplement existing search technology and support users has been intensifying in recent years (e.g. [18][34]). However, it is challenging to implement such techniques at large scale with tolerable latencies.
Typical queries submitted to Web search engines take the form of a series of tokens separated by spaces. There is generally an implied Boolean AND operator between tokens that restricts search results to documents containing all query terms. De Lima and Pedersen [7] investigated the effect of parsing, phrase recognition, and expansion on Web search queries. They showed that the automatic recognition of phrases in queries can improve result precision in Web search. However, the value of advanced syntax for typical searchers has generally been limited, since most users do not know about advanced syntax or do not understand how to use it [15]. Since it appears operators can help retrieve relevant documents, further investigation of their use is warranted.
In this paper we explore the use of query operators in more detail and propose alternative applications that do not require all users to use advanced syntax explicitly. We hypothesize that searchers who use advanced query syntax demonstrate a degree of search expertise that the majority of the user population does not; an assertion supported by previous research [13]. Studying the behavior of these advanced search engine users may yield important insights about searching and result browsing from which others may benefit.
Using logs gathered from a large number of consenting users, we investigate differences between the search behavior of those that use advanced syntax and those that do not, and differences in the information those users target. We are interested in answering three research questions: (i) Is there a relationship between the use of advanced syntax and other characteristics of a search? (ii) Is there a relationship between the use of advanced syntax and post-query navigation behaviors? (iii) Is there a relationship between the use of advanced syntax and measures of search success?
Through an experimental study and analysis, we offer potential answers for each of these questions. A relationship between the use of advanced syntax and any of these features could support the design of systems tailored to advanced search engine users, or use advanced users" interactions to help non-advanced users be more successful in their searches.
We describe related work in Section 2, the data we used in this log-based study in Section 3, the search characteristics on which we focus our analysis in Section 4, and the findings of this analysis in Section 5. In Section 6 we discuss the implications of this research, and we conclude in Section 7.
Factors such as lack of domain knowledge, poor understanding of the document collection being searched, and a poorly developed information need can all influence the quality of the queries that users submit to IR systems ([24],[28]). There has been a variety of research into different ways of helping users specify their information needs more effectively. Belkin et al. [4] experimented with providing additional space for users to type a more verbose description of their information needs. A similar approach was attempted by Kelly et al. [18], who used clarification forms to elicit additional information about the search context from users.
These approaches have been shown to be effective in best-match retrieval systems where longer queries generally lead to more relevant search results [4]. However, in Web search, where many of the systems are based on an extended Boolean retrieval model, longer queries may actually hurt retrieval performance, leading to a small number of potentially irrelevant results being retrieved. It is not simply sufficient to request more information from users; this information must be of better quality.
Relevance Feedback (RF) [22] and interactive query expansion [9] are popular techniques that have been used to improve the quality of information that users provide to IR systems regarding their information needs. In the case of RF, the user presents the system with examples of relevant information that are then used to formulate an improved query or retrieve a new set of documents.
It has proven difficult to get users to use RF in the Web domain due to difficulty in conveying the meaning and the benefit of RF to typical users [17]. Query suggestions offered based on query logs have the potential to improve retrieval performance with limited user burden. This approach is limited to re-executing popular queries, and searchers often ignore the suggestions presented to them [1]. In addition, both of these techniques do not help users learn to produce more effective queries.
Most commercial search engines provide advanced query syntax that allows users to specify their information needs in more detail.
Query modifiers such as ‘+" (plus), ‘−" (minus), and ‘  " (double quotes) can be used to emphasize, deemphasize, and group query terms. Boolean operators (AND, OR, and NOT) can join terms and phrases, and modifiers such as site: and link: can be used to restrict the search space. Queries created with these techniques can be powerful. However, this functionality is often hidden from the immediate view of the searcher, and unless she knows the syntax, she must use text fields, pull-down menus and combo boxes available via a dedicated advanced search interface to access these features.
Log-based analysis of users" interactions with the Excite and AltaVista search engines has shown that only 10-20% of queries contained any advanced syntax [14][25]. This analysis can be a useful way of capturing characteristics of users interacting with IR systems. Research in user modeling [6] and personalization [30] has shown that gathering more information about users can improve the effectiveness of searches, but require more information about users than is typically available from interaction logs alone. Unless coupled with a qualitative technique, such as a post-session questionnaire [23], it can be difficult to associate interactions with user characteristics. In our study we conjecture that given the difficulty in locating advanced search features within the typical search interface, and the potential problems in understanding the syntax, those users that do use advanced syntax regularly represent a distinct class of searchers who will exhibit other common search behaviors.
Other studies of advanced searchers" search behaviors have attempted to better understand the strategic knowledge they have acquired. However, such studies are generally limited in size (e.g., [13][19]) or focus on domain expertise in areas such as healthcare or e-commerce (e.g., [5]). Nonetheless, they can give valuable insight about the behaviors of users with domain, system, or search expertise that exceeds that of the average user.
Querying behavior in particular has been studied extensively to better understand users [31] and support other users [16].
In this paper we study other search characteristics of users of advanced syntax in an attempt to determine whether there is anything different about how these search engine users search, and whether their searches can be used to benefit those who do not make use of the advanced features of search engines. To do this we use interaction logs gathered from large set of consenting users over a prolonged period.
In the next section we describe the data we use to study the behavior of the users who use advanced syntax, relative to those that do not use this syntax.
To perform this study we required a description of the querying and browsing behavior of many searchers, preferably over a period of time to allow patterns in user behavior to be analyzed.
To obtain these data we mined the interaction logs of consenting Web users over a period of 13 weeks, from January to April 2006.
When downloading a partner client-side application, the users were invited to consent to their interaction with Web pages being anonymously recorded (with a unique identifier assigned to each user) and used to improve the performance of future systems.1 The information contained in these log entries included a unique identifier for the user, a timestamp for each page view, a unique browser window identifier (to resolve ambiguities in determining which browser a page was viewed), and the URL of the Web page visited. This provided us with sufficient data on querying behavior (from interaction with search engines), and browsing behavior (from interaction with the pages that follow a search) to more broadly investigate search behavior.
In addition to the data gathered during the course of this study we also had relevance judgments of documents that users examined for 10,680 unique query statements present in the interaction logs.
These judgments were assigned on a six-point scale by trained human judges at the time the data were collected. We use these judgments in this analysis to assess the relevance of sites users visited on their browse trail away from search result pages.
We studied the interaction logs of 586,029 unique users, who submitted millions of queries to three popular search enginesGoogle, Yahoo!, and MSN Search - over the 13-week duration of the study. To limit the effect of search engine bias, we used four operators common to all three search engines: + (plus), − (minus), (double quotes), and site: (to restrict the search to a domain or Web page) as advanced syntax. 1.12% of the queries submitted contained at least one of these four operators. 51,080 (8.72%) of users used query operators in any of their queries. In the remainder of this paper, we will refer to these users as advanced searchers. We acknowledge that the direct relationship between query syntax usage and search expertise has only been studied 1 It is worth noting that if users did not provide their consent, then their interaction was not recorded and analyzed in this study. (and shown) in a few studies (e.g., [13]), but we feel that this is a reasonable criterion for a log-based investigation. We conjecture that these advanced searchers do possess a high level of search expertise, and will show later in the paper that they demonstrate behavioral characteristics consistent with search expertise.
To handle potential outlier users that may skew our data analysis, we removed users who submitted fewer than 50 queries in the study"s 13-week duration. This left us with 188,405 users − 37,795 (20.1%) advanced users and 150,610 (79.9%) nonadvanced users − whose interactions we study in more detail. If significant differences emerge between these groups, it is conceivable that these interactions could be used to automatically classify users and adjust a search system"s interface and result weighting to better match the current user.
The privacy of our volunteers was maintained throughout the entire course of the study: no personal information was elicited about them, participants were assigned a unique anonymous identifier that could not be traced back to them, and we made no attempt to identify a particular user or study individual behavior in any way. All findings were aggregated over multiple users, and no information other than consent for logging was elicited.
To find out more about these users we studied whether those using advanced syntax exhibited other search behaviors that were not observed in those who did not use this syntax. We focused on querying, navigation, and overall search success to compare the user groups. In the next section we describe in more detail the search features that we used.
We elected to choose features that described a variety of aspects of the search process: queries, result clicks, post-query browsing, and search success. The query and result-click characteristics we chose to examine are described in more detail in Table 1.
Table 1. Query and result-click features (per user).
Feature Meaning Queries Per Second (QPS) Avg. number of queries per second between initial query and end-of-session Query Repeat Rate (QRR) Fraction of queries that are repeats Query Word Length (QWL) Avg. number of words in query Queries Per Day (QPD) Avg. number of queries per day Avg. Click Position (ACP) Avg. rank of clicked results Click Probability (CP) Ratio of result clicks to queries Avg. Seconds To Click (ASC) Avg. search to result click interval These seven features give us a useful overview of users" direct interactions with search engines, but not of how users are looking for relevant information beyond the result page or how successful they are in locating relevant information. Therefore, in addition to these characteristics we also studied some relevant aspects of users" post-query browsing behavior. To do this, we extracted search trails from the interaction logs described in the previous section. A search trail is a series of visited Web pages connected via a hyperlink trail, initiated with a search result page and terminating on one of the following events: navigation to any page not linked from the current page, closing of the active browser window, or a session inactivity timeout of 30 minutes. More detail on the extraction of the search trails are provided in previous work [33]. In total, around 12.5 million search trails (containing around 60 million documents) were extracted from the logs for all users. The median number of search trails per user was
trails contained one search result page and at least one page on a hyperlink trail leading from the result page.
The extraction of these trails allowed us to study aspects of postquery browsing behavior, namely the average duration of users" search sessions, the average duration of users" search trails, the average display time of each document, the average number of steps in users" search trails, the number of branches in users" navigation patterns, and the number of back operations in users" search trails. All search trails contain at least one branch representing any forward motion on the browse path. A trail can have additional branches if the user clicks the browser"s back button and immediately proceeds forward to another page prior to the next (if any) back operation. The post-query browsing features are described further in Table 2.
Table 2. Post-query browsing features (per trail).
Feature Meaning Session Seconds (SS) Average session length (in seconds) Trail Seconds (TS) Average trail length (in seconds) Display Seconds (DS) Average display time for each page on the trail (in seconds) Num. Steps (NS) Average number of steps from the page following the results page to the end of the trail Num. Branches (NB) Average number of branches Num. Backs (NBA) Average number of back operations As well as using these attributes of users" interactions, we also used the relevance judgments described earlier in the paper to measure the degree of search success based on the relevance judgments assigned to pages that lie on the search trail. Given that we did not have access to relevance assessments from our users, we approximated these assessments using judgments collected as part of ongoing research into search engine performance.2 These judgments were created by trained human assessors for 10,680 unique queries. Of the 1,420,625 steps on search trails that started with any one of these queries, we have relevance judgments for 802,160 (56.4%). We use these judgments to approximate search success for a given trail in a number of ways. In Table 3 we list these measures. 2 Our assessment of search success is fairly crude compared to what would have been possible if we had been able to contact our subjects. We address this problem in a manner similar to that used by the Text Retrieval Conference (TREC) [21], in that since we cannot determine perceived search success, we approximate search success based on assigned relevance scores of visited documents.
Table 3. Relevance judgment measures (per trail).
Measure Meaning First Judgment assigned to the first page in the trail Last Judgment assigned to the last page in the trail Average Average judgment across all pages in the trail Maximum Maximum judgment across all pages in the trail These measures are used during our analysis to estimate the relevance of the pages viewed at different stages in the trails, and allow us to estimate search success in different ways. We chose multiple measures, as users may encounter relevant information in many ways and at different points in the trail (e.g., single highlyrelevant document or gradually over the course of the trail).
The features described in this section allowed us to analyze important attributes of the search process that must be better understood if we are to support users in their searching. In the next section we present the findings of the analysis.
Our analysis is divided into three parts: analysis of query behavior and interaction with the results page, analysis of post-query navigation behavior, and search success in terms of locating judged-relevant documents. Parametric statistical testing is used, and the level of significance for the statistical tests is set to .05.
We were interested in comparing the query and result-click behaviors of our advanced and non-advanced users. In Table 4 we show the mean average values for each of the seven search features for our users. We use padvanced to denote the percentage of all queries from each user that contains advanced syntax (i.e., padvanced = 0% means a user never used advanced syntax). The table shows values for users that do not use query operators (0%), users who submitted at least one query with operators (≥ 0%), through to users whose queries contained operators at least threequarters of the time (≥ 75%).
Table 4. Query and result click features (per user).
Feature padvanced 0% > 0% ≥ 25% ≥ 50% ≥ 75% QPS .028 .010 .012 .013 .015 QRR .53 .57 .58 .61 .62 QWL 2.02 2.83 3.40 3.66 4.04 QPD 2.01 3.52 2.70 2.66 2.31 ACP 6.83 9.12 10.09 10.17 11.37 CP .57 .51 .47 .47 .47 ASC 87.71 88.16 112.44 102.12 79.13 %Users 79.90% 20.10% .79% .18% .04% We compared the query and result click features of users who did not use any advanced syntax (padvanced = 0%) in any of their queries with those who used advanced syntax in at least one query (padvanced > 0%). The columns corresponding to these two groups are bolded in Table 4. We performed an independent measures ttest between these groups for each of the features. Since this analysis involved many features, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .007, i.e., .05 divided by the number of features. This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true. All differences between the groups were statistically significant (all t(188403) ≥ 2.81, all p ≤ .002).
However, given the large sample sizes, all differences in the means were likely to be statistically significant. We applied a Cohen"s d-test to determine the effect size for each of the comparisons between the advanced and non-advanced user groups. Ordering in descending order by effect size, the main findings are that relative to non-advanced users, advanced search engine users: • Query less frequently in a session (d = 1.98) • Compose longer queries (d = .69) • Click further down the result list (d = .67) • Submit more queries per day (d = .49) • Are less likely to click on a result (d = .32) • Repeat queries more often (d = .16) The increased likelihood that advanced search engine users will click further down the result list implies that they may be less trusting of the search engines" ability to rank the most relevant document first, that they are more willing to explore beyond the most popular pages for a given query, that they may be submitting different types of queries (e.g., informational rather than navigational), or that they may have customized their search settings to display more than only the default top-10 results.
Many of the findings listed are consistent with those identified in other studies of advanced searchers" querying and result-click behaviors [13][34]. Given that the only criteria we employed to classify a user as an advanced searcher was their use of advanced syntax, it is certainly promising that this criterion seems to identify users that interact in a way consistent with that reported previously for those with more search expertise.
As mentioned earlier, the advanced search engine users for which the average values shown in Table 4 are computed are those who submit 50 or more queries in the 13 week duration of the data collection and submit at least one query containing advanced query operators. In other words, we consider users whose percentage of queries containing advanced syntax, padvanced, is greater than zero. The use of query operators in any queries, regardless of frequency, suggests that a user knows about the existence of the operators, and implies a greater degree of familiarity with the search system. We further hypothesized that users whose queries more frequently contained advanced syntax may be more advanced search engine users. To test this we investigated varying the query threshold required to qualify for advanced status (padvanced). We incremented padvanced one percentage point at a time, and recorded the values of the seven query and result-click features at each point. The values of the features at four milestones (> 0%, ≥ 25%, ≥ 50%, and ≥ 75%) are shown in Table 4. As can be seen in the table, as padvanced increases, differences in the features between those using advanced syntax and those not using advanced syntax become more substantial. However, it is interesting to note that as padvanced increases, the number of queries submitted per day actually falls (Pearson"s R = −.512, t(98) = 5.98, p < .0001). More advanced users may need to pose fewer queries to find relevant information.
To study the patterns of relationship among these dependent variables (including the padvanced), we applied factor analysis [26].
Table 5 shows the intercorrelation matrix between the features and the percentage of queries with operators (Padvanced). Each cell in the table contains the Pearson"s correlation coefficient between the two features for a given row-column pair.
Table 5. Intercorrelation matrix (query / result-click features). padv. QPS QRR QWL QPD ACP CP ASC padv. 1.00 .946 .970 .987 −.512 .930 −.746 −.583 QPS 1.00 .944 .943 −.643 .860 −.594 −.712 QRR 1.00 .934 −.462 .919 −.621 -.667 QWL 1.00 −.392 .612 −.445 .735 QPD 1.00 .676 .780 .943 ACP 1.00 .838 .711 CP 1.00 .654 ASC 1.00 It is only the first data column and row that reflect the correlations between padvanced and the other query and result-click features.
Columns 2 - 8 show the inter-correlations between the other features. There are strong positive correlations between some of the features (e.g., the number of words in the query (QWL) and the average probability of clicking on a search result (ACP)).
However, there were also fairly strong negative correlations between some features (e.g., the average length of the queries (QWL) and the probability of clicking on a search result (CP)).
The factor analysis revealed the presence of two factors that account for 83.6% of the variance. As is standard practice in factor analysis, all features with an absolute factor loading of .30 or less were removed. The two factors that emerged, with their respective loadings, can be expressed as: Factor A = .98(QRR) + .97(padv) + .97(QPS) + .71(ACP) + .69(QWL) Factor B = .96(CP) + .90(QPD) + .67(ACP) + .52(ASC) Variance in the query and result-click behavior of our advanced search engine users can be expressed using these two constructs.
Factor A is the most powerful, contributing 50.5% of the variance.
It appears to represent a very basic dimension of variance that covers query attributes and querying behavior, and suggests a relationship between query properties (length, frequency, complexity, and repetition) and the position of users" clicks in the result list. The dimension underlying Factor B accounts for
behavior, and a strong correlation between result clicks and the number of queries submitted each day.
Summary: In this section we have shown that there are marked differences in aspects of the querying and result-clickthrough behaviors of advanced users relative to non-advanced users. We have also shown that the greater the proportion of queries that contain advanced syntax, the larger the differences in query and clickthrough behaviors become. A factor analysis revealed the presence of two dimensions that adequately characterize variance in the query and result-click features. In the querying dimension query attributes, such as the length and proportion that contain advanced syntax, and querying behavior, such as the number of queries submitted per day both affect result-click position. In addition, in the result-click dimension, it appears that daily querying frequency influences result-click features such as the likelihood that a user will click on a search result and the amount of time between result presentation and the search result click.
The features used in this section are only interactions with search engines in the form of queries and result clicks. We did not address how users searched for information beyond the result page. In the next section we use the search trails described in Section 4 to analyze the post-query browsing behavior of users.
In this section we look at several attributes of the search trails users followed beyond the results page in an attempt to discern whether the use of advanced search syntax can be used as a predictor of aspects of post-query interaction behavior.
As we did previously, we first describe the mean average values for each of the browsing features, across all advanced users (i.e. padvanced > 0%), all non-advanced users (i.e., padvanced = 0%), and all users regardless of their estimated search expertise level. We then look at the effect on the browsing features of increasing the value of padvanced required to be considered advanced from 1% to 100%. In Table 6 we present the average values for each of these features for the two groups of users. Also shown are the percentage of search trails (%Trails) and the percentage of users (%Users) used to compute the averages.
Table 6. Post-query browsing features (per trail).
Feature padvanced 0% > 0% ≥ 25% ≥ 50% ≥ 75% Session secs. 701.10 706.21 792.65 903.01 1114.71 Trail secs. 205.39 159.56 156.45 147.91 136.79 Display secs. 36.95 32.94 34.91 33.11 30.67 Num. steps 4.88 4.72 4.40 4.40 4.39 Num. backs 1.20 1.02 1.03 1.03 1.02 Num. branches 1.55 1.51 1.50 1.47 1.44 %Trails 72.14% 27.86% .83% .23% .05% %Users 79.90% 20.10% .79% .18% .04% As can be seen from Table 6, there are differences in the postquery interaction behaviors of advanced users (padvanced > 0%) relative to that do not use query operators in any of their queries (padvanced = 0%). Once again, the columns of interest in this comparison are bolded. As we did in Section 5.1 for query and result-click behavior, we performed an independent measures ttest between the values reported for each of the post-query browsing features. The results of this test suggest that differences between those that use advanced syntax and those that do not are significant (t(12495029) ≥ 3.09, p ≤ .002, α = .008). Given the sample sizes, all of the differences between means in the two groups were significant. However, we once again applied a Cohen"s d-test to determine the effect size. The findings (ranked in descending order based on effect size), show that relative to non-advanced users, advanced search engine users: • Revisit pages in the trail less often (d = .45) • Spend less time traversing each search trail (d = .38) • Spend less time viewing each document (d = .28) • Branch (i.e., proceed to new pages following a back operation) less often (d = .18) • Follow search trails with fewer steps (d = .16) It seems that advanced users use a more directed searching style than non-advanced users. They spend less time following search trails and view the documents that lie on those trails for less time.
This is in accordance with our earlier proposition that advanced users seem able to discern document relevance in less time.
Advanced users also tend to deviate less from a direct path as they search, with fewer revisits to previously-visited pages and less branching during their searching.
As we did in the previous section, we increased the padvanced threshold one point at a time. With the exception of number of back operations (NB), the values attributable to each of the features change as padvanced increased. It seems that the differences noted earlier between non-advanced users and those that use any advanced syntax become more significant as padvanced increases.
As in the previous section, we conducted a factor analysis of these features and padvanced. Table 7 shows the intercorrelation matrix for all these variables.
Table 7. Intercorrelation matrix (post-query browsing). padv SS TS DS NS NB NBA padv 1.00 .977 −.843 −.867 −.395 −.339 −.249 SS 1.00 −.765 −.875 −.374 −.335 −.237 TS 1.00 .948 .387 .281 .250 DS 1.00 .392 .344 .257 NS 1.00 .891 .934 NB 1.00 .918 NBA 1.00 As the proportion of queries containing advanced syntax increases, the values of many of the post-query browsing features decrease. Only the average session time (SS) exhibits a strong positive correlation with padvanced. The factor analysis revealed the presence of two factors that account for 89.8% of the variance.
Once again, all features with an absolute factor loading of .30 or less were removed. The two factors that emerged, with their respective loadings, can be expressed as: Factor A = .95(DS) + .88 (TS) − .91(SS) − .95(padv) Factor B = .99(NBA) + .93(NS) + .91(NB) Variance in the query and result-click behavior of those who use query operators can be expressed using these two constructs.
Factor A is the most powerful, contributing 50.1% of the variance.
It appears to represent a very basic temporal dimension that covers timing and percentage of queries with advanced syntax, and suggests a negative relationship between time spent searching and overall session time, and a negative relationship between time spent searching and padvanced. The navigation dimension underlying Factor B accounts for 39.7% of the variance, and describes attributes of post-query navigation, all of which seem to be strongly correlated with each other but not padvanced or timing.
Summary: In this section we have shown that advanced users" post-query browsing behavior appears more directed than that of non-advanced users. Although their search sessions are longer, advanced users follow fewer search trails during their sessions, (i.e., submit fewer queries), their search trails are shorter, and their trails exhibit fewer deviations or regressions to previously encountered pages. We also showed that as padvanced increases, session time increases (perhaps more advanced users are multitasking between search and other operations), and search interaction becomes more focused, perhaps because advanced users are able target relevant information more effectively, with less need for regressions or deviations in their search trails.
As well as interaction behaviors such as queries, result clicks, and post-query browse behavior, another important aspect of the search process is the attainment of information relevant to the query. In the next section we analyze the success of advanced and non-advanced users in obtaining relevant information.
As described earlier, we used six-level relevance judgments assigned to query-document pairs as an approximate measure of search success based on documents encountered on search trails.
However, the queries for which we have judgments generally did not contain advanced operators. To maximize the likelihood of coverage we removed advanced operators from all queries when retrieving the relevance judgments. The mean average relevance judgment values for each of the four metrics - first, last, average, and maximum - are shown in Table 8 for non-advanced users (0%) and advanced users (> 0%).
Table 8. Search success (min. = 1, max. = 6) (per trail).
Feature padvanced 0% > 0% ≥ 25% ≥ 50% ≥ 75% First M 4.03 4.19 4.24 4.26 4.57 SD 1.58 1.56 1.34 1.38 1.27 Last M 3.79 3.92 4.00 4.13 4.35 SD 1.60 1.57 1.29 1.25 .89 Max. M 4.04 4.20 4.19 4.19 4.46 SD 1.63 1.51 1.28 1.37 1.25 Avg. M 3.93 4.06 4.08 4.08 4.26 SD 1.57 1.51 1.23 1.32 1.14 The findings suggest that users who use advanced syntax at all (padvanced > 0%) were more successful - across all four measuresthan those who never used advanced syntax (padvanced = 0%). Not only were these users more successful in their searching, but they were consistently more successful (i.e., the standard deviation in relevance scores is lower for advanced users and continues to drop as padvanced increases). The differences in the four mean average relevance scores for each metric between these two user groups were significant with independent measures t-tests (all t(516765) ≥ 3.29, p ≤ .001, α = .0125). As we increase the value of padvanced as in previous sections, the average relevance score across all metrics also increases (all Pearson"s R ≥ .654), suggesting that more advanced users are also more likely to succeed in their searching. The searchers that use advanced operators may have additional skills in locating relevant information, or may know where this information resides based on previous experience.3 Despite the fact that the four metrics targeted different parts of the search trail (e.g., first vs. last) or different ways to gather relevant information (e.g., average vs. maximum), the differences between groups and within the advanced group were consistent. 3 Although in our logs there was no obvious indication of more revisitation by advanced search engine users.
To see whether there were any differences in the nature of the queries submitted by advanced search engine users, we studied the distribution of the four advanced operators: quotation marks, plus, minus, and site:. In Table 9 we show how these operators were distributed in all queries submitted by these users.
Table 9. Distribution of query operators.
Feature padvanced > 0% ≥ 25% ≥ 50% ≥ 75% Quotes () 71.08 77.09 70.33 70.00 Plus (+) 6.84 13.31 19.21 33.90 Minus (−) 6.62 2.88 1.96 2.42 Site: 21.55 12.72 13.04 9.86 Avg. num. operators 1.08 1.14 1.28 1.49 The distribution of the quotes, plus, and minus operators are similar amongst the four levels of padvanced, with quotes being the most popular of the four operators used. However, it appears that the plus operator is the main differentiator between the padvanced user groups. This operator, which forces the search engine to include in the query terms that are usually excluded by default (e.g. the, a), may account for some portion of the difference in observed search success.4 However, this does not capture the contribution that each of these operators makes to the increase in relevance compared with excluding the operator. To gain some insight into this, we examined the impact that each of the operators had on the relevance of retrieved results. We focused on queries in padvanced > 0% where the same user had issued a query without operators and the same query with operators either before or afterwards. Although there were few queries with matching pairs - and almost all of them contained quotes - there was a small (approximately 10%) increase in the average relevance judgment score assigned to documents on the trail with quotes in the initial query. It may be the case that quoted queries led to retrieval of more relevant documents, or that they better match the perceived needs of relevance judges and therefore lead to judged documents receiving higher scores. More analysis similar to [8] is required to test these propositions further.
Summary: In this section we have used several measures to study the search success of advanced and non-advanced users. The findings of our analysis suggest that advanced search engine users are more successful and have more consistency in the relevance of the pages they visit. Their additional search expertise may make these users better able to make better decisions about which documents to view, meaning they encounter consistently more relevant information on their searches. In addition, within the group of advanced users there is a strong correlation between padvanced and the degree of search success. Advanced search engine users may be more adept at combining query operators to formulate powerful query statements. We now discuss the findings from all three subsections and their implications for the design of improved Web search systems. 4 It is worth noting that there were no significant differences in the distribution of usage of the three search engines - Google,
Yahoo!, or Windows Live Search - amongst advanced search engine users, or between advanced users and non-advanced.
Our findings indicate significant differences in the querying, result-click, post-query navigation, and search success of those that use advanced syntax versus those that do not. Many of these findings mirror those already found in previous studies with groups of self-identified novices and experts [13][19]. There are several ways in which a commercial search engine system might benefit from a quantitative indication of searcher expertise. This might be yet another feature available to a ranking engine; i.e. it may be the case that expert searchers in some cases prefer different pages than novice searchers. The user interface to a search engine might be tailored to a user"s expertise level; perhaps even more advanced features such as term weighting and query expansion suggestions could be presented to more experienced searchers while preserving the simplicity of the basic interface for novices. Result presentation might also be customized based on search skill level; future work might re-evaluate the benefits of content snippets, thumbnails, etc. in a manner that allows different outcomes for different expertise levels. Additionally, if browsing histories are available, the destinations of advanced searchers could be used as suggested results for queries, bypassing and potentially improving upon the traditional search process [10].
The use of the interaction of advanced search engine users to guide others with less expertise is an attractive proposition for the designers of search systems. In part, these searchers may have more post-query browsing expertise that allows them to overcome the shortcomings of search systems [29]. Their interactions can be used to point users to places that advanced search engine users visit [32] or simply to train less experienced searchers how to search more effectively. However, if expert users are going to be used in this way, issues of data sparsity will need to be overcome.
Our advanced users only accounted for 20.1% of the users whose interactions we studied. Whilst these may be amongst the most active users it is unlikely that they will view documents that cover large number of subject areas. However, rather than focusing on where they go (which is perhaps more appropriate for those with domain knowledge), advanced search engine users may use moves, tactics and strategies [2] that inexperienced users can learn from. Encouraging users to use advanced syntax helps them learn how to formulate better search queries; leveraging the searching style of expert searchers could help them learn more successful post-query interactions.
One potential limitation to the results we report is that in prior research, it has been shown that query operators do not significantly improve the effectiveness of Web search results [8], and that searchers may be able to perform just as well without them [27]. It could therefore be argued that the users who do not use query operators are in fact more advanced, since they do not waste time using potentially redundant syntax in their query statements. However, this seems unlikely given that those who use advanced syntax exhibited search behaviors typical of users with expertise [13], and are more successful in their searching.
However, in future work we will expand of definition of advanced user beyond attributes of the query to also include other interaction behaviors, some of which we have defined in this study, and other avenues of research such as eye-tracking [12].
In this paper we have described a log-based study of search behavior on the Web that has demonstrated that the use of advanced search syntax is correlated with other aspects of search behavior such as querying, result clickthrough, post-query navigation, and search success. Those that use this syntax are active online for longer, spend less time querying and traversing search trails, exhibit less deviation in their trails, are more likely to explore search results, take less time to click on results, and are more successful in there searching. These are all traits that we would expect expert searchers to exhibit. Crude classification of users based on just one feature that is easily extractable from the query stream yields remarkable results about the interaction behavior of users that do not use the syntax and those that do. As we have suggested, search systems may leverage the interactions of these users for improved document ranking, page recommendation, or even user training. Future work will include the development of search interfaces and modified retrieval engines that make use of these information-rich features, and further investigation into the use of these features as indicators of search expertise, including a cross-correlation analysis between result click and post-query behavior.
The authors are grateful to Susan Dumais for her thoughtful and constructive comments on a draft of this paper.
[1] Anick, P. (2003). Using terminological feedback for Web search refinement: A log-based study. In Proc. ACM SIGIR, pp. 88-95. [2] Bates, M. (1990). Where should the person stop and the information search interface start? Inf. Proc. Manage. 26, 5, 575-591. [3] Belkin, N.J. (2000). Helping people find what they don"t know. Comm. ACM, 43, 8, 58-61. [4] Belkin, N.J. et al. (2003). Query length in interactive information retrieval. In Proc. ACM SIGIR, pp. 205-212. [5] Bhavnani, S.K. (2001). Domain-specific search strategies for the effective retrieval of healthcare and shopping information. In Proc. ACM SIGCHI, pp. 610-611. [6] Chi, E. H., Pirolli, P. L., Chen, K. & Pitkow, J. E. (2001).
Using information scent to model user information needs and actions and the Web. In Proc. ACM SIGCHI, pp. 490-497. [7] De Lima, E.F. & Pedersen, J.O. (1999). Phrase recognition and expansion for short, precision-biased queries based on a query log. In Proc. of ACM SIGIR, pp. 145-152. [8] Eastman, C.M. & Jansen, B.J. (2003). Coverage, relevance, and ranking: The impact of query operators on Web search engine results. ACM TOIS, 21, 4, 383-411. [9] Efthimiadis, E.N. (1996). Query expansion. Annual Review of Information Science and Technology, 31, 121-187. [10] Furnas, G. (1985). Experience with an adaptive indexing scheme. In Proc. ACM SIGCHI, pp. 131-135. [11] Furnas, G.W., Landauer, T.K., Gomez, L.M. & Dumais, S.T. (1987). The vocabulary problem in human-system communication: An analysis and a solution. Comm. ACM, 30, 11, 964-971. [12] Granka, L., Joachims, T. & Gay, G. (2004). Eye-tracking analysis of user behavior in WWW search. In Proc. ACM SIGIR, pp. 478-479. [13] Hölscher, C. & Strube, G. (2000). Web search behavior of internet experts and newbies. In Proc.WWW, pp. 337-346. [14] Jansen, B.J. (2000). An investigation into the use of simple queries on Web IR systems. Inf. Res. 6, 1. [15] Jansen, B.J., Spink, A. & Saracevic, T. (2000). Real life, real users, and real needs: A study and analysis of user queries on the Web. Inf. Proc. Manage. 36, 2, 207-227. [16] Jones, R., Rey, B., Madani, O. & Greiner, W. (2006).
Generating query substitutions. In Proc. WWW, pp. 387-396. [17] Kaski, S., Myllymäki, P. & Kojo, I. (2005). User models from implicit feedback for proactive information retrieval.
In Workshop at UM Conference; Machine Learning for User Modeling: Challenges. [18] Kelly, D., Dollu, V.D. & Fu, X. (2005). The loquacious user: a document-independent source of terms for query expansion. In Proc. ACM SIGIR, pp. 457-464. [19] Lazonder, A.W., Biemans, H.J.A. & Woperis, I.G.J.H. (2000). Differences between novice and experienced users in searching for information on the World Wide Web. J.
ASIST. 51, 6, 576-581. [20] Morita, M. & Shinoda, Y. (1994). Information filtering based on user behavior analysis and best match text retrieval. In Proc. ACM SIGIR, pp. 272-281. [21] NIST Special Publication 500-266: The Fourteenth Text Retrieval Conference Proceedings (TREC 2005). [22] Oddy, R. (1977). Information retrieval through man-machine dialogue. J. Doc. 33, 1, 1-14. [23] Rose, D.E. & Levinson, D. (2004). Understanding user goals in Web search. In Proc. WWW, pp. 13-19. [24] Salton, G. and Buckley, C. (1990). Improving retrieval performance by relevance feedback. J. ASIST, 41 4, 288-287. [25] Silverstein, C., Marais, H., Henzinger, M. & Moricz, M. (1999). Analysis of a very large web search engine query log. SIGIR Forum, 33, 1, 6-12. [26] Spearman, C. (1904). General intelligence, objectively determined and measured. Amer. J. Psy. 15, 201-293. [27] Spink, A., Bateman, J. & Jansen, B.J. (1998). Searching heterogeneous collections on the Web: Behavior of Excite users. Inf. Res. 4, 2, 317-328. [28] Spink, A., Griesdorf, H. & Bateman, J. (1998). From highly relevant to not relevant: examining different regions of relevance. Inf. Proc. Manage. 34 5, 599-621. [29] Teevan, J. et al. (2004). The perfect search engine is not enough: A study of orienteering behavior in directed search.

In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].
This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user"s assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescope"s repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.
We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. Second, because a term takes less time to judge than a document"s full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback. This is especially helpful for interactive adhoc search. Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard. This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.
During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user"s information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.
The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.
The experiment results are given in Section 6. Section 7 concludes this paper.
Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the user"s information need better than the original one, which is often just a short keyword query.
A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.
Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23]. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).
In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10]. For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms. In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.
Our work differs from the previous ones in two important aspects. First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects. Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership. Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance. The combination of the two aspects allows our method to perform much better than the baseline.
The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method.
We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25]. With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents. We adopt this view, and cast our task as updating the query model from user term feedback.
There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the user"s information need. Second, how to compute an updated query model based on this term feedback evidence, so that it captures the user"s information need and translates into good retrieval performance.
Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback. If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need. If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results. Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the user"s information need. This is similar to active feedback[21], which suggests that a retrieval system should actively probe the user"s information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).
In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user. These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance. Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.
The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents. This method, however, has two drawbacks. First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering. Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.
We solve the above problems by two corresponding measures.
First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list. Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.
We rely on the mixture multinomial model, which is used for theme discovery in [26]. Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic. The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi. We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is w"s frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate. The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm. For its details, we refer the reader to [26]. Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3). Note that only the middle cluster is relevant.
Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms. If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation. We also filter out terms in the original query text because they tend to always be relevant when the query is short. The selected terms are then presented to the user for judgment. A sample (completed) feedback form is shown in Figure 1.
In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance. We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance). We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment.
TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback. The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the user"s information need.
First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . . K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . . K, j = 1 . . . L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise.
This is a straight-forward form of term feedback that does not involve any secondary structure. We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant. We call this method TFB (direct Term FeedBack).
If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does. If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.
Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.
Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit
Here we exploit the cluster structure that played an important role when we selected the presentation terms. The clusters represent different aspects of the query topic, each of which may or may not be relevant. If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones). We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.
Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.
Because each cluster has an equal number of terms presented to the user, the simplest measure of a cluster"s relevance is the number of terms that are judged relevant in it. Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification. If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.
We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25].
TFB and CFB both have their drawbacks. TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the user"s ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones. For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster. CFB remedies TFB"s problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged. Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user. Therefore, we try to combine the two methods, hoping to get the best out of both.
We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B )
In this section, we describe our experiment results. We first describe our experiment setup and present an overview of various methods" performance. Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms. Next we analyze user term feedback behavior and its relation to retrieval performance. Finally we compare term feedback to relevance feedback and show that it has its particular advantage.
We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms. The tracks used the AQUAINT collection, a 3GB corpus of English newswire text. The topics included 50 ones previously known to be hard, i.e. with low retrieval performance. It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.
Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types. The last row is the percentage of MAP improvement over the baseline. The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.
Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster. They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each. The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering. For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form. The sample clarification form shown in Figure 1 is of type 3 × 16. It is a simple and compact interface in which the user can check relevant terms. The form is self-explanatory; there is no need for extra user training on how to use it.
Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length. As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents. We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments). For all other parameters we use Lemur"s default settings. The baseline turns out to perform above average among the track participants. After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms. After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.
We evaluate the different retrieval methods" performance on their rankings of the top 1000 documents. The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR). Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K). For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.
From Table 2 we can make the following observations: 1 http://www.lemurproject.com
pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the user"s information need.
In other words, term feedback is truly helpful for improving retrieval accuracy.
3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones.
counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial. CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback.
by interpolation, it is able to outperform both. This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).
Except for TCFB6C v.s. CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test. This is not true in the case of TFB v.s. CFB, each of which is better than the other in nearly half of the topics.
In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.
It is interesting to know whether our algorithms" performance deteriorates when the user is presented with fewer terms. Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 . Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others. Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.
We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB. For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively. We conjecture the reason to be that while TFB"s performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work. Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is
for CFB it is 95.0% against 98.0%. This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.
Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small. For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms.
In this part we study several aspects of user"s term feedback behavior, and whether they are connected to retrieval performance.
Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 . We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes. This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.
We find that a user often makes mistakes when judging term relevance. Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user. Other times a dubious term may be included but turns out to be irrelevant. Take the topic in Figure 1 for example. There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.
Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event. Indeed, without proper context it would be hard to make perfect judgment.
What is then, the extent to which the user is good at term feedback? Does it have serious impact on retrieval performance? To answer these questions, we need a measure of individual terms" true relevance. We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment. If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.
We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0. We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user. Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.
Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and
checked terms. There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms. Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.
The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18]. In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6. The user is able to recognize only 6.9 of these terms on average. Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect. On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.
We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback. Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.
Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341
Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance. The feedback process is simulated using document relevance judgment from NIST. We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.
Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run. However, this does not hold for term feedback. Thus, to make it fair w.r.t. user"s information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out. Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front. Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.
Table 6: Performance of relevance feedback for different number of feedback documents (N).
N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents. Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.
We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback. This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic. We can then compare the terms in the truncated model with the checked terms. Figure 3 shows the distribution of the terms" σKLD scores.
We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents). This does not contradict the fact that the latter yields higher retrieval performance. Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304. The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.
We are interested to know under what circumstances term feedback has advantage over relevance feedback. One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless. This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20. When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.
Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0). Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics). We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.
This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form. Second, for some topics, a document needs to meet some special condition in order to be relevant. The top N documents may be related to the topic, but nonetheless irrelevant. In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones. For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant. But nevertheless, the feedback terms such as retail, commerce are good for query expansion.
In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.
We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.
Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C"s performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.
We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment.
Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents.
This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472.
[1] J. Allan. Relevance feedback with too much data. In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan. HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents. In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick. Using terminological feedback for web search refinement: a log-based study. In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni. The paraphrase search assistant: terminological feedback for iterative information seeking. In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal. Automatic query expansion using SMART. In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman. Towards interactive query expansion. In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan. UMass at TREC 2003: HARD and QA. In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu. Hierarchical presentation of expansion terms. In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and status. Technical Report 446,
Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a document-independent source of terms for query expansion. In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu. Elicitation of term relevance feedback: an investigation of term source and context. In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin. A case for interaction: A study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft. Relevance-based language models. In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon. Evaluation of the real and perceived value of automatic and interactive query expansion. In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte. A Language Modeling Approach to Information Retrieval.
PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.
Okapi at TREC-3. In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio. Relevance feedback in information retrieval. In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven. Re-examining the potential effectiveness of interactive query expansion. In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai. Active feedback in ad-hoc information retrieval. In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink. Term relevance feedback and query expansion: relation to design. In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft. Query expansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.

State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.
However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).
Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]). If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.
The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].
Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].
In this paper, we present a general approach for learning ranking functions that maximize MAP performance.
Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.
In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution. Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.
We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus. We have also developed a software package implementing our algorithm that is available for public use1 .
Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus). In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y. The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP. We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y). The goal is to find a function h such that the risk (i.e., expected loss),
R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized. Of course, P(x, y) is unknown. But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk,
R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).
In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.
We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0). We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0. We further assume that all predicted rankings are complete rankings (no ties).
Let p = rank(y) and ˆp = rank(ˆy). The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy. MAP is the mean of the average precision scores of a group of queries.
Most learning algorithms optimize for accuracy or ROCArea. While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.
ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair. In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.
Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b.
Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1. These two hypotheses predict a ranking for query x over a corpus of eight documents.
Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2. Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score.
Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.
Models which optimize for accuracy are not directly concerned with the ranking. Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.
Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses. Table 3 shows the predictions of the two hypotheses on a single query x.
Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q). The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds. For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of
5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73. A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score.
We build upon the approach used by [13] for optimizing ROCArea. Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.
Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant). Let Cx and C¯x  denote the set of relevant and non-relevant documents of C for query x, respectively.
We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)). Our approach is to learn a discriminant function F : X × Y → over input-output pairs. Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e.,
F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .
We represent rankings as a matrix of pairwise orderings,
Y ⊂ {−1, 0, +1}|C|×|C| . For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank. We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity). Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings. Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).
Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y). As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order. We will discuss later the choices of φ we used for our experiments.
The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings. Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training. Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training. We now present a method based on structural SVMs [19] to address this problem.
We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .
Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss,
P ξi. As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.
Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.
For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem. Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)). During prediction, our model chooses the ranking which maximizes the discriminant (1). If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.
Therefore, the sum of slacks,
P ξi, upper bounds the MAP loss.
This is stated formally in Proposition 1.
Proposition 1. Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set. Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.
Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19]. The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.
If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1"s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .
Theorem 1. Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y. Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term. Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable.
Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13]. Solving argmax H for ∆map is more difficult. This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair. MAP, on the other hand, does not decompose in the same way as ROCArea. The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.
One useful property of ∆map is that it is invariant to swapping two documents with equal relevance. For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.
By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves. However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).
This leads us to Observation 1.
Observation 1. Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant). Every ranking which satisfies the same set of constraints will have the same ∆map. If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.
Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.
By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists. For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d). For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .
We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 . For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di). The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1. The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .
Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1). The bottom ranking differs from the top only where d¯x j slides up one rank. The difference in the value of H for these two rankings is exactly δj(i, i + 1).
For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j )  .
Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document. Due to Observation 1, this encoding uniquely identifies a complete ranking. We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function,
H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking. Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.
We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8). Conceptually, Algorithm 2 starts with a perfect ranking. Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant.
Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents. In other words, the algorithm maximizes H for each non-relevant document, d¯x j ,
Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).
In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8). Therefore, it suffices to prove that Algorithm 2 satisfies (10). We first prove that δj(·, ·) is monotonically decreasing in j.
Lemma 1. For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).
Proof. Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms. We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.
Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)). We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1). In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k.
It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k.
The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .
Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.
The result of Lemma 1 leads directly to our main correctness result: Theorem 2. In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.
Proof. We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).
Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.
Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.
Suppose for contradiction that optj+1 < optj. Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13). Therefore, it must be the case that optj ≤ optj+1, which completes the proof.
The running time of Algorithm 2 can be split into two parts. The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |. The second part computes each optj, which requires O(|Cx | · |C¯x |) time.
Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n). For most real-world datasets,
Algorithm 2 is dominated by the sort and has complexity O(n log n).
Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.
Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour). Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).
We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process. To improve performance, it is advisable to use the standard C implementation4 of SVMstruct .
The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea. We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus. For each query, TREC provides the relevance judgments of the documents. We generated our features using the scores of existing retrieval functions on these queries.
While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features. As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our method"s ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.
We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods. Comparing with the best base functions tests our method"s ability to learn a useful combination. Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice. The rest of this section describes the base functions and the feature generation method in detail.
We chose two sets of base functions for our experiments.
For the first set, we generated three indices over the WT10g corpus using Indri5 . The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indri"s default stopwords.
For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indri"s built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior. All parameters were kept as their defaults.
We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total. For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.
For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.
We used only the non-manual, non-short submissions from both years. For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively. A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning
In order to generate input examples for our method, a concrete instantiation of φ must be provided. For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 -
2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values. From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.
Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions. Figure 2 shows an example of our feature mapping method. In this example we have a single feature F = {f}. Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc . For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .
This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.
We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10. For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f.
Using the four choices of F, we generated four datasets for our main experiments. Table 5 contains statistics of the generated datasets. There are many ways to generate features, and we are not advocating our method over others.
This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model.
For each dataset in Table 5, we performed 50 trials. For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.
Models were trained using a wide range of C values. The model which performed best on the validation set was selected and tested on the remaining 35 queries.
All queries were selected to be in the training, validation and test sets the same number of times. Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20]. All SVM methods used a linear kernel. We reported the average performance of all models over the 50 trials.
In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 -
2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 -
2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions? Table 6 presents the comparison of SVM∆ map with the best Indri base functions. Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function. The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score. Significance tests were performed using the two-tailed Wilcoxon signed rank test. Two stars indicate a significance level of 0.95.
All tables displaying our experimental results are structured identically. Here, we find that SVM∆ map significantly outperforms the best base functions.
Table 7 shows the comparison when trained on TREC submissions. While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant. Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions. As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.
Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed. Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training). Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission.
The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods? Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively. Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.
To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically. As such, we tried several approaches to improve the performance of SVMacc.
One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 -
roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 -
roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments. The vast majority of the documents are not relevant. SVMacc2 addresses this problem by assigning more penalty to false negative errors. For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset. Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.
Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.
It may be that different queries require different values of b. Having the learning method trying to find a good b value (when one does not exist) may be detrimental.
We took two approaches to address this issue. The first method, SVMacc3, converts the retrieval function scores into percentiles. For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9. Each Kf contains 50 evenly spaced values between 0 and 1. Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.
The second method, SVMacc4, normalizes the scores given by f for each query. For example, assume for query q that f outputs scores in the range 0.2 to 0.7. Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8. Each Kf contains 50 evenly spaced values between 0 and 1. Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map
SVM∆ roc performed much better than SVMacc in our experiments. When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.
However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.
Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.
The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.
TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 -
roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best)
We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.
Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. The computational cost for training is very reasonable in practice. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.
The learning framework used by our method is fairly general. A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank.
This work was funded under NSF Award IIS-0412894,
NSF CAREER Award 0237381, and a gift from Yahoo! Research. The third author was also partly supported by a Microsoft Research Fellowship.
[1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.
Automatic combination of multiple ranked retrieval systems. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q. Le. Learning to rank with non-smooth cost functions. In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W.
Hon. Adapting ranking SVM to document retrieval. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova. Learning a ranking from pairwise preferences. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes. Ensemble selection from libraries of models.
In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves. In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking. Overview of the TREC-9 web track. In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell. Overview of the TREC-2001 web track. In Proceedings of TREC-2001,
Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression.
Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti. Optimising area under the ROC curve using gradient descent. In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims. A support vector method for multivariate performance measures. In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.
ACM Press. [14] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classification in nonstandard situations.
Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.
Combining statistical learning with a knowledge-based approach. In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson. The probability ranking principle in ir. journal of documentation. Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik. Statistical Learning Theory. Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.

Uncertainty is an inherent feature of information retrieval.
Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user"s information need may be vague or incompletely specified by these queries. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.
Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio"s formula [16], or more recent language modeling approaches such as Relevance Models [10]. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 . The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.
Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models.
Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.
We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model"s weight combines two complementary factors: the model"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight. 1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see [10], p. 62).
Figure 1: Estimating the uncertainty of the feedback model for a single query.
In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models. In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined.
Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance. We make no other assumptions about f(D, Q). The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators. In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries. Our specific query method is given in Section 3.
We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document. We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution. We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs. Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space). Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation. We call this distribution over possible feedback models the feedback model distribution. Our goal in this section is to estimate a useful approximation to the feedback model distribution.
For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval [15].
The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively. We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C). For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models.
To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.
Our sample space is the set of all possible language models LF that may be output as feedback models. Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood. For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution. Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution.
We would like an approximation to the posterior distribution of the feedback model LF . To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.
Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.
Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb using the black box feedback method. We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution. Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range. Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C). Thus, a document is more likely to be chosen the higher it is in the ranking.
The rationale for our sampling approach has two parts.
First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable. In this respect, our approach resembles bagging [4], an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors. In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.
Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination. For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities,
Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method. The language model that would have been chosen by the baseline expansion is at the center of each map. The similarity function is JensenShannon divergence.
Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.
Each point in our sample space is a language model, which typically has several thousand dimensions. To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to ‘flatten" and visualize the high-dimensional density function2 .
The density maps for three TREC topics are shown in Figure 2 above. The dark areas represent regions of high similarity between language models. The light areas represent regions of low similarity - the ‘valleys" between clusters. Each diagram is centered on the language model that would have been chosen by the baseline expansion. A single peak (mode) is evident in some examples, but more complex structure appears in others. Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c. In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse).
After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution. We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }. To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13]. We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small. In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.
Note that for this step we are re-using the existing retrieved documents and not performing additional queries.
Given the parameters of an N-dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.) For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten. Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance. We leave this for future study.
We use the following methods for generating variants of the original query. Each variant corresponds to a different assumption about which aspects of the original query may be important. This is a form of deterministic sampling.
We selected three simple methods that cover complimentary assumptions about the query.
No-expansion Use only the original query. The assumption is that the given terms are a complete description of the information need.
Leave-one-out A single term is left out of the original query. The assumption is that one of the query terms is a noise term.
Single-term A single term is chosen from the original query.
This assumes that only one aspect of the query, namely, that represented by the term, is most important.
After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too ‘far". In this study, we set αSUB = 0.5. For example, using the Indri [12] query language, a leave-oneout variant of the initial query that omits the term ‘ireland" for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks)
from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination. To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant. Each score is given by that term"s probability in the Dirichlet distribution. The term scores are weighted by the inverse of the variance of the term in the enhanced feedback model"s Dirichlet distribution. The prior probability of a word"s membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model.
In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query.
We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g,
TREC-9&10). We chose these for their varied content and document properties. For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.
Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1]. Our queries were derived from the words in the title field of the TREC topics. Phrases were not used. To generate the baseline queries passed to Indri, we wrapped the query terms with Indri"s #combine operator. For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.
Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words. However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical. This is discussed further in Section 3.6.
For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method. This method first selects terms using a log-odds calculation described by Ponte [14], but assigns final term weights using Lavrenko"s relevance model[10].
We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with. In a TREC evaluation using the GOV2 corpus [6], the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries. In this study, it achieves an average gain in MAP of 17.25% over the four collections.
Indri"s expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen. Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval. We use Dirichlet smoothing of p(v|D) with μ = 1000.
This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.
By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated. For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...)
We measure our feedback algorithm"s effectiveness by two main criteria: precision, and robustness. Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4. In this section, we examine average precision and precision in the top 10 documents (P10). We also include recall at 1,000 documents.
For each query, we obtained a set of B feedback models using the Indri baseline. Each feedback model was obtained from a random sample of the top k documents taken with replacement. For these experiments, B = 30 and k = 50.
Each feedback model contained 20 terms. On the query side, we used leave-one-out (LOO) sampling to create the query variants. Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling. We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.
We call our method ‘resampling expansion" and denote it as RS-FB here. We denote the Indri baseline feedback method as Base-FB. Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.
We observe several trends in this table. First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections. The Indri baseline expansion gain was 17.25%. Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics. The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.
Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets.
We use the term robustness to mean the worst-case average precision performance of a feedback algorithm. Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.
To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 . For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt. Here, by ‘helped" we mean obtaining a higher average precision as a result of feedback. The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al. [17].
Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB). Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora. The x-axis gives the change in MAP over using baseline expansion with α = 0.5. The yaxis gives the Robustness Index (RI). Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow. Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.
Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB). Also shown are the actual number of queries hurt by feedback (n−) for each method and collection. Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped. The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q. However, it is easy to understand as a general indication of robustness.
One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query.
We call this the ‘small-α" strategy. Since we are also reducing the potential gains when the feedback model is ‘right", however, we would expect some trade-off between average precision and robustness. We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method. The results are summarized in Figure 3. In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight. As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated. For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point. Higher and to the right is better. This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.
Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined. Queries are binned by % change in AP compared to the unexpanded query.
Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.
Table 2 gives the Robustness Index scores for Base-FB and RS-FB. The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.
A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4.
Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP.
sampling methods Given our algorithm"s improved robustness seen in Section 3.4, an important question is what component of our system is responsible. Is it the use of document re-sampling, the use of multiple query variants, or some other factor? The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness. When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets. In two cases, the RI measure drops by more than 50%.
We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies. The ‘uniform weighting" strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection. In contrast, the ‘relevance-score weighting" strategy chose documents with probability proportional to their relevance scores. In this way, documents that were more highly ranked were more likely to be selected. Results are shown in Table 4.
The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets. The difference in average precision between the methods, however, is less marked. This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items. On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.
For space reasons we only summarize our findings on sample size here. The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples. We used 30 samples for our experiments. Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases.
term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context. Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots. In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.
This happens, for example, even with the Relevance Model approach that is part of our baseline feedback. To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here. If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.) Indri"s method attempts to address the stopword problem by applying an initial step based on Ponte [14] to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection. Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.
Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling. The percentage change compared to uniform sampling is shown in parentheses. QV indicates that query variants were used in both runs.
Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.
Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically. In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten. We observed similar feedback term behavior across many other topics. The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff. While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample. As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature.
Our approach is related to previous work from several areas of information retrieval and machine learning. Our use of query variation was inspired by the work of YomTov et al. [20], Carpineto et al. [5], and Amati et al. [2], among others. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics.
In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision.
The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. Xu and Croft"s method of Local Context Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.
On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. [17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method. Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.
Greiff, Morgan and Ponte [8] explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models.
Estimates of output variance have recently been used for improved text classification. Lee et al. [11] used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.
Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector ˆwi of the documents is calculated. Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig. (7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query. The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method.
Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.
The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents.
We have presented a new approach to pseudo-relevance feedback based on document and query sampling. The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.
Applications such as selective expansion may then be implemented in a principled way.
While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.
Acknowledgements We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123. Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors.
[1] The Lemur toolkit for language modeling and retrieval. http://www.lemurproject.org. [2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004), pages 127-137. [3] R. K. Ando and T. Zhang. A high-performance semi-supervised learning method for text chunking. In Proc. of the 43rd Annual Meeting of the ACL, pages 1-9, June 2005. [4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996. [5] C. Carpineto, G. Romano, and V. Giannini. Improving retrieval feedback with multiple term-ranking function combination.
ACM Trans. Info. Systems, 20(3):259 - 290. [6] K. Collins-Thompson, P. Ogilvie, and J. Callan. Initial results with structured queries and language models on half a terabyte of text. In Proc. of 2005 Text REtrieval Conference. NIST Special Publication. [7] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. Wiley and Sons, 2nd edition, 2001. [8] W. R. Greiff, W. T. Morgan, and J. M. Ponte. The role of variance in term weighting for probabilistic information retrieval. In Proc. of the 11th Intl. Conf. on Info. and Knowledge Mgmt. (CIKM 2002), pages 252-259. [9] T. Kohonen, J. Hynninen, J. Kangas, and J. Laaksonen.
SOMPAK: The self-organizing map program package. Technical Report A31, Helsinki University of Technology, 1996. http://www.cis.hut.fi/research/papers/som tr96.ps.Z. [10] V. Lavrenko. A Generative Theory of Relevance. PhD thesis,
University of Massachusetts, Amherst, 2004. [11] C.-H. Lee, R. Greiner, and S. Wang. Using query-specific variance estimates to combine Bayesian classifiers. In Proc. of the 23rd Intl. Conf. on Machine Learning (ICML 2006), pages 529-536. [12] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Info. Processing and Mgmt., 40(5):735-750, 2004. [13] T. Minka. Estimating a Dirichlet distribution. Technical report,
[14] J. Ponte. Advances in Information Retrieval, chapter Language models for relevance feedback, pages 73-96. 2000.
W.B. Croft, ed. [15] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275-281. [16] J. Rocchio. The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval, pages 313-323.
Prentice-Hall, 1971. G. Salton, ed. [17] T. Sakai, T. Manabe, and M. Koyama. Flexible pseudo-relevance feedback via selective sampling. ACM Transactions on Asian Language Information Processing (TALIP), 4(2):111-135, 2005. [18] T. Tao and C. Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 162-169. [19] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans.

Queries, especially short queries, do not provide a complete specification of the information need. Many relevant terms can be absent from queries and terms included may be ambiguous. These issues have been addressed in a large number of previous studies.
Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc. In these studies, however, it has been generally assumed that query is the only element available about the user"s information need. In reality, query is always formulated in a search context. As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments. These factors include, among many others, the user"s domain of interest, knowledge, preferences, etc. All these elements specify the contexts around the query. So we call them context around query in this paper. It has been demonstrated that user"s query should be placed in its context for a correct interpretation.
Recent studies have investigated the integration of some contexts around the query [9][30][23]. Typically, a user profile is constructed to reflect the user"s domains of interest and background. A user profile is used to favor the documents that are more closely related to the profile. However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query. For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored. A possible solution to this problem is to use query-related profiles or models instead of user-centric ones. In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query. This method allows us to select more appropriate query-specific context around the query.
Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science. Using this relation, one would be able to expand the query program with the term computer. However, domain knowledge is available only for a few domains (e.g. Medicine). The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27]. However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query. For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program. Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.
Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV. So the important question is how we can serve these context words in queries to select the appropriate relations to apply. These context words form a context within query. In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer). Although improvements are observed in some cases, they are limited. We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations. The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.
This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains. The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.
Our approach has been tested on several TREC collections. The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary. We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.
This paper is organized as follows. In section 2, we review some related work and introduce the principle of our approach. Section 3 presents our general model. Then sections 4 and 5 describe respectively the domain model and the knowledge model. Section 6 explains the method for parameter training. Experiments are presented in section 7 and conclusions in section 8.
There are many contextual factors in IR: the user"s domain of interest, knowledge about the subject, preference, document recency, and so on [2][14]. Among them, the user"s domain of interest and knowledge are considered to be among the most important ones [20][21]. In this section, we review some of the studies in IR concerning these aspects.
Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query. It can be used in different ways. Most often, a user profile is created to encompass all the domains of interest of a user [23]. In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user. The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user. On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on user"s computer or extracted from user"s search history. In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains. The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile. This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.
A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest. The domains related to a query are then identified according to the query. This will enable us to use a more appropriate query-specific profile, instead of a user-centric one. This approach is used in [18] in which ODP directories are used. However, only a small scale experiment has been carried out. A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them. However, the experiments showed variable results. It remains unclear whether domain models can be effectively used in IR.
In this study, we also model topic domains. We will carry out experiments on both automatic and manual identification of query domains. Domain models will also be integrated with other factors. In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.
Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31]. In both cases, the relations are defined between two single terms such as t1→t2. If a query contains term t1, then t2 is always considered as a candidate for expansion. As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not. For example, program→computer should not be applied to TV program even if the latter contains program. However, little information is available in the relation to help us determine if an application context is appropriate.
To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31]. Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word. Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.
For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.
It is possible to integrate stronger control on the utilization of knowledge. For example, [17] defined strong logical relations to encode knowledge of different domains. If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied. However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.
In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts. As a result, computer will be used to expand queries Java program or program algorithm, but not TV program. This principle is similar to that of [33] for word sense disambiguation. However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts. From this point of view, our approach is more similar to word sense discrimination [27].
In this paper, we use the same approach and we will integrate it into a more global model with other context factors. As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query. Within query context exists in many queries. In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity). Some context words are often used together with it. In these cases, contexts within query are created and can be exploited.
Query profile and other factors Many attempts have been made in IR to create query-specific profiles. We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family. A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the user"s intent behind the query. In order to create a good query model, such a query-specific feedback model should be integrated.
There are many other contextual factors ([26]) that we do not deal with in this paper. However, it seems clear that many factors are complementary. As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context. Both types of contexts have been proven useful [32]. Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment. These terms are often presumed when a user issues a query such as waste cleanup in the domain.
It is useful to add them into the query. We see a clear complementarity among these factors. It is then useful to combine them together in a single IR model.
In this study, we will integrate all the above factors within a unified framework based on language modeling. Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them. This is described in the following section.
In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.
Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1'| +−= (2) where λ is an interpolation parameter and θC the collection model.
In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing. In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query. To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need. In particular, all the related and presumed words should be included in the query model. A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34]. In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms. They are then combined through interpolation.
In this paper, we generalize this approach and integrate more models for the query. Let us use 0 Qθ to denote the original query model,
F Qθ for the feedback model created from feedback documents,
Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.
F Qθ has been used in several previous studies [16][35]. In this paper,
F Qθ is extracted using the 20 blind feedback documents. We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.
Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.
Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model. Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].
The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting). We describe this in the following sections.
MODELS As in previous studies, we exploit a set of documents already classified in each domain. These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP. In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains. By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents. The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.
An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned. These domains have been mapped to ODP categories. It is found that both approaches mentioned above are equally effective and result in comparable performance. Therefore, in this study, we only use the second approach. This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query. This will be explained in detail in our experiments.
Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted. If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.
Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model). Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1'| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]). The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θ"Dom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ' ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35]. It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language. This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).
Table 1. Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query. This can be done manually by the user or automatically by the system using query classification. We will compare both approaches.
Query classification has been investigated in several studies [18][28]. In this study, we use a simple classification method: the selected domain is the one with which the query"s KL-divergence score is the lowest, i.e.: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22]. The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise. Therefore, we only retain the top 100 strongest terms. The same strategy is used for Knowledge model.
Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large. This is particularly true for large domains such as Science and technology defined in TREC queries. Using such a large domain model as the background can introduce much noise terms. Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query. These documents are the top-ranked documents retrieved with the original query within the domain. This approach is indeed a combination of domain and feedback models. In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used.
TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.
In general, a term relation can be represented as A→B. Both A and B have been restricted to single terms in previous studies. A single term in A means that the relation is applicable to all the queries containing that term. As we explained earlier, this is the source of many wrong applications. The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query. For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query. The term added in the condition specifies a stricter context to apply the relation. We call this type of relation context-dependent relation.
In principle, the addition is not restricted to one term. However, we will make this restriction due to the following reasons: • User queries are usually very short. Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.
The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13]. Here, we use a simple co-occurrence analysis. Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.
In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related. We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.
This is a direct extension of the translation model proposed in [3] to our context-dependent relations. The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used.
There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3). As the parameter λ only affects document model, we will set it to the same value in all our experiments. The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.
The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction. We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged. Each direction is searched in turn, until no improvement in MAP is observed.
In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected.
The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3. The choice of this test collection is due to the availability of manually specified domain for each query. This allows us to compare with an approach using automatic domain identification. Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests. Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1. We can see that the distribution varies strongly between domains and between the two query sets.
We have also tested on TREC 7 and 8 data. For this series of tests, each collection is used in turn as training data while the other is used for testing. Some statistics of the data are described in Tab. 2.
All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used. Some queries (4, 5 and 3 in the three query sets) only contain one word. For these queries, knowledge model is not applicable.
On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically? How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries. How do they compare?
On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.
Finally, we will see the impact of each component model when all the factors are combined.
Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback. In all the experiments, document models are created using Jelinek-Mercer smoothing. This choice is made according to the observation in [36] that the method performs very well for long queries. In our case, as queries are expanded, they perform similarly to long queries. In our preliminary tests, we also found this method performed better than the other methods (e.g. Dirichlet), especially for the main baseline method with Feedback model. Table 3 shows the retrieval effectiveness on all the collections.
This model is combined with both baseline models (with or without feedback). We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries. This latter selects expansion terms with strongest global relation to the query. This relation is measured by the sum of relations to each of the query terms. This method is equivalent to [24]. It is also similar to the translation model [3]. We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.
M edical&Bio.M ilitaryPolitics Sci.&Tech.
U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1. Distribution of domains Table 2. TREC collection statistics Collection Document Size (GB) Voc. # of Doc. Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4. T-test is also performed for statistical significance.
As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used. All the improvements over cooccurrence model are statistically significant (this is not shown in the table). The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion. This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms. The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work
Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man
develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport
japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower. However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3. This demonstrates that the impacts produced by feedback and term relations are different and complementary.
In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.
Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.
C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.
Strategies for using domain models: U1 - The domain model is determined by the user manually.
U2 - The domain model is determined by the system.
We test strategies C1 and C2. In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models. The same method is used on queries 1-50 to tune the parameters.
Table 3. Baseline models Unigram Model Coll. Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4. Knowledge models Co-occurrence Knowledge model Coll. Measure Without FB With FB Without FB With FB AvgP
(+20.00%)++
(+3.75%)**
(+37.83%)++
(+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP
(+10.08%)++
(+8.00%)*
(+30.25%)++
(+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP
(+5.53%)
(+0.58%)
(+14.12%)++
(+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.) Table 5. Domain models with relevant documents (C1) Domain Sub-Domain Coll. Measure Without FB With FB Without FB With FB AvgP
(+8.28%)++
(+4.69%)**
(+22.17%)++
(+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP
(+3.56%)++
(+9.79%)*
(+11.23%)++
(+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP
(+2. 30%)
(+1.65%)
(+7.37%)
(+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6. Domain models with top-100 documents (C2) Domain Sub-Domain Coll. Measure Without FB With FB Without FB With FB AvgP
(+9.43%)++
(+4.78%)**
(+14.59%)++
(+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP
(+6.58%)++
(+10.06%)**
(+7.79%)++
(+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP
(+1.97%)
(+1.38%)
(+2.26%)
(+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain). In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).
First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases. The improvements on Disks 1-3 and TREC7 are statistically significant. However, the improvement scales are smaller than using Feedback and Relation models. Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models. In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.
Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5). In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models. This may seem surprising.
An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.
Relevant documents for all in-domain queries vary greatly.
Therefore, in some large domains, characteristic terms have variable effects on queries. On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents. Thus both strategies produce very similar effects. This result opens the door for a simpler method that does not require relevance judgments, for example using search history.
Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5). However, once Feedback model is used, the advantage disappears. On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain. It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred. On the other hand, sub-domain models capture similar characteristics to Feedback model. So when the latter is used, sub-domain models become superfluous. However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences. This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents.
It is not realistic to always ask users to specify a domain for their queries. Here, we examine the possibility to automatically identify query domains. Table 7 shows the results with this strategy using both strategies for domain model construction. We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models). This shows that automatic domain identification is a way to select domain model as effective as manual identification. This also demonstrates the feasibility to use domain models for queries when no domain information is provided.
Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.
This is much lower than the above 80% rates reported in [18]. A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g. International relations,
International politics, Politics). However, in this situation, wrong domains assigned to queries are not always irrelevant and useless. For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query. Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models.
The results with the complete model are shown in Table 8. This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model. We have tested both strategies to create domain models, but the differences between them are very small.
So we only report the results with the relevant documents.
Our first observation is that the complete models produce the best results. All the improvements over the baseline model (with feedback) are statistically significant. This result confirms that the integration of contextual factors is effective. Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.
Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6. We see that the most important factor is Feedback model. This is also the single factor which produced the highest improvements over the original query model. This observation seems to indicate that this model has the highest capability to capture the information need behind the query. However, even with lower weights, the other models do have strong impacts on the final effectiveness. This demonstrates the benefit of integrating more contextual factors in IR.
Table 7. Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll. Measure Without FB With FB Without FB With FB AvgP
(+5.10%)++
(+4.27%)**
(+6.37%)++
(+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8. Complete models (C1) All Doc. Domain Coll. Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A
AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A
Traditional IR approaches usually consider the query as the only element available for the user information need. Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile. In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased. Similarly to some previous studies, we propose to model topic domains instead of the user.
Previous investigations on context focused on factors around the query. We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.
We have integrated the above contextual factors, together with feedback model, in a single language model. Our experimental results strongly confirm the benefit of using contexts in IR. This work also shows that the language modeling framework is appropriate for integrating many contextual factors.
This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains. It would also be interesting to test the method on Web search using user search history. We will investigate these problems in our future research.
[1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP"06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval"93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR"99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche d"information contextuelle, Conf. en Recherche d"Information et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185,
[6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R.,
Robbins, D. C., Stuff I've seen: a system for personal information retrieval and re-use, SIGIR'03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR"06, pp.115-122,
[11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval. SIGIR"05, pp. 290-297,
[12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.
SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks,
WEBKDD"05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR"01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR"04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM"02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR '04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service,
JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models. Inf. Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D.,
Edmonds, A., Adar, E., Breuel, T., Personalized Search,
Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Concept based query expansion.
SIGIR"93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang,
Q. Query enrichment for web-query classification.
ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR"05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR"05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations. SIGIR"94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR"96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y.,
Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR"06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM"01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.

Users of information retrieval systems are required to express complex information needs in terms of Boolean expressions, a short list of keywords, a sentence, a question, or possibly a longer narrative. A great deal of information is lost during the process of translating from the information need to the actual query. For this reason, there has been a strong interest in query expansion techniques. Such techniques are used to augment the original query to produce a representation that better reflects the underlying information need.
Query expansion techniques have been well studied for various models in the past and have shown to significantly improve effectiveness in both the relevance feedback and pseudo-relevance feedback setting [12, 21, 28, 29].
Recently, a Markov random field (MRF) model for information retrieval was proposed that goes beyond the simplistic bag of words assumption that underlies BM25 and the (unigram) language modeling approach to information retrieval [20, 22]. The MRF model generalizes the unigram, bigram, and other various dependence models [14]. Most past term dependence models have failed to show consistent, significant improvements over unigram baselines, with few exceptions [8]. The MRF model, however, has been shown to be highly effective across a number of tasks, including ad hoc retrieval [14, 16], named-page finding [16], and Japanese language web search [6].
Until now, the model has been solely used for ranking documents in response to a given query. In this work, we show how the model can be extended and used for query expansion using a technique that we call latent concept expansion (LCE). There are three primary contributions of our work.
First, LCE provides a mechanism for combining term dependence with query expansion. Previous query expansion techniques are based on bag of words models. Therefore, by performing query expansion using the MRF model, we are able to study the dynamics between term dependence and query expansion.
Next, as we will show, the MRF model allows arbitrary features to be used within the model. Query expansion techniques in the past have implicitly only made use of term occurrence features. By using more robust feature sets, it is possible to produce better expansion terms that discriminate between relevant and non-relevant documents better.
Finally, our proposed approach seamlessly provides a mechanism for generating both single and multi-term concepts.
Most previous techniques, by default, generate terms independently. There have been several approaches that make use of generalized concepts, however such approaches were somewhat heuristic and done outside of the model [19, 28].
Our approach is both formally motivated and a natural extension of the underlying model.
The remainder of this paper is laid out as follows. In Section 2 we describe related query expansion approaches.
Section 3 provides an overview of the MRF model and details our proposed latent concept expansion technique. In Section 4 we evaluate our proposed model and analyze the results. Finally, Section 5 concludes the paper and summarizes the major results.
One of the classic and most widely used approaches to query expansion is the Rocchio algorithm [21]. Rocchio"s approach, which was developed within the vector space model, reweights the original query vector by moving the weights towards the set of relevant or pseudo-relevant documents and away from the non-relevant documents. Unfortunately, it is not possible to formally apply Rocchio"s approach to a statistical retrieval model, such as language modeling for information retrieval.
A number of formalized query expansion techniques have been developed for the language modeling framework, including Zhai and Lafferty"s model-based feedback and Lavrenko and Croft"s relevance models [12, 29]. Both approaches attempt to use pseudo-relevant or relevant documents to estimate a better query model.
Model-based feedback finds the model that best describes the relevant documents while taking a background (noise) model into consideration. This separates the content model from the background model. The content model is then interpolated with the original query model to form the expanded query.
The other technique, relevance models, is more closely related to our work. Therefore, we go into the details of the model. Much like model-based feedback, relevance models estimate an improved query model. The only difference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant documents.
Instead, they model a more generalized notion of relevance, as we now show.
Given a query Q, a relevance model is a multinomial distribution, P(·|Q), that encodes the likelihood of each term given the query as evidence. It is computed as: P(w|Q) = D P(w|D)P(D|Q) ≈ D∈RQ P(w|D)P(Q|D)P(D) w D∈RQ P(w|D)P(Q|D)P(D) (1) where RQ is the set of documents that are relevant or pseudorelevant to query Q. In the pseudo-relevant case, these are the top ranked documents for query Q. Furthermore, it is assumed that P(D) is uniform over this set. These mild assumptions make computing the Bayesian posterior more practical.
After the model is estimated, documents are ranked by clipping the relevance model by choosing the k most likely terms from P(·|Q). This clipped distribution is then interpolated with with the original, maximum likelihood query model [1]. This can be thought of as expanding the original query by k weighted terms. Throughout the remainder of this work, we refer to this instantiation of relevance models as RM3.
There has been relatively little work done in the area of query expansion in the context of dependence models [9].
However, there have been several attempts to expand using multi-term concepts. Xu and Croft"s local context analysis (LCA) method combined passage-level retrieval with concept expansion, where concepts were single terms and phrases [28]. Expansion concepts were chosen and weighted using a metric based on co-occurrence statistics. However, it is not clear based on the analysis done how much the phrases helped over the single terms alone.
Papka and Allan investigate using relevance feedback to perform multi-term concept expansion for document routing [19]. The concepts used in their work are more general than those used in LCA, and include InQuery query language structures, such as #UW50(white house), which corresponds to the concept the terms white and house occur, in any order, within 50 terms of each other. Results showed that combining single term and large window multi-term concepts significantly improved effectiveness. However, it is unclear whether the same approach is also effective for ad hoc retrieval, due to the differences in the tasks.
This section details our proposed latent concept expansion technique. As mentioned previously, the technique is an extension of the MRF model for information retrieval [14].
Therefore, we begin by providing an overview of the MRF model and our proposed extensions.
Markov random fields, which are undirected graphical models, provide a compact, robust way of modeling a joint distribution. Here, we are interested in modeling the joint distribution over a query Q = q1, . . . , qn and a document D. It is assumed the underlying distribution over pairs of documents and queries is a relevance distribution. That is, sampling from the distribution gives pairs of documents and queries, such that the document is relevant to the query.
A MRF is defined by a graph G and a set of non-negative potential functions over the cliques in G. The nodes in the graph represent the random variables and the edges define the independence semantics of the distribution. A MRF satisfies the Markov property, which states that a node is independent of all of its non-neighboring nodes given observed values for its neighbors.
Given a graph G, a set of potentials ψi, and a parameter vector Λ, the joint distribution over Q and D is given by: PG,Λ(Q, D) = 1 ZΛ c∈C(G) ψ(c; Λ) where Z is a normalizing constant. We follow common convention and parameterize the potentials as ψi(c; Λ) = exp[λifi(c)], where fi(c) is a real-valued feature function.
Given a query Q, the graph G can be constructed in a number of ways. However, following previous work, we consider three simple variants [14]. These variants are full independence, where each query term is independent of each other given a document, sequential dependence, which assumes a dependence exists between adjacent query terms, and full dependence, which makes no independence assumptions.
MRFs are commonly parameterized based on the maximal cliques of G. However, such a parameterization is too coarse for our needs. We need a parameterization that allows us to associate feature functions with cliques on a more fine grained level, while keeping the number of features, and thus the number of parameters, reasonable. Therefore, we allow cliques to share feature functions and parameters based on clique sets. That is, all of the cliques within a clique set are associated with the same feature function and share a single parameter. This effectively ties together the parameters of the features associated with each set, which significantly reduces the number of parameters while still providing a mechanism for fine-tuning on the level of clique sets.
We propose seven clique sets for use with information retrieval. The first three clique sets consist of cliques that contain one or more query terms and the document node.
Features over these cliques should encode how well the terms in the clique configuration describe the document. These sets are: • TD - set of cliques containing the document node and exactly one query term. • OD - set of cliques containing the document node and two or more query terms that appear in sequential order within the query. • UD - set of cliques containing the document node and two or more query terms that appear in any order within the query.
Note that UD is a superset of OD. By tying the parameters among the cliques within each set we can control how much influence each type gets. This also avoids the problem of trying to determine how to estimate weights for each clique within the sets. Instead, we now must only estimate a single parameter per set.
Next, we consider cliques that only contain query term nodes. These cliques, which were not considered in [14], are defined in an analogous way to those just defined, except the the cliques are only made up of query term nodes and do not contain the document node. Feature functions over these cliques should capture how compatible query terms are to one another. These clique features may take on the form of language models that impose well-formedness of the terms.
Therefore, we define following query-dependent clique sets: • TQ - set of cliques containing exactly one query term. • OQ - set of cliques containing two or more query terms that appear in sequential order within the query. • UQ - set of cliques containing two or more query terms that appear in any order within the query.
Finally, there is the clique that only contains the document node. Features over this node can be used as a type of document prior, encoding document-centric properties.
This trivial clique set is then: • D - clique set containing only the singleton node D We note that our clique sets form a set cover over the cliques of G, but are not a partition, since some cliques appear in multiple clique sets.
After tying the parameters in our clique sets together and using the exponential potential function form, we end up with the following simplified form of the joint distribution: log PG,Λ(Q, D) = λTD c∈TD fTD (c) + λOD c∈OD fOD (c) + λUD c∈UD fUD (c) FDQ(D,Q) - document and query dependent + λTQ c∈TQ fTQ (c) + λOQ c∈OQ fOQ (c) + λUQ c∈UQ fUQ (c) FQ(Q) - query dependent + λDfD(D) FD(D) - document dependent − log ZΛ document + query independent where FDQ, FQ, and FD are convenience functions defined by the document and query dependent, query dependent, and document dependent components of the joint distribution, respectively. These will be used to simplify and clarify expressions derived throughout the remainder of the paper.
Any arbitrary feature function over clique configurations can be used in the model. The correct choice of features depends largely on the retrieval task and the evaluation metric. Therefore, there is likely not to be a single, universally applicable set of features.
To provide an idea of the range of features that can be used, we now briefly describe possible types of features that could be used. Possible query term dependent features include tf, idf, named entities, term proximity, and text style to name a few. Many types of document dependent features can be used, as well, including document length, PageRank, readability, and genre, among others.
Since it is not our goal here to find optimal features, we use a simple, fixed set of features that have been shown to be effective in previous work [14]. See Table 1 for a list of features used. These features attempt to capture term occurrence and term proximity. Better feature selection in the future will likely lead to improved effectiveness.
Given a query Q, we wish to rank documents in descending order according to PG,Λ(D|Q). After dropping document independent expressions from log PG,Λ(Q, D), we derive the following ranking function: PG,Λ(D|Q) rank = FDQ(D, Q) + FD(D) (2) which is a simple weighted linear combination of feature functions that can be computed efficiently for reasonable graphs.
Now that the model has been fully specified, the final step is to estimate the model parameters. Although MRFs are generative models, it is inappropriate to train them using Feature Value fTD (qi, D) log (1 − α) tfqi,D |D| + α cfqi |C| fOD (qi, qi+1 . . . , qi+k, D) log (1 − β) tf#1(qi...qi+k),D |D| + β cf#1(qi...qi+k) |C| fUD (qi, ..., qj, D) log (1 − β) tf#uw(qi...qj ),D |D| + β cf#uw(qi...qj ) |C| fTQ (qi) − log cfqi |C| fOQ (qi, qi+1 . . . , qi+k) − log cf#1(qi...qi+k) |C| fUQ (qi, ..., qj) − log cf#uw(qi...qj ) |C| fD 0 Table 1: Feature functions used in Markov random field model. Here, tfw,D is the number of times term w occurs in document D, tf#1(qi...qi+k),D denotes the number of times the exact phrase qi . . . qi+k occurs in document D, tf#uw(qi...qj ),D is the number of times the terms qi, . . . qj appear ordered or unordered within a window of N terms, and |D| is the length of document D. The cf and |C| values are analogously defined on the collection level. Finally, α and β are model hyperparameters that control smoothing for single term and phrase features, respectively. conventional likelihood-based approaches because of metric divergence [17]. That is, the maximum likelihood estimate is unlikely to be the estimate that maximizes our evaluation metric. For this reason, we discriminatively train our model to directly maximize the evaluation metric under consideration [14, 15, 25]. Since our parameter space is small, we make use of a simple hill climbing strategy, although other more sophisticated approaches are possible [10].
In this section we describe how this extended MRF model can be used in a novel way to generate single and multiterm concepts that are topically related to some original query. As we will show, the concepts generated using our technique can be used for query expansion or other tasks, such as suggesting alternative query formulations.
We assume that when a user formulates their original query, they have some set of concepts in mind, but are only able to express a small number of them in the form of a query. We treat the concepts that the user has in mind, but did not explicitly express in the query, as latent concepts.
These latent concepts can consist of a single term, multiple terms, or some combination of the two. It is, therefore, our goal to recover these latent concepts given some original query.
This can be accomplished within our framework by first expanding the original graph G to include the type of concept we are interested in generating. We call this expanded graph H. In Figure 1, the middle graph provides an example of how to construct an expanded graph that can generate single term concepts. Similarly, the graph on the right illustrates an expanded graph that generates two term concepts.
Although these two examples make use of the sequential dependence assumption (i.e. dependencies between adjacent query terms), it is important to note that both the original query and the expansion concepts can use any independence structure.
After H is constructed, we compute PH,Λ(E|Q), a probability distribution over latent concepts, according to: PH,Λ(E|Q) = D∈R PH,Λ(Q, E, D) D∈R E PH,Λ(Q, E, D) where R is the universe of all possible documents and E is some latent concept that may consist of one or more terms. Since it is not practical to compute this summation, we must approximate it. We notice that PH,Λ(Q, E, D) is likely to be peaked around those documents D that are highly ranked according to query Q. Therefore, we approximate PH,Λ(E|Q) by only summing over a small subset of relevant or pseudo-relevant documents for query Q. This is computed as follows: PH,Λ(E|Q) ≈ D∈RQ PH,Λ(Q, E, D) D∈RQ E PH,Λ(Q, E, D) (3) ∝ D∈RQ exp FQD(Q, D) + FD(D) + FQD(E, D) + FQ(E) where RQ is a set of relevant or pseudo-relevant documents for query Q and all clique sets are constructed using H.
As we see, the likelihood contribution for each document in RQ is a combination of the original query"s score for the document (see Equation 2), concept E"s score for the document, and E"s document-independent score. Therefore, this equation can be interpreted as measuring how well Q and E account for the top ranked documents and the goodness of E, independent of the documents. For maximum robustness, we use a different set of parameters for FQD(Q, D) and FQD(E, D), which allows us to weight the term, ordered, and unordered window features differently for the original query and the candidate expansion concept.
To use this framework for query expansion, we first choose an expansion graph H that encodes the latent concept structure we are interested in expanding the query using. We then select the k latent concepts with the highest likelihood given by Equation 3. A new graph G is constructed by augmenting the original graph G with the k expansion concepts E1, . . . , Ek. Finally, documents are ranked according to PG ,Λ(D|Q, E1, . . . , Ek) using Equation 2.
Inspecting Equations 1 and 3 reveals the close connection that exists between LCE and relevance models. Both Figure 1: Graphical model representations of relevance modeling (left), latent concept expansion using single term concepts (middle), and latent concept expansion using two term concepts (right) for a three term query. models essentially compute the likelihood of a term (or concept) in the same manner. It is easy to see that just as the MRF model can be viewed as a generalization of language modeling, so too can LCE be viewed as a generalization of relevance models.
There are important differences between MRFs/LCE and unigram language models/relevance models. See Figure 1 for graphical model representations of both models.
Unigram language models and relevance models are based on the multinomial distribution. This distributional assumption locks the model into the bag of words representation and the implicit use of term occurrence features. However, the distribution underlying the MRF model allows us to move beyond both of these assumptions, by modeling both dependencies between query terms and allowing arbitrary features to be explicitly used.
Moving beyond the simplistic bag of words assumption in this way results in a general, robust model and, as we show in the next section, translates into significant improvements in retrieval effectiveness.
In order to better understand the strengths and weaknesses of our technique, we evaluate it on a wide range of data sets. Table 2 provides a summary of the TREC data sets considered. The WSJ, AP, and ROBUST collections are smaller and consist entirely of newswire articles, whereas WT10g and GOV2 are large web collections. For each data set, we split the available topics into a training and test set, where the training set is used solely for parameter estimation and the test set is used for evaluation purposes.
All experiments were carried out using a modified version of Indri, which is part of the Lemur Toolkit [18, 23]. All collections were stopped using a standard list of 418 common terms and stemmed using a Porter stemmer. In all cases, only the title portion of the TREC topics are used to construct queries. We construct G using the sequential dependence assumption for all data sets [14].
We now investigate how well our model performs in practice in a pseudo-relevance feedback setting. We compare unigram language modeling (with Dirichlet smoothing), the MRF model (without expansion), relevance models, and LCE to better understand how each model performs across the various data sets.
For the unigram language model, the smoothing parameter was trained. For the MRF model, we train the model parameters (i.e. Λ) and model hyperparameters (i.e. α, β).
For RM3 and LCE, we also train the number of pseudoName Description # Docs Train Topics Test Topics WSJ Wall St.
Journal 87-92 173,252 51-150 151-200 AP Assoc. Press 88-90 242,918 51-150 151-200 ROBUST Robust 2004 data 528,155 301-450 601-700 WT10g TREC Web collection 1,692,096 451-500 501-550 GOV2 2004 crawl of .gov domain 25,205,179 701-750 751-800 Table 2: Overview of TREC collections and topics. relevant feedback documents used and the number of expansion terms.
We begin by evaluating how well our model performs when expanding using only single terms. Before we describe and analyze the results, we explicitly state how expansion term likelihoods are computed under this setup (i.e. using the sequential dependence assumption, expanding with single term concepts, and using our feature set). The expansion term likelihoods are computed as follows: PH,Λ(e|Q) ∝ D∈RQ exp λTD w∈Q log (1 − α) tfw,D |D| + α cfw |C| + λOD b∈Q log (1 − β) tf#1(b),D |D| + β cf#1(b) |C| + λUD b∈Q log (1 − β) tf#uw(b),D |D| + β cf#uw(b) |C| + log (1 − α) tfe,D |D| + α cfe |C| λTD cfe |C| λTQ (4) where b ∈ Q denotes the set of bigrams in Q. This equation clearly shows how LCE differs from relevance models. When we set λTD = λT,D = 1 and all other parameters to 0, we obtain the exact formula that is used to compute term likelihoods in the relevance modeling framework. Therefore,
LCE adds two very important factors to the equation. First, it adds the ordered and unordered window features that are applied to the original query. Second, it applies an intuitive tf.idf-like form to the candidate expansion term w. The idf factor, which is not present in relevance models, plays an important role in expansion term selection. <= −100% (−100%, −75%] (−75%, −50%] (−50%, −25%] (−25%, 0%] (0%, 25%] (25%, 50%] (50%, 75%] (75%, 100%] > 100% RM3 LCE 05101520 AP <= −100% (−100%, −75%] (−75%, −50%] (−50%, −25%] (−25%, 0%] (0%, 25%] (25%, 50%] (50%, 75%] (75%, 100%] > 100% RM3 LCE 05101520253035 ROBUST <= −100% (−100%, −75%] (−75%, −50%] (−50%, −25%] (−25%, 0%] (0%, 25%] (25%, 50%] (50%, 75%] (75%, 100%] > 100% RM3 LCE 0510152025 WT10G Figure 2: Histograms that demonstrate and compare the robustness of relevance models (RM3) and latent concept expansion (LCE) with respect to the query likelihood model (QL) for the AP, ROBUST, and WT10G data sets.
The results, evaluated using mean average precision, are given in Table 3. As we see, the MRF model, relevance models, and LCE always significantly outperform the unigram language model. In addition, LCE shows significant improvements over relevance models across all data sets. The relative improvements over relevance models is 6.9% for AP,
Furthermore, LCE shows small, but not significant, improvements over relevance modeling for metrics such as precision at 5, 10, and 20. However, both relevance modeling and LCE show statistically significant improvements in such metrics over the unigram language model.
Another interesting result is that the MRF model is statistically equivalent to relevance models on the two web data sets. In fact, the MRF model outperforms relevance models on the WT10g data set. This reiterates the importance of non-unigram, proximity-based features for content-based web search observed previously [14, 16].
Although our model has more free parameters than relevance models, there is surprisingly little overfitting. Instead, the model exhibits good generalization properties.
We also investigated expanding using both single and two word concepts. For each query, we expanded using a set of single term concepts and a set of two term concepts. The sets were chosen independently. Unfortunately, only negligible increases in mean average precision were observed.
This result may be due to the fact that strong correlations exist between the single term expansion concepts. We found that the two word concepts chosen often consisted of two highly correlated terms that are also chosen as single term concepts. For example, the two term concept stock market was chosen while the single term concepts stock and market were also chosen. Therefore, many two word concepts are unlikely to increase the discriminative power of the expanded query. This result suggests that concepts should be chosen according to some criteria that also takes novelty, diversity, or term correlations into account.
Another potential issue is the feature set used. Other feature sets may ultimately yield different results, especially if they reduce the correlation among the expansion concepts.
Therefore, our experiments yield no conclusive results with regard to expansion using multi-term concepts. Instead, the results introduce interesting open questions and directions for future exploration.
LM MRF RM3 LCE WSJ .3258 .3425α .3493α .3943αβγ AP .2077 .2147α .2518αβ .2692αβγ ROBUST .2920 .3096α .3382αβ .3601αβγ WT10g .1861 .2053α .1944α .2269αβγ GOV2 .3234 .3520α .3656α .3924αβγ Table 3: Test set mean average precision for language modeling (LM), Markov random field (MRF), relevance models (RM3), and latent concept expansion (LCE). The superscripts α, β, and γ indicate statistically significant improvements (p < 0.05) over LM, MRF, and RM3, respectively.
As we have shown, relevance models and latent concept expansion can significantly improve retrieval effectiveness over the baseline query likelihood model. In this section we analyze the robustness of these two methods. Here, we define robustness as the number queries whose effectiveness are improved/hurt (and by how much) as the result of applying these methods. A highly robust expansion technique will significantly improve many queries and only minimally hurt a few.
Figure 2 provides an analysis of the robustness of relevance modeling and latent concept expansion for the AP,
ROBUST, and WT10G data sets. The analysis for the two data sets not shown is similar. The histograms provide, for various ranges of relative decreases/increases in mean average precision, the number of queries that were hurt/improved with respect to the query likelihood baseline.
As the results show, LCE exhibits strong robustness for each data set. For AP, relevance models improve 38 queries and hurt 11, whereas LCE improves 35 and hurts 14.
Although relevance models improve the effectiveness of 3 more queries than LCE, the relative improvement exhibited by LCE is significantly larger. For the ROBUST data set, relevance models improve 67 queries and hurt 32, and LCE improves 77 and hurts 22. Finally, for the WT10G collection, relevance models improve 32 queries and hurt 16, and LCE improves 35 and hurts 14. As with AP, the amount of improvement exhibited by the LCE versus relevance models is significantly larger for both the ROBUST and WT10G data sets. In addition, when LCE does hurt performance, it is less likely to hurt as much as relevance modeling, which is a desirable property. 1 word concepts 2 word concepts 3 word concepts telescope hubble telescope hubble space telescope hubble space telescope hubble telescope space space hubble space space telescope hubble mirror telescope mirror space telescope NASA NASA telescope hubble hubble telescope astronomy launch mirror telescope NASA hubble space astronomy telescope NASA space telescope mirror shuttle telescope space telescope space NASA test hubble mirror hubble telescope mission new NASA hubble mirror mirror mirror discovery telescope astronomy space telescope launch time telescope optical space telescope discovery universe hubble optical shuttle space telescope optical telescope discovery hubble telescope flaw light telescope shuttle two hubble space Table 4: Fifteen most likely one, two, and three word concepts constructed using the top 25 documents retrieved for the query hubble telescope achievements on the ROBUST collection.
Overall, LCE improves effectiveness for 65%-80% of queries, depending on the data set. When used in combination with a highly accurate query performance prediction system, it may be possible to selectively expand queries and minimize the loss associated with sub-baseline performance.
Although we found that expansion using multi-term concepts failed to produce conclusive improvements in effectiveness, there are other potential tasks that these concepts may be useful for, such as query suggestion/reformulation, summarization, and concept mining. For example, for a query suggestion task, the original query could be used to generate a set of latent concepts which correspond to alternative query formulations.
Although evaluating our model on these tasks is beyond the scope of this work, we wish to show an illustrative example of the types of concepts generated using our model. In Table 4, we present the most likely one, two, and three term concepts generated using LCE for the query hubble telescope achievements using the top 25 ranked documents from the ROBUST collection.
It is well known that generating multi-term concepts using a unigram-based model produces unsatisfactory results, since it fails to consider term dependencies. This is not the case when generating multi-term concepts using our model. Instead, a majority of the concepts generated are well-formed and meaningful. There are several cases where the concepts are less coherent, such as mirror mirror mirror.
In this case, the likelihood of the term mirror appearing in a pseudo-relevant document outweighs the language modeling features (e.g. fOQ ), which causes this non-coherent concept to have a high likelihood. Such examples are in the minority, however.
Not only are the concepts generated well-formed and meaningful, but they are also topically relevant to the original query. As we see, all of the concepts generated are on topic and in some way related to the Hubble telescope. It is interesting to see that the concept hubble telescope flaw is one of the most likely three term concepts, given that it is somewhat contradictory to the original query. Despite this contradiction, documents that discuss the telescope flaws are also likely to describe the successes, as well, and therefore this is likely to be a meaningful concept.
One important thing to note is that the concepts LCE generates are of a different nature than those that would be generated using a bigram relevance model. For example, a bigram model would be unlikely to generate the concept telescope space NASA, since none of the bigrams that make up the concept have high likelihood. However, since our model is based on a number of different features over various types of cliques, it is more general and robust than a bigram model.
Although we only provided the concepts generated for a single query, we note that the same analysis and conclusions generalize across other data sets, with coherent, topically related concepts being consistently generated using LCE.
Our latent concept expansion technique captures two semiorthogonal types of dependence. In information retrieval, there has been a long-term interest in understanding the role of term dependence. Out of this research, two broad types of dependencies have been identified.
The first type of dependence is syntactic dependence. This type of dependence covers phrases, term proximity, and term co-occurrence [2, 4, 5, 7, 26]. These methods capture the fact that queries implicitly or explicitly impose a certain set of positional dependencies.
The second type is semantic dependence. Examples of semantic dependence are relevance feedback, pseudo-relevance feedback, synonyms, and to some extent stemming [3]. These techniques have been explored on both the query and document side. On the query side, this is typically done using some form of query expansion, such as relevance models or LCE. On the document side, this is done as document expansion or document smoothing [11, 13, 24].
Although there may be some overlap between syntactic and semantic dependencies, they are mostly orthogonal. Our model uses both types of dependencies. The use of phrase and proximity features within the model captures syntactic dependencies, whereas LCE captures query-side semantic dependence. This explains why the initial improvement in effectiveness achieved by using the MRF model is not lost after query expansion. If the same types of dependencies were capture by both syntactic and semantic dependencies,
LCE would be expected to perform about equally as well as relevance models. Therefore, by modeling both types of dependencies we see an additive effect, rather than an absorbing effect.
An interesting area of future work is to determine whether or not modeling document-side semantic dependencies can add anything to the model. Previous results that have combined query- and document-side semantic dependencies have shown mixed results [13, 27].
In this paper we proposed a robust query expansion technique called latent concept expansion. The technique was shown to be a natural extension of the Markov random field model for information retrieval and a generalization of relevance models. LCE is novel in that it performs single or multi-term expansion within a framework that allows the modeling of term dependencies and the use of arbitrary features, whereas previous work has been based on the bag of words assumption and term occurrence features.
We showed that the technique can be used to produce high quality, well formed, topically relevant multi-term expansion concepts. The concepts generated can be used in an alternative query suggestion module. We also showed that the model is highly effective. In fact, it achieves significant improvements in mean average precision over relevance models across a selection of TREC data sets. It was also shown the MRF model itself, without any query expansion, outperforms relevance models on large web data sets. This reconfirms previous observations that modeling dependencies via the use of proximity features within the MRF has more of an impact on larger, noisier collections than smaller, well-behaved ones.
Finally, we reiterated the importance of choosing expansion terms that model relevance, rather than the relevant documents and showed how LCE captures both syntactic and query-side semantic dependencies. Future work will look at incorporating document-side dependencies, as well.
Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #CNS-0454018, in part by ARDA and NSF grant #CCF-0205575, and in part by Microsoft Live Labs. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the sponsor.
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey,
X. Li, M. D. Smucker, and C. Wade. UMass at TREC 2004: Novelty and HARD. In Online proceedings of the 2004 Text Retrieval Conf., 2004. [2] C. L. A. Clarke and G. V. Cormack. Shortest-substring retrieval and ranking. ACM Trans. Inf. Syst., 18(1):44-78, 2000. [3] K. Collins-Thompson and J. Callan. Query expansion using random walk models. In Proc. 14th Intl. Conf. on Information and Knowledge Management, pages 704-711, 2005. [4] W. B. Croft. Boolean queries and term dependencies in probabilistic retrieval models. Journal of the American Society for Information Science, 37(4):71-77, 1986. [5] W. B. Croft, H. Turtle, and D. Lewis. The use of phrases and structured queries in information retrieval. In Proc. 14th Ann.
Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 32-45, 1991. [6] K. Eguchi. NTCIR-5 query expansion experiments using term dependence models. In Proc. of the Fifth NTCIR Workshop Meeting on Evaluation of Information Access Technologies, pages 494-501, 2005. [7] J. Fagan. Automatic phrase indexing for document retrieval: An examination of syntactic and non-syntactic methods. In Proc. tenth Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 91-101, 1987. [8] J. Gao, J. Nie, G. Wu, and G. Cao. Dependence language model for information retrieval. In Proc. 27th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 170-177, 2004. [9] D. Harper and C. J. van Rijsbergen. An evaluation of feedback in document retrieval using co-occurrence data. Journal of Documentation, 34(3):189-216, 1978. [10] T. Joachims. A support vector method for multivariate performance measures. In Proc. of the International Conf. on Machine Learning, pages 377-384, 2005. [11] O. Kurland and L. Lee. Corpus structure, language models, and ad-hoc information retrieval. In Proc. 27th Ann. Intl.
ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 194-201, 2004. [12] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. 24th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 120-127, 2001. [13] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. 27th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 186-193, 2004. [14] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. 28th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 472-479, 2005. [15] D. Metzler and W. B. Croft. Linear feature based models for information retrieval. Information Retrieval, to appear, 2006. [16] D. Metzler, T. Strohman, Y. Zhou, and W. B. Croft. Indri at terabyte track 2005. In Online proceedings of the 2005 Text Retrieval Conf., 2005. [17] W. Morgan, W. Greiff, and J. Henderson. Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach. Technical report, MITRE, 2004. [18] P. Ogilvie and J. P. Callan. Experiments using the lemur toolkit. In Proc. of the Text REtrieval Conf., 2001. [19] R. Papka and J. Allan. Why bigger windows are better than smaller ones. Technical report, University of Massachusetts,
Amherst, 1997. [20] S. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. Okapi at trec-3. In Online proceedings of the Third Text Retrieval Conf., pages 109-126, 1995. [21] J. J. Rocchio. Relevance Feedback in Information Retrieval, pages 313-323. Prentice-Hall, 1971. [22] F. Song and W. B. Croft. A general language model for information retrieval. In Proc. eighth international conference on Information and knowledge management (CIKM 99), pages 316-321, 1999. [23] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language model-based serach engine for complex queries. In Proc. of the International Conf. on Intelligence Analysis,
[24] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In Proc. of HLT/NAACL, pages 407-414, 2006. [25] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Proc. of Advances in Neural Information Processing Systems (NIPS 2003), 2003. [26] C. J. van Rijsbergen. A theoretical basis for the use of cooccurrence data in information retrieval. Journal of Documentation, 33(2):106-119, 1977. [27] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In Proc. 29th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 178-185, 2006. [28] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans.
Inf. Syst., 18(1):79-112, 2000. [29] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proc. 10th Intl.

As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents. In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d). We can then rank documents based on the likelihood of generating the query.
Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18]. The multinomial distribution is especially popular and also shown to be quite effective. The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text. Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms. However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting. Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.
In this paper, we propose and study a new family of query generation models based on the Poisson distribution. In this new family of models, we model the frequency of each term independently with a Poisson distribution. To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.
In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing. Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.
As in the existing work on multinomial language models, smoothing is critical for this new family of models. We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions. We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing. In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model. We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model. This advantage is seen for both one-stage and two-stage smoothing. Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula. This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.
The rest of the paper is organized as follows. In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions. In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval. We then design empirical experiments to compare the two families of language models in Section 4. We discuss the related work in 5 and conclude in 6.
PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document. In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution. Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20].
Let V = {w1, ..., wn} be a vocabulary set. Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document. We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.
Suppose t is the time period during which the author composed the text. With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time. The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k! Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.
With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w). We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.
Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above. The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].
Given a document d, we may estimate a Poisson language model Λd using d as a sample. The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.
In practice, we have the flexibility to choose the vocabulary V . In one extreme, we can use the vocabulary of the whole collection. However, this may bring in noise and considerable computational cost. In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms. As a compromise, we may conflate all the non-query terms as one single pseudo term. In other words, we may assume that there is exactly one non-query term in the vocabulary for each query. In our experiments, we adopt this pseudo non-query term strategy.
A document can be scored with the likelihood in Equation 1. However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero. As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d).
In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29]. In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words. In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1). Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word. Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.
In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions.
Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .
A document is assumed to be generated from a potentially different model. Given a particular document d, we want to estimate Λd. The rate of a term is estimated independently of other terms. We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.
The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28].
Another straightforward method is to decompose the query generation model as a mixture of two component models.
One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w.
For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]). With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models. In the retrieval formula above, the first summation can be computed efficiently. The second summation can be actually treated as a document prior, which penalizes long documents.
As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| .
As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.
In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the user"s queries. We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q. This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.
With no prior knowledge on p(·|U), we could set it to p(·|C). Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.
The empirical study of the smoothing methods is presented in Section 4.
MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model. This is expected since they both belong to the exponential family [26]. However, there are many differences when these two families of models are applied with different smoothing methods. From the perspective of retrieval, will these two language models perform equivalently? If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits? In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models.
Let us begin with the assumption that all the query terms appear in every document. Under this assumption, no smoothing is needed. A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) . Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate. Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28]. Note that this equivalence holds only when the document length variation is modeled with Poisson process.
This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval. With other smoothing strategies, however, the two models would be different. Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored. Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model. In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models.
One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing. Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights. With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].
This parameter can be made specific for different queries, but always has to be a constant for all the terms. This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1. However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query. For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model. Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term. Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models. Below we present a possible way to explore term dependent smoothing with Poisson language models.
Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw. This coefficient should intuitively be larger if w is a common word and smaller if it is a content word. The key problem is to find a method to assign reasonable values to δw. Empirical tuning is infeasible for so many parameters. We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.
With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents. Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection. Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d.
If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents. Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆. The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low. We again assume our vocabulary containing all query terms plus a pseudo non-query term. Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term. In our experiments, we set it to the average over δw of all query terms.
With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values. In Section 4, we use empirical experiments to prove this hypothesis.
Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)). One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29]. Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| . However, this assumption usually does not hold, since the collection is far more complex than a single document. Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc. Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.
Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27]. All the approaches can be easily adopted using Poisson language models.
However, a common problem of these approaches is that they all require heavy computation to construct the background model. With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.
Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson. The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function. There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katz"s K-Mixture [9]. Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].
All these mixtures have closed forms, and can be estimated from the collection of documents efficiently. This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval. For example, the probability density function of Katz"s K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.
With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection. To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query. This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4.
In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages. For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization. Intuitively, when the document has more unique words, it will be penalized more. On the other hand, if a document is exactly n copies of another document, it would not get over penalized. This feature is desirable and not achieved with the Dirichlet model [5].
Potentially, this component could penalize a document according to what types of terms it contains. With term specific settings of δ, we could get even more flexibility for document length normalization.
Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.
With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model. We could also utilize the relevant documents to learn better per-term smoothing coefficients.
In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval. In this section, we compare these two families of models empirically.
Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing. Using Poisson mixture as background model also improves the retrieval performance.
Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web). To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.
The documents are stemmed with the Porter"s stemmer, and we do not remove any stop word. For each parameter, we vary its value to cover a reasonably wide range.
We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors. Table 1 shows that the two JM-smoothed models perform similarly on all data sets. Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented. We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods. The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1. Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity. This similarity of performance is expected as we discussed in Section 3.1.
Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved. As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries. This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent. The parameter µ of the first stage Gamma smoothing is empirically tuned. The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2. The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2. The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models.
To test the effectiveness of the term dependent smoothing, we conduct the following two experiments. In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term. Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero. We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations. The documents are then still scored with Formula 3, but using learnt δw. The results are labeled with JM+L. in Table 2.
Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.
With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.
However, in some cases (e.g., Trec7/SV), it performs poorly.
This might be caused by the problem of EM estimation with unsmoothed document models. Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly. This indicates that there is still room to find better methods to estimate δw. Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.
As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance. To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29]. Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ. However, since their model is based on multinomial language modeling, they could not get per-term coefficients. We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms. We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2. Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries. The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.
This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.
This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial. In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method.
In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models. Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katz"s K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.
Data Query JM. Poisson JM. K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katz"s K-Mixture background model is compared in Table 3. Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.
Figure 3 shows that the performance changes over different parameters for short verbose queries. The model using K-Mixture background is less sensitive than the one using single Poisson background. Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0
Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance.
To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.
Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. The most popular and fundamental one is the query-generation language model [21, 13]. All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18]. We introduce a new family of language models, based on Poisson distribution. Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial. However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents. Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katz"s KMixture [9] has shown to be effective to model and retrieve documents. Once again, none of this work explores Poisson distribution in the query generation framework.
Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful. We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically.
We present a new family of query generation language models for retrieval based on Poisson distribution. We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.
We compare the new models with the popular multinomial retrieval models both analytically and experimentally. Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences. In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing. We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models. Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model. Our work opens up many interesting directions for further exploration in this new family of models.
Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work. It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost.
We thank the anonymous SIGIR 07 reviewers for their useful comments. This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852.
[1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale. Poisson mixtures. Nat. Lang.
Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors. Language Modeling and Information Retrieval. Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra. Using Language Models for Information Retrieval. PhD thesis, University of Twente, Enschede,
Netherlands, 2001. [7] D. Hiemstra. Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann. Probabilistic latent semantic indexing.
In Proceedings of ACM SIGIR"99, pages 50-57, 1999. [9] S. M. Katz. Distribution of content words and phrases in text and language modelling. Nat. Lang. Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee. Corpus structure, language models, and ad-hoc information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR"01, pages 111-119,
Sept 2001. [12] J. Lafferty and C. Zhai. Probabilistic IR models based on query and document generation. In Proceedings of the Language Modeling and IR workshop, pages 1-5,
May 31 - June 1 2001. [13] J. Lafferty and C. Zhai. Probabilistic relevance models based on document and query generation. In W. B.
Croft and J. Lafferty, editors, Language Modeling and Information Retrieval. Kluwer Academic Publishers,
[14] V. Lavrenko and B. Croft. Relevance-based language models. In Proceedings of SIGIR"01, pages 120-127,
Sept 2001. [15] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193,
[16] E. L. Margulis. Modelling documents with multiple poisson distributions. Inf. Process. Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft. Formal multiple-bernoulli models for language modeling. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz. A hidden Markov model information retrieval system. In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis. Probability, random variables and stochastic processes. New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR"94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones,
M. M.Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang. A parallel derivation of probabilistic information retrieval models. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion.
In Proceedings of HLT/NAACL 2006, pages 407-414,
[26] J. Teevan and D. R. Karger. Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185,

ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.
The Definition questions, also called Other questions in recent years, are defined as follows. Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?. The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic. Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic.
Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets. Each informative nugget is a sentence fragment that describe some factual information about the topic. Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.
From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets. Rather, these topic nuggets have a trivia-like quality associated with them. Typically, these are out of the ordinary pieces of information about a topic that can pique a human reader"s interest. For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets. In essence, interesting nuggets answer the questions What is X famous for?,
What defines X? or What is extraordinary about X?.
We now have two very different perspective as to what constitutes an answer to Definition questions. An answer can be some important factual information about the topic or some novel and interesting aspect about the topic. This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman. Certain answer nuggets are more informative while other nuggets are more interesting in nature.
Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.
Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.
As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the world"s oldest boxing champion. Foreman"s waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom. As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers.
Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets. In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets. A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system. We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets.
There are currently two general methods for Definitional Question Answering. The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14]. Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.
For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system. Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created. A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains. Here, a musician template may contain lexical patterns that identify information such as the musician"s musical style, songs sung by the musician and the band, if any, that the musician belongs to. As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.
This process requires a lot of manual labor, expertise and is not scalable. This lead to the development of the soft-pattern approach by Cui et al. [4, 11]. Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences. Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.
Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a person"s birthdate, or the name of a company"s CEO. However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations. This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities. For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being. Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.
This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.
Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1]. A similar approach has also been used as a baseline system for TREC 2003 [14]. More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.
Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic. The baseline system in TREC 2003 simply uses the topic words as its definitional corpus. Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional. Chen et al. [3] collect snippets from Google to build its definitional corpus.
From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected. This centroid vector or set of centroid words is taken to be highly indicative of the topic. Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.
BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality. Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.
As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus. However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences. Thus such methods identify relevant sentences and not sentences containing definitional nuggets. Yet, the TREC 2003 baseline system [14] outperformed all but one other system. The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach. At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].
We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords. This may explain why relevance-based method can perform competitively in definitional question answering. However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner. Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets. We will describe how we expand upon such methods to identify interesting nuggets in the next section.
Getting a computer system to identify sentences that a human reader would find interesting is a tall order. However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic. What"s more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in. Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics. We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.
This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents. This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified. Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight. This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.
In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm. We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus. Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering.
There exists on the internet articles on just about any topic a human can think of. What"s more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge. For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.
Such articles are useful as they contain concise information about the topic. More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.
We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.
Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers. This resource has been used by many Question Answering system as a source of knowledge about each topic. We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.
NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies. For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.
Google Snippets are retrieved by issuing the topic as a query to the Google search engine. From the search results, we extracted the top 100 snippets. While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.
Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus. We also extracted documents from other resources. However, as these resources are more specific in nature, we do not always get any single relevant document. These resources are listed below.
Biography.com is the website for the Biography television cable channel. The channel"s website contains searchable biographies on over 25,000 notable people. If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.
Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.
Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.
Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one. We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.
Figure 1: Human Interest Model Architecture.
WordNet WordNet is an well-known electronic semantic lexicon for the English language. Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset. We add this short definition, if there is one, into our Interest Corpus.
We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I.
We have seen that interesting nuggets are highly specific to a topic. Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence. We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.
Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids. Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers. By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm). Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus. In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topic"s Interest Corpus. An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic.
Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.
The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented. Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.
The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.
The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper. The module first builds the unigram language model, I, from the collected web documents. This language model will be used to weight the importance of terms within sentences. Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.
Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness. We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.
A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents. To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm. Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus. When both sentences share the same term, the similarity score is incremented by the two times the term"s weight and every dissimilar term decrements the similarity score by the dissimilar term"s weight.
We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence. In this manner, every candidate sentence is ranked by interestingness. Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic.
The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets. Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering. This is done in order to explore how interestingness plays a factor in definitional answers. In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11]. In order to ensure comparable results, both systems are provided identical input data. Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module. Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.
For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems. Both systems are evaluated the results using the standard scoring methodology for TREC definitions. TREC provides a list of vital and okay nuggets for each question topic.
Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall. Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12]. The evaluation is automatically conducted using Pourpre v1.0c [10].
FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types.
Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets. We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13]. Table 1 shows the F3 score the three systems for the TREC 2005 question set.
The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303. The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].
This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.
In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table
question topics into 4 main entity types [13]. The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2. Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics. This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system. In general, it is harder to locate a single web article that describes an event or a general object. However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types. This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers.
Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm.
The word trivia refer to tidbits of unimportant or uncommon information. As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings. From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.
There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets. A standard unigram language model would not capture these low-frequency terms as important terms. To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms. The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].
TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus. For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.
Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions. Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.
General English words are likely to have similar distributions in both language models I and A. Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.
In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.
DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A. While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words. These high frequency topic specific words occur very much more frequently in I than in A. As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare. For this reason, we explored another divergence measure as a possible term weighting scheme.
Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3. As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A.
DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.
However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4. The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.
DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above. As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms. Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model. TFIDF performed the worst as we had anticipated. The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms. This causes the IDF component to be the main factor in scoring sentences.
As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.
This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus. Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights. We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.
Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments. From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list. Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms. However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required. However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights
In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain. We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model. For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.
W - Wikipedia: Text from the most relevant article found in Wikipedia.
S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.
M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.
We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination. All runs were conducted on Human Interest Model using JS divergence as term weighting scheme. The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference. A consistent trend can be observed for each entity class.
For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence. This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets. We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia. We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures. Human readers are likely to be interested in news events that spotlight these personalities.
Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedia"s most relevant article on the topic with Google snippets again providing additional information for organizations.
With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158.
INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets. However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional. Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic. We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.
Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M
1 N+S+M W+S W+M N+M
2 N+S N+W+S W+S+M N+S+M
3 N+M N+W+S+M W+S N+S
Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions. The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets. The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets. We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set. However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.
The reason is that both systems are picking up very different sentences as definitional answers. In essence, our two experts are disagreeing on which sentences are definitional. In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets. The remaining answers were completely different. Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.
Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.
There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content. This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different. Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.
To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems. When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was
sentence agreement rate, both systems are generally still picking up different answer nuggets. We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets. It is also indication that in general, interesting and informative nuggets are quite different in nature.
There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches. However, the differences between the two systems also cause issues when we attempt to combine both answer sets.
Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.
We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.
When only one system believes that the sentence is definitional, we simply retain that system"s normalized score as the unified score.
When both systems agree agree that the sentence is definitional, the sentence"s score is boosted by the degree of agreement between between both systems.
Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2]. Using the approach described here, we achieve a F3 score of 0.3081. This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model.
This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.
Interesting nuggets are uncommon pieces of information about the topic that can evoke a human reader"s curiosity. The notion of an average human reader is an important consideration in our approach. This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.
Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings. Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.
We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers. What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.
Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic. While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al."s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task. Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features. As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.
We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers. Although the methods we used are simple, they have been shown experimentally to be effective. Our approach may also provide some insight into a few anomalies in past definitional question answering"s trials. For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets. We suspect the main contributor to the system"s performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co.,
Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion,
Sony Pictures Entertainment (SPE),
Telefonica of Spain, Lions Club International, AMWAY, McDonald"s Corporation,
Harley-Davidson, U.S. Naval Academy,
OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa,
Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites,
Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement,
Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games,
Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St.
Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Google"s PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.
In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers.
[1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.
A hybrid approach for qa track definitional questions. In TREC "03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang. Reranking answers for definitional qa using language modeling. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney,
Australia, July 2006. Association for Computational Linguistics. [4] H. Cui, M.-Y. Kan, and T.-S. Chua. Generic soft pattern models for definitional question answering. In SIGIR "05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005. ACM Press. [5] T. G. Dietterich. Ensemble methods in machine learning.
Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. Employing two question answering systems at trec 2005. In TREC "05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber. Experiments at the university of edinburgh for the trec 2006 qa track. In TREC "06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006. National Institute of Standards and Technology. [8] J. Lin. Divergence measures based on the shannon entropy.
IEEE Transactions on Information Theory, 37(1):145 - 151,
Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu. A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my! In TREC "05: Proceedings of the 14th Text REtrieval Conference,
Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman. Automatically evaluating answers to definition questions. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October
[11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.
Kan. Using syntactic and semantic relation analysis in question answering. In TREC "05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees. Overview of the trec 2003 question answering track. In Text REtrieval Conference 2003,
Gaithersburg, Maryland, 2003. National Institute of Standards and Technology. [13] E. M. Voorhees. Overview of the trec 2005 question answering track. In TREC "05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.
National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel. TREC 2003 QA at BBN: Answering definitional questions. In TREC "03: Proceedings of the 12th Text REtrieval Conference,
Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee. A language modeling approach to passage question answering. In TREC "03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland,

Recently ‘learning to rank" has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15]. Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.
Several methods for learning to rank have been developed and applied to document retrieval. For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost. All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.
In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank. AdaRank utilizes a linear combination of ‘weak rankers" as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.
We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.
A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.
AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.
Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.
Tuning ranking models using certain training data and a performance measure is a common practice in IR [1]. As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.
Recently, direct optimization of performance measures in learning has become a hot research topic. Several methods for classification [17] and ranking [5, 19] have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.
The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discussions are given in Section 4. Section 5 concludes this paper and gives future work.
The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1]. For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.
Recently methods of ‘learning to rank" have been applied to ranking model construction and some promising results have been obtained. For example, Joachims [16] applies Ranking SVM to document retrieval. He utilizes click-through data to deduce training data for the model creation. Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR. Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents. Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval. The method is referred to as ‘RankNet".
There are three topics in machine learning which are related to our current work. They are ‘learning to rank", boosting, and direct optimization of performance measures.
Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores. Several approaches have been proposed to tackle the problem. One major approach to learning to rank is that of transforming it into binary classification on instance pairs. This ‘pair-wise" approach fits well with information retrieval and thus is widely used in IR. Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3]. For other approaches to learning to rank, refer to [2, 11, 31].
In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16]. In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.
Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to repeatedly construct ‘weak learners" by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is ‘boosted". Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction). Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26]. Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8]. In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the ‘exponential loss function" with respect to the training data [26]. Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.
Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification. Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].
Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.
AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique.
We first describe the general framework of learning to rank for document retrieval. In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores. The relevance scores are calculated with a ranking function (model). In learning (training), a number of queries and their corresponding retrieved documents are given. Furthermore, the relevance levels of the documents with respect to the queries are also provided. The relevance levels are represented as ranks (i.e., categories in a total order). The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing.
Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks. There exists a total order between the ranks r r −1 · · · r1, where ‘ " denotes a preference relationship.
In training, a set of queries Q = {q1, q2, · · · , qm} is given. Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j. A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.
The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores. Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself. We use π( j) to denote the position of item j (i.e., di j). The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.
Table 1: Notations and explanations.
Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.
In information retrieval, query-based performance measures are used to evaluate the ‘goodness" of a ranking function. By query based measure, we mean a measure defined over a ranking list of documents with respect to a query. These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15]. We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures. The first argument of E is the permutation π created using the ranking function f on di. The second argument is the list of ranks yi given by humans. E measures the agreement between π and yi. Table 1 gives a summary of notations described above.
Next, as examples of performance measures, we present the definitions of MAP and NDCG. Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.
Given a query qi, the list of ranks yi, and a permutation πi on di,
NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i "s NDCG score at position m is 1.
Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. The algorithm is referred to as ‘AdaRank" and is shown in Figure 1.
AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters. AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T). Finally, it outputs a ranking model f by linearly combining the weak rankers.
At each round, AdaRank maintains a distribution of weights over the queries in the training data. We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.
For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .
End For Output ranking model: f(x) = fT (x).
Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i). Initially, AdaRank sets equal weights to the queries. At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far. As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those ‘hard" queries.
At each round, a weak ranker ht is constructed based on training data with weight distribution Pt. The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).
Several methods for weak ranker construction can be considered.
For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt. In this paper, we use single features as weak rankers, as will be explained in Section 3.6.
Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker. Intuitively, αt measures the importance of ht.
A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1.
The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). In contrast, AdaRank tries to optimize a loss function based on queries.
Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. The measures can be MAP,
NDCG, WTA, MRR, or any other measures whose range is within [−1, +1]. We next explain why this is the case.
Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions. This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle. We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ . We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Several ways of computing coefficients αt and weak rankers ht may be considered. Following the idea of AdaBoost, in AdaRank we take the approach of ‘forward stage-wise additive modeling" [12] and get the algorithm in Figure
accuracy for AdaRank on training data, as presented in Theorem 1.
T 1. The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T.
A proof of the theorem can be found in appendix. The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.
AdaRank is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above. In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.
First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].
Notice that the major IR measures meet this requirement. In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].
Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms. The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data. The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].
Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods. Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs. As a result,
AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed. In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval. The existing methods cannot focus on the training on the tops, as indicated in [4]. Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem. In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4]. AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning.
AdaRank is a boosting algorithm. In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.
First, the types of instances are different. AdaRank makes use of queries and their corresponding document lists as instances. The labels in training data are lists of ranks (relevance levels). AdaBoost makes use of feature vectors as instances. The labels in training data are simply +1 and −1.
Second, the performance measures are different. In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query. In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as ‘margin" [25].
Third, the ways of updating weights are also different. In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner. In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1. Note that AdaBoost can also adopt the weight updating method used in AdaRank. For AdaBoost they are equivalent (cf., [12] page 305). However, this is not true for AdaRank.
We consider an efficient implementation for weak ranker construction, which is also used in our experiments. In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).
Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero.
We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.
Table 2: Features used in the experiments on OHSUMED,
WSJ, and AP datasets. C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency.
c(wi,C) + 1)
|d| + 1)
|d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1)
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data.
Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods. Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).
For AdaRank, the parameter T was determined automatically during each experiment. Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined). As the measure E, MAP and NDCG@5 were utilized. The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively.
In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank. The OHSUMED dataset consists of 348,566 documents and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. The relevance judgments are either ‘d" (definitely relevant), ‘p" (possibly relevant), or ‘n"(not relevant). The data have been used in many experiments in IR, for example [4, 29].
As features, we adopted those used in document retrieval [4].
Table 2 shows the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features. BM25 score itself is also a feature. Stop words were removed and stemming was conducted in the data.
We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments. We tuned the parameters for BM25 during one of the trials and applied them to the other trials. The results reported in Figure 2 are those averaged over four trials. In MAP calculation, we define the rank ‘d" as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.
Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant. From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures. We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP. The results indicate that all the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5. The improvements are also statistically significant.
In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300). Each query has a number of documents associated and they are labeled as ‘relevant" or ‘irrelevant" (to the query). Following the practice in [28], the queries that have less than 10 relevant documents were discarded. Table 3 shows the statistics on the two datasets.
In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. We also conducted 4-fold cross-validation experiments. The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively. From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP. We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25,
Ranking SVM, and RankBoost on WSJ and AP. The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05). However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points).
In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.
The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002. There are a total
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset.
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.
Table 4: Features used in the experiments on .Gov dataset.
of 1,053,110 web pages with 11,164,829 hyperlinks in the data.
The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used. The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant. The number of relevant pages vary from query to query (from 1 to 86).
We extracted 14 features from each query-document pair.
Table 4 gives a list of the features. They are the outputs of some well-known algorithms (systems). These features are different from those in Table 2, because the task is different.
Again, we conducted 4-fold cross-validation experiments. The results averaged over four trials are reported in Figure 5. From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures. We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost. Some of the improvements are not statistically significant. This is because we have only
too small.
We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.
First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost. Specifically we com0.58
d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. The results averaged over four trials in the 4-fold cross validation are shown in Figure 6. We use ‘d-n" to stand for the pairs between ‘definitely relevant" and ‘not relevant", ‘d-p" the pairs between ‘definitely relevant" and ‘partially relevant", and ‘p-n" the pairs between ‘partially relevant" and ‘not relevant". From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for ‘d-n" and ‘d-p", which are related to the tops of rankings and are important. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.
We also made statistics on the number of document pairs per query in the training data (for trial 1). The queries are clustered into different groups based on the the number of their associated document pairs. Figure 7 shows the distribution of the query groups. In the figure, for example, ‘0-1k" is the group of queries whose number of document pairs are between 0 and 999. We can see that the numbers of document pairs really vary from query to query. Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group. The results are reported in Figure 8. We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost. Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., ‘0-1k", ‘1k-2k", and ‘2k-3k"). The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs. For AdaRank.NDCG, similar results can be observed.
MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups.
trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.
We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5. The experiment was conducted for each trial. Figure
respectively. We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.
Finally, we tried to verify the correctness of Theorem 1. That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation. From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak. The result agrees well with Theorem 1.
In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures. It employs a boosting technique in ranking model learning. AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost.
trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5.
MAP number of rounds Figure 11: Learning curve of AdaRank.
Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures.
We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper.

Electronic communication is increasingly plagued by unwanted or harmful content known as spam. The most well known form of spam is email spam, which remains a major problem for large email systems. Other forms of spam are also becoming problematic, including blog spam, in which spammers post unwanted comments in blogs [21], and splogs, which are fake blogs constructed to enable link spam with the hope of boosting the measured importance of a given webpage in the eyes of automated search engines [17]. There are a variety of methods for identifying these many forms of spam, including compiling blacklists of known spammers, and conducting link analysis.
The approach of content analysis has shown particular promise and generality for combating spam. In content analysis, the actual message text (often including hyper-text and meta-text, such as HTML and headers) is analyzed using machine learning techniques for text classification to determine if the given content is spam. Content analysis has been widely applied in detecting email spam [11], and has also been used for identifying blog spam [21] and splogs [17].
In this paper, we do not explore the related problem of link spam, which is currently best combated by link analysis [13].
The anti-spam community has been divided on the choice of the best machine learning method for content-based spam detection. Academic researchers have tended to favor the use of Support Vector Machines (SVMs), a statistically robust machine learning method [7] which yields state-of-theart performance on general text classification [14]. However,
SVMs typically require training time that is quadratic in the number of training examples, and are impractical for largescale email systems. Practitioners requiring content-based spam filtering have typically chosen to use the faster (if less statistically robust) machine learning method of Naive Bayes text classification [11, 12, 20]. This Bayesian method requires only linear training time, and is easily implemented in an online setting with incremental updates. This allows a deployed system to easily adapt to a changing environment over time. Other fast methods for spam filtering include compression models [1] and logistic regression [10]. It has not yet been empirically demonstrated that SVMs give improved performance over these methods in an online spam detection setting [4].
In this paper, we address the anti-spam controversy and offer a potential resolution. We first demonstrate that online SVMs do indeed provide state-of-the-art spam detection through empirical tests on several large benchmark data sets of email spam. We then analyze the effect of the tradeoff parameter in the SVM objective function, which shows that the expensive SVM methodology may, in fact, be overkill for spam detection. We reduce the computational cost of SVM learning by relaxing this requirement on the maximum margin in online settings, and create a Relaxed Online SVM,
ROSVM, appropriate for high performance content-based spam filtering in large-scale settings.
The controversy between academics and practitioners in spam filtering centers on the use of SVMs. The former advocate their use, but have yet to demonstrate strong performance with SVMs on online spam filtering. Indeed, the results of [4] show that, when used with default parameters,
SVMs actually perform worse than other methods. In this section, we review the basic workings of SVMs and describe a simple Online SVM algorithm. We then show that Online SVMs indeed achieve state-of-the-art performance on filtering email spam, blog comment spam, and splogs, so long as the tradeoff parameter C is set to a high value. However, the cost of Online SVMs turns out to be prohibitive for largescale applications. These findings motivate our proposal of Relaxed Online SVMs in the following section.
SVMs are a robust machine learning methodology which has been shown to yield state-of-the-art performance on text classification [14]. by finding a hyperplane that separates two classes of data in data space while maximizing the margin between them.
We use the following notation to describe SVMs, which draws from [23]. A data set X contains n labeled example vectors {(x1, y1) . . . (xn, yn)}, where each xi is a vector containing features describing example i, and each yi is the class label for that example. In spam detection, the classes spam and ham (i.e., not spam) are assigned the numerical class labels +1 and −1, respectively. The linear SVMs we employ in this paper use a hypothesis vector w and bias term b to classify a new example x, by generating a predicted class label f(x): f(x) = sign(< w, x > +b) SVMs find the hypothesis w, which defines the separating hyperplane, by minimizing the following objective function over all n training examples: τ(w, ξ) = 1 2 ||w||2 + C nX i=i ξi under the constraints that ∀i = {1..n} : yi(< w, xi > +b) ≥ 1 − ξi, ξi ≥ 0 In this objective function, each slack variable ξi shows the amount of error that the classifier makes on a given example xi. Minimizing the sum of the slack variables corresponds to minimizing the loss function on the training data, while minimizing the term 1 2 ||w||2 corresponds to maximizing the margin between the two classes [23]. These two optimization goals are often in conflict; the tradeoff parameter C determines how much importance to give each of these tasks.
Linear SVMs exploit data sparsity to classify a new instance in O(s) time, where s is the number of non-zero features. This is the same classification time as other linear Given: data set X = (x1, y1), . . . , (xn, yn), C, m: Initialize w := 0, b := 0, seenData := { } For Each xi ∈ X do: Classify xi using f(xi) = sign(< w, xi > +b) IF yif(xi) < 1 Find w , b using SMO on seenData, using w, b as seed hypothesis.
Add xi to seenData done Figure 1: Pseudo code for Online SVM. classifiers, and as Naive Bayesian classification. Training SVMs, however, typically takes O(n2 ) time, for n training examples. A variant for linear SVMs was recently proposed which trains in O(ns) time [15], but because this method has a high constant, we do not explore it here.
In many traditional machine learning applications, SVMs are applied in batch mode. That is, an SVM is trained on an entire set of training data, and is then tested on a separate set of testing data. Spam filtering is typically tested and deployed in an online setting, which proceeds incrementally. Here, the learner classifies a new example, is told if its prediction is correct, updates its hypothesis accordingly, and then awaits a new example. Online learning allows a deployed system to adapt itself in a changing environment.
Re-training an SVM from scratch on the entire set of previously seen data for each new example is cost prohibitive.
However, using an old hypothesis as the starting point for re-training reduces this cost considerably. One method of incremental and decremental SVM learning was proposed in [2]. Because we are only concerned with incremental learning, we apply a simpler algorithm for converting a batch SVM learner into an online SVM (see Figure 1 for pseudocode), which is similar to the approach of [16].
Each time the Online SVM encounters an example that was poorly classified, it retrains using the old hypothesis as a starting point. Note that due to the Karush-Kuhn-Tucker (KKT) conditions, it is not necessary to re-train on wellclassified examples that are outside the margins [23].
We used Platt"s SMO algorithm [22] as a core SVM solver, because it is an iterative method that is well suited to converge quickly from a good initial hypothesis. Because previous work (and our own initial testing) indicates that binary feature values give the best results for spam filtering [20, 9], we optimized our implementation of the Online SMO to exploit fast inner-products with binary vectors. 1
Extracting machine learning features from text may be done in a variety of ways, especially when that text may include hyper-content and meta-content such as HTML and header information. However, previous research has shown that simple methods from text classification, such as bag of words vectors, and overlapping character-level n-grams, can achieve strong results [9]. Formally, a bag of words vector is a vector x with a unique dimension for each possible 1 Our source code is freely available at www.cs.tufts.edu/∼dsculley/onlineSMO. 1
ROCArea C 2-grams 3-grams 4-grams words Figure 2: Tuning the Tradeoff Parameter C. Tests were conducted with Online SMO, using binary feature vectors, on the spamassassin data set of 6034 examples. Graph plots C versus Area under the ROC curve. word, defined as a contiguous substring of non-whitespace characters. An n-gram vector is a vector x with a unique dimension for each possible substring of n total characters.
Note that n-grams may include whitespace, and are overlapping. We use binary feature scoring, which has been shown to be most effective for a variety of spam detection methods [20, 9]. We normalize the vectors with the Euclidean norm. Furthermore, with email data, we reduce the impact of long messages (for example, with attachments) by considering only the first 3,000 characters of each string. For blog comments and splogs, we consider the whole text, including any meta-data such as HTML tags, as given. No other feature selection or domain knowledge was used.
The SVM tradeoff parameter C must be tuned to balance the (potentially conflicting) goals of maximizing the margin and minimizing the training error. Early work on SVM based spam detection [9] showed that high values of C give best performance with binary features. Later work has not always followed this lead: a (low) default setting of C was used on splog detection [17], and also on email spam [4].
Following standard machine learning practice, we tuned C on separate tuning data not used for later testing. We used the publicly available spamassassin email spam data set, and created an online learning task by randomly interleaving all 6034 labeled messages to create a single ordered set.
For tuning, we performed a coarse parameter search for C using powers of ten from .0001 to 10000. We used the Online SVM described above, and tested both binary bag of words vectors and n-gram vectors with n = {2, 3, 4}. We used the first 3000 characters of each message, which included header information, body of the email, and possibly attachments.
Following the recommendation of [6], we use Area under the ROC curve as our evaluation measure. The results (see Figure 2) agree with [9]: there is a plateau of high performance achieved with all values of C ≥ 10, and performance degrades sharply with C < 1. For the remainder of our experiments with SVMs in this paper, we set C = 100. We will return to the observation that very high values of C do not degrade performance as support for the intuition that relaxed SVMs should perform well on spam.
Table 1: Results for Email Spam filtering with Online SVM on benchmark data sets. Score reported is (1-ROCA)%, where 0 is optimal. trec05p-1 trec06p OnSVM: words 0.015 (.011-.022) 0.034 (.025-.046) 3-grams 0.011 (.009-.015) 0.025 (.017-.035) 4-grams 0.008 (.007-.011) 0.023 (.017-.032) SpamProbe 0.059 (.049-.071) 0.092 (.078-.110) BogoFilter 0.048 (.038-.062) 0.077 (.056-.105) TREC Winners 0.019 (.015-.023) 0.054 (.034-.085) 53-Ensemble 0.007 (.005-.008) 0.020 (.007-.050) Table 2: Results for Blog Comment Spam Detection using SVMs and Leave One Out Cross Validation.
We report the same performance measures as in the prior work for meaningful comparison. accuracy precision recall SVM C = 100: words 0.931 0.946 0.954 3-grams 0.951 0.963 0.965 4-grams 0.949 0.967 0.956 Prior best method 0.83 0.874 0.874
With C tuned on a separate tuning set, we then tested the performance of Online SVMs in spam detection. We used two large benchmark data sets of email spam as our test corpora. These data sets are the 2005 TREC public data set trec05p-1 of 92,189 messages, and the 2006 TREC public data sets, trec06p, containing 37,822 messages in English. (We do not report our strong results on the trec06c corpus of Chinese messages as there have been questions raised over the validity of this test set.) We used the canonical ordering provided with each of these data sets for fair comparison.
Results for these experiments, with bag of words vectors and and n-gram vectors appear in Table 1. To compare our results with previous scores on these data sets, we use the same (1-ROCA)% measure described in [6], which is one minus the area under the ROC curve, expressed as a percent.
This measure shows the percent chance of error made by a classifier asserting that one message is more likely to be spam than another. These results show that Online SVMs do give state of the art performance on email spam. The only known system that out-performs the Online SVMs on the trec05p-1 data set is a recent ensemble classifier which combines the results of 53 unique spam filters [19]. To our knowledge, the Online SVM has out-performed every other single filter on these data sets, including those using Bayesian methods [5, 3], compression models [5, 3], logistic regression [10], and perceptron variants [3], the TREC competition winners [5, 3], and open source email spam filters BogoFilter v1.1.5 and SpamProbe v1.4d.
Blog comment spam is similar to email spam in many regards, and content-based methods have been proposed for detecting these spam comments [21]. However, large benchmark data sets of labeled blog comment spam do not yet exist. Thus, we run experiments on the only publicly available data set we know of, which was used in content-based blog Table 3: Results for Splog vs. Blog Detection using SVMs and Leave One Out Cross Validation. We report the same evaluation measures as in the prior work for meaningful comparison. features precision recall F1 SVM C = 100: words 0.921 0.870 0.895 3-grams 0.904 0.866 0.885 4-grams 0.928 0.876 0.901 Prior SVM with: words 0.887 0.864 0.875 4-grams 0.867 0.844 0.855 words+urls 0.893 0.869 0.881 comment spam detection experiments by [21]. Because of the small size of the data set, and because prior researchers did not conduct their experiments in an on-line setting, we test the performance of linear SVMs using leave-one-out cross validation, with SVM-Light, a standard open-source SVM implementation [14]. We use the parameter setting C = 100, with the same feature space mappings as above.
We report accuracy, precision, and recall to compare these to the results given on the same data set by [21]. These results (see Table 2) show that SVMs give superior performance on this data set to the prior methodology.
As with blog comment spam, there is not yet a large, publicly available benchmark corpus of labeled splog detection test data. However, the authors of [17] kindly provided us with the labeled data set of 1,389 blogs and splogs that they used to test content-based splog detection using SVMs. The only difference between our methodology and that of [17] is that they used default parameters for C, which SVM-Light sets to 1 avg||x||2 . (For normalized vectors, this default value sets C = 1.) They also tested several domain-informed feature mappings, such as giving special features to url tags.
For our experiments, we used the same feature mappings as above, and tested the effect of setting C = 100. As with the methodology of [17], we performed leave one out cross validation for apples-to-apples comparison on this data. The results (see Table 3) show that a high value of C produces higher performance for the same feature space mappings, and even enables the simple 4-gram mapping to out-perform the previous best mapping which incorporated domain knowledge by using words and urls.
The results presented in this section demonstrate that linfeatures trec06p trec05p-1 words 12196s 66478s 3-grams 44605s 128924s 4-grams 87519s 242160s corpus size 32822 92189 Table 4: Execution time for Online SVMs with email spam detection, in CPU seconds. These times do not include the time spent mapping strings to feature vectors. The number of examples in each data set is given in the last row as corpus size.
A B Figure 3: Visualizing the effect of C.
Hyperplane A maximizes the margin while accepting a small amount of training error. This corresponds to setting C to a low value. Hyperplane B accepts a smaller margin in order to reduce training error. This corresponds to setting C to a high value. Content-based spam filtering appears to do best with high values of C. ear SVMs give state of the art performance on content-based spam filtering. However, this performance comes at a price.
Although the blog comment spam and splog data sets are too small for the quadratic training time of SVMs to appear problematic, the email data sets are large enough to illustrate the problems of quadratic training cost.
Table 4 shows computation time versus data set size for each of the online learning tasks (on same system). The training cost of SVMs are prohibitive for large-scale content based spam detection, or a large blog host. In the following section, we reduce this cost by relaxing the expensive requirements of SVMs.
One of the main benefits of SVMs is that they find a decision hyperplane that maximizes the margin between classes in the data space. Maximizing the margin is expensive, typically requiring quadratic training time in the number of training examples. However, as we saw in the previous section, the task of content-based spam detection is best achieved by SVMs with a high value of C. Setting C to a high value for this domain implies that minimizing training loss is more important than maximizing the margin (see Figure 3).
Thus, while SVMs do create high performance spam filters, applying them in practice is overkill. The full margin maximization feature that they provide is unnecessary, and relaxing this requirement can reduce computational cost.
We propose three ways to relax Online SVMs: • Reduce the size of the optimization problem by only optimizing over the last p examples. • Reduce the number of training updates by only training on actual errors. • Reduce the number of iterations in the iterative SVM Given: dataset X = (x1, y1), . . . , (xn, yn), C, m, p: Initialize w := 0, b := 0, seenData := { } For Each xi ∈ X do: Classify xi using f(xi) = sign(< w, xi > +b) If yif(xi) < m Find w , b with SMO on seenData, using w, b as seed hypothesis. set (w, b) := (w",b") If size(seenData) > p remove oldest example from seenData Add xi to seenData done Figure 4: Pseudo-code for Relaxed Online SVM. solver by allowing an approximate solution to the optimization problem.
As we describe in the remainder of this subsection, all of these methods trade statistical robustness for reduced computational cost. Experimental results reported in the following section show that they equal or approach the performance of full Online SVMs on content-based spam detection.
In the full Online SVMs, we re-optimize over the full set of seen data on every update, which becomes expensive as the number of seen data points grows. We can bound this expense by only considering the p most recent examples for optimization (see Figure 4 for pseudo-code).
Note that this is not equivalent to training a new SVM classifier from scratch on the p most recent examples, because each successive optimization problem is seeded with the previous hypothesis w [8]. This hypothesis may contain values for features that do not occur anywhere in the p most recent examples, and these will not be changed. This allows the hypothesis to remember rare (but informative) features that were learned further than p examples in the past.
Formally, the optimization problem is now defined most clearly in the dual form [23]. In this case, the original softmargin SVM is computed by maximizing at example n: W (α) = nX i=1 αi − 1 2 nX i,j=1 αiαjyiyj < xi, xj >, subject to the previous constraints [23]: ∀i ∈ {1, . . . , n} : 0 ≤ αi ≤ C and nX i=1 αiyi = 0 To this, we add the additional lookback buffer constraint ∀j ∈ {1, . . . , (n − p)} : αj = cj where cj is a constant, fixed as the last value found for αj while j > (n − p). Thus, the margin found by an optimization is not guaranteed to be one that maximizes the margin for the global data set of examples {x1, . . . , xn)}, but rather one that satisfies a relaxed requirement that the margin be maximized over the examples { x(n−p+1), . . . , xn}, subject to the fixed constraints on the hyperplane that were found in previous optimizations over examples {x1, . . . , x(n−p)}. (For completeness, when p ≥ n, define (n − p) = 1.) This set of constraints reduces the number of free variables in the optimization problem, reducing computational cost.
As noted before, the KKT conditions show that a well classified example will not change the hypothesis; thus it is not necessary to re-train when we encounter such an example. Under the KKT conditions, an example xi is considered well-classified when yif(xi) > 1. If we re-train on every example that is not well-classified, our hyperplane will be guaranteed to be optimal at every step.
The number of re-training updates can be reduced by relaxing the definition of well classified. An example xi is now considered well classified when yif(xi) > M, for some
hyperplane. The learner may encounter an example that lies within the margins, but farther from the margins than M.
Such an example means the hypothesis is no longer globally optimal for the data set, but it is considered good enough for continued use without immediate retraining.
This update procedure is similar to that used by variants of the Perceptron algorithm [18]. In the extreme case, we can set M = 0, which creates a mistake driven Online SVM. In the experimental section, we show that this version of Online SVMs, which updates only on actual errors, does not significantly degrade performance on content-based spam detection, but does significantly reduce cost.
As an iterative solver, SMO makes repeated passes over the data set to optimize the objective function. SMO has one main loop, which can alternate between passing over the entire data set, or the smaller active set of current support vectors [22]. Successive iterations of this loop bring the hyperplane closer to an optimal value. However, it is possible that these iterations provide less benefit than their expense justifies. That is, a close first approximation may be good enough. We introduce a parameter T to control the maximum number of iterations we allow. As we will see in the experimental section, this parameter can be set as low as 1 with little impact on the quality of results, providing computational savings.
In Section 2, we argued that the strong performance on content-based spam detection with SVMs with a high value of C show that the maximum margin criteria is overkill, incurring unnecessary computational cost. In Section 3, we proposed ROSVM to address this issue, as both of these methods trade away guarantees on the maximum margin hyperplane in return for reduced computational cost. In this section, we test these methods on the same benchmark data sets to see if state of the art performance may be achieved by these less costly methods. We find that ROSVM is capable of achieving these high levels of performance with greatly reduced cost. Our main tests on content-based spam detection are performed on large benchmark sets of email data.
We then apply these methods on the smaller data sets of blog comment spam and blogs, with similar performance.
In Section 3, we proposed three approaches for reducing the computational cost of Online SMO: reducing the prob0.005
(1-ROCA)% Buffer Size trec05p-1 trec06p 0 50000 100000 150000 200000 250000
CPUSec.
Buffer Size trec05p-1 trec06p Figure 5: Reduced Size Tests. lem size, reducing the number of optimization iterations, and reducing the number of training updates. Each of these approaches relax the maximum margin criteria on the global set of previously seen data. Here we test the effect that each of these methods has on both effectiveness and efficiency. In each of these tests, we use the large benchmark email data sets, trec05p-1 and trec06p.
For our first ROSVM test, we experiment on the effect of reducing the size of the optimization problem by only considering the p most recent examples, as described in the previous section. For this test, we use the same 4-gram mappings as for the reference experiments in Section 2, with the same value C = 100. We test a range of values p in a coarse grid search. Figure 5 reports the effect of the buffer size p in relationship to the (1-ROCA)% performance measure (top), and the number of CPU seconds required (bottom).
The results show that values of p < 100 do result in degraded performance, although they evaluate very quickly.
However, p values from 500 to 10,000 perform almost as well as the original Online SMO (represented here as p = 100, 000), at dramatically reduced computational cost.
These results are important for making state of the art performance on large-scale content-based spam detection practical with online SVMs. Ordinarily, the training time would grow quadratically with the number of seen examples.
However, fixing a value of p ensures that the training time is independent of the size of the data set. Furthermore, a lookback buffer allows the filter to adjust to concept drift.
10521 (1-ROCA)% Max Iters. trec06p trec05p-1 50000 100000 150000 200000 250000 10521 CPUSec.
Max Iters. trec06p trec05p-1 Figure 6: Reduced Iterations Tests.
In the second ROSVM test, we experiment with reducing the number of iterations. Our initial tests showed that the maximum number of iterations used by Online SMO was rarely much larger than 10 on content-based spam detection; thus we tested values of T = {1, 2, 5, ∞}. Other parameters were identical to the original Online SVM tests.
The results on this test were surprisingly stable (see Figure 6). Reducing the maximum number of SMO iterations per update had essentially no impact on classification performance, but did result in a moderate increase in speed. This suggests that any additional iterations are spent attempting to find improvements to a hyperplane that is already very close to optimal. These results show that for content-based spam detection, we can reduce computational cost by allowing only a single SMO iteration (that is, T = 1) with effectively equivalent performance.
For our third ROSVM experiment, we evaluate the impact of adjusting the parameter M to reduce the total number of updates. As noted before, when M = 1, the hyperplane is globally optimal at every step. Reducing M allows a slightly inconsistent hyperplane to persist until it encounters an example for which it is too inconsistent. We tested values of M from 0 to 1, at increments of 0.1. (Note that we used p = 10000 to decrease the cost of evaluating these tests.) The results for these tests are appear in Figure 7, and show that there is a slight degradation in performance with reduced values of M, and that this degradation in performance is accompanied by an increase in efficiency. Values of
(1-ROCA)% M trec05p-1 trec06p 5000 10000 15000 20000 25000 30000 35000 40000
CPUSec.
M trec05p-1 trec06p Figure 7: Reduced Updates Tests.
M > 0.7 give effectively equivalent performance as M = 1, and still reduce cost.
We now compare ROSVM against Online SVMs on the email spam, blog comment spam, and splog detection tasks.
These experiments show comparable performance on these tasks, at radically different costs. In the previous section, the effect of the different relaxation methods was tested separately. Here, we tested these methods together to create a full implementation of ROSVM. We chose the values p = 10000, T = 1, M = 0.8 for the email spam detection tasks. Note that these parameter values were selected as those allowing ROSVM to achieve comparable performance results with Online SVMs, in order to test total difference in computational cost. The splog and blog data sets were much smaller, so we set p = 100 for these tasks to allow meaningful comparisons between the reduced size and full size optimization problems. Because these values were not hand-tuned, both generalization performance and runtime results are meaningful in these experiments.
We compared Online SVMs and ROSVM on email spam, blog comment spam, and splog detection. For the email spam, we used the two large benchmark corpora, trec05p-1 and trec06p, in the standard online ordering. We randomly ordered both the blog comment spam corpus and the splog corpus to create online learning tasks. Note that this is a different setting than the leave-one-out cross validation task presented on these corpora in Section 2 - the results are not directly comparable. However, this experimental design Table 5: Email Spam Benchmark Data. These results compare Online SVM and ROSVM on email spam detection, using binary 4-gram feature space.
Score reported is (1-ROCA)%, where 0 is optimal. trec05p-1 trec05p-1 trec06p trec06p (1-ROC)% CPUs (1-ROC)% CPUs OnSVM 0.0084 242,160 0.0232 87,519 ROSVM 0.0090 24,720 0.0240 18,541 Table 6: Blog Comment Spam. These results comparing Online SVM and ROSVM on blog comment spam detection using binary 4-gram feature space.
Acc. Prec. Recall F1 CPUs OnSVM 0.926 0.930 0.962 0.946 139 ROSVM 0.923 0.925 0.965 0.945 11 does allow meaningful comparison between our two online methods on these content-based spam detection tasks.
We ran each method on each task, and report the results in Tables 5, 6, and 7. Note that the CPU time reported for each method was generated on the same computing system.
This time reflects only the time needed to complete online learning on tokenized data. We do not report the time taken to tokenize the data into binary 4-grams, as this is the same additive constant for all methods on each task. In all cases,
ROSVM was significantly less expensive computationally.
The comparison results shown in Tables 5, 6, and 7 are striking in two ways. First, they show that the performance of Online SVMs can be matched and even exceeded by relaxed margin methods. Second, they show a dramatic disparity in computational cost. ROSVM is an order of magnitude more efficient than the normal Online SVM, and gives comparable results. Furthermore, the fixed lookback buffer ensures that the cost of each update does not depend on the size of the data set already seen, unlike Online SVMs. Note the blog and splog data sets are relatively small, and results on these data sets must be considered preliminary. Overall, these results show that there is no need to pay the high cost of SVMs to achieve this level of performance on contentbased detection of spam. ROSVMs offer a far cheaper alternative with little or no performance loss.
In the past, academic researchers and industrial practitioners have disagreed on the best method for online contentbased detection of spam on the web. We have presented one resolution to this debate. Online SVMs do, indeed, proTable 7: Splog Data Set. These results compare Online SVM and ROSVM on splog detection using binary 4-gram feature space.
Acc. Prec. Recall F1 CPUs OnSVM 0.880 0.910 0.842 0.874 29353 ROSVM 0.878 0.902 0.849 0.875 1251 duce state-of-the-art performance on this task with proper adjustment of the tradeoff parameter C, but with cost that grows quadratically with the size of the data set. The high values of C required for best performance with SVMs show that the margin maximization of Online SVMs is overkill for this task. Thus, we have proposed a less expensive alternative, ROSVM, that relaxes this maximum margin requirement, and produces nearly equivalent results. These methods are efficient enough for large-scale filtering of contentbased spam in its many forms.
It is natural to ask why the task of content-based spam detection gets strong performance from ROSVM. After all, not all data allows the relaxation of SVM requirements. We conjecture that email spam, blog comment spam, and splogs all share the characteristic that a subset of features are particularly indicative of content being either spam or not spam.
These indicative features may be sparsely represented in the data set, because of spam methods such as word obfuscation, in which common spam words are intentionally misspelled in an attempt to reduce the effectiveness of word-based spam detection. Maximizing the margin may cause these sparsely represented features to be ignored, creating an overall reduction in performance. It appears that spam data is highly separable, allowing ROSVM to be successful with high values of C and little effort given to maximizing the margin.
Future work will determine how applicable relaxed SVMs are to the general problem of text classification.
Finally, we note that the success of relaxed SVM methods for content-based spam detection is a result that depends on the nature of spam data, which is potentially subject to change. Although it is currently true that ham and spam are linearly separable given an appropriate feature space, this assumption may be subject to attack. While our current methods appear robust against primitive attacks along these lines, such as the good word attack [24], we must explore the feasibility of more sophisticated attacks.
[1] A. Bratko and B. Filipic. Spam filtering using compression models. Technical Report IJS-DP-9227,
Department of Intelligent Systems, Jozef Stefan Institute, L jubljana, Slovenia, 2005. [2] G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In NIPS, pages 409-415, 2000. [3] G. V. Cormack. TREC 2006 spam track overview. In To appear in: The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings, 2006. [4] G. V. Cormack and A. Bratko. Batch and on-line spam filter comparison. In Proceedings of the Third Conference on Email and Anti-Spam (CEAS), 2006. [5] G. V. Cormack and T. R. Lynam. TREC 2005 spam track overview. In The Fourteenth Text REtrieval Conference (TREC 2005) Proceedings, 2005. [6] G. V. Cormack and T. R. Lynam. On-line supervised spam filter evaluation. Technical report, David R.
Cheriton School of Computer Science, University of Waterloo, Canada, February 2006. [7] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines. Cambridge University Press,
[8] D. DeCoste and K. Wagstaff. Alpha seeding for support vector machines. In KDD "00: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 345-349,
[9] H. Drucker, V. Vapnik, and D. Wu. Support vector machines for spam categorization. IEEE Transactions on Neural Networks, 10(5):1048-1054, 1999. [10] J. Goodman and W. Yin. Online discriminative spam filter training. In Proceedings of the Third Conference on Email and Anti-Spam (CEAS), 2006. [11] P. Graham. A plan for spam. 2002. [12] P. Graham. Better bayesian filtering. 2003. [13] Z. Gyongi and H. Garcia-Molina. Spam: It"s not just for inboxes anymore. Computer, 38(10):28-34, 2005. [14] T. Joachims. Text categorization with suport vector machines: Learning with many relevant features. In ECML "98: Proceedings of the 10th European Conference on Machine Learning, pages 137-142,
[15] T. Joachims. Training linear svms in linear time. In KDD "06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217-226, 2006. [16] J. Kivinen, A. Smola, and R. Williamson. Online learning with kernels. In Advances in Neural Information Processing Systems 14, pages 785-793.
MIT Press, 2002. [17] P. Kolari, T. Finin, and A. Joshi. SVMs for the blogosphere: Blog identification and splog detection.
AAAI Spring Symposium on Computational Approaches to Analyzing Weblogs, 2006. [18] W. Krauth and M. M´ezard. Learning algorithms with optimal stability in neural networks. Journal of Physics A, 20(11):745-752, 1987. [19] T. Lynam, G. Cormack, and D. Cheriton. On-line spam filter fusion. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 123-130, 2006. [20] V. Metsis, I. Androutsopoulos, and G. Paliouras.
Spam filtering with naive bayes - which naive bayes?
Third Conference on Email and Anti-Spam (CEAS),
[21] G. Mishne, D. Carmel, and R. Lempel. Blocking blog spam with language model disagreement. Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web (AIRWeb), May
[22] J. Platt. Sequenital minimal optimization: A fast algorithm for training support vector machines. In B. Scholkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector Learning. MIT Press, 1998. [23] B. Scholkopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization,

While the PageRank algorithm [13] has proven to be very effective for ranking Web pages, inaccurate PageRank results are induced because of web page manipulations by people for commercial interests. The manipulation problem is also called the Web spam, which refers to hyperlinked pages on the World Wide Web that are created with the intention of misleading search engines [7]. It is reported that approximately 70% of all pages in the .biz domain and about 35% of the pages in the .us domain belong to the spam category [12]. The reason for the increasing amount of Web spam is explained in [12]: some web site operators try to influence the positioning of their pages within search results because of the large fraction of web traffic originating from searches and the high potential monetary value of this traffic.
From the viewpoint of the Web site operators who want to increase the ranking value of a particular page for search engines, Keyword Stuffing and Link Stuffing are being used widely [7, 12]. From the viewpoint of the search engine managers, the Web spam is very harmful to the users" evaluations and thus their preference to choosing search engines because people believe that a good search engine should not return irrelevant or low-quality results. There are two methods being employed to combat the Web spam problem. Machine learning methods are employed to handle the keyword stuffing. To successfully apply machine learning methods, we need to dig out some useful textual features for Web pages, to mark part of the Web pages as either spam or non-spam, then to apply supervised learning techniques to mark other pages. For example, see [5, 12]. Link analysis methods are also employed to handle the link stuffing problem. One example is the TrustRank [7], a link-based method, in which the link structure is utilized so that human labelled trusted pages can propagate their trust scores trough their links.
This paper focuses on the link-based method.
The rest of the materials are organized as follows. In the next section, we give a brief literature review on various related ranking techniques. We establish the Heat Diffusion Model (HDM) on various cases in Section 3, and propose DiffusionRank in Section 4. In Section 5, we describe the data sets that we worked on and the experimental results.
Finally, we draw conclusions in Section 6.
The importance of a Web page is determined by either the textual content of pages or the hyperlink structure or both. As in previous work [7, 13], we focus on ranking methods solely determined by hyperlink structure of the Web graph. All the mentioned ranking algorithms are established on a graph. For our convenience, we first give some notations. Denote a static graph by G = (V, E), where V = {v1, v2, . . . , vn}, E = {(vi, vj) | there is an edge from vi to vj}. Ii and di denote the in-degree and the out-degree of page i respectively.
The importance of a Web page is an inherently subjective matter, which depends on the reader"s interests, knowledge and attitudes [13]. However, the average importance of all readers can be considered as an objective matter. PageRank tries to find such average importance based on the Web link structure, which is considered to contain a large amount of statistical data. The Web is modelled by a directed graph G in the PageRank algorithms, and the rank or importance xi for page vi ∈ V is defined recursively in terms of pages which point to it: xi = (j,i)∈E aijxj, where aij is assumed to be 1/dj if there is a link from j to i, and 0 otherwise. Or in matrix terms, x = Ax. When the concept of random jump is introduced, the matrix form is changed to x = [(1 − α)g1T + αA]x, (1) where α is the probability of following the actual link from a page, (1 − α) is the probability of taking a random jump, and g is a stochastic vector, i.e., 1T g = 1. Typically, α =
n
the vector of all ones [6, 13].
TrustRank [7] is composed of two parts. The first part is the seed selection algorithm, in which the inverse PageRank was proposed to help an expert of determining a good node. The second part is to utilize the biased PageRank, in which the stochastic distribution g is set to be shared by all the trusted pages found in the first part. Moreover, the initial input of x is also set to be g. The justification for the inverse PageRank and the solid experiments support its advantage in combating the Web spam. Although there are many variations of PageRank, e.g., a family of link-based ranking algorithms in [2], TrustRank is especially chosen for comparisons for three reasonss: (1) it is designed for combatting spamming; (2) its fixed parameters make a comparison easy; and (3) it has a strong theoretical relations with PageRank and DiffusionRank.
In [17], the idea of ranking on the data manifolds was proposed. The data points represented as vectors in Euclidean space are considered to be drawn from a manifold. From the data points on such a manifold, an undirected weighted graph is created, then the weight matrix is given by the Gaussian Kernel smoothing. While the manifold ranking algorithm achieves an impressive result on ranking images, the biased vector g and the parameter k in the general personalized PageRank in [17] are unknown in the Web graph setting; therefore we do not include it in the comparisons.
Heat diffusion is a physical phenomena. In a medium, heat always flow from position with high temperature to position with low temperature. Heat kernel is used to describe the amount of heat that one point receives from another point. Recently, the idea of heat kernel on a manifold is borrowed in applications such as dimension reduction [3] and classification [9, 10, 14]. In these work, the input data is considered to lie in a special structure.
All the above topics are related to our work. The readers can find that our model is a generalization of PageRank in order to resist Web manipulation, that we inherit the first part of TrustRank, that we borrow the concept of ranking on the manifold to introduce our model, and that heat diffusion is a main scheme in this paper.
Heat diffusion provides us with another perspective about how we can view the Web and also a way to calculate ranking values. In this paper, the Web pages are considered to be drawn from an unknown manifold, and the link structure forms a directed graph, which is considered as an approximation to the unknown manifold. The heat kernel established on the Web graph is considered as the representation of the relationship between Web pages. The temperature distribution after a fixed time period, induced by a special initial temperature distribution, is considered as the rank scores on the Web pages. Before establishing the proposed models, we first show our motivations.
There are two points to explain that PageRank is susceptible to web spam. • Over-democratic. There is a belief behind PageRank-all pages are born equal. This can be seen from the equal voting ability of one page: the sum of each column is equal to one. This equal voting ability of all pages gives the chance for a Web site operator to increase a manipulated page by creating a large number of new pages pointing to this page since all the newly created pages can obtain an equal voting right. • Input-independent. For any given non-zero initial input, the iteration will converge to the same stable distribution corresponding to the maximum eigenvalue
property makes it impossible to set a special initial input (larger values for trusted pages and less values even negative values for spam pages) to avoid web spam.
The input-independent feature of PageRank can be further explained as follows. P = [(1 − α)g1T + αA] is a positive stochastic matrix if g is set to be a positive stochastic vector (the uniform distribution is one of such settings), and so the largest eigenvalue is 1 and no other eigenvalue whose absolute value is equal to 1, which is guaranteed by the Perron Theorem [11]. Let y be the eigenvector corresponding to 1, then we have Py = y. Let {xk} be the sequence generated from the iterations xk+1 = Pxk, and x0 is the initial input.
If {xk} converges to x, then xk+1 = Pxk implies that x must satisfy Px = x. Since the only maximum eigenvalue is 1, we have x = cy where c is a constant, and if both x and y are normalized by their sums, then c = 1. The above discussions show that PageRank is independent of the initial input x0.
In our opinion, g and α are objective parameters determined by the users" behaviors and preferences. A, α and g are the true web structure. While A is obtained by a crawler and the setting α = 0.85 is accepted by the people, we think that g should be determined by a user behavior investigation, something like [1]. Without any prior knowledge, g has to be set as g = 1 n
TrustRank model does not follow the true web structure by setting a biased g, but the effects of combatting spamming are achieved in [7]; PageRank is on the contrary in some ways. We expect a ranking algorithm that has an effect of anti-manipulation as TrustRank while respecting the true web structure as PageRank.
We observe that the heat diffusion model is a natural way to avoid the over-democratic and input-independent feature of PageRank. Since heat always flows from a position with higher temperatures to one with lower temperatures, points are not equal as some points are born with high temperatures while others are born with low temperatures. On the other hand, different initial temperature distributions will give rise to different temperature distributions after a fixed time period. Based on these considerations, we propose the novel DiffusionRank. This ranking algorithm is also motivated by the viewpoint for the Web structure. We view all the Web pages as points drawn from a highly complex geometric structure, like a manifold in a high dimensional space. On a manifold, heat can flow from one point to another through the underlying geometric structure in a given time period. Different geometric structures determine different heat diffusion behaviors, and conversely the diffusion behavior can reflect the geometric structure. More specifically, on the manifold, the heat flows from one point to another point, and in a given time period, if one point x receives a large amount of heat from another point y, we can say x and y are well connected, and thus x and y have a high similarity in the sense of a high mutual connection.
We note that on a point with unit mass, the temperature and the heat of this point are equivalent, and these two terms are interchangeable in this paper. In the following, we first show the HDM on a manifold, which is the origin of HDM, but cannot be employed to the World Wide Web directly, and so is considered as the ideal case. To connect the ideal case and the practical case, we then establish HDM on a graph as an intermediate case. To model the real world problem, we further build HDM on a random graph as a practical case. Finally we demonstrate the DiffusionRank which is derived from the HDM on a random graph.
If the underlying manifold is known, the heat flow throughout a geometric manifold with initial conditions can be described by the following second order differential equation: ∂f(x,t) ∂t − ∆f(x, t) = 0, where f(x, t) is the heat at location x at time t, and ∆f is the Laplace-Beltrami operator on a function f. The heat diffusion kernel Kt(x, y) is a special solution to the heat equation with a special initial condition-a unit heat source at position y when there is no heat in other positions. Based on this, the heat kernel Kt(x, y) describes the heat distribution at time t diffusing from the initial unit heat source at position y, and thus describes the connectivity (which is considered as a kind of similarity) between x and y. However, it is very difficult to represent the World Wide Web as a regular geometry with a known dimension; even the underlying is known, it is very difficult to find the heat kernel Kt(x, y), which involves solving the heat equation with the delta function as the initial condition. This motivates us to investigate the heat flow on a graph. The graph is considered as an approximation to the underlying manifold, and so the heat flow on the graph is considered as an approximation to the heat flow on the manifold.
On an undirected graph G, the edge (vi, vj) is considered as a pipe that connects nodes vi and vj. The value fi(t) describes the heat at node vi at time t, beginning from an initial distribution of heat given by fi(0) at time zero. f(t) (f(0)) denotes the vector consisting of fi(t) (fi(0)).
We construct our model as follows. Suppose, at time t, each node i receives M(i, j, t, ∆t) amount of heat from its neighbor j during a period of ∆t. The heat M(i, j, t, ∆t) should be proportional to the time period ∆t and the heat difference fj(t) − fi(t). Moreover, the heat flows from node j to node i through the pipe that connects nodes i and j.
Based on this consideration, we assume that M(i, j, t, ∆t) = γ(fj(t) − fi(t))∆t. As a result, the heat difference at node i between time t + ∆t and time t will be equal to the sum of the heat that it receives from all its neighbors. This is formulated as fi(t + ∆t) − fi(t) = j:(j,i)∈E γ(fj(t) − fi(t))∆t, (2) where E is the set of edges. To find a closed form solution to Eq. (2), we express it in a matrix form: (f(t + ∆t) − f(t))/∆t = γHf(t), where d(v) denotes the degree of the node v. In the limit ∆t → 0, it becomes d dt f(t) = γHf(t).
Solving it, we obtain f(t) = eγtH f(0), especially we have f(1) = eγH f(0), Hij =    −d(vj), j = i, 1, (vj, vi) ∈ E, 0, otherwise, (3) where eγH is defined as eγH = I+γH+ γ2 2! H2 + γ3 3! H3 +· · · .
The above heat diffusion model must be modified to fit the situation where the links between Web pages are directed.
On one Web page, when the page-maker creates a link (a, b) to another page b, he actually forces the energy flow, for example, people"s click-through activities, to that page, and so there is added energy imposed on the link. As a result, heat flows in a one-way manner, only from a to b, not from b to a. Based on such consideration, we modified the heat diffusion model on an undirected graph as follows.
On a directed graph G, the pipe (vi, vj) is forced by added energy such that heat flows only from vi to vj. Suppose, at time t, each node vi receives RH = RH(i, j, t, ∆t) amount of heat from vj during a period of ∆t. We have three assumptions: (1) RH should be proportional to the time period ∆t; (2) RH should be proportional to the the heat at node vj; and (3) RH is zero if there is no link from vj to vi. As a result, vi will receive j:(vj ,vi)∈E σjfj(t)∆t amount of heat from all its neighbors that points to it.
On the other hand, node vi diffuses DH(i, t, ∆t) amount of heat to its subsequent nodes. We assume that: (1) The heat DH(i, t, ∆t) should be proportional to the time period ∆t. (2) The heat DH(i, t, ∆t) should be proportional to the the heat at node vi. (3) Each node has the same ability of diffusing heat. This fits the intuition that a Web surfer only has one choice to find the next page that he wants to browse. (4) The heat DH(i, t, ∆t) should be uniformly distributed to its subsequent nodes. The real situation is more complex than what we assume, but we have to make this simple assumption in order to make our model concise. As a result, node vi will diffuse γfi(t)∆t/di amount of heat to any of its subsequent nodes, and each of its subsequent node should receive γfi(t)∆t/di amount of heat. Therefore σj = γ/dj.
To sum up, the heat difference at node vi between time t+∆t and time t will be equal to the sum of the heat that it receives, deducted by what it diffuses. This is formulated as fi(t + ∆t) − fi(t) = −γfi(t)∆t + j:(vj ,vi)∈E γ/djfj(t)∆t.
Similarly, we obtain f(1) = eγH f(0), Hij =    −1, j = i, 1/dj, (vj, vi) ∈ E, 0, otherwise. (4)
For real world applications, we have to consider random edges. This can be seen in two viewpoints. The first one is that in Eq. (1), the Web graph is actually modelled as a random graph, there is an edge from node vi to node vj with a probability of (1 − α)gj (see the item (1 − α)g1T ), and that the Web graph is predicted by a random graph [15, 16]. The second one is that the Web structure is a random graph in essence if we consider the content similarity between two pages, though this is not done in this paper.
For these reasons, the model would become more flexible if we extend it to random graphs. The definition of a random graph is given below.
Definition 1. A random graph RG = (V, P = (pij)) is defined as a graph with a vertex set V in which the edges are chosen independently, and for 1 ≤ i, j ≤ |V | the probability of (vi, vj) being an edge is exactly pij.
The original definition of random graphs in [4], is changed slightly to consider the situation of directed graphs. Note that every static graph can be considered as a special random graph in the sense that pij can only be 0 or 1.
On a random graph RG = (V, P), where P = (pij) is the probability of the edge (vi, vj) exists. In such a random graph, the expected heat difference at node i between time t + ∆t and time t will be equal to the sum of the expected heat that it receives from all its antecedents, deducted by the expected heat that it diffuses.
Since the probability of the link (vj, vi) is pji, the expected heat flow from node j to node i should be multiplied by pji, and so we have fi(t + ∆t) − fi(t) = −γ fi(t) ∆t + j:(vj ,vi)∈E γpjifj(t)∆t/RD+ (vj), where RD+ (vi) is the expected out-degree of node vi, it is defined as k pik.
Similarly we have f(1) = eγR f(0), Rij =    −1, j = i; pji/RD+ (vj), j = i. (5) When the graph is large, a direct computation of eγR is time-consuming, and we adopt its discrete approximation: f(1) = (I + γ N R)N f(0). (6) The matrix (I+ γ N R)N in Eq. (6) and matrix eγR in Eq. (5) are called Discrete Diffusion Kernel and the Continuous Diffusion Kernel respectively. Based on the Heat Diffusion Models and their solutions, DiffusionRank can be established on undirected graphs, directed graphs, and random graphs. In the next section, we mainly focus on DiffusionRank in the random graph setting.
For a random graph, the matrix (I + γ N R)N or eγR can measure the similarity relationship between nodes. Let fi(0)= 1, fj(0) = 0 if j = i, then the vector f(0) represent the unit heat at node vi while all other nodes has zero heat. For such f(0) in a random graph, we can find the heat distribution at time 1 by using Eq. (5) or Eq. (6). The heat distribution is exactly the i−th row of the matrix of (I + γ N R)N or eγR . So the ith-row jth-column element hij in the matrix (I + γ∆tR)N or eγR means the amount of heat that vi can receive from vj from time 0 to 1. Thus the value hij can be used to measure the similarity from vj to vi. For a static graph, similarly the matrix (I + γ N H)N or eγH can measure the similarity relationship between nodes.
The intuition behind is that the amount h(i, j) of heat that a page vi receives from a unit heat in a page vj in a unit time embodies the extent of the link connections from page vj to page vi. Roughly speaking, when there are more uncrossed paths from vj to vi, vi will receive more heat from vj; when the path length from vj to vi is shorter, vi will receive more heat from vj; and when the pipe connecting vj and vi is wide, the heat will flow quickly. The final heat that vi receives will depend on various paths from vj to vi, their length, and the width of the pipes.
Algorithm 1 DiffusionRank Function Input: The transition matrix A; the inverse transition matrix U; the decay factor αI for the inverse PageRank; the decay factor αB for PageRank; number of iterations MI for the inverse PageRank; the number of trusted pages L; the thermal conductivity coefficient γ.
Output: DiffusionRank score vector h. 1: s = 1 2: for i = 1 TO MI do 3: s = αI · U · s + (1 − αI ) · 1 n · 1 4: end for 5: Sort s in a decreasing order: π = Rank({1, . . . , n}, s) 6: d = 0, Count = 0, i = 0 7: while Count ≤ L do 8: if π(i) is evaluated as a trusted page then 9: d(π(i)) = 1, Count + + 10: end if 11: i + + 12: end while 13: d = d/|d| 14: h = d 15: Find the iteration number MB according to λ 16: for i = 1 TO MB do 17: h = (1 − γ MB )h + γ MB (αB · A · h + (1 − αB) · 1 n · 1) 18: end for 19: RETURN h
For the ranking task, we adopt the heat kernel on a random graph. Formally the DiffusionRank is described in Algorithm 1, in which,the element Uij in the inverse transition matrix U is defined to be 1/Ij if there is a link from i to j, and 0 otherwise. This trusted pages selection procedure by inverse PageRank is completely borrowed from TrustRank [7] except for a fix number of the size of the trusted set.
Although the inverse PageRank is not perfect in its ability of determining the maximum coverage, it is appealing because of its polynomial execution time and its reasonable intuition-we actually inverse the original link when we try to build the seed set from those pages that point to many pages that in turn point to many pages and so on. In the algorithm, the underlying random graph is set as P = αB · A + (1 − αB) · 1 n · 1n×n, which is induced by the Web graph. As a result, R = −I + P.
In fact, the more general setting for DiffusionRank is P = αB ·A+(1−αB)· 1 n ·g·1T . By such a setting, DiffusionRank is a generalization of TrustRank when γ tends to infinity and when g is set in the same way as TrustRank. However, the second part of TrustRank is not adopted by us. In our model, g should be the true teleportation determined by the user"s browse habits, popularity distribution over all the Web pages, and so on; P should be the true model of the random nature of the World Wide Web. Setting g according to the trusted pages will not be consistent with the basic idea of Heat Diffusion on a random graph. We simply set g = 1 only because we cannot find it without any priori knowledge.
Remark. In a social network interpretation,
DiffusionRank first recognizes a group of trusted people, who may not be highly ranked, but they know many other people.
The initially trusted people are endowed with the power to decide who can be further trusted, but cannot decide the final voting results, and so they are not dictators.
Next we show the four advantages for DiffusionRank.
First, its solutions have two forms, both of which are closed form. One takes the discrete form, and has the advantage of fast computing while the other takes the continuous form, and has the advantage of being easily analyzed in theoretical aspects. The theoretical advantage has been shown in the proof of theorem in the next section. (a) Group to Group Relations (b) An undirected graph Figure 1: Two graphs
Second, it can be naturally employed to detect the groupgroup relation. For example, let G2 and G1 denote two groups, containing pages (j1, j2, . . . , js) and (i1, i2, . . . , it), respectively. Then u,v hiu,jv is the total amounts of heat that G1 receives from G2, where hiu,jv is the iu−th row jv−th column element of the heat kernel. More specifically, we need to first set f(0) for such an application as follows.
In f(0) = (f1(0), f2(0), . . . , fn(0))T , if i ∈ {j1, j2, . . . , js}, then fi(0) = 1, and 0 otherwise. Then we employ Eq. (5) to calculate f(1) = (f1(1), f2(1), . . . , fn(1))T , finally we sum those fj(1) where j ∈ {i1, i2, . . . , it}. Fig. 1 (a) shows the results generated by the DiffusionRank. We consider five groups-five departments in our Engineering Faculty: CSE,
MAE, EE, IE, and SE. γ is set to be 1, the numbers in Fig. 1 (a) are the amount of heat that they diffuse to each other. These results are normalized by the total number of each group, and the edges are ignored if the values are less than 0.000001. The group-to-group relations are therefore detected, for example, we can see that the most strong overall tie is from EE to IE. While it is a natural application for DiffusionRank because of the easy interpretation by the amount heat from one group to another group, it is difficult to apply other ranking techniques to such an application because they lack such a physical meaning.
Third, it can be used to partition the Web graph into several parts. A quick example is shown below. The graph in Fig. 1 (b) is an undirected graph, and so we employ the Eq. (3). If we know that node 1 belongs to one community and that node 12 belongs to another community, then we can put one unit positive heat source on node 1 and one unit negative heat source on node 12. After time 1, if we set γ = 0.5, the heat distribution is [0.25, 0.16, 0.17,
we set γ = 1, it will be [0.17, 0.16, 0.17, 0.16, 0.16, 0.12,
can easily divide the graph into two parts: {1, 2, 3, 4, 5, 6, 7} with positive temperatures and {8, 9, 10, 11, 12} with negative temperatures. For directed graphs and random graphs, similarly we can cut them by employing corresponding heat solution.
Fourth, it can be used to combat manipulation. Let G2 contain trusted Web pages (j1, j2, . . . , js), then for each page i, v hi,jv is the heat that page i receives from G2, and can be computed by the discrete approximation of Eq. (4) in the case of a static graph or Eq. (6) in the case of a random graph, in which f(0) is set to be a special initial heat distribution so that the trusted Web pages have unit heat while all the others have zero heat. In doing so, manipulated Web page will get a lower rank unless it has strong in-links from the trusted Web pages directly or indirectly. The situation is quite different for PageRank because PageRank is inputindependent as we have shown in Section 3.1. Based on the fact that the connection from a trusted page to a bad page should be weak-less uncross paths, longer distance and narrower pipe, we can say DiffusionRank can resist web spam if we can select trusted pages. It is fortunate that the trusted pages selection method in [7]-the first part of TrustRank can help us to fulfill this task. For such an application of DiffusionRank, the computation complexity for Discrete Diffusion Kernel is the same as that for PageRank in cases of both a static graph and a random graph. This can be seen in Eq. (6), by which we need N iterations and for each iteration we need a multiplication operation between a matrix and a vector, while in Eq. (1) we also need a multiplication operation between a matrix and a vector for each iteration.
γ plays an important role in the anti-manipulation effect of DiffusionRank. γ is the thermal conductivity-the heat diffusion coefficient. If it has a high value, heat will diffuse very quickly. Conversely, if it is small, heat will diffuse slowly. In the extreme case, if it is infinitely large, then heat will diffuse from one node to other nodes immediately, and this is exactly the case corresponding to PageRank. Next, we will interpret it mathematically.
Theorem 1. When γ tends to infinity and f(0) is not the zero vector, eγR f(0) is proportional to the stable distribution produced by PageRank.
Let g = 1 n
that 1 is the largest eigenvalue of P = [(1 − α)g1T + αA], and that no other eigenvalue whose absolute value is equal to 1. Let x be the stable distribution, and so Px = x. x is the eigenvector corresponding to the eigenvalue 1. Assume the n − 1 other eigenvalues of P are |λ2| < 1, . . . , |λn| < 1, we can find an invertible matrix S = ( x S1 ) such that S−1 PS =     
... ∗
     . (7) Since eγR = eγ(−I+P) = S−1     
∗ ∗
... ∗
     S, (8) all eigenvalues of the matrix eγR are 1, eγ(λ2−1) , . . . , eγ(λn−1) .
When γ → ∞, they become 1, 0, . . . , 0, which means that 1 is the only nonzero eigenvalue of eγR when γ → ∞. We can see that when γ → ∞, eγR eγR f(0) = eγR f(0), and so eγR f(0) is an eigenvector of eγR when γ → ∞. On the other hand, eγR x = (I+γR+γ2 2! R2 +γ3 3! R3 +. . .)x = Ix+γRx+γ2 2! R2 x+ γ3 3! R3 x + . . . = x since Rx = (−I + P)x = −x + x = 0, and hence x is the eigenvector of eγR for any γ. Therefore both x and eγR f(0) are the eigenvectors corresponding the unique eigenvalue 1 of eγR when γ → ∞, and consequently x = ceγR f(0).
By this theorem, we see that DiffusionRank is a generalization of PageRank. When γ = 0, the ranking value is most robust to manipulation since no heat is diffused and the system is unchangeable, but the Web structure is completely ignored since eγR f(0) = e0R f(0) = If(0) = f(0); when γ = ∞, DiffusionRank becomes PageRank, it can be manipulated easily. We expect an appropriate setting of γ that can balance both. For this, we have no theoretical result, but in practice we find that γ = 1 works well in Section 5. Next we discuss how to determine the number of iterations if we employ the discrete heat kernel.
While we enjoy the advantage of the concise form of the exponential heat kernel, it is better for us to calculate DiffusionRank by employing Eq. (6) in an iterative way. Then the problem about determining N-the number of iterations arises: For a given threshold , find N such that ||((I + γ N R)N − eγR )f(0)|| < for any f(0) whose sum is one.
Since it is difficult to solve this problem, we propose a heuristic motivated by the following observations. When R = −I+P, by Eq. (7), we have (I+ γ N R)N = (I+ γ N (−I+ P))N = S−1     
N )N ∗ ∗
... ∗
N )N      S. (9) Comparing Eq. (8) and Eq. (9), we observe that the eigenvalues of (I + γ N R)N − eγR are (1 + γ(λn−1) N )N − eγ(λn−1) .
We propose a heuristic method to determine N so that the difference between the eigenvalues are less than a threshold for only positive λs.
We also observe that if γ = 1, λ < 1, then |(1+ γ(λ−1) N )N − eγ(λ−1) | < 0.005 if N ≥ 100, and |(1+ γ(λ−1) N )N −eγ(λ−1) | <
according to different accuracy requirements. In this paper, we use the relatively accurate setting N = 100 to make the real eigenvalues in (I + γ N R)N − eγR less than 0.005.
In this section, we show the experimental data, the methodology, the setting, and the results.
Our input data consist of a toy graph, a middle-size realworld graph, and a large-size real-world graph. The toy graph is shown in Fig. 2 (a). The graph below it shows node
such that they all point to node 1, and node 1 points to them all. The data of two real Web graph were obtained from the domain in our institute in October, 2004. The total number of pages found are 18,542 in the middle-size graph, and 607,170 in the large-size graph respectively. The middle-size graph is a subgraph of the large-size graph, and they were obtained by the same crawler: one is recorded by the crawler in its earlier time, and the other is obtained when the crawler stopped.
The algorithms we run include PageRank, TrustRank and DiffusionRank. All the rank values are multiplied by the number of nodes so that the sum of the rank values is equal to the number of nodes. By this normalization, we can compare the results on graphs with different sizes since the average rank value is one for any graph after such normalization.
We will need value difference and pairwise order difference as comparison measures. Their definitions are listed as follows.
Value Difference. The value difference between A = {Ai}n i=1 and B = {Bi}n i=1 is measured as n i=1 |Ai − Bi|.
Pairwise Order Difference. The order difference between A and B is measured as the number of significant order differences between A and B. The pair (A[i], A[j]) and (B[i], B[j]) is considered as a significant order difference if one of the following cases happens: both A[i] > [ <]A[j]+0.1 and B[i] ≤ [ ≥]A[j]; both A[i] ≤ [ ≥]A[j] and B[i] > [ < ]A[j] + 0.1.
A 1 B C ... 2 5
4 1
0 2 4 6 8 10 12 Gamma ValueDifference Trust set={1} Trust set={2} Trust set={3} Trust set={4} Trust set={5} Trust set={6} (a) (b) Figure 2: (a) The toy graph consisting of six nodes, and node 1 is being manipulated by adding new nodes A, B, C, . . . (b) The approximation tendency to PageRank by DiffusionRank
The experiments on the middle-size graph and the largesize graphs are conducted on the workstation, whose hardware model is Nix Dual Intel Xeon 2.2GHz with 1GB RAM and a Linux Kernel 2.4.18-27smp (RedHat7.3). In calculating DiffusionRank, we employ Eq. (6) and the discrete approximation of Eq. (4) for such graphs. The related tasks are implemented using C language. While in the toy graph, we employ the continuous diffusion kernel in Eq. (4) and Eq. (5), and implement related tasks using Matlab.
For nodes that have zero out-degree (dangling nodes), we employ the method in the modified PageRank algorithm [8], in which dangling nodes of are considered to have random links uniformly to each node. We set α = αI = αB = 0.85 in all algorithms. We also set g to be the uniform distribution in both PageRank and DiffusionRank. For DiffusionRank, we set γ = 1. According to the discussions in Section 4.3 and Section 4.4, we set the iteration number to be MB = 100 in DiffusionRank, and for accuracy consideration, the iteration number in all the algorithms is set to be 100.
We show that when γ tends to infinity, the value differences between DiffusionRank and PageRank tend to zero.
Fig. 2 (b) shows the approximation property of DiffusionRank, as proved in Theorem 1, on the toy graph. The horizontal axis of Fig. 2 (b) marks the γ value, and vertical axis corresponds to the value difference between DiffusionRank and PageRank. All the possible trusted sets with L = 1 are considered. For L > 1, the results should be the linear combination of some of these curves because of the linearity of the solutions to heat equations. On other graphs, the situations are similar.
In this section, we show how the rank values change as the intensity of manipulation increases. We measure the intensity of manipulation by the number of newly added points that point to the manipulated point. The horizontal axes of Fig. 3 stand for the numbers of newly added points, and vertical axes show the corresponding rank values of the manipulated nodes. To be clear, we consider all six situations.
Every node in Fig. 2 (a) is manipulated respectively, and its
0 10 20 30 40 50 RankoftheManipulatdNode−1 DiffusionRank−Trust 4 PageRank TrustRanl−Trust 4
0 10 20 30 40 50 RankoftheManipulatdNode−2 DiffusionRank−Trust 4 PageRank TrustRanl−Trust 4
0 10 20 30 40 50 RankoftheManipulatdNode−3 DiffusionRank−Trust 4 PageRank TrustRanl−Trust 4
0 10 20 30 40 50 Number of New Added Nodes RankoftheManipulatdNode−4 DiffusionRank−Trust 3 PageRank TrustRanl−Trust 3
0 10 20 30 40 50 Number of New Added Nodes RankoftheManipulatdNode−5 DiffusionRank−Trust 4 PageRank TrustRanl−Trust 4
0 10 20 30 40 50 Number of New Added Nodes RankoftheManipulatdNode−6 DiffusionRank−Trust 4 PageRank TrustRanl−Trust 4 Figure 3: The rank values of the manipulated nodes on the toy graph 200040006000800010000 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of New Added Points RankoftheManipulatdNode PageRank DiffusionRank−uniform DiffusionRank0 DiffusionRank1 DiffusionRank2 DiffusionRank3 TrustRank0 TrustRank1 TrustRank2 TrustRank3
0 20 40 60 80 100 120 140 160 180 Number of New Added Points RankoftheManipulatdNode PageRank DiffusionRank TrustRank DiffusionRank−uniform (a) (b) Figure 4: (a) The rank values of the manipulated nodes on the middle-size graph; (b) The rank values of the manipulated nodes on the large-size graph corresponding values for PageRank, TrustRank (TR),
DiffusionRank (DR) are shown in the one of six sub-figures in Fig. 3. The vertical axes show which node is being manipulated. In each sub-figure, the trusted sets are computed below. Since the inverse PageRank yields the results [1.26, 0.85, 1.31, 1.36, 0.51, 0.71]. Let L = 1. If the manipulated node is not 4, then the trusted set is {4}, and otherwise {3}. We observe that in all the cases, rank values of the manipulated node for DiffusionRank grow slowest as the number of the newly added nodes increases. On the middle-size graph and the large-size graph, this conclusion is also true, see Fig. 4. Note that, in Fig. 4 (a), we choose four trusted sets (L = 1), on which we test DiffusionRank and TrustRank, the results are denoted by DiffusionRanki and TrustRanki (i = 0, 1, 2, 3 denotes the four trusted set); in Fig. 4 (b), we choose one trusted set (L = 1). Moreover, in both Fig. 4 (a) and Fig. 4 (b), we show the results for DiffusionRank when we have no trusted set, and we trust all the pages before some of them are manipulated.
We also test the order difference between the ranking order A before the page is manipulated and the ranking order PA after the page is manipulated. Because after manipulation, the number of pages changes, we only compare the common part of A and PA. This experiment is used to test the stability of all these algorithms. The less the order difference, the stabler the algorithm, in the sense that only a smaller part of the order relations is affected by the manipulation. Figure 5 (a) shows that the order difference values change when we add new nodes that point to the manipulated node. We give several γ settings. We find that when γ = 1, the least order difference is achieved by DiffusionRank. It is interesting to point out that as γ increases, the order difference will increase first; after reaching a maximum value, it will decrease, and finally it tends to the PageRank results. We show this tendency in Fig. 5 (b), in which we choose three different settings-the number of manipulated nodes are 2,000, 5,000, and 10,000 respectively. From these figures, we can see that when γ < 2, the values are less than those for PageRank, and that when γ > 20, the difference between PageRank and DiffusionRank is very small.
After these investigations, we find that in all the graphs we tested, DiffusionRank (when γ = 1) is most robust to manipulation both in value difference and order difference. The trust set selection algorithm proposed in [7] is effective for both TrustRank and DiffusionRank.
0
1
2
3 x 10 5 Number of New Added Points PairwiseOrderDifference PageRank DiffusionRank−Gamma=1 DiffusionRank−Gamma=2 DiffusionRank−Gamma=3 DiffusionRank−Gamma=4 DiffusionRank−Gamma=5 DiffusionRank−Gamma=15 TrustRank
0
1
2
x 10 5 Gamma PairwiseOrderDifference DiffusionRank: when added 2000 nodes DiffusionRank: when added 5000 nodes DiffusionRank: when added 10000 nodes PageRank (a) (b) Figure 5: (a) Pairwise order difference on the middle-size graph, the least it is, the more stable the algorithm; (b) The tendency of varying γ
We conclude that DiffusionRank is a generalization of PageRank, which is interesting in that the heat diffusion coefficient γ can balance the extent that we want to model the original Web graph and the extent that we want to reduce the effect of link manipulations. The experimental results show that we can actually achieve such a balance by setting γ = 1, although the best setting including varying γi is still under further investigation. This anti-manipulation feature enables DiffusionRank to be a candidate as a penicillin for Web spamming. Moreover, DiffusionRank can be employed to find group-group relations and to partition Web graph into small communities. All these advantages can be achieved in the same computational complexity as PageRank. For the special application of anti-manipulation,
DiffusionRank performs best both in reduction effects and in its stability among all the three algorithms.
We thank Patrick Lau, Zhenjiang Lin and Zenglin Xu for their help. This work is fully supported by two grants from the Research Grants Council of the Hong Kong Special administrative Region, China (Project No. CUHK4205/04E and Project No. CUHK4235/04E).
[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search ranking by incorporating user behavior information. In E. N.
Efthimiadis, S. T. Dumais, D. Hawking, and K. J¨arvelin, editors, Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 19-26, 2006. [2] R. A. Baeza-Yates, P. Boldi, and C. Castillo. Generalizing pagerank: damping functions for link-based ranking algorithms. In E. N. Efthimiadis, S. T. Dumais, D. Hawking, and K. J¨arvelin, editors, Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 308-315, 2006. [3] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373-1396, Jun 2003. [4] B. Bollob´as. Random Graphs. Academic Press Inc. (London),
[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML), pages 89-96, 2005. [6] N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the web frontier. In Proceeding of the 13th World Wide Web Conference (WWW), pages 309-318, 2004. [7] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. Combating web spam with trustrank. In M. A. Nascimento, M. T. ¨Ozsu,
D. Kossmann, R. J. Miller, J. A. Blakeley, and K. B. Schiefer, editors, Proceedings of the Thirtieth International Conference on Very Large Data Bases (VLDB), pages 576-587, 2004. [8] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H.
Golub. Exploiting the block structure of the web for computing pagerank. Technical report, Stanford University, 2003. [9] R. I. Kondor and J. D. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In C. Sammut and A. G.
Hoffmann, editors, Proceedings of the Nineteenth International Conference on Machine Learning (ICML), pages 315-322, 2002. [10] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. Journal of Machine Learning Research, 6:129-163,

Personal Information Management (PIM) is a rapidly growing area of research concerned with how people store, manage and re-find information. PIM systems - the methods and procedures by which people handle, categorize, and retrieve information on a day-to-day basis [18] - are becoming increasingly popular. However the evaluation of these PIM systems is problematic. One of the main difficulties is caused by the personal nature of PIM. People collect information as a natural consequence of completing other tasks.
This means that the collections people generate are unique to them alone and the information within a collection is intrinsically linked with the owner"s personal experiences. As personal collections are unique, we cannot create evaluation tasks that are applicable to all participants in an evaluation.
Secondly, personal collections may contain information that the participants are uncomfortable sharing within an evaluation. The precise nature of this information - what information individuals would prefer to keep private - varies across individuals making it difficult to base search tasks on the contents of individual collections. Therefore, experimenters face a number of challenges in order to conduct realistic but controlled PIM evaluations.
A particular feature of PIM research is that many systems have been designed to assist users with managing and re-finding their information, but very few have been evaluated; a situation noted by several scholars [1, 6, 7]. Recently, however, researchers have started to focus on ways to address the problem of PIM evaluation. For example, Kelly [16] proposes that numerous methodologies must be taken to examine and understand the many issues involved in PIM, although, she makes explicit reference to the need for laboratory based PIM studies and a common set of shared tasks to make this possible. Capra [6] also identifies the need for controlled PIM lab evaluations to complement other evaluation techniques, placing specific emphasis on the need to understand PIM behaviour at the task level.
In this paper, we attempt to address the difficulties involved to faciliate controlled laboratory PIM evaluations.
In the first part of this paper we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. We also look at the features of the tasks that make re-finding difficult. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using different methods of task creation. Thus, this paper offers two contributions to the field: an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations.
A variety of approaches are available to study PIM.
Naturalistic approaches study participants performing naturally, completing their own tasks as they occur, within familiar environments. These approaches allow researchers to overcome many of the difficulties caused by the personal nature of PIM. As the tasks performed are real and not simulated, the participants can utilise their own experiences, previous knowledge and information collections to complete the tasks. A benefit of the approach is that data can be captured continuously over extended time periods and measurements can be taken at fixed points in time within these [15]. Naturalistic approaches can be applied by conducting fieldwork [17, 8], ethnographic methods as suggested by [15] or via log file analysis [9, 7]. Both ethnographic and fieldwork methods require the presence of an experimenter to assess how PIM is performed, which raises a number of issues. Firstly, evaluation in this way is expensive; taking long time periods to study small numbers of participants and these small samples may not be representative of the behaviour of larger populations. Secondly, because participants cannot be continually observed, experimenters must choose when to observe and this may affect the findings.
An alternative strategy to conducting naturalistic evaluations is to utilise log file analysis. This approach makes use of logging software that captures a broad sampling of user activities in the context of natural use of a system. In [9] a novel PIM search tool was deployed to 234 users and the log data provided detailed information about the nature of user queries, interactions with the query interface and about properties of the items retrieved. Log file analysis is a powerful methodology as it allows the capture of a large quantity of detailed information about how users behave with the system without the expense and distracting influence of an observer. Nevertheless, there are limitations to this strategy. Firstly, to attain useful results, the deployed prototype must be something that people would use i.e. it has to be a fully functional piece of software that offers improvement on the systems ordinarily available to participants.
Developing a research prototype to this standard is beyond the resources of many researchers. Further, caution must be taken when analysing logs, as the captured data shows nothing about the goals and intentions that the user had at the time. It is, therefore, difficult to make any concrete statements about the reasons for the behaviour depicted in the logs. This reveals a need to complement naturalistic studies with controlled experiments where the experimenter can relate the behaviour of study participants to goals associated with known search tasks.
Laboratory-based studies simulate users" real world environment in the controlled setting of the laboratory, offering the ability to study issues that are tightly defined and narrow in scope. One difficulty in performing this kind of evaluation is sourcing collections to evaluate. Kelly [16] proposes the introduction of a shared test collection that would provide sharable, reusable data sets, tasks and metrics for those interested in conducting PIM research. This may be useful for testing algorithms in a way similar to TREC in mainstream IR [13]. However, a shared collection would be unsuitable for user studies because it would not be possible to incorporate the personal aspects of PIM while using a common, unfamiliar collection. One alternative approach is to ask users to provide their own information collections to simulate familiar environments within the lab. This approach has been applied to study the re-finding of personal photographs [11], email messages [20], and web-bookmarks [21]. The usefulness of this approach depends on how easy it is to transfer the collection or gain remote access.
Another solution is to use the entire web as a collection when studying web page re-finding [4]. This may be appropriate for studying web page re-finding because previous studies have shown that people often use web search engines for this purpose [5].
A second difficulty in performing PIM laboratory studies is creating tasks for participants to perform that can be solved by searching a shared or personal collection. Tasks relate to the activity that results in a need for information [14] and are acknowledged to be important in determining user behaviour [26]. A large body of work has been carried out to understand the nature of tasks and how the type of task influences user information seeking behaviour. For example, tasks have been categorised in terms of increasing complexity [3] and task complexity has been suggested to affect how searchers perceive their information needs [25] and how they try to find information [3]. Other previous work has provided methodologies that allow the simulation of tasks when studying information seeking behaviour [2].
However, little is known about the kinds of tasks that cause people to search their personal stores or re-find information that they have seen before. Consequently, it is difficult to devise simulated work task situations for PIM. The exception is the study of personal photograph management, where Rodden"s work on categorising personal photograph search tasks has facilitated the creation of simulated work task situations [22]. There have been other suggestions as to how to classify PIM tasks. For example, [5] asked participants to classify tasks based on how frequently they perform the task type in their daily life and how familiar they were with the location of the sought after information and several scholars have classified information objects by the frequency of their use e.g. [24]. While these are interesting properties that may affect how a task will be performed, they do not give experimenters enough scope to devise tasks.
Personal collections are one reason why task creation is so difficult. Rodden"s photo task taxonomy provides a solution here because it allows tasks, tailored to private collections to be categorised. Systems can then be compared across task types for different users [11]. Unfortunately, no equivalent taxonomy exists for other types of information object.
Further, other types of object are more sensitive to privacy than photographs; it is unlikely that participants would be as content to allow researchers to browse their email collections to create tasks as they were with photographs in [11].
This presents a serious problem - how can researchers devise tasks that correspond to private collections without an understanding of the kinds of tasks people perform or jeopardising the privacy of study participants? A few methods have been proposed. For example, [20] studied email search by asking participants to re-find emails that had been sent to every member in a department; allowing the same tasks to be used for all of the study participants. This approach ensured that privacy issues were avoided and participants could use things that they remember to complete tasks.
Nevertheless, the systems were only tested using one type of task - participants were asked to find single emails, each of which shared common properties. In section 4 we show that people perform a wider range of email re-finding tasks than this. In [4], generic search tasks were artificially created by running evaluations over two sessions. In the first session, participants were asked to complete work tasks that involved finding some unknown information. In the second session, participants completed the same tasks again, which naturally involved some re-finding behaviour. The limitations of this technique are that it does not allow participants to exploit any personal connections with the information because the information they are looking for may not correspond to any other aspect of their lives. Further, if time is utilised by a system or interface being tested the approach is unsuitable because all of the objects found in the first session will have been accessed within the same time period.
Our review of evaluation approaches motivates a requirement for controlled laboratory experiments that allow tightly defined aspects of systems or interfaces to be tested.
Unfortunately, it has also been shown that there are difficulties involved in performing this type of evaluation - it is difficult to source collections and to devise tasks that correspond to private collections, while at the same time protect the privacy of the study participants.
In the following section we present a diary study of refinding tasks for email and web pages. The outcome is a classification of tasks similar to that devised by Rodden for personal photographs [22]. In section 5 we build on this work by examining methods for creating tasks that do not compromise the privacy of participants and discuss how our work can facilitate task-based PIM user evaluations. We show that by collecting tasks using electronic diaries, not only can we learn about the tasks that cause people to re-find personal information, but we can learn about the contents of private collections without compromising the privacy of the participants. This knowledge can then be used to construct tasks for use in PIM evaluations.
Diary Studies are a naturalistic technique, offering the ability to capture factual data, in a natural setting, without the distracting influence of an observer. Limitations of the technique include difficulties in maintaining participant dedication levels and convincing participants that seemingly mundane information is useful and should be reported [19]. [12] suggest that the effects of the negatives can be limited, however, with careful design and good implementation. In our diary study, we followed the suggestions in [12] to achieve the best possible data. To this end, we restricted the recorded tasks to web and email re-finding. By asking users to record fewer tasks it was anticipated that participant apathy would be reduced and dedication levels maintained. The participants were provided with a personalised web form in which they could record details about their information needs and the contexts in which these needs developed. Web forms were deployed rather than paperbased diaries because to re-find web and email information the user would be at a computer with an Internet connection and there would be no need to search for a paper-based diary and pen.
The diary form solicited the following information: whether the information need related to re-finding a web page or an email message and a description of the task they are performing. This description was to contain both the information that the participant wished to find and the reason that they needed the information. To help with this, the form gave three example task descriptions, which were also explained verbally to each participant during an introductory session. The experimenter ensured that the participants understood that the tasks to be recorded were not limited to the types shown in the examples. The examples were supplied purely to get participants thinking about the kinds of things they could record and to show the level of and type of details expected. The form also asked participants to rate each task in terms of difficulty (on a scale from 1-5, where 1 was very easy and 5 was very hard). Finally, they were asked when was the last time they looked at the sought after information. Again, they were able to choose from 5 options (less than a day ago, less than a week ago, less than a month ago, less than a year ago, more than a year ago).
Time information was used to examine the frequency with which the participants re-found old and new information, and when combined with difficulty ratings created a picture of whether or not the time period between accessing and re-accessing impacted on how difficult the participants perceived tasks to be. 36 participants, recruited by mass advertisement through departmental communication channels, research group meetings and undergraduate lectures, were asked to digitally record details of their information re-finding tasks over a period of approximately 3 weeks. The final population consisted of 4 academic staff members, 8 research staff members, 6 research students and 18 undergraduate students.
The ages of participants ranged from 19-59. As both personal and work tasks were recorded, the results collected cover a broad range of re-finding tasks.
Several analyses were performed on the captured data.
The following sections present the findings. Firstly, we examine the kinds of re-finding tasks that were performed both when searching on email and on the web. Next, we consider the distribution of tasks - which kinds of tasks were performed most often by participants. Lastly, we explore the kinds of re-finding tasks that participants perceived as difficult.
During the study 412 tasks were recorded. 150 (36.41%) of these tasks were email based, 262 (63.59%) were webbased. As with most diary studies, the number of tasks recorded varied extensively between particpants. The median number of tasks per participant was 8 (interquartile range (IQR)=9.5). More web tasks (median=5,IQR=7.5) were recorded than email tasks (median=3, IQR=3). This means that on average each participant recorded approximately one task every two days.
From the descriptions supplied by the participants, we found similar features in the recorded tasks for both email and web re-finding. Based on this observation a joint classification scheme was devised, encompassing both email and web tasks. The tasks were classified as one of three types: lookup tasks, item tasks and multi-item tasks. Lookup tasks involve searching for specific information from within a resource, for example an email or a web page, where the resource may or may not be known. Some recorded examples of lookup tasks were: • LU1: Looking for the course code for a class - it"s used in a script that is run to set up a practical. I"d previously obtained this about 3 weeks ago from our website. • LU2: I am trying to determine the date by which I step down as an External Examiner. This is in an email somewhere • LU3: Looking for description of log format from system R developed for student project. I think he sent me in it an email Item tasks involve looking for a particular email or web page, perhaps to pass on to someone else or when the entire contents are needed to complete the task. Some recorded examples of item tasks were: • I1: Looking for SIGIR 2002 paper to give to another student • I2: Find the receipt of an online airline purchase required to claim expenses • I3: I need the peer evaluation forms for the MIA class E sent me them by email To clarify, lookup tasks differ from item tasks in two ways - in the quantity of information required and in what the user knows about what they are looking for. Lookup tasks involve a need for a small piece of information e.g. a phone number or an ingredient, and the user may or may not know exactly the resource that contains this information. In item tasks the user knows exactly the resource they are looking for and needs the entire contents of that resource.
Multi-item tasks were tasks that required information that was contained within numerous web pages or email messages. Often these tasks required the user to process or collate the information in order to solve the task. Some recorded examples were: • MI1: Looking for obituaries and other material on the novelist John Fowles, who died at the weekend. Accessed the online Guradian and IMES • MI2: Trying to find details on Piccolo graphics framework.
Remind myself of what it is and what it does. Looking to build a GUI within Eclipse • MI3: I am trying to file my emails regarding IPM and I am looking for any emails from or about this journal There were a number of tasks that were difficult to classify.
For example, consider the following recorded task: • LU4: re-find AS"s paper on graded relevance assessments because I want to see how she presented her results for a paper I am writing This task actually consists of two sub-tasks: 1 item task(refind the paper) and 1 lookup task (look for specific information within the paper). It was decided to treat this as a lookup task because the user"s ultimate goal was to access and use the information within the resource.
There were a number of examples of combined tasks, mainly of the form item then lookup, but there were also examples of item then multi-item. For example: • MI4: re-find Kelkoo website so that I can re-check the prices of hair-straighteners for my girlfriend A second source of ambiguity came from tasks such as finding an email containing a URL as a means of re-accessing a web page. It was also decided to categorise these as lookup tasks because in all cases these were logged by participants as email searches and, within this context, what they were looking for was information within an email.
Another problem was that some of the logs lacked the detail required to perform a categorisation e.g. • U1: searching for how to retrieve user"s selection from a message box. Decided to use some other means Such tasks were labelled as U for unclassifiable. To verify the consistency of the taxonomy, the tasks were recategorised by the same researcher after a delay of two weeks.
The agreement between the results of the two analyses was largely consistent (96.8%). Further, we asked a researcher with no knowledge of the project or the field to classify a sample of 50 tasks. The second researcher achieved a 90% agreement. We feel that this high agreement on a large number of tasks by more than one researcher provides evidence for the reliability of the classification scheme.
The distribution of task types is shown in table 1. Overall, lookup and item tasks were the most common, with multiitem tasks only representing 8.98% of those recorded. The distribution of the task types was different for web and email re-finding. The majority of email tasks (60%) involved looking for information within an email (lookup), in contrast to web tasks where the majority of tasks (52.67%) involved looking for a single web page (item). Another distinction was the number of recorded multi-item tasks for web and email. Multi-item tasks were very rare for email re-finding (only 2.67% of email tasks involved searching for multiple resources), but comparatively common for web re-finding (12.6%).
Lookup Item Multi-item Unclass.
Email 90(60%) 52(34.67%) 4(2.67%) 4(2.67%) Web 87(33.21%) 138(52.67%) 33(12.60%) 4(1.53%) All 177(42.96%) 190(46.12%) 37(8.98%) 8(1.94%) Table 1: The distribution of task types In addition to the three-way classification described above, the recorded tasks were classified with respect to the temperature metaphor proposed by [24], which classifies information as one of three temperatures: hot, warm and cold.
We classified the tasks using the form data. Information that had been seen less than a day or less than a week before the task were defined as hot, information that had been seen less than a month before the task as warm, and information that had been seen less than a year or more than a year before the task as cold. Unfortunately, a technical difficulty with the form only allowed 335(81.3%) of the tasks to be classified. The remainder were defined as U for unclassifiable. A cross-tabulation of task types and temperatures is shown in table 2.
Hot Warm Cold Unclass.
Email 50(33.33%) 36(24.00%) 37(24.67%) 27(18%) Web 112(42.75%) 60(22.90%) 40(15.27%) 50(19.08%) All 162(39.32%) 96(23.30%) 77(18.69%) 77(18.69%) Table 2: The distribution of temperatures Most of the tasks that caused people to re-find web pages (42.75%) and email messages (33.33%) involved searching for information that has been accessed in the last week.
However there were also a number of re-finding tasks that involved searching for older information: 23.30% of the tasks recorded (24.00% for email and 22.90% for web) involved searching for information accessed in the last month and
for web) were looking for even older information. This is important with respect to evaluation because there is psychological evidence suggesting that people remember less over time e.g. [23]. This means that users may find searching for older information more difficult or perhaps alter their seeking strategy when looking for hot, warm or cold information.
We looked for patterns in the recorded data to determine if certain tasks were perceived as more difficult than others. For example, we examined whether the media type affected how difficult the participants perceived the task to be.
There was no evidence that participants found either email (median=2 IQR=2) or web (median=2 IQR=2) tasks more difficult. We also investigated whether the type of task or the length of time between accessing and re-accessing made a task more difficult. Figure 1 shows this information graphically.
Figure 1: Difficulty ratings for task types From figure 1, it does not appear that any particular task type was perceived as difficult with respect to the others, although there is a suggestion that lookup tasks were perceived more difficult when looking for cold information than hot and item tasks were perceived more difficult for warm information than hot. To assess the relationship between information temperature and the perceived difficulty, we used Mood"s median tests to determine whether the rank of difficulty scores was in agreement for the information temperatures being compared (p<0.05). For the look-up task data, there was evidence that hot tasks were perceived easier than cold (p=0.0001) and that warm tasks were perceived easier than cold tasks(p=0.0041), but there was no evidence to distinguish between the difficulty ratings of hot and warm tasks(p=0.593). For the item task data, there was evidence that hot and cold tasks were rated differently (p=0.024), but no evidence to distinguish between hot and warm tasks(p=0.05) or warm and cold tasks(p=0.272).
These tests confirm that the length of time between accessing and re-accessing the sought after information indeed influenced how difficult participants perceived the task to be. Nevertheless, the large number of tasks of all types and temperatures rated by participants as easy i.e. < 3, suggests that there are other factors that influence how difficult a task is perceived to be. To learn about these factors would require the kind of user evaluations proposed by [16, 6] - the kind of evaluations facilitated by our work.
In the first part of this paper, we described a diary study of web and email re-finding tasks. We examined the types of task that caused the participants to search their personal stores and found three main categories of task: tasks where the user requires specific information from within a single resource, tasks where a single resource is required, and tasks that require information to be recovered from multiple resources. It was discovered that look-up and item tasks were recorded with greater frequency than multi-item tasks.
Although no evidence was found that web or email tasks were more difficult, there was some evidence showing that the time between accessing and re-accessing affected how difficult the participants perceived tasks to be. These findings have implications for evaluating PIM behaviour at the task level. The remainder of this paper concentrates on this, discussing what the findings mean with respect to performing task-based PIM user evaluations.
The findings described in section 4 are useful with respect to evaluation because they provide experimenters with enough knowledge to conduct controlled user evaluations in lab conditions. Greco-Latin square experimental designs can be constructed where participants are assigned n tasks of the three types described above to perform on their own collections using x systems. This would allow the performance of the systems or the behaviour of the participants using different systems to be analysed with respect to the type of task being performed (look-up, item, or multi-item). In the following sections we evaluate the feasibility of this approach when employing different methods of task creation.
One method of creating realistic re-finding tasks without compromising the privacy of participants is to use real tasks.
Diary-studies, similar to that described above, would allow experimenters to capture a pool of tasks for participants to complete by searching on their own collections. This is extremely advantageous because it would allow experimenters to evaluate the behaviour of real users, completing real search tasks on real collections while in a controlled environment. There is also the additional benefit that the task descriptions would not make any assumptions about what the user would remember in a real life situation because they would only include the information that had been recorded i.e. the information that was available when the user originally performed the task. Nevertheless, to gain these benefits we must, firstly, confirm that the task descriptions recorded are of sufficient quality to enable the task to be re-performed at a later date. Secondly, we must ensure that a diary-study would provide experimenters with enough tasks to construct a balanced experimental design that would satisfy their data needs.
To examine the quality of recorded tasks, 6 weeks after the diary study had completed, we asked 6 of our participants, selected randomly from the pool of those who recorded enough tasks, to re-perform 5 of their own tasks.
The tasks were selected randomly from the pool of those available. The issued tasks consisted of 10 email and 20 web tasks, 9 of which were lookup tasks, 12 were item tasks, and 8 were multi-item tasks. The issued tasks represented a broad-sampling of the complete set of recorded tasks. They also included tasks with vague descriptions e.g. • LU5:Find a software key for an application I required to reinstall. • LU6:Trying to find a quote to use in a paper. Cannot remember the person or the exact quote The usefulness of such tasks would rely on the memories of participants i.e. would the recorder of task LU5 remember which application he referred to and would the recorder of LU6 remember enough about the context in which the task took place to re-perform the task?
Presented with the tasks exactly as they recorded them, the participants were asked to re-perform each task with any system of their choice. Of the 30 tasks issued, 26 (86.67%) were completed without problems, 2 (6.67%) of the tasks were not completed because the description recorded was insufficent to recreate the task, and 2 tasks (6.67%) were not completed because the task was too difficult or the required web page no longer existed. Experimenters are likely to be interested in the final group of tasks because it is important to discover what makes a task difficult and how user behaviour changes in these circumstances. Therefore, from the 30 tasks tested, only 2 tasks were not of sufficient quality to be used in an evaluation situation. Further, there did not seem to be any issue of the type, temperature or difficulty ratings affecting the quality of the task descriptions.
These findings suggest that the participants who recorded most tasks in the diary study also recorded tasks with sufficient quality. However, did the diary study generate enough tasks to satisfy the needs of experimenters?
Participant Tasks Lookup Item Multi-item Unclass. 10 26 16 8 2 0 43 9 4 5 0 0 26 9 5 4 0 0 8 9 8 1 0 0 40 8 5 3 0 0 18 7 3 4 0 0 4 6 5 1 0 0 7 6 5 0 1 0 12 5 4 0 0 1 22 5 4 1 0 0 36 5 0 5 0 0 46 5 2 2 0 1 3 5 3 2 0 0 Table 3: The quantities of recorded email tasks Participant Tasks Lookup Item Multi-item Unclass. 26 32 7 20 5 0 32 31 11 18 2 0 10 19 0 10 7 2 33 18 5 13 0 0 5 15 0 7 2 4 8 11 0 6 5 0 22 10 0 3 5 2 28 10 1 7 2 0 37 10 1 9 0 0 35 9 7 2 0 0 6 9 0 1 8 0 40 7 1 5 1 0 9 7 0 0 5 2 12 7 1 0 3 2 42 6 0 4 2 0 29 6 0 3 3 0 15 5 0 2 1 2 4 5 0 4 1 0 43 5 2 3 0 0 18 5 0 0 3 2 Table 4: The quantities of recorded web tasks Naturally the exact number of tasks required to perform a user evaluation will depend on the goals of the evaluation, the number of users and the number of systems to be tested etc. However, for illustrative purposes we chose 5 tasks as a cut-off point for our data. From tables 3 and 4, which show the quantities of email and web tasks recorded for each participant, we can see that of the 36 participants, only 13 (36.1%) recorded 5 or more email tasks and 20 (55.6%) recorded 5 or more web tasks. This means that many of the recruited participants could not actually participate in the final evaluation. This is a major limitation of using recorded tasks in evaluations because participant recruitment for user tests is challenging and it may not be possible to recruit enough participants if experimenters lose between half and two-thirds of their populations.
Further, there was some imbalance in the numbers of recorded tasks of different types. Some participants recorded several lookup tasks but very few item tasks and others recorded several item tasks but few lookup tasks. There was also a specific lack of multi-item email tasks. This situation makes it very difficult for experimenters to prepare balanced experimental designs. Therefore, even though our first test suggests that the quality of recorded tasks was sufficient for the participants to re-perform the tasks at a later stage, the number of tasks recorded was probably too low to make this a viable option for experimental task creation.
However, it may be possible to increase the number of tasks recorded by frequently reminding participants or by making personal visits etc.
Another benefit of diary-studies is that they provide information about the contents and uses of private collections without invading participants" privacy. This section explores the possibility of using a combination of the knowledge gained from diary studies and other attributes known about participants to artificially create re-finding tasks corresponding to the taxonomy defined in section 4.1. We explain the techniques used and demonstrate the feasibility of creating simulated tasks within the context of a user evaluation investigating email re-finding behaviour. Space limitations prevent us from reporting our findings; instead we concentrate on the methods of task creation.
As preparation for the evaluation, we performed a second diary-study, where 34 new participants, consisting of 16 post-graduate students and 18 under-graduate students, recorded 150 email tasks over a period of approximately 3 weeks. The collected data revealed several patterns that helped with the creation of artificial tasks. For example, students in both groups recorded tasks relating to classes that they were taking at the time and often different participants recorded tasks that involved searching for the same information. This was useful because it provided us with a clue that even though some of the participants did not record a particular task, it was possible that the task may still be applicable to their collections. Other patterns revealed included that students within the same group often searched for emails containing announcements from the same source.
For example, several undergraduate students recorded tasks that included re-finding information relating to job vacancies. There were also tasks that were recorded by participants in both groups. For example, searching for an email that would re-confirm the pin code required to access the computer labs.
To supplement our knowledge of the participants" email collections, we asked 2 participants from each group to provide email tours. These consisted of short 5-10 minute sessions, where participants were asked to explain why they use email, who sends them email, and their organisational strategies. This approach has been used successfully in the past as a non-intrusive means to learn about how people store and maintain their personal information [17].
Originally, we had planned to ask more participants to provide tours, but we found 2 tours per group was sufficient for our needs. Again, patterns emerged that helped with task creation. We found content overlap within and between groups that confirmed many of our observations from the diary study data. For example, the students who gave tours revealed that they received emails from lecturers for particular class assignments, receipts for completed assignments, and various announcements from systems support and about job vacancies. Importantly, the participants were also able to confirm which other students had received the same information. This confirmed that many of tasks recorded during the diary study were applicable, not only to the recorder, but to every participant in 1 or both groups.
Based on this initial investigatory work, a set of 15 tasks (5 of each type in our taxonomy) was created for each group of participants. We also created a set of tasks for a third group of participants that consisted of research and academic staff members, based on our knowledge of the emails our colleagues receive. Where possible we used the information recorded in the diary study descriptions to provide a context for the task i.e. a work task or motivation that would require the task to be performed. When the diary study data did not provide sufficient context information to supply the participants with a robust description of the information need, we created simulated work task situations according to the guidelines of [2]. A further advantage of using simulated tasks in this way, rather than real-tasks, is that some of the users will not have performed the task in the recent past and this allows the examination of tasks that look for information of different temperatures. If only real-tasks had been used all of the participants would have performed the tasks during the period of the diary study.
The created tasks were used in a final evaluation, where we examined the email re-finding behaviour of users with three different email systems. 21 users (7 in each group) performed 9 tasks each (1 task of each type on each system) using their own personal collections in a Greco-Latin square experimental design. Performing a PIM evaluation in this way allowed the examination of re-finding behaviour in a way not possible before - we were able to observe the email re-finding strategies employed by real users, performing realistic tasks, on their own collections in a controlled environment. The study revealed that the participants remembered different attributes of emails, demostrated different finding behaviour, and exhibited different levels of performance when asked to complete tasks of the different types in the taxonomy. The key to both the task creation and the analysis of the results was our taxonomy, which provided the template to create tasks and also a means to compare the behaviour and performance of different users (and systems) performing different tasks of the same type. Some of the findings of the evaluation will be published in [10].
Summarising the approach, to conduct a user experiment using our methodology, researchers would be required to perform the following steps: 1)Conduct a diary study as above 1 . 2)Analyse the recorded tasks looking for overlap between the participants. 3)Supplement the gained knowledge about the contents of participants" collections by asking a selection of the participants to provide a tour of their collection. 4)Use the knowledge gained to devise tasks of the three different types defined within the taxonomy. More de1 Information about this and the diary forms required can be found at http://www.cis.strath.ac.uk/˜dce/PIMevaluations tailed information on how to use the research described in this paper to perform task-based PIM evaluations can be found at our website (see footnote 1).
This paper has focused on overcoming the difficulties involved in performing PIM evaluations. The personal nature of PIM means that it is difficult to construct balanced experiments because participants each have their own unique collections that are self-generated by completing other tasks.
We suggested that to incorporate the personal aspects of PIM in evaluations, the performance of systems or users should be examined when users complete tasks on their own collections. This approach itself has problems because task creation for personal collections is difficult: researchers don"t know much about the kinds of re-finding tasks people perform and they don"t know what information is within individual personal collections. In this paper we described ways of overcoming these challenges to facilitate task based PIM user evaluations.
In the first part of the paper we performed a diary study that examined the tasks that caused people to re-find email messages and web pages. The collected data included a wide range of both work and non-work related tasks, and based on the data we created a taxonomy of web and email re-finding tasks. We discovered that people perform three main types of re-finding task: tasks that require specific information from within a single resource, tasks that require a single complete resource, and tasks that require information to be recovered from multiple resources. In the second part of the paper, we discussed the significance of the taxonomy with respect to PIM evaluation. We demonstrated that balanced experiments could be conducted comparing system or user performance on the task categories within the taxonomy.
We also suggested two methods of creating tasks that can be completed on personal collections. These methods do not compromise the privacy of study participants. We examined the techniques suggested, firstly by simulating an experimental situation - participants were asked to re-perform their own tasks as they recorded them, and secondly, in the context of a full evaluation. Performing evaluations in this way will allow systems that have been proposed to improve users" ability to manage and re-find their information to be tested, so that we can learn about the needs and desires of users. Thus, this paper has offered two contributions to the field: an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations.
We would like to thank Dr Mark Baillie for his insightful comments and help analysing the data.
[1] R. Boardman, Improving tool support for personal information management, Ph.D. thesis, Imperial College London, 2004. [2] P. Borlund, The iir evaluation model: A framework for evaluation of interactive information retrieval systems,
Information Research 8 (2003), no. 3, paper no. 152. [3] K. Bystr¨om and K. J¨arvelin, Task complexity affects information seeking and use, Information Processing and Management 31 (1995), no. 2, 191-213. [4] R. G. Capra and M. A. Perez-Quinones, Re-finding found things: An exploratory study of how users re-find information, Tech. report, Virginia Tech, 2003. [5] R. G. Capra and M. A. Perez-Quinones, Using web search engines to find and refind information,
Computer 38 (2005), no. 10, 36-42. [6] R. G. Capra and M. A. Perez-Quinones, Factors and evaluation of refinding behaviors., SIGIR 2006 Workshop on Personal Information Management,
August 10-11, 2006, Seattle, Washington, 2006. [7] E. Cutrell, D.Robbins, S.Dumais, and R.Sarin, Fast, flexible filtering with phlat, Proc. SIGCHI "06 (New York, NY, USA), ACM Press, 2006, pp. 261-270. [8] M. Czerwinski, E. Horvitz, and S. Wilhite, A diary study of task switching and interruptions, Proc.
SIGCHI "04, 2004, pp. 175-182. [9] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, and D.C. Robbins, Stuff i"ve seen: a system for personal information retrieval and re-use, Proc. SIGIR "03:, 2003, pp. 72-79. [10] D. Elsweiler and I. Ruthven, Memory and email re-finding, In preparation for ACM TOIS CFP special issue on Keeping, Re-finding, and Sharing Personal Information (2007). [11] D. Elsweiler, I. Ruthven, and C. Jones, Dealing with fragmented recollection of context in information management, Context-Based Information Retrieval (CIR-05) Workshop in CONTEXT-05, 2005. [12] D. Elsweiler, I. Ruthven, and C. Jones, Towards memory supporting personal information management tools, (to appear in) Journal of the American Society for Information Science and Technology (2007). [13] D. Harman, What we have learned, and not learned, from trec, Proc. ECIR 2000, 2000. [14] P. Ingwersen, Information retrieval interaction, Taylor Graham, 1992. [15] D. Kelly, B. Bederson, M. Czerwinski, J. Gemmell,
W. Pratt, and M. Skeels (eds.), Pim workshop report: Measurement and design, 2005. [16] D. Kelly and J. Teevan, (to appear in) personal information management, ch. Understanding what works: Evaluating personal information management tools, Seattle: University of Washington Press., 2007. [17] B. H. Kwasnik, How a personal document"s intended use or purpose affects its classification in an office,
SIGIR"89 23 (1989), no. SI, 207-210. [18] M.W. Lansdale, The psychology of personal information management., Appl Ergon 19 (1988), no. 1, 55-66. [19] L. Palen and M. Salzman, Voice-mail diary studies for naturalistic data capture under mobile conditions,
CSCW "02: Proceedings of the 2002 ACM conference on Computer supported cooperative work, 2002. [20] M. Ringel, E. Cutrell, S. Dumais, and E. Horvitz,
Milestones in time: The value of landmarks in retrieving information from personal stores., Proc.
INTERACT 2003, 2003. [21] G. Robertson, M. Czerwinski, K. Larson, D. C.
Robbins, D. Thiel, and M. van Dantzich, Data mountain: using spatial memory for document management, Proc. UIST "98:, 1998. [22] K. Rodden, How do people organise their photographs,
BCS IRSG 21st Annual Colloquium on Information Retrieval Research,Glasgow, Scotland, 1999. [23] D.C. Rubin and A.E. Wenzel, One hundred years of forgetting: A quantitative description of retention,
Psychological Bulletin 103 (1996), 734-760. [24] A. J. Sellen and R. H. R. Harper, The myth of the paperless office, MIT Press, Cambridge, MA, USA,

Query suggestion is a functionality to help users of a search engine to better specify their information need, by narrowing down or expanding the scope of the search with synonymous queries and relevant queries, or by suggesting related queries that have been frequently used by other users. Search engines, such as Google, Yahoo!, MSN, Ask Jeeves, all have implemented query suggestion functionality as a valuable addition to their core search method. In addition, the same technology has been leveraged to recommend bidding terms to online advertiser in the pay-forperformance search market [12].
Query suggestion is closely related to query expansion which extends the original query with new search terms to narrow the scope of the search. But different from query expansion, query suggestion aims to suggest full queries that have been formulated by users so that the query integrity and coherence are preserved in the suggested queries.
Typical methods for query suggestion exploit query logs and document collections, by assuming that in the same period of time, many users share the same or similar interests, which can be expressed in different manners [12, 14, 26]. By suggesting the related and frequently used formulations, it is hoped that the new query can cover more relevant documents. However, all of the existing studies dealt with monolingual query suggestion and to our knowledge, there is no published study on cross-lingual query suggestion (CLQS). CLQS aims to suggest related queries but in a different language. It has wide applications on World Wide Web: for cross-language search or for suggesting relevant bidding terms in a different language. 1 CLQS can be approached as a query translation problem, i.e., to suggest the queries that are translations of the original query.
Dictionaries, large size of parallel corpora and existing commercial machine translation systems can be used for translation. However, these kinds of approaches usually rely on static knowledge and data. It cannot effectively reflect the quickly shifting interests of Web users. Moreover, there are some problems with translated queries in target language. For instance, the translated terms can be reasonable translations, but they are not popularly used in the target language. For example, the French query aliment biologique is translated into biologic food by Google translation tool2 , yet the correct formulation nowadays should be organic food. Therefore, there exist many mismatch cases between the translated terms and the really used terms in target language. This mismatch makes the suggested terms in the target language ineffective.
A natural thinking of solving this mismatch is to map the queries in the source language and the queries in the target language, by using the query log of a search engine. We exploit the fact that the users of search engines in the same period of time have similar interests, and they submit queries on similar topics in different languages. As a result, a query written in a source language likely has an equivalent in a query log in the target language. In particular, if the user intends to perform CLIR, then original query is even more likely to have its correspondent included in the target language query log. Therefore, if a candidate for CLQS appears often in the query log, then it is more likely the appropriate one to be suggested.
In this paper, we propose a method of calculating the similarity between source language query and the target language query by exploiting, in addition to the translation information, a wide spectrum of bilingual and monolingual information, such as term co-occurrences, query logs with click-through data, etc. A discriminative model is used to learn the cross-lingual query similarity based on a set of manually translated queries. The model is trained by optimizing the cross-lingual similarity to best fit the monolingual similarity between one query and the other query"s translation. Besides being benchmarked as an independent module, the resulting CLQS system is tested as a new means of query translation in CLIR task on TREC collections. The results show that this new translation method is more effective than the traditional query translation method.
The remainder of this paper is organized as follows: Section 2 introduces the related work; Section 3 describes in detail the discriminative model for estimating cross-lingual query similarity; Section 4 presents a new CLIR approach using cross-lingual query suggestion as a bridge across language boundaries. Section 5 discusses the experiments and benchmarks; finally, the paper is concluded in Section 6.
Most approaches to CLIR perform a query translation followed by a monolingual IR. Typically, queries are translated either using a bilingual dictionary [22], a machine translation software [9] or a parallel corpus [20].
Despite the various types of resources used, out-of-vocabulary (OOV) words and translation disambiguation are the two major bottlenecks for CLIR [20]. In [7, 27], OOV term translations are mined from the Web using a search engine. In [17], bilingual knowledge is acquired based on anchor text analysis. In addition, word co-occurrence statistics in the target language has been leveraged for translation disambiguation [3, 10, 11, 19]. 2 http://www.google.com/language_tools Nevertheless, it is arguable that accurate query translation may not be necessary for CLIR. Indeed, in many cases, it is helpful to introduce words even if they are not direct translations of any query word, but are closely related to the meaning of the query.
This observation has led to the development of cross-lingual query expansion (CLQE) techniques [2, 16, 18]. [2] reports the enhancement on CLIR by post-translation expansion. [16] develops a cross-lingual relevancy model by leveraging the crosslingual co-occurrence statistics in parallel texts. [18] makes performance comparison on multiple CLQE techniques, including pre-translation expansion and post-translation expansion.
However, there is lack of a unified framework to combine the wide spectrum of resources and recent advances of mining techniques for CLQE.
CLQS is different from CLQE in that it aims to suggest full queries that have been formulated by users in another language.
As CLQS exploits up-to-date query logs, it is expected that for most user queries, we can find common formulations on these topics in the query log in the target language. Therefore, CLQS also plays a role of adapting the original query formulation to the common formulations of similar topics in the target language.
Query logs have been successfully used for monolingual IR [8, 12, 15, 26], especially in monolingual query suggestions [12] and relating the semantically relevant terms for query expansion [8, 15]. In [1], the target language query log has been exploited to help query translation in CLIR.
QUERY SIMILARITY A search engine has a query log containing user queries in different languages within a certain period of time. In addition to query terms, click-through information is also recorded. Therefore, we know which documents have been selected by users for each query. Given a query in the source language, our CLQS task is to determine one or several similar queries in the target language from the query log.
The key problem with cross-lingual query suggestion is how to learn a similarity measure between two queries in different languages. Although various statistical similarity measures have been studied for monolingual terms [8, 26], most of them are based on term co-occurrence statistics, and can hardly be applied directly in cross-lingual settings.
In order to define a similarity measure across languages, one has to use at least one translation tool or resource. So the measure is based on both translation relation and monolingual similarity. In this paper, as our purpose is to provide up-to-date query similarity measure, it may not be sufficient to use only a static translation resource. Therefore, we also integrate a method to mine possible translations on the Web. This method is particularly useful for dealing with OOV terms.
Given a set of resources of different natures, the next question is how to integrate them in a principled manner. In this paper, we propose a discriminative model to learn the appropriate similarity measure. The principle is as follows: we assume that we have a reasonable monolingual query similarity measure. For any training query example for which a translation exists, its similarity measure (with any other query) is transposed to its translation.
Therefore, we have the desired cross-language similarity value for this example. Then we use a discriminative model to learn the cross-language similarity function which fits the best these examples.
In the following sections, let us first describe the detail of the discriminative model for cross-lingual query similarity estimation.
Then we introduce all the features (monolingual and cross-lingual information) that we will use in the discriminative model.
Cross-Lingual Query Similarity In this section, we propose a discriminative model to learn crosslingual query similarities in a principled manner. The principle is as follows: for a reasonable monolingual query similarity between two queries, a cross-lingual correspondent can be deduced between one query and another query"s translation. In other words, for a pair of queries in different languages, their crosslingual similarity should fit the monolingual similarity between one query and the other query"s translation. For example, the similarity between French query pages jaunes (i.e., yellow page in English) and English query telephone directory should be equal to the monolingual similarity between the translation of the French query yellow page and telephone directory. There are many ways to obtain a monolingual similarity measure between terms, e.g., term co-occurrence based mutual information and 2 χ . Any of them can be used as the target for the cross-lingual similarity function to fit. In this way, cross-lingual query similarity estimation is formulated as a regression task as follows: Given a source language query fq , a target language query eq , and a monolingual query similarity MLsim , the corresponding cross-lingual query similarity CLsim is defined as follows: ),(),( eqMLefCL qTsimqqsim f = (1) where fqT is the translation of fq in the target language.
Based on Equation (1), it would be relatively easy to create a training corpus. All it requires is a list of query translations. Then an existing monolingual query suggestion system can be used to automatically produce similar query to each translation, and create the training corpus for cross-lingual similarity estimation. Another advantage is that it is fairly easy to make use of arbitrary information sources within a discriminative modeling framework to achieve optimal performance.
In this paper, support vector machine (SVM) regression algorithm [25] is used to learn the cross-lingual term similarity function. Given a vector of feature functions f between fq and eq , ),( efCL ttsim is represented as an inner product between a weight vector and the feature vector in a kernel space as follows: )),((),( efefCL ttfwttsim φ•= (2) where φ is the mapping from the input feature space onto the kernel space, and wis the weight vector in the kernel space which will be learned by the SVM regression training. Once the weight vector is learned, the Equation (2) can be used to estimate the similarity between queries of different languages.
We want to point out that instead of regression, one can definitely simplify the task as a binary or ordinal classification, in which case CLQS can be categorized according to discontinuous class labels, e.g., relevant and irrelevant, or a series of levels of relevancies, e.g., strongly relevant, weakly relevant, and irrelevant. In either case, one can resort to discriminative classification approaches, such as an SVM or maximum entropy model, in a straightforward way. However, the regression formalism enables us to fully rank the suggested queries based on the similarity score given by Equation (1).
The Equations (1) and (2) construct a regression model for cross-lingual query similarity estimation. In the following sections, the monolingual query similarity measure (see Section
Section 3.3) will be presented.
Based on Click-through Information Any monolingual term similarity measure can be used as the regression target. In this paper, we select the monolingual query similarity measure presented in [26] which reports good performance by using search users" click-through information in query logs. The reason to choose this monolingual similarity is that it is defined in a similar context as ours − according to a user log that reflects users" intention and behavior. Therefore, we can expect that the cross-language term similarity learned from it can also reflect users" intention and expectation.
Following [26], our monolingual query similarity is defined by combining both query content-based similarity and click-through commonality in the query log.
First the content similarity between two queries p and q is defined as follows: ))(),(( ),( ),( qknpknMax qpKN qpsimilarity content = (3) where )( xkn is the number of keywords in a query x, ),( qpKN is the number of common keywords in the two queries.
Secondly, the click-through based similarity is defined as follows, ))(),(( ),( ),( qrdprdMax qpRD qpsimilarity throughclick =− (4) where )(xrd is the number of clicked URLs for a query x, and ),( qpRD is the number of common URLs clicked for two queries.
Finally, the similarity between two queries is a linear combination of the content-based and click-through-based similarities, and is presented as follows: ),(* ),(*),( qpsimilarity qpsimilarityqpsimilarity throughclick content − += β α (5) where α and β are the relative importance of the two similarity measures. In this paper, we set ,4.0=α and 6.0=β following the practice in [26]. Queries with similarity measure higher than a threshold with another query will be regarded as relevant monolingual query suggestions (MLQS) for the latter. In this paper, the threshold is set as 0.9 empirically.
Query Similarity Measure This section presents the extraction of candidate relevant queries from the log with the assistance of various monolingual and bilingual resources. Meanwhile, feature functions over source query and the cross-lingual relevant candidates are defined. Some of the resources being used here, such as bilingual lexicon and parallel corpora, were for query translation in previous work. But note that we employ them here as an assistant means for finding relevant candidates in the log rather than for acquiring accurate translations.
In this subsection, a built-in-house bilingual dictionary containing 120,000 unique entries is used to retrieve candidate queries. Since multiple translations may be associated with each source word, co-occurrence based translation disambiguation is performed [3, 10]. The process is presented as follows: Given an input query }{ ,2,1 fnfff wwwq K= in the source language, for each query term fiw , a set of unique translations are provided by the bilingual dictionary D : },,{)( ,2,1 imiifi tttwD K= .
Then the cohesion between the translations of two query terms is measured using mutual information which is computed as follows: )()( ),( log),()( , klij klij klijklij tPtP ttP ttPttMI = (6) where . )( )(, ),( ),( N tC tP N ttC ttP klij klij == Here ),( yxC is the number of queries in the log containing both x and y , )(xC is the number of queries containing term x , and N is the total number of queries in the log.
Based on the term-term cohesion defined in Equation (6), all the possible query translations are ranked using the summation of the term-term cohesion ∑≠ = kiki klijqdict ttMITS f ,, ),()( . The set of top-4 query translations is denoted as )( fqTS . For each possible query translation )( fqTST∈ , we retrieve all the queries containing the same keywords as T from the target language log. The retrieved queries are candidate target queries, and are assigned )(TSdict as the value of the feature Dictionary-based Translation Score.
Parallel corpora are precious resources for bilingual knowledge acquisition. Different from the bilingual dictionary, the bilingual knowledge learned from parallel corpora assigns probability for each translation candidate which is useful in acquiring dominant query translations.
In this paper, the Europarl corpus (a set of parallel French and English texts from the proceedings of the European Parliament) is used. The corpus is first sentence aligned. Then word alignments are derived by training an IBM translation model 1 [4] using GIZA++ [21]. The learned bilingual knowledge is used to extract candidate queries from the query log. The process is presented as follows: Given a pair of queries, fq in the source language and eq in the target language, the Bi-Directional Translation Score is defined as follows: )|()|(),( 111 feIBMefIBMefIBM qqpqqpqqS = (7) where )|(1 xypIBM is the word sequence translation probability given by IBM model 1 which has the following form: ∏∑= =+ = || 1 || 0 ||1 )|( )1|(| 1 )|( y j x i ijyIBM xyp x xyp (8) where )|( ij xyp is the word to word translation probability derived from the word-aligned corpora.
The reason to use bidirectional translation probability is to deal with the fact that common words can be considered as possible translations of many words. By using bidirectional translation, we test whether the translation words can be translated back to the source words. This is helpful to focus on the translation probability onto the most specific translation candidates.
Now, given an input query fq , the top 10 queries }{ eq with the highest bidirectional translation scores with fq are retrieved from the query log, and ),(1 efIBM qqS in Equation (7) is assigned as the value for the feature Bi-Directional Translation Score.
OOV word translation is a major knowledge bottleneck for query translation and CLIR. To overcome this knowledge bottleneck, web mining has been exploited in [7, 27] to acquire EnglishChinese term translations based on the observation that Chinese terms may co-occur with their English translations in the same web page. In this section, this web mining approach is adapted to acquire not only translations but semantically related queries in the target language.
It is assumed that if a query in the target language co-occurs with the source query in many web pages, they are probably semantically related. Therefore, a simple method is to send the source query to a search engine (Google in our case) for Web pages in the target language in order to find related queries in the target language. For instance, by sending a French query pages jaunes to search for English pages, the English snippets containing the key words yellow pages or telephone directory will be returned. However, this simple approach may induce significant amount of noise due to the non-relevant returns from the search engine. In order to improve the relevancy of the bilingual snippets, we extend the simple approach by the following query modification: the original query is used to search with the dictionary-based query keyword translations, which are unified by the ∧ (and) ∨ (OR) operators into a single Boolean query. For example, for a given query abcq = where the set of translation entries in the dictionary of for a is },,{ 321 aaa , b is },{ 21 bb and c is }{ 1c , we issue 121321 )()( cbbaaaq ∧∨∧∨∨∧ as one web query.
From the returned top 700 snippets, the most frequent 10 target queries are identified, and are associated with the feature Frequency in the Snippets.
Furthermore, we use Co-Occurrence Double-Check (CODC) Measure to weight the association between the source and target queries. CODC Measure is proposed in [6] as an association measure based on snippet analysis, named Web Search with Double Checking (WSDC) model. In WSDC model, two objects a and b are considered to have an association if b can be found by using a as query (forward process), and a can be found by using b as query (backward process) by web search. The forward process counts the frequency of b in the top N snippets of query a, denoted as )@( abfreq . Similarly, the backward process count the frequency of a in the top N snippets of query b, denoted as )@( bafreq . Then the CODC association score is defined as follows: ⎪ ⎩ ⎪ ⎨ ⎧ =× = ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎣ ⎡ × otherwise, 0)@()@(if,0 ),( )( )@( )( )@( log α e ef f fe qfreq qqfreq qfreq qqfreq effe efCODC e qqfreqqqfreq qqS (9) CODC measures the association of two terms in the range between 0 and 1, where under the two extreme cases, eq and fq are of no association when 0)@( =fe qqfreq or 0)@( =ef qqfreq , and are of the strongest association when )()@( ffe qfreqqqfreq = and )()@( eef qfreqqqfreq = . In our experiment, α is set at 0.15 following the practice in [6].
Any query eq mined from the Web will be associated with a feature CODC Measure with ),( efCODC qqS as its value.
For all the candidate queries 0Q being retrieved using dictionary (see Section 3.3.1), parallel data (see Section 3.3.2) and web mining (see Section 3.3.3), monolingual query suggestion system (described in Section 3.1) is called to produce more related queries in the target language. For each target query eq , its monolingual source query )( eML qSQ is defined as the query in 0Q with the highest monolingual similarity with eq , i.e., ),(maxarg)( 0 eeMLQqeML qqsimqSQ e ′= ∈′ (10) Then the monolingual similarity between eq and )( eML qSQ is used as the value of the eq "s Monolingual Query Suggestion Feature. For any target query 0Qq∈ , its Monolingual Query Suggestion Feature is set as 1.
For any query 0Qqe ∉ , its values of Dictionary-based Translation Score, Bi-Directional Translation Score, Frequency in the Snippet, and CODC Measure are set to be equal to the feature values of )( eML qSQ .
In summary, four categories of features are used to learn the crosslingual query similarity. SVM regression algorithm [25] is used to learn the weights in Equation (2). In this paper, LibSVM toolkit [5] is used for the regression training.
In the prediction stage, the candidate queries will be ranked using the cross-lingual query similarity score computed in terms of )),((),( efefCL ttfwttsim φ•= , and the queries with similarity score lower than a threshold will be regarded as nonrelevant. The threshold is learned using a development data set by fitting MLQS"s output.
QUERY SUGGESTION In Section 3, we presented a discriminative model for cross lingual query suggestion. However, objectively benchmarking a query suggestion system is not a trivial task. In this paper, we propose to use CLQS as an alternative to query translation, and test its effectiveness in CLIR tasks. The resulting good performance of CLIR corresponds to the high quality of the suggested queries.
Given a source query fq , a set of relevant queries }{ eq in the target language are recommended using the cross-lingual query suggestion system. Then a monolingual IR system based on the BM25 model [23] is called using each }{ eqq∈ as queries to retrieve documents. Then the retrieved documents are re-ranked based on the sum of the BM25 scores associated with each monolingual retrieval.
In this section, we will benchmark the cross-lingual query suggestion system, comparing its performance with monolingual query suggestion, studying the contribution of various information sources, and testing its effectiveness when being used in CLIR tasks.
In our experiments, French and English are selected as the source and target language respectively. Such selection is due to the fact that large scale query logs are readily available for these two languages. A one-month English query log (containing 7 million unique English queries with occurrence frequency more than 5) of MSN search engine is used as the target language log. And a monolingual query suggestion system is built based on it. In addition, 5,000 French queries are selected randomly from a French query log (containing around 3 million queries), and are manually translated into English by professional French-English translators. Among the 5,000 French queries, 4,171 queries have their translations in the English query log, and are used for CLQS training and testing. Furthermore, among the 4,171 French queries, 70% are used for cross-lingual query similarity training, 10% are used as the development data to determine the relevancy threshold, and 20% are used for testing. To retrieve the crosslingual related queries, a built-in-house French-English bilingual lexicon (containing 120,000 unique entries) and the Europarl corpus are used.
Besides benchmarking CLQS as an independent system, the CLQS is also tested as a query translation system for CLIR tasks. Based on the observation that the CLIR performance heavily relies on the quality of the suggested queries, this benchmark measures the quality of CLQS in terms of its effectiveness in helping CLIR. To perform such benchmark, we use the documents of TREC6 CLIR data (AP88-90 newswire, 750MB) with officially provided 25 short French-English queries pairs (CL1-CL25). The selection of this data set is due to the fact that the average length of the queries are 3.3 words long, which matches the web query logs we use to train CLQS.
Suggestion Mean-square-error (MSE) is used to measure the regression error and it is defined as follows: ( )2 ),(),( 1 ∑ −= i eiqMLeifiCL qTsimqqsim l MSE fi where l is the total number of cross-lingual query pairs in the testing data.
As described in Section 3.4, a relevancy threshold is learned using the development data, and only CLQS with similarity value above the threshold is regarded as truly relevant to the input query. In this way, CLQS can also be benchmarked as a classification task using precision (P) and recall (R) which are defined as follows: CLQS MLQSCLQS P S SS I = ,
MLQS MLQSCLQS R S SS I = where CLQSS is the set of relevant queries suggested by CLQS,
MLQSS is the set of relevant queries suggested by MLQS (see Section 3.2).
The benchmarking results with various feature configurations are shown in Table 1.
Regression Classification Features MSE P R DD 0.274 0.723 0.098 DD+PC 0.224 0.713 0.125 DD+PC+ Web
DD+PC+ Web+ML QS
Table 1. CLQS performance with different feature settings (DD: dictionary only; DD+PC: dictionary and parallel corpora; DD+PC+Web: dictionary, parallel corpora, and web mining; DD+PC+Web+MLQS: dictionary, parallel corpora, web mining and monolingual query suggestion) Table 1 reports the performance comparison with various feature settings. The baseline system (DD) uses a conventional query translation approach, i.e., a bilingual dictionary with cooccurrence-based translation disambiguation. The baseline system only covers less than 10% of the suggestions made by MLQS.
Using additional features obviously enables CLQS to generate more relevant queries. The most significant improvement on recall is achieved by exploiting MLQS. The final CLQS system is able to generate 42% of the queries suggested by MLQS. Among all the feature combinations, there is no significant change in precision. This indicates that our methods can improve the recall by effectively leveraging various information sources without losing the accuracy of the suggestions.
Besides benchmarking CLQS by comparing its output with MLQS output, 200 French queries are randomly selected from the French query log. These queries are double-checked to make sure that they are not in the CLQS training corpus. Then CLQS system is used to suggest relevant English queries for them. On average, for each French query, 8.7 relevant English queries are suggested.
Then the total 1,740 suggested English queries are manually checked by two professional English/French translators with cross-validation. Among the 1,747 suggested queries, 1,407 queries are recognized as relevant to the original ones, hence the accuracy is 80.9%. Figure 1 shows an example of CLQS of the French query terrorisme international (international terrorism in English).
In this section, CLQS is tested with French to English CLIR tasks.
We conduct CLIR experiments using the TREC 6 CLIR dataset described in Section 5.1. The CLIR is performed using a query translation system followed by a BM25-based [23] monolingual IR module. The following three different systems have been used to perform query translation: (1) CLQS: our CLQS system; (2) MT: Google French to English machine translation system; (3) DT: a dictionary based query translation system using cooccurrence statistics for translation disambiguation. The translation disambiguation algorithm is presented in Section 3.3.1.
Besides, the monolingual IR performance is also reported as a reference. The average precision of the four IR systems are reported in Table 2, and the 11-point precision-recall curves are shown in Figure 2.
Table 2. Average precision of CLIR on TREC 6 Dataset (Monolingual: monolingual IR system; MT: CLIR based on machine translation; DT: CLIR based on dictionary translation; CLQS: CLQS-based CLIR) IR System Average Precision % of Monolingual IR Monolingual 0.266 100% MT 0.217 81.6% DT 0.186 69.9% CLQS 0.233 87.6% Figure 1. An example of CLQS of the French query terrorisme international international terrorism (0.991); what is terrorism (0.943); counter terrorism (0.920); terrorist (0.911); terrorist attacks (0.898); international terrorist (0.853); world terrorism (0.845); global terrorism (0.833); transnational terrorism (0.821); human rights (0.811); terrorist groups (0. 777); patterns of global terrorism (0.762) september 11 (0.734) 11-point P-R curves (TREC6) 0
Recall Precison Monolingual MT DT CLQS The benchmark shows that using CLQS as a query translation tool outperforms CLIR based on machine translation by 7.4%, outperforms CLIR based on dictionary translation by 25.2%, and achieves 87.6% of the monolingual IR performance.
The effectiveness of CLQS lies in its ability in suggesting closely related queries besides accurate translations. For example, for the query CL14 terrorisme international (international terrorism), although the machine translation tool translates the query correctly, CLQS system still achieves higher score by recommending many additional related terms such as global terrorism, world terrorism, etc. (as shown in Figure 1). Another example is the query La pollution causée par l'automobile (air pollution due to automobile) of CL6. The MT tool provides the translation the pollution caused by the car, while CLQS system enumerates all the possible synonyms of car, and suggest the following queries car pollution, auto pollution, automobile pollution. Besides, other related queries such as global warming are also suggested. For the query CL12 La culture écologique (organic farming), the MT tool fails to generate the correct translation. Although the correct translation is neither in our French-English dictionary, CLQS system generates organic farm as a relevant query due to successful web mining.
The above experiment demonstrates the effectiveness of using CLQS to suggest relevant queries for CLIR enhancement. A related research is to perform query expansion to enhance CLIR [2, 18]. So it is very interesting to compare the CLQS approach with the conventional query expansion approaches. Following [18], post-translation expansion is performed based on pseudorelevance feedback (PRF) techniques. We first perform CLIR in the same way as before. Then we use the traditional PRF algorithm described in [24] to select expansion terms. In our experiments, the top 10 terms are selected to expand the original query, and the new query is used to search the collection for the second time. The new CLIR performance in terms of average precision is shown in Table 3. The 11-point P-R curves are drawn in Figure 3.
Although being enhanced by pseudo-relevance feedback, the CLIR using either machine translation or dictionary-based query translation still does not perform as well as CLQS-based approach. Statistical t-test [13] is conducted to indicate whether the CLQS-based CLIR performs significantly better. Pair-wise pvalues are shown in Table 4. Clearly, CLQS significantly outperforms MT and DT without PRF as well as DT+PRF, but its superiority over MT+PRF is not significant. However, when combined with PRF, CLQS significant outperforms all the other methods. This indicates the higher effectiveness of CLQS in related term identification by leveraging a wide spectrum of resources. Furthermore, post-translation expansion is capable of improving CLQS-based CLIR. This is due to the fact that CLQS and pseudo-relevance feedback are leveraging different categories of resources, and both approaches can be complementary.
IR System AP without PRF AP with PRF Monolingual 0.266 (100%) 0.288 (100%) MT 0.217 (81.6%) 0.222 (77.1%) DT 0.186 (69.9%) 0.220 (76.4%) CLQS 0.233 (87.6%) 0.259 (89.9%) 11-point P-R curves with pseudo relevance feedback (TREC6) 0
Recall Precison Monolingual MT DT CLQS MT DT MT+PRF DT+PRF CLQS 0.0298 3.84e-05 0.1472 0.0282 CLQS+PR F
In this paper, we proposed a new approach to cross-lingual query suggestion by mining relevant queries in different languages from query logs. The key solution to this problem is to learn a crosslingual query similarity measure by a discriminative model exploiting multiple monolingual and bilingual resources. The model is trained based on the principle that cross-lingual similarity should best fit the monolingual similarity between one query and the other query"s translation.
Figure 2. 11 points precision-recall on TREC6 CLIR data set Figure 3. 11 points precision-recall on TREC6 CLIR dataset with pseudo relevance feedback Table 3. Comparison of average precision (AP) on TREC 6 without and with post-translation expansion. (%) are the relative percentages over the monolingual IR performance Table 4. The results of pair-wise significance t-test. Here pvalue < 0.05 is considered statistically significant The baseline CLQS system applies a typical query translation approach, using a bilingual dictionary with co-occurrence-based translation disambiguation. This approach only covers 10% of the relevant queries suggested by an MLQS system (when the exact translation of the original query is given). By leveraging additional resources such as parallel corpora, web mining and logbased monolingual query expansion, the final system is able to cover 42% of the relevant queries suggested by an MLQS system with precision as high as 79.6%.
To further test the quality of the suggested queries, CLQS system is used as a query translation system in CLIR tasks.
Benchmarked using TREC 6 French to English CLIR task, CLQS demonstrates higher effectiveness than the traditional query translation methods using either bilingual dictionary or commercial machine translation tools.
The improvement on TREC French to English CLIR task by using CLQS demonstrates the high quality of the suggested queries. This also shows the strong correspondence between the input French queries and English queries in the log. In the future, we will build CLQS system between languages which may be more loosely correlated, e.g., English and Chinese, and study the CLQS performance change due to the less strong correspondence among queries in such languages.

Link graph features such as in-degree and PageRank have been shown to significantly improve the performance of text retrieval algorithms on the web. The HITS algorithm is also believed to be of interest for web search; to some degree, one may expect HITS to be more informative that other link-based features because it is query-dependent: it tries to measure the interest of pages with respect to a given query.
However, it remains unclear today whether there are practical benefits of HITS over other link graph measures. This is even more true when we consider that modern retrieval algorithms used on the web use a document representation which incorporates the document"s anchor text, i.e. the text of incoming links. This, at least to some degree, takes the link graph into account, in a query-dependent manner.
Comparing HITS to PageRank or in-degree empirically is no easy task. There are two main difficulties: scale and relevance. Scale is important because link-based features are known to improve in quality as the document graph grows.
If we carry out a small experiment, our conclusions won"t carry over to large graphs such as the web. However, computing HITS efficiently on a graph the size of a realistic web crawl is extraordinarily difficult. Relevance is also crucial because we cannot measure the performance of a feature in the absence of human judgments: what is crucial is ranking at the top of the ten or so documents that a user will peruse.
To our knowledge, this paper is the first attempt to evaluate HITS at a large scale and compare it to other link-based features with respect to human evaluated judgment.
Our results confirm many of the intuitions we have about link-based features and their relationship to text retrieval methods exploiting anchor text. This is reassuring: in the absence of a theoretical model capable of tying these measures with relevance, the only way to validate our intuitions is to carry out realistic experiments. However, we were quite surprised to find that HITS, a query-dependent feature, is about as effective as web page in-degree, the most simpleminded query-independent link-based feature. This continues to be true when the link-based features are combined with a text retrieval algorithm exploiting anchor text.
The remainder of this paper is structured as follows: Section 2 surveys related work. Section 3 describes the data sets we used in our study. Section 4 reviews the performance measures we used. Sections 5 and 6 describe the PageRank and HITS algorithms in more detail, and sketch the computational infrastructure we employed to carry out large scale experiments. Section 7 presents the results of our evaluations, and Section 8 offers concluding remarks.
The idea of using hyperlink analysis for ranking web search results arose around 1997, and manifested itself in the HITS [16, 17] and PageRank [5, 21] algorithms. The popularity of these two algorithms and the phenomenal success of the Google search engine, which uses PageRank, have spawned a large amount of subsequent research.
There are numerous attempts at improving the effectiveness of HITS and PageRank. Query-dependent link-based ranking algorithms inspired by HITS include SALSA [19],
Randomized HITS [20], and PHITS [7], to name a few.
Query-independent link-based ranking algorithms inspired by PageRank include TrafficRank [22], BlockRank [14], and TrustRank [11], and many others.
Another line of research is concerned with analyzing the mathematical properties of HITS and PageRank. For example, Borodin et al. [3] investigated various theoretical properties of PageRank, HITS, SALSA, and PHITS, including their similarity and stability, while Bianchini et al. [2] studied the relationship between the structure of the web graph and the distribution of PageRank scores, and Langville and Meyer examined basic properties of PageRank such as existence and uniqueness of an eigenvector and convergence of power iteration [18].
Given the attention that has been paid to improving the effectiveness of PageRank and HITS, and the thorough studies of the mathematical properties of these algorithms, it is somewhat surprising that very few evaluations of their effectiveness have been published. We are aware of two studies that have attempted to formally evaluate the effectiveness of HITS and of PageRank. Amento et al. [1] employed quantitative measures, but based their experiments on the result sets of just 5 queries and the web-graph induced by topical crawls around the result set of each query. A more recent study by Borodin et al. [4] is based on 34 queries, result sets of 200 pages per query obtained from Google, and a neighborhood graph derived by retrieving 50 in-links per result from Google. By contrast, our study is based on over 28,000 queries and a web graph covering 2.9 billion URLs.
Our evaluation is based on two data sets: a large web graph and a substantial set of queries with associated results, some of which were labeled by human judges.
Our web graph is based on a web crawl that was conducted in a breadth-first-search fashion, and successfully retrieved 463,685,607 HTML pages. These pages contain 17,672,011,890 hyperlinks (after eliminating duplicate hyperlinks embedded in the same web page), which refer to a total of 2,897,671,002 URLs. Thus, at the end of the crawl there were 2,433,985,395 URLs in the frontier set of the crawler that had been discovered, but not yet downloaded. The mean out-degree of crawled web pages is 38.11; the mean in-degree of discovered pages (whether crawled or not) is 6.10. Also, it is worth pointing out that there is a lot more variance in in-degrees than in out-degrees; some popular pages have millions of incoming links. As we will see, this property affects the computational cost of HITS.
Our query set was produced by sampling 28,043 queries from the MSN Search query log, and retrieving a total of 66,846,214 result URLs for these queries (using commercial search engine technology), or about 2,838 results per query on average. It is important to point out that our 2.9 billion URL web graph does not cover all these result URLs. In fact, only 9,525,566 of the result URLs (about 14.25%) were covered by the graph. 485,656 of the results in the query set (about 0.73% of all results, or about 17.3 results per query) were rated by human judges as to their relevance to the given query, and labeled on a six-point scale (the labels being definitive, excellent, good, fair, bad and detrimental).
Results were selected for judgment based on their commercial search engine placement; in other words, the subset of labeled results is not random, but biased towards documents considered relevant by pre-existing ranking algorithms.
Involving a human in the evaluation process is extremely cumbersome and expensive; however, human judgments are crucial for the evaluation of search engines. This is so because no document features have been found yet that can effectively estimate the relevance of a document to a user query. Since content-match features are very unreliable (and even more so link features, as we will see) we need to ask a human to evaluate the results in order to compare the quality of features.
Evaluating the retrieval results from document scores and human judgments is not trivial and has been the subject of many investigations in the IR community. A good performance measure should correlate with user satisfaction, taking into account that users will dislike having to delve deep in the results to find relevant documents. For this reason, standard correlation measures (such as the correlation coefficient between the score and the judgment of a document), or order correlation measures (such as Kendall tau between the score and judgment induced orders) are not adequate.
In this study, we quantify the effectiveness of various ranking algorithms using three measures: NDCG, MRR, and MAP.
The normalized discounted cumulative gains (NDCG) measure [13] discounts the contribution of a document to the overall score as the document"s rank increases (assuming that the best document has the lowest rank). Such a measure is particularly appropriate for search engines, as studies have shown that search engine users rarely consider anything beyond the first few results [12]. NDCG values are normalized to be between 0 and 1, with 1 being the NDCG of a perfect ranking scheme that completely agrees with the assessment of the human judges. The discounted cumulative gain at a particular rank-threshold T (DCG@T) is defined to be PT j=1 1 log(1+j)  2r(j) − 1  , where r(j) is the rating (0=detrimental, 1=bad, 2=fair, 3=good, 4=excellent, and 5=definitive) at rank j. The NDCG is computed by dividing the DCG of a ranking by the highest possible DCG that can be obtained for that query. Finally, the NDGCs of all queries in the query set are averaged to produce a mean NDCG.
The reciprocal rank (RR) of the ranked result set of a query is defined to be the reciprocal value of the rank of the highest-ranking relevant document in the result set. The RR at rank-threshold T is defined to be 0 if none of the highestranking T documents is relevant. The mean reciprocal rank (MRR) of a query set is the average reciprocal rank of all queries in the query set.
Given a ranked set of n results, let rel(i) be 1 if the result at rank i is relevant and 0 otherwise. The precision P(j) at rank j is defined to be 1 j Pj i=1 rel(i), i.e. the fraction of the relevant results among the j highest-ranking results.
The average precision (AP) at rank-threshold k is defined to be Pk i=1 P (i)rel(i) Pn i=1 rel(i) . The mean average precision (MAP) of a query set is the mean of the average precisions of all queries in the query set.
The above definitions of MRR and MAP rely on the notion of a relevant result. We investigated two definitions of relevance: One where all documents rated fair or better were deemed relevant, and one were all documents rated good or better were deemed relevant. For reasons of space, we only report MAP and MRR values computed using the latter definition; using the former definition does not change the qualitative nature of our findings. Similarly, we computed NDCG, MAP, and MRR values for a wide range of rank-thresholds; we report results here at rank 10; again, changing the rank-threshold never led us to different conclusions.
Recall that over 99% of documents are unlabeled. We chose to treat all these documents as irrelevant to the query.
For some queries, however, not all relevant documents have been judged. This introduces a bias into our evaluation: features that bring new documents to the top of the rank may be penalized. This will be more acute for features less correlated to the pre-existing commercial ranking algorithms used to select documents for judgment. On the other hand, most queries have few perfect relevant documents (i.e. home page or item searches) and they will most often be within the judged set.
WEB GRAPH PageRank is a query-independent measure of the importance of web pages, based on the notion of peer-endorsement: A hyperlink from page A to page B is interpreted as an endorsement of page B"s content by page A"s author. The following recursive definition captures this notion of endorsement: R(v) = X (u,v)∈E R(u) Out(u) where R(v) is the score (importance) of page v, (u, v) is an edge (hyperlink) from page u to page v contained in the edge set E of the web graph, and Out(u) is the out-degree (number of embedded hyperlinks) of page u. However, this definition suffers from a severe shortcoming: In the fixedpoint of this recursive equation, only edges that are part of a strongly-connected component receive a non-zero score. In order to overcome this deficiency, Page et al. grant each page a guaranteed minimum score, giving rise to the definition of standard PageRank: R(v) = d |V | + (1 − d) X (u,v)∈E R(u) Out(u) where |V | is the size of the vertex set (the number of known web pages), and d is a damping factor, typically set to be between 0.1 and 0.2.
Assuming that scores are normalized to sum up to 1,
PageRank can be viewed as the stationary probability distribution of a random walk on the web graph, where at each step of the walk, the walker with probability 1 − d moves from its current node u to a neighboring node v, and with probability d selects a node uniformly at random from all nodes in the graph and jumps to it. In the limit, the random walker is at node v with probability R(v).
One issue that has to be addressed when implementing PageRank is how to deal with sink nodes, nodes that do not have any outgoing links. One possibility would be to select another node uniformly at random and transition to it; this is equivalent to adding edges from each sink nodes to all other nodes in the graph. We chose the alternative approach of introducing a single phantom node. Each sink node has an edge to the phantom node, and the phantom node has an edge to itself.
In practice, PageRank scores can be computed using power iteration. Since PageRank is query-independent, the computation can be performed off-line ahead of query time. This property has been key to PageRank"s success, since it is a challenging engineering problem to build a system that can perform any non-trivial computation on the web graph at query time.
In order to compute PageRank scores for all 2.9 billion nodes in our web graph, we implemented a distributed version of PageRank. The computation consists of two distinct phases. In the first phase, the link files produced by the web crawler, which contain page URLs and their associated link URLs in textual form, are partitioned among the machines in the cluster used to compute PageRank scores, and converted into a more compact format along the way.
Specifically, URLs are partitioned across the machines in the cluster based on a hash of the URLs" host component, and each machine in the cluster maintains a table mapping the URL to a 32-bit integer. The integers are drawn from a densely packed space, so as to make suitable indices into the array that will later hold the PageRank scores. The system then translates our log of pages and their associated hyperlinks into a compact representation where both page URLs and link URLs are represented by their associated 32-bit integers. Hashing the host component of the URLs guarantees that all URLs from the same host are assigned to the same machine in our scoring cluster. Since over 80% of all hyperlinks on the web are relative (that is, are between two pages on the same host), this property greatly reduces the amount of network communication required by the second stage of the distributed scoring computation.
The second phase performs the actual PageRank power iteration. Both the link data and the current PageRank vector reside on disk and are read in a streaming fashion; while the new PageRank vector is maintained in memory.
We represent PageRank scores as 64-bit floating point numbers. PageRank contributions to pages assigned to remote machines are streamed to the remote machine via a TCP connection.
We used a three-machine cluster, each machine equipped with 16 GB of RAM, to compute standard PageRank scores for all 2.9 billion URLs that were contained in our web graph. We used a damping factor of 0.15, and performed 200 power iterations. Starting at iteration 165, the L∞ norm of the change in the PageRank vector from one iteration to the next had stopped decreasing, indicating that we had reached as much of a fixed point as the limitations of 64-bit floating point arithmetic would allow.
Number of back-links sampled per result NDCG@10 hits-aut-all hits-aut-ih hits-aut-id
Number of back-links sampled per result MAP@10 hits-aut-all hits-aut-ih hits-aut-id
Number of back-links sampled per result MRR@10 hits-aut-all hits-aut-ih hits-aut-id Figure 1: Effectiveness of authority scores computed using different parameterizations of HITS.
A post-processing phase uses the final PageRank vectors (one per machine) and the table mapping URLs to 32-bit integers (representing indices into each PageRank vector) to score the result URL in our query log. As mentioned above, our web graph covered 9,525,566 of the 66,846,214 result URLs. These URLs were annotated with their computed PageRank score; all other URLs received a score of 0.
HITS, unlike PageRank, is a query-dependent ranking algorithm. HITS (which stands for Hypertext Induced Topic Search) is based on the following two intuitions: First, hyperlinks can be viewed as topical endorsements: A hyperlink from a page u devoted to topic T to another page v is likely to endorse the authority of v with respect to topic T. Second, the result set of a particular query is likely to have a certain amount of topical coherence. Therefore, it makes sense to perform link analysis not on the entire web graph, but rather on just the neighborhood of pages contained in the result set, since this neighborhood is more likely to contain topically relevant links. But while the set of nodes immediately reachable from the result set is manageable (given that most pages have only a limited number of hyperlinks embedded into them), the set of pages immediately leading to the result set can be enormous. For this reason, Kleinberg suggests sampling a fixed-size random subset of the pages linking to any high-indegree page in the result set. Moreover,
Kleinberg suggests considering only links that cross host boundaries, the rationale being that links between pages on the same host (intrinsic links) are likely to be navigational or nepotistic and not topically relevant.
Given a web graph (V, E) with vertex set V and edge set E ⊆ V × V , and the set of result URLs to a query (called the root set R ⊆ V ) as input, HITS computes a neighborhood graph consisting of a base set B ⊆ V (the root set and some of its neighboring vertices) and some of the edges in E induced by B. In order to formalize the definition of the neighborhood graph, it is helpful to first introduce a sampling operator and the concept of a linkselection predicate.
Given a set A, the notation Sn[A] draws n elements uniformly at random from A; Sn[A] = A if |A| ≤ n.
A link section predicate P takes an edge (u, v) ∈ E. In this study, we use the following three link section predicates: all(u, v) ⇔ true ih(u, v) ⇔ host(u) = host(v) id(u, v) ⇔ domain(u) = domain(v) where host(u) denotes the host of URL u, and domain(u) denotes the domain of URL u. So, all is true for all links, whereas ih is true only for inter-host links, and id is true only for inter-domain links.
The outlinked-set OP of the root set R w.r.t. a linkselection predicate P is defined to be: OP = [ u∈R {v ∈ V : (u, v) ∈ E ∧ P(u, v)} The inlinking-set IP s of the root set R w.r.t. a link-selection predicate P and a sampling value s is defined to be: IP s = [ v∈R Ss[{u ∈ V : (u, v) ∈ E ∧ P(u, v)}] The base set BP s of the root set R w.r.t. P and s is defined to be: BP s = R ∪ IP s ∪ OP The neighborhood graph (BP s , NP s ) has the base set BP s as its vertex set and an edge set NP s containing those edges in E that are covered by BP s and permitted by P: NP s = {(u, v) ∈ E : u ∈ BP s ∧ v ∈ BP s ∧ P(u, v)} To simplify notation, we write B to denote BP s , and N to denote NP s .
For each node u in the neighborhood graph, HITS computes two scores: an authority score A(u), estimating how authoritative u is on the topic induced by the query, and a hub score H(u), indicating whether u is a good reference to many authoritative pages. This is done using the following algorithm:
q 1 |B| , A(u) := q 1 |B| .
(a) For all v ∈ B : A (v) := P (u,v)∈N H(u) (b) For all u ∈ B : H (u) := P (u,v)∈N A(v) (c) H := H 2, A := A 2 where X 2 normalizes the vector X to unit length in euclidean space, i.e. the squares of its elements sum up to 1.
In practice, implementing a system that can compute HITS within the time constraints of a major search engine (where the peak query load is in the thousands of queries per second, and the desired query response time is well below one second) is a major engineering challenge. Among other things, the web graph cannot reasonably be stored on disk, since .221 .106 .105 .104 .102 .095 .092 .090 .038 .036 .035 .034 .032 .032 .011
bm25f degree-in-id degree-in-ih hits-aut-id-25 hits-aut-ih-100 degree-in-all pagerank hits-aut-all-100 hits-hub-all-100 hits-hub-ih-100 hits-hub-id-100 degree-out-all degree-out-ih degree-out-id random NDCG@10 .100 .035 .033 .033 .033 .029 .027 .027 .008 .007 .007 .006 .006 .006 .002
bm25f hits-aut-id-9 degree-in-id hits-aut-ih-15 degree-in-ih degree-in-all pagerank hits-aut-all-100 hits-hub-all-100 hits-hub-ih-100 hits-hub-id-100 degree-out-all degree-out-ih degree-out-id random MAP@10 .273 .132 .126 .117 .114 .101 .101 .097 .032 .032 .030 .028 .027 .027 .007
bm25f hits-aut-id-9 hits-aut-ih-15 degree-in-id degree-in-ih degree-in-all hits-aut-all-100 pagerank hits-hub-all-100 hits-hub-ih-100 hits-hub-id-100 degree-out-all degree-out-ih degree-out-id random MRR@10 Figure 2: Effectiveness of different features. seek times of modern hard disks are too slow to retrieve the links within the time constraints, and the graph does not fit into the main memory of a single machine, even when using the most aggressive compression techniques.
In order to experiment with HITS and other query-dependent link-based ranking algorithms that require non-regular accesses to arbitrary nodes and edges in the web graph, we implemented a system called the Scalable Hyperlink Store, or SHS for short. SHS is a special-purpose database, distributed over an arbitrary number of machines that keeps a highly compressed version of the web graph in memory and allows very fast lookup of nodes and edges. On our hardware, it takes an average of 2 microseconds to map a URL to a 64-bit integer handle called a UID, 15 microseconds to look up all incoming or outgoing link UIDs associated with a page UID, and 5 microseconds to map a UID back to a URL (the last functionality not being required by HITS).
The RPC overhead is about 100 microseconds, but the SHS API allows many lookups to be batched into a single RPC request.
We implemented the HITS algorithm using the SHS infrastructure. We compiled three SHS databases, one containing all 17.6 billion links in our web graph (all), one containing only links between pages that are on different hosts (ih, for inter-host), and one containing only links between pages that are on different domains (id). We consider two URLs to belong to different hosts if the host portions of the URLs differ (in other words, we make no attempt to determine whether two distinct symbolic host names refer to the same computer), and we consider a domain to be the name purchased from a registrar (for example, we consider news.bbc.co.uk and www.bbc.co.uk to be different hosts belonging to the same domain). Using each of these databases, we computed HITS authority and hub scores for various parameterizations of the sampling operator S, sampling between 1 and 100 back-links of each page in the root set.
Result URLs that were not covered by our web graph automatically received authority and hub scores of 0, since they were not connected to any other nodes in the neighborhood graph and therefore did not receive any endorsements.
We performed forty-five different HITS computations, each combining one of the three link selection predicates (all, ih, and id) with a sampling value. For each combination, we loaded one of the three databases into an SHS system running on six machines (each equipped with 16 GB of RAM), and computed HITS authority and hub scores, one query at a time. The longest-running combination (using the all database and sampling 100 back-links of each root set vertex) required 30,456 seconds to process the entire query set, or about 1.1 seconds per query on average.
For a given query Q, we need to rank the set of documents satisfying Q (the result set of Q). Our hypothesis is that good features should be able to rank relevant documents in this set higher than non-relevant ones, and this should result in an increase in each performance measure over the query set. We are specifically interested in evaluating the usefulness of HITS and other link-based features. In principle, we could do this by sorting the documents in each result set by their feature value, and compare the resulting NDCGs. We call this ranking with isolated features.
Let us first examine the relative performance of the different parameterizations of the HITS algorithm we examined. Recall that we computed HITS for each combination of three link section schemes - all links (all), inter-host links only (ih), and inter-domain links only (id) - with back-link sampling values ranging from 1 to 100. Figure 1 shows the impact of the number of sampled back-links on the retrieval performance of HITS authority scores. Each graph is associated with one performance measure. The horizontal axis of each graph represents the number of sampled back-links, the vertical axis represents performance under the appropriate measure, and each curve depicts a link selection scheme.
The id scheme slightly outperforms ih, and both vastly outperform the all scheme - eliminating nepotistic links pays off. The performance of the all scheme increases as more back-links of each root set vertex are sampled, while the performance of the id and ih schemes peaks at between 10 and 25 samples and then plateaus or even declines, depending on the performance measure.
Having compared different parameterizations of HITS, we will now fix the number of sampled back-links at 100 and compare the three link selection schemes against other isolated features: PageRank, in-degree and out-degree counting links of all pages, of different hosts only and of different domains only (all, ih and id datasets respectively), and a text retrieval algorithm exploiting anchor text: BM25F[24].
BM25F is a state-of-the art ranking function solely based on textual content of the documents and their associated anchor texts. BM25F is a descendant of BM25 that combines the different textual fields of a document, namely title, body and anchor text. This model has been shown to be one of the best-performing web search scoring functions over the last few years [8, 24]. BM25F has a number of free parameters (2 per field, 6 in our case); we used the parameter values described in [24]. .341 .340 .339 .337 .336 .336 .334 .311 .311 .310 .310 .310 .310 .231
degree-in-id degree-in-ih degree-in-all hits-aut-ih-100 hits-aut-all-100 pagerank hits-aut-id-10 degree-out-all hits-hub-all-100 degree-out-ih hits-hub-ih-100 degree-out-id hits-hub-id-10 bm25f NDCG@10 .152 .152 .151 .150 .150 .149 .149 .137 .136 .136 .128 .127 .127 .100
degree-in-ih degree-in-id degree-in-all hits-aut-ih-100 hits-aut-all-100 hits-aut-id-10 pagerank hits-hub-all-100 degree-out-ih hits-hub-id-100 degree-out-all degree-out-id hits-hub-ih-100 bm25f MAP@10 .398 .397 .394 .394 .392 .392 .391 .356 .356 .356 .356 .356 .355 .273
degree-in-id degree-in-ih degree-in-all hits-aut-ih-100 hits-aut-all-100 pagerank hits-aut-id-10 degree-out-all hits-hub-all-100 degree-out-ih hits-hub-ih-100 degree-out-id hits-hub-id-10 bm25f MRR@10 Figure 3: Effectiveness measures for linear combinations of link-based features with BM25F.
Figure 2 shows the NDCG, MRR, and MAP measures of these features. Again all performance measures (and for all rank-thresholds we explored) agree. As expected,
BM25F outperforms all link-based features by a large margin. The link-based features are divided into two groups, with a noticeable performance drop between the groups.
The better-performing group consists of the features that are based on the number and/or quality of incoming links (in-degree, PageRank, and HITS authority scores); and the worse-performing group consists of the features that are based on the number and/or quality of outgoing links (outdegree and HITS hub scores). In the group of features based on incoming links, features that ignore nepotistic links perform better than their counterparts using all links.
Moreover, using only inter-domain (id) links seems to be marginally better than using inter-host (ih) links.
The fact that features based on outgoing links underperform those based on incoming links matches our expectations; if anything, it is mildly surprising that outgoing links provide a useful signal for ranking at all. On the other hand, the fact that in-degree features outperform PageRank under all measures is quite surprising. A possible explanation is that link-spammers have been targeting the published PageRank algorithm for many years, and that this has led to anomalies in the web graph that affect PageRank, but not other link-based features that explore only a distance-1 neighborhood of the result set. Likewise, it is surprising that simple query-independent features such as in-degree, which might estimate global quality but cannot capture relevance to a query, would outperform query-dependent features such as HITS authority scores.
However, we cannot investigate the effect of these features in isolation, without regard to the overall ranking function, for several reasons. First, features based on the textual content of documents (as opposed to link-based features) are the best predictors of relevance. Second, link-based features can be strongly correlated with textual features for several reasons, mainly the correlation between in-degree and numFeature Transform function bm25f T(s) = s pagerank T(s) = log(s + 3 · 10−12 ) degree-in-* T(s) = log(s + 3 · 10−2 ) degree-out-* T(s) = log(s + 3 · 103 ) hits-aut-* T(s) = log(s + 3 · 10−8 ) hits-hub-* T(s) = log(s + 3 · 10−1 ) Table 1: Near-optimal feature transform functions. ber of textual anchor matches.
Therefore, one must consider the effect of link-based features in combination with textual features. Otherwise, we may find a link-based feature that is very good in isolation but is strongly correlated with textual features and results in no overall improvement; and vice versa, we may find a link-based feature that is weak in isolation but significantly improves overall performance.
For this reason, we have studied the combination of the link-based features above with BM25F. All feature combinations were done by considering the linear combination of two features as a document score, using the formula score(d) =Pn i=1 wiTi(Fi(d)), where d is a document (or documentquery pair, in the case of BM25F), Fi(d) (for 1 ≤ i ≤ n) is a feature extracted from d, Ti is a transform, and wi is a free scalar weight that needs to be tuned. We chose transform functions that we empirically determined to be well-suited.
Table 1 shows the chosen transform functions.
This type of linear combination is appropriate if we assume features to be independent with respect to relevance and an exponential model for link features, as discussed in [8]. We tuned the weights by selecting a random subset of 5,000 queries from the query set, used an iterative refinement process to find weights that maximized a given performance measure on that training set, and used the remaining 23,043 queries to measure the performance of the thus derived scoring functions.
We explored the pairwise combination of BM25F with every link-based scoring function. Figure 3 shows the NDCG,
MRR, and MAP measures of these feature combinations, together with a baseline BM25F score (the right-most bar in each graph), which was computed using the same subset of 23,045 queries that were used as the test set for the feature combinations. Regardless of the performance measure applied, we can make the following general observations:
results in a substantial performance improvement over BM25F in isolation.
incoming links (PageRank, in-degree, and HITS authority scores) performs substantially better than the combination with features based on outgoing links (HITS hub scores and out-degree).
combinations of BM25F with features based on incoming links is comparatively small, and the relative ordering of feature combinations is fairly stable across the
MAP@10
bm25fnorm pagerank degree-in-id hits-aut-id-100 Figure 4: Effectiveness measures for selected isolated features, broken down by query specificity. ferent performance measures used. However, the combination of BM25F with any in-degree variant, and in particular with id in-degree, consistently outperforms the combination of BM25F with PageRank or HITS authority scores, and can be computed much easier and faster.
Finally, we investigated whether certain features are better for some queries than for others. Particularly, we are interested in the relationship between the specificity of a query and the performance of different ranking features. The most straightforward measure of the specificity of a query Q would be the number of documents in a search engine"s corpus that satisfy Q. Unfortunately, the query set available to us did not contain this information. Therefore, we chose to approximate the specificity of Q by summing up the inverse document frequencies of the individual query terms comprising Q. The inverse document frequency (IDF) of a term t with respect to a corpus C is defined to be logN/doc(t), where doc(t) is the number of documents in C containing t and N is the total number of documents in C. By summing up the IDFs of the query terms, we make the (flawed) assumption that the individual query terms are independent of each other. However, while not perfect, this approximation is at least directionally correct.
We broke down our query set into 13 buckets, each bucket associated with an interval of query IDF values, and we computed performance metrics for all ranking functions applied (in isolation) to the queries in each bucket. In order to keep the graphs readable, we will not show the performance of all the features, but rather restrict ourselves to the four most interesting ones: PageRank, id HITS authority scores, id in-degree, and BM25F. Figure 4 shows the MAP@10 for all 13 query specificity buckets. Buckets on the far left of each graph represent very general queries; buckets on the far right represent very specific queries. The figures on the upper x axis of each graph show the number of queries in each bucket (e.g. the right-most bucket contains 1,629 queries).
BM25F performs best for medium-specific queries, peaking at the buckets representing the IDF sum interval [12,14).
By comparison, HITS peaks at the bucket representing the IDF sum interval [4,6), and PageRank and in-degree peak at the bucket representing the interval [6,8), i.e. more general queries.
This paper describes a large-scale evaluation of the effectiveness of HITS in comparison with other link-based ranking algorithms, in particular PageRank and in-degree, when applied in isolation or in combination with a text retrieval algorithm exploiting anchor text (BM25F).
Evaluation is carried out with respect to a large number of human evaluated queries, using three different measures of effectiveness: NDCG, MRR, and MAP. Evaluating link-based features in isolation, we found that web page in-degree outperforms PageRank, and is about as effwective as HITS authority scores. HITS hub scores and web page out-degree are much less effective ranking features, but still outperform a random ordering. A linear combination of any link-based features with BM25F produces a significant improvement in performance, and there is a clear difference between combining BM25F with a feature based on incoming links (indegree, PageRank, or HITS authority scores) and a feature based on outgoing links (HITS hub scores and out-degree), but within those two groups the precise choice of link-based feature matters relatively little.
We believe that the measurements presented in this paper provide a solid evaluation of the best well-known link-based ranking schemes. There are many possible variants of these schemes, and many other link-based ranking algorithms have been proposed in the literature, hence we do not claim this work to be the last word on this subject, but rather the first step on a long road. Future work includes evaluation of different parameterizations of PageRank and HITS. In particular, we would like to study the impact of changes to the PageRank damping factor on effectiveness, the impact of various schemes meant to counteract the effects of link spam, and the effect of weighing hyperlinks differently depending on whether they are nepotistic or not. Going beyond PageRank and HITS, we would like to measure the effectiveness of other link-based ranking algorithms, such as SALSA. Finally, we are planning to experiment with more complex feature combinations.
[1] B. Amento, L. Terveen, and W. Hill. Does authority mean quality? Predicting expert quality ratings of web documents. In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 296-303, 2000. [2] M. Bianchini, M. Gori, and F. Scarselli. Inside PageRank. ACM Transactions on Internet Technology, 5(1):92-128, 2005. [3] A. Borodin, G. O. Roberts, and J. S. Rosenthal.
Finding authorities and hubs from link structures on the World Wide Web. In Proc. of the 10th International World Wide Web Conference, pages 415-429, 2001. [4] A. Borodin, G. O. Roberts, J. S. Rosenthal, and P. Tsaparas. Link analysis ranking: algorithms, theory, and experiments. ACM Transactions on Interet Technology, 5(1):231-297, 2005. [5] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1-7):107-117, 1998. [6] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. of the 22nd International Conference on Machine Learning, pages 89-96, New York, NY, USA, 2005. ACM Press. [7] D. Cohn and H. Chang. Learning to probabilistically identify authoritative documents. In Proc. of the 17th International Conference on Machine Learning, pages 167-174, 2000. [8] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In Proc. of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 416-423,
[9] E. Garfield. Citation analysis as a tool in journal evaluation. Science, 178(4060):471-479, 1972. [10] Z. Gy¨ongyi and H. Garcia-Molina. Web spam taxonomy. In 1st International Workshop on Adversarial Information Retrieval on the Web, 2005. [11] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen.
Combating web spam with TrustRank. In Proc. of the 30th International Conference on Very Large Databases, pages 576-587, 2004. [12] B. J. Jansen, A. Spink, J. Bateman, and T. Saracevic.
Real life information retrieval: a study of user queries on the web. ACM SIGIR Forum, 32(1):5-17, 1998. [13] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422-446, 2002. [14] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H. Golub. Extrapolation methods for accelerating PageRank computations. In Proc. of the 12th International World Wide Web Conference, pages 261-270, 2003. [15] M. M. Kessler. Bibliographic coupling between scientific papers. American Documentation, 14(1):10-25, 1963. [16] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. In Proc. of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 668-677, 1998. [17] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604-632, 1999. [18] A. N. Langville and C. D. Meyer. Deeper inside PageRank. Internet Mathematics, 1(3):2005, 335-380. [19] R. Lempel and S. Moran. The stochastic approach for link-structure analysis (SALSA) and the TKC effect.

Evaluation is a primary concern in the Information Retrieval (IR) field. TREC (Text REtrieval Conference) [12, 15] is an annual benchmarking exercise that has become a de facto standard in IR evaluation: before the actual conference, TREC provides to participants a collection of documents and a set of topics (representations of information needs). Participants use their systems to retrieve, and submit to TREC, a list of documents for each topic. After the lists have been submitted and pooled, the TREC organizers employ human assessors to provide relevance judgements on the pooled set. This defines a set of relevant documents for each topic. System effectiveness is then measured by well established metrics (Mean Average Precision being the most used). Other conferences such as NTCIR, INEX, CLEF provide comparable data.
Network analysis is a discipline that studies features and properties of (usually large) networks, or graphs. Of particular importance is Social Network Analysis [16], that studies networks made up by links among humans (friendship, acquaintance, co-authorship, bibliographic citation, etc.).
Network analysis and IR fruitfully meet in Web Search Engine implementation, as is already described in textbooks [3,6]. Current search engines use link analysis techniques to help rank the retrieved documents. Some indicators (and the corresponding algorithms that compute them) have been found useful in this respect, and are nowadays well known: Inlinks (the number of links to a Web page), PageRank [7], and HITS (Hyperlink-Induced Topic Search) [5]. Several extensions to these algorithms have been and are being proposed. These indicators and algorithms might be quite general in nature, and can be used for applications which are very different from search result ranking. One example is using HITS for stemming, as described by Agosti et al. [1].
In this paper, we propose and demonstrate a method for constructing a network, specifically a weighted complete bidirectional directed bipartite graph, on a set of TREC topics and participating systems. Links represent effectiveness measurements on system-topic pairs. We then apply analysis methods originally developed for search applications to the resulting network. This reveals phenomena previously hidden in TREC data. In passing, we also provide a small generalization to Kleinberg"s HITS algorithm, as well as to Inlinks and PageRank.
The paper is organized as follows: Sect. 2 gives some motivations for the work. Sect. 3 presents the basic ideas of normalizing average precision and of constructing a systemstopics graph, whose properties are analyzed in Sect. 4; Sect. 5 presents some experiments on TREC 8 data; Sect. 6 discusses some issues and Sect. 7 closes the paper.
We are interested in the following hypotheses:
t1 · · · tn MAP s1 AP(s1, t1) · · · AP(s1, tn) MAP(s1) ... ... ... sm AP(sm, t1) · · · AP(sm, tn) MAP(sm) AAP AAP(t1) · · · AAP(tn) (a) t1 t2 · · · MAP s1 0.5 0.4 · · · 0.6 s2 0.4 · · · · · · 0.3 ... ... ... ...
AAP 0.6 0.3 · · · (b) Table 1: AP, MAP and AAP
easy and difficult topics;
more or less effective systems.
The first of these hypotheses needs no further justification - every reported significant difference between any two systems supports it. There is now also quite a lot of evidence for the second, centered on the TREC Robust Track [14].
Our primary interest is in the third and fourth. The third might be regarded as being of purely academic interest; however, the fourth has the potential for being of major practical importance in evaluation studies. If we could identify a relatively small number of topics which were really good at distinguishing effective and ineffective systems, we could save considerable effort in evaluating systems.
One possible direction from this point would be to attempt direct identification of such small sets of topics. However, in the present paper, we seek instead to explore the relationships suggested by the hypotheses, between what different topics tell us about systems and what different systems tell us about topics. We seek methods of building and analysing a matrix of system-topic normalised performances, with a view to giving insight into the issue and confirming or refuting the third and fourth hypotheses. It turns out that the obvious symmetry implied by the above formulation of the hypotheses is a property worth investigating, and the investigation does indeed give us valuable insights.
From TREC results, one can produce an Average Precision (AP) table (see Tab. 1a): each AP(si, tj) value measures the AP of system si on topic tj.
Besides AP values, the table shows Mean Average Precision (MAP) values i.e., the mean of the AP values for a single system over all topics, and what we call Average Average Precision (AAP) values i.e., the average of the AP values for a single topic over all systems: MAP(si) = 1 n nX j=1 AP(si, tj), (1) AAP(tj) = 1 m mX i=1 AP(si, tj). (2) MAPs are indicators of systems performance: higher MAP means good system. AAP are indicators of the performance on a topic: higher AAP means easy topic - a topic on which all or most systems have good performance.
MAP is a standard, well known, and widely used IR effectiveness measure. Single AP values are used too (e.g., in AP histograms). Topic difficulty is often discussed (e.g., in TREC Robust track [14]), although AAP values are not used and, to the best of our knowledge, have never been proposed (the median, not the average, of AP on a topic is used to produce TREC AP histograms [11]). However, the AP values in Tab. 1 present two limitations, which are symmetric in some respect: • Problem 1. They are not reliable to compare the effectiveness of a system on different topics, relative to the other systems. If, for example, AP(s1, t1) > AP(s1, t2), can we infer that s1 is a good system (i.e., has a good performance) on t1 and a bad system on t2? The answer is no: t1 might be an easy topic (with high AAP) and t2 a difficult one (low AAP). See an example in Tab. 1b: s1 is outperformed (on average) by the other systems on t1, and it outperforms the other systems on t2. • Problem 2. Conversely, if, for example, AP(s1, t1) > AP(s2, t1), can we infer that t1 is considered easier by s1 than by s2? No, we cannot: s1 might be a good system (with high MAP) and s2 a bad one (low MAP); see an example in Tab. 1b.
These two problems are a sort of breakdown of the well known high influence of topics on IR evaluation; again, our formulation makes explicit the topics / systems symmetry.
To avoid these two problems, we can normalize the AP table in two ways. The first normalization removes the influence of the single topic ease on system performance. Each AP(si, tj) value in the table depends on both system goodness and topic ease (the value will increase if a system is good and/or the topic is easy). By subtracting from each AP(si, tj) the AAP(tj) value, we obtain normalized AP values (APA(si, tj), Normalized AP according to AAP): APA(si, tj) = AP(si, tj) − AAP(tj), (3) that depend on system performance only (the value will increase only if system performance is good). See Tab. 2a.
The second normalization removes the influence of the single system effectiveness on topic ease: by subtracting from each AP(si, tj) the MAP(si) value, we obtain normalized AP values (APM(si, tj), Normalized AP according to MAP): APM(si, tj) = AP(si, tj) − MAP(si), (4) that depend on topic ease only (the value will increase only if the topic is easy, i.e., all systems perform well on that topic). See Tab. 2b.
In other words, APA avoids Problem 1: APA(s, t) values measure the performance of system s on topic t normalized t1 · · · tn MAP s1 APA(s1, t1) · · · APA(s1, tn) MAP(s1) ... ... ... sm APA(sm, t1) · · · APA(sm, tn) MAP(sm)
(a) t1 · · · tn s1 APM(s1, t1) · · · APM(s1, tn) 0 ... ... ... sm APM(sm, t1) · · · APM(sm, tn) 0 AAP AAP(t1) · · · AAP(tn) 0 (b) t1 t2 · · · MAP s1 −0.1 0.1 · · · . . . s2 0.2 · · · · · · . . . ... ... ...
t1 t2 · · · s1 −0.1 −0.2 · · · 0 s2 0.1 · · · · · · 0 ... ... ...
AAP . . . . . . · · · (c) (d) Table 2: Normalizations: APA and MAP: normalized AP (APA) and MAP (MAP) (a); normalized AP (APM) and AAP (AAP) (b); a numeric example (c) and (d) according to the ease of the topic (easy topics will not have higher APA values). Now, if, for example, APA(s1, t2) > APA(s1, t1), we can infer that s1 is a good system on t2 and a bad system on t1 (see Tab. 2c). Vice versa, APM avoids Problem 2: APM(s, t) values measure the ease of topic t according to system s, normalized according to goodness of the system (good systems will not lead to higher APM values). If, for example, APM(s2, t1) > APM(s1, t1), we can infer that t1 is considered easier by s2 than by s1 (see Tab. 2d).
On the basis of Tables 2a and 2b, we can also define two new measures of system effectiveness and topic ease, i.e., a Normalized MAP (MAP), obtained by averaging the APA values on one row in Tab. 2a, and a Normalized AAP (AAP), obtained by averaging the APM values on one column in Tab. 2b: MAP(si) = 1 n nX j=1 APA(si, tj) (5) AAP(tj) = 1 m mX i=1 APM(si, tj). (6) Thus, overall system performance can be measured, besides by means of MAP, also by means of MAP. Moreover,
MAP is equivalent to MAP, as can be immediately proved by using Eqs. (5), (3), and (1): MAP(si) = 1 n nX j=1 (AP(si, tj) − AAP(tj)) = = MAP(si) − 1 n nX j=1 AAP(tj) (and 1 n Pn j=1 AAP(tj) is the same for all systems). And, conversely, overall topic ease can be measured, besides by t1 · · · tn s1 ... APM sm t1 · · · tn s1 ... APA sm s1 · · · sm t1 · · · tn s1 ... 0 APM 0 sm t1 ... APA T
tn MAP AAP 0 Figure 1: Construction of the adjacency matrix.
APA T is the transpose of APA. means of AAP, also by means of AAP, and this is equivalent (the proof is analogous, and relies on Eqs. (6), (4), and (2)).
The two Tables 2a and 2b are interesting per se, and can be analyzed in several different ways. In the following we propose an analysis based on network analysis techniques, mainly Kleinberg"s HITS algorithm. There is a little further discussion of these normalizations in Sect. 6.
The two tables 2a and 2b can be merged into a single one with the procedure shown in Fig. 1. The obtained matrix can be interpreted as the adjacency matrix of a complete weighted bipartite graph, that we call Systems-Topics graph.
Arcs and weights in the graph can be interpreted as follows: • (weight on) arc s → t: how much the system s thinks that the topic t is easy - assuming that a system has no knowledge of the other systems (or in other words, how easy we might think the topic is, knowing only the results for this one system). This corresponds to APM values, i.e., to normalized topic ease (Fig. 2a). • (weight on) arc s ← t: how much the topic t thinks that the system s is good - assuming that a topic has no knowledge of the other topics (or in other words, how good we might think the system is, knowing only the results for this one topic). This corresponds to APA (normalized system effectiveness, Fig. 2b).
Figs. 2c and 2d show the Systems-Topics complete weighted bipartite graph, on a toy example with 4 systems and 2 topics; the graph is split in two parts to have an understandable graphical representation: arcs in Fig. 2c are labeled with APM values; arcs in Fig. 2d are labeled with APA values.
The sum of weighted outlinks, i.e., the sum of the weights on the outgoing arcs from each node, is always zero: • The outlinks on each node corresponding to a system s (Fig. 2c) is the sum of all the corresponding APM values on one row of the matrix in Tab. 2b. • The outlinks on each node corresponding to a topic t (Fig. 2d) is the sum of all the corresponding APA (a) (b) (c) (d) Figure 2: The relationships between systems and topics (a) and (b); and the Systems-Topics graph for a toy example (c) and (d). Dashed arcs correspond to negative values. h (a) s1 ... sm t1 ... tn = s1 · · · sm t1 · · · tn s1 ... 0 APM (APA) sm t1 ... APA T 0 tn (APM T ) · a (h) s1 ... sm t1 ... tn Figure 3: Hub and Authority computation values on one row of the transpose of the matrix in Tab. 2a.
The average1 of weighted inlinks is: • MAP for each node corresponding to a system s; this corresponds to the average of all the corresponding APA values on one column of the APA T part of the adjacency matrix (see Fig. 1). • AAP for each node corresponding to a topic t; this corresponds to the average of all the corresponding APM values on one column of the APM part of the adjacency matrix (see Fig. 1).
Therefore, weighted inlinks measure either system effectiveness or topic ease; weighted outlinks are not meaningful. We could also apply the PageRank algorithm to the network; the meaning of the PageRank of a node is not quite so obvious as Inlinks and Outlinks, but it also seems a sensible measure of either system effectiveness or topic ease: if a system is effective, it will have several incoming links with high 1 Usually, the sum of the weights on the incoming arcs to each node is used in place of the average; since the graph is complete, it makes no difference. weights (APA); if a topic is easy it will have high weights (APM) on the incoming links too. We will see experimental confirmation in the following.
Let us now turn to more sophisticated indicators.
Kleinberg"s HITS algorithm defines, for a directed graph, two indicators: hubness and authority; we reiterate here some of the basic details of the HITS algorithm in order to emphasize both the nature of our generalization and the interpretation of the HITS concepts in this context. Usually, hubness and authority are defined as h(x) = P x→y a(y) and a(x) = P y→x h(y), and described intuitively as a good hub links many good authorities; a good authority is linked from many good hubs. As it is well known, an equivalent formulation in linear algebra terms is (see also Fig. 3): h = Aa and a = AT h (7) (where h is the hubness vector, with the hub values for all the nodes; a is the authority vector; A is the adjacency matrix of the graph; and AT its transpose). Usually, A contains 0s and 1s only, corresponding to presence and absence of unweighted directed arcs, but Eq. (7) can be immediately generalized to (in fact, it is already valid for) A containing any real value, i.e., to weighted graphs.
Therefore we can have a generalized version (or rather a generalized interpretation, since the formulation is still the original one) of hubness and authority for all nodes in a graph. An intuitive formulation of this generalized HITS is still available, although slightly more complex: a good hub links, by means of arcs having high weights, many good authorities; a good authority is linked, by means of arcs having high weights, from many good hubs. Since arc weights can be, in general, negative, hub and authority values can be negative, and one could speak of unhubness and unauthority; the intuitive formulation could be completed by adding that a good hub links good unauthorities by means of links with highly negative weights; a good authority is linked by good unhubs by means of links with highly negative weights.
And, also, a good unhub links positively good unauthorities and negatively good authorities; a good unauthority is linked positively from good unhubs and negatively from good hubs.
Let us now apply generalized HITS to our Systems-Topics graph. We compute a(s), h(s), a(t), and h(t). Intuitively, we expect that a(s) is somehow similar to Inlinks, so it should be a measure of either systems effectiveness or topic ease. Similarly, hubness should be more similar to Outlinks, thus less meaningful, although the interplay between hub and authority might lead to the discovery of something different. Let us start by remarking that authority of topics and hubness of systems depend only on each other; similarly hubness of topics and authority of systems depend only on each other: see Figs. 2c, 2d and 3.
Thus the two graphs in Figs. 2c and 2d can be analyzed independently. In fact the entire HITS analysis could be done in one direction only, with just APM(s, t) values or alternatively with just APA(s, t). As discussed below, probably most interest resides in the hubness of topics and the authority of systems, so the latter makes sense. However, in this paper, we pursue both analyses together, because the symmetry itself is interesting.
Considering Fig. 2c we can state that: • Authority a(t) of a topic node t increases when: - if h(si) > 0, APM(si, t) increases (or if APM(si, t) > 0, h(si) increases); - if h(si) < 0, APM(si, t) decreases (or if APM(si, t) < 0, h(si) decreases). • Hubness h(s) of a system node s increases when: - if a(tj) > 0, APM(s, tj) increases (or, if APM(s, tj) > 0, a(tj) increases); - if a(tj) < 0, APM(s, tj) decreases (or, if APM(s, tj) < 0, a(tj) decreases).
We can summarize this as: a(t) is high if APM(s, t) is high for those systems with high h(s); h(s) is high if APM(s, t) is high for those topics with high a(t). Intuitively, authority a(t) of a topic measures topic ease; hubness h(s) of a system measures system"s capability to recognize easy topics. A system with high unhubness (negative hubness) would tend to regard easy topics as hard and hard ones as easy.
The situation for Fig. 2d, i.e., for a(s) and h(t), is analogous. Authority a(s) of a system node s measures system effectiveness: it increases with the weight on the arc (i.e.,
APA(s, tj)) and the hubness of the incoming topic nodes tj.
Hubness h(t) of a topic node t measures topic capability to recognize effective systems: if h(t) > 0, it increases further if APA(s, tj) increases; if h(t) < 0, it increases if APA(s, tj) decreases.
Intuitively, we can state that A system has a higher authority if it is more effective on topics with high hubness; and A topic has a higher hubness if it is easier for those systems which are more effective in general. Conversely for system hubness and topic authority: A topic has a higher authority if it is easier on systems with high hubness; and A system has a higher hubness if it is more effective for those topics which are easier in general.
Therefore, for each system we have two indicators: authority (a(s)), measuring system effectiveness, and hubness (h(s)), measuring system capability to estimate topic ease.
And for each topic, we have two indicators: authority (a(t)), measuring topic ease, and hubness (h(t)), measuring topic capability to estimate systems effectiveness. We can define them formally as a(s) = X t h(t) · APA(s, t), h(t) = X s a(s) · APA(s, t), a(t) = X s h(s) · APM(s, t), h(s) = X t a(t) · APM(s, t).
We observe that the hubness of topics may be of particular interest for evaluation studies. It may be that we can evaluate the effectiveness of systems efficiently by using relatively few high-hubness topics.
We now turn to discuss if these indicators are meaningful and useful in practice, and how they correlate with standard measures used in TREC. We have built the Systems-Topics graph for TREC 8 data (featuring 1282 systems - actually, 2 Actually, TREC 8 data features 129 systems; due to some bug in our scripts, we did not include one system (8manexT3D1N0), but the results should not be affected. 0
-1 -0.5 0 0.5 1 NAPM NAPA AP Figure 4: Distributions of AP, APA, and APM values in TREC 8 data MAP In PR H A MAP 1 1.0 1.0 .80 .99 Inlinks 1 1.0 .80 .99 PageRank 1 .80 .99 Hub 1 .87 (a) AAP In PR H A AAP 1 1.0 1.0 .92 1.0 Inlinks 1 1.0 .92 1.0 PageRank 1 .92 1.0 Hub 1 .93 (b) Table 3: Correlations between network analysis measures and MAP (a) and AAP (b) runs - on 50 topics). This section illustrates the results obtained mining these data according to the method presented in previous sections.
Fig. 4 shows the distributions of AP, APA, and APM: whereas AP is very skewed, both APA and APM are much more symmetric (as it should be, since they are constructed by subtracting the mean). Tables 3a and 3b show the Pearson"s correlation values between Inlinks, PageRank, Hub,
Authority and, respectively, MAP or AAP (Outlinks values are not shown since they are always zero, as seen in Sect. 4). As expected, Inlinks and PageRank have a perfect correlation with MAP and AAP. Authority has a very high correlation too with MAP and AAP; Hub assumes slightly lower values.
Let us analyze the correlations more in detail. The correlations chart in Figs. 5a and 5b demonstrate the high correlation between Authority and MAP or AAP. Hubness presents interesting phenomena: both Fig. 5c (correlation with MAP) and Fig. 5d (correlation with AAP) show that correlation is not exact, but neither is it random. This, given the meaning of hubness (capability in estimating topic ease and system effectiveness), means two things: (i) more effective systems are better at estimating topic ease; and (ii) easier topics are better at estimating system effectiveness.
Whereas the first statement is fine (there is nothing against it), the second is a bit worrying. It means that system effectiveness in TREC is affected more by easy topics than by difficult topics, which is rather undesirable for quite obvious reasons: a system capable of performing well on a difficult topic, i.e., on a topic on which the other systems perform badly, would be an important result for IR effectiveness; con-8E-5 -6E-5 -4E-5 -2E-5 0E+0 2E-5 4E-5 6E-5
-3E-1 -2E-1 -1E-1 0E+0 1E-1 2E-1 3E-1 4E-1 5E-1
(a) (b) 0E+0 2E-2 4E-2 6E-2 8E-2 1E-1 1E-1 1E-1
0E+0 1E-5 2E-5 3E-5 4E-5 5E-5 6E-5 7E-5
(c) (d) Figure 5: Correlations: MAP (x axis) and authority (y axis) of systems (a); AAP and authority of topics (b); MAP and hub of systems (c) and AAP and hub of topics (d) versely, a system capable of performing well on easy topics is just a confirmation of the state of the art. Indeed, the correlation between hubness and AAP (statement (i) above) is higher than the correlation between hubness and MAP (corresponding to statement (ii)): 0.92 vs. 0.80. However, this phenomenon is quite strong. This is also confirmed by the work being done on the TREC Robust Track [14].
In this respect, it is interesting to see what happens if we use a different measure from MAP (and AAP). The GMAP (Geometric MAP) metric is defined as the geometric mean of AP values, or equivalently as the arithmetic mean of the logarithms of AP values [8]. GMAP has the property of giving more weight to the low end of the AP scale (i.e., to low AP values), and this seems reasonable, since, intuitively, a performance increase in MAP values from 0.01 to 0.02 should be more important than an increase from 0.81 to 0.82. To use GMAP in place of MAP and AAP, we only need to take the logarithms of initial AP values, i.e., those in Tab. 1a (zero values are modified into ε = 0.00001). We then repeat the same normalization process (with GMAP and GAAP - Geometric AAP - replacing MAP and AAP): whereas authority values still perfectly correlate with GMAP (0.99) and GAAP (1.00), the correlation with hubness largely disappears (values are −0.16 and −0.09 - slightly negative but not enough to concern us).
This is yet another confirmation that TREC effectiveness as measured by MAP depends mainly on easy topics; GMAP appears to be a more balanced measure. Note that, perhaps surprisingly, GMAP is indeed fairly well balanced, not biased in the opposite direction - that is, it does not overemphasize the difficult topics.
In Sect. 6.3 below, we discuss another transformation, replacing the log function used in GMAP with logit. This has a similar effect: the correlation of mean logitAP and average logitAP with hubness are now small positive numbers (0.23 and 0.15 respectively), still comfortably away from the high correlations with regular MAP and AAP, i.e., not presenting the problematic phenomenon (ii) above (over-dependency on easy topics).
We also observe that hub values are positive, whereas authority assumes, as predicted, both positive and negative values. An intuitive justification is that negative hubness would indicate a node which disagrees with the other nodes, e.g., a system which does better on difficult topics, or a topic on which bad systems do better; such systems and topics would be quite strange, and probably do not appear in TREC. Finally, although one might think that topics with several relevant documents are more important and difficult, this is not the case: there is no correlation between hub (or any other indicator) and the number of documents relevant to a topic.
There has been considerable interest in recent years in questions of statistical significance of effectiveness comparisons between systems (e.g. [2, 9]), and related questions of how many topics might be needed to establish differences (e.g. [13]). We regard some results of the present study as in some way complementary to this work, in that we make a step towards answering the question Which topics are best for establishing differences?.
The results on evaluation without relevance judgements such as [10] show that, to some extent, good systems agree on which are the good documents. We have not addressed the question of individual documents in the present analysis, but this effect is certainly analogous to our results.
At this point it is also worthwhile to analyze what would happen without the MAP- and AAP-normalizations defined in Sect. 3.3. Indeed, the process of graph construction (Sect. 3.4) is still valid: both the APM and APA matrices are replaced by the AP one, and then everything goes on as above. Therefore, one might think that the normalizations are unuseful in this setting.
This is not the case. From the theoretical point of view, the AP-only graph does not present the interesting properties above discussed: since the AP-only graph is symmetrical (the weight on each incoming link is equal to the weight on the corresponding outgoing link), Inlinks and Outlinks assume the same values. There is symmetry also in computing hub and authority, that assume the same value for each node since the weights on the incoming and outgoing arcs are the same. This could be stated in more precise and formal terms, but one might still wonder if on the overall graph there are some sort of counterbalancing effects. It is therefore easier to look at experimental data, which confirm that the normalizations are needed: the correlations between AP,
Inlinks, Outlinks, Hub, and/or Authority are all very close to one (none of them is below 0.98).
It might be argued that (in the case of APA, for example) the amount we have subtracted from each AP value is topicdependent, therefore the range of the resulting APA value is also topic-dependent (e.g. the maximum is 1 − AAP(tj) and the minimum is − AAP(tj)). This suggests that the cross-topic comparisons of these values suggested in Sect. 3.3 may not be reliable. A similar issue arises for APM and comparisons across systems.
One possible way to overcome this would be to use an unconstrained measure whose range is the full real line. Note that in applying the method to GMAP by using log AP, we avoid the problem with the lower limit but retain it for the upper limit. One way to achieve an unconstrainted range would be to use the logit function rather than the log [4,8].
We have also run this variant (as already reported in Sect. 5 above), and it appears to provide very similar results to the GMAP results already given. This is not surprising, since in practice the two functions are very similar over most of the operative range. The normalizations thus seem reliable.
and AT A It is well known that h and a vectors are the principal left eigenvectors of AAT and AT A, respectively (this can be easily derived from Eqs. (7)), and that, in the case of citation graphs, AAT and AT A represent, respectively, bibliographic coupling and co-citations. What is the meaning, if any, of AAT and AT A in our Systems-Topics graph? It is easy to derive that: AAT [i, j] = 8 >< >: 0  if i ∈ S ∧ j ∈ T or i ∈ T ∧ j ∈ S P k A[i, k] · A[j, k] otherwise AT A[i, j] = 8 >< >: 0  if i ∈ S ∧ j ∈ T or i ∈ T ∧ j ∈ S P k A[k, i] · A[k, j] otherwise (where S is the set of indices corresponding to systems and T the set of indices corresponding to topics). Thus AAT and AT A are block diagonal matrices, with two blocks each, one relative to systems and one relative to topics: (a) if i, j ∈ S, then AAT [i, j] = P k∈T APM(i, k)·APM(j, k) measures how much the two systems i and j agree in estimating topics ease (APM): high values mean that the two systems agree on topics ease. (b) if i, j ∈ T, then AAT [i, j] = P k∈S APA(k, i)·APA(k, j) measures how much the two topics i and j agree in estimating systems effectiveness (APA): high values mean that the two topics agree on systems effectiveness (and that TREC results would not change by leaving out one of the two topics). (c) if i, j ∈ S, then AT A[i, j] = P k∈T APA(i, k) · APA(j, k) measures how much agreement on the effectiveness of two systems i and j there is over all topics: high values mean that many topics quite agree on the two systems effectiveness; low values single out systems that are somehow controversial, and that need several topics to have a correct effectiveness assessment. (d) if i, j ∈ T, then AT A[i, j] = P k∈S APM(k, i)·APM(k, j) measures how much agreement on the ease of the two topics i and j there is over all systems: high values mean that many systems quite agree on the two topics ease.
Therefore, these matrices are meaningful and somehow interesting. For instance, the submatrix (b) corresponds to a weighted undirected complete graph, whose nodes are the topics and whose arc weights are a measure of how much two topics agree on systems effectiveness. Two topics that are very close on this graph give the same information, and therefore one of them could be discarded without changes in TREC results. It would be interesting to cluster the topics on this graph. Furthermore, the matrix/graph (a) could be useful in TREC pool formation: systems that do not agree on topic ease would probably find different relevant documents, and should therefore be complementary in pool formation. Note that no notion of single documents is involved in the above analysis.
As indicated, the primary contribution of this paper has been a method of analysis. However, in the course of applying this method to one set of TREC results, we have achieved some insights relating to the hypotheses formulated in Sect. 2: • We confirm Hypothesis 2 above, that some topics are easier than others. • Differences in the hubness of systems reveal that some systems are better than others at distinguishing easy and difficult topics; thus we have some confirmation of Hypothesis 3. • There are some relatively idiosyncratic systems which do badly on some topics generally considered easy but well on some hard topics. However, on the whole, the more effective systems are better at distinguishing easy and difficult topics. This is to be expected: a really bad system will do badly on everything, while even a good system may have difficulty with some topics. • Differences in the hubness of topics reveal that some topics are better than others at distinguising more or less effective systems; thus we have some confirmation of Hypothesis 4. • If we use MAP as the measure of effectiveness, it is also true that the easiest topics are better at distinguishing more or less effective systems. As argued in Sect. 5, this is an undesirable property. GMAP is more balanced.
Clearly these ideas need to be tested on other data sets.
However, they reveal that the method of analysis proposed in this paper can provide valuable information.
The confirmation of Hypothesis 4 leads, as indicated, to the idea that we could do reliable system evaluation on a much smaller set of topics, provided we could select such an appropriate set. This selection may not be straightforward, however. It is possible that simply selecting the high hubness topics will achieve this end; however, it is also possible that there are significant interactions between topics which would render such a simple rule ineffective. This investigation would therefore require serious experimentation. For this reason we have not attempted in this paper to point to the specific high hubness topics as being good for evaluation.
This is left for future work.
DEVELOPMENTS The contribution of this paper is threefold: • we propose a novel way of normalizing AP values; • we propose a novel method to analyse TREC data; • the method applied on TREC data does indeed reveal some hidden properties.
More particularly, we propose Average Average Precision (AAP), a measure of topic ease, and a novel way of normalizing the average precision measure in TREC, on the basis of both MAP (Mean Average Precision) and AAP. The normalized measures (APM and APA) are used to build a bipartite weighted Systems-Topics graph, that is then analyzed by means of network analysis indicators widely known in the (social) network analysis field, but somewhat generalised. We note that no such approach to TREC data analysis has been proposed so far. The analysis shows that, with current measures, a system that wants to be effective in TREC needs to be effective on easy topics. Also, it is suggested that a cluster analysis on topic similarity can lead to relying on a lower number of topics.
Our method of analysis, as described in this paper, can be applied only a posteriori, i.e., once we have all the topics and all the systems available. Adding (removing) a new system / topic would mean re-computing hubness and authority indicators. Moreover, we are not explicitly proposing a change to current TREC methodology, although this could be a by-product of these - and further - analyses.
This is an initial work, and further analyses could be performed. For instance, other effectiveness metrics could be used, in place of AP. Other centrality indicators, widely used in social network analysis, could be computed, although probably with similar results to PageRank. It would be interesting to compute the higher-order eigenvectors of AT A and AAT . The same kind of analysis could be performed at the document level, measuring document ease. Hopefully, further analyses of the graph defined in this paper, according to the approach described, can be insightful for a better understanding of TREC or similar data.
Acknowledgments We would like to thank Nick Craswell for insightful discussions and the anonymous referees for useful remarks. Part of this research has been carried on while the first author was visiting Microsoft Research Cambridge, whose financial support is acknowledged.
[1] M. Agosti, M. Bacchin, N. Ferro, and M. Melucci.
Improving the automatic retrieval of text documents.
In Proceedings of the 3rd CLEF Workshop, volume
[2] C. Buckley and E. Voorhees. Evaluating evaluation measure stability. In 23rd SIGIR, pages 33-40, 2000. [3] S. Chakrabarti. Mining the Web. Morgan Kaufmann,
[4] G. V. Cormack and T. R. Lynam. Statistical precision of information retrieval evaluation. In 29th SIGIR, pages 533-540, 2006. [5] J. Kleinberg. Authoritative sources in a hyperlinked environment. J. of the ACM, 46(5):604-632, 1999. [6] M. Levene. An Introduction to Search Engines and Web Navigation. Addison Wesley, 2006. [7] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank Citation Ranking: Bringing Order to the Web, 1998. http://dbpubs.stanford.edu:8090/pub/1999-66. [8] S. Robertson. On GMAP - and other transformations.
In 13th CIKM, pages 78-83, 2006. [9] M. Sanderson and J. Zobel. Information retrieval system evaluation: effort, sensitivity, and reliability. In 28th SIGIR, pages 162-169, 2005. http://doi.acm.org/10.1145/1076034.1076064. [10] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In 24th SIGIR, pages 66-73, 2001. [11] TREC Common Evaluation Measures, 2005. http://trec.nist.gov/pubs/trec14/appendices/ CE.MEASURES05.pdf (Last visit: Jan. 2007). [12] Text REtrieval Conference (TREC). http://trec.nist.gov/ (Last visit: Jan. 2007). [13] E. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In 25th SIGIR, pages 316-323, 2002. [14] E. M. Voorhees. Overview of the TREC 2005 Robust Retrieval Track. In TREC 2005 Proceedings, 2005. [15] E. M. Voorhees and D. K. Harman.
TRECExperiment and Evaluation in Information Retrieval.
MIT Press, 2005. [16] S. Wasserman and K. Faust. Social Network Analysis.

With the advance of the World Wide Web, more and more hypertext documents become available on the Web. Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs). Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a student"s homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on. Performing information management tasks on such structured data raises many new research challenges.
In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.
For the classification problem of web pages, a simple approach is to treat web pages as independent documents. The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem. However, this approach relies only on the content of web pages and ignores the structure of links among them. Link structures provide invaluable information about properties of the documents as well as relationships among them. For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects). Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more. From this point of view, the traditional classification methods that ignore the link structure may not be suitable.
On the other hand, a few studies, for example [25], rely solely on link structures. It is however a very rare case that content information can be ignorable. For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.
To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration. To achieve this goal, a simple approach is to convert one type of information to the other. For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog. In document classification,
Kurland and Lee [14] convert content similarity among documents into weights of links. However, link and content information have different properties. For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way. Therefore, directly converting one type of information to the other usually degrades the quality of information. On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them. We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.
Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure. In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.
In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively. But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18]. In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors. Our model contains two components. The first component captures the content information. This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval. That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors. The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members. A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students). It is worth noting that we do not explicitly define the semantic of a factor a priori. Instead, similar to LSI, the factors are learned from the data. Traditional factor analysis models the variables associated with entities through the factors. However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs. Therefore, the model should involve factors of both vertices of the edge. This is a key difference between traditional factor analysis and our model. In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links). By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.
In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links. Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.
This paper is organized as follows. Section 2 reviews related work. Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.
Section 4 extends the basic framework and a few variants for fine tune.
Section 5 shows the experiment results. Section 6 discusses the details of this approach and Section 7 concludes.
In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8]. LSI maps documents into a lower dimensional latent space. The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space. The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space. Analysis tasks, such as classification, could be performed on the latent space. The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents. Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.
In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities. Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs. Instead of using two categories,
PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.
He et al. [9] propose a clustering algorithm for web document clustering. The algorithm incorporates link structure and the co-citation patterns. In the algorithm, all links are treated as undirected edge of the link graph. The content information is only used for weighing the links by the textual similarity of both ends of the links. Zhang et al. [23] uses the undirected graph regularization framework for document classification. Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.
Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning. The framework combines the hub and authority information of web pages. But it is difficult to combine the content information into that framework. Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.
Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5]. The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis. In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page. In the model, the outgoing links of the destination web page have no effect on the source web page. In other words, the overall link structure is not utilized in PHITS. In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself. The factor of the destination web page contains information of its outgoing links. In turn, such information is passed to the factor of the source web page. As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.
Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships. The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships. RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques. However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions. On the other hand, the inference for RMNs is intractable and requires belief propagation.
The following are some work on combining documents and links, but the methods are loosely related to our approach. The experiments of [21] show that using terms from the linked document improves the classification accuracy. Chakrabarti et al.[3] use co-citation information in their classification model. Joachims et al.[11] combine text kernels and co-citation kernels for classification. Oh et al [16] use the Naive Bayesian frame to combine link information with content.
In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis. Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages.
Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages. Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper. For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise. Note that A is an asymmetric matrix, because hyperlinks are directed.
Most machine learning algorithms assume a feature-vector representation of instances. For web page classification, however, the link graph does not readily give such a vector representation for web pages. If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages. On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .
The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page. The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.
One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A. The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm. The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U. In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features. A link generation model proposed in [2] is similar to the PCA approach. Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar. However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I. The solution Z is identical subject to a scaling factor.
However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization. The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages). The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects. To see the drawback, let"s consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.
Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj. This is obviously a miss interpretation to the original link path.
To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix. Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.
Again, each row vector of Z corresponds to a feature vector of a web pages. The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z. The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space. We call the Rl space the factor space. Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}. The factor loadings, U, explain how these observed connections happened based on {zi}. Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.
Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1. Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2. In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix. A good low-rank representation should reveal the structure of the factor graph.
First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4
.7 .7 0 0 0 .7 .7 0 0 0
3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4
.5 −.5 0 0 0 .5 −.5 0 0 0
.7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes. The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes. It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.
Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.
Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4
3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4
3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U. When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.
Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A.
Now let us consider the content information on the vertices. To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links. Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords. Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V . Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness.
There are many ways to employ both the content and link information for web page classification. Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation. To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization. The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z. The relationship among these matrices can be depicted as Figure 3.
A Y C U Z V Figure 3: Relationship among the matrices. Node Y is the target of classification.
Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods. Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U =  Z ZUZ Z − Z AZ  + γU, ∂J ∂V =α  V Z Z − C Z  + βV, ∂J ∂Z =  ZU Z ZU + ZUZ ZU − A ZU − AZU  + α  ZV V − CV  .
Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A. Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ). Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).
The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n). Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n.
FACTORIZATION Consider a web page classification problem. We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification. However, this approach does not take data labels into account in the first step. Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22]. Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.
Let C be the set of classes. For simplicity, we first consider binary class problem, i.e. C = {−1, 1}. Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise. We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate. Here, w is the norm of the decision boundary. Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss,
X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0. However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem. To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >:
1 4 (x − 2)2 when 0 < x < 2.
We reduce a multiclass problem into multiple binary ones. One simple scheme of reduction is the one-against-rest coding scheme.
In the one-against-rest scheme, we assign a label vector for each class label. The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled. Let Y be the label matrix, each column of which is a label vector. Therefore,
Y is a matrix of n × c, where c is the number of classes, |C|. Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes,
LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.
To derive a robust solution, we also use Tikhonov regularization for W,
ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.
Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).
We can also use gradient methods to solve the problem of Eq. (8).
The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >:
−1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.
Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results.
In this section, we perform classification on two datasets, to demonstrate the our approach. The two datasets are the WebKB data set[1] and the Cora data set [15]. The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin). The web pages are classified into seven categories. The numbers of pages in each category are shown in Table 1. The Cora data set consists of the abstracts and references of about 34,000 computer science research papers. We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL). We remove those articles without reference to other articles in the set.
The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora
The task of the experiments is to classify the data based on their content information and/or link structure. We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.
PLSI+PHITS link-content MF link-content sup. MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup. MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents. The features are the bag-ofwords and all word are stemmed. This method ignores link structure in the data. Linear SVM is used. The regularization parameter of SVM is selected using the cross-validation method. The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei. We apply SVM on link features. This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods. We use different weights for these two set of features. The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24]. This method is solely based on link structure. • PLSI+PHITS This method is described in [6]. This method combines text content information and link structure for analysis. The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint. It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3. We use 50 latent factors for Z.
After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup. MF This method is our approach of the supervised matrix factorization in Section 4. We use 50 latent factors for Z. After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.
We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training. During the training process, we use the crossvalidation to select all model parameters. We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set. The results are shown as the average classification accuracies and it standard deviation over the five repeats.
The average classification accuracies for the WebKB data set are shown in Table 3. For this task, the accuracies of SVM on links are worse than that of SVM on content. But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy. This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information. The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information. Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches. The difference between the results of our two methods is not significant. However in the experiments below, we show the difference between them.
The classification accuracies for the Cora data set are shown in Table 4. In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links. This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.
PLSI+PHITS link-content MF link-content sup. MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields. The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task. Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly.
As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors. We perform experiments to study how the number of factors affects the accuracy of predication. We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set. The result shown in Figure 4(a) and 4(b). The figures show that the accuracy 88 89 90 91 92 93 94 95
accuracy(%) number of factors link-content sup. MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80
accuracy(%) number of factors link-content sup. MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases. It is a different concept from choosing the optimal number of clusters in clustering application. It is how much information to represent in the latent variables. We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors. To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.
The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.
This indicates that the usefulness of supervised matrix factorization at lower number of factors.
The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience. Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers. Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g. LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .
In our paper, we mainly discuss the application of classification.
A entry of matrix Z means the relationship of a web page and a factor. The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics. Therefore, we allow the components take any possible real values. When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters. Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages. As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want. Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering.
In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application. We propose a simple approach using factors to model the text content and link structure of web pages/documents.
The directed links are generated from the linear combination of linkage of between source and destination factors. By sharing factors between text content and link structure, it is easy to combine both the content information and link structure. Our experiments show our approach is effective for classification. We also discuss an extension for clustering application.
Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm. Also, thanks to the reviewers for constructive comments.
[1] CMU world wide knowledge base (WebKB) project.
Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry. Web search via hub synthesis. In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk. Enhanced hypertext categorization using hyperlinks. In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998. ACM Press, New York, US. [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang. Learning to probabilistically identify authoritative documents. Proc. ICML 2000. pp.167-174.,
[6] D. Cohn and T. Hofmann. The missing link - a probabilistic model of document content and hypertext connectivity. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436. MIT Press, 2001. [7] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X. He, H. Zha, C. Ding, and H. Simon. Web document clustering using hyperlink structures. Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite kernels for hypertext categorisation. In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001. Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi. SVMs for the Blogosphere: Blog Identification and Splog Detection. In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee. Pagerank without hyperlinks: structural re-ranking using links induced by language models. In SIGIR "05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.
Automating the contruction of internet portals with machine learning. Information Retrieval Journal, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng, and M.-H. Lee. A practical hypertext catergorization method using links and incrementally available class information. In SIGIR "00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd. PageRank citation ranking: bring order to the web. Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman. General Intelligence, objectively determined and measured. The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix factorization. In SIGIR "03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani. A study of approaches to hypertext categorization. Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp. Multi-label informed latent semantic indexing. In SIGIR "05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265,
New York, NY, USA, 2005. ACM Press. [23] T. Zhang, A. Popescul, and B. Dom. Linear prediction models with graph regularization for web-page categorization. In KDD "06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA,
[24] D. Zhou, J. Huang, and B. Sch¨olkopf. Learning from labeled and unlabeled data on a directed graph. In Proceedings of the 22nd International Conference on Machine Learning, Bonn,

In this work we address time-travel text search over temporally versioned document collections. Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t.
An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds. Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents. Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.
Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.
For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians. Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives. If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalist"s information need.
Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered. Looking at their evolutionary history, we are faced with even larger data volumes. As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.
This paper presents an efficient solution to time-travel text search by making the following key contributions:
transparently extended to enable time-travel text search.
indexsize explosion while keeping results highly accurate.
improve index performance that allow trading off space vs. performance.
approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.
The remainder of this paper is organized as follows. The presented work is put in context with related work in Section 2. We delineate our model of a temporally versioned document collection in Section 3. We present our time-travel inverted index in Section 4. Building on it, temporal coalescing is described in Section 5. In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7.
We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index. We briefly review work under these categories here.
To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents. Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.
Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.
Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents. Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results. Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives. This adaptation, however, does not provide the intended time-travel text search functionality. In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28]. Unlike the inverted file index, their applicability to text search is not well understood.
Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size. Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context. More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size. None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality. Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result. They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction. It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here.
In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following. Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .
Each version dti has an associated timestamp ti reflecting when the version was created. Each version is a vector of searchable terms or features. Any modification to a document version results in the insertion of a new version with corresponding timestamp. We employ a discrete definition of time, so that timestamps are non-negative integers. The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥. The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).
Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .
As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.
As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well. For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .
In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.
We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered. The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .
It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti. The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively. The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version.
The inverted file index is a standard technique for text indexing, deployed in many systems. In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search.
An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list. The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload. The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.
The sort-order of index lists depends on which queries are to be supported efficiently. For Boolean queries it is favorable to sort index lists in document-order.
Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31]. A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists. For an excellent recent survey about inverted file indexes we refer to [35].
In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information. The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid. The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.
As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .
Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree. Unlike the tf-score, the idf-score of every term could vary with every change in the corpus. Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times.
During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary. Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings. We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing. To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)). Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost. As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.
We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists. As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable.
If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version. For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.
The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size. It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched. As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all. Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded. This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.
Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example. The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.
We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions. Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.
As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .
Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately. We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.
Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold . In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .
In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .
Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee. Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20]. Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence. In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.
Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time. Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].
The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work. As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21]. This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.
Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . . O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.
While doing so, it coalesces sequences of postings having maximal length. The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details). When reading the next posting, the algorithm tries to add it to the current sequence of postings. It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee. If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized. The time complexity of the algorithm is in O(n).
Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document.
Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings. Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains. In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index. Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist. Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists. Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document
Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t.
We illustrate the idea of sublist materialization using an example shown in Figure 2. The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3. For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10. Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list. Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case. Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively. Then, we can process the above query with optimal cost by reading only those postings that existed at this t.
At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section. However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone. The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries. Further, the two techniques, can be applied separately and are independent. If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.
We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as,
Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .
To aid the presentation in the rest of the paper, we first provide some definitions. Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv. Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals. We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed. We also assume that intervals in M are disjoint. We can make this assumption without ruling out any optimal solution with regard to space or performance defined below. The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1). The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ). Thus, in order to optimize the performance of processing queries we minimize their processing costs.
One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved. Therefore, we will refer to this approach as Popt in the remainder. The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder. This approach requires minimal space, since it keeps each posting exactly once.
Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance. The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach.
The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists. In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting. If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3). The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained. In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1. Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .
An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .
Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.
Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].
The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed. The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems.
So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space. In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit. The technique presented next, which is named SB, tackles this very problem. The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt. The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance). In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).
Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.
X m∈M |Lv : m| ≤ κ |Lv| .
The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization. A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5]. Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.
We obtain an approximate solution to the problem using simulated annealing [22, 23]. Simulated annealing takes a fixed number R of rounds to explore the solution space.
In each round a random successor of the current solution is looked at. If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).
A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution. If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds. In addition, throughout all rounds, the method keeps track of the best solution seen so far. The solution space for the problem at hand can be efficiently explored. As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals. We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set. Note that b1 and bn are always set to true. Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }. A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables. The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round. Its space complexity is in O(n) - for keeping the n boolean variables.
As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance.
We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper.
The techniques described in this paper were implemented in a prototype system using Java JDK 1.5. All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003. All data and indexes are kept in an Oracle 10g database that runs on the same machine. For our experiments we used two different datasets.
The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file. This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download). We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.). This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18. We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.). The thus extracted queries contained a total of 422 distinct terms. For each extracted query, we randomly picked a time point for each month covered by the dataset. This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.
The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and
out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper. This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79). We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc.), and randomly sampling a time point for every month within the two year period spanned by the dataset.
Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset. In total 522 terms appear in the extracted queries.
The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets.
Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality. For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.
WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00%
Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings. As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size. Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude. Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size. Index size continues to reduce on both datasets, as we increase the value of .
How does the reduction in index size affect the query results? In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively. We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendall"s τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value
Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01. Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.
It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.
For = 0.01, the smallest value of in our experiments,
RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0
1 ε
Relative Recall @ 10 (WIKI) Kendall's τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendall's τ @ 10 (UKGOV) (a) @10 -1 -0.5 0
1 ε
Relative Recall @ 100 (WIKI) Kendall's τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendall's τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendall"s τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index. Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.
For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively. On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values.
We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6. For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10. In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations. However, note that the postings in the materialized sublists still retain their original timestamps. For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows. The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists. To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points. We report the mean EPC, as well as the 5%- and 95%-percentile. In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.
The Sopt and Popt approaches are, by their definition, parameter-free. For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0. Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0. Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.
Table 2 lists the obtained space and performance figures.
Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus. Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption. Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost. The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.
We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12.
In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections. Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.
The present work opens up many interesting questions for future research, e.g.: How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?. How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point? How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)?
We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2.
[1] V. N. Anh and A. Moffat. Pruned Query Evaluation Using Pre-Computed Impacts. In SIGIR, 2006. [2] V. N. Anh and A. Moffat. Pruning Strategies for Mixed-Mode Querying. In CIKM, 2006.
WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn. Versioning a Full-Text Information Retrieval System. In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum. A Time Machine for Text search.
Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.
Coalescing in Temporal Databases. In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna. Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations. In WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici,
R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.
Indexing Shared Content in Information Retrieval Systems. In EDBT, 2006. [9] C. Buckley and A. F. Lewit. Optimization of Inverted Vector Searches. In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen. Method and Apparatus for Generating and Searching Range-Based Index of Word Locations. U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke. A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems. In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi,
M. Herscovici, Y. S. Maarek, and A. Soffer. Static Index Pruning for Information Retrieval Systems. In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing Top k Lists. SIAM J. Discrete Math., 17(1):134-160,
[15] R. Fagin, A. Lotem, and M. Naor. Optimal Aggregation Algorithms for Middleware. J. Comput.
Syst. Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J. Woo. REHIST: Relative Error Histogram Construction Algorithms. In VLDB,
[17] M. Hersovici, R. Lempel, and S. Yogev. Efficient Indexing of Versioned Document Sequences. In ECIR,
[18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala. Balancing Histogram Optimality and Practicality for Query Result Size Estimation. In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan,
V. Poosala, K. C. Sevcik, and T. Suel. Optimal Histograms with Quality Guarantees. In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An Online Algorithm for Segmenting Time Series. In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.
Optimization by Simulated Annealing. Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos. Algorithm Design.
Addison-Wesley, 2005. [24] U. Manber. Introduction to Algorithms: A Creative Approach. Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø. DyST: Dynamic and Scalable Temporal Text Indexing. In TIME, 2006. [26] J. M. Ponte and W. B. Croft. A Language Modeling Approach to Information Retrieval. In SIGIR, 1998. [27] S. E. Robertson and S. Walker. Okapi/Keenbow at TREC-8. In TREC, 1999. [28] B. Salzberg and V. J. Tsotras. Comparison of Access Methods for Time-Evolving Data. ACM Comput.
Surv., 31(2):158-221, 1999. [29] M. Stack. Full Text Search of Web Archive Collections. In IWAW, 2006. [30] E. Terzi and P. Tsaparas. Efficient Algorithms for Sequence Segmentation. In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel. Top-k Query Evaluation with Probabilistic Guarantees. In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel. Efficient Search in Large Textual Collections with Redundancy. In WWW,

Query performance prediction has many applications in a variety of information retrieval (IR) areas such as improving retrieval consistency, query refinement, and distributed IR. The importance of this problem has been recognized by IR researchers and a number of new methods have been proposed for prediction recently [1, 2, 17].
Most work on prediction has focused on the traditional ad-hoc retrieval task where query performance is measured according to topical relevance. These prediction models are evaluated on TREC document collections which typically consist of no more than one million relatively homogenous newswire articles. With the popularity and influence of the Web, prediction techniques that will work well for web-style queries are highly preferable.
However, web search environments pose significant challenges to current prediction models that are mainly designed for traditional TREC settings. Here we outline some of these challenges.
First, web collections, which are much larger than conventional TREC collections, include a variety of documents that are different in many aspects such as quality and style. Current prediction techniques can be vulnerable to these characteristics of web collections. For example, the reported prediction accuracy of the ranking robustness technique and the clarity technique on the GOV2 collection (a large web collection) is significantly worse compared to the other TREC collections [1]. Similar prediction accuracy on the GOV2 collection using another technique is reported in [2], confirming the difficult of predicting query performance on a large web collection.
Furthermore, web search goes beyond the scope of the ad-hoc retrieval task based on topical relevance. For example, the Named-Page (NP) finding task, which is a navigational task, is also popular in web retrieval. Query performance prediction for the NP task is still necessary since NP retrieval performance is far from perfect. In fact, according to the report on the NP task of the
poorly (no correct answer in the first 10 search results) even in the best run from the top group. To our knowledge, little research has explicitly addressed the problem of NP-query performance prediction. Current prediction models devised for content-based queries will be less effective for NP queries considering the fundamental differences between the two.
Third, in real-world web search environments, user queries are usually a mixture of different types and prior knowledge about the type of each query is generally unavailable. The mixed-query situation raises new problems for query performance prediction.
For instance, we may need to incorporate a query classifier into prediction models. Despite these problems, the ability to handle this situation is a crucial step towards turning query performance prediction from an interesting research topic into a practical tool for web retrieval.
In this paper, we present three techniques to address the above challenges that current prediction models face in Web search environments. Our work focuses on query performance prediction for the content-based (ad-hoc) retrieval task and the name-page finding task in the context of web retrieval. Our first technique, called weighted information gain (WIG), makes use of both single term and term proximity features to estimate the quality of top retrieved documents for prediction. We find that WIG offers consistent prediction accuracy across various test collections and query types. Moreover, we demonstrate that good prediction accuracy can be achieved for the mixed-query situation by using WIG with the help of a query type classifier. Query feedback and first rank change, which are our second and third prediction techniques, perform well for content-based queries and NP queries respectively.
Our main contributions include: (1) considerably improved prediction accuracy for web content-based queries over several state-of-the-art techniques. (2) new techniques for successfully predicting NP-query performance. (3) a practical and fully automatic solution to predicting mixed-query performance. In addition, one minor contribution is that we find that the robustness score [1], which was originally proposed for performance prediction, is helpful for query classification.
As we mentioned in the introduction, a number of prediction techniques have been proposed recently that focus on contentbased queries in the topical relevance (ad-hoc) task. We know of no published work that addresses other types of queries such as NP queries, let alone a mixture of query types. Next we review some representative models.
The major difficulty of performance prediction comes from the fact that many factors have an impact on retrieval performance.
Each factor affects performance to a different degree and the overall effect is hard to predict accurately. Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [4] and the average IDF of query terms [5], do not predict well. In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set to estimate topic difficulty. For example, the clarity score [6] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model. The robustness score [1] quantifies another property of a ranked list: the robustness of the ranking in the presence of uncertainty. Carmel et al. [2] found that the distance measured by the Jensen-Shannon divergence between the retrieved document set and the collection is significantly correlated to average precision. Vinay et al.[7] proposed four measures to capture the geometry of the top retrieved documents for prediction. The most effective measure is the sensitivity to document perturbation, an idea somewhat similar to the robustness score. Unfortunately, their way of measuring the sensitivity does not perform equally well for short queries and prediction accuracy drops considerably when a state-of-the-art retrieval technique (like Okapi or a language modeling approach) is adopted for retrieval instead of the tf-idf weighting used in their paper [16].
The difficulties of applying these models in web search environments have already been mentioned. In this paper, we mainly adopt the clarity score and the robustness score as our baselines. We experimentally show that the baselines, even after being carefully tuned, are inadequate for the web environment.
One of our prediction models, WIG, is related to the Markov random field (MRF) model for information retrieval [8]. The MRF model directly models term dependence and is found be to highly effective across a variety of test collections (particularly web collections) and retrieval tasks. This model is used to estimate the joint probability distribution over documents and queries, an important part of WIG. The superiority of WIG over other prediction techniques based on unigram features, which will be demonstrated later in our paper, coincides with that of MRF for retrieval. In other word, it is interesting to note that term dependence, when being modeled appropriately, can be helpful for both improving and predicting retrieval performance.
This section introduces a weighted information gain approach that incorporates both single term and proximity features for predicting performance for both content-based and Named-Page (NP) finding queries.
Given a set of queries Q={Qs} (s=1,2,..N) which includes all possible user queries and a set of documents D={Dt} (t=1,2…M), we assume that each query-document pair (Qs,Dt) is manually judged and will be put in a relevance list if Qs is found to be relevant to Dt. The joint probability P(Qs,Dt) over queries Q and documents D denotes the probability that pair (Qs,Dt) will be in the relevance list. Such assumptions are similar to those used in [8]. Assuming that the user issues query Qi ∈Q and the retrieval results in response to Qi is a ranked list L of documents, we calculate the amount of information contained in P(Qs,Dt) with respect to Qi and L by Eq.1 which is a variant of entropy called the weighted entropy[13]. The weights in Eq.1 are solely determined by Qi and L. )1(),(log),(),( , , ∑−= ts tststsLQ DQPDQweightDQH i In this paper, we choose the weights as follows: LindocumentsKtopthecontainsLTwhere otherwise LTDandisifK DQweight K Kt ts )( )2( ,0 )(,/1 ),( ⎩ ⎨ ⎧ ∈= = The cutoff rank K is a parameter in our model that will be discussed later. Accordingly, Eq.1 can be simplified as follows: )3(),(log 1 ),( )( , ∑∈ −= LTD titsLQ Kt i DQP K DQH Unfortunately, weighted entropy ),(, tsLQ DQH i computed by Eq.3, which represents the amount of information about how likely the top ranked documents in L would be relevant to query Qi on average, cannot be compared across different queries, making it inappropriate for directly predicting query performance. To mitigate this problem, we come up with a background distribution P(Qs,C) over Q and D by imagining that every document in D is replaced by the same special document C which represents average language usage. In this paper, C is created by concatenating every document in D. Roughly speaking, C is the collection (the document set) {Dt} without document boundaries.
Similarly, weighted entropy ),(, CQH sLQi calculated by Eq.3 represents the amount of information about how likely an average document (represented by the whole collection) would be relevant to query Qi.
Now we introduce our performance predictor WIG which is the weighted information gain [13] computed as the difference between ),(, tsLQ DQH i and ),(, CQH sLQi .Specifically, given query Qi, collection C and ranked list L of documents, WIG is calculated as follows: )4( ),( ),( log 1 ),( ),( log),( ),(),(),,( )(, ,, ∑∑ ∈ == −= LTD i ti ts s ts ts tsLQsLQi Kt ii CQP DQP KCQP DQP DQweight DQHCQHLCQWIG WIG computed by Eq.4 measures the change in information about the quality of retrieval (in response to query Qi) from an imaginary state that only an average document is retrieved to a posterior state that the actual search results are observed. We hypothesize that WIG is positively correlated with retrieval effectiveness because high quality retrieval should be much more effective than just returning the average document.
The heart of this technique is how to estimate the joint distribution P(Qs,Dt). In the language modeling approach to IR, a variety of models can be applied readily to estimate this distribution. Although most of these models are based on the bagof-words assumption, recent work on modeling term dependence under the language modeling framework have shown consistent and significant improvements in retrieval effectiveness over bagof-words models. Inspired by the success of incorporating term proximity features into language models, we decide to adopt a good dependence model to estimate the probability P(Qs,Dt). The model we chose for this paper is Metzler and Croft"s Markov Random Field (MRF) model, which has already demonstrated superiority over a number of collections and different retrieval tasks [8,9].
According to the MRF model, log P(Qi, Dt) can be written as )5()|(loglog),(log )(
+−= iQF tti DPZDQP ξ ξ ξλ where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.
F(Qi) consists of a set of features expanded from the original query Qi . For example, assuming that query Qi is talented student program, F(Qi) includes features like program and talented student. We consider two kinds of features: single term features T and proximity features P. Proximity features include exact phrase (#1) and unordered window (#uwN) features as described in [8]. Note that F(Qi) is the union of T(Qi) and P(Qi). For more details on F(Qi) such as how to expand the original query Qi to F(Qi), we refer the reader to [8] and [9].
P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.
More details on P(ξ|Dt) will be provided later in this section. The choice of λξ is somewhat different from that used in [8] since λξ plays a dual role in our model. The first role, which is the same as in [8], is to weight between single term and proximity features.
The other role, which is specific to our prediction task, is to normalize the size of F(Qi).We found that the following weight strategy for λξ satisfies the above two roles and generalizes well on a variety of collections and query types. )6( )(, |)(| 1 )(, |)(| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ − ∈ = i i T i i T QP QP QT QT ξ λ ξ λ λξ where |T(Qi)| and |P(Qi)| denote the number of single term and proximity features in F(Qi) respectively. The reason for choosing the square root function in the denominator of λξ is to penalize a feature set of large size appropriately, making WIG more comparable across queries of various lengths. λT is a fixed parameter and set to 0.8 according to [8] throughout this paper.
Similarly, log P(Qi,C) can be written as: )7()|(loglog),(log )(
+−= iQF i CPZCQP ξ ξ ξλ When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can be rewritten as follows by plugging in Eq.5 and Eq.7 : )8( )|( )|( log 1 ),,( )( )( ∑ ∑∈ ∈ = LTD QF t i Kt i CP DP K LCQWIG ξ ξ ξ ξ λ One of the advantages of WIG over other techniques is that it can handle well both content-based and NP queries. Based on the type (or the predicted type) of Qi, the calculation of WIG in Eq. 8 differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and (2) how to choose K.
For content-based queries, P(ξ|C) is estimated by the relative frequency of feature ξ in collection C as a whole. The estimation of P(ξ|Dt) is the same as in [8]. Namely, we estimate P(ξ|Dt) by the relative frequency of feature ξ in Dt linearly smoothed with collection frequency P(ξ|C). K in Eq.8 is treated as a free parameter. Note that K is the only free parameter in the computation of WIG for content-based queries because all parameters involved in P(ξ|Dt) are assumed to be fixed by taking the suggested values in [8].
Regarding NP queries, we make use of document structure to estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language models proposed in [10] and incorporated into the MRF model for Named-Page finding retrieval in [9]. The basic idea is that a document (collection) is divided into several fields such as the title field, the main-body field and the heading field. P(ξ|Dt) and P(ξ|C) are estimated by a linear combination of the language models from each field. Due to space constraints, we refer the reader to [9] for details. We adopt the exact same set of parameters as used in [9] for estimation. With regard to K in Eq.8, we set K to 1 because the Named-Page finding task heavily focuses on the first ranked document. Consequently, there are no free parameters in the computation of WIG for NP queries.
In this section, we introduce another technique called query feedback (QF) for prediction. Suppose that a user issues query Q to a retrieval system and a ranked list L of documents is returned.
We view the retrieval system as a noisy channel. Specifically, we assume that the output of the channel is L and the input is Q.
After going through the channel, Q becomes corrupted and is transformed to ranked list L.
By thinking about the retrieval process this way, the problem of predicting retrieval effectiveness turns to the task of evaluating the quality of the channel. In other words, prediction becomes finding a way to measure the degree of corruption that arises when Q is transformed to L. As directly computing the degree of the corruption is difficult, we tackle this problem by approximation. Our main idea is that we measure to what extent information on Q can be recovered from L on the assumption that only L is observed. Specifically, we design a decoder that can accurately translate L back into new query Q" and the similarity S between the original query Q and the new query Q" is adopted as a performance predictor. This is a sketch of how the QF technique predicts query performance. Before filling in more details, we briefly discuss why this method would work.
There is a relation between the similarity S defined above and retrieval performance. On the one hand, if the retrieval has strayed from the original sense of the query Q, the new query Q" extracted from ranked list L in response to Q would be very different from the original query Q. On the other hand, a query distilled from a ranked list containing many relevant documents is likely to be similar to the original query. Further examples in support of the relation will be provided later.
Next we detail how to build the decoder and how to measure the similarity S.
In essence, the goal of the decoder is to compress ranked list L into a few informative terms that should represent the content of the top ranked documents in L. Our approach to this goal is to represent ranked list L by a language model (distribution over terms). Then terms are ranked by their contribution to the language model"s KL (Kullback-Leibler) divergence from the background collection model. Top ranked terms will be chosen to form the new query Q". This approach is similar to that used in Section 4.1 of [11].
Specifically, we take three steps to compress ranked list L into query Q" without referring to the original query.
language model based on ranked list L. The model can be written as: )9()|()|()|( ∑∈ = LD LDPDwPLwP where w is any term, D is a document. P(D|L) is estimated by a linearly decreasing function of the rank of document D.
contribution: )10( )|( )|( log)|( CwP LwP LwP where P(w|C) is the collection model estimated by the relative frequency of term w in collection C as a whole.
Q"={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and weight ti is the KL-divergence contribution of wi in Eq. 10.
Term cruise ship vessel sea passenger KL contribution
Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TREC topic 719; average precision: 0.08) and the other for a high performing query prostate cancer treatments( TREC topic 710; average precision:
indicate how the similarity between the original and the new query correlates with retrieval performance. The parameter N in step 3 is set to 20 empirically and choosing a larger value of N is unnecessary since the weights after the top 20 are usually too small to make any difference.
Term prostate cancer treatment men therapy KL contribution
Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q", we first use Q" to do retrieval on the same collection. A variant of the query likelihood model [15] is adopted for retrieval.
Namely, documents are ranked by: )11()|()|'( '),( ∑∈ = Qtw t i ii i DwPDQP where wi is a term in Q" and ti is the associated weight. D is a document.
Let L" denote the new ranked list returned from the above retrieval. The similarity is measured by the overlap of documents in L and L". Specifically, the percentage of documents in the top K documents of L that are also present in the top K documents in L". the cutoff K is treated as a free parameter.
We summarize here how the QF technique predicts performance given a query Q and the associated ranked list L. We first obtain a weighted query Q" compressed from L by the above three steps.
Then we use Q" to perform retrieval and the new ranked list is L".
The overlap of documents in L and L" is used for prediction.
In this section, we propose a method called the first rank change (FRC) for performance prediction for NP queries. This method is derived from the ranking robustness technique [1] that is mainly designed for content-based queries. When directly applied to NP queries, the robustness technique will be less effective because it takes the top ranked documents as a whole into account while NP queries usually have only one single relevant document. Instead, our technique focuses on the first rank document while the main idea of the robustness method remains. Specifically, the pseudocode for computing FRC is shown in figure 1.
Input: (1) ranked list L={Di} where i=1,100. Di denotes the i-th ranked document. (2) query Q
where Di" denotes the perturbed version of Di.
Figure 1: pseudo-code for computing FRC FRC approximates the probability that the first ranked document in the original list L will remain ranked first even after the documents are perturbed. The higher the probability is, the more confidence we have in the first ranked document. On the other hand, in the extreme case of a random ranking, the probability would be as low as 0.5. We expect that FRC has a positive association with NP query performance. We adopt [1] to implement the document perturbation step (step 4 in Fig.1) using Poisson distributions. For more details, we refer the reader to [1].
We now present the results of predicting query performance by our models. Three state-of-the-art techniques are adopted as our baselines. We evaluate our techniques across a variety of Web retrieval settings. As mentioned before, we consider two types of queries, that is, content-based (CB) queries and Named-Page(NP) finding queries.
First, suppose that the query types are known. We investigate the correlation between the predicted retrieval performance and the actual performance for both types of queries separately. Results show that our methods yield considerable improvements over the baselines.
We then consider a more challenging scenario where no prior information on query types is available. Two sub-cases are considered. In the first one, there exists only one type of query but the actual type is unknown. We assume a mixture of the two query types in the second case. We demonstrate that our models achieve good accuracy under this demanding scenario, making prediction practical in a real-world Web search environment.
Our evaluation focuses on the GOV2 collection which contains about 25 million documents crawled from web sites in the .gov domain during 2004 [3]. We create two kinds of data set for CB queries and NP queries respectively. For the CB type, we use the ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and name them TB04-adhoc, TB05-adhoc and TB06-adhoc respectively. In addition, we also use the ad-hoc topics of the
techniques to a non-Web environment. For NP queries, we use the Named-Page finding topics of the Terabyte Tracks of 2005 and
queries used in our experiments are titles of TREC topics as we center on web retrieval. Table 3 summarizes the above data sets.
Name Collection Topic Number Query Type TB04-adhoc GOV2 701-750 CB TB05-adhoc GOV2 751-800 CB TB06-adhoc GOV2 801-850 CB RT04 Disk 4+5 (minus CR)  301-450;601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Table 3: Summary of test collections and topics Retrieval performance of individual content-based and NP queries is measured by the average precision and reciprocal rank of the first correct answer respectively. We make use of the Markov Random field model for both ad-hoc and Named-Page finding retrieval. We adopt the same setting of retrieval parameters used in [8,9]. The Indri search engine [12] is used for all of our experiments. Though not reported here, we also tried the query likelihood model for ad-hoc retrieval and found that the results change little because of the very high correlation between the query performances obtained by the two retrieval models (0.96 measured by Pearson"s coefficient).
Suppose that query types are known. We treat each type of query separately and measure the correlation with average precision (or the reciprocal rank in the case of NP queries). We adopt the Pearson"s correlation test which reflects the degree of linear relationship between the predicted and the actual retrieval performance.
Methods Clarity Robust JSD WIG QF WIG +QF TB04+0
TB06 adhoc
Table 4: Pearson"s correlation coefficients for correlation with average precision on the Terabyte Tracks (ad-hoc) for clarity score, robustness score, the JSD-based method(we directly cites the score reported in [2]), WIG, query feedback(QF) and a linear combination of WIG and QF. Bold cases mean the results are statistically significant at the 0.01 level.
Table 4 shows the correlation with average precision on two data sets: one is a combination of TB04-adhoc and TB05-adhoc(100 topics in total) and the other is TB06-adhoc (50 topics). The reason that we put TB04-adhoc and TB05-adhoc together is to make our results comparable to [2]. Our baselines are the clarity score (clarity) [6],the robustness score (robust)[1] and the JSDbased method (JSD) [2]. For the clarity and robustness score, we have tried different parameter settings and report the highest correlation coefficients we have found. We directly cite the result of the JSD-based method reported in [2]. The table also shows the results for the Weighted Information Gain (WIG) method and the Query Feedback (QF) method for predicting content-based queries. As we described in the previous section, both WIG and QF have one free parameter to set, that is, the cutoff rank K. We train the parameter on one dataset and test on the other. When combining WIG and QF, a simple linear combination is used and the combination weight is learned from the training data set.
From these results, we can see that our methods are considerably more accurate compared to the baselines. We also observe that further improvements are obtained from the combination of WIG and QF, suggesting that they measure different properties of the retrieval process that relate to performance.
We discover that our methods generalize well on TB06-adhoc while the correlation for the clarity score with retrieval performance on this data set is considerably worse. Further investigation shows that the mean average precision of TB06-adhoc is 0.342 and is about 10% better than that of the first data set.
While the other three methods typically consider the top 100 or less documents given a ranked list, the clarity method usually needs the top 500 or more documents to adequately measure the coherence of a ranked list. Higher mean average precision makes ranked lists retrieved by different queries more similar in terms of coherence at the level of top 500 documents. We believe that this is the main reason for the low accuracy of the clarity score on the second data set.
Though this paper focuses on a Web search environment, it is desirable that our techniques will work consistently well in other situations. To this end, we examine the effectiveness of our techniques on the Robust 2004 Track. For our methods, we evenly divide all of the test queries into five groups and perform five-fold cross validation. Each time we use one group for training and the remaining four groups for testing. We make use of all of the queries for our two baselines, that is, the clarity score and the robustness score. The parameters for our baselines are the same as those used in [1].The results shown in Table 5 demonstrate that the prediction accuracy of our methods is on a par with that of the two strong baselines.
Clarity Robust WIG QF
Table 5: Comparison of Pearson"s correlation coefficients on the 2004 Robust Track for clarity score, robustness score,
WIG and query feedback (QF). Bold cases mean the results are statistically significant at the 0.01 level.
Furthermore, we examine the prediction sensitivity of our methods to the cutoff rank K. With respect to WIG, it is quite robust to K on the Terabyte Tracks (2004-2006) while it prefers a small value of K like 5 on the 2004 Robust Track. In other words, a small value of K is a nearly-optimal choice for both kinds of tracks. Considering the fact that all other parameters involved in WIG are fixed and consequently the same for the two cases, this means WIG can achieve nearly-optimal prediction accuracy in two considerably different situations with exactly the same parameter setting. Regarding QF, it prefers a larger value of K such as 100 on the Terabyte Tracks and a smaller value of K such as 25 on the 2004 Robust Track.
We adopt WIG and first rank change (FRC) for predicting NPquery performance. We also try a linear combination of the two as in the previous section. The combination weight is obtained from the other data set. We use the correlation with the reciprocal ranks measured by the Pearson"s correlation test to evaluate prediction quality. The results are presented in Table 6. Again, our baselines are the clarity score and the robustness score.
To make a fair comparison, we tune the clarity score in different ways. We found that using the first ranked document to build the query model yields the best prediction accuracy. We also attempted to utilize document structure by using the mixture of language models mentioned in section 3.1. Little improvement was obtained. The correlation coefficients for the clarity score reported in Table 6 are the best we have found. As we can see, our methods considerably outperform the clarity score technique on both of the runs. This confirms our intuition that the use of a coherence-based measure like the clarity score is inappropriate for NP queries.
Methods Clarity Robust. WIG FRC WIG+FRC TB05-NP 0.150 -0.370 0.458 0.440 0.525 TB06-NP 0.112 -0.160 0.478 0.386 0.515 Table 6: Pearson"s correlation coefficients for correlation with reciprocal ranks on the Terabyte Tracks (NP) for clarity score, robustness score, WIG, the first rank change (FRC) and a linear combination of WIG and FRC. Bold cases mean the results are statistically significant at the 0.01 level.
Regarding the robustness score, we also tune the parameters and report the best we have found. We observe an interesting and surprising negative correlation with reciprocal ranks. We explain this finding briefly. A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents. The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of relevant documents [1]. However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query. The existence of such documents can confuse the ranking function and lead to low retrieval performance. Although the negative correlation with retrieval performance exists, the strength of the correlation is weaker and less consistent compared to our methods as shown in Table 6.
Based on the above analysis, we can see that current prediction techniques like clarity score and robustness score that are mainly designed for content-based queries face significant challenges and are inadequate to deal with NP queries. Our two techniques proposed for NP queries consistently demonstrate good prediction accuracy, displaying initial success in solving the problem of predicting performance for NP queries. Another point we want to stress is that the WIG method works well for both types of queries, a desirable property that most prediction techniques lack.
In this section, we run two kinds of experiments without access to query type labels. First, we assume that only one type of query exists but the type is unknown. Second, we experiment on a mixture of content-based and NP queries. The following two subsections will report results for the two conditions respectively.
We assume that all queries are of the same type, that is, they are either NP queries or content-based queries. We choose WIG to deal with this case because it shows good prediction accuracy for both types of queries in the previous section. We consider two cases: (1) CB: all 150 title queries from the ad-hoc task of the Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the named page finding task of the Terabyte Tracks 2005 and 2006.
We take a simple strategy by labeling all of the queries in each case as the same type (either NP or CB) regardless of their actual type. The computation of WIG will be based on the labeled query type instead of the actual type. There are four possibilities with respect to the relation between the actual type and the labeled type. The correlation with retrieval performance under the four possibilities is presented in Table 7. For example, the value 0.445 at the intersection between the second row and the third column shows the Pearson"s correlation coefficient for correlation with average precision when the content-based queries are incorrectly labeled as the NP type.
Based on these results, we recommend treating all queries as the NP type when only one query type exists and accurate query classification is not feasible, considering the risk that a large loss of accuracy will occur if NP queries are incorrectly labeled as content-based queries. These results also demonstrate the strong adaptability of WIG to different query types.
CB (labeled) NP (labeled) CB (actual) 0.536 0.445 NP (actual) 0.174 0.467 Table 7: Comparison of Pearson"s correlation coefficients for correlation with retrieval performance under four possibilities on the Terabyte Tracks (NP). Bold cases mean the results are statistically significant at the 0.01 level.
A mixture of the two types of queries is a more realistic situation that a Web search engine will meet. We evaluate prediction accuracy by how accurately poorly-performing queries can be identified by the prediction method assuming that actual query types are unknown (but we can predict query types). This is a challenging task because both the predicted and actual performance for one type of query can be incomparable to that for the other type.
Next we discuss how to implement our evaluation. We create a query pool which consists of all of the 150 ad-hoc title queries from Terabyte Track 2004-2006 and all of the 433 NP queries from Terabyte Track 2005&2006. We divide the queries in the pool into classes: good (better than 50% of the queries of the same type in terms of retrieval performance) and bad (otherwise). According to these standards, a NP query with the reciprocal rank above 0.2 or a content-based query with the average precision above 0.315 will be considered as good.
Then, each time we randomly select one query Q from the pool with probability p that Q is contented-based. The remaining queries are used as training data. We first decide the type of query Q according to a query classifier. Namely, the query classifier tells us whether query Q is NP or content-based. Based on the predicted query type and the score computed for query Q by a prediction technique, a binary decision is made about whether query Q is good or bad by comparing to the score threshold of the predicted query type obtained from the training data. Prediction accuracy is measured by the accuracy of the binary decision. In our implementation, we repeatedly take a test query from the query pool and prediction accuracy is computed as the percentage of correct decisions, that is, a good(bad) query is predicted to be good (bad). It is obvious that random guessing will lead to 50% accuracy.
Let us take the WIG method for example to illustrate the process.
Two WIG thresholds (one for NP queries and the other for content-based queries) are trained by maximizing the prediction accuracy on the training data. When a test query is labeled as the NP (CB) type by the query type classifier, it will be predicted to be good if and only if the WIG score for this query is above the NP (CB) threshold. Similar procedures will be taken for other prediction techniques.
Now we briefly introduce the automatic query type classifier used in this paper. We find that the robustness score, though originally proposed for performance prediction, is a good indicator of query types. We find that on average content-based queries have a much higher robustness score than NP queries. For example,
Figure 2 shows the distributions of robustness scores for NP and content-based queries. According to this finding, the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from the training data. 0
1
2
-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Content-based Figure 2: Distribution of robustness scores for NP and CB queries. The NP queries are the 252 NP topics from the 2005 Terabyte Track. The content-based queries are the 150 ad-hoc title from the Terabyte Tracks 2004-2006. The probability distributions are estimated by the Kernel density estimation method.
Strategies Robust WIG-1 WIG-2 WIG-3 Optimal p=0.6 0.565 0.624 0.665 0.684 0.701 P=0.4 0.567 0.633 0.654 0.673 0.696 Table 8: Comparison of prediction accuracy for five strategies in the mixed-query situation. Two ways to sample a query from the pool: (1) the sampled query is content-based with the probability p=0.6. (that is, the query is NP with probability
We consider five strategies in our experiments. In the first strategy (denoted by robust), we use the robustness score for query performance prediction with the help of a perfect query classifier that always correctly map a query into one of the two categories (that is, NP or CB). This strategy represents the level of prediction accuracy that current prediction techniques can achieve in an ideal condition that query types are known. In the next following three strategies, the WIG method is adopted for performance prediction. The difference among the three is that three different query classifiers are used for each strategy: (1) the classifier always classifies a query into the NP type. (2) the Robustness Score ProbabilityDensity classifier is the robust score classifier mentioned above. (3) the classifier is a perfect one. These three strategies are denoted by WIG-1, WIG-2 and WIG-3 respectively. The reason we are interested in WIG-1 is based on the results from section 4.3.1. In the last strategy (denoted by Optimal) which serves as an upper bound on how well we can do so far, we fully make use of our prediction techniques for each query type assuming a perfect query classifier is available. Specifically, we linearly combine WIG and QF for content-based queries and WIG and FRC for NP queries.
The results for the five strategies are shown in Table 8. For each strategy, we try two ways to sample a query from the pool: (1) the sampled query is CB with probability p=0.6. (the query is NP with probability 0.4) (2) set the probability p=0.4. From Table 8 We can see that in terms of prediction accuracy WIG-2 (the WIG method with the automatic query classifier) is not only better than the first two cases, but also is close to WIG-3 where a perfect classifier is assumed. Some further improvements over WIG-3 are observed when combined with other prediction techniques. The merit of WIG-2 is that it provides a practical solution to automatically identifying poorly performing queries in a Web search environment with mixed query types, which poses considerable obstacles to traditional prediction techniques.
To our knowledge, our paper is the first to thoroughly explore prediction of query performance in web search environments. We demonstrated that our models resulted in higher prediction accuracy than previously published techniques not specially devised for web search scenarios. In this paper, we focus on two types of queries in web search: content-based and Named-Page (NP) finding queries, corresponding to the ad-hoc retrieval task and the Named-Page finding task respectively. For both types of web queries, our prediction models were shown to be substantially more accurate than the current state-of-the-art techniques. Furthermore, we considered a more realistic case that no prior information on query types is available. We demonstrated that the WIG method is particularly suitable for this situation. Considering the adaptability of WIG to a range of collections and query types, one of our future plans is to apply this method to predict user preference of search results on realistic data collected from a commercial search engine.
Other than accuracy, another major issue that prediction techniques have to deal with in a Web environment is efficiency.
Fortunately, since the WIG score is computed just over the terms and the phrases that appear in the query, this calculation can be made very efficient with the support of index. On the other hand, the computation of QF and FRC is relatively less efficient since QF needs to retrieve the whole collection twice and FRC needs to repeatedly rank the perturbed documents. How to improve the efficiency of QF and FRC is our future work.
In addition, the prediction techniques proposed in this paper have the potential of improving retrieval performance by combining with other IR techniques. For example, our techniques can be incorporated to popular query modification techniques such as query expansion and query relaxation. Guided by performance prediction, we can make a better decision on when to or how to modify queries to enhance retrieval effectiveness. We would like to carry out research in this direction in the future.
This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C0023, and in part by an award from Google. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. In addition, we thank Donald Metzler for his valuable comments on this work.

An organization"s intranet provides a means for exchanging information between employees and for facilitating employee collaborations. To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.
At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks. The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.
This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].
The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track. While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet. With only one test collection it is not possible to generalize conclusions to other realistic settings.
In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations. Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages). This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.
We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms? How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above? More generally, do the lessons from the Expert Finding task at TREC carry over to this setting? How does the inclusion or exclusion of different documents affect expertise retrieval tasks? In addition to, how can the topical and organizational structure be used for retrieval purposes?
To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people. This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks. For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above. Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT). This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves. Using the UvT Expert collection, we conduct two sets of experiments. The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting. A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.
Apart from the research questions and data set that we contribute, our main contributions are as follows. The baseline models developed for expertise finding perform well on the new data set. While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case. We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.
Taking the similarity between topics into account can significantly improve retrieval performance. The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well. Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.
The remainder of this paper is organized as follows. In the next section we review related work. Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling. In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5. Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8. We formulate our conclusions in Section 9.
Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11]. Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords. For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5]. More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.
In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise. One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods. Balog et al."s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts. In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.
Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20]. Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates. Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.
Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles. While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization. Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.
As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].
We use generative language modeling to find associations between topics and people. In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates" names and query terms-the language modeling setting allows us to do this in a transparent manner. Our modeling proceeds in two steps. In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19]. The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before.
In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).
To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models. In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling. By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing).
Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?. E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any). Or, they may be in need a trained specialist for consultancy on a specific problem.
Within an organization there are usually many possible candidates who could be experts for given topic. We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?
That is, we determine p(ca|q), and rank candidates ca according to this probability. The candidates with the highest probability given the query are deemed the most likely experts for that topic. The challenge is how to estimate this probability accurately. Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes" Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query. Since p(q) is a constant, it can be ignored for ranking purposes. Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.
Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20].
While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about? Essentially, this turns the questions of expert finding around. The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas. This is the candidate"s topical profile.
Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization. However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time. By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .
A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate ca"s expertise on a given topic ki, (i.e., s(ca, ki)). Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidate"s topical profile. Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area. Examples of such context are: Who does she work with? What are her contact details? Is she well-connected, just in case she is not able to help us herself? What is her role in the organization? Who is her superior? Collaborators, and affiliations, etc. are all part of the candidate"s social profile, and can serve as a background against which the system"s recommendations should be interpreted. In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.
We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidate"s (expertise) profile? where s(ca, ki) is defined by p(ki|ca). Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.
Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca). The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task. This prior does not apply to the profiling task since the candidate (individual) is fixed.
In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people. Both expert finding and expert profiling boil down to this estimation. We employ three models for calculating this probability.
Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model. For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0. From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q. The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities. Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document. The estimation of this probability is presented later, in Section 4.2.
The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .
Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate. This model is used to predict how likely a candidate would produce a query q. This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.
Using Document Models: Model 2 Model 2 [4] takes a different approach. Here, the process is broken into two parts. Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d). Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document. The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .
The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details). Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.
Instead, the document acts like a hidden variable in the process which separates the query from the candidate. This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document. By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.
Using Topic Models: Model 3 We introduce a third model, Model 3.
Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q). This approach is similar to the model presented in [3, 19]. As with the previous models, a language model is inferred, but this time for the query. We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.
The procedure is as follows. Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents. Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model). The main task is to estimate p(t|θk), the probability of a term given the topic model. Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated. Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q. We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms. In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U. If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.) With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic. We assume that p(d) is uniform across U. In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic. The candidate model θca is defined in Eq. 4. By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems.
For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca. In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document. However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.) Hence, we set p(d|ca) to be
is 0. In Section 6 we describe how authorship can be determined on different types of documents within the collection.
The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3. The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands. Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list. In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.
Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.
Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection. Every Dutch Webwijs page has an English translation. Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.
About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile. In addition, about 27% of the experts link to their academic homepage from their Webwijs page. These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.) We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text. We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications. We restricted ourselves to pages where the classifier was confident about the language used on the page.
This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP). Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.
The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways. The UvT setting is one with relatively small amounts of multilingual data. Document-author associations are clear and the data is structured and clean. The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.
Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.
Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.
Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two. Also realistic are the large differences in the amount of information available for each expert. Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all. This leaves us with 743 Dutch and
statistics for the UvT Expert collection.
Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers. In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.
Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and
As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy. This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics.
Below, we evaluate Section 4"s models for expert finding and profiling onthe UvT Expert collection. We detail our research questions and experimental setup, and then present our results.
We address the following research questions. Both expert finding and profiling rely on the estimations of p(q|ca). The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection. In [4], Model 2 outperformed Model 1 on the W3C collection. How do they compare on our data set? And how does Model 3 compare to Model 1? What about performance differences between the two languages in our test collection?
The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements. Results were evaluated separately for English and Dutch. For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered. The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.
We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively.
Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks. The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations. RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.
Looking at Table 1 we see that Model 2 performs the best across the board. However, when the data is clean and very focused (RD),
Model 3 outperforms it in a number of cases. Model 1 has the best coverage of candidates (%ca) and topics (%q). The various document types differ in their characteristics and how they improve the finding and profiling tasks. Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task. Adding the homepages does not prove to be particularly useful.
When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding. Apart from that, the scores fall in the same range for both languages. For the profiling task the coverage of the candidates (%ca) is very similar for both languages. However, the performance is substantially better for the English topics.
While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition). For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4]. For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3]. The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting.
Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection.
One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task). The top and bottom blocks correspond to English and Dutch respectively. The best scores are in boldface. to how related the other requests are to the original query. This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q . To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ). We consider four methods for calculating the similarity score between two topics. Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).
The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are. A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary. Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).
Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables. The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection. The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query. To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.
The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17]. Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q . Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p). The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).
Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy. The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated. We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ).
Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness. We model it as follows: p (q|ca) =
P ou∈OU(ca) λou  · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou. The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations. An organizational unit is associated with all the documents that its members have authored. That is, p(d|ou) = maxca∈ou p(d|ca).
For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical. The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries. While a simplification, this is a sensible first approach. That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1.
In this section we present an experimental evaluation of our advanced models.
Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates). Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.
Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities. Runs were evaluated on the main topics set. Best scores are in boldface.
Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness? Which of the various methods for capturing word relationships is most effective? Furthermore, is our way of bringing in contextual information useful? For which tasks? And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements?
Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset. This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.) This main set consists of 132 Dutch and 119 English topics. The relevance judgements were restricted to the main topic set, but were not expanded with subtopics.
Table 4 presents the results. The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV),
PointLang. Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST). We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task. For both tasks, the LL method performed best.
The content-based approaches performed consistently better than HDIST.
A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection. The unit a person belongs to is used as a context for that person. First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN). An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.
Table 5 reports on the results. As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision. However, the expert profiling task shows a different picture: the scores are low, and the task seems hard. The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.
Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again). Table 6 reports on the results. We find a positive impact of the context models only for expert finding. Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP. The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with.
In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.
In our setting the set of languages consists of English and Dutch: L = {UK, NL}. The weights on these languages were set to be identical (λUK = λNL = 0.5). We performed experiments with various λ settings, but did not observe significant differences in performance.
Table 3 reports on the multilingual results, where performance is evaluated on the full topic set. All three models significantly imLang. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL). Best scores are in boldface. proved over all measures for both tasks. The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases. The relative improvement of the precision scores ranges from 10% to 80%. These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline.
In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area. Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.
To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations. The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.
We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure). We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.
Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves.
Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.
Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005,
FP for RTD under project MultiMATCH contract IST-033104.
The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project.
[1] L. Azzopardi. Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval. PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke. Finding similar experts. In This volume,
[3] K. Balog and M. de Rijke. Determining expert profiles (with an application to expert finding). In IJCAI "07: Proc. 20th Intern. Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR "06: Proc. 29th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez. The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems. In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom. Expertise identification using email communications. In CIKM "03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y. Nie, and J. Bai. Integrating word relationships into language models. In SIGIR "05: Proc. 28th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J. A. Thomas. Elements of Information Theory.
Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins. P@noptic expert: Searching for experts not just for documents. In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff. Overview of the TREC2005 Enterprise Track. In The Fourteenth Text REtrieval Conf. Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak. Working Knowledge: How Organizations Manage What They Know. Harvard Business School Press,
Boston, MA, 1998. [12] T. Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager. Tell me what you do and I"ll tell you what you are: Learning occupation-related activities for biographies. In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR "01: Proc. 24th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft. Cross-lingual relevance models. In SIGIR "02: Proc. 25th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 175-182,
[16] C. Macdonald and I. Ounis. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM "06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb. Expertise browser: a quantitative approach to identifying expertise. In ICSE "02: Proc. 24th Intern. Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft. Hierarchical language models for expert finding in enterprise corpora. In Proc. ICTAI 2006, pages 599-608,
[20] I. Soboroff, A. de Vries, and N. Craswell. Overview of the TREC

Web advertising supports a large swath of today"s Internet ecosystem. The total internet advertiser spend in US alone in 2006 is estimated at over 17 billion dollars with a growth rate of almost 20% year over year. A large part of this market consists of textual ads, that is, short text messages usually marked as sponsored links or similar. The main advertising channels used to distribute textual ads are:
consists in placing ads on the result pages from a web search engine, with ads driven by the originating query.
All major current web search engines (Google, Yahoo!, and Microsoft) support such ads and act simultaneously as a search engine and an ad agency.
to the placement of commercial ads within the content of a generic web page. In contextual advertising usually there is a commercial intermediary, called an ad-network, in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between publisher and ad-network) and improving user experience. Again, all major current web search engines (Google, Yahoo!, and Microsoft) provide such ad-networking services but there are also many smaller players.
The SS market developed quicker than the CM market, and most textual ads are still characterized by bid phrases representing those queries where the advertisers would like to have their ad displayed. (See [5] for a brief history).
However, today, almost all of the for-profit non-transactional web sites (that is, sites that do not sell anything directly) rely at least in part on revenue from context match. CM supports sites that range from individual bloggers and small niche communities to large publishers such as major newspapers. Without this model, the web would be a lot smaller! The prevalent pricing model for textual ads is that the advertisers pay a certain amount for every click on the advertisement (pay-per-click or PPC). There are also other models used: pay-per-impression, where the advertisers pay for the number of exposures of an ad and pay-per-action where the advertiser pays only if the ad leads to a sale or similar transaction. For simplicity, we only deal with the PPC model in this paper.
Given a page, rather than placing generic ads, it seems preferable to have ads related to the content to provide a better user experience and thus to increase the probability of clicks. This intuition is supported by the analogy to conventional publishing where there are very successful magazines (e.g. Vogue) where a majority of the content is topical advertising (fashion in the case of Vogue) and by user studies that have confirmed that increased relevance increases the number of ad-clicks [4, 13].
Previous published approaches estimated the ad relevance based on co-occurrence of the same words or phrases within the ad and within the page (see [7, 8] and Section 3 for more details). However targeting mechanisms based solely on phrases found within the text of the page can lead to problems: For example, a page about a famous golfer named John Maytag might trigger an ad for Maytag dishwashers since Maytag is a popular brand. Another example could be a page describing the Chevy Tahoe truck (a popular vehicle in US) triggering an ad about Lake Tahoe vacations. Polysemy is not the only culprit: there is a (maybe apocryphal) story about a lurid news item about a headless body found in a suitcase triggering an ad for Samsonite luggage! In all these examples the mismatch arises from the fact that the ads are not appropriate for the context.
In order to solve this problem we propose a matching mechanism that combines a semantic phase with the traditional keyword matching, that is, a syntactic phase. The semantic phase classifies the page and the ads into a taxonomy of topics and uses the proximity of the ad and page classes as a factor in the ad ranking formula. Hence we favor ads that are topically related to the page and thus avoid the pitfalls of the purely syntactic approach. Furthermore, by using a hierarchical taxonomy we allow for the gradual generalization of the ad search space in the case when there are no ads matching the precise topic of the page. For example if the page is about an event in curling, a rare winter sport, and contains the words Alpine Meadows, the system would still rank highly ads for skiing in Alpine Meadows as these ads belong to the class skiing which is a sibling of the class curling and both of these classes share the parent winter sports.
In some sense, the taxonomy classes are used to select the set of applicable ads and the keywords are used to narrow down the search to concepts that are of too small granularity to be in the taxonomy. The taxonomy contains nodes for topics that do not change fast, for example, brands of digital cameras, say Canon. The keywords capture the specificity to a level that is more dynamic and granular. In the digital camera example this would correspond to the level of a particular model, say Canon SD450 whose advertising life might be just a few months. Updating the taxonomy with new nodes or even new vocabulary each time a new model comes to the market is prohibitively expensive when we are dealing with millions of manufacturers.
In addition to increased click through rate (CTR) due to increased relevance, a significant but harder to quantify benefit of the semantic-syntactic matching is that the resulting page has a unified feel and improves the user experience. In the Chevy Tahoe example above, the classifier would establish that the page is about cars/automotive and only those ads will be considered. Even if there are no ads for this particular Chevy model, the chosen ads will still be within the automotive domain.
To implement our approach we need to solve a challenging problem: classify both pages and ads within a large taxonomy (so that the topic granularity would be small enough) with high precision (to reduce the probability of mis-match).
We evaluated several classifiers and taxonomies and in this paper we present results using a taxonomy with close to
This classifier gave the best results in both page and ad classification, and ultimately in ad relevance.
The paper proceeds as follows. In the next section we review the basic principles of the contextual advertising.
Section 3 overviews the related work. Section 4 describes the taxonomy and document classifier that were used for page and ad classification. Section 5 describes the semanticsyntactic method. In Section 6 we briefly discuss how to search efficiently the ad space in order to return the top-k ranked ads. Experimental evaluation is presented in Section 7. Finally, Section 8 presents the concluding remarks.
ADVERTISING Contextual advertising is an interplay of four players: • The publisher is the owner of the web pages on which the advertising is displayed. The publisher typically aims to maximize advertising revenue while providing a good user experience. • The advertiser provides the supply of ads. Usually the activity of the advertisers are organized around campaigns which are defined by a set of ads with a particular temporal and thematic goal (e.g. sale of digital cameras during the holiday season). As in traditional advertising, the goal of the advertisers can be broadly defined as the promotion of products or services. • The ad network is a mediator between the advertiser and the publisher and selects the ads that are put on the pages. The ad-network shares the advertisement revenue with the publisher. • Users visit the web pages of the publisher and interact with the ads.
Contextual advertising usually falls into the category of direct marketing (as opposed to brand advertising), that is advertising whose aim is a direct response where the effect of an campaign is measured by the user reaction. One of the advantages of online advertising in general and contextual advertising in particular is that, compared to the traditional media, it is relatively easy to measure the user response. Usually the desired immediate reaction is for the user to follow the link in the ad and visit the advertiser"s web site and, as noted, the prevalent financial model is that the advertiser pays a certain amount for every click on the advertisement (PPC). The revenue is shared between the publisher and the network.
Context match advertising has grown from Sponsored Search advertising, which consists in placing ads on the result pages from a web search engine, with ads driven by the originating query. In most networks, the amount paid by the advertiser for each SS click is determined by an auction process where the advertisers place bids on a search phrase, and their position in the tower of ads displayed in conjunction with the result is determined by their bid. Thus each ad is annotated with one or more bid phrases. The bid phrase has no direct bearing on the ad placement in CM. However, it is a concise description of target ad audience as determined by the advertiser and it has been shown to be an important feature for successful CM ad placement [8]. In addition to the bid phrase, an ad is also characterized by a title usually displayed in a bold font, and an abstract or creative, which is the few lines of text, usually less than 120 characters, displayed on the page.
The ad-network model aligns the interests of the publishers, advertisers and the network. In general, clicks bring benefits to both the publisher and the ad network by providing revenue, and to the advertiser by bringing traffic to the target web site. The revenue of the network, given a page p, can be estimated as: R = X i=1..k P(click|p, ai)price(ai, i) where k is the number of ads displayed on page p and price(ai, i) is the click-price of the current ad ai at position i. The price in this model depends on the set of ads presented on the page. Several models have been proposed to determine the price, most of them based on generalizations of second price auctions. However, for simplicity we ignore the pricing model and concentrate on finding ads that will maximize the first term of the product, that is we search for arg max i P(click|p, ai) Furthermore we assume that the probability of click for a given ad and page is determined by its relevance score with respect to the page, thus ignoring the positional effect of the ad placement on the page. We assume that this is an orthogonal factor to the relevance component and could be easily incorporated in the model.
Online advertising in general and contextual advertising in particular are emerging areas of research. The published literature is very sparse. A study presented in [13] confirms the intuition that ads need to be relevant to the user"s interest to avoid degrading the user"s experience and increase the probability of reaction.
A recent work by Ribeiro-Neto et. al. [8] examines a number of strategies to match pages to ads based on extracted keywords. The ads and pages are represented as vectors in a vector space. The first five strategies proposed in that work match the pages and the ads based on the cosine of the angle between the ad vector and the page vector. To find out the important part of the ad, the authors explore using different ad sections (bid phrase, title, body) as a basis for the ad vector. The winning strategy out of the first five requires the bid phrase to appear on the page and then ranks all such ads by the cosine of the union of all the ad sections and the page vectors.
While both pages and ads are mapped to the same space, there is a discrepancy (impendence mismatch) between the vocabulary used in the ads and in the pages. Furthermore, since in the vector model the dimensions are determined by the number of unique words, plain cosine similarity will not take into account synonyms. To solve this problem,
Ribeiro-Neto et al expand the page vocabulary with terms from other similar pages weighted based on the overall similarity of the origin page to the matched page, and show improved matching precision.
In a follow-up work [7] the authors propose a method to learn impact of individual features using genetic programming to produce a matching function. The function is represented as a tree composed of arithmetic operators and the log function as internal nodes, and different numerical features of the query and ad terms as leafs. The results show that genetic programming finds matching functions that significantly improve the matching compared to the best method (without page side expansion) reported in [8].
Another approach to contextual advertising is to reduce it to the problem of sponsored search advertising by extracting phrases from the page and matching them with the bid phrase of the ads. In [14] a system for phrase extraction is described that used a variety of features to determine the importance of page phrases for advertising purposes. The system is trained with pages that have been hand annotated with important phrases. The learning algorithm takes into account features based on tf-idf, html meta data and query logs to detect the most important phrases. During evaluation, each page phrase up to length 5 is considered as potential result and evaluated against a trained classifier.
In our work we also experimented with a phrase extractor based on the work reported in [12]. While increasing slightly the precision, it did not change the relative performance of the explored algorithms.
The semantic match of the pages and the ads is performed by classifying both into a common taxonomy. The matching process requires that the taxonomy provides sufficient differentiation between the common commercial topics. For example, classifying all medical related pages into one node will not result into a good classification since both sore foot and flu pages will end up in the same node. The ads suitable for these two concepts are, however, very different. To obtain sufficient resolution, we used a taxonomy of around 6000 nodes primarily built for classifying commercial interest queries, rather than pages or ads. This taxonomy has been commercially built by Yahoo! US. We will explain below how we can use the same taxonomy to classify pages and ads as well.
Each node in our source taxonomy is represented as a collection of exemplary bid phrases or queries that correspond to that node concept. Each node has on average around 100 queries. The queries placed in the taxonomy are high volume queries and queries of high interest to advertisers, as indicated by an unusually high cost-per-click (CPC) price.
The taxonomy has been populated by human editors using keyword suggestions tools similar to the ones used by ad networks to suggest keywords to advertisers. After initial seeding with a few queries, using the provided tools a human editor can add several hundreds queries to a given node. Nevertheless, it has been a significant effort to develop this 6000-nodes taxonomy and it has required several person-years of work. A similar-in-spirit process for building enterprise taxonomies via queries has been presented in [6]. However, the details and tools are completely different.
Figure 1 provides some statistics about the taxonomy used in this work.
As explained, the semantic phase of the matching relies on ads and pages being topically close. Thus we need to classify pages into the same taxonomy used to classify ads.
In this section we overview the methods we used to build a page and an ad classifier pair. The detailed description and evaluation of this process is outside the scope of this paper.
Given the taxonomy of queries (or bid-phrases - we use these terms interchangeably) described in the previous section, we tried three methods to build corresponding page and ad classifiers. For the first two methods we tried to find exemplary pages and ads for each concept as follows: Number of Categories By Level 0 200 400 600 800 1000 1200 1400 1600 1800 2000
Level NumberofCategories Number of Children per Nodes 0 50 100 150 200 250 300 350 400 0 2 4 6 8 10 12 14 16 18 20 22 24 29 31 33 35 52 Number of Children NumberofNodes Queries per Node 0 500 1000 1500 2000 2500 3000 1 50 80 120 160 200 240 280 320 360 400 440 480 Number Queries (up to 500+) NumberofNodes Figure 1: Taxonomy statistics: categories per level; fanout for non-leaf nodes; and queries per node We generated a page training set by running the queries in the taxonomy over a Web search index and using the top
query"s label. On the ad side we generated a training set for each class by selecting the ads that have a bid phrase assigned to this class. Using this training sets we then trained a hierarchical SVM [2] (one against all between every group of siblings) and a log-regression [11] classifier. (The second method differs from the first in the type of secondary filtering used. This filtering eliminates low content pages, pages deemed unsuitable for advertising, pages that lead to excessive class confusion, etc.) However, we obtained the best performance by using the third document classifier, based on the information in the source taxonomy queries only. For each taxonomy node we concatenated all the exemplary queries into a single metadocument. We then used the meta document as a centroid for a nearest-neighbor classifier based on Rocchio"s framework [9] with only positive examples and no relevance feedback. Each centroid is defined as a sum of the tf-idf values of each term, normalized by the number of queries in the class cj = 1 |Cj| X q∈Cj q q where cj is the centroid for class Cj; q iterates over the queries in a particular class.
The classification is based on the cosine of the angle between the document d and the centroid meta-documents: Cmax = arg max Cj ∈C cj cj · d d = arg max Cj ∈C P i∈|F | ci j· di qP i∈|F |(ci j)2 qP i∈|F |(di)2 where F is the set of features. The score is normalized by the document and class length to produce comparable score.
The terms ci and di represent the weight of the ith feature in the class centroid and the document respectively. These weights are based on the standard tf-idf formula. As the score of the max class is normalized with regard to document length, the scores for different documents are comparable.
We conducted tests using professional editors to judge the quality of page and ad class assignments. The tests showed that for both ads and pages the Rocchio classifier returned the best results, especially on the page side. This is probably a result of the noise induced by using a search engine to machine generate training pages for the SVM and logregression classifiers. It is an area of current investigation how to improve the classification using a noisy training set.
Based on the test results we decided to use the Rocchio"s classifier on both the ad and the page side.
Contextual advertising systems process the content of the page, extract features, and then search the ad space to find the best matching ads. Given a page p and a set of ads A = {a1 . . . as} we estimate the relative probability of click P(click|p, a) with a score that captures the quality of the match between the page and the ad. To find the best ads for a page we rank the ads in A and select the top few for display. The problem can be formally defined as matching every page in the set of all pages P = {p1, . . . ppc} to one or more ads in the set of ads. Each page is represented as a set of page sections pi = {pi,1, pi,2 . . . pi,m}. The sections of the page represent different structural parts, such as title, metadata, body, headings, etc. In turn, each section is an unordered bag of terms (keywords). A page is represented by the union of the terms in each section: pi = {pws1
m} where pw stands for a page word and the superscript indicates the section of each term. A term can be a unigram or a phrase extracted by a phrase extractor [12].
Similarly, we represent each ad as a set of sections a = {a1, a2, . . . al}, each section in turn being an unordered set of terms: ai = {aws1
l } where aw is an ad word. The ads in our experiments have
produce the match score we use only the ad/page textual information, leaving user information and other data for future work.
Next, each page and ad term is associated with a weight based on the tf-idf values. The tf value is determined based on the individual ad sections. There are several choices for the value of idf, based on different scopes. On the ad side, it has been shown in previous work that the set of ads of one campaign provide good scope for the estimation of idf that leads to improved matching results [8]. However, in this work for simplicity we do not take into account campaigns.
To combine the impact of the term"s section and its tf-idf score, the ad/page term weight is defined as: tWeight(kwsi ) = weightSection(Si) · tf idf(kw) where tWeight stands for term weight and weightSection(Si) is the weight assigned to a page or ad section. This weight is a fixed parameter determined by experimentation.
Each ad and page is classified into the topical taxonomy.
We define these two mappings: Tax(pi) = {pci1, . . . pciu} Tax(aj) = {acj1 . . . acjv} where pc and ac are page and ad classes correspondingly.
Each assignment is associated with a weight given by the classifier. The weights are normalized to sum to 1: X c∈T ax(xi) cWeight(c) = 1 where xi is either a page or an ad, and cWeights(c) is the class weight - normalized confidence assigned by the classifier. The number of classes can vary between different pages and ads. This corresponds to the number of topics a page/ad can be associated with and is almost always in the range 1-4.
Now we define the relevance score of an ad ai and page pi as a convex combination of the keyword (syntactic) and classification (semantic) score: Score(pi, ai) = α · TaxScore(Tax(pi), Tax(ai)) +(1 − α) · KeywordScore(pi, ai) The parameter α determines the relative weight of the taxonomy score and the keyword score.
To calculate the keyword score we use the vector space model [1] where both the pages and ads are represented in n-dimensional space - one dimension for each distinct term. The magnitude of each dimension is determined by the tWeight() formula. The keyword score is then defined as the cosine of the angle between the page and the ad vectors: KeywordScore(pi, ai) = P i∈|K| tWeight(pwi)· tWeight(awi) qP i∈|K|(tWeight(pwi))2 qP i∈|K|(tWeight(awi))2 where K is the set of all the keywords. The formula assumes independence between the words in the pages and ads. Furthermore, it ignores the order and the proximity of the terms in the scoring. We experimented with the impact of phrases and proximity on the keyword score and did not see a substantial impact of these two factors.
We now turn to the definition of the TaxScore. This function indicates the topical match between a given ad and a page. As opposed to the keywords that are treated as independent dimensions, here the classes (topics) are organized into a hierarchy. One of the goals in the design of the TaxScore function is to be able to generalize within the taxonomy, that is accept topically related ads.
Generalization can help to place ads in cases when there is no ad that matches both the category and the keywords of the page.
The example in Figure 2 illustrates this situation. In this example, in the absence of ski ads, a page about skiing containing the word Atomic could be matched to the available snowboarding ad for the same brand.
In general we would like the match to be stronger when both the ad and the page are classified into the same node Figure 2: Two generalization paths and weaker when the distance between the nodes in the taxonomy gets larger. There are multiple ways to specify the distance between two taxonomy nodes. Besides the above requirement, this function should lend itself to an efficient search of the ad space. Given a page we have to find the ad in a few milliseconds, as this impacts the presentation to a waiting user. This will be further discussed in the next section.
To capture both the generalization and efficiency requirements we define the TaxScore function as follows: TaxScore(PC, AC) = X pc∈P C X ac∈AC idist(LCA(pc, ac), ac)·cWeight(pc)·cWeight(ac) In this function we consider every combination of page class and ad class. For each combination we multiply the product of the class weights with the inverse distance function between the least common ancestor of the two classes (LCA) and the ad class. The inverse distance function idist(c1, c2) takes two nodes on the same path in the class taxonomy and returns a number in the interval [0, 1] depending on the distance of the two class nodes. It returns 1 if the two nodes are the same, and declines toward 0 when LCA(pc, ac) or ac is the root of the taxonomy. The rate of decline determines the weight of the generalization versus the other terms in the scoring formula.
To determine the rate of decline we consider the impact on the specificity of the match when we substitute a class with one of its ancestors. In general the impact is topic dependent. For example the node Hobby in our taxonomy has tens of children, each representing a different hobby, two examples being Sailing and Knitting. Placing an ad about Knitting on a page about Sailing does not make lots of sense. However, in the Winter Sports example above, in the absence of better alternative, skiing ads could be put on snowboarding pages as they might promote the same venues, equipment vendors etc. Such detailed analysis on a case by case basis is prohibitively expensive due to the size of the taxonomy.
One option is to use the confidences of the ancestor classes as given by the classifier. However we found these numbers not suitable for this purpose as the magnitude of the confidences does not necessarily decrease when going up the tree. Another option is to use explore-exploit methods based on machine-learning from the click feedback as described in [10]. However for simplicity, in this work we chose a simple heuristic to determine the cost of generalization from a child to a parent. In this heuristic we look at the broadening of the scope when moving from a child to a parent. We estimate the broadening by the density of ads classified in the parent nodes vs the child node. The density is obtained by classifying a large set of ads in the taxonomy using the document classifier described above. Based on this, let nc be the number of document classified into the subtree rooted at c. Then we define: idist(c, p) = nc np where c represents the child node and p is the parent node.
This fraction can be viewed as a probability of an ad belonging to the parent topic being suitable for the child topic.
In the previous section we discussed the choice of scoring function to estimate the match between an ad and a page.
The top-k ads with the highest score are offered by the system for placement on the publisher"s page. The process of score calculation and ad selection is to be done in real time and therefore must be very efficient. As the ad collections are in the range of hundreds of millions of entries, there is a need for indexed access to the ads.
Inverted indexes provide scalable and low latency solutions for searching documents. However, these have been traditionally used to search based on keywords. To be able to search the ads on a combination of keywords and classes we have mapped the classification match to term match and adapted the scoring function to be suitable for fast evaluation over inverted indexes. In this section we overview the ad indexing and the ranking function of our prototype ad search system for matching ads and pages.
We used a standard inverted index framework where there is one posting list for each distinct term. The ads are parsed into terms and each term is associated with a weight based on the section in which it appears. Weights from distinct occurrences of a term in an ad are added together, so that the posting lists contain one entry per term/ad combination.
The next challenge is how to index the ads so that the class information is preserved in the index? A simple method is to create unique meta-terms for the classes and annotate each ad with one meta-term for each assigned class. However this method does not allow for generalization, since only the ads matching an exact label of the page would be selected.
Therefore we annotated each ad with one meta-term for each ancestor of the assigned class. The weights of meta-terms are assigned according to the value of the idist() function defined in the previous section. On the query side, given the keywords and the class of a page, we compose a keyword only query by inserting one class term for each ancestor of the classes assigned to the page.
The scoring function is adapted to the two part scoreone for the class meta-terms and another for the text term.
During the class score calculation, for each class path we use only the lowest class meta-term, ignoring the others. For example, if an ad belongs to the Skiing class and is annotated with both Skiing and its parent Winter Sports, the index will contain the special class meta-terms for both Skiing and Winter Sports (and all their ancestors) with the weights according to the product of the classifier confidence and the idist function. When matching with a page classified into Skiing, the query will contain terms for class Skiing and for each of its ancestors. However when scoring an ad classified into Skiing we will use the weight for the Skiing class meta-term. Ads classified into Snowboarding will be scored using the weight of the Winter Sports meta-term. To make this check efficiently we keep a sorted list of all the class paths and, at scoring time, we search the paths bottom up for a meta-term appearing in the ad. The first meta-term is used for scoring, the rest are ignored.
At runtime, we evaluate the query using a variant of the WAND algorithm [3]. This is a document-at-a-time algorithm [1] that uses a branch-and-bound approach to derive efficient moves for the cursors associated to the postings lists. It finds the next cursor to be moved based on an upper bound of the score for the documents at which the cursors are currently positioned. The algorithm keeps a heap of current best candidates. Documents with an upper bound smaller than the current minimum score among the candidate documents can be eliminated from further considerations, and thus the cursors can skip over them. To find the upper bound for a document, the algorithm assumes that all cursors that are before it will hit this document (i.e. the document contains all those terms represented by cursors before or at that document). It has been shown that WAND can be used with any function that is monotonic with respect to the number of matching terms in the document.
Our scoring function is monotonic since the score can never decrease when more terms are found in the document.
In the special case when we add a cursor representing an ancestor of a class term already factored in the score, the score simply does not change (we add 0). Given these properties, we use an adaptation of the WAND algorithm where we change the calculation of the scoring function and the upper bound score calculation to reflect our scoring function.
The rest of the algorithm remains unchanged.
We quantify the effect of the semantic-syntactic matching using a set of 105 pages. This set of pages was selected by a random sample of a larger set of around 20 million pages with contextual advertising. Ads for each of these pages have been selected from a larger pool of ads (tens of millions) by previous experiments conducted by Yahoo! US for other purposes. Each page-ad pair has been judged by three or more human judges on a 1 to 3 scale:
the main subject of the page. For example if the page is about the National Football League and the ad is about tickets for NFL games, it would be scored as 1.
secondary subject of the page, or is related to the main topic of the page in a general way. In the NFL page example, an ad about NFL branded products would be judged as 2.
example a mention of the NFL player John Maytag triggers washing machine ads on a NFL page. pages 105 words per page 868 judgments 2946 judg. inter-editor agreement 84% unique ads 2680 unique ads per page 25.5 page classification precision 70% ad classification precision 86% Table 1: Dataset statistics To obtain a score for a page-ad pair we average all the scores and then round to the closest integer. We then used these judgments to evaluate how well our methods distinguish the positive and the negative ad assignments for each page. The statistics of the page dataset is given in Table 1.
The original experiments that paired the pages and the ads are loosely related to the syntactic keyword based matching and classification based assignment but used different taxonomies and keyword extraction techniques. Therefore we could not use standard pooling as an evaluation method since we did not control the way the pairs were selected and could not precisely establish the set of ads from which the placed ads were selected.
Instead, in our evaluation for each page we consider only those ads for which we have judgment. Each different method was applied to this set and the ads were ranked by the score.
The relative effectiveness of the algorithms were judged by comparing how well the methods separated the ads with positive judgment from the ads with negative judgment. We present precision on various levels of recall within this set.
As the set of ads per page is relatively small, the evaluation reports precision that is higher than it would be with a larger set of negative ads. However, these numbers still establish the relative performance of the algorithms and we can use it to evaluate performance at different score thresholds.
In addition to the precision-recall over the judged ads, we also present Kendall"s τ rank correlation coefficient to establish how far from the perfect ordering are the orderings produced by our ranking algorithms. For this test we ranked the judged ads by the scores assigned by the judges and then compared this order to the order assigned by our algorithms.
Finally we also examined the precision at position 1, 3 and
Figure 3 shows the precision recall curves for the syntactic matching (keywords only used) vs. a syntactic-semantic matching with the optimal value of α = 0.8 (judged by the 11-point score [1]). In this figure, we assume that the adpage pairs judged with 1 or 2 are positive examples and the 3s are negative examples. We also examined counting only the pairs judged with 1 as positive examples and did not find a significant change in the relative performance of the tested methods. Overlaid are also results using phrases in the keyword match. We see that these additional features do not change the relative performance of the algorithm.
The graphs show significant impact of the class information, especially in the mid range of recall values. In the low recall part of the chart the curves meet. This indicates that when the keyword match is really strong (i.e. when the ad is almost contained within the page) the precision
1
Recall Precision Alpha=.9, no phrase Alpha=0, no phrase Alpha=0, phrase Alpha=.9, phrase Figure 3: Data Set 2: Precision vs. Recall of syntactic match (α = 0) vs. syntactic-semantic match (α = 0.8) α Kendall"s τ α = 0 0.086 α = 0.25 0.155 α = 0.50 0.166 α = 0.75 0.158 α = 1 0.136 Table 2: Kendall"s τ for different α values of the syntactic keyword match is comparable with that of the semantic-syntactic match. Note however that the two methods might produce different ads and could be used as a complement at level of recall.
The semantic components provides largest lift in precision at the mid range of recall where 25% improvement is achieved by using the class information for ad placement.
This means that when there is somewhat of a match between the ad and the page, the restriction to the right classes provides a better scope for selecting the ads.
Table 2 shows the Kendall"s τ values for different values of α. We calculated the values by ranking all the judged ads for each page and averaging the values over all the pages. The ads with tied judgment were given the rank of the middle position. The results show that when we take into account all the ad-page pairs, the optimal value of α is around 0.5.
Note that purely syntactic match (α = 0) is by far the weakest method.
Figure 4 shows the effect of the parameter α in the scoring.
We see that in most cases precision grows or is flat when we increase α, except at the low level of recall where due to small number of data points there is a bit of jitter in the results.
Table 3 shows the precision at positions 1, 3 and 5. Again, the purely syntactic method has clearly the lowest score by individual positions and the total number of correctly placed ads. The numbers are close due to the small number of the ads considered, but there are still some noticeable trends.
For position 1 the optimal α is in the range of 0.25 to 0.75.
For positions 3 and 5 the optimum is at α = 1. This also indicates that for those ads that have a very high keyword score, the semantic information is somewhat less important.
If almost all the words in an ad appear in the page, this ad is Precision Vs Alpha for Different Levels of Recall (Data Set 2)
Alpha Precision 80% Recall 60% Recall 40% Recall 20% Recall Figure 4: Impact of α on precision for different levels of recall α #1 #3 #5 sum α = 0 80 70 68 218 α = 0.25 89 76 73 238 α = 0.5 89 74 73 236 α = 0.75 89 78 73 240 α = 1 86 79 74 239 Table 3: Precision at position 1, 3 and 5 likely to be relevant for this page. However when there is no such clear affinity, the class information becomes a dominant factor.
Contextual advertising is the economic engine behind a large number of non-transactional sites on the Web. Studies have shown that one of the main success factors for contextual ads is their relevance to the surrounding content. All existing commercial contextual match solutions known to us evolved from search advertising solutions whereby a search query is matched to the bid phrase of the ads. A natural extension of search advertising is to extract phrases from the page and match them to the bid phrase of the ads. However, individual phrases and words might have multiple meanings and/or be unrelated to the overall topic of the page leading to miss-matched ads.
In this paper we proposed a novel way of matching advertisements to web pages that rely on a topical (semantic) match as a major component of the relevance score. The semantic match relies on the classification of pages and ads into a 6000 nodes commercial advertising taxonomy to determine their topical distance. As the classification relies on the full content of the page, it is more robust than individual page phrases. The semantic match is complemented with a syntactic match and the final score is a convex combination of the two sub-scores with the relative weight of each determined by a parameter α.
We evaluated the semantic-syntactic approach against a syntactic approach over a set of pages with different contextual advertising. As shown in our experimental evaluation, the optimal value of the parameter α depends on the precise objective of optimization (precision at particular position, precision at given recall). However in all cases the optimal value of α is between 0.25 and 0.9 indicating significant effect of the semantic score component. The effectiveness of the syntactic match depends on the quality of the pages used. In lower quality pages we are more likely to make classification errors that will then negatively impact the matching. We demonstrated that it is feasible to build a large scale classifier that has sufficient good precision for this application.
We are currently examining how to employ machine learning algorithms to learn the optimal value of α based on a collection of features of the input pages.
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. ACM, 1999. [2] Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik.
A training algorithm for optimal margin classifiers. In Computational Learning Theory, pages 144-152, 1992. [3] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM "03: Proc. of the twelfth intl. conf. on Information and knowledge management, pages 426-434, New York, NY, 2003. ACM. [4] P. Chatterjee, D. L. Hoffman, and T. P. Novak. Modeling the clickstream: Implications for web-based advertising efforts. Marketing Science, 22(4):520-541, 2003. [5] D. Fain and J. Pedersen. Sponsored search: A brief history.
In In Proc. of the Second Workshop on Sponsored Search Auctions, 2006. Web publication, 2006. [6] S. C. Gates, W. Teiken, and K.-Shin F. Cheng. Taxonomies by the numbers: building high-performance taxonomies. In CIKM "05: Proc. of the 14th ACM intl. conf. on Information and knowledge management, pages 568-577,
New York, NY, 2005. ACM. [7] A. Lacerda, M. Cristo, M. Andre; G., W. Fan, N. Ziviani, and B. Ribeiro-Neto. Learning to advertise. In SIGIR "06: Proc. of the 29th annual intl. ACM SIGIR conf., pages 549-556, New York, NY, 2006. ACM. [8] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S. de Moura. Impedance coupling in content-targeted advertising. In SIGIR "05: Proc. of the 28th annual intl.
ACM SIGIR conf., pages 496-503, New York, NY, 2005.
ACM. [9] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323. PrenticeHall, 1971. [10] P. Sandeep, D. Agarwal, D. Chakrabarti, and V. Josifovski.
Bandits for taxonomies: A model-based approach. In In Proc. of the SIAM intl. conf. on Data Mining, 2007. [11] T. Santner and D. Duffy. The Statistical Analysis of Discrete Data. Springer-Verlag, 1989. [12] R. Stata, K. Bharat, and F. Maghoul. The term vector database: fast access to indexing terms for web pages.
Computer Networks, 33(1-6):247-255, 2000. [13] C. Wang, P. Zhang, R. Choi, and M. D. Eredita.
Understanding consumers attitude toward advertising. In Eighth Americas conf. on Information System, pages 1143-1148, 2002. [14] W. Yih, J. Goodman, and V. R. Carvalho. Finding advertising keywords on web pages. In WWW "06: Proc. of the 15th intl. conf. on World Wide Web, pages 213-222,

In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue. Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding. Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics. Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents. During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning. Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs. This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.
IR evaluations are comparative in nature (cf. TREC).
Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics. Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets. While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.
If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation. An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.
In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a user"s query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.
Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not. If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch.
Accounting for term mismatch between the terms in user queries and the documents relevant to users" information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47]. Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users" information needs. Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37]. Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2]. Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).
In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others. QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27]. Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift. As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].
The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank,
Bpref, etc. [7, 8, 44, 14]. In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].
In addition, the IR research community has given attention to differences between the performance of individual queries. Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance. In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9]. There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP. GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].
However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms. By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.
The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time. In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms. Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch.
In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner. Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes. We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query. Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.
This removal process affects only the relevant documents in the search collection. The queries themselves remain unaltered. Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms. In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents. Notice that, for a given query, only relevant documents are modified. Non-relevant documents are left unchanged, even in the case that they contain query terms.
Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents. Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap. Removing a query term from relevant documents simply masks the presence of that query term in those documents. It does not in any way change the conceptual relevancy of the documents.
The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.
The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C.
This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.
Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection. In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.
In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric. In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query. It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.
The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time. Two decisions must be made when choosing a removal strategy S.
The first is the order in which S removes terms from the relevant documents. Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection. Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.
Once an order for removal has been decided, a manner for term removal/masking must be decided. It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on). The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch. Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term.
We used the proposed evaluation framework to evaluate four IR systems on two test collections. Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (we"ll call it TCS, for Thomson Concept Search). TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.
This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched. Translation probabilities for QE [2] are calculated from these large external corpora.
Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines. Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms. We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities. As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.
The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7. When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms. The QL model used Jelinek-Mercer smoothing, with λ = 0.6.
We evaluated the performance of the four IR systems outlined above on two different test collections. The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.
The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.
The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length. In our evaluation, we used both the title and the description fields of topics
Collection. After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms.
In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection. Terms with high IDF values tend to influence document ranking more than those with lower IDF values. Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.
For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents. The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents. For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents.
In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents. Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework.
Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection. As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection. The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.
TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms). Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search. Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0
MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis).
Number of Query Terms Removed from Relevant Documents 0
Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection. However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search.
description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries. The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS. On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).
This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which
Number of Query Terms Removed from Relevant Documents 0
1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis).
Number of Query Terms Removed from Relevant Documents 0
MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries. MAP is measured as a function of the number of query terms removed.
Number of Query Terms Removed from Relevant Documents 0
Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries. P at 10 is measured as a function of the number of query terms removed.
Number of Query Terms Removed from Relevant Documents 0
Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.
Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents. At two query terms removed, TCS starts outperforming Okapi FB.
If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.
It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FB"s performance as terms are removed. This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch.
Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries. As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation. As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed. On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.
Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections.
Number of Query Terms Removed from Relevant Documents 0
MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed.
Number of Query Terms Removed from Relevant Documents 0
Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed.
Number of Query Terms Removed from Relevant Documents 0
Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed.
The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents. In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.
Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.
A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection. The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be. This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked. For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.
Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents. This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.
Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.
The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents. However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted. Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.
As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.
Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection. This could lead to insight as to when QE should and should not be applied.
This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch. In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known. By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems. Such robustness makes a system more user-friendly, especially to non-expert users.
This paper presents a novel framework for IR system evaluation, the applications of which are numerous. The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied. To be sure, there is much future work that could be done using this framework.
In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch. Indeed, this framework would also benefit from further testing on a larger collection.
The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or don"t overcome) term mismatch between queries and relevant documents. Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval. By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents. Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.
The evaluation framework proposed in this paper is attractive for several reasons. Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch. In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections. Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.
It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users" information needs as represented by their queries. An IR system that is easy to use should be good at retrieving documents that are relevant to users" information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents.
[1] Amati, G., C. Carpineto, and G. Romano. Query difficulty, robustness and selective application of query expansion. In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D. Lafferty. 1999. Information retrieval as statistical translation. In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J.
Zobel. 2003. Query expansion using associated queries.
In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003. When Query Expansion Fails. In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004. Questioning Query Expansion: An Examination of Behaviour and Parameters. In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005. Document Expansion versus Query Expansion for ad-hoc Retrieval. In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.
What Makes A Query Difficult? In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.
Informative Term Selection for Automatic Query Expansion. In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005. Incremental Test Collections. In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.
Minimal Test Collections for Retrieval Evaluation. In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L. Clarke. 1998.
Efficient Construction of Large Test Collections. In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R. Lynam. 2006. Statistical Precision of Information Retrieval Evaluation. In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B. Croft. 2004.
A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N. Query Expansion. 1996. In Martha E. Williams (ed.), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995. CLARIT-TREC Experiments. Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X. Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004. Dependence language model for information retrieval. In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992. Relevance feedback revisited. In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993. The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994. The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995. The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998. Towards Interactive Query Expansion. In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B. Croft. 1994. The Association Thesaurus for Information Retrieval. In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B. Keefer. Query expansion/reduction and its impact on retrieval effectiveness. In: D.K.
Harman, ed. The Third Text REtrieval Conference (TREC-3). Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002. Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources. In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.
Improving Automatic Query Expansion. In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991. The limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B. Croft. 1998. A language modeling approach to information retrieval. In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993. Concept based query expansion. In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006. On GMAP - and other transformations. In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976. Relevance Weighting of Search Terms. Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1994. Okapi at TREC-2. In D.K. Harman (ed). 1994. The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1995. Okapi at TREC-3. In D.K. Harman (ed). 1995. The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971. Relevance feedback in information retrieval. In G. Salton (Ed.), The SMART Retrieval System. Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968. Automatic Information Organization and Retrieval. McGraw-Hill. [39] Salton, G. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing.
Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980. Automatic term class construction using relevance-a summary of work in automatic pseudoclassification. Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988. On the Use of Spreading Activation Methods in Automatic Information Retrieval. In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994. Word sense disambiguation and information retrieval. In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004. Forming test collections with no system pooling. In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J. Van Rijsbergen. 1983. The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System. Computer Journal. 26(3):239-246. [46] Song, F. and W.B. Croft. 1999. A general language model for information retrieval. In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971. Automatic Keyword Classification for Information Retrieval. London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004. Scoring missing terms in information retrieval tasks. In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994. Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance. In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a. On Expanding Query Vectors with Lexically Related Words. In Harman, D. K., ed.

In information retrieval, a user poses a query to a system.
The system retrieves n documents each receiving a realvalued score indicating the predicted degree of relevance.
If we randomly select pairs of documents from this set, we expect some pairs to share the same topic and other pairs to not share the same topic. Take two topically-related documents from the set and call them a and b. If the scores of a and b are very different, we may be concerned about the performance of our system. That is, if a and b are both on the topic of the query, we would like them both to receive a high score; if a and b are not on the topic of the query, we would like them both to receive a low score. We might become more worried as we find more differences between scores of related documents. We would be more comfortable with a retrieval where scores are consistent between related documents.
Our paper studies the quantification of this inconsistency in a retrieval from a spatial perspective. Spatial analysis is appropriate since many retrieval models embed documents in some vector space. If documents are embedded in a space, proximity correlates with topical relationships. Score consistency can be measured by the spatial version of autocorrelation known as the Moran coefficient or IM [5, 10]. In this paper, we demonstrate a strong correlation between IM and retrieval performance.
The discussion up to this point is reminiscent of the cluster hypothesis. The cluster hypothesis states: closely-related documents tend to be relevant to the same request [12]. As we shall see, a retrieval function"s spatial autocorrelation measures the degree to which closely-related documents receive similar scores. Because of this, we interpret autocorrelation as measuring the degree to which a retrieval function satisfies the clustering hypothesis. If this connection is reasonable, in Section 6, we present evidence that failure to satisfy the cluster hypothesis correlates strongly with poor performance.
In this work, we provide the following contributions,
performance of retrievals with zero relevance judgments (Section 3).
motivations behind several state-of-the-art performance prediction techniques (Section 4).
single run performance prediction (Sections 5 and 6).
Given a query, an information retrieval system produces a ranking of documents in the collection encoded as a set of scores associated with documents. We refer to the set of scores for a particular query-system combination as a retrieval. We would like to predict the performance of this retrieval with respect to some evaluation measure (eg, mean average precision). In this paper, we present results for ranking retrievals from arbitrary systems. We would like this ranking to approximate the ranking of retrievals by the evaluation measure. This is different from ranking queries by the average performance on each query. It is also different from ranking systems by the average performance on a set of queries.
Scores are often only computed for the top n documents from the collection. We place these scores in the length n vector, y, where yi refers to the score of the ith-ranked document. We adjust scores to have zero mean and unit variance. We use this method because of its simplicity and its success in previous work [15].
In information retrieval, we often assume that the representations of documents exist in some high-dimensional vector space. For example, given a vocabulary, V, this vector space may be an arbitrary |V|-dimensional space with cosine inner-product or a multinomial simplex with a distributionbased distance measure. An embedding space is often selected to respect topical proximity; if two documents are near, they are more likely to share a topic.
Because of the prevalence and success of spatial models of information retrieval, we believe that the application of spatial data analysis techniques are appropriate. Whereas in information retrieval, we are concerned with the score at a point in a space, in spatial data analysis, we are concerned with the value of a function at a point or location in a space.
We use the term function here to mean a mapping from a location to a real value. For example, we might be interested in the prevalence of a disease in the neighborhood of some city. The function would map the location of a neighborhood to an infection rate.
If we want to quantify the spatial dependencies of a function, we would employ a measure referred to as the spatial autocorrelation [5, 10]. High spatial autocorrelation suggests that knowing the value of a function at location a will tell us a great deal about the value at a neighboring location b. There is a high spatial autocorrelation for a function representing the temperature of a location since knowing the temperature at a location a will tell us a lot about the temperature at a neighboring location b. Low spatial autocorrelation suggests that knowing the value of a function at location a tells us little about the value at a neighboring location b. There is low spatial autocorrelation in a function measuring the outcome of a coin toss at a and b.
In this section, we will begin by describing what we mean by spatial proximity for documents and then define a measure of spatial autocorrelation. We conclude by extending this model to include information from multiple retrievals from multiple systems for a single query.
Our work does not focus on improving a specific similarity measure or defining a novel vector space. Instead, we choose an inner product known to be effective at detecting interdocument topical relationships. Specifically, we adopt tf.idf document vectors, ˜di = di log „ (n + 0.5) − ci
« (1) where d is a vector of term frequencies, c is the length-|V| document frequency vector. We use this weighting scheme due to its success for topical link detection in the context of Topic Detection and Tracking (TDT) evaluations [6].
Assuming vectors are scaled by their L2 norm, we use the inner product, ˜di, ˜dj , to define similarity.
Given documents and some similarity measure, we can construct a matrix which encodes the similarity between pairs of documents. Recall that we are given the top n documents retrieved in y. We can compute an n × n similarity matrix, W. An element of this matrix, Wij represents the similarity between documents ranked i and j. In practice, we only include the affinities for a document"s k-nearest neighbors. In all of our experiments, we have fixed k to 5.
We leave exploration of parameter sensitivity to future work.
We also row normalize the matrix so that Pn j=1 Wij = 1 for all i.
Recall that we are interested in measuring the similarity between the scores of spatially-close documents. One such suitable measure is the Moran coefficient of spatial autocorrelation. Assuming the function y over n locations, this is defined as ˜IM = n eTWe P i,j Wijyiyj P i y2 i = n eTWe yT Wy yTy (2) where eT We = P ij Wij.
We would like to compare autocorrelation values for different retrievals. Unfortunately, the bound for Equation 2 is not consistent for different W and y. Therefore, we use the Cauchy-Schwartz inequality to establish a bound, ˜IM ≤ n eTWe s yTWTWy yTy And we define the normalized spatial autocorrelation as IM = yT Wy p yTy × yTWTWy Notice that if we let ˜y = Wy, then we can write this formula as,
IM = yT ˜y y 2 ˜y 2 (3) which can be interpreted as the correlation between the original retrieval scores and a set of retrieval scores diffused in the space.
We present some examples of autocorrelations of functions on a grid in Figure 1.
Sometimes we are interested in the performance of a single retrieval but have access to scores from multiple systems for (a) IM = 0.006 (b) IM = 0.241 (c) IM = 0.487 Figure 1: The Moran coefficient, IM for a several binary functions on a grid. The Moran coefficient is a local measure of function consistency. From the perspective of information retrieval, each of these grid spaces would represent a document and documents would be organized so that they lay next to topically-related documents. Binary retrieval scores would define a pattern on this grid. Notice that, as the Moran coefficient increases, neighboring cells tend to have similar values. the same query. In this situation, we can use combined information from these scores to construct a surrogate for a high-quality ranking [17]. We can treat the correlation between the retrieval we are interested in and the combined scores as a predictor of performance.
Assume that we are given m score functions, yi, for the same n documents. We will represent the mean of these vectors as yµ = Pm i=1 yi. We use the mean vector as an approximation to relevance. Since we use zero mean and unit variance normalization, work in metasearch suggests that this assumption is justified [15]. Because yµ represents a very good retrieval, we hypothesize that a strong similarity between yµ and y will correlate positively with system performance. We use Pearson"s product-moment correlation to measure the similarity between these vectors, ρ(y, yµ) = yT yµ y 2 yµ 2 (4) We will comment on the similarity between Equation 3 and
Of course, we can combine ρ(y, ˜y) and ρ(y, yµ) if we assume that they capture different factors in the prediction.
One way to accomplish this is to combine these predictors as independent variables in a linear regression. An alternative means of combination is suggested by the mathematical form of our predictors. Since ˜y encodes the spatial dependencies in y and yµ encodes the spatial properties of the multiple runs, we can compute a third correlation between these two vectors, ρ(˜y, yµ) = ˜yT yµ ˜y 2 yµ 2 (5) We can interpret Equation 5 as measuring the correlation between a high quality ranking (yµ) and a spatially smoothed version of the retrieval (˜y).
PREDICTORS One way to predict the effectiveness of a retrieval is to look at the shared vocabulary of the top n retrieved documents. If we computed the most frequent content words in this set, we would hope that they would be consistent with our topic. In fact, we might believe that a bad retrieval would include documents on many disparate topics, resulting in an overlap of terminological noise. The Clarity of a query attempts to quantify exactly this [7]. Specifically,
Clarity measures the similarity of the words most frequently used in retrieved documents to those most frequently used in the whole corpus. The conjecture is that a good retrieval will use language distinct from general text; the overlapping language in a bad retrieval will tend to be more similar to general text. Mathematically, we can compute a representation of the language used in the initial retrieval as a weighted combination of document language models,
P(w|θQ) = nX i=1 P(w|θi) P(Q|θi) Z (6) where θi is the language model of the ith-ranked document, P(Q|θi) is the query likelihood score of the ith-ranked document and Z = Pn i=1 P(Q|θi) is a normalization constant. The similarity between the multinomial P(w|θQ) and a model of general text can be computed using the Kullback-Leibler divergence, DV KL(θQ θC ). Here, the distribution P(w|θC ) is our model of general text which can be computed using term frequencies in the corpus. In Figure 2a, we present Clarity as measuring the distance between the weighted center of mass of the retrieval (labeled y) and the unweighted center of mass of the collection (labeled O).
Clarity reaches a minimum when a retrieval assigns every document the same score.
Let"s again assume we have a set of n documents retrieved for our query. Another way to quantify the dispersion of a set of documents is to look at how clustered they are. We may hypothesize that a good retrieval will return a single, tight cluster. A poorly performing retrieval will return a loosely related set of documents covering many topics. One proposed method of quantifying this dispersion is to measure the distance from a random document a to it"s nearest neighbor, b. A retrieval which is tightly clustered will, on average, have a low distance between a and b; a retrieval which is less tightly-closed will, on average have high distances between a and b. This average corresponds to using the Cox-Lewis statistic to measure the randomness of the top n documents retrieved from a system [18]. In Figure 2a, this is roughly equivalent to measuring the area of the set n. Notice that we are throwing away information about the retrieval function y. Therefore the Cox-Lewis statistic is highly dependent on selecting the top n documents.1 Remember that we have n documents and a set of scores.
Let"s assume that we have access to the system which provided the original scores and that we can also request scores for new documents. This suggests a third method for predicting performance. Take some document, a, from the retrieved set and arbitrarily add or remove words at random to create a new document ˜a. Now, we can ask our system to score ˜a with respect to our query. If, on average over the n documents, the scores of a and ˜a tend to be very different, we might suspect that the system is failing on this query. So, an alternative approach is to measure the simi1 The authors have suggested coupling the query with the distance measure [18]. The information introduced by the query, though, is retrieval-independent so that, if two retrievals return the same set of documents, the approximate Cox-Lewis statistic will be the same regardless of the retrieval scores. yOy (a) Global Divergence µ(y)˜y y (b) Score Perturbation µ(y) y (c) Multirun Averaging Figure 2: Representation of several performance predictors on a grid. In Figure 2a, we depict predictors which measure the divergence between the center of mass of a retrieval and the center of the embedding space. In Figure 2b, we depict predictors which compare the original retrieval, y, to a perturbed version of the retrieval, ˜y. Our approach uses a particular type of perturbation based on score diffusion. Finally, in Figure 2c, we depict prediction when given retrievals from several other systems on the same query. Here, we can consider the fusion of these retrieval as a surrogate for relevance. larity between the retrieval and a perturbed version of that retrieval [18, 19]. This can be accomplished by either perturbing the documents or queries. The similarity between the two retrievals can be measured using some correlation measure. This is depicted in Figure 2b. The upper grid represents the original retrieval, y, while the lower grid represents the function after having been perturbed, ˜y. The nature of the perturbation process requires additional scorings or retrievals. Our predictor does not require access to the original scoring function or additional retrievals. So, although our method is similar to other perturbation methods in spirit, it can be applied in situations when the retrieval system is inaccessible or costly to access.
Finally, assume that we have, in addition to the retrieval we want to evaluate, m retrievals from a variety of different systems. In this case, we might take a document a, compare its rank in the retrieval to its average rank in the m retrievals. If we believe that the m retrievals provide a satisfactory approximation to relevance, then a very large difference in rank would suggest that our retrieval is misranking a. If this difference is large on average over all n documents, then we might predict that the retrieval is bad. If, on the other hand, the retrieval is very consistent with the m retrievals, then we might predict that the retrieval is good. The similarity between the retrieval and the combined retrieval may be computed using some correlation measure. This is depicted in Figure 2c. In previous work, the Kullback-Leibler divergence between the normalized scores of the retrieval and the normalized scores of the combined retrieval provides the similarity [1].
Our experiments focus on testing the predictive power of each of our predictors: ρ(y, ˜y), ρ(y, yµ), and ρ(˜y, yµ). As stated in Section 2, we are interested in predicting the performance of the retrieval generated by an arbitrary system.
Our methodology is consistent with previous research in that we predict the relative performance of a retrieval by comparing a ranking based on our predictor to a ranking based on average precision.
We present results for two sets of experiments. The first set of experiments presents detailed comparisons of our predictors to previously-proposed predictors using identical data sets. Our second set of experiments demonstrates the generalizability of our approach to arbitrary retrieval methods, corpus types, and corpus languages.
In these experiments, we will predict the performance of language modeling scores using our autocorrelation predictor, ρ(y, ˜y); we do not consider ρ(y, yµ) or ρ(˜y, yµ) because, in these detailed experiments, we focus on ranking the retrievals from a single system. We use retrievals, values for baseline predictors, and evaluation measures reported in previous work [19].
These performance prediction experiments use language model retrievals performed for queries associated with collections in the TREC corpora. Using TREC collections allows us to confidently associate an average precision with a retrieval. In these experiments, we use the following topic collections: TREC 4 ad-hoc, TREC 5 ad-hoc, Robust 2004,
Terabyte 2004, and Terabyte 2005.
We provide two baselines. Our first baseline is the classic Clarity predictor presented in Equation 6. Clarity is designed to be used with language modeling systems. Our second baseline is Zhou and Croft"s ranking robustness predictor. This predictor corrupts the top k documents from retrieval and re-computes the language model scores for these corrupted documents. The value of the predictor is the Spearman rank correlation between the original ranking and the corrupted ranking. In our tables, we will label results for Clarity using DV KL and the ranking robustness predictor using P.
Our predictors do not require a particular baseline retrieval system; the predictors can be computed for an arbitrary retrieval, regardless of how scores were generated. We believe that that is one of the most attractive aspects of our algorithm. Therefore, in a second set of experiments, we demonstrate the ability of our techniques to generalize to a variety of collections, topics, and retrieval systems.
We gathered a diverse set of collections from all possible TREC corpora. We cast a wide net in order to locate collections where our predictors might fail. Our hypothesis is that documents with high topical similarity should have correlated scores. Therefore, we avoided collections where scores were unlikely to be correlated (eg, question-answering) or were likely to be negatively correlated (eg, novelty).
Nevertheless, our collections include corpora where correlations are weakly justified (eg, non-English corpora) or not justified at all (eg, expert search). We use the ad-hoc tracks from TREC3-8, TREC Robust 2003-2005, TREC Terabyte 20042005, TREC4-5 Spanish, TREC5-6 Chinese, and TREC Enterprise Expert Search 2005. In all cases, we use only the automatic runs for ad-hoc tracks submitted to NIST.
For all English and Spanish corpora, we construct the matrix W according to the process described in Section 3.1. For Chinese corpora, we use na¨ıve character-based tf.idf vectors.
For entities, entries in W are proportional to the number of documents in which two entities cooccur.
In our detailed experiments, we used the Clarity measure as a baseline. Since we are predicting the performance of retrievals which are not based on language modeling, we use a version of Clarity referred to as ranked-list Clarity [7]. Ranked-list clarity converts document ranks to P(Q|θi) values. This conversion begins by replacing all of the scores in y with the respective ranks. Our estimation of P(Q|θi) from the ranks, then is,
P(Q|θi) = ( 2(c+1−yi) c(c+1) if yi ≤ c
(7) where c is a cutoff parameter. As suggested by the authors, we fix the algorithm parameters c and λ2 so that c = 60 and λ2 = 0.10. We use Equation 6 to estimate P(w|θQ) and DV KL(θQ θC ) to compute the value of the predictor. We will refer to this predictor as DV KL, superscripted by V to indicate that the Kullback-Leibler divergence is with respect to the term embedding space.
When information from multiple runs on the same query is available, we use Aslam and Pavlu"s document-space multinomial divergence as a baseline [1]. This rank-based method first normalizes the scores in a retrieval as an n-dimensional multinomial. As with ranked-list Clarity, we begin by replacing all of the scores in y with their respective ranks.
Then, we adjust the elements of y in the following way, ˆyi = 1 2n 0 @1 + nX k=yi 1 k 1 A (8) In our multirun experiments, we only use the top 75 documents from each retrieval (n = 75); this is within the range of parameter values suggested by the authors. However, we admit not tuning this parameter for either our system or the baseline. The predictor is the divergence between the candidate distribution, y, and the mean distribution, yµ . With the uniform linear combination of these m retrievals represented as yµ, we can compute the divergence as Dn KL(ˆy ˆyµ) where we use the superscript n to indicate that the summation is over the set of n documents. This baseline was developed in the context of predicting query difficulty but we adopt it as a reasonable baseline for predicting retrieval performance.
When given multiple retrievals, we use documents in the union of the top k = 75 documents from each of the m retrievals for that query. If the size of this union is ˜n, then yµ and each yi is of length ˜n. In some cases, a system did not score a document in the union. Since we are making a Gaussian assumption about our scores, we can sample scores for these unseen documents from the negative tail of the distribution. Specifically, we sample from the part of the distribution lower than the minimum value of in the normalized retrieval. This introduces randomness into our algorithm but we believe it is more appropriate than assigning an arbitrary fixed value.
We optimized the linear regression using the square root of each predictor. We found that this substantially improved fits for all predictors, including the baselines. We considered linear combinations of pairs of predictors (labeled by the components) and all predictors (labeled as β).
Given a set of retrievals, potentially from a combination of queries and systems, we measure the correlation of the rank ordering of this set by the predictor and by the performance metric. In order to ensure comparability with previous results, we present Kendall"s τ correlation between the predictor"s ranking and ranking based on average precision of the retrieval. Unless explicitly noted, all correlations are significant with p < 0.05.
Predictors can sometimes perform better when linearly combined [9, 11]. Although previous work has presented the coefficient of determination (R2 ) to measure the quality of the regression, this measure cannot be reliably used when comparing slight improvements from combining predictors.
Therefore, we adopt the adjusted coefficient of determination which penalizes models with more variables. The adjusted R2 allows us to evaluate the improvement in prediction achieved by adding a parameter but loses the statistical interpretation of R2 . We will use Kendall"s τ to evaluate the magnitude of the correlation and the adjusted R2 to evaluate the combination of variables.
We present results for our detailed experiments comparing the prediction of language model scores in Table 1. Although the Clarity measure is theoretically designed for language model scores, it consistently underperforms our system-agnostic predictor. Ranking robustness was presented as an improvement to Clarity for web collections (represented in our experiments by the terabyte04 and terabyte05 collections), shifting the τ correlation from 0.139 to 0.150 for terabyte04 and
are slight compared to the performance of autocorrelation on these collections. Our predictor achieves a τ correlation of 0.454 for terabyte04 and 0.383 for terabyte05. Though not always the strongest, autocorrelation achieves correlations competitive with baseline predictors. When examining the performance of linear combinations of predictors, we note that in every case, autocorrelation factors as a necessary component of a strong predictor. We also note that the adjusted R2 for individual baselines are always significantly improved by incorporating autocorrelation.
We present our generalizability results in Table 2. We begin by examining the situation in column (a) where we are presented with a single retrieval and no information from additional retrievals. For every collection except one, we achieve significantly better correlations than ranked-list Clarity. Surprisingly, we achieve relatively strong correlations for Spanish and Chinese collections despite our na¨ıve processing. We do not have a ranked-list clarity correlation for ent05 because entity modeling is itself an open research question. However, our autocorrelation measure does not achieve high correlations perhaps because relevance for entity retrieval does not propagate according to the cooccurrence links we use.
As noted above, the poor Clarity performance on web data is consistent with our findings in the detailed experiments. Clarity also notably underperforms for several news corpora (trec5, trec7, and robust04). On the other hand, autocorrelation seems robust to the changes between different corpora.
Next, we turn to the introduction of information from multiple retrievals. We compare the correlations between those predictors which do not use this information in column (a) and those which do in column (b). For every collection, the predictors in column (b) outperform the predictors in column (a), indicating that the information from additional runs can be critical to making good predictions.
Inspecting the predictors in column (b), we only draw weak conclusions. Our new predictors tend to perform better on news corpora. And between our new predictors, the hybrid ρ(˜y, yµ) predictor tends to perform better. Recall that our ρ(˜y, yµ) measure incorporates both spatial and multiple retrieval information. Therefore, we believe that the improvement in correlation is the result of incorporating information from spatial behavior.
In column (c), we can investigate the utility of incorporating spatial information with information from multiple retrievals. Notice that in the cases where autocorrelation, ρ(y, ˜y), alone performs well (trec3, trec5-spanish, and trec6-chinese), it is substantially improved by incorporating multiple-retrieval information from ρ(y, yµ) in the linear regression, β. In the cases where ρ(y, yµ) performs well, incorporating autocorrelation rarely results in a significant improvement in performance. In fact, in every case where our predictor outperforms the baseline, it includes information from multiple runs.
The most important result from our experiments involves prediction when no information is available from multiple runs (Tables 1 and 2a). This situation arises often in system design. For example, a system may need to, at retrieval time, assess its performance before deciding to conduct more intensive processing such as pseudo-relevance feedback or interaction. Assuming the presence of multiple retrievals is unrealistic in this case.
We believe that autocorrelation is, like multiple-retrieval algorithms, approximating a good ranking; in this case by diffusing scores. Why is ˜y a reasonable surrogate? We know that diffusion of scores on the web graph and language model graphs improves performance [14, 16]. Therefore, if score diffusion tends to, in general, improve performance, then diffused scores will, in general, provide a good surrogate for relevance. Our results demonstrate that this approximation is not as powerful as information from multiple retrievals.
Nevertheless, in situations where this information is lacking, autocorrelation provides substantial information.
The success of autocorrelation as a predictor may also have roots in the clustering hypothesis. Recall that we regard autocorrelation as the degree to which a retrieval satisfies the clustering hypothesis. Our experiments, then, demonstrate that a failure to respect the clustering hypothesis correlates with poor performance. Why might systems fail to conform to the cluster hypothesis? Query-based information retrieval systems often score documents independently. The score of document a may be computed by examining query term or phrase matches, the document length, and perhaps global collection statistics. Once computed, a system rarely compares the score of a to the score of a topically-related document b. With some exceptions, the correlation of document scores has largely been ignored.
We should make it clear that we have selected tasks where topical autocorrelation is appropriate. There are certainly cases where there is no reason to believe that retrieval scores will have topical autocorrelation. For example, ranked lists which incorporate document novelty should not exhibit spatial autocorrelation; if anything autocorrelation should be negative for this task. Similarly, answer candidates in a question-answering task may or may not exhibit autocorrelation; in this case, the semantics of links is questionable too. It is important before applying this measure to confirm that, given the semantics for some link between two retrieved items, we should expect a correlation between scores.
In this section we draw more general comparisons to other work in performance prediction and spatial data analysis.
There is a growing body of work which attempts to predict the performance of individual retrievals [7, 3, 11, 9, 19]. We have attempted to place our work in the context of much of this work in Section 4. However, a complete comparison is beyond the scope of this paper. We note, though, that our experiments cover a larger and more diverse set of retrievals, collections, and topics than previously examined.
Much previous work-particularly in the context of TRECfocuses on predicting the performance of systems. Here, each system generates k retrievals. The task is, given these retrievals, to predict the ranking of systems according to some performance measure. Several papers attempt to address this task under the constraint of few judgments [2, 4].
Some work even attempts to use zero judgments by leveraging multiple retrievals for the same query [17]. Our task differs because we focus on ranking retrievals independent of the generating system. The task here is not to test the hypothesis system A is superior to system B but to test the hypothesis retrieval A is superior to retrieval B.
Autocorrelation manifests itself in many classification tasks.
Neville and Jensen define relational autocorrelation for relational learning problems and demonstrate that many classification tasks manifest autocorrelation [13]. Temporal autocorrelation of initial retrievals has also been used to predict performance [9]. However, temporal autocorrelation is performed by projecting the retrieval function into the temporal embedding space. In our work, we focus on the behavior of the function over the relationships between documents. τ adjusted R2 DV KL P ρ(y, ˜y) DV KL P ρ(y, ˜y) DV KL, P DV KL, ρ(y, ˜y) Pρ(y, ˜y) β trec4 0.353 0.548 0.513 0.168 0.363 0.422 0.466 0.420 0.557 0.553 trec5 0.311 0.329 0.357 0.116 0.190 0.236 0.238 0.244 0.266 0.269 robust04 0.418 0.398 0.373 0.256 0.304 0.278 0.403 0.373 0.402 0.442 terabyte04 0.139 0.150 0.454 0.059 0.045 0.292 0.076 0.293 0.289 0.284 terabyte05 0.171 0.208 0.383 0.022 0.072 0.193 0.120 0.225 0.218 0.257 Table 1: Comparison to Robustness and Clarity measures for language model scores. Evaluation replicates experiments from [19]. We present correlations between the classic Clarity measure (DV KL), the ranking robustness measure (P), and autocorrelation (ρ(y, ˜y)) each with mean average precision in terms of Kendall"s τ. The adjusted coefficient of determination is presented to measure the effectiveness of combining predictors.
Measures in bold represent the strongest correlation for that test/collection pair. multiple run (a) (b) (c) τ τ adjusted R2 DKL ρ(y, ˜y) Dn KL ρ(y, yµ) ρ(˜y, yµ) Dn KL ρ(y, ˜y) ρ(y, yµ) ρ(˜y, yµ) β trec3 0.201 0.461 0.461 0.439 0.456 0.444 0.395 0.394 0.386 0.498 trec4 0.252 0.396 0.455 0.482 0.489 0.379 0.263 0.429 0.482 0.483 trec5 0.016 0.277 0.433 0.459 0.393 0.280 0.157 0.375 0.323 0.386 trec6 0.230 0.227 0.352 0.428 0.418 0.203 0.089 0.323 0.325 0.325 trec7 0.083 0.326 0.341 0.430 0.483 0.264 0.182 0.363 0.442 0.400 trec8 0.235 0.396 0.454 0.508 0.567 0.402 0.272 0.490 0.580 0.523 robust03 0.302 0.354 0.377 0.385 0.447 0.269 0.206 0.274 0.392 0.303 robust04 0.183 0.308 0.301 0.384 0.453 0.200 0.182 0.301 0.393 0.335 robust05 0.224 0.249 0.371 0.377 0.404 0.341 0.108 0.313 0.328 0.336 terabyte04 0.043 0.245 0.544 0.420 0.392 0.516 0.105 0.357 0.343 0.365 terabyte05 0.068 0.306 0.480 0.434 0.390 0.491 0.168 0.384 0.309 0.403 trec4-spanish 0.307 0.388 0.488 0.398 0.395 0.423 0.299 0.282 0.299 0.388 trec5-spanish 0.220 0.458 0.446 0.484 0.475 0.411 0.398 0.428 0.437 0.529 trec5-chinese 0.092 0.199 0.367 0.379 0.384 0.379 0.199 0.273 0.276 0.310 trec6-chinese 0.144 0.276 0.265 0.353 0.376 0.115 0.128 0.188 0.223 0.199 ent05 - 0.181 0.324 0.305 0.282 0.211 0.043 0.158 0.155 0.179 Table 2: Large scale prediction experiments. We predict the ranking of large sets of retrievals for various collections and retrieval systems. Kendall"s τ correlations are computed between the predicted ranking and a ranking based on the retrieval"s average precision. In column (a), we have predictors which do not use information from other retrievals for the same query. In columns (b) and (c) we present performance for predictors which incorporate information from multiple retrievals. The adjusted coefficient of determination is computed to determine effectiveness of combining predictors. Measures in bold represent the strongest correlation for that test/collection pair.
Finally, regularization-based re-ranking processes are also closely-related to our work [8]. These techniques seek to maximize the agreement between scores of related documents by solving a constrained optimization problem. The maximization of consistency is equivalent to maximizing the Moran autocorrelation. Therefore, we believe that our work provides explanation for why regularization-based re-ranking works.
We have presented a new method for predicting the performance of a retrieval ranking without any relevance judgments. We consider two cases. First, when making predictions in the absence of retrievals from other systems, our predictors demonstrate robust, strong correlations with average precision. This performance, combined with a simple implementation, makes our predictors, in particular, very attractive. We have demonstrated this improvement for many, diverse settings. To our knowledge, this is the first large scale examination of zero-judgment, single-retrieval performance prediction. Second, when provided retrievals from other systems, our extended methods demonstrate competitive performance with state of the art baselines. Our experiments also demonstrate the limits of the usefulness of our predictors when information from multiple runs is provided.
Our results suggest two conclusions. First, our results could affect retrieval algorithm design. Retrieval algorithms designed to consider spatial autocorrelation will conform to the cluster hypothesis and improve performance. Second, our results could affect the design of minimal test collection algorithms. Much of the recent work in ranking systems sometimes ignores correlations between document labels and scores. We believe that these two directions could be rewarding given the theoretical and experimental evidence in this paper.
This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the author"s and do not necessarily reflect those of the sponsor. We thank Yun Zhou and Desislava Petkova for providing data and Andre Gauthier for technical assistance.
[1] J. Aslam and V. Pavlu. Query hardness estimation using jensen-shannon divergence among multiple scoring functions. In ECIR 2007: Proceedings of the 29th European Conference on Information Retrieval, 2007. [2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In S. Dumais, E. N. Efthimiadis, D. Hawking, and K. Jarvelin, editors, Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 541-548. ACM Press, August
[3] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 390-397, New York, NY, USA, 2006. ACM Press. [4] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 268-275, New York, NY, USA, 2006. ACM Press. [5] A. D. Cliff and J. K. Ord. Spatial Autocorrelation. Pion Ltd., 1973. [6] M. Connell, A. Feng, G. Kumaran, H. Raghavan, C. Shah, and J. Allan. Umass at tdt 2004. Technical Report CIIR Technical Report IR - 357, Department of Computer Science, University of Massachusetts, 2004. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Precision prediction based on ranked list coherence. Inf. Retr., 9(6):723-755, 2006. [8] F. Diaz. Regularizing ad-hoc retrieval scores. In CIKM "05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 672-679,
New York, NY, USA, 2005. ACM Press. [9] F. Diaz and R. Jones. Using temporal profiles of queries for precision prediction. In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 18-24,
New York, NY, USA, 2004. ACM Press. [10] D. A. Griffith. Spatial Autocorrelation and Spatial Filtering. Springer Verlag, 2003. [11] B. He and I. Ounis. Inferring Query Performance Using Pre-retrieval Predictors. In The Eleventh Symposium on String Processing and Information Retrieval (SPIRE),
[12] N. Jardine and C. J. V. Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7:217-240, 1971. [13] D. Jensen and J. Neville. Linkage and autocorrelation cause feature selection bias in relational learning. In ICML "02: Proceedings of the Nineteenth International Conference on Machine Learning, pages 259-266, San Francisco, CA,
USA, 2002. Morgan Kaufmann Publishers Inc. [14] O. Kurland and L. Lee. Corpus structure, language models, and ad-hoc information retrieval. In SIGIR "04: Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 194-201, New York, NY, USA, 2004. ACM Press. [15] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In CIKM "01: Proceedings of the tenth international conference on Information and knowledge management, pages 427-433, New York, NY,
USA, 2001. ACM Press. [16] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A study of relevance propagation for web search. In SIGIR "05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 408-415, New York, NY, USA,
[17] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR "01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 66-73, New York, NY, USA, 2001. ACM Press. [18] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. Wood. On ranking the effectiveness of searches. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 398-404, New York, NY, USA, 2006. ACM Press. [19] Y. Zhou and W. B. Croft. Ranking robustness: a novel framework to predict query performance. In CIKM "06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 567-574,

Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering (AF) is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples (if available), an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. The incremental learning nature of AF systems makes them more powerful than standard search engines that support ad-hoc retrieval (e.g. Google and Yahoo) in terms of finding relevant information with respect to long-lasting topics of interest, and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs, without having to modify their queries manually.
A variety of supervised learning algorithms (Rocchio-style classifiers, Exponential-Gaussian models, local regression and logistic regression approaches) have been studied for adaptive settings, examined with explicit and implicit relevance feedback, and evaluated with respect to utility optimization on large benchmark data collections in TREC (Text Retrieval Conferences) and TDT (Topic Detection and Tracking) forums [1, 4, 7, 15, 16, 20, 24, 23]. Regularized logistic regression [21] has been found representative for the state-of-the-art approaches, and highly efficient for frequent model adaptations over large document collections such as the TREC-10 corpus (over 800,000 documents and 84 topics). Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications:
adaptive filtering setup - he or she reacts to the system only when the system makes a ‘yes" decision on a document, by confirming or rejecting that decision. A more ‘active" alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents (or passages) per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists.
The latter form of user interaction has been highly effective in standard retrieval for ad-hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research.
‘no") is restricted to the document level in conventional AF. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents.
redundant. A major news event, for example, would be reported by multiple sources repeatedly for a while, making most of the information content in those articles redundant with each other. A conventional AF system would select all these redundant news stories for user feedback, wasting the user"s time while offering little gain. Clearly, techniques for novelty detection can help in principle [25, 2, 22] for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to user"s (fine-grained) feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.
To address the above limitations of current AF systems, we propose and examine a new approach in this paper, combining the strengths of conventional AF (incremental learning of topic models), multi-pass passage retrieval for long-lasting queries conditioned on topic, and novelty detection for removal of redundancy from user interactions with the system. We call the new process utility-based information distillation.
Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus - the TDT4 collection of news stories and TV broadcasts - with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 .
To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG (Normalized Discounted Cumulated Gain) [9] for assessing the utility of ranked passages as a function of both relevance and novelty.
The rest of this paper is organized as follows. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAF´E - CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives.
Consider a news event - the escape of seven convicts from a Texas prison in December 2000 and their capture a month later. Assuming a user were interested in this event since its early stage, the information need could be: ‘Find information about the escape of convicts from Texas prison, and information related to their recapture". The associated lower-level questions could be:
prison?
We call such an information need a task, and the associated questions as the queries in this task. A distillation system is supposed to monitor the incoming documents, process them chunk by chunk in a temporal order, select potentially relevant and novel passages from each chunk with respect to each query, and present a ranked list of passages to the user. Passage ranking here is based on how relevant a passage is with respect to the current query, how novel it is with respect to the current user history (of his or her interactions with the system), and how redundant it is compared to other passages with a higher rank in the list.
When presented with a list of passages, the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant. These spans of text are taken as positive examples in the adaptation of the query profile, and also added to the user"s history. Passages not marked by the user are taken as negative examples. As soon as the query profile is updated, the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low, based on user preference. For example, if the user highlights ‘...officials have posted a $100,000 reward for their capture..." as relevant answer to the query What steps have been taken so far?, then the highlighted piece is used as an additional positive training example in the adaptation of the query profile. This piece of feedback is also added to the user history as a seen example, so that in future, the system will not place another passage mentioning ‘$100,000 reward" at the top of the ranked list. However, an article mentioning ‘...officials have doubled the reward money to $200,000..." might be ranked high since it is both relevant to the (updated) query profile and novel with respect to the (updated) user history. The user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly. Clearly, novelty detection is very important for the utility of such a system because of the iterative search. Without novelty detection, the old relevant passages would be shown to the user repeatedly in each ranked list.
Through the above example, we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents. Our framework combines and extends the power of adaptive filtering (AF), ad-hoc retrieval (IR) and novelty detection (ND). Compared to standard IR, our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task. Compared to conventional AF, it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.
Compared to past work, this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages. The combination of AF, IR and ND with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system? Existing metrics in standard IR, AF and ND are insufficient, and new solutions must be explored, as we will discuss in Section 4, after describing the technical cores of our system in the next section.
The core components of CAF´E are - 1) AF for incremental learning of query profiles, 2) IR for estimating relevance of passages with respect to query profiles, 3) ND for assessing novelty of passages with respect to user"s history, and 4) anti-redundancy component to remove redundancy from ranked lists.
We use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for AF [21].
Logistic regression (LR) is a supervised learning algorithm for statistical classification. Based on a training set of labeled instances, it learns a class model which can then by used to predict the labels of unseen instances. Its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required, as is the case in adaptive filtering, where the system must learn from each new feedback provided by the user. (See [21] and [23] for computational complexity and implementation issues).
In adaptive filtering, each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query. For training the model, we use the query itself as the initial positive training example of the class, and the user-highlighted pieces of text (marked as Relevant or Not-relevant) during feedback as additional training examples. To address the cold start issue in the early stage before any user feedback is obtained, the system uses a small sample from a retrospective corpus as the initial negative examples in the training set. The details of using logistic regression for adaptive filtering (assigning different weights to positive and negative training instances, and regularizing the objective function to prevent over-fitting on training data) are presented in [21].
The class model w∗ learned by Logistic Regression, or the query profile, is a vector whose dimensions are individual terms and whose elements are the regression coefficients, indicating how influential each term is in the query profile.
The query profile is updated whenever a new piece of user feedback is received. A temporally decaying weight can be applied to each training example, as an option, to emphasize the most recent user feedback.
We use standard IR techniques in this part of our system.
Incoming documents are processed in chunks, where each chunk can be defined as a fixed span of time or as a fixed number of documents, as preferred by the user. For each incoming document, corpus statistics like the IDF (Inverted Document Frequency) of each term are updated. We use a state-of-the-art named entity identifier and tracker [8, 12] to identify person and location names, and merge them with co-referent named entities seen in the past. Then the documents are segmented into passages, which can be a whole document, a paragraph, a sentence, or any other continuous span of text, as preferred. Each passage is represented using a vector of TF-IDF (Term FrequencyInverse Document Frequency) weights, where term can be a word or a named entity.
Given a query profile, i.e. the logistic regression solution w∗ as described in Section 3.1, the system computes the posterior probability of relevance for each passage x as fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1) Passages are ordered by their relevance scores, and the ones with scores above a threshold (tuned on a training set) comprise the relevance list that is passed on to the novelty detection step.
CAF´E maintains a user history H(t), which contains all the spans of text hi that the user highlighted (as feedback) during his or her past interactions with the system, up to the current time t. Denoting the history as H(t) = n h1, h2, ..., ht o , (2) the novelty score of a new candidate passage x is computed as: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3) where both candidate passage x and highlighted spans of text hi are represented as TF-IDF vectors.
The novelty score of each passage is compared to a prespecified threshold (also tuned on a training set), and any passage with a score below this threshold is removed from the relevance list.
Although the novelty detection component ensures that only novel (previously unseen) information remains in the relevance list, this list might still contain the same novel information at multiple positions in the ranked list.
Suppose, for example, that the user has already read about a $100,000 reward for information about the escaped convicts.
A new piece of news that the award has been increased to $200,000 is novel since the user hasn"t read about it yet.
However, multiple news sources would report this news and we might end up showing (redundant) articles from all these sources in a ranked list. Hence, a ranked list should also be made non-redundant with respect to its own contents. We use a simplified version of the Maximal Marginal Relevance method [5], originally developed for combining relevance and novelty in text retrieval and summarization. Our procedure starts with the current list of passages sorted by relevance (section 3.2), filtered by Novelty Detection component (section 3.3), and generates a new non-redundant list as follows:
in the new list.
list only if fAR(x) > t where fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} and Lnew is the set of passages already selected in the new list.
have been examined.
After applying the above-mentioned algorithm, each passage in the new list is sufficiently dissimilar to others, thus favoring diversity rather than redundancy in the new ranked list. The anti-redundancy threshold t is tuned on a training set.
The approach we proposed above for information distillation raises important issues regarding evaluation methodology. Firstly, since our framework allows the output to be passages at different levels of granularity (e.g. k-sentence windows where k may vary) instead of a fixed length, it is not possible to have pre-annotated relevance judgments at all such granularity levels. Secondly, since we wish to measure the utility of the system output as a combination of both relevance and novelty, traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time. Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists. None of the existing measures in ad-hoc retrieval, adaptive filtering, novelty detection or other related areas (text summarization and question answering) have desirable properties in all the three aspects. Therefore, we must develop a new evaluation methodology.
To enable the evaluation of a system whose output consists of passages of arbitrary length, we borrow the concept of answer keys from the Question Answering (QA) community, where systems are allowed to return arbitrary spans of text as answers. Answer keys define what should be present in a system response to receive credit, and are comprised of a collection of information nuggets, i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them.
Defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping [19], since system-returned passages can contain the same information expressed in many different ways. Hence,
QA evaluations have relied on human assessors for the mapping between various expressions, making the process costly, time consuming, and not scalable to large query and document collections, and extensive system evaluations with various parameter settings.
Automatic evaluation methods would allow for faster system building and tuning, as well as provide an objective and affordable way of comparing various systems. Recently, such methods have been proposed, more or less, based on the idea of n-gram co-occurrences. Pourpre [10] assigns a fractional recall score to a system response based on its unigram overlap with a given nugget"s description. For example, a system response ‘A B C" has recall 3/4 with respect to a nugget with description ‘A B C D". However, such an approach is unfair to systems that present the same information but using words other than A, B, C, and D.
Another open issue is how to weight individual words in measuring the closeness of a match. For example, consider the question How many prisoners escaped?. In the nugget ‘Seven prisoners escaped from a Texas prison", there is no indication that ‘seven" is the keyword, and that it must be matched to get any relevance credit. Using IDF values does not help, since ‘seven" will generally not have a higher IDF than words like ‘texas" and ‘prison". Also, redefining the nugget as just ‘seven" does not solve the problem since now it might spuriously match any mention of ‘seven" out of context. Nuggeteer [13] works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold. However, it is also plagued by ‘spurious relevance" since not all words contained in the nugget description (or known correct responses) are central to the nugget.
We propose a reliable automatic method for determining whether a snippet of text contains a given nugget, based on nugget-matching rules, which are generated using a semiautomatic procedure explained below. These rules are essentially Boolean queries that will only match against snippets that contain the nugget. For instance, a candidate rule for matching answers to How many prisoners escaped? is (Texas AND seven AND escape AND (convicts OR prisoners)), possibly with other synonyms and variants in the rule. For a corpus of news articles, which usually follow a typical formal prose, it is fairly easy to write such simple rules to match expected answers using a bootstrap approach, as described below.
We propose a two-stage approach, inspired by Autoslog [14], that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents. In the first stage, human subjects annotated (using a highlighting tool) portions of ontopic documents that contained answers to each nugget 2 .
In the second stage, subjects used our rule generation tool to create rules that would match the annotations for each nugget. The tool allows users to enter a Boolean rule as a disjunction of conjunctions (e.g. ((a AND b) OR (a AND c AND d) OR (e))). Given a candidate rule, our tool uses it as a Boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match. Hence, the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision. We observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions (adding more ANDs), and improving the recall by adding more conjunctions to the disjunction (adding more ORs).
As an example, let"s try to create a rule for the nugget which says that seven prisoners escaped from the Texas prison. We start with a simple rule - (seven). When we input this into the rule generation tool, we realize that this rule matches many spurious occurrences of seven (e.g. ‘...seven states...") and thus gets a low precision score.
We can further qualify our rule - Texas AND seven AND convicts. Next, by looking at the ‘missed annotations", we realize that some news articles mentioned ...seven prisoners escaped.... We then replace convicts with the disjunction (convicts OR prisoners). We continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the (small number of) misses and false alarms can be safely ignored.
Thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget, while avoiding matching incorrect (or out of context) responses. Human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably.
Ranked Lists The utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information, and how much the user lost in terms of time and energy. We calculate this utility from the utilities of individual passages as follows. After reading each passage returned by the system, the user derives some gain depending on the presence of relevant and novel information, and incurs a loss in terms of the time and energy spent in going through the passage. However, the likelihood that the user would actually read a passage depends on its position in the ranked list. Hence, for a query q, the expected utility 2 LDC [18] already provides relevance judgments for 100 topics on the TDT4 corpus. We further ensured that these judgments are exhaustive on the entire corpus using pooling. of a passage pi at rank i can be defined as U(pi, q) = P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (4) where P(i) is the probability that the user would go through a passage at rank i.
The expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage: U(q) = nX i=1 P(i) ∗ (Gain(pi, q) − Loss(pi, q)) (5) Note that if we ignore the loss term and define P(i) as P(i) ∝ 1/ logb(b + i − 1) (6) then we get the recently popularized metric called Discounted Cumulated Gain (DCG) [9], where Gain(pi, q) is defined as the graded relevance of passage pi. However, without the loss term, DCG is a purely recall-oriented metric and not suitable for an adaptive filtering setting, where the system"s utility depends in part on its ability to limit the number of items shown to the user.
Although P(i) could be defined based on empirical studies of user behavior, for simplicity, we use P(i) exactly as defined in equation 6.
The gain G(pi, q) of passage pi with respect to the query q is a function of - 1) the number of relevant nuggets present in pi, and 2) the novelty of each of these nuggets. We combine these two factors as follows. For each nugget Nj, we assign an initial weight wj, and also keep a count nj of the number of times this nugget has been seen by the user in the past.
The gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ.
Thus, G(pi, q) is defined as G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) where C(pi, q) is the set of all nuggets that appear in passage pi and also belong to the answer key of query q. The initial weights wj are all set of be 1.0 in our experiments, but can also be set based on a pyramid approach [11].
The choice of dampening factor γ determines the user"s tolerance for redundancy. When γ = 0, a nugget will only receive credit for its first occurrence i.e. when nj is zero3 . For 0 < γ < 1, a nugget receives smaller credit for each successive occurrence. When γ = 1, no dampening occurs and repeated occurrences of a nugget receive the same credit. Note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system, since the users are expected to remember what the system showed them in the past.
We define the loss L(pi, q) as a constant cost c (we use 0.1) incurred when reading a system-returned passage. Thus, our metric can be re-written as U(q) = nX i=1 Gain(pi, q) logb(b + i − 1) − L(n) (8) where L(n) is the loss associated with a ranked list of length n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Note that 00 = 1 Due to the similarity with Discounted Cumulated Gain (DCG), we call our metric Discounted Cumulated Utility (DCU). The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage.
TDT4 was the benchmark corpus used in TDT2002 and TDT2003 evaluations. The corpus consists of over 90, 000 news articles from multiple sources (AP, NYT, CNN, ABC,
NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) published between October 2000 and January 2001, in the languages of Arabic, English, and Mandarin.
Speech-recognized and machine-translated versions of the non-English articles were provided as well.
LDC [18] has annotated the corpus with 100 topics, that correspond to various news events in this time period. Out of these, we selected a subset of 12 actionable events, and defined corresponding tasks for them4 . For each task, we manually defined a profile consisting of an initial set of (5 to 10) queries, a free-text description of the user history, i.e., what the user already knows about the event, and a list of known on-topic and off-topic documents (if available) as training examples.
For each query, we generated answer keys and corresponding nugget matching rules using the procedure described in section 4.1.2, and produced a total of 120 queries, with an average of 7 nuggets per query.
We used Indri [17], a popular language-model based retrieval engine, as a baseline for comparison with CAF´E.
Indri supports standard search engine functionality, including pseudo-relevance feedback (PRF) [3, 6], and is representative of a typical query-based retrieval system.
Indri does not support any kind of novelty detection.
We compare Indri with PRF turned on and off, against CAF´E with user feedback, novelty detection and antiredundant ranking turned on and off.
We divided the TDT4 corpus spanning 4 months into 10 chunks, each defined as a period of 12 consecutive days.
At any given point of time in the distillation process, each system accessed the past data up to the current point, and returned a ranked list of up 50 passages per query.
The 12 tasks defined on the corpus were divided into a training and test set with 6 tasks each. Each system was allowed to use the training set to tune its parameters for optimizing NDCU (equation 8), including the relevance threshold for both Indri and CAF´E, and the novelty and antiredundancy thresholds for CAF´E.
The NDCU for each system run is calculated automatically. User feedback was also simulated - relevance judgments for each system-returned passage (as determined by the nugget matching rules described in section 4.1.2) were 4 URL: http://nyc.lti.cs.cmu.edu/downloads Figure 1: Performance of Indri across chunks Figure 2: Performance of CAF´E across chunks used as user feedback in the adaptation of query profiles and user histories.
In Table 1, we show the NDCU scores of the two systems under various settings. These scores are averaged over the six tasks in the test set, and are calculated with two dampening factors (see section 4.2): γ = 0 and 0.1, to simulate no tolerance and small tolerance for redundancy, respectively.
Using γ = 0 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information. Hence, the improvement obtained from enabling user feedback is smaller with γ = 0 than the improvement obtained from feedback with γ = 0.1. This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information.
However, the user does not benefit from seeing such redundant passages, and is usually interested in other passages containing related information. It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.
Sometimes, however, users might indeed be interested in seeing the same information from multiple sources, as an Table 1: NDCU Scores of Indri and CAF´E for two dampening factors (γ), and various settings (F: Feedback,
N: Novelty Detection, A: Anti-Redundant Ranking) Indri CAF´E γ Base +PRF Base +F +F+N +F+A +F+N+A 0 0.19 0.19 0.22 0.23 0.24 0.24 0.24
indicator of its importance or reliability. In such a case, we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy, and hence let the system tune its parameters accordingly.
Since documents were processed chunk by chunk, it would be interesting to see how the performance of systems improves over time. Figures 1 and 2 show the performance trends for both the systems across chunks. While the performance with and without feedback on the first few chunks is expected to be close, for subsequent chunks, the performance curve with feedback enabled rises above the one with the no-feedback setting. The performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks, making some queries ‘easier" than others in certain chunks.
Moreover, since Indri uses pseudo-relevance feedback while CAF´E uses feedback based on actual relevance judgments, the improvement in case of Indri is less dramatic than that of CAF´E.
This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAF´E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off.
We would like to thank Rosta Farzan, Jonathan Grady,
Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments. This work is supported in parts by the National Science Foundation (NSF) under grant IIS0434035, and the Defense Advanced Research Project Agency (DARPA) under contracts NBCHD030010 and W0550432. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡
[1] J. Allan. Incremental Relevance Feedback for Information Filtering. Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 270-278,
[2] J. Allan, C. Wade, and A. Bolivar. Retrieval and Novelty Detection at the Sentence Level. Proceedings of the ACM SIGIR conference on research and development in information retrieval, 2003. [3] C. Buckley, G. Salton, and J. Allan. Automatic Retrieval with Locality Information using SMART.
NIST special publication, (500207):59-72, 1993. [4] J. Callan. Learning While Filtering Documents.
Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 224-231, 1998. [5] J. Carbonell and J. Goldstein. The use of MMR,
Diversity-based Reranking for Reordering Documents and Producing Summaries. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335-336, 1998. [6] E. Efthimiadis. Query Expansion. Annual Review of Information Science and Technology (ARIST), 31:p121-87, 1996. [7] J. Fiscus and G. Duddington. Topic Detection and Tracking Overview. Topic Detection and Tracking: Event-based Information Organization, pages 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. A Statistical Model for Multilingual Entity Detection and Tracking. NAACL/HLT, 2004. [9] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated Gain-based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin and D. Demner-Fushman. Automatically Evaluating Answers to Definition Questions.
Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005),
∗ Statistics Dept., Purdue University, West Lafayette, USA † Language Technologies Inst., Carnegie Mellon University,
Pittsburgh, USA ‡ School of Information Sciences, Univ. of Pittsburgh,
Pittsburgh, USA [11] J. Lin and D. Demner-Fushman. Will Pyramids Built of nUggets Topple Over. Proceedings of HLT-NAACL,
[12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree. Proc. of ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments.
HLT/NAACL, 2006. [14] E. Riloff. Automatically Constructing a Dictionary for Information Extraction Tasks. Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, 1993. [15] S. Robertson and S. Walker. Microsoft Cambridge at TREC-9: Filtering track. The Ninth Text REtrieval Conference (TREC-9), pages 361-368. [16] R. Schapire, Y. Singer, and A. Singhal. Boosting and Rocchio Applied to Text Filtering. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle, and W. Croft.
Indri: A Language Model-based Search Engine for Complex Queries. Proceedings of the International Conference on Intelligence Analysis, 2004. [18] The Linguistic Data Consortium. http://www.ldc.upenn.edu/. [19] E. Voorhees. Overview of the TREC 2003 Question Answering Track. Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), 2003. [20] Y. Yang and B. Kisiel. Margin-based Local Regression for Adaptive Filtering. Proceedings of the twelfth international conference on Information and knowledge management, pages 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 98-105,
[22] C. Zhai, W. Cohen, and J. Lafferty. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-17, 2003. [23] J. Zhang and Y. Yang. Robustness of Regularized Linear Classification Methods in Text Categorization.
Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 190-197, 2003. [24] Y. Zhang. Using Bayesian Priors to Combine Classifiers for Adaptive Filtering. Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 345-352,
[25] Y. Zhang, J. Callan, and T. Minka. Novelty and Redundancy Detection in Adaptive Filtering.

A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean,
Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request. The result lists produced by these approaches depend on the exact definition of the relevance concept.
Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking. Such approaches give rise to metasearch engines in the Web context.
We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores. This corresponds indeed to the reality, where only ordinal information is available.
Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].
Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods. For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant. Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents. Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances. These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22]. Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].
The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].
Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.
They also give rigid interpretation to the exact ranking of the items. Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.
The remaining of the paper is organized as follows. We first review current rank aggregation methods in Section 2.
Then we outline the specificities of the data fusion problem in the IR context (Section 3). In Section 4, we present a new aggregation method which is proven to best fit the IR context. Experimental results are presented in Section 5 and conclusions are provided in a final section.
As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked. These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature.
We first introduce some basic notations to present the rank aggregation methods in a uniform way. Let D = {d1, d2, . . . , dnd } be a set of nd documents. A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).
Thus, di j di means di ‘is ranked better than" di in j.
When Dj = D, j is said to be a full list. Otherwise, it is a partial list. If di belongs to Dj, rj i denotes the rank or position of di in j. We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|. Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n). Restricting PR to the rankings containing document di defines PRi. We also call the number of rankings which contain document di the rank hits of di [19].
The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking.
This method [5] first assigns a score n j=1 rj i to each document di. Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily.
This family of methods basically combine scores of documents. When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].
For instance, Callan et al. [6] used the inference networks model [30] to combine rankings. Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.
The first three operators correspond to the sum, min and max operators, respectively. CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits. It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.
Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings.
In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].
Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |. It extends to several lists as follows. Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).
Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i . This formulation has the advantage that it considers the intensity of preferences.
This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance. During the training process, probabilities of relevance are calculated. For subsequent queries, documents are ranked based on these probabilities.
For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j). For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k . Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores. Training data is needed to infer the model parameters.
The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest. Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di ‘should be ranked better than" di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.
The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.
Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26].
As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance. Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists. It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem.
Markov chains (MCs) have been used by Dwork et al. [11] as a ‘natural" method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event. In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D.
If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.
The consensus ranking corresponds to the stationary distribution of MC4.
AGGREGATION PROBLEM IN THE IR CONTEXT
The exact positions of documents in one input ranking have limited significance and should not be overemphasized.
For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value. Indeed, in the IR context, the complete order provided by an input method may hide ties. In this case, we call such rankings semi orders. This was outlined in [13] as the problem of aggregation with ties. It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance. Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases.
In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists. This was outlined in [14] as the problem of having to merge top-k results from various input lists. For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.
Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions:
long lists. We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.
H1 all: We consider all the documents from each input ranking.
rankings, we must decide which documents should be kept in the consensus ranking. Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).
H2 all: We consider all the documents which are ranked in at least one input ranking.
Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents. We also call a candidate document which is missing in one or more rankings, a missing document.
some input rankings. Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available. We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.
H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document.
k holds, each input ranking may contain documents which will not be considered in the consensus ranking. Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.
H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.
In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties.
AGGREGATION
Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information. This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths. Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results. For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents. Whether we assign to each of the missing documents the position 1, 501, 750 or
yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.
Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings. Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties. Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.
Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion. Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).
Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.
The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .
Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined. They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.
A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv. This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n). It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].
Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist. Therefore, we need specific procedures in order to derive a consensus ranking. We propose the following procedure which finds its roots in [27]. It consists in partitioning the set of documents into r ranked classes.
Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed. Documents within the same equivalence class are ranked arbitrarily.
Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered ‘worse" than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered ‘better" than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .
Each class Ch results from a distillation process. It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure:
to Sk , i.e. sk(di, Ek−1),
When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1. When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1.
This section illustrates the concepts and procedures of section 4.1. Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}. The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).
Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively. The following tables give the concordance, discordance and outranking matrices. Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.
Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.
Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.
Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix. The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 . They are respectively 2, 2, 2, -2 and -4. Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.
Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated. At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \ C1 = {d4, d5}. They are respectively 1 and -1. So C3 = E1 = {d4}. The last document d5 is the only document of the last class C3. Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}.
To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation. In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].
In this task, there are 75 topics where only a short description of each is given. For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams. The performances of these runs are reported in table 3.
Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents. The number of documents retrieved by all these runs ranges from 543 to 5769. Their average (median) number is 3340 (3386). It is worth noting that we found similar distributions of the documents among the rankings as in [11].
For evaluation, we used the ‘trec eval" standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.
Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms. In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs. In the tables of the following section, statistically significant differences are marked with an asterisk. Values between brackets of the first column of each table, indicate the parameter value of the corresponding run.
We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.
We set our basic run mcm with the following parameters.
We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%). Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking. They consequently may vary from one ranking to another. In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0). Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi . Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).
To test the run mcm, we had chosen the following assumptions. We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2
assumed H3 no and H4 new. In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.
Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters. This was validated by preliminary experiments.
Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.
Afterwards, we study the impact of tuning parameters. Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms.
Table 4 summarizes the performance variation of the outranking approach under different working hypotheses. In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.
Moreover, S@1 moves from 41.33% to 34.67% (-16.11%). This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change. We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings. Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.
Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar. Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.
From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.
Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances. This result was predictable since in both cases we have more detailed information on the relative importance of documents.
Tables 5 and 6 confirm this evidence. Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase. It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.
Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .
Values between brackets are rank hits. For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful. This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded. Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.
Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds.
Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.
We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%. Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%. We can thus conclude that the input rankings are semi orders rather than complete orders.
Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold. We can conclude that in order to put document di before di in the consensus ranking,
Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant. Performance drops significantly for very low and very high values of the concordance threshold. In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively. Therefore, the outranking relation becomes either too weak or too strong respectively.
Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures. In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily. Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold. For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased.
Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered. Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].
Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking. Therefore, when they are considered, performance decreases.
Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00%
Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies. We also examined the performance of one majoritarian method which is the Markov chain method (MC4). For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.
The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2
new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.
For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no. The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2
new), i.e. changing the number of retained documents from 100 to 1000. The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking. The fourth row corresponds to the assumption set A4 = (H1 100, H2
init), i.e. keeping the original ranks of successful documents.
The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered. Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track. This set of runs aims to show whether relative performance of the various methods is task-dependent.
The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row. This aims to show whether relative performance of the various methods changes from year to year.
Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.
Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average). This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance. For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%. This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150. This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all. This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ. It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant.
In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused. We noticed that the input rankings can hide ties, so they should not be considered as complete orders. Only robust information should be used from each input ranking.
Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings. They should be adapted by considering specific working assumptions.
We propose a new outranking method for rank aggregation which is well adapted to the IR context. Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document. There is also no need to make specific assumptions on the positions of the missing documents. This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.
Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies. It also out-performs a good performing majoritarian methods which is the Markov chain method. These results are tested against different test collections and queries. From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.
The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.
Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.
Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper.

The rapidly increasing amount of spoken data calls for solutions to index and search this data.
The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition (LVCSR) tool. In the past decade, most of the research efforts on spoken data retrieval have focused on extending classical IR techniques to word transcripts. Some of these works have been done in the framework of the NIST TREC Spoken Document Retrieval tracks and are described by Garofolo et al. [12]. These tracks focused on retrieval from a corpus of broadcast news stories spoken by professionals. One of the conclusions of those tracks was that the effectiveness of retrieval mostly depends on the accuracy of the transcripts. While the accuracy of automatic speech recognition (ASR) systems depends on the scenario and environment, state-of-the-art systems achieved better than 90% accuracy in transcription of such data. In 2000,
Garofolo et al. concluded that Spoken document retrieval is a solved problem [12].
However, a significant drawback of such approaches is that search on queries containing out-of-vocabulary (OOV) terms will not return any results. OOV terms are missing words from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable, given the recognition acoustic model and the language model. It has been experimentally observed that over 10% of user queries can contain OOV terms [16], as queries often relate to named entities that typically have a poor coverage in the ASR vocabulary. The effects of OOV query terms in spoken data retrieval are discussed by Woodland et al. [28].
In many applications the OOV rate may get worse over time unless the recognizer"s vocabulary is periodically updated.
Another approach consists of converting the speech to phonetic transcripts and representing the query as a sequence of phones. The retrieval is based on searching the sequence of phones representing the query in the phonetic transcripts. The main drawback of this approach is the inherent high error rate of the transcripts. Therefore, such approach cannot be an alternative to word transcripts, especially for in-vocabulary (IV) query terms that are part of the vocabulary of the ASR system.
A solution would be to combine the two different approaches presented above: we index both word transcripts and phonetic transcripts; during query processing, the information is retrieved from the word index for IV terms and from the phonetic index for OOV terms. We would like to be able to process also hybrid queries, i.e, queries that include both IV and OOV terms. Consequently, we need to merge pieces of information retrieved from word index and phonetic index. Proximity information on the occurrences of the query terms is required for phrase search and for proximity-based ranking. In classical IR, the index stores for each occurrence of a term, its offset. Therefore, we cannot merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable.
The only element of comparison between phonetic and word transcripts are the timestamps. No previous work combining word and phonetic approach has been done on phrase search. We present a novel scheme for information retrieval that consists of storing, during the indexing process, for each unit of indexing (phone or word) its timestamp. We search queries by merging the information retrieved from the two different indices, word index and phonetic index, according to the timestamps of the query terms. We analyze the retrieval effectiveness of this approach on the NIST Spoken Term Detection 2006 evaluation data [1].
The paper is organized as follows. We describe the audio processing in Section 2. The indexing and retrieval methods are presented in section 3. Experimental setup and results are given in Section 4. In Section 5, we give an overview of related work. Finally, we conclude in Section 6.
SYSTEM We use an ASR system for transcribing speech data. It works in speaker-independent mode. For best recognition results, a speaker-independent acoustic model and a language model are trained in advance on data with similar characteristics.
Typically, ASR generates lattices that can be considered as directed acyclic graphs. Each vertex in a lattice is associated with a timestamp and each edge (u, v) is labeled with a word or phone hypothesis and its prior probability, which is the probability of the signal delimited by the timestamps of the vertices u and v, given the hypothesis. The 1-best path transcript is obtained from the lattice using dynamic programming techniques.
Mangu et al. [18] and Hakkani-Tur et al. [13] propose a compact representation of a word lattice called word confusion network (WCN). Each edge (u, v) is labeled with a word hypothesis and its posterior probability, i.e., the probability of the word given the signal. One of the main advantages of WCN is that it also provides an alignment for all of the words in the lattice. As explained in [13], the three main steps for building a WCN from a word lattice are as follows:
word lattice.
the 1-best, the longest or any random path), and call it the pivot path of the alignment.
with the pivot, merging the transitions that correspond to the same word (or label) and occur in the same time interval by summing their posterior probabilities.
The 1-best path of a WCN is obtained from the path containing the best hypotheses. As stated in [18], although WCNs are more compact than word lattices, in general the 1-best path obtained from WCN has a better word accuracy than the 1-best path obtained from the corresponding word lattice.
Typical structures of a lattice and a WCN are given in Figure 1.
Figure 1: Typical structures of a lattice and a WCN.
The main problem with retrieving information from spoken data is the low accuracy of the transcription particularly on terms of interest such as named entities and content words. Generally, the accuracy of a word transcript is characterized by its word error rate (WER). There are three kinds of errors that can occur in a transcript: substitution of a term that is part of the speech by another term, deletion of a spoken term that is part of the speech and insertion of a term that is not part of the speech.
Substitutions and deletions reflect the fact that an occurrence of a term in the speech signal is not recognized. These misses reduce the recall of the search. Substitutions and insertions reflect the fact that a term which is not part of the speech signal appears in the transcript. These misses reduce the precision of the search.
Search recall can be enhanced by expanding the transcript with extra words. These words can be taken from the other alternatives provided by the WCN; these alternatives may have been spoken but were not the top choice of the ASR.
Such an expansion tends to correct the substitutions and the deletions and consequently, might improve recall but will probably reduce precision. Using an appropriate ranking model, we can avoid the decrease in precision. Mamou et al. have presented in [17] the enhancement in the recall and the MAP by searching on WCN instead of considering only the 1-best path word transcript in the context of spoken document retrieval. We have adapted this model of IV search to term detection. In word transcripts, OOV terms are deleted or substituted. Therefore, the usage of phonetic transcripts is more desirable. However, due to their low accuracy, we have preferred to use only the 1-best path extracted from the phonetic lattices. We will show that the usage of phonetic transcripts tends to improve the recall without affecting the precision too much, using an appropriate ranking.
As stated in the STD 2006 evaluation plan [2], the task consists in finding all the exact matches of a specific query in a given corpus of speech data. A query is a phrase containing several words. The queries are text and not speech.
Note that this task is different from the more classical task of spoken document retrieval. Manual transcripts of the speech are not provided but are used by the evaluators to find true occurrences. By definition, true occurrences of a query are found automatically by searching the manual transcripts using the following rule: the gap between adjacent words in a query must be less than 0.5 seconds in the corresponding speech. For evaluating the results, each system output occurrence is judged as correct or not according to whether it is close in time to a true occurrence of the query retrieved from manual transcripts; it is judged as correct if the midpoint of the system output occurrence is less than or equal to 0.5 seconds from the time span of a true occurrence of the query.
We have used the same indexing process for WCN and phonetic transcripts. Each occurrence of a unit of indexing (word or phone) u in a transcript D is indexed with the following information: • the begin time t of the occurrence of u, • the duration d of the occurrence of u.
In addition, for WCN indexing, we store • the confidence level of the occurrence of u at the time t that is evaluated by its posterior probability Pr(u|t, D), • the rank of the occurrence of u among the other hypotheses beginning at the same time t, rank(u|t, D).
Note that since the task is to find exact matches of the phrase queries, we have not filtered stopwords and the corpus is not stemmed before indexing.
In the following, we present our approach for accomplishing the STD task using the indices described above. The terms are extracted from the query. The vocabulary of the ASR system building word transcripts is given. Terms that are part of this vocabulary are IV terms; the other terms are OOV. For an IV query term, the posting list is extracted from the word index. For an OOV query term, the term is converted to a sequence of phones using a joint maximum entropy N-gram model [10]. For example, the term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy). The posting list of each phone is extracted from the phonetic index.
The next step consists of merging the different posting lists according to the timestamp of the occurrences in order to create results matching the query. First, we check that the words and phones appear in the right order according to their begin times. Second, we check that the gap in time between adjacent words and phones is reasonable.
Conforming to the requirements of the STD evaluation, the distance in time between two adjacent query terms must be less than
in time between two adjacent phones of a query term is less that 0.2 seconds; this value has been determined empirically.
In such a way, we can reduce the effect of insertion errors since we allow insertions between the adjacent words and phones. Our query processing does not allow substitutions and deletions.
Example: Let us consider the phrase query prosody research. The term prosody is OOV and the term research is IV. The term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy). The posting list of each phone is extracted from the phonetic index. We merge the posting lists of the phones such that the sequence of phones appears in the right order and the gap in time between the pairs of phones (p, r), (r, aa), (aa, z), (z, ih), (ih, d), (d, iy) is less than 0.2 seconds. We obtain occurrences of the term prosody. The posting list of research is extracted from the word index and we merge it with the occurrences found for prosody such that they appear in the right order and the distance in time between prosody and research is less than
Note that our indexing model allows to search for different types of queries:
index.
using the word index for IV terms and the phonetic index for OOV terms; for query processing, the different sets of matches are unified if the query terms have OR semantics and intersected if the query terms have AND semantics.
query processing, the posting lists of the IV terms retrieved from the word index are merged with the posting lists of the OOV terms retrieved from the phonetic index. The merging is possible since we have stored the timestamps for each unit of indexing (word and phone) in both indices.
The STD evaluation has focused on the fourth query type.
It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices.
Since IV terms and OOV terms are retrieved from two different indices, we propose two different functions for scoring an occurrence of a term; afterward, an aggregate score is assigned to the query based on the scores of the query terms.
Because the task is term detection, we do not use a document frequency criterion for ranking the occurrences.
Let us consider a query Q = (k0, ..., kn), associated with a boosting vector B = (B1, ..., Bj). This vector associates a boosting factor to each rank of the different hypotheses; the boosting factors are normalized between 0 and 1. If the rank r is larger than j, we assume Br = 0.
For IV term ranking, we extend the work of Mamou et al. [17] on spoken document retrieval to term detection. We use the information provided by the word index. We define the score score(k, t, D) of a keyword k occurring at a time t in the transcript D, by the following formula: score(k, t, D) = Brank(k|t,D) × Pr(k|t, D) Note that 0 ≤ score(k, t, D) ≤ 1.
For OOV term ranking, we use the information provided by the phonetic index. We give a higher rank to occurrences of OOV terms that contain phones close (in time) to each other. We define a scoring function that is related to the average gap in time between the different phones. Let us consider a keyword k converted to the sequence of phones (pk
l ). We define the normalized score score(k, tk
of a keyword k = (pk
l ), where each pk i occurs at time tk i with a duration of dk i in the transcript D, by the following formula: score(k, tk
l i=1 5 × (tk i − (tk i−1 + dk i−1)) l Note that according to what we have ex-plained in Section 3.3, we have ∀1 ≤ i ≤ l, 0 < tk i − (tk i−1 + dk i−1) <
i − (tk i−1 + dk i−1)) < 1, and consequently,
occurrence is tk l − tk
l .
Example: let us consider the sequence (p, r, aa, z, ih, d, iy) and two different occurrences of the sequence.
For each phone, we give the begin time and the duration in second.
Occurrence 1: (p, 0.25, 0.01), (r, 0.36, 0.01), (aa, 0.37, 0.01), (z, 0.38, 0.01), (ih, 0.39, 0.01), (d, 0.4, 0.01), (iy, 0.52, 0.01).
Occurrence 2: (p, 0.45, 0.01), (r, 0.46, 0.01), (aa, 0.47, 0.01), (z, 0.48, 0.01), (ih, 0.49, 0.01), (d, 0.5, 0.01), (iy, 0.51, 0.01).
According to our formula, the score of the first occurrence is 0.83 and the score of the second occurrence is 1. In the first occurrence, there are probably some insertion or silence between the phone p and r, and between the phone d and iy.
The silence can be due to the fact that the phones belongs to two different words ans therefore, it is not an occurrence of the term prosody.
The score of an occurrence of a query Q at time t0 in the document D is determined by the multiplication of the score of each keyword ki, where each ki occurs at time ti with a duration di in the transcript D: score(Q, t0, D) = n i=0 score(ki, ti, D)γn Note that according to what we have ex-plained in Section 3.3, we have ∀1 ≤ i ≤ n, 0 < ti −(ti−1 +di−1) < 0.5 sec.
Our goal is to estimate for each found occurrence how likely the query appears. It is different from classical IR that aims to rank the results and not to score them. Since the probability to have a false alarm is inversely proportional to the length of the phrase query, we have boosted the score of queries by a γn exponent, that is related to the number of keywords in the phrase. We have determined empirically the value of γn = 1/n.
The begin time of the query occurrence is determined by the begin time t0 of the first query term and the duration of the query occurrence by tn − t0 + dn.
Our corpus consists of the evaluation set provided by NIST for the STD 2006 evaluation [1]. It includes three different source types in US English: three hours of broadcast news (BNEWS), three hours of conversational telephony speech (CTS) and two hours of conference room meetings (CONFMTG). As shown in Section 4.2, these different collections have different accuracies. CTS and CONFMTG are spontaneous speech. For the experiments, we have processed the query set provided by NIST that includes 1100 queries.
Each query is a phrase containing between one to five terms, common and rare terms, terms that are in the manual transcripts and those that are not. Testing and determination of empirical values have been achieved on another set of speech data and queries, the development set, also provided by NIST.
We have used the IBM research prototype ASR system, described in [26], for transcribing speech data. We have produced WCNs for the three different source types. 1-best phonetic transcripts were generated only for BNEWS and CTS, since CONFMTG phonetic transcripts have too low accuracy. We have adapted Juru [7], a full-text search library written in Java, to index the transcripts and to store the timestamps of the words and phones; search results have been retrieved as described in Section 3.
For each found occurrence of the given query, our system outputs: the location of the term in the audio recording (begin time and duration), the score indicating how likely is the occurrence of query, (as defined in Section 3.4) and a hard (binary) decision as to whether the detection is correct. We measure precision and recall by comparing the results obtained over the automatic transcripts (only the results having true hard decision) to the results obtained over the reference manual transcripts. Our aim is to evaluate the ability of the suggested retrieval approach to handle transcribed speech data. Thus, the closer the automatic results to the manual results is, the better the search effectiveness over the automatic transcripts will be. The results returned from the manual transcription for a given query are considered relevant and are expected to be retrieved with highest scores. This approach for measuring search effectiveness using manual data as a reference is very common in speech retrieval research [25, 22, 8, 9, 17].
Beside the recall and the precision, we use the evaluation measures defined by NIST for the 2006 STD evaluation [2]: the Actual Term-Weighted Value (ATWV) and the Maximum Term-Weighted Value (MTWV). The term-weighted value (TWV) is computed by first computing the miss and false alarm probabilities for each query separately, then using these and an (arbitrarily chosen) prior probability to compute query-specific values, and finally averaging these query-specific values over all queries q to produce an overall system value: TWV (θ) = 1 − averageq{Pmiss(q, θ) + β × PF A(q, θ)} where β = C V (Pr−1 q − 1). θ is the detection threshold. For the evaluation, the cost/value ratio, C/V , has been determined to 0.1 and the prior probability of a query Prq to 10−4 . Therefore, β = 999.9.
Miss and false alarm probabilities for a given query q are functions of θ: Pmiss(q, θ) = 1 − Ncorrect(q, θ) Ntrue(q) PF A(q, θ) = Nspurious(q, θ) NNT (q) corpus WER(%) SUBR(%) DELR(%) INSR(%) BNEWS WCN 12.7 49 42 9 CTS WCN 19.6 51 38 11 CONFMTG WCN 47.4 47 49 3 Table 1: WER and distribution of the error types over word 1-best path extracted from WCNs for the different source types. where: • Ncorrect(q, θ) is the number of correct detections (retrieved by the system) of the query q with a score greater than or equal to θ. • Nspurious(q, θ) is the number of spurious detections of the query q with a score greater than or equal to θ. • Ntrue(q) is the number of true occurrences of the query q in the corpus. • NNT (q) is the number of opportunities for incorrect detection of the query q in the corpus; it is the NonTarget query trials. It has been defined by the following formula: NNT (q) = Tspeech − Ntrue(q). Tspeech is the total amount of speech in the collection (in seconds).
ATWV is the actual term-weighted value; it is the detection value attained by the system as a result of the system output and the binary decision output for each putative occurrence. It ranges from −∞ to +1. MTWV is the maximum term-weighted value over the range of all possible values of θ. It ranges from 0 to +1.
We have also provided the detection error tradeoff (DET) curve [19] of miss probability (Pmiss) vs. false alarm probability (PF A).
We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV,
MTWV and the DET curve.
We have determined empirically the following values for the boosting vector defined in Section 3.4: Bi = 1 i .
We use the word error rate (WER) in order to characterize the accuracy of the transcripts. WER is defined as follows: S + D + I N × 100 where N is the total number of words in the corpus, and S, I, and D are the total number of substitution, insertion, and deletion errors, respectively. The substitution error rate (SUBR) is defined by S S + D + I × 100.
Deletion error rate (DELR) and insertion error rate (INSR) are defined in a similar manner.
Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs.
The WER of the 1-best path phonetic transcripts is approximately two times worse than the WER of word transcripts.
That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data.
We have determined empirically a detection threshold θ per source type and the hard decision of the occurrences having a score less than θ is set to false; false occurrences returned by the system are not considered as retrieved and therefore, are not used for computing ATWV, precision and recall.
The value of the threshold θ per source type is reported in Table 2. It is correlated to the accuracy of the transcripts.
Basically, setting a threshold aims to eliminate from the retrieved occurrences, false alarms without adding misses.
The higher the WER is, the higher the θ threshold should be.
BNEWS CTS CONFMTG
Table 2: Values of the θ threshold per source type.
We report in Table 3 the processing resource profile.
Concerning the index size, note that our index is compressed using IR index compression techniques. The indexing time includes both audio processing (generation of word and phonetic transcripts) and building of the searchable indices.
Index size 0.3267 MB/HS Indexing time 7.5627 HP/HS Index Memory Usage 1653.4297 MB Search speed 0.0041 sec.P/HS Search Memory Usage 269.1250 MB Table 3: Processing resource profile. (HS: Hours of Speech. HP: Processing Hours. sec.P: Processing seconds)
We compare our approach (WCN phonetic) presented in Section 4.1 with another approach (1-best-WCN phonetic).
The only difference between these two approaches is that, in 1-best-WCN phonetic, we index only the 1-best path extracted from the WCN instead of indexing all the WCN.
WCN phonetic was our primary system for the evaluation and 1-best-WCN phonetic was one of our contrastive systems. Average precision and recall, MTWV and ATWV on the 1100 queries are given in Table 4. We provide also the DET curve for WCN phonetic approach in Figure 2. The point that maximizes the TWV, the MTWV, is specified on each curve. Note that retrieval performance has been evaluated separately for each source type since the accuracy of the speech differs per source type as shown in Section 4.2.
As expected, we can see that MTWV and ATWV decrease in higher WER. The retrieval performance is improved when measure BNEWS CTS CONFMTG WCN phonetic ATWV 0.8485 0.7392 0.2365 MTWV 0.8532 0.7408 0.2508 precision 0.94 0.90 0.65 recall 0.89 0.81 0.37 1-best-WCN phonetic ATWV 0.8279 0.7102 0.2381 MTWV 0.8319 0.7117 0.2512 precision 0.95 0.91 0.66 recall 0.84 0.75 0.37 Table 4: ATWV, MTWV, precision and recall per source type.
Figure 2: DET curve for WCN phonetic approach. using WCNs relatively to 1-best path. It is due to the fact that miss probability is improved by indexing all the hypotheses provided by the WCNs. This observation confirms the results shown by Mamou et al. [17] in the context of spoken document retrieval. The ATWV that we have obtained is close to the MTWV; we have combined our ranking model with appropriate threshold θ to eliminate results with lower score. Therefore, the effect of false alarms added by WCNs is reduced.
WCN phonetic approach was used in the recent NIST STD evaluation and received the highest overall ranking among eleven participants. For comparison, the system that ranked at the third place, obtained an ATWV of 0.8238 for BNEWS,
the retrieval performance We have analysed the retrieval performance according to the average duration of the occurrences in the manual transcripts. The query set was divided into three different quantiles according to the duration; we have reported in Table 5 ATWV and MTWV according to the duration. We can see that we performed better on longer queries. One of the reasons is the fact that the ASR system is more accurate on long words. Hence, it was justified to boost the score of the results with the exponent γn, as explained in Section 3.4.3, according to the length of the query. quantile 0-33 33-66 66-100 BNEWS ATWV 0.7655 0.8794 0.9088 MTWV 0.7819 0.8914 0.9124 CTS ATWV 0.6545 0.8308 0.8378 MTWV 0.6551 0.8727 0.8479 CONFMTG ATWV 0.1677 0.3493 0.3651 MTWV 0.1955 0.4109 0.3880 Table 5: ATWV, MTWV according to the duration of the query occurrences per source type.
We have randomly chosen three sets of queries from the query sets provided by NIST: 50 queries containing only IV terms; 50 queries containing only OOV terms; and 50 hybrid queries containing both IV and OOV terms. The following experiment has been achieved on the BNEWS collection and IV and OOV terms has been determined according to the vocabulary of BNEWS ASR system.
We would like to compare three different approaches of retrieval: using only word index; using only phonetic index; combining word and phonetic indices. Table 6 summarizes the retrieval performance according to each approach and to each type of queries. Using a word-based approach for dealing with OOV and hybrid queries affects drastically the performance of the retrieval; precision and recall are null.
Using a phone-based approach for dealing with IV queries affects also the performance of the retrieval relatively to the word-based approach.
As expected, the approach combining word and phonetic indices presented in Section 3 leads to the same retrieval performance as the word approach for IV queries and to the same retrieval performance as the phonetic approach for OOV queries. This approach always outperforms the others and it justifies the fact that we need to combine word and phonetic search.
In the past decade, the research efforts on spoken data retrieval have focused on extending classical IR techniques to spoken documents. Some of these works have been done in the context of the TREC Spoken Document Retrieval evaluations and are described by Garofolo et al. [12]. An LVCSR system is used to transcribe the speech into 1-best path word transcripts. The transcripts are indexed as clean text: for each occurrence, its document, its word offset and additional information are stored in the index. A generic IR system over the text is used for word spotting and search as described by Brown et al. [6] and James [14]. This stratindex word phonetic word and phonetic precision recall precision recall precision recall IV queries 0.8 0.96 0.11 0.77 0.8 0.96 OOV queries 0 0 0.13 0.79 0.13 0.79 hybrid queries 0 0 0.15 0.71 0.89 0.83 Table 6: Comparison of word and phonetic approach on IV and OOV queries egy works well for transcripts like broadcast news collections that have a low WER (in the range of 15%-30%) and are redundant by nature (the same piece of information is spoken several times in different manners). Moreover, the algorithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors.
An alternative approach consists of using word lattices in order to improve the effectiveness of SDR. Singhal et al. [24, 25] propose to add some terms to the transcript in order to alleviate the retrieval failures due to ASR errors. From an IR perspective, a classical way to bring new terms is document expansion using a similar corpus. Their approach consists in using word lattices in order to determine which words returned by a document expansion algorithm should be added to the original transcript. The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with, lack information about word probabilities.
Chelba and Acero in [8, 9] propose a more compact word lattice, the position specific posterior lattice (PSPL). This data structure is similar to WCN and leads to a more compact index. The offset of the terms in the speech documents is also stored in the index. However, the evaluation framework is carried out on lectures that are relatively planned, in contrast to conversational speech. Their ranking model is based on the term confidence level but does not take into consideration the rank of the term among the other hypotheses. Mamou et al. [17] propose a model for spoken document retrieval using WCNs in order to improve the recall and the MAP of the search. However, in the above works, the problem of queries containing OOV terms is not addressed.
Popular approaches to deal with OOV queries are based on sub-words transcripts, where the sub-words are typically phones, syllables or word fragments (sequences of phones) [11, 20, 23]. The classical approach consists of using phonetic transcripts. The transcripts are indexed in the same manner as words in using classical text retrieval techniques; during query processing, the query is represented as a sequence of phones. The retrieval is based on searching the string of phones representing the query in the phonetic transcript. To account for the high recognition error rates, some other systems use richer transcripts like phonetic lattices.
They are attractive as they accommodate high error rate conditions as well as allow for OOV queries to be used [15, 3, 20, 23, 21, 27]. However, phonetic lattices contain many edges that overlap in time with the same phonetic label, and are difficult to index. Moreover, beside the improvement in the recall of the search, the precision is affected since phonetic lattices are often inaccurate. Consequently, phonetic approaches should be used only for OOV search; for searching queries containing also IV terms, this technique affects the performance of the retrieval in comparison to the word based approach.
Saraclar and Sproat in [22] show improvement in word spotting accuracy for both IV and OOV queries, using phonetic and word lattices, where a confidence measure of a word or a phone can be derived. They propose three different retrieval strategies: search both the word and the phonetic indices and unify the two different sets of results; search the word index for IV queries, search the phonetic index for OOV queries; search the word index and if no result is returned, search the phonetic index. However, no strategy is proposed to deal with phrase queries containing both IV and OOV terms. Amir et al. in [5, 4] propose to merge a word approach with a phonetic approach in the context of video retrieval. However, the phonetic transcript is obtained from a text to phonetic conversion of the 1-best path of the word transcript and is not based on a phonetic decoding of the speech data.
An important issue to be considered when looking at the state-of-the-art in retrieval of spoken data, is the lack of a common test set and appropriate query terms. This paper uses such a task and the STD evaluation is a good summary of the performance of different approaches on the same test conditions.
This work studies how vocabulary independent spoken term detection can be performed efficiently over different data sources. Previously, phonetic-based and word-based approaches have been used for IR on speech data. The former suffers from low accuracy and the latter from limited vocabulary of the recognition system. In this paper, we have presented a vocabulary independent model of indexing and search that combines both the approaches. The system can deal with all kinds of queries although the phrases that need to combine for the retrieval, information extracted from two different indices, a word index and a phonetic index. The scoring of OOV terms is based on the proximity (in time) between the different phones. The scoring of IV terms is based on information provided by the WCNs. We have shown an improvement in the retrieval performance when using all the WCN and not only the 1-best path and when using phonetic index for search of OOV query terms. This approach always outperforms the other approaches using only word index or phonetic index.
As a future work, we will compare our model for OOV search on phonetic transcripts with a retrieval model based on the edit distance.
Jonathan Mamou is grateful to David Carmel and Ron Hoory for helpful and interesting discussions.
[1] NIST Spoken Term Detection 2006 Evaluation Website, http://www.nist.gov/speech/tests/std/. [2] NIST Spoken Term Detection (STD) 2006 Evaluation Plan, http://www.nist.gov/speech/tests/std/docs/std06-evalplan-v10.pdf. [3] C. Allauzen, M. Mohri, and M. Saraclar. General indexation of weighted automata - application to spoken utterance retrieval. In Proceedings of the HLT-NAACL 2004 Workshop on Interdiciplinary Approaches to Speech Indexing and Retrieval, Boston,
MA, USA, 2004. [4] A. Amir, M. Berg, and H. Permuter. Mutual relevance feedback for multimodal query formulation in video retrieval. In MIR "05: Proceedings of the 7th ACM SIGMM international workshop on Multimedia information retrieval, pages 17-24, New York, NY,
USA, 2005. ACM Press. [5] A. Amir, A. Efrat, and S. Srinivasan. Advances in phonetic word spotting. In CIKM "01: Proceedings of the tenth international conference on Information and knowledge management, pages 580-582, New York,
NY, USA, 2001. ACM Press. [6] M. Brown, J. Foote, G. Jones, K. Jones, and S. Young.
Open-vocabulary speech indexing for voice and video mail retrieval. In Proceedings ACM Multimedia 96, pages 307-316, Hong-Kong, November 1996. [7] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek,
Y. Petruschka, and A. Soffer. Juru at TREC 10Experiments with Index Pruning. In Proceedings of the Tenth Text Retrieval Conference (TREC-10). National Institute of Standards and Technology. NIST, 2001. [8] C. Chelba and A. Acero. Indexing uncertainty for spoken document search. In Interspeech 2005, pages 61-64, Lisbon, Portugal, 2005. [9] C. Chelba and A. Acero. Position specific posterior lattices for indexing speech. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL), Ann Arbor, MI,
[10] S. Chen. Conditional and joint models for grapheme-to-phoneme conversion. In Eurospeech 2003,
Geneva, Switzerland, 2003. [11] M. Clements, S. Robertson, and M. Miller. Phonetic searching applied to on-line distance learning modules.
In Digital Signal Processing Workshop, 2002 and the 2nd Signal Processing Education Workshop.
Proceedings of 2002 IEEE 10th, pages 186-191, 2002. [12] J. Garofolo, G. Auzanne, and E. Voorhees. The TREC spoken document retrieval track: A success story. In Proceedings of the Ninth Text Retrieval Conference (TREC-9). National Institute of Standards and Technology. NIST, 2000. [13] D. Hakkani-Tur and G. Riccardi. A general algorithm for word graph matrix decomposition. In Proceedings of the IEEE Internation Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 596-599, Hong-Kong, 2003. [14] D. James. The application of classical information retrieval techniques to spoken documents. PhD thesis,
University of Cambridge, Downing College, 1995. [15] D. A. James. A system for unrestricted topic retrieval from radio news broadcasts. In Proc. ICASSP "96, pages 279-282, Atlanta, GA, 1996. [16] B. Logan, P. Moreno, J. V. Thong, and E. Whittaker.
An experimental study of an audio indexing system for the web. In Proceedings of ICSLP, 1996. [17] J. Mamou, D. Carmel, and R. Hoory. Spoken document retrieval from call-center conversations. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 51-58,
New York, NY, USA, 2006. ACM Press. [18] L. Mangu, E. Brill, and A. Stolcke. Finding consensus in speech recognition: word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373-400, 2000. [19] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki. The DET curve in assessment of detection task performance. In Proc. Eurospeech "97, pages 1895-1898, Rhodes, Greece, 1997. [20] K. Ng and V. W. Zue. Subword-based approaches for spoken document retrieval. Speech Commun., 32(3):157-186, 2000. [21] Y. Peng and F. Seide. Fast two-stage vocabulary-independent search in spontaneous speech.
In Acoustics, Speech, and Signal Processing.
Proceedings. (ICASSP). IEEE International Conference, volume 1, pages 481-484, 2005. [22] M. Saraclar and R. Sproat. Lattice-based search for spoken utterance retrieval. In HLT-NAACL 2004: Main Proceedings, pages 129-136, Boston,
Massachusetts, USA, 2004. [23] F. Seide, P. Yu, C. Ma, and E. Chang.
Vocabulary-independent search in spontaneous speech.
In ICASSP-2004, IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004. [24] A. Singhal, J. Choi, D. Hindle, D. Lewis, and F. Pereira. AT&T at TREC-7. In Proceedings of the Seventh Text Retrieval Conference (TREC-7).
National Institute of Standards and Technology.
NIST, 1999. [25] A. Singhal and F. Pereira. Document expansion for speech retrieval. In SIGIR "99: Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 34-41, New York, NY, USA, 1999. ACM Press. [26] H. Soltau, B. Kingsbury, L. Mangu, D. Povey,
G. Saon, and G. Zweig. The IBM 2004 conversational telephony system for rich transcription. In Proceedings of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), March 2005. [27] K. Thambiratnam and S. Sridharan. Dynamic match phone-lattice searches for very fast and accurate unrestricted vocabulary keyword spotting. In Acoustics, Speech, and Signal Processing. Proceedings. (ICASSP). IEEE International Conference, 2005. [28] P. C. Woodland, S. E. Johnson, P. Jourlin, and K. S.
Jones. Effects of out of vocabulary words in spoken document retrieval (poster session). In SIGIR "00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 372-374, New York, NY,

Web search has now become a major tool in our daily lives for information seeking. One of the important issues in Web search is that user queries are often not best formulated to get optimal results. For example, running shoe is a query that occurs frequently in query logs. However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.
Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs. This is difficult even for experienced users, and especially difficult for non-native speakers. One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term. Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.
Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing. Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.
Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.
When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6]. In addition, it reduces system performance because the search engine has to match all the word variants. As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa. Thus, one needs to be very cautious when using stemming in Web search engines.
One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word. For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored. For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query. Transforming book store to match book stores is fine, but matching book storing or booking store is not. A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query. However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].
A second problem of traditional stemming is its blind matching of all occurrences in documents. For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.
Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected. Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.
To alleviate these two problems, we propose a context sensitive stemming approach for Web search. Our solution consists of two context sensitive analysis, one on the query side and the other on the document side. On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms. On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query. Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.
We use pluralization handling as a running example for our stemming approach. The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance. As far as we know, no previous research has systematically investigated the usage of pluralization in Web search. As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion. Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.
In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.
We describe the details of the context sensitive stemming approach in Section 3. We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5. Finally, we conclude the paper in Section 6.
Stemming is a long studied technology. Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18]. The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.
However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology. Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23]. We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.
Using stemming in information retrieval is also a well known technique [8, 10]. However, the effectiveness of stemming for English query systems was previously reported to be rather limited. Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance. Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper). They also proposed selective stemming based on query length and term importance, but no positive results were reported. On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).
However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search. These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2]. We suspect the previous failures were mainly due to the two problems we mentioned in the introduction. Blind stemming, or a simple query length based selective stemming as used in [9] is not enough. Stemming has to be decided on case by case basis, not only at the query level but also at the document level. As we will show, if handled correctly, significant improvement can be achieved.
A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25]. To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21]. This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search. In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).
Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query. Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search. On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent. While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision. The increase on recall is obvious. With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.
On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query. This is similar to the relevance feedback in the sense that it requires multiple passes retrieval. If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful. It is expensive to deploy this in production Web search engines. Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.
In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming. Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision. It"s simple, yet very efficient and effective, making real time stemming feasible for Web search. Our results will affirm researchers that stemming is indeed very important to large scale information retrieval.
Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching. Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary. For an input query, we first segment query into concepts and detect the head word for each concept (component 2). We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4). Below we discuss each of the components in more detail.
Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: "hotel" "comparisons" hotel −> hotels Figure 1: System Components
One of the ways to generate candidates is using the Porter stemmer [18]. The Porter stemmer simply uses morphological rules to convert a word to its base form. It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past. A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26]. The corpus analysis we do is based on word distributional similarity [15]. The rationale of using distributional word similarity is that true variants tend to be used in similar contexts. In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word. We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus. The similarity between two words is the cosine similarity between the two corresponding feature vectors. The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score
Table 1: Top 20 most similar candidates to word develop. Column score is the similarity score.
To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list. After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental. For the pluralization handling purpose, only the candidate develops is retained.
One thing we note from observing the distributionally similar words is that they are closely related semantically.
These words might serve as candidates for general query expansion, a topic we will investigate in the future.
For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts. We first break a query into segments, each segment representing a concept which normally is a noun phrase.
For each of the noun phrases, we then detect the most important word which we call the head word. Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.
To break a query into segments, we have to define a criteria to measure the strength of the relation between words.
One effective method is to use mutual information as an indicator on whether or not to split two words [19]. We use a log of 25M queries and collect the bigram and unigram frequencies from it. For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word. We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.
Table 2 shows some examples of query segmentation.
The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query. Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short. Applying a parser trained on sentences from documents to queries will have poor performance. In our solution, we just use simple heuristics rules, and it works very well in practice for English. For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW. In such cases, the head word is typically the last nonstop word of XYZ.
After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.
Our statistics show that about half of the queries can be transformed by pluralization via naive stemming. Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse. Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost. In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful. Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts. Head words hotel and comparison can be expanded to hotels and comparisons. Are both transformations useful?
To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.
The more likely a query to occur on the Web, the more relevant documents this query is able to return. Now the whole problem becomes how to calculate the probability of query to occur on the Web.
Calculating the probability of string occurring in a corpus is a well known language modeling problem. The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur). The simplest and most successful approach to language modeling is still based on the n-gram model. By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.
Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus. Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary. This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6). Also, because of the heavy tailed nature of language (i.e. Zipf"s law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.
One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.
Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) =
X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1)
X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing,
Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5]. We used absolute smoothing in our experiments.
Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.
Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3. We can see that all alternatives are less likely than the input query. It is therefore not useful to make an expansion for this query. On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded. To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.
Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face.
Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants. For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison. However, not every occurrence of comparison in the document is of interest. A page which is about comparing customer service can contain all of the words hotel price comparisons comparison. This page is not a good page for the query.
If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval. To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document. A variant match is considered valid only if the variant occurs in the same context as the original word does. The context is the left or the right non-stop segments 1 of the original word. Taking the same query as an example, the context of comparisons is price. The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price. Thus, we should only match those occurrences of comparison in the document if they occur after the word price. Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size. If the expanded word comparison occurs within the context of price within a window, it is considered valid. The smaller the window size is, the more restrictive the matching. We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases.
We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word.
We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11]. Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight. A page is graded into one of the five scales: Perfect, Excellent, Good,
Fair, Bad, with corresponding weights. We use dcg to represent the average DCG(5) over a set of test queries.
Another metric is to measure the additional cost incurred by stemming. Given the same level of relevance improvement, we prefer a stemming method that has less additional cost. We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed.
We randomly sample 870 queries from a three month query log, with 290 from each month. Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming. We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words. In the end, we have 529 correctly spelled queries with at least 2 words.
Before explaining the experiments and results in detail, we"d like to describe the traditional way of using stemming for Web search, referred as the naive model. This is to treat every word variant equivalent for all possible words in the query. The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments.
The baseline model is the model without stemming. We first run the naive model to see how well it performs over the baseline. Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model. This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5. The naive model and document sensitive matching model stem the most queries. Out of the
corresponding to 46.7% query traffic (out of a total of 870). We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model. Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model. We experiment with unigram language model and bigram language model. Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.
To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data. The oracle model only expands a word if the stemming will give better results.
To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries. Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words.
We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5. Each row in Table 4 is a stemming strategy described in section 4.4. The first column is the name of the strategy. The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg. The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy). The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.
There are several observations about the results. We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%. Looking at Table 5, it gives an improvement of 2.7% on short queries. However, it also hurts long queries by -2.4%. Overall, the improvement is canceled out. The reason that it improves short queries is that most short queries only have one word that can be stemmed. Thus, blindly pluralizing short queries is relatively safe. However for long queries, most queries can have multiple words that can be pluralized. Expanding all of them without selection will significantly hurt precision.
Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%. The improvement comes from the conservative context sensitive document matching. An expanded word is valid only if it occurs within the context of original query in the document. This reduces many spurious matches. However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem. While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches. It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.
Selective word pluralization further helps resolving the problem faced by document context sensitive stemming. It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place. By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision. With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%. In particular, it gives significant improvements on long queries.
The dcg gain is turned from negative to positive, from −1.6% to 1.1%. This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement. For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.
The advantages of predictive word expansion with a language model is further boosted with a better bigram language model. The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic. For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272. For long queries, bigram language model improves dcg gain from
100/136. We observe that the bigram language model gives a larger lift for long queries. This is because the uncertainty in long queries is larger and a more powerful language model is needed. We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.
Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying. For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model. For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model. We may gain additional benefit with a more powerful language model for long queries. However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem. These problems have to be addressed separately. Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful. We currently capture about half of them. Further reduction of the overhead requires sacrificing the dcg gain.
Now we can compare the stemming strategies from a different aspect. Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only. We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).
For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%. An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries).
As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web. The language model should describe the occurrence of the string on the Web. However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming. General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.
Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.
To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction. We observed a slight performance decrease compared to the model trained on Web frequencies. In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries. Thus, the query log can serve as a good approximation of the Web frequencies.
Some linguistic knowledge is useful in stemming. For the pluralization handling case, pluralization and de-pluralization is not symmetric. A plural word used in a query indicates a special intent. For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California. A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank. To capture this intent, we have to make sure the document is a general page about hotels in new york. We do this by requiring that the plural word hotels appears in the document.
On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information. We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property.
One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming. Generally speaking, pluralization or depluralization keeps the original intent. However, the intent could change in a few cases. For one example of such a query, job at apple, we pluralize job to jobs. This stemming makes the original query ambiguous. The query job OR jobs at apple has two intents. One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company. Thus, the results after query stemming returns Steve Jobs as one of the results in top 5. One solution is performing results set based analysis to check if the intent is changed. This is similar to relevance feedback and requires second phase ranking.
A second type of mistakes is the entity/concept recognition problem, These include two kinds. One is that the stemmed word variant now matches part of an entity or concept. For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco. The results will match cookie jar in san francisco. Although cookie still means the same thing as cookies, cookie jar is a different concept. Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words. For example, quote ICE is pluralized to quote OR quotes ICE. The original intent for this query is searching for stock quote for ticker ICE. However, we noticed that among the top results, one of the results is Food quotes: Ice cream. This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes. The unchanged word ICE matches part of the noun phrase ice cream here. To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.
A third type of mistakes occurs in long queries. For the query bar code reader software, two words are pluralized. code to codes and reader to readers. In fact, bar code reader in the original query is a strong concept and the internal words should not be changed. This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking. For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept.
We have presented a simple yet elegant way of stemming for Web search. It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side. Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost. It also significantly improves Web click through rate (details not reported in the paper).
For the future work, we are investigating the problems we identified in the error analysis section. These include: entity and noun phrase matching mistakes, and improved segmentation.

Biologists search for literature on a daily basis. For most biologists, PubMed, an online service of U.S. National Library of Medicine (NLM), is the most commonly used tool for searching the biomedical literature. PubMed allows for keyword search by using Boolean operators. For example, if one desires documents on the use of the drug propanolol in the disease hypertension, a typical PubMed query might be propanolol AND hypertension, which will return all the documents having the two keywords.
Keyword search in PubMed is effective if the query is well-crafted by the users using their expertise. However, information needs of biologists, in some cases, are expressed as complex questions [8][9], which PubMed is not designed to handle. While NLM does maintain an experimental tool for free-text queries [6], it is still based on PubMed keyword search.
The Genomics track of the 2006 Text REtrieval Conference (TREC) provides a common platform to assess the methods and techniques proposed by various groups for biomedical information retrieval. The queries were collected from real biologists and they are expressed as complex questions, such as How do mutations in the Huntingtin gene affect Huntington"s disease?. The document collection contains 162,259 Highwire full-text documents in HTML format. Systems from participating groups are expected to find relevant passages within the full-text documents. A passage is defined as any span of text that does not include the HTML paragraph tag (i.e., <P> or </P>).
We approached the problem by utilizing domain-specific knowledge in a conceptual retrieval model. Domain-specific knowledge, in this paper, refers to information about concepts and relationships between concepts in a certain domain. We assume that appropriate use of domain-specific knowledge might improve the effectiveness of retrieval. For example, given a query What is the role of gene PRNP in the Mad Cow Disease?, expanding the gene symbol PRNP with its synonyms Prp, PrPSc, and prion protein, more relevant documents might be retrieved.
PubMed and many other biomedical systems [8][9][10][13] also make use of domain-specific knowledge to improve retrieval effectiveness.
Intuitively, retrieval on the level of concepts should outperform bag-of-words approaches, since the semantic relationships among words in a concept are utilized. In some recent studies [13][15], positive results have been reported for this hypothesis. In this paper, concepts are entry terms of the ontology Medical Subject Headings (MeSH), a controlled vocabulary maintained by NLM for indexing biomedical literature, or gene symbols in the Entrez gene database also from NLM. A concept could be a word, such as the gene symbol PRNP, or a phrase, such as Mad cow diseases. In the conceptual retrieval model presented in this paper, the similarity between a query and a document is measured on both concept and word levels.
This paper makes two contributions:
knowledge in an IR system to improve its effectiveness in retrieving biomedical literature. Based on this approach, our system achieved significant improvement (23%) over the best reported result in passage retrieval in the Genomics track of TREC 2006.
types of domain-specific knowledge in performance contribution.
This paper is organized as follows: problem statement is given in the next section. The techniques are introduced in section 3. In section 4, we present the experimental results. Related works are given in section 5 and finally, we conclude the paper in section 6.
We describe the queries, document collection and the system output in this section.
The query set used in the Genomics track of TREC 2006 consists of 28 questions collected from real biologists. As described in [8], these questions all have the following general format: Biological object (1..m) Relationship ←⎯⎯⎯⎯→ Biological process (1..n) (1) where a biological object might be a gene, protein, or gene mutation and a biological process can be a physiological process or disease. A question might involve multiple biological objects (m) and multiple biological processes (n). These questions were derived from four templates (Table 2).
Table 2 Query templates and examples in the Genomics track of TREC 2006 Template Example What is the role of gene in disease?
What is the role of DRD4 in alcoholism?
What effect does gene have on biological process?
What effect does the insulin receptor gene have on tumorigenesis?
How do genes interact in organ function?
How do HMG and HMGB1 interact in hepatitis?
How does a mutation in gene influence biological process?
How does a mutation in Ret influence thyroid function?
Features of the queries: 1) They are different from the typical Web queries and the PubMed queries, both of which usually consist of 1 to 3 keywords; 2) They are generated from structural templates which can be used by a system to identify the query components, the biological object or process.
The document collection contains 162,259 Highwire full-text documents in HTML format.
The output of the system is a list of passages ranked according to their similarities with the query. A passage is defined as any span of text that does not include the HTML paragraph tag (i.e., <P> or </P>). A passage could be a part of a sentence, a sentence, a set of consecutive sentences or a paragraph (i.e., the whole span of text that are inside of <P> and </P> HTML tags).
This is a passage-level information retrieval problem with the attempt to put biologists in contexts where relevant information is provided.
We approached the problem by first retrieving the top-k most relevant paragraphs, then extracting passages from these paragraphs, and finally ranking the passages. In this process, we employed several techniques and methods, which will be introduced in this section. First, we give two definitions: Definition 3.1 A concept is 1) a entry term in the MeSH ontology, or 2) a gene symbol in the Entrez gene database. This definition of concept can be generalized to include other biomedical dictionary terms.
Definition 3.2 A semantic type is a category defined in the Semantic Network of the Unified Medical Language System (UMLS) [14]. The current release of the UMLS Semantic Network contains 135 semantic types such as Disease or Syndrome. Each entry term in the MeSH ontology is assigned one or more semantic types. Each gene symbol in the Entrez gene database maps to the semantic type Gene or Genome. In addition, these semantic types are linked by 54 relationships. For example, Antibiotic prevents Disease or Syndrome. These relationships among semantic types represent general biomedical knowledge. We utilized these semantic types and their relationships to identify related concepts.
The rest of this section is organized as follows: in section 3.1, we explain how the concepts are identified within a query. In section
and introduce how they are compiled. In section 3.3, we present our conceptual IR model. Finally, our strategy for passage extraction is described in section 3.4.
A concept, defined in Definition 3.1, is a gene symbol or a MeSH term. We make use of the query templates to identify gene symbols. For example, the query How do HMG and HMGB1 interact in hepatitis? is derived from the template How do genes interact in organ function?. In this case, HMG and HMGB1 will be identified as gene symbols. In cases where the query templates are not provided, programs for recognition of gene symbols within texts are needed.
We use the query translation functionality of PubMed to extract MeSH terms in a query. This is done by submitting the whole query to PubMed, which will then return a file in which the MeSH terms in the query are labeled. In Table 3.1, three MeSH terms within the query What is the role of gene PRNP in the Mad cow disease? are found in the PubMed translation: "encephalopathy, bovine spongiform" for Mad cow disease, genes for gene, and role for role.
Table 3.1 The PubMed translation of the query "What is the role of gene PRNP in the Mad cow disease?".
Term PubMed translation Mad cow disease "bovine spongiform encephalopathy"[Text Word] OR "encephalopathy, bovine spongiform"[MeSH Terms] OR Mad cow disease[Text Word] gene ("genes"[TIAB] NOT Medline[SB]) OR "genes"[MeSH Terms] OR gene[Text Word] role "role"[MeSH Terms] OR role[Text Word]
In this paper, domain-specific knowledge refers to information about concepts and their relationships in a certain domain. We used five types of domain-specific knowledge in the domain of genomics: Type 1. Synonyms (terms listed in the thesauruses that refer to the same meaning) Type 2. Hypernyms (more generic terms, one level only) Type 3. Hyponyms (more specific terms, one level only) Type 4. Lexical variants (different forms of the same concept, such as abbreviations. They are commonly used in the literature, but might not be listed in the thesauruses) Type 5. Implicitly related concepts (terms that are semantically related and also co-occur more frequently than being independent in the biomedical texts) Knowledge of type 1-3 is retrieved from the following two thesauruses: 1) MeSH, a controlled vocabulary maintained by NLM for indexing biomedical literature. The 2007 version of MeSH contains information about 190,000 concepts. These concepts are organized in a tree hierarchy; 2) Entrez Gene, one of the most widely used searchable databases of genes. The current version of Entrez Gene contains information about 1.7 million genes. It does not have a hierarchy. Only synonyms are retrieved from Entrez Gene. The compiling of type 4-5 knowledge is introduced in section 3.2.1 and 3.2.2, respectively.
Lexical variants of gene symbols New gene symbols and their lexical variants are regularly introduced into the biomedical literature [7]. However, many reference databases, such as UMLS and Entrez Gene, may not be able to keep track of all this kind of variants. For example, for the gene symbol "NF-kappa B", at least 5 different lexical variants can be found in the biomedical literature: "NF-kappaB", "NFkappaB", "NFkappa B", "NF-kB", and "NFkB", three of which are not in the current UMLS and two not in the Entrez Gene. [3][21] have shown that expanding gene symbols with their lexical variants improved the retrieval effectiveness of their biomedical IR systems. In our system, we employed the following two strategies to retrieve lexical variants of gene symbols.
Strategy I: This strategy is to automatically generate lexical variants according to a set of manually crafted heuristics [3][21].
For example, given a gene symbol PLA2, a variant PLAII is generated according to the heuristic that Roman numerals and Arabic numerals are convertible when naming gene symbols.
Another variant, PLA 2, is also generated since a hyphen or a space could be inserted at the transition between alphabetic and numerical characters in a gene symbol.
Strategy II: This strategy is to retrieve lexical variants from an abbreviation database. ADAM [22] is an abbreviation database which covers frequently used abbreviations and their definitions (or long-forms) within MEDLINE, the authoritative repository of citations from the biomedical literature maintained by the NLM.
Given a query How does nucleoside diphosphate kinase (NM23) contribute to tumor progression?, we first identify the abbreviation NM23 and its long-form nucleoside diphosphate kinase using the abbreviation identification program from [4].
Searching the long-form nucleoside diphosphate kinase in ADAM, other abbreviations, such as NDPK or NDK, are retrieved. These abbreviations are considered as the lexical variants of NM23.
Lexical variants of MeSH concepts ADAM is used to obtain the lexical variants of MeSH concepts as well. All the abbreviations of a MeSH concept in ADAM are considered as lexical variants to each other. In addition, those long-forms that share the same abbreviation with the MeSH concept and are different by an edit distance of 1 or 2 are also considered as its lexical variants. As an example, "human papilloma viruses" and "human papillomaviruses" have the same abbreviation HPV in ADAM and their edit distance is 1. Thus they are considered as lexical variants to each other. The edit distance between two strings is measured by the minimum number of insertions, deletions, and substitutions of a single character required to transform one string into the other [12].
Motivation: In some cases, there are few documents in the literature that directly answer a given query. In this situation, those documents that implicitly answer their questions or provide supporting information would be very helpful. For example, there are few documents in PubMed that directly answer the query "What is the role of the genes HNF4 and COUP-tf I in the suppression in the function of the liver?". However, there exist some documents about the role of "HNF4" and "COUP-tf I" in regulating "hepatitis B virus" transcription. It is very likely that the biologists would be interested in these documents because "hepatitis B virus" is known as a virus that could cause serious damage to the function of liver. In the given example, "hepatitis B virus" is not a synonym, hypernym, hyponym, nor a lexical variant of any of the query concepts, but it is semantically related to the query concepts according to the UMLS Semantic Network. We call this type of concepts implicitly related concepts of the query. This notion is similar to the B-term used in [19] for relating two disjoint literatures for biomedical hypothesis generation. The difference is that we utilize the semantic relationships among query concepts to exclusively focus on concepts of certain semantic types.
A query q in format (1) of section 2 can be represented by q = (A, C) where A is the set of biological objects and C is the set of biological processes. Those concepts that are semantically related to both A and C according to the UMLS Semantic Network are considered as the implicitly related concepts of the query. In the above example, A = {HNF4, COUP-tf I}, C = {function of liver}, and "hepatitis B virus" is one of the implicitly related concepts.
We make use of the MEDLINE database to extract the implicitly related concepts. The 2006 version of MEDLINE database contains citations (i.e., abstracts, titles, and etc.) of over 15 million biomedical articles. Each document in MEDLINE is manually indexed by a list of MeSH terms to describe the topics covered by that document. Implicitly related concepts are extracted and ranked in the following steps: Step 1. Let list_A be the set of MeSH terms that are 1) used for indexing those MEDLINE citations having A, and 2) semantically related to A according to the UMLS Semantic Network. Similarly, list_C is created for C. Concepts in B = list_A ∩ list_C are considered as implicitly related concepts of the query.
Step 2. For each concept b∈B, compute the association between b and A using the mutual information measure [5]: P( , ) ( , ) log P( )P( ) b A I b A b A = where P(x) = n/N, n is the number of MEDLINE citations having x and N is the size of MEDLINE. A large value for I(b, A) means that b and A co-occur much more often than being independent.
I(b, C) is computed similarly.
Step 3. Let r(b) = (I(b, A), I(b, C)), for b∈ B. Given b1, b2 ∈ B, we say r(b1) ≤ r(b2) if I(b1, A) ≤ I(b2, A) and I(b1, C) ≤ I(b2, C).
Then the association between b and the query q is measured by: { : and ( ) ( )} ( , ) { : and ( ) ( )} x x B r x r b score b q x x B r b r x ∈ ≤ = ∈ ≤ (2) The numerator in Formula 2 is the number of the concepts in B that are associated with both A and C equally with or less than b.
The denominator is the number of the concepts in B that are associated with both A and C equally with or more than b. Figure
query.
Figure 3.2.2 Top 4 implicitly related concepts for the query "How do interactions between HNF4 and COUP-TF1 suppress liver function?".
In Figure 3.2.2, the top 4 implicitly related concepts are all highly associated with liver: Hepatocytes are liver cells; Hepatoblastoma is a malignant liver neoplasm occurring in young children; the vast majority of Gluconeogenesis takes place in the liver; and Hepatitis B virus is a virus that could cause serious damage to the function of liver.
The top-k ranked concepts in B are used for query expansion: if I(b, A) ≥ I(b, C), then b is considered as an implicit related concept of A. A document having b but not A will receive a partial weight of A. The expansion is similar for C when I(b, A) < I(b, C).
We now discuss our conceptual IR model. We first give the basic conceptual IR model in section 3.3.1. Then we explain how the domain-specific knowledge is incorporated in the model using query expansion in section 3.3.2. A pseudo-feedback strategy is introduced in section 3.3.3. In section 3.3.4, we give a strategy to improve the ranking by avoiding incorrect match of abbreviations.
Given a query q and a document d, our model measures two similarities, concept similarity and word similarity: ( , ) ( , )( , ) ( , ) concept word sim q d sim q d sim q d= Concept similarity Two vectors are derived from a query q,
( , ) ( , ,..., ) ( , ,..., ) m n q v v v c c c v c c c = = = where v1 is a vector of concepts describing the biological object(s) and v2 is a vector of concepts describing the biological process(es).
Given a vector of concepts v, let s(v) be the set of concepts in v.
The weight of vi is then measured by: ( ) max{log : ( ) ( ) and 0}i i v v N w v s v s v n n = ⊆ > where v is a vector that contains a subset of concepts in vi and nv is the number of documents having all the concepts in v.
The concept similarity between q and d is then computed by 2 1 ( )( , ) i i concept i w vsim q d α = = ×∑ where αi is a parameter to indicate the completeness of vi that document d has covered. αi is measured by: and i i i c c d c v c c v idf idf α ∈ ∈ ∈ = ∑ ∑ (3) where idfc is the inverse document frequency of concept c.
An example: suppose we have a query How does Nurr-77 delete T cells before they migrate to the spleen or lymph nodes and how does this impact autoimmunity?. After identifying the concepts in the query, we have: 1 2 ('Nurr-77') ('T cells', 'spleen', 'autoimmunity', 'lymph nodes') v v = = Suppose that some document frequencies of different combinations of concepts are as follows:
The weight of vi is then computed by (note that there does not exist a document having all the concepts in v2): 1 2 ( ) log( / 25) ( ) log( /82) w v N w v N = = .
Now suppose a document d contains concepts ‘Nurr-77", 'T cells', 'spleen', and 'lymph nodes', but not ‘autoimmunity", then the value of parameter αi is computed as follows: 1 2 1 ('T cells')+ ('spleen')+ ('lymph nodes') ('T cells')+ ('spleen')+ ('lymph nodes')+ ('autoimmunity') idf idf idf idf idf idf idf α α = = Word similarity The similarity between q and d on the word level is computed using Okapi [17]:
log( )( , )
N n k tf sim q d n K tf∈ − + + = + + ∑ (4) where N is the size of the document collection; n is the number of documents containing w; K=k1 × ((1-b)+b × dl/avdl) and k1=1.2,
C Function of Liver Implicitly related concepts (B) Hepatocytes Hepatoblastoma Gluconeogenesis Hepatitis B virus HNF4 and COUP-tf I A b=0.75 are constants. dl is the document length of d and avdl is the average document length; tf is the term frequency of w within d.
The model Given two documents d1 and d2, we say 1 2( , ) ( , )sim q d sim q d> or d1 will be ranked higher than d2, with respect to the same query q, if either 1) 1 2( , ) ( , ) concept concept sim q d sim q d> or 2) 1 2 1 2and( , ) ( , ) ( , ) ( , ) concept concept word word sim q d sim q d sim q d sim q d= > This conceptual IR model emphasizes the similarity on the concept level. A similar model but applied to non-biomedical domain has been given in [15].
Given a concept c, a vector u is derived by incorporating its domain-specific knowledge:
where u1 is a vector of its synonyms, hyponyms, and lexical variants; u2 is a vector of its hypernyms; and u3 is a vector of its implicitly related concepts. An occurrence of any term in u1 will be counted as an occurrence of c. idfc in Formula 3 is updated as: 1, logc c u N D idf = 1,c uD is the set of documents having c or any term in 1u . The weight that a document d receives from u is given by: max{ : and }tw t u t d∈ ∈ where wt = β .cidf× The weighting factor β is an empirical tuning parameter determined as:
its lexical variant;
the number of selected top ranked implicitly related concepts (see section 3.2.2); i is the position of t in the ranking of implicitly related concepts.
Pseudo-feedback is a technique commonly used to improve retrieval performance by adding new terms into the original query.
We used a modified pseudo-feedback strategy described in [2].
Step 1. Let C be the set of concepts in the top 15 ranked documents. For each concept c in C, compute the similarity between c and the query q, the computation of sim(q,c) can be found in [2].
Step 2. The top-k ranked concepts by sim(q,c) are selected.
Step 3. Associate each selected concept c' with the concept cq in q that 1) has the same semantic type as c', and 2) is most related to c' among all the concepts in q. The association between c' and cq is computed by: P( ', ) ( ', ) log P( ')P( ) q q q c c I c c c c = where P(x) = n/N, n is the number of documents having x and N is the size of the document collection. A document having c' but not cq receives a weight given by: (0.5× (k-i+1)/k) ,qcidf× where i is the position of c' in the ranking of step 2.
Some gene symbols are very short and thus ambiguous. For example, the gene symbol APC could be the abbreviation for many non-gene long-forms, such as air pollution control, aerobic plate count, or argon plasma coagulation. This step is to avoid incorrect match of abbreviations in the top ranked documents.
Given an abbreviation X with the long-form L in the query, we scan the top-k ranked (k=1000) documents and when a document is found with X, we compare L with all the long-forms of X in that document. If none of these long-forms is equal or close to L (i.e., the edit distance between L and the long-form of X in that document is 1 or 2), then the concept similarity of X is subtracted.
The goal of passage extraction is to highlight the most relevant fragments of text in paragraphs. A passage is defined as any span of text that does not include the HTML paragraph tag (i.e., <P> or </P>). A passage could be a part of a sentence, a sentence, a set of consecutive sentences or a paragraph (i.e., the whole span of text that are inside of <P> and </P> HTML tags). It is also possible to have more than one relevant passage in a single paragraph. Our strategy for passage extraction assumes that the optimal passage(s) in a paragraph should have all the query concepts that the whole paragraph has. Also they should have higher density of query concepts than other fragments of text in the paragraph.
Suppose we have a query q and a paragraph p represented by a sequence of sentences 1 2... .np s s s= Let C be the set of concepts in q that occur in p and S = Φ.
Step 1. For each sequence of consecutive sentences 1... ,i i js s s+ 1 ≤ i ≤ j ≤ n, let S = S 1{ ... }i i js s s+∪ if 1...i i js s s+ satisfies that: 1) Every query concept in C occurs in 1...i i js s s+ and 2) There does not exist k, such that i < k < j and every query concept in C occurs in 1...i i ks s s+ or 1 2... .k k js s s+ + Condition 1 requires 1...i i js s s+ having all the query concepts in p and condition 2 requires 1...i i js s s+ be the minimal.
Step 2. Let 1min{ 1: ... }i i jL j i s s s S+= − + ∈ . For every
remove those sequences of sentences in S that have lower density of query concepts.
Step 3. For every two sequences of consecutive sentences 1 1 1 2 2 21 1... , and ...i i j i i js s s S s s s S+ +∈ ∈ , if
, and 1 i i j j i j ≤ ≤ ≤ + (5) then do Repeat this step until for every two sequences of consecutive sentences in S, condition (5) does not apply. This step is to merge those sequences of sentences in S that are adjacent or overlapped.
Finally the remaining sequences of sentences in S are returned as the optimal passages in the paragraph p with respect to the query.
1
1 { ... } { ... } { ... } i i j i i j i i j S S s s s S S s s s S S s s s + + + = ∪ = − = −
The evaluation of our techniques and the experimental results are given in this section. We first describe the datasets and evaluation metrics used in our experiments and then present the results.
Our experiments were performed on the platform of the Genomics track of TREC 2006. The document collection contains 162,259 full-text documents from 49 Highwire biomedical journals. The set of queries consists of 28 queries collected from real biologists.
The performance is measured on three different levels (passage, aspect, and document) to provide better insight on how the question is answered from different perspectives. Passage MAP: As described in [8], this is a character-based precision calculated as follows: At each relevant retrieved passage, precision will be computed as the fraction of characters overlapping with the gold standard passages divided by the total number of characters included in all nominated passages from this system for the topic up until that point. Similar to regular MAP, relevant passages that were not retrieved will be added into the calculation as well, with precision set to 0 for relevant passages not retrieved. Then the mean of these average precisions over all topics will be calculated to compute the mean average passage precision. Aspect MAP: A question could be addressed from different aspects. For example, the question what is the role of gene PRNP in the Mad cow disease? could be answered from aspects like Diagnosis,
Neurologic manifestations, or Prions/Genetics. This measure indicates how comprehensive the question is answered. Document MAP: This is the standard IR measure. The precision is measured at every point where a relevant document is obtained and then averaged over all relevant documents to obtain the average precision for a given query. For a set of queries, the mean of the average precision for all queries is the MAP of that IR system.
The output of the system is a list of passages ranked according to their similarities with the query. The performances on the three levels are then calculated based on the ranking of the passages.
The Wilcoxon signed-rank test was employed to determine the statistical significance of the results. In the tables of the following sections, statistically significant improvements (at the 5% level) are marked with an asterisk.
The initial baseline was established using word similarity only computed by the Okapi (Formula 4). Another run based on our basic conceptual IR model was performed without using query expansion, pseudo-feedback, or abbreviation correction. The experimental result is shown in Table 4.2.1. Our basic conceptual IR model significantly outperforms the Okapi on all three levels, which suggests that, although it requires additional efforts to identify concepts, retrieval on the concept level can achieve substantial improvements over purely term-based retrieval model.
A series of experiments were performed to examine how each type of domain-specific knowledge contributes to the retrieval performance. A new baseline was established using the basic conceptual IR model without incorporating any type of domainspecific knowledge. Then five runs were conducted by adding each individual type of domain-specific knowledge. We also conducted a run by adding all types of domain-specific knowledge.
Results of these experiments are shown in Table 4.2.2.
We found that any available type of domain-specific knowledge improved the performance in passage retrieval. The biggest improvement comes from the lexical variants, which is consistent with the result reported in [3]. This result also indicates that biologists are likely to use different variants of the same concept according to their own writing preferences and these variants might not be collected in the existing biomedical thesauruses. It also suggests that the biomedical IR systems can benefit from the domain-specific knowledge extracted from the literature by text mining systems.
Synonyms provided the second biggest improvement.
Hypernyms, hyponyms, and implicitly related concepts provided similar degrees of improvement. The overall performance is an accumulative result of adding different types of domain-specific knowledge and it is better than any individual addition. It is clearly shown that the performance is significantly improved (107% on passage level, 63.1% on aspect level, and 49.6% on document level) when the domain-specific knowledge is appropriately incorporated. Although it is not explicitly shown in Table 4.2.3, different types of domain-specific knowledge affect different subsets of queries. More specifically, each of these types (with the exception of the lexical variants which affects a large number of queries) affects only a few queries. But for those affected queries, their improvement is significant. As a consequence, the accumulative improvement is very significant.
Using the Baseline+All in Table 4.2.2 as a new baseline, the contribution of abbreviation correction and pseudo-feedback is given in Table 4.2.3. There is little improvement by avoiding incorrect matching of abbreviations. The pseudo-feedback contributed about 4.6% improvement in passage retrieval.
We compared our result with the results reported in the Genomics track of TREC 2006 [8] on the conditions that 1) systems are automatic systems and 2) passages are extracted from paragraphs.
The performance of our system relative to the best reported results is shown in Table 4.2.4 (in TREC 2006, some systems returned the whole paragraphs as passages. As a consequence, excellent retrieval results were obtained on document and aspect levels at the expense of performance on the passage level. We do not include the results of such systems here).
Table 4.2.4 Performance compared with best-reported results.
Passage MAP Aspect MAP Document MAP Best reported results 0.1486 0.3492 0.5320 Our results 0.1823 0.3811 0.5391 Improvement 22.68% 9.14% 1.33% The best reported results in the first row of Table 4.2.4 on three levels (passage, aspect, and document) are from different systems.
Our result is from a single run on passage retrieval in which it is better than the best reported result by 22.68% in passage retrieval and at the same time, 9.14% better in aspect retrieval, and 1.33% better in document retrieval (Since the average precision of each individual query was not reported, we can not apply the Wilcoxon signed-rank test to calculate the significance of difference between our performance and the best reported result.).
Table 4.2.1 Basic conceptual IR model vs. term-based model Run Passage Aspect Document MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%) Okapi 0.064 N/A 0.175 N/A 0.285 N/A Basic conceptual IR model 0.084* (+31.3%) 17 (65.4%) 0.233* (+33.1%) 12 (46.2%) 0.359* (+26.0%) 15 (57.7%) Table 4.2.2 Contribution of different types of domain-specific knowledge Run Passage Aspect Document MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%) Baseline = Basic conceptual IR model
Baseline+Synonyms 0.105 (+25%) 11 (42.3%) 0.246 (+5.6%) 9 (34.6%) 0.420 (+17%) 13 (50%) Baseline+Hypernyms 0.088 (+4.8%) 11 (42.3%) 0.225 (-3.4%) 9 (34.6%) 0.390 (+8.6%) 16 (61.5%) Baseline+Hyponyms 0.087 (+3.6%) 10 (38.5%) 0.217 (-6.9%) 7 (26.9%) 0.389 (+8.4%) 10 (38.5%) Baseline+Variants 0.150* (+78.6%) 16 (61.5%) 0.348* (+49.4%) 13 (50%) 0.495* (+37.9%) 10 (38.5%) Baseline+Related 0.086 (+2.4%) 9 (34.6%) 0.220 (-5.6%) 9 (34.6%) 0.387 (+7.8%) 13 (50%) Baseline+All 0.174* (107%) 25 (96.2%) 0.380* (+63.1%) 19 (73.1%) 0.537* (+49.6%) 14 (53.8%) Table 4.2.3 Contribution of abbreviation correction and pseudo-feedback Run Passage Aspect Document MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%) Baseline+All 0.174 N/A 0.380 N/A 0.537 N/A Baseline+All+Abbr 0.175 (+0.6%) 5 (19.2%) 0.375 (-1.3%) 4 (15.4%) 0.535 (-0.4%) 4 (15.4%) Baseline+All+Abbr+PF 0.182 (+4.6%) 10 (38.5%) 0.381 (+0.3%) 6 (23.1%) 0.539 (+0.4%) 9 (34.6%) A separate experiment has been done using a second testbed, the ad-hoc Task of TREC Genomics 2005, to evaluate our knowledge-intensive conceptual IR model for document retrieval of biomedical literature. The overall performance in terms of MAP is 35.50%, which is about 22.92% above the best reported result [9]. Notice that the performance was only measured on the document level for the ad-hoc Task of TREC Genomics 2005.
Many studies used manually-crafted thesauruses or knowledge databases created by text mining systems to improve retrieval effectiveness based on either word-statistical retrieval systems or conceptual retrieval systems. [11][1] assessed query expansion using the UMLS Metathesaurus. Based on a word-statistical retrieval system, [11] used definitions and different types of thesaurus relationships for query expansion and a deteriorated performance was reported. [1] expanded queries with phrases and UMLS concepts determined by the MetaMap, a program which maps biomedical text to UMLS concepts, and no significant improvement was shown. We used MeSH, Entrez gene, and other non-thesaurus knowledge resources such as an abbreviation database for query expansion. A critical difference between our work and those in [11][1] is that our retrieval model is based on concepts, not on individual words.
The Genomics track in TREC provides a common platform to evaluate methods and techniques proposed by various groups for biomedical information retrieval. As summarized in [8][9][10], many groups utilized domain-specific knowledge to improve retrieval effectiveness. Among these groups, [3] assessed both thesaurus-based knowledge, such as gene information, and non thesaurus-based knowledge, such as lexical variants of gene symbols, for query expansion. They have shown that query expansion with acronyms and lexical variants of gene symbols produced the biggest improvement, whereas, the query expansion with gene information from gene databases deteriorated the performance. [21] used a similar approach for generating lexical variants of gene symbols and reported significant improvements.
Our system utilized more types of domain-specific knowledge, including hyponyms, hypernyms and implicitly related concepts.
In addition, under the conceptual retrieval framework, we examined more comprehensively the effects of different types of domain-specific knowledge in performance contribution. [20][15] utilized WordNet, a database of English words and their lexical relationships developed by Princeton University, for query expansion in the non-biomedical domain. In their studies, queries were expanded using the lexical semantic relations such as synonyms, hypernyms, or hyponyms. Little benefit has been shown in [20]. This has been due to ambiguity of the query terms which have different meanings in different contexts. When these synonyms having multiple meanings are added to the query, substantial irrelevant documents are retrieved. In the biomedical domain, this kind of ambiguity of query terms is relatively less frequent, because, although the abbreviations are highly ambiguous, general biomedical concepts usually have only one meaning in the thesaurus, such as UMLS, whereas a term in WordNet usually have multiple meanings (represented as synsets in WordNet). Besides, we have implemented a post-ranking step to reduce the number of incorrect matches of abbreviations, which will hopefully decrease the negative impact caused by the abbreviation ambiguity. Besides, we have implemented a postranking step to reduce the number of incorrect matches of abbreviations, which will hopefully decrease the negative impact caused by the abbreviation ambiguity. The retrieval model in [15] emphasized the similarity between a query and a document on the phrase level assuming that phrases are more important than individual words when retrieving documents. Although the assumption is similar, our conceptual model is based on the biomedical concepts, not phrases. [13] presented a good study of the role of knowledge in the document retrieval of clinical medicine. They have shown that appropriate use of semantic knowledge in a conceptual retrieval framework can yield substantial improvements. Although the retrieval model is similar, we made a study in the domain of genomics, in which the problem structure and task knowledge is not as well-defined as in the domain of clinical medicine [18].
Also, our similarity function is very different from that in [13].
In summary, our approach differs from previous works in four important ways: First, we present a case study of conceptual retrieval in the domain of genomics, where many knowledge resources can be used to improve the performance of biomedical IR systems. Second, we have studied more types of domainspecific knowledge than previous researchers and carried out more comprehensive experiments to look into the effects of different types of domain-specific knowledge in performance contribution.
Third, although some of the techniques seem similar to previously published ones, they are actually quite different in details. For example, in our pseudo-feedback process, we require that the unit of feedback is a concept and the concept has to be of the same semantic type as a query concept. This is to ensure that our conceptual model of retrieval can be applied. As another example, the way in which implicitly related concepts are extracted in this paper is significantly different from that given in [19]. Finally, our conceptual IR model is actually based on complex concepts because some biomedical meanings, such as biological processes, are represented by multiple simple concepts.
This paper proposed a conceptual approach to utilize domainspecific knowledge in an IR system to improve its effectiveness in retrieving biomedical literature. We specified five different types of domain-specific knowledge (i.e., synonyms, hyponyms, hypernyms, lexical variants, and implicitly related concepts) and examined their effects in performance contribution. We also evaluated other two techniques, pseudo-feedback and abbreviation correction. Experimental results have shown that appropriate use of domain-specific knowledge in a conceptual IR model yields significant improvements (23%) in passage retrieval over the best known results. In our future work, we will explore the use of other existing knowledge resources, such as UMLS and the Wikipedia, and evaluate techniques such as disambiguation of gene symbols for improving retrieval effectiveness. The application of our conceptual IR model in other domains such as clinical medicine will be investigated.
insightful discussion.
[1] Aronson A.R., Rindflesch T.C. Query expansion using the UMLS Metathesaurus. Proc AMIA Annu Fall Symp. 1997. 485-9. [2] Baeza-Yates R., Ribeiro-Neto B. Modern Information Retrieval. Addison-Wesley, 1999, 129-131. [3] Buttcher S., Clarke C.L.A., Cormack G.V. Domain-specific synonym expansion and validation for biomedical information retrieval (MultiText experiments for TREC 2004). TREC"04. [4] Chang J.T., Schutze H., Altman R.B. Creating an online dictionary of abbreviations from MEDLINE. Journal of the American Medical Informatics Association. 2002 9(6). [5] Church K.W., Hanks P. Word association norms, mutual information and lexicography. Computational Linguistics. 1990;16:22, C29. [6] Fontelo P., Liu F., Ackerman M. askMEDLINE: a free-text, natural language query tool for MEDLINE/PubMed. BMC Med Inform Decis Mak. 2005 Mar 10;5(1):5. [7] Fukuda K., Tamura A., Tsunoda T., Takagi T. Toward information extraction: identifying protein names from biological papers. Pac Symp Biocomput. 1998;:707-18. [8] Hersh W.R., and etc. TREC 2006 Genomics Track Overview.
TREC"06. [9] Hersh W.R., and etc. TREC 2005 Genomics Track Overview.
In TREC"05. [10] Hersh W.R., and etc. TREC 2004 Genomics Track Overview.
In TREC"04. [11] Hersh W.R., Price S., Donohoe L. Assessing thesaurus-based query expansion using the UMLS Metathesaurus. Proc AMIA Symp. 344-8. 2000. [12] Levenshtein, V. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics - Doklady 10, 10 (1996), 707-710. [13] Lin J., Demner-Fushman D. The Role of Knowledge in Conceptual Retrieval: A Study in the Domain of Clinical Medicine. SIGIR"06. 99-06. [14] Lindberg D., Humphreys B., and McCray A. The Unified Medical Language System. Methods of Information in Medicine. 32(4):281-291, 1993. [15] Liu S., Liu F., Yu C., and Meng W.Y. An Effective Approach to Document Retrieval via Utilizing WordNet and Recognizing Phrases. SIGIR"04. 266-272 [16] Proux D., Rechenmann F., Julliard L., Pillet V.V., Jacq B.

The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects. With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications. Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone. This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.
The idf alone works better than the tf alone does. An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms. We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document. We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term. The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document. Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf . In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .
From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.
The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g.: informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved. An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1]. For example, consider a normalisation based on the maximal idf -value. Let T be the set of terms occurring in a collection.
Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.
Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?
When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events. These observations are reported in section 3. We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events. In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term. The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory. In section 4, we link the results of the previous sections to probability theory. We show the steps from possible worlds to binomial distribution and Poisson distribution.
In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf . Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions.
The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers. In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model. The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability. The Poisson model was here applied to the term frequency of a term in a document. We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively. Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates. The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson. Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf . We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference. A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events. The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.
Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related. The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences. The Pareto distribution is used by [2] for term frequency normalisation.
The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do. This makes Pareto interesting since Poisson is felt to be too radical on frequent events.
We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based. This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination.
We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3).
and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.
We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0. The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.
P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal. The document containment probability reflect the chance that a document occurs in a collection. This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.
Containment could be derived, for example, from the size, quality, age, links, etc. of a document. For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability. We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.
P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0. Next, we define the frequency-based noise probability and the total noise probability for disjoint documents. We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.
Definition 1. The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2. The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .
Theorem 1. IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.
Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof. The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf . The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.
From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate. Still, idf works well. This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents. From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise. The occurrence and containment can be term specific. For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N). We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c). We obtain P(t∧d|c) = 1/ND(c).
In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document. This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability. The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.
The disjointness assumption is typical for frequency-based probabilities. From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .
But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document? In the next section, we replace the disjointness assumption by the independence assumption.
for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.
P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2. For independent documents, we use now the conjunction of negated events.
Definition 3. The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N. Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability. We show through the next theorem that the upper bound of the noise probability depends on λ.
Theorem 2. The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof. The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function). With x = −λ, we obtain: lim N→∞
λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.
Therefore, a term with n = N has the largest noise probability.
For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞
λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents. In the disjoint case, the noise probability is one for a term that occurs in all documents.
If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents. Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents. Alternatively, we can assume a constant containment and a term-dependent occurrence. If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document. The common assumption is that the average containment or occurrence probability is proportional to n(t). However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).
For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ . In the next section, we relate the value e−λ to information theory.
signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition. Consider the definition of the entropy contribution H(t) of a signal t.
H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use:
The entropy contribution H(t) is maximal for P(t) = e−1 .
This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = −
ln b We summarise this result in the following theorem: Theorem 3. The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal. The entropy of a maximal informative signal is Hmax = e−1 .
Proof. The probability and entropy follow from the derivation above.
The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal. We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ . We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy. By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory. Next, we link independent documents to probability theory.
We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution.
Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.
For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true.
The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p.
P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events. With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.
The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞
λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit.
For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal. This shows the relationship of the Poisson distribution and information theory.
After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability. First, we define the Poisson noise probability: Definition 4. The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k! For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.
Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy. Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space.
DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1. We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection. Let ND := |D| and NT := |T| be the number of documents and terms, respectively. We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t.
In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document. The document dimension in a collection space corresponds to the location (position) dimension in a document space.
The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document. For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.
We have highlighted in this section the duality between the collection space and the document space. We concentrate in this paper on the probability of a term to be noisy and informative. Those probabilities are defined in the collection space. However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document. Also, the results can be applied to containment of documents and locations.
INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.
In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.
Definition 5. The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).
Definition 6. The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1. Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ . We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t). The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.
Consider the illustration of the noise and informativeness definitions in figure 1. The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs. The frequency-based noise corresponds to the linear solid curve in the noise figure. With an independence assumption, we obtain the curve in the lower triangle of the noise figure. By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve. The noise figure shows the lifting for the value λ := ln N ≈
frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ. Whether we can draw more conclusions from this setting is an open question.
We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0
1
Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0
1
Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy. On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.
The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ. For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one. This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution. Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! The two dimensional Poisson shows a plateau between λ1 =
behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.
For the informativeness, we observe that the radical behaviour of Poisson is preserved. The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5. The logarithm of the noise is normalised by the logarithm of a very small number, namely
+ 0.5 · e−2000 . That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero. This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little. The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.
The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.
However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.
For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance. For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).
In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does. Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters. The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters. The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit. The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents. Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.
This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.
In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability. This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections.
The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches. We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.
By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document. We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.
By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson. From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative. The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively. We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.
The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper. Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively. Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document. This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models. The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.
Acknowledgment: I would like to thank Mounia Lalmas,
Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces. My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent. This work was funded by a research fellowship from Queen Mary University of London.
[1] A. Aizawa. An information-theoretic perspective of tf-idf measures. Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen. Term frequency normalization via Pareto distributions. In 24th BCS-IRSG European Colloquium on IR Research,
Glasgow, Scotland, 2002. [3] R. K. Belew. Finding out about. Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein. Taschenbuch der Mathematik. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale. Poisson mixtures. Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale. Inverse document frequency: A measure of deviations from poisson. In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel. Links between information construction and information gain: Entropy and bibliometric distribution. Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis. N-poisson document modelling. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994. Springer-Verlag. [11] S. Wong and Y. Yao. An information-theoric measure of term specificity. Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao. On modeling information retrieval with probabilistic inference. ACM Transactions on Information Systems, 13(1):38-68,

The emergence of the Internet has opened up new marketing opportunities. In fact, a company has now the possibility of showing its advertisements (ads) to millions of people at a low cost. During the 90"s, many companies invested heavily on advertising in the Internet with apparently no concerns about their investment return [16]. This situation radically changed in the following decade when the failure of many Web companies led to a dropping in supply of cheap venture capital and a considerable reduction in on-line advertising investments [15,16].
It was clear then that more effective strategies for on-line advertising were required. For that, it was necessary to take into account short-term and long-term interests of the users related to their information needs [9,14]. As a consequence, many companies intensified the adoption of intrusive techniques for gathering information of users mostly without their consent [8]. This raised privacy issues which stimulated the research for less invasive measures [16].
More recently, Internet information gatekeepers as, for example, search engines, recommender systems, and comparison shopping services, have employed what is called paid placement strategies [3]. In such methods, an advertiser company is given prominent positioning in advertisement lists in return for a placement fee. Amongst these methods, the most popular one is a non-intrusive technique called keyword targeted marketing [16]. In this technique, keywords extracted from the user"s search query are matched against keywords associated with ads provided by advertisers. A ranking of the ads, which also takes into consideration the amount that each advertiser is willing to pay, is computed.
The top ranked ads are displayed in the search result page together with the answers for the user query.
The success of keyword targeted marketing has motivated information gatekeepers to offer their advertisement services in different contexts. For example, as shown in Figure 1, relevant ads could be shown to users directly in the pages of information portals. The motivation is to take advantage of 496 the users immediate information interests at browsing time.
The problem of matching ads to a Web page that is browsed, which we also refer to as content-targeted advertising [1], is different from that of keyword marketing. In this case, instead of dealing with users" keywords, we have to use the contents of a Web page to decide which ads to display.
Figure 1: Example of content-based advertising in the page of a newspaper. The middle slice of the page shows the beginning of an article about the launch of a DVD movie. At the bottom slice, we can see advertisements picked for this page by Google"s content-based advertising system, AdSense.
It is important to notice that paid placement advertising strategies imply some risks to information gatekeepers.
For instance, there is the possibility of a negative impact on their credibility which, at long term, can demise their market share [3]. This makes investments in the quality of ad recommendation systems even more important to minimize the possibility of exhibiting ads unrelated to the user"s interests. By investing in their ad systems, information gatekeepers are investing in the maintenance of their credibility and in the reinforcement of a positive user attitude towards the advertisers and their ads [14]. Further, that can translate into higher clickthrough rates that lead to an increase in revenues for information gatekeepers and advertisers, with gains to all parts [3].
In this work, we focus on the problem of content-targeted advertising. We propose new strategies for associating ads with a Web page. Five of these strategies are referred to as matching strategies. They are based on the idea of matching the text of the Web page directly to the text of the ads and its associated keywords. Five other strategies, which we here introduce, are referred to as impedance coupling strategies.
They are based on the idea of expanding the Web page with new terms to facilitate the task of matching ads and Web pages. This is motivated by the observation that there is frequently a mismatch between the vocabulary of a Web page and the vocabulary of an advertisement. We say that there is a vocabulary impedance problem and that our technique provides a positive effect of impedance coupling by reducing the vocabulary impedance. Further, all our strategies rely on information that is already available to information gatekeepers that operate keyword targeted advertising systems.
Thus, no other data from the advertiser is required.
Using a sample of a real case database with over 93,000 ads and 100 Web pages selected for testing, we evaluate our ad recommendation strategies. First, we evaluate the five matching strategies. They match ads to a Web page using a standard vector model and provide what we may call trivial solutions. Our results indicate that a strategy that matches the ad plus its keywords to a Web page, requiring the keywords to appear in the Web page, provides improvements in average precision figures of roughly 60% relative to a strategy that simply matches the ads to the Web page.
Such strategy, which we call AAK (for ads and keywords), is then taken as our baseline.
Following we evaluate the five impedance coupling strategies. They are based on the idea of expanding the ad and the Web page with new terms to reduce the vocabulary impedance between their texts. Our results indicate that it is possible to generate extra improvements in average precision figures of roughly 50% relative to the AAK strategy.
The paper is organized as follows. In section 2, we introduce five matching strategies to solve content-targeted advertising. In section 3, we present our impedance coupling strategies. In section 4, we describe our experimental methodology and datasets and discuss our results. In section 5 we discuss related work. In section 6 we present our conclusions.
Keyword advertising relies on matching search queries to ads and its associated keywords. Context-based advertising, which we address here, relies on matching ads and its associated keywords to the text of a Web page.
Given a certain Web page p, which we call triggering page, our task is to select advertisements related to the contents of p. Without loss of generality, we consider that an advertisement ai is composed of a title, a textual description, and a hyperlink. To illustrate, for the first ad by Google shown in Figure 1, the title is Star Wars Trilogy Full, the description is Get this popular DVD free. Free w/ free shopping. Sign up now, and the hyperlink points to the site www.freegiftworld.com. Advertisements can be grouped by advertisers in groups called campaigns, such that a campaign can have one or more advertisements.
Given our triggering page p and a set A of ads, a simple way of ranking ai ∈ A with regard to p is by matching the contents of p to the contents of ai. For this, we use the vector space model [2], as discussed in the immediately following.
In the vector space model, queries and documents are represented as weighted vectors in an n-dimensional space. Let wiq be the weight associated with term ti in the query q and wij be the weight associated with term ti in the document dj. Then, q = (w1q, w2q, ..., wiq, ..., wnq) and dj = (w1j, w2j, ..., wij, ..., wnj) are the weighted vectors used to represent the query q and the document dj. These weights can be computed using classic tf-idf schemes. In such schemes, weights are taken as the product between factors that quantify the importance of a term in a document (given by the term frequency, or tf, factor) and its rarity in the whole collection (given by the inverse document factor, or idf, factor), see [2] for details. The ranking of the query q with regard to the document dj is computed by the cosine similarity 497 formula, that is, the cosine of the angle between the two corresponding vectors: sim(q, dj) = q • dj |q| × |dj| = Pn i=1 wiq · wij qPn i=1 w2 iq qPn i=1 w2 ij (1) By considering p as the query and ai as the document, we can rank the ads with regard to the Web page p. This is our first matching strategy. It is represented by the function AD given by: AD(p, ai) = sim(p, ai) where AD stands for direct match of the ad, composed by title and description and sim(p, ai) is computed according to Eq. (1).
In our second method, we use other source of evidence provided by the advertisers: the keywords. With each advertisement ai an advertiser associates a keyword ki, which may be composed of one or more terms. We denote the association between an advertisement ai and a keyword ki as the pair (ai, ki) ∈ K, where K is the set of associations made by the advertisers. In the case of keyword targeted advertising, such keywords are used to match the ads to the user queries. In here, we use them to match ads to the Web page p. This provides our second method for ad matching given by: KW(p, ai) = sim(p, ki) where (ai, ki) ∈ K and KW stands for match the ad keywords.
We notice that most of the keywords selected by advertisers are also present in the ads associated with those keywords. For instance, in our advertisement test collection, this is true for 90% of the ads. Thus, instead of using the keywords as matching devices, we can use them to emphasize the main concepts in an ad, in an attempt to improve our AD strategy. This leads to our third method of ad matching given by: AD KW(p, ai) = sim(p, ai ∪ ki) where (ai, ki) ∈ K and AD KW stands for match the ad and its keywords.
Finally, it is important to notice that the keyword ki associated with ai could not appear at all in the triggering page p, even when ai is highly ranked. However, if we assume that ki summarizes the main topic of ai according to an advertiser viewpoint, it can be interesting to assure its presence in p. This reasoning suggests that requiring the occurrence of the keyword ki in the triggering page p as a condition to associate ai with p might lead to improved results. This leads to two extra matching strategies as follows: ANDKW(p, ai) =  sim(p, ai) if ki p
AD ANDKW(p, ai) = AAK(p, ai) =  sim(p, ai ∪ ki) if ki p
where (ai, ki) ∈ K, ANDKW stands for match the ad keywords and force their appearance, and AD ANDKW (or AAK for ads and keywords) stands for match the ad, its keywords, and force their appearance.
As we will see in our results, the best among these simple methods is AAK. Thus, it will be used as baseline for our impedance coupling strategies which we now discuss.
Two key issues become clear as one plays with the contenttargeted advertising problem. First, the triggering page normally belongs to a broader contextual scope than that of the advertisements. Second, the association between a good advertisement and the triggering page might depend on a topic that is not mentioned explicitly in the triggering page.
The first issue is due to the fact that Web pages can be about any subject and that advertisements are concise in nature. That is, ads tend to be more topic restricted than Web pages. The second issue is related to the fact that, as we later discuss, most advertisers place a small number of advertisements. As a result, we have few terms describing their interest areas. Consequently, these terms tend to be of a more general nature. For instance, a car shop probably would prefer to use car instead of super sport to describe its core business topic. As a consequence, many specific terms that appear in the triggering page find no match in the advertisements. To make matters worst, a page might refer to an entity or subject of the world through a label that is distinct from the label selected by an advertiser to refer to the same entity.
A consequence of these two issues is that vocabularies of pages and ads have low intersection, even when an ad is related to a page. We cite this problem from now on as the vocabulary impedance problem. In our experiments, we realized that this problem limits the final quality of direct matching strategies. Therefore, we studied alternatives to reduce the referred vocabulary impedance.
For this, we propose to expand the triggering pages with new terms. Figure 2 illustrates our intuition. We already know that the addition of keywords (selected by the advertiser) to the ads leads to improved results. We say that a keyword reduces the vocabulary impedance by providing an alternative matching path. Our idea is to add new terms (words) to the Web page p to also reduce the vocabulary impedance by providing a second alternative matching path.
We refer to our expansion technique as impedance coupling.
For this, we proceed as follows. expansion terms keyword vocabulary impedance triggering page p ad Figure 2: Addition of new terms to a Web page to reduce the vocabulary impedance.
An advertiser trying to describe a certain topic in a concise way probably will choose general terms to characterize that topic. To facilitate the matching between this ad and our triggering page p, we need to associate new general terms with p. For this, we assume that Web documents similar to the triggering page p share common topics. Therefore, 498 by inspecting the vocabulary of these similar documents we might find good terms for better characterizing the main topics in the page p. We now describe this idea using a Bayesian network model [10,11,13] depicted in Figure 3.
R D0 D1 Dj Dk T1 T2 T3 Ti Tm ... ... ... ...
Figure 3: Bayesian network model for our impedance coupling technique.
In our model, which is based on the belief network in [11], the nodes represent pieces of information in the domain.
With each node is associated a binary random variable, which takes the value 1 to mean that the corresponding entity (a page or terms) is observed and, thus, relevant in our computations. In this case, we say that the information was observed. Node R represents the page r, a new representation for the triggering page p. Let N be the set of the k most similar documents to the triggering page, including the triggering page p itself, in a large enough Web collection C.
Root nodes D0 through Dk represent the documents in N, that is, the triggering page D0 and its k nearest neighbors,
D1 through Dk, among all pages in C. There is an edge from node Dj to node R if document dj is in N. Nodes T1 through Tm represent the terms in the vocabulary of C.
There is an edge from node Dj to a node Ti if term ti occurs in document dj. In our model, the observation of the pages in N leads to the observation of a new representation of the triggering page p and to a set of terms describing the main topics associated with p and its neighbors.
Given these definitions, we can now use the network to determine the probability that a term ti is a good term for representing a topic of the triggering page p. In other words, we are interested in the probability of observing the final evidence regarding a term ti, given that the new representation of the page p has been observed, P(Ti = 1|R = 1).
This translates into the following equation1 : P(Ti|R) = 1 P(R) X d P(Ti|d)P(R|d)P(d) (2) where d represents the set of states of the document nodes.
Since we are interested just in the states in which only a single document dj is observed and P(d) can be regarded as a constant, we can rewrite Eq. (2) as: P(Ti|R) = ν P(R) kX j=0 P(Ti|dj)P(R|dj) (3) where dj represents the state of the document nodes in which only document dj is observed and ν is a constant 1 To simplify our notation we represent the probabilities P(X = 1) as P(X) and P(X = 0) as P(X). associated with P(dj). Eq. (3) is the general equation to compute the probability that a term ti is related to the triggering page. We now define the probabilities P(Ti|dj) and P(R|dj) as follows: P(Ti|dj) = η wij (4) P(R|dj) =  (1 − α) j = 0 α sim(r, dj) 1 ≤ j ≤ k (5) where η is a normalizing constant, wij is the weight associated with term ti in the document dj, and sim(p, dj) is given by Eq. (1), i.e., is the cosine similarity between p and dj. The weight wij is computed using a classic tf-idf scheme and is zero if term ti does not occur in document dj. Notice that P(Ti|dj) = 1 − P(Ti|dj) and P(R|dj) = 1 − P(R|dj).
By defining the constant α, it is possible to determine how important should be the influence of the triggering page p to its new representation r. By substituting Eq. (4) and Eq. (5) into Eq. (3), we obtain: P(Ti|R) = ρ ((1 − α) wi0 + α kX j=1 wij sim(r, dj)) (6) where ρ = η ν is a normalizing constant.
We use Eq. (6) to determine the set of terms that will compose r, as illustrated in Figure 2. Let ttop be the top ranked term according to Eq. (6). The set r is composed of the terms ti such that P (Ti|R) P (Ttop|R) ≥ β, where β is a given threshold. In our experiments, we have used β = 0.05.
Notice that the set r might contain terms that already occur in p. That is, while we will refer to the set r as expansion terms, it should be clear that p ∩ r = ∅.
By using α = 0, we simply consider the terms originally in page p. By increasing α, we relax the context of the page p, adding terms from neighbor pages, turning page p into its new representation r. This is important because, sometimes, a topic apparently not important in the triggering page offers a good opportunity for advertising. For example, consider a triggering page that describes a congress in London about digital photography. Although London is probably not an important topic in this page, advertisements about hotels in London would be appropriate. Thus, adding hotels to page p is important. This suggests using α > 0, that is, preserving the contents of p and using the terms in r to expand p.
In this paper, we examine both approaches. Thus, in our sixth method we match r, the set of new expansion terms, directly to the ads, as follows: AAK T(p, ai) = AAK(r, ai) where AAK T stands for match the ad and keywords to the set r of expansion terms.
In our seventh method, we match an expanded page p to the ads as follows: AAK EXP(p, ai) = AAK(p ∪ r, ai) where AAK EXP stands for match the ad and keywords to the expanded triggering page. 499 To improve our ad placement methods, other external source that we can use is the content of the page h pointed to by the advertisement"s hyperlink, that is, its landing page.
After all, this page comprises the real target of the ad and perhaps could present a more detailed description of the product or service being advertised. Given that the advertisement ai points to the landing page hi, we denote this association as the pair (ai, hi) ∈ H, where H is the set of associations between the ads and the pages they point to.
Our eighth method consists of matching the triggering page p to the landing pages pointed to by the advertisements, as follows: H(p, ai) = sim(p, hi) where (ai, hi) ∈ H and H stands for match the hyperlink pointed to by the ad.
We can also combine this information with the more promising methods previously described, AAK and AAK EXP as follows. Given that (ai, hi) ∈ H and (ai, ki) ∈ K, we have our last two methods: AAK H(p, ai) =  sim(p, ai ∪ hi ∪ ki) if ki p
AAK EXP H(p, ai) =  sim(p ∪ r, ai ∪ hi ∪ ki) if ki (p ∪ r)
where AAK H stands for match ads and keywords also considering the page pointed by the ad and AAH EXP H stands for match ads and keywords with expanded triggering page, also considering the page pointed by the ad.
Notice that other combinations were not considered in this study due to space restrictions. These other combinations led to poor results in our experimentation and for this reason were discarded.
To evaluate our ad placement strategies, we performed a series of experiments using a sample of a real case ad collection with 93,972 advertisements, 1,744 advertisers, and 68,238 keywords2 . The advertisements are grouped in 2,029 campaigns with an average of 1.16 campaigns per advertiser.
For the strategies AAK T and AAK EXP, we had to generate a set of expansion terms. For that, we used a database of Web pages crawled by the TodoBR search engine [12] (http://www.todobr.com.br/). This database is composed of 5,939,061 pages of the Brazilian Web, under the domain .br. For the strategies H, AAK H, and AAK EXP H, we also crawled the pages pointed to by the advertisers. No other filtering method was applied to these pages besides the removal of HTML tags.
Since we are initially interested in the placement of advertisements in the pages of information portals, our test collection was composed of 100 pages extracted from a Brazilian newspaper. These are our triggering pages. They were crawled in such a way that only the contents of their articles was preserved. As we have no preferences for particular 2 Data in portuguese provided by an on-line advertisement company that operates in Brazil. topics, the crawled pages cover topics as diverse as politics, economy, sports, and culture.
For each of our 100 triggering pages, we selected the top three ranked ads provided by each of our 10 ad placement strategies. Thus, for each triggering page we select no more than 30 ads. These top ads were then inserted in a pool for that triggering page. Each pool contained an average of
submitted to a manual evaluation by a group of 15 users.
The average number of relevant advertisements per page pool was 5.15. Notice that we adopted the same pooling method used to evaluate the TREC Web-based collection [6].
To quantify the precision of our results, we used 11-point average figures [2]. Since we are not able to evaluate the entire ad collection, recall values are relative to the set of evaluated advertisements.
We start by analyzing the impact of different idf factors in our advertisement collection. Idf factors are important because they quantify how discriminative is a term in the collection. In our ad collection, idf factors can be computed by taking ads, advertisers or campaigns as documents. To exemplify, consider the computation of ad idf for a term ti that occurs 9 times in a collection of 100 ads. Then, the inverse document frequency of ti is given by: idfi = log 100 9 Hence, we can compute ad, advertiser or campaign idf factors. As we observe in Figure 4, for the AD strategy, the best ranking is obtained by the use of campaign idf, that is, by calculating our idf factor so that it discriminates campaigns.
Similar results were obtained for all the other methods. 0
precision recall Campaign idf Advertiser idf Ad idf Figure 4: Precision-recall curves obtained for the AD strategy using ad, advertiser, and campaign idf factors.
This reflects the fact that terms might be better discriminators for a business topic than for an specific ad. This effect can be accomplished by calculating the factor relative to idf advertisers or campaigns instead of ads. In fact, campaign idf factors yielded the best results. Thus, they will be used in all the experiments reported from now on. 500
Matching Strategies Figure 5 displays the results for the matching strategies presented in Section 2. As shown, directly matching the contents of the ad to the triggering page (AD strategy) is not so effective. The reason is that the ad contents are very noisy.
It may contain messages that do not properly describe the ad topics such as requisitions for user actions (e.g, visit our site) and general sentences that could be applied to any product or service (e.g, we delivery for the whole country). On the other hand, an advertiser provided keyword summarizes well the topic of the ad. As a consequence, the KW strategy is superior to the AD and AD KW strategies. This situation changes when we require the keywords to appear in the target Web page. By filtering out ads whose keywords do not occur in the triggering page, much noise is discarded.
This makes ANDKW a better alternative than KW. Further, in this new situation, the contents of the ad becomes useful to rank the most relevant ads making AD ANDKW (or AAK for ads and keywords) the best among all described methods.
For this reason, we adopt AAK as our baseline in the next set of experiments. 0
precision recall AAK ANDKW KW AD_KW AD Figure 5: Comparison among our five matching strategies. AAK (ads and keywords) is superior.
Table 1 illustrates average precision figures for Figure 5.
We also present actual hits per advertisement slot. We call hit an assignment of an ad (to the triggering page) that was considered relevant by the evaluators. We notice that our AAK strategy provides a gain in average precision of 60% relative to the trivial AD strategy. This shows that careful consideration of the evidence related to the problem does pay off.
Impedance Coupling Strategies Table 2 shows top ranked terms that occur in a page covering Argentinean wines produced using grapes derived from the Bordeaux region of France. The p column includes the top terms for this page ranked according to our tf-idf weighting scheme. The r column includes the top ranked expansion terms generated according to Eq. (6). Notice that the expansion terms not only emphasize important terms of the target page (by increasing their weights) such as wines and Methods Hits 11-pt average #1 #2 #3 total score gain(%) AD 41 32 13 86 0.104 AD KW 51 28 17 96 0.106 +1.9 KW 46 34 28 108 0.125 +20.2 ANDKW 49 37 35 121 0.153 +47.1 AD ANDKW (AAK) 51 48 39 138 0.168 +61.5 Table 1: Average precision figures, corresponding to Figure 5, for our five matching strategies. Columns labelled #1, #2, and #3 indicate total of hits in first, second, and third advertisement slots, respectively. The AAK strategy provides improvements of 60% relative to the AD strategy.
Rank p r term score term score
...
-...
Table 2: Top ranked terms for the triggering page p according to our tf-idf weighting scheme and top ranked terms for r, the expansion terms for p, generated according to Eq. (6). Ranking scores were normalized in order to sum up to 1. Terms marked with ‘*" are not shared by the sets p and r. whites, but also reveal new terms related to the main topic of the page such as aroma and red. Further, they avoid some uninteresting terms such as obtained and country.
Figure 6 illustrates our results when the set r of expansion terms is used. They show that matching the ads to the terms in the set r instead of to the triggering page p (AAK T strategy) leads to a considerable improvement over our baseline, AAK. The gain is even larger when we use the terms in r to expand the triggering page (AAK EXP method).
This confirms our hypothesis that the triggering page could have some interesting terms that should not be completely discarded.
Finally, we analyze the impact on the ranking of using the contents of pages pointed by the ads. Figure 7 displays our results. It is clear that using only the contents of the pages pointed by the ads (H strategy) yields very poor results.
However, combining evidence from the pages pointed by the ads with our baseline yields improved results. Most important, combining our best strategy so far (AAK EXP) with pages pointed by ads (AAK EXP H strategy) leads to superior results. This happens because the two additional sources of evidence, expansion terms and pages pointed by the ads, are distinct and complementary, providing extra and valuable information for matching ads to a Web page. 501 0
precision recall AAK_EXP AAK_T AAK Figure 6: Impact of using a new representation for the triggering page, one that includes expansion terms. 0
precision recall AAK_EXP_H AAK_H AAK H Figure 7: Impact of using the contents of the page pointed by the ad (the hyperlink).
Figure 8 and Table 3 summarize all results described in this section. In Figure 8 we show precision-recall curves and in Table 3 we show 11-point average figures. We also present actual hits per advertisement slot and gains in average precision relative to our baseline, AAK. We notice that the highest number of hits in the first slot was generated by the method AAK EXP. However, the method with best overall retrieval performance was AAK EXP H, yielding a gain in average precision figures of roughly 50% over the baseline (AAK).
In a keyword targeted advertising system, ads are assigned at query time, thus the performance of the system is a very important issue. In content-targeted advertising systems, we can associate ads with a page at publishing (or updating) time. Also, if a new ad comes in we might consider assigning this ad to already published pages in oﬄine mode.
That is, we might design the system such that its performance depends fundamentally on the rate that new pages 0
precision recall AAK_EXP_H AAK_EXP AAK_T AAK_H AAK H Figure 8: Comparison among our ad placement strategies.
Methods Hits 11-pt average #1 #2 #3 total score gain(%) H 28 5 6 39 0.026 -84.3 AAK 51 48 39 138 0.168 AAK H 52 50 46 148 0.191 +13.5 AAK T 65 49 43 157 0.226 +34.6 AAK EXP 70 52 53 175 0.242 +43.8 AAK EXP H 64 61 51 176 0.253 +50.3 Table 3: Results for our impedance coupling strategies. are published and the rate that ads are added or modified.
Further, the data needed by our strategies (page crawling, page expansion, and ad link crawling) can be gathered and processed oﬄine, not affecting the user experience. Thus, from this point of view, the performance is not critical and will not be addressed in this work.
Several works have stressed the importance of relevance in advertising. For example, in [14] it was shown that advertisements that are presented to users when they are not interested on them are viewed just as annoyance. Thus, in order to be effective, the authors conclude that advertisements should be relevant to consumer concerns at the time of exposure. The results in [9] enforce this conclusion by pointing out that the more targeted the advertising, the more effective it is.
Therefore it is not surprising that other works have addressed the relevance issue. For instance, in [8] it is proposed a system called ADWIZ that is able to adapt online advertisement to a user"s short-term interests in a non-intrusive way. Contrary to our work, ADWIZ does not directly use the content of the page viewed by the user. It relies on search keywords supplied by the user to search engines and on the URL of the page requested by the user. On the other hand, in [7] the authors presented an intrusive approach in which an agent sits between advertisers and the user"s browser allowing a banner to be placed into the currently viewed page.
In spite of having the opportunity to use the page"s content, 502 the agent infers relevance based on category information and user"s private information collected along the time.
In [5] the authors provide a comparison between the ranking strategies used by Google and Overture for their keyword advertising systems. Both systems select advertisements by matching them to the keywords provided by the user in a search query and rank the resulting advertisement list according to the advertisers" willingness to pay. In particular, Google approach also considers the clickthrough rate of each advertisement as an additional evidence for its relevance. The authors conclude that Google"s strategy is better than that used by Overture. As mentioned before, the ranking problem in keyword advertising is different from that of content-targeted advertising. Instead of dealing with keywords provided by users in search queries, we have to deal with the contents of a page which can be very diffuse.
Finally, the work in [4] focuses on improving search engine results in a TREC collection by means of an automatic query expansion method based on kNN [17]. Such method resembles our expansion approach presented in section 3.
Our method is different from that presented by [4]. They expand user queries applied to a document collection with terms extracted from the top k documents returned as answer to the query in the same collection. In our case, we use two collections: an advertisement and a Web collection.
We expand triggering pages with terms extracted from the Web collection and then we match these expanded pages to the ads from the advertisement collection. By doing this, we emphasize the main topics of the triggering pages, increasing the possibility of associating relevant ads with them.
In this work we investigated ten distinct strategies for associating ads with a Web page that is browsed (contenttargeted advertising). Five of our strategies attempt to match the ads directly to the Web page. Because of that, they are called matching strategies. The other five strategies recognize that there is a vocabulary impedance problem among ads and Web pages and attempt to solve the problem by expanding the Web pages and the ads with new terms.
Because of that they are called impedance coupling strategies.
Using a sample of a real case database with over 93 thousand ads, we evaluated our strategies. For the five matching strategies, our results indicated that planned consideration of additional evidence (such as the keywords provided by the advertisers) yielded gains in average precision figures (for our test collection) of 60%. This was obtained by a strategy called AAK (for ads and keywords), which is taken as the baseline for evaluating our more advanced impedance coupling strategies.
For our five impedance coupling strategies, the results indicate that additional gains in average precision of 50% (now relative to the AAK strategy) are possible. These were generated by expanding the Web page with new terms (obtained using a sample Web collection containing over five million pages) and the ads with the contents of the page they point to (a hyperlink provided by the advertisers).
These are first time results that indicate that high quality content-targeted advertising is feasible and practical.
This work was supported in part by the GERINDO project, grant MCT/CNPq/CT-INFO 552.087/02-5, by CNPq grant 300.188/95-1 (Berthier Ribeiro-Neto), and by CNPq grant 303.576/04-9 (Edleno Silva de Moura). Marco Cristo is supported by Fucapi, Manaus, AM, Brazil.
[1] The Google adwords. Google content-targeted advertising. http://adwords.google.com/select/ct_faq.html, November
[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley-Longman, 1st edition, 1999. [3] H. K. Bhargava and J. Feng. Paid placement strategies for internet search engines. In Proceedings of the eleventh international conference on World Wide Web, pages 117-123.
ACM Press, 2002. [4] E. P. Chan, S. Garcia, and S. Roukos. Trec-5 ad-hoc retrieval using k nearest-neighbors re-scoring. In The Fifth Text REtrieval Conference (TREC-5). National Institute of Standards and Technology (NIST), November 1996. [5] J. Feng, H. K. Bhargava, and D. Pennock. Comparison of allocation rules for paid placement advertising in search engines. In Proceedings of the 5th international conference on Electronic commerce, pages 294-299. ACM Press, 2003. [6] D. Hawking, N. Craswell, and P. B. Thistlewaite. Overview of TREC-7 very large collection track. In The Seventh Text REtrieval Conference (TREC-7), pages 91-104, Gaithersburg,
Maryland, USA, November 1998. [7] Y. Kohda and S. Endo. Ubiquitous advertising on the www: merging advertisement on the browser. Comput. Netw. ISDN Syst., 28(7-11):1493-1499, 1996. [8] M. Langheinrich, A. Nakamura, N. Abe, T. Kamba, and Y. Koseki. Unintrusive customization techniques for web advertising. Comput. Networks, 31(11-16):1259-1272, 1999. [9] T. P. Novak and D. L. Hoffman. New metrics for new media: toward the development of web measurement standards. World Wide Web J., 2(1):213-246, 1997. [10] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference. Morgan Kaufmann Publishers, 2nd edition, 1988. [11] B. Ribeiro-Neto and R. Muntz. A belief network model for IR.
In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 253-260, Zurich, Switzerland, August 1996. [12] A. Silva, E. Veloso, P. Golgher, B. Ribeiro-Neto, A. Laender, and N. Ziviani. CobWeb - a crawler for the brazilian web. In Proceedings of the String Processing and Information Retrieval Symposium (SPIRE"99), pages 184-191, Cancun,
Mexico, September 1999. [13] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Transactions on Information Systems, 9(3):187-222, July 1991. [14] C. Wang, P. Zhang, R. Choi, and M. Daeredita. Understanding consumers attitude toward advertising. In Eighth Americas Conference on Information Systems, pages 1143-1148, August
[15] M. Weideman. Ethical issues on content distribution to digital consumers via paid placement as opposed to website visibility in search engine results. In The Seventh ETHICOMP International Conference on the Social and Ethical Impacts of Information and Communication Technologies, pages 904-915. Troubador Publishing Ltd, April 2004. [16] M. Weideman and T. Haig-Smith. An investigation into search engines as a form of targeted advert delivery. In Proceedings of the 2002 annual research conference of the South African institute of computer scientists and information technologists on Enablement through technology, pages 258-258. South African Institute for Computer Scientists and Information Technologists, 2002. [17] Y. Yang. Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. In W. B.

Although many information retrieval systems (e.g., web search engines and digital library systems) have been successfully deployed, the current retrieval systems are far from optimal. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users [17]. This inherent non-optimality is seen clearly in the following two cases: (1) Different users may use exactly the same query (e.g., Java) to search for different information (e.g., the Java island in Indonesia or the Java programming language), but existing IR systems return the same results for these users. Without considering the actual user, it is impossible to know which sense Java refers to in a query. (2) A user"s information needs may change over time. The same user may use Java sometimes to mean the Java island in Indonesia and some other times to mean the programming language.
Without recognizing the search context, it would be again impossible to recognize the correct sense.
In order to optimize retrieval accuracy, we clearly need to model the user appropriately and personalize search according to each individual user. The major goal of user modeling for information retrieval is to accurately model a user"s information need, which is, unfortunately, a very difficult task. Indeed, it is even hard for a user to precisely describe what his/her information need is.
What information is available for a system to infer a user"s information need? Obviously, the user"s query provides the most direct evidence. Indeed, most existing retrieval systems rely solely on the query to model a user"s information need. However, since a query is often extremely short, the user model constructed based on a keyword query is inevitably impoverished . An effective way to improve user modeling in information retrieval is to ask the user to explicitly specify which documents are relevant (i.e., useful for satisfying his/her information need), and then to improve user modeling based on such examples of relevant documents. This is called relevance feedback, which has been proved to be quite effective for improving retrieval accuracy [19, 20]. Unfortunately, in real world applications, users are usually reluctant to make the extra effort to provide relevant examples for feedback [11].
It is thus very interesting to study how to infer a user"s information need based on any implicit feedback information, which naturally exists through user interactions and thus does not require any extra user effort. Indeed, several previous studies have shown that implicit user modeling can improve retrieval accuracy. In [3], a web browser (Curious Browser) is developed to record a user"s explicit relevance ratings of web pages (relevance feedback) and browsing behavior when viewing a page, such as dwelling time, mouse click, mouse movement and scrolling (implicit feedback).
It is shown that the dwelling time on a page, amount of scrolling on a page and the combination of time and scrolling have a strong correlation with explicit relevance ratings, which suggests that implicit feedback may be helpful for inferring user information need.
In [10], user clickthrough data is collected as training data to learn a retrieval function, which is used to produce a customized ranking of search results that suits a group of users" preferences. In [25], the clickthrough data collected over a long time period is exploited through query expansion to improve retrieval accuracy. 824 While a user may have general long term interests and preferences for information, often he/she is searching for documents to satisfy an ad-hoc information need, which only lasts for a short period of time; once the information need is satisfied, the user would generally no longer be interested in such information. For example, a user may be looking for information about used cars in order to buy one, but once the user has bought a car, he/she is generally no longer interested in such information. In such cases, implicit feedback information collected over a long period of time is unlikely to be very useful, but the immediate search context and feedback information, such as which of the search results for the current information need are viewed, can be expected to be much more useful. Consider the query Java again. Any of the following immediate feedback information about the user could potentially help determine the intended meaning of Java in the query: (1) The previous query submitted by the user is hashtable (as opposed to, e.g., travel Indonesia). (2) In the search results, the user viewed a page where words such as programming, software, and applet occur many times.
To the best of our knowledge, how to exploit such immediate and short-term search context to improve search has so far not been well addressed in the previous work. In this paper, we study how to construct and update a user model based on the immediate search context and implicit feedback information and use the model to improve the accuracy of ad-hoc retrieval. In order to maximally benefit the user of a retrieval system through implicit user modeling, we propose to perform eager implicit feedback. That is, as soon as we observe any new piece of evidence from the user, we would update the system"s belief about the user"s information need and respond with improved retrieval results based on the updated user model. We present a decision-theoretic framework for optimizing interactive information retrieval based on eager user model updating, in which the system responds to every action of the user by choosing a system action to optimize a utility function. In a traditional retrieval paradigm, the retrieval problem is to match a query with documents and rank documents according to their relevance values. As a result, the retrieval process is a simple independent cycle of query and result display. In the proposed new retrieval paradigm, the user"s search context plays an important role and the inferred implicit user model is exploited immediately to benefit the user. The new retrieval paradigm is thus fundamentally different from the traditional paradigm, and is inherently more general.
We further propose specific techniques to capture and exploit two types of implicit feedback information: (1) identifying related immediately preceding query and using the query and the corresponding search results to select appropriate terms to expand the current query, and (2) exploiting the viewed document summaries to immediately rerank any documents that have not yet been seen by the user. Using these techniques, we develop a client-side web search agent UCAIR (User-Centered Adaptive Information Retrieval) on top of a popular search engine (Google). Experiments on web search show that our search agent can improve search accuracy over Google. Since the implicit information we exploit already naturally exists through user interactions, the user does not need to make any extra effort. Thus the developed search agent can improve existing web search performance without additional effort from the user.
The remaining sections are organized as follows. In Section 2, we discuss the related work. In Section 3, we present a decisiontheoretic interactive retrieval framework for implicit user modeling.
In Section 4, we present the design and implementation of an intelligent client-side web search agent (UCAIR) that performs eager implicit feedback. In Section 5, we report our experiment results using the search agent. Section 6 concludes our work.
Implicit user modeling for personalized search has been studied in previous work, but our work differs from all previous work in several aspects: (1) We emphasize the exploitation of immediate search context such as the related immediately preceding query and the viewed documents in the same session, while most previous work relies on long-term collection of implicit feedback information [25]. (2) We perform eager feedback and bring the benefit of implicit user modeling as soon as any new implicit feedback information is available, while the previous work mostly exploits longterm implicit feedback [10]. (3) We propose a retrieval framework to integrate implicit user modeling with the interactive retrieval process, while the previous work either studies implicit user modeling separately from retrieval [3] or only studies specific retrieval models for exploiting implicit feedback to better match a query with documents [23, 27, 22]. (4) We develop and evaluate a personalized Web search agent with online user studies, while most existing work evaluates algorithms offline without real user interactions.
Currently some search engines provide rudimentary personalization, such as Google Personalized web search [6], which allows users to explicitly describe their interests by selecting from predefined topics, so that those results that match their interests are brought to the top, and My Yahoo! search [16], which gives users the option to save web sites they like and block those they dislike. In contrast, UCAIR personalizes web search through implicit user modeling without any additional user efforts. Furthermore, the personalization of UCAIR is provided on the client side. There are two remarkable advantages on this. First, the user does not need to worry about the privacy infringement, which is a big concern for personalized search [26]. Second, both the computation of personalization and the storage of the user profile are done at the client side so that the server load is reduced dramatically [9].
There have been many works studying user query logs [1] or query dynamics [13]. UCAIR makes direct use of a user"s query history to benefit the same user immediately in the same search session. UCAIR first judges whether two neighboring queries belong to the same information session and if so, it selects terms from the previous query to perform query expansion.
Our query expansion approach is similar to automatic query expansion [28, 15, 5], but instead of using pseudo feedback to expand the query, we use user"s implicit feedback information to expand the current query. These two techniques may be combined.
In interactive IR, a user interacts with the retrieval system through an action dialogue, in which the system responds to each user action with some system action. For example, the user"s action may be submitting a query and the system"s response may be returning a list of 10 document summaries. In general, the space of user actions and system responses and their granularities would depend on the interface of a particular retrieval system.
In principle, every action of the user can potentially provide new evidence to help the system better infer the user"s information need.
Thus in order to respond optimally, the system should use all the evidence collected so far about the user when choosing a response.
When viewed in this way, most existing search engines are clearly non-optimal. For example, if a user has viewed some documents on the first page of search results, when the user clicks on the Next link to fetch more results, an existing retrieval system would still return the next page of results retrieved based on the original query without considering the new evidence that a particular result has been viewed by the user. 825 We propose to optimize retrieval performance by adapting system responses based on every action that a user has taken, and cast the optimization problem as a decision task. Specifically, at any time, the system would attempt to do two tasks: (1) User model updating: Monitor any useful evidence from the user regarding his/her information need and update the user model as soon as such evidence is available; (2) Improving search results: Rerank immediately all the documents that the user has not yet seen, as soon as the user model is updated. We emphasize eager updating and reranking, which makes our work quite different from any existing work. Below we present a formal decision theoretic framework for optimizing retrieval performance through implicit user modeling in interactive information retrieval.
Let A be the set of all user actions and R(a) be the set of all possible system responses to a user action a ∈ A. At any time, let At = (a1, ..., at) be the observed sequence of user actions so far (up to time point t) and Rt−1 = (r1, ..., rt−1) be the responses that the system has made responding to the user actions. The system"s goal is to choose an optimal response rt ∈ R(at) for the current user action at.
Let M be the space of all possible user models. We further define a loss function L(a, r, m) ∈ , where a ∈ A is a user action, r ∈ R(a) is a system response, and m ∈ M is a user model.
L(a, r, m) encodes our decision preferences and assesses the optimality of responding with r when the current user model is m and the current user action is a. According to Bayesian decision theory, the optimal decision at time t is to choose a response that minimizes the Bayes risk, i.e., r∗ t = argmin r∈R(at) M L(at, r, mt)P(mt|U, D, At, Rt−1)dmt (1) where P(mt|U, D, At, Rt−1) is the posterior probability of the user model mt given all the observations about the user U we have made up to time t.
To simplify the computation of Equation 1, let us assume that the posterior probability mass P(mt|U, D, At, Rt−1) is mostly concentrated on the mode m∗ t = argmaxmt P(mt|U, D, At, Rt−1).
We can then approximate the integral with the value of the loss function at m∗ t . That is, r∗ t ≈ argminr∈R(at)L(at, r, m∗ t ) (2) where m∗ t = argmaxmt P(mt|U, D, At, Rt−1).
Leaving aside how to define and estimate these probabilistic models and the loss function, we can see that such a decision-theoretic formulation suggests that, in order to choose the optimal response to at, the system should perform two tasks: (1) compute the current user model and obtain m∗ t based on all the useful information. (2) choose a response rt to minimize the loss function value L(at, rt, m∗ t ). When at does not affect our belief about m∗ t , the first step can be omitted and we may reuse m∗ t−1 for m∗ t .
Note that our framework is quite general since we can potentially model any kind of user actions and system responses. In most cases, as we may expect, the system"s response is some ranking of documents, i.e., for most actions a, R(a) consists of all the possible rankings of the unseen documents, and the decision problem boils down to choosing the best ranking of unseen documents based on the most current user model. When a is the action of submitting a keyword query, such a response is exactly what a current retrieval system would do. However, we can easily imagine that a more intelligent web search engine would respond to a user"s clicking of the Next link (to fetch more unseen results) with a more optimized ranking of documents based on any viewed documents in the current page of results. In fact, according to our eager updating strategy, we may even allow a system to respond to a user"s clicking of browser"s Back button after viewing a document in the same way, so that the user can maximally benefit from implicit feedback.
These are precisely what our UCAIR system does.
A user model m ∈ M represents what we know about the user U, so in principle, it can contain any information about the user that we wish to model. We now discuss two important components in a user model.
The first component is a component model of the user"s information need. Presumably, the most important factor affecting the optimality of the system"s response is how well the response addresses the user"s information need. Indeed, at any time, we may assume that the system has some belief about what the user is interested in, which we model through a term vector x = (x1, ..., x|V |), where V = {w1, ..., w|V |} is the set of all terms (i.e., vocabulary) and xi is the weight of term wi. Such a term vector is commonly used in information retrieval to represent both queries and documents. For example, the vector-space model, assumes that both the query and the documents are represented as term vectors and the score of a document with respect to a query is computed based on the similarity between the query vector and the document vector [21]. In a language modeling approach, we may also regard the query unigram language model [12, 29] or the relevance model [14] as a term vector representation of the user"s information need.
Intuitively, x would assign high weights to terms that characterize the topics which the user is interested in.
The second component we may include in our user model is the documents that the user has already viewed. Obviously, even if a document is relevant, if the user has already seen the document, it would not be useful to present the same document again. We thus introduce another variable S ⊂ D (D is the whole set of documents in the collection) to denote the subset of documents in the search results that the user has already seen/viewed.
In general, at time t, we may represent a user model as mt = (S, x, At, Rt−1), where S is the seen documents, x is the system"s understanding of the user"s information need, and (At, Rt−1) represents the user"s interaction history. Note that an even more general user model may also include other factors such as the user"s reading level and occupation.
If we assume that the uncertainty of a user model mt is solely due to the uncertainty of x, the computation of our current estimate of user model m∗ t will mainly involve computing our best estimate of x. That is, the system would choose a response according to r∗ t = argminr∈R(at)L(at, r, S, x∗ , At, Rt−1) (3) where x∗ = argmaxx P(x|U, D, At, Rt−1). This is the decision mechanism implemented in the UCAIR system to be described later. In this system, we avoided specifying the probabilistic model P(x|U, D, At, Rt−1) by computing x∗ directly with some existing feedback method.
The exact definition of loss function L depends on the responses, thus it is inevitably application-specific. We now briefly discuss some possibilities when the response is to rank all the unseen documents and present the top k of them. Let r = (d1, ..., dk) be the top k documents, S be the set of seen documents by the user, and x∗ be the system"s best guess of the user"s information need. We 826 may simply define the loss associated with r as the negative sum of the probability that each of the di is relevant, i.e., L(a, r, m) = − k i=1 P(relevant|di, m). Clearly, in order to minimize this loss function, the optimal response r would contain the k documents with the highest probability of relevance, which is intuitively reasonable.
One deficiency of this top-k loss function is that it is not sensitive to the internal order of the selected top k documents, so switching the ranking order of a non-relevant document and a relevant one would not affect the loss, which is unreasonable. To model ranking, we can introduce a factor of the user model - the probability of each of the k documents being viewed by the user, P(view|di), and define the following ranking loss function: L(a, r, m) = − k i=1 P(view|di)P(relevant|di, m) Since in general, if di is ranked above dj (i.e., i < j), P(view|di) > P(view|dj), this loss function would favor a decision to rank relevant documents above non-relevant ones, as otherwise, we could always switch di with dj to reduce the loss value. Thus the system should simply perform a regular retrieval and rank documents according to the probability of relevance [18].
Depending on the user"s retrieval preferences, there can be many other possibilities. For example, if the user does not want to see redundant documents, the loss function should include some redundancy measure on r based on the already seen documents S.
Of course, when the response is not to choose a ranked list of documents, we would need a different loss function. We discuss one such example that is relevant to the search agent that we implement. When a user enters a query qt (current action), our search agent relies on some existing search engine to actually carry out search. In such a case, even though the search agent does not have control of the retrieval algorithm, it can still attempt to optimize the search results through refining the query sent to the search engine and/or reranking the results obtained from the search engine. The loss functions for reranking are already discussed above; we now take a look at the loss functions for query refinement.
Let f be the retrieval function of the search engine that our agent uses so that f(q) would give us the search results using query q.
Given that the current action of the user is entering a query qt (i.e., at = qt), our response would be f(q) for some q. Since we have no choice of f, our decision is to choose a good q. Formally, r∗ t = argminrt L(a, rt, m) = argminf(q)L(a, f(q), m) = f(argminqL(qt, f(q), m)) which shows that our goal is to find q∗ = argminqL(qt, f(q), m), i.e., an optimal query that would give us the best f(q). A different choice of loss function L(qt, f(q), m) would lead to a different query refinement strategy. In UCAIR, we heuristically compute q∗ by expanding qt with terms extracted from rt−1 whenever qt−1 and qt have high similarity. Note that rt−1 and qt−1 are contained in m as part of the user"s interaction history.
Implicit user modeling is captured in our framework through the computation of x∗ = argmaxx P(x|U, D, At, Rt−1), i.e., the system"s current belief of what the user"s information need is. Here again there may be many possibilities, leading to different algorithms for implicit user modeling. We now discuss a few of them.
First, when two consecutive queries are related, the previous query can be exploited to enrich the current query and provide more search context to help disambiguation. For this purpose, instead of performing query expansion as we did in the previous section, we could also compute an updated x∗ based on the previous query and retrieval results. The computed new user model can then be used to rank the documents with a standard information retrieval model.
Second, we can also infer a user"s interest based on the summaries of the viewed documents. When a user is presented with a list of summaries of top ranked documents, if the user chooses to skip the first n documents and to view the (n+1)-th document, we may infer that the user is not interested in the displayed summaries for the first n documents, but is attracted by the displayed summary of the (n + 1)-th document. We can thus use these summaries as negative and positive examples to learn a more accurate user model x∗ . Here many standard relevance feedback techniques can be exploited [19, 20]. Note that we should use the displayed summaries, as opposed to the actual contents of those documents, since it is possible that the displayed summary of the viewed document is relevant, but the document content is actually not. Similarly, a displayed summary may mislead a user to skip a relevant document.
Inferring user models based on such displayed information, rather than the actual content of a document is an important difference between UCAIR and some other similar systems.
In UCAIR, both of these strategies for inferring an implicit user model are implemented.
SEARCH AGENT
In this section, we present a client-side web search agent called UCAIR, in which we implement some of the methods discussed in the previous section for performing personalized search through implicit user modeling. UCAIR is a web browser plug-in 1 that acts as a proxy for web search engines. Currently, it is only implemented for Internet Explorer and Google, but it is a matter of engineering to make it run on other web browsers and interact with other search engines.
The issue of privacy is a primary obstacle for deploying any real world applications involving serious user modeling, such as personalized search. For this reason, UCAIR is strictly running as a client-side search agent, as opposed to a server-side application.
This way, the captured user information always resides on the computer that the user is using, thus the user does not need to release any information to the outside. Client-side personalization also allows the system to easily observe a lot of user information that may not be easily available to a server. Furthermore, performing personalized search on the client-side is more scalable than on the serverside, since the overhead of computation and storage is distributed among clients.
As shown in Figure 1, the UCAIR toolbar has 3 major components: (1) The (implicit) user modeling module captures a user"s search context and history information, including the submitted queries and any clicked search results and infers search session boundaries. (2) The query modification module selectively improves the query formulation according to the current user model. (3) The result re-ranking module immediately re-ranks any unseen search results whenever the user model is updated.
In UCAIR, we consider four basic user actions: (1) submitting a keyword query; (2) viewing a document; (3) clicking the Back button; (4) clicking the Next link on a result page. For each of these four actions, the system responds with, respectively, (1) 1 UCAIR is available at: http://sifaka.cs.uiuc.edu/ir/ucair/download.html 827 Search Engine (e.g.,
Google) Search History Log (e.g.,past queries, clicked results) Query Modification Result Re-Ranking User Modeling Result Buffer UCAIR Userquery results clickthrough… Figure 1: UCAIR architecture generating a ranked list of results by sending a possibly expanded query to a search engine; (2) updating the information need model x; (3) reranking the unseen results on the current result page based on the current model x; and (4) reranking the unseen pages and generating the next page of results based on the current model x.
Behind these responses, there are three basic tasks: (1) Decide whether the previous query is related to the current query and if so expand the current query with useful terms from the previous query or the results of the previous query. (2) Update the information need model x based on a newly clicked document summary. (3) Rerank a set of unseen documents based on the current model x.
Below we describe our algorithms for each of them.
expansion To effectively exploit previous queries and their corresponding clickthrough information, UCAIR needs to judge whether two adjacent queries belong to the same search session (i.e., detect session boundaries). Existing work on session boundary detection is mostly in the context of web log analysis (e.g., [8]), and uses statistical information rather than textual features. Since our clientside agent does not have access to server query logs, we make session boundary decisions based on textual similarity between two queries. Because related queries do not necessarily share the same words (e.g., java island and travel Indonesia), it is insufficient to use only query text. Therefore we use the search results of the two queries to help decide whether they are topically related. For example, for the above queries java island and travel Indonesia", the words java, bali, island, indonesia and travel may occur frequently in both queries" search results, yielding a high similarity score.
We only use the titles and summaries of the search results to calculate the similarity since they are available in the retrieved search result page and fetching the full text of every result page would significantly slow down the process. To compensate for the terseness of titles and summaries, we retrieve more results than a user would normally view for the purpose of detecting session boundaries (typically 50 results).
The similarity between the previous query q and the current query q is computed as follows. Let {s1, s2, . . . , sn } and {s1, s2, . . . , sn} be the result sets for the two queries. We use the pivoted normalization TF-IDF weighting formula [24] to compute a term weight vector si for each result si. We define the average result savg to be the centroid of all the result vectors, i.e., (s1 + s2 + . . . + sn)/n. The cosine similarity between the two average results is calculated as s avg · savg/ s 2 avg · s2 avg If the similarity value exceeds a predefined threshold, the two queries will be considered to be in the same information session.
If the previous query and the current query are found to belong to the same search session, UCAIR would attempt to expand the current query with terms from the previous query and its search results. Specifically, for each term in the previous query or the corresponding search results, if its frequency in the results of the current query is greater than a preset threshold (e.g. 5 results out of 50), the term would be added to the current query to form an expanded query. In this case, UCAIR would send this expanded query rather than the original one to the search engine and return the results corresponding to the expanded query. Currently, UCAIR only uses the immediate preceding query for query expansion; in principle, we could exploit all related past queries.
Suppose at time t, we have observed that the user has viewed k documents whose summaries are s1, ..., sk. We update our user model by computing a new information need vector with a standard feedback method in information retrieval (i.e., Rocchio [19]).
According to the vector space retrieval model, each clicked summary si can be represented by a term weight vector si with each term weighted by a TF-IDF weighting formula [21]. Rocchio computes the centroid vector of all the summaries and interpolates it with the original query vector to obtain an updated term vector. That is, x = αq + (1 − α) 1 k k i=1 si where q is the query vector, k is the number of summaries the user clicks immediately following the current query and α is a parameter that controls the influence of the clicked summaries on the inferred information need model. In our experiments, α is set to 0.5. Note that we update the information need model whenever the user views a document.
In general, we want to rerank all the unseen results as soon as the user model is updated. Currently, UCAIR implements reranking in two cases, corresponding to the user clicking the Back button and Next link in the Internet Explorer. In both cases, the current (updated) user model would be used to rerank the unseen results so that the user would see improved search results immediately.
To rerank any unseen document summaries, UCAIR uses the standard vector space retrieval model and scores each summary based on the similarity of the result and the current user information need vector x [21]. Since implicit feedback is not completely reliable, we bring up only a small number (e.g. 5) of highest reranked results to be followed by any originally high ranked results. 828 Google result (user query = java map) UCAIR result (user query =java map) previous query = travel Indonesia previous query = hashtable expanded user query = java map Indonesia expanded user query = java map class
www.btinternet.com/ se16/js/mapproj.htm www.lonelyplanet.com/mapshells/... java.sun.com/j2se/1.4.2/docs/...
www.btinternet.com/ se16/js/oldmapproj.htm www.indonesia-tourism.com/... java.sun.com/j2se/1.3/docs/api/java/...
java.sun.com/developer/... www.indonesia-tourism.com/ ... www.oracle.com/technology/...
java.sun.com/developer/onlineTraining/... www.indostreets.com/maps/java/ www.theserverside.com/news/...
science.nasa.gov/Realtime/... www.maps2anywhere.com/Maps/... www.koders.com/java/
www.oracle.com/technology/... www.maps2anywhere.com/Maps/... www.ibm.com/developerworks/java/...
www.lonelyplanet.com/mapshells/ www.embassyworld.com/maps/... tmap.pmel.noaa.gov/...
www.onjava.com/pub/a/onjava/api map/ users.powernet.co.uk/... jalbum.net/api/se/datadosen/util/Scope.html
www.gtasanandreas.net/sam/ users.powernet.co.uk/mkmarina/indonesia/ jalbum.net/api/se/datadosen/...
www.indonesia-tourism.com/... www.indonesiaphoto.com/... www.fawcette.com/javapro/...
Table 1: Sample results of query expansion
We now present some results on evaluating the two major UCAIR functions: selective query expansion and result reranking based on user clickthrough data.
The query expansion strategy implemented in UCAIR is intentionally conservative to avoid misinterpretation of implicit user models. In practice, whenever it chooses to expand the query, the expansion usually makes sense. In Table 1, we show how UCAIR can successfully distinguish two different search contexts for the query java map, corresponding to two different previous queries (i.e., travel Indonesia vs. hashtable). Due to implicit user modeling,
UCAIR intelligently figures out to add Indonesia and class, respectively, to the user"s query java map, which would otherwise be ambiguous as shown in the original results from Google on March 21, 2005. UCAIR"s results are much more accurate than Google"s results and reflect personalization in search.
The eager implicit feedback component is designed to immediately respond to a user"s activity such as viewing a document. In Figure 2, we show how UCAIR can successfully disambiguate an ambiguous query jaguar by exploiting a viewed document summary. In this case, the initial retrieval results using jaguar (shown on the left side) contain two results about the Jaguar cars followed by two results about the Jaguar software. However, after the user views the web page content of the second result (about Jaguar car) and returns to the search result page by clicking Back button, UCAIR automatically nominates two new search results about Jaguar cars (shown on the right side), while the original two results about Jaguar software are pushed down on the list (unseen from the picture).
To further evaluate UCAIR quantitatively, we conduct a user study on the effectiveness of the eager implicit feedback component. It is a challenge to quantitatively evaluate the potential performance improvement of our proposed model and UCAIR over Google in an unbiased way [7]. Here, we design a user study, in which participants would do normal web search and judge a randomly and anonymously mixed set of results from Google and UCAIR at the end of the search session; participants do not know whether a result comes from Google or UCAIR.
We recruited 6 graduate students for this user study, who have different backgrounds (3 computer science, 2 biology, and 1 chem<top> <num> Number: 716 <title> Spammer arrest sue <desc> Description: Have any spammers been arrested or sued for sending unsolicited e-mail? <narr> Narrative: Instances of arrests, prosecutions, convictions, and punishments of spammers, and lawsuits against them are relevant. Documents which describe laws to limit spam without giving details of lawsuits or criminal trials are not relevant. </top> Figure 3: An example of TREC query topic, expressed in a form which might be given to a human assistant or librarian istry). We use query topics from TREC 2
and TREC 2003 Web track [4] topic distillation task in the way to be described below.
An example topic from TREC 2004 Terabyte track appears in Figure 3. The title is a short phrase and may be used as a query to the retrieval system. The description field provides a slightly longer statement of the topic requirement, usually expressed as a single complete sentence or question. Finally the narrative supplies additional information necessary to fully specify the requirement, expressed in the form of a short paragraph.
Initially, each participant would browse 50 topics either from Terabyte track or Web track and pick 5 or 7 most interesting topics.
For each picked topic, the participant would essentially do the normal web search using UCAIR to find many relevant web pages by using the title of the query topic as the initial keyword query.
During this process, the participant may view the search results and possibly click on some interesting ones to view the web pages, just as in a normal web search. There is no requirement or restriction on how many queries the participant must submit or when the participant should stop the search for one topic. When the participant plans to change the search topic, he/she will simply press a button 2 Text REtrieval Conference: http://trec.nist.gov/ 829 Figure 2: Screen shots for result reranking to evaluate the search results before actually switching to the next topic.
At the time of evaluation, 30 top ranked results from Google and UCAIR (some are overlapping) are randomly mixed together so that the participant would not know whether a result comes from Google or UCAIR. The participant would then judge the relevance of these results. We measure precision at top n (n = 5, 10, 20, 30) documents of Google and UCAIR. We also evaluate precisions at different recall levels.
Altogether, 368 documents judged as relevant from Google search results and 429 documents judged as relevant from UCAIR by participants. Scatter plots of precision at top 10 and top 20 documents are shown in Figure 4 and Figure 5 respectively (The scatter plot of precision at top 30 documents is very similar to precision at top
precisions of Google and UCAIR on one query topic.
Table 2 shows the average precision at top n documents among
search results from UCAIR are consistently better than those from Google by all the measures. Moreover, the performance improvement is more dramatic for precision at top 20 documents than that at precision at top 10 documents. One explanation for this is that the more interaction the user has with the system, the more clickthrough data UCAIR can be expected to collect. Thus the retrieval system can build more precise implicit user models, which lead to better retrieval accuracy.
Ranking Method prec@5 prec@10 prec@20 prec@30 Google 0.538 0.472 0.377 0.308 UCAIR 0.581 0.556 0.453 0.375 Improvement 8.0% 17.8% 20.2% 21.8% Table 2: Table of average precision at top n documents for 32 query topics The plot in Figure 6 shows the precision-recall curves for UCAIR and Google, where it is clearly seen that the performance of UCAIR
0
1 UCAIR prec@10 Googleprec@10 Scatterplot of Precision at Top 10 Documents Figure 4: Precision at top 10 documents of UCAIR and Google is consistently and considerably better than that of Google at all levels of recall.
In this paper, we studied how to exploit implicit user modeling to intelligently personalize information retrieval and improve search accuracy. Unlike most previous work, we emphasize the use of immediate search context and implicit feedback information as well as eager updating of search results to maximally benefit a user. We presented a decision-theoretic framework for optimizing interactive information retrieval based on eager user model updating, in which the system responds to every action of the user by choosing a system action to optimize a utility function. We further propose specific techniques to capture and exploit two types of implicit feedback information: (1) identifying related immediately preceding query and using the query and the corresponding search results to select appropriate terms to expand the current query, and (2) exploiting the viewed document summaries to immediately rerank any documents that have not yet been seen by the user. Using these techniques, we develop a client-side web search agent (UCAIR) on top of a popular search engine (Google). Experiments on web search show that our search agent can improve search accuracy over 830
0
1 UCAIR prec@20 Googleprec@20 Scatterplot of Precision at Top 20 documents Figure 5: Precision at top 20 documents of UCAIR and Google
recall precision Precision−Recall curves Google Result UCAIR Result Figure 6: Precision at top 20 result of UCAIR and Google Google. Since the implicit information we exploit already naturally exists through user interactions, the user does not need to make any extra effort. The developed search agent thus can improve existing web search performance without any additional effort from the user.
We thank the six participants of our evaluation experiments. This work was supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472.
[1] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder. Hourly analysis of a very large topically categorized web query log. In Proceedings of SIGIR 2004, pages 321-328, 2004. [2] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2004 terabyte track. In Proceedings of TREC 2004,
[3] M. Claypool, P. Le, M. Waseda, and D. Brown. Implicit interest indicators. In Proceedings of Intelligent User Interfaces 2001, pages 33-40, 2001. [4] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
Overview of the TREC 2003 web track. In Proceedings of TREC 2003, 2003. [5] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.
Relevance feedback and personalization: A language modeling perspective. In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [6] Google Personalized. http://labs.google.com/personalized. [7] D. Hawking, N. Craswell, P. B. Thistlewaite, and D. Harman.
Results and challenges in web search evaluation. Computer Networks, 31(11-16):1321-1330, 1999. [8] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic web log session identification with statistical language models. Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [9] G. Jeh and J. Widom. Scaling personalized web search. In Proceedings of WWW 2003, pages 271-279, 2003. [10] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of SIGKDD 2002, pages 133-142,
[11] D. Kelly and J. Teevan. Implicit feedback for inferring user preference: A bibliography. SIGIR Forum, 37(2):18-28,
[12] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR"01, pages 111-119, 2001. [13] T. Lau and E. Horvitz. Patterns of search: Analyzing and modeling web query refinement. In Proceedings of the Seventh International Conference on User Modeling (UM), pages 145 -152, 1999. [14] V. Lavrenko and B. Croft. Relevance-based language models. In Proceedings of SIGIR"01, pages 120-127, 2001. [15] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In Proceedings of SIGIR 1998, pages 206-214, 1998. [16] My Yahoo! http://mysearch.yahoo.com. [17] G. Nunberg. As google goes, so goes the nation. New York Times, May 2003. [18] S. E. Robertson. The probability ranking principle in ı˚.
Journal of Documentation, 33(4):294-304, 1977. [19] J. J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323. Prentice-Hall Inc.,
[20] G. Salton and C. Buckley. Improving retrieval performance by retrieval feedback. Journal of the American Society for Information Science, 41(4):288-297, 1990. [21] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, 1983. [22] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of SIGIR 2005, pages 43-50, 2005. [23] X. Shen and C. Zhai. Exploiting query history for document ranking in interactive information retrieval (Poster). In Proceedings of SIGIR 2003, pages 377-378, 2003. [24] A. Singhal. Modern information retrieval: A brief overview.
Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 24(4):35-43, 2001. [25] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based on user profile constructed without any effort from users. In Proceedings of WWW 2004, pages 675-684,
[26] E. Volokh. Personalization and privacy. Communications of the ACM, 43(8):84-88, 2000. [27] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven. A simulated study of implicit feedback models.

Wireless data dissemination is an economical and efficient way to make desired data available to a large number of mobile or static users. The mode of data transfer is essentially asymmetric, that is, the capacity of the transfer of data (downstream communication) from the server to the client (mobile user) is significantly larger than the client or mobile user to the server (upstream communication). The effectiveness of a data dissemination system is judged by its ability to provide user the required data at anywhere and at anytime. One of the best ways to accomplish this is through the dissemination of highly personalized Location Based Services (LBS) which allows users to access personalized location dependent data. An example would be someone using their mobile device to search for a vegetarian restaurant. The LBS application would interact with other location technology components or use the mobile user's input to determine the user's location and download the information about the restaurants in proximity to the user by tuning into the wireless channel which is disseminating LDD.
We see a limited deployment of LBS by some service providers. But there are every indications that with time some of the complex technical problems such as uniform location framework, calculating and tracking locations in all types of places, positioning in various environments, innovative location applications, etc., will be resolved and LBS will become a common facility and will help to improve market productivity and customer comfort. In our project called DAYS, we use wireless data broadcast mechanism to push LDD to users and mobile users monitor and tune the channel to find and download the required data. A simple broadcast, however, is likely to cause significant performance degradation in the energy constrained mobile devices and a common solution to this problem is the use of efficient air indexing. The indexing approach stores control information which tells the user about the data location in the broadcast and how and when he could access it. A mobile user, thus, has some free time to go into the doze mode which conserves valuable power. It also allows the user to personalize his own mobile device by selectively tuning to the information of his choice.
Access efficiency and energy conservation are the two issues which are significant for data broadcast systems. Access efficiency refers to the latency experienced when a request is initiated till the response is received. Energy conservation [7, 10] refers to the efficient use of the limited energy of the mobile device in accessing broadcast data. Two parameters that affect these are the tuning time and the access latency. Tuning time refers to the time during which the mobile unit (MU) remains in active state to tune the channel and download its required data. It can also be defined as the number of buckets tuned by the mobile device in active state to get its required data. Access latency may be defined as the time elapsed since a request has been issued till the response has been received. 1 This research was supported by a grant from NSF IIS-0209170.
Several indexing schemes have been proposed in the past and the prominent among them are the tree based and the exponential indexing schemes [17]. The main disadvantages of the tree based schemes are that they are based on centralized tree structures. To start a search, the MU has to wait until it reaches the root of the next broadcast tree. This significantly affects the tuning time of the mobile unit. The exponential schemes facilitate index replication by sharing links in different search trees. For broadcasts with large number of pages, the exponential scheme has been shown to perform similarly as the tree based schemes in terms of access latency. Also, the average length of broadcast increases due to the index replication and this may cause significant increase in the access latency. None of the above indexing schemes is equally effective in broadcasting location dependent data. In addition to providing low latency, they lack properties which are used to address LDD issues. We propose an indexing scheme in DAYS which takes care of some these problems. We show with simulation results that our scheme outperforms some of the earlier indexing schemes for broadcasting LDD in terms of tuning time.
The rest of the paper is presented as follows. In section 2, we discuss previous work related to indexing of broadcast data.
Section 3 describes our DAYS architecture. Location dependent data, its generation and subsequent broadcast is presented in section 4. Section 5 discusses our indexing scheme in detail.
Simulation of our scheme and its performance evaluation is presented in section 6. Section 7 concludes the paper and mentions future related work.
Several disk-based indexing techniques have been used for air indexing. Imielinski et al. [5, 6] applied the B+ index tree, where the leaf nodes store the arrival times of the data items. The distributed indexing method was proposed to efficiently replicate and distribute the index tree in a broadcast. Specifically, the index tree is divided into a replicated part and a non replicated part.
Each broadcast consists of the replicated part and the nonreplicated part that indexes the data items immediately following it. As such, each node in the non-replicated part appears only once in a broadcast and, hence, reduces the replication cost and access latency while achieving a good tuning time. Chen et al. [2] and Shivakumar et al. [8] considered unbalanced tree structures to optimize energy consumption for non-uniform data access. These structures minimize the average index search cost by reducing the number of index searches for hot data at the expense of spending more on cold data. Tan and Yu discussed data and index organization under skewed broadcast Hashing and signature methods have also been suggested for wireless broadcast that supports equality queries [9]. A flexible indexing method was proposed in [5]. The flexible index first sorts the data items in ascending (or descending) order of the search key values and then divides them into p segments. The first bucket in each data segment contains a control index, which is a binary index mapping a given key value to the segment containing that key, and a local index, which is an m-entry index mapping a given key value to the buckets within the current segment. By tuning the parameters of p and m, mobile clients can achieve either a good tuning time or good access latency. Another indexing technique proposed is the exponential indexing scheme [17]. In this scheme, a parameterized index, called the exponential index is used to optimize the access latency or the tuning time. It facilitates index replication by linking different search trees. All of the above mentioned schemes have been applied to data which are non related to each other. These non related data may be clustered or non clustered. However, none of them has specifically addressed the requirements of LDD. Location dependent data are data which are associated with a location. Presently there are several applications that deal with LDD [13, 16]. Almost all of them depict LDD with the help of hierarchical structures [3, 4]. This is based on the containment property of location dependent data.
The Containment property helps determining relative position of an object by defining or identifying locations that contains those objects. The subordinate locations are hierarchically related to each other. Thus, Containment property limits the range of availability or operation of a service. We use this containment property in our indexing scheme to index LDD.
DAYS has been conceptualized to disseminate topical and nontopical data to users in a local broadcast space and to accept queries from individual users globally. Topical data, for example, weather information, traffic information, stock information, etc., constantly changes over time. Non topical data such as hotel, restaurant, real estate prices, etc., do not change so often. Thus, we envision the presence of two types of data distribution: In the first case, server pushes data to local users through wireless channels. The other case deals with the server sending results of user queries through downlink wireless channels. Technically, we see the presence of two types of queues in the pull based data access. One is a heavily loaded queue containing globally uploaded queries. The other is a comparatively lightly loaded queue consisting of locally uploaded queries. The DAYS architecture [12] as shown in figure 1 consists of a Data Server,
Broadcast Scheduler, DAYS Coordinator, Network of LEO satellites for global data delivery and a Local broadcast space.
Data is pushed into the local broadcast space so that users may tune into the wireless channels to access the data. The local broadcast space consists of a broadcast tower, mobile units and a network of data staging machines called the surrogates. Data staging in surrogates has been earlier investigated as a successful technique [12, 15] to cache users' related data. We believe that data staging can be used to drastically reduce the latency time for both the local broadcast data as well as global responses. Query request in the surrogates may subsequently be used to generate the popularity patterns which ultimately decide the broadcast schedule [12]. 18 Popularity Feedback from Surrogates for Broadcast Scheduler Local Broadcast Space Broadcast Tower SurrogateMU MU MU MU Data ServerBroadcast schedulerDAYS Coordinator Local downlink channel Global downlink channel Pull request queue Global request queue Local request queue Location based index Starbucks Plaza Kansas City Figure 1. DAYS Architecture Figure 2. Location Structure ofStarbucks, Plaza
We argue that incorporating location information in wireless data broadcast can significantly decrease the access latency. This property becomes highly useful for mobile unit which has limited storage and processing capability. There are a variety of applications to obtain information about traffic, restaurant and hotel booking, fast food, gas stations, post office, grocery stores, etc. If these applications are coupled with location information, then the search will be fast and highly cost effective. An important property of the locations is Containment which helps to determine the relative location of an object with respect to its parent that contains the object. Thus, Containment limits the range of availability of a data. We use this property in our indexing scheme. The database contains the broadcast contents which are converted into LDD [14] by associating them with respective locations so that it can be broadcasted in a clustered manner. The clustering of LDD helps the user to locate information efficiently and supports containment property. We present an example to justify our proposition.
Example: Suppose a user issues query Starbucks Coffee in Plaza please. to access information about the Plaza branch of Starbucks Coffee in Kansas City. In the case of location independent set up the system will list all Starbucks coffee shops in Kansas City area. It is obvious that such responses will increase access latency and are not desirable. These can be managed efficiently if the server has location dependent data, i.e., a mapping between a Starbucks coffee shop data and its physical location. Also, for a query including range of locations of Starbucks, a single query requesting locations for the entire region of Kansas City, as shown in Figure 2, will suffice. This will save enormous amount of bandwidth by decreasing the number of messages and at the same time will be helpful in preventing the scalability bottleneck in highly populated area.
The example justifies the need for a mapping function to process location dependent queries. This will be especially important for pull based queries across the globe for which the reply could be composed for different parts of the world. The mapping function is necessary to construct the broadcast schedule.
We define Global Property Set (GPS) [11], Information Content (IC) set, and Location Hierarchy (LH) set where IC ⊆ GPS and LH ⊆ GPS to develop a mapping function. LH = {l1, l2, l3…,lk} where li represent locations in the location tree and IC = {ic1, ic2, ic3,…,icn} where ici represent information type. For example, if we have traffic, weather, and stock information are in broadcast then IC = {ictraffic, icweather, and icstock}. The mapping scheme must be able to identify and select an IC member and a LH node for (a) correct association, (b) granularity match, (c) and termination condition. For example, weather ∈ IC could be associated with a country or a state or a city or a town of LH. The granularity match between the weather and a LH node is as per user requirement.
Thus, with a coarse granularity weather information is associated with a country to get country"s weather and with town in a finer granularity. If a town is the finest granularity, then it defines the terminal condition for association between IC and LH for weather.
This means that a user cannot get weather information about subdivision of a town. In reality weather of a subdivision does not make any sense.
We develop a simple heuristic mapping approach scheme based on user requirement. Let IC = {m1, m2,m3 .,..., mk}, where mi represent its element and let LH = {n1, n2, n3, ..., nl}, where ni represents LH"s member. We define GPS for IC (GPSIC) ⊆ GPS and for LH (GPSLH) ⊆ GPS as GPSIC = {P1, P2,…, Pn}, where P1, P2, P3,…, Pn are properties of its members and GPSLH = {Q1,
Q2,…, Qm} where Q1, Q2,…, Qm are properties of its members.
The properties of a particular member of IC are a subset of GPSIC. It is generally true that (property set (mi∈ IC) ∪ property set (mj∈ IC)) ≠ ∅, however, there may be cases where the intersection is not null. For example, stock ∈ IC and movie ∈ IC rating do not have any property in common. We assume that any two or more members of IC have at least one common geographical property (i.e. location) because DAYS broadcasts information about those categories, which are closely tied with a location. For example, stock of a company is related to a country, weather is related to a city or state, etc.
We define the property subset of mi∈ IC as PSm i ∀ mi ∈ IC and PSm i = {P1, P2, ..., Pr} where r ≤ n. ∀ Pr {Pr ∈ PSm i → Pr∈ GPSIC} which implies that ∀ i, PSm i ⊆ GPSIC. The geographical properties of this set are indicative of whether mi ∈ IC can be mapped to only a single granularity level (i.e. a single location) in LH or a multiple granularity levels (i.e. more than one nodes in 19 the hierarchy) in LH. How many and which granularity levels should a mi map to, depends upon the level at which the service provider wants to provide information about the mi in question.
Similarly we define a property subset of LH members as PSn j ∀ nj ∈ LH which can be written as PSn j ={Q1, Q2, Q3, …, Qs} where s ≤ m. In addition, ∀ Qs {Qs∈ PSn j → Qs∈ GPSLH} which implies that ∀j, PSn j ⊆ GPSLH.
The process of mapping from IC to LH is then identifying for some mx∈ IC one or more ny∈ LH such that PSmx ∩ PSnv ≠ φ.
This means that when mx maps to ny and all children of ny if mx can map to multiple granularity levels or mx maps only to ny if mx can map to a single granularity level.
We assume that new members can join and old member can leave IC or LH any time. The deletion of members from the IC space is simple but addition of members to the IC space is more restrictive.
If we want to add a new member to the IC space, then we first define a property set for the new member: PSmnew_m ={P1, P2, P3, …, Pt} and add it to the IC only if the condition:∀ Pw {Pw∈ PSpnew_m → Pw∈ GPSIC} is satisfied. This scheme has an additional benefit of allowing the information service providers to have a control over what kind of information they wish to provide to the users. We present the following example to illustrate the mapping concept.
IC = {Traffic, Stock, Restaurant, Weather, Important history dates, Road conditions} LH = {Country, State, City, Zip-code, Major-roads} GPSIC = {Surface-mobility, Roads, High, Low, Italian-food,
StateName, Temp, CityName, Seat-availability, Zip, Traffic-jams,
Stock-price, CountryName, MajorRoadName, Wars, Discoveries,
World} GPSLH = {Country, CountrySize, StateName, CityName, Zip,
MajorRoadName} Ps(ICStock) = {Stock-price, CountryName, High, Low} Ps(ICTraffic) = {Surface-mobility, Roads, High, Low, Traffic-jams,
CityName} Ps(ICImportant dates in history) = {World, Wars, Discoveries} Ps(ICRoad conditions) = {Precipitation, StateName, CityName} Ps(ICRestaurant) = {Italian-food, Zip code} Ps(ICWeather) = {StateName, CityName, Precipitation,
Temperature} PS(LHCountry) = {CountryName, CountrySize} PS(LHState = {StateName, State size},
PS(LHCity) ={CityName, City size} PS(LHZipcode) = {ZipCodeNum } PS(LHMajor roads) = {MajorRoadName} Now, only PS(ICStock) ∩ PSCountry ≠φ. In addition, PS(ICStock) indicated that Stock can map to only a single location Country.
When we consider the member Traffic of IC space, only PS(ICTraffic) ∩ PScity ≠ φ. As PS(ICTraffic) indicates that Traffic can map to only a single location, it maps only to City and none of its children. Now unlike Stock, mapping of Traffic with Major roads, which is a child of City, is meaningful. However service providers have right to control the granularity levels at which they want to provide information about a member of IC space.
PS(ICRoad conditions) ∩ PSState ≠φ and PS(ICRoad conditions) ∩ PSCity≠φ.
So Road conditions maps to State as well as City. As PS(ICRoad conditions) indicates that Road conditions can map to multiple granularity levels, Road conditions will also map to Zip Code and Major roads, which are the children of State and City. Similarly,
Restaurant maps only to Zip code, and Weather maps to State,
City and their children, Major Roads and Zip Code.
This section discusses our location based indexing scheme (LBIS). The scheme is designed to conform to the LDD broadcast in our project DAYS. As discussed earlier, we use the containment property of LDD in the indexing scheme. This significantly limits the search of our required data to a particular portion of broadcast. Thus, we argue that the scheme provides bounded tuning time.
We describe the architecture of our indexing scheme. Our scheme contains separate data buckets and index buckets. The index buckets are of two types. The first type is called the Major index.
The Major index provides information about the types of data broadcasted. For example, if we intend to broadcast information like Entertainment, Weather, Traffic etc., then the major index points to either these major types of information and/or their main subtypes of information, the number of main subtypes varying from one information to another. This strictly limits number of accesses to a Major index. The Major index never points to the original data. It points to the sub indexes called the Minor index.
The minor indexes are the indexes which actually points to the original data. We called these minor index pointers as Location Pointers as they points to the data which are associated with a location. Thus, our search for a data includes accessing of a major index and some minor indexes, the number of minor index varying depending on the type of information.
Thus, our indexing scheme takes into account the hierarchical nature of the LDD, the Containment property, and requires our broadcast schedule to be clustered based on data type and location. The structure of the location hierarchy requires the use of different types of index at different levels. The structure and positions of index strictly depend on the location hierarchy as described in our mapping scheme earlier. We illustrate the implementation of our scheme with an example. The rules for framing the index are mentioned subsequently. 20 A1 Entertainment Resturant Movie A2 A3 A4 R1 R2 R3 R4 R5 R6 R7 R8 Weather KC SL JC SF Entertainment R1 R2 R3 R4 R5 R6 R7 R8 KC SL JC SF (A, R, NEXT = 8) 3, R5 4, R7 Type (S, L) ER W E EM (1, 4) (5, 4) (1, 4), (9, 4) (9, 4) Type (S, L) W E EM ER (1, 4) (5, 8) (5, 4) (9, 4) Type (S, L) E EM ER W (1, 8) (1, 4) (5, 4) (9, 4) A1 A2 A3 A4 Movie Resturant Weather
Major index Major index Major index Minor index Major index Minor index Figure 3. Location Mapped Information for Broadcast Figure 4. Data coupled with Location based Index Example: Let us suppose that our broadcast content contains ICentertainment and ICweather which is represented as shown in Fig. 3.
Ai represents Areas of City and Ri represents roads in a certain area. The leaves of Weather structure represent four cities. The index structure is given in Fig. 4 which shows the position of major and minor index and data in the broadcast schedule.
We propose the following rules for the creation of the air indexed broadcast schedule: • The major index and the minor index are created. • The major index contains the position and range of different types of data items (Weather and Entertainment, Figure 3) and their categories. The sub categories of Entertainment,
Movie and Restaurant, are also in the index. Thus, the major index contains Entertainment (E), Entertainment-Movie (EM), Entertainment-Restaurant (ER), and Weather (W). The tuple (S, L) represents the starting position (S) of the data item and L represents the range of the item in terms of number of data buckets. • The minor index contains the variables A, R and a pointer Next. In our example (Figure 3), road R represents the first node of area A. The minor index is used to point to actual data buckets present at the lowest levels of the hierarchy. In contrast, the major index points to a broader range of locations and so it contains information about main and sub categories of data. • Index information is not incorporated in the data buckets.
Index buckets are separate containing only the control information. • The number of major index buckets m=#(IC), IC = {ic1, ic2, ic3,…,icn} where ici represent information type and # represents the cardinality of the Information Content set IC.
In this example, IC= {icMovie, icWeather, icRestaurant} and so #(IC) =3. Hence, the number of major index buckets is 3. • Mechanism to resolve the query is present in the java based coordinator in MU. For example, if a query Q is presented as Q (Entertainment, Movie, Road_1), then the resultant search will be for the EM information in the major index. We say,
Q EM.
Our proposed index works as follows: Let us suppose that an MU issues a query which is represented by Java Coordinator present in the MU as Restaurant information on Road 7. This is resolved by the coordinator as Q ER. This means one has to search for ER unit of index in the major index. Let us suppose that the MU logs into the channel at R2. The first index it receives is a minor index after R2. In this index, value of Next variable = 4, which means that the next major index is present after bucket 4. The MU may go into doze mode. It becomes active after bucket 4 and receives the major index. It searches for ER information which is the first entry in this index. It is now certain that the MU will get the position of the data bucket in the adjoining minor index. The second unit in the minor index depicts the position of the required data R7. It tells that the data bucket is the first bucket in Area 4.
The MU goes into doze mode again and becomes active after bucket 6. It gets the required data in the next bucket. We present the algorithm for searching the location based Index.
Algorithm 1 Location based Index Search in DAYS
bucket and observe the Minor Index
equal to (A,R)found then
21
Conservation of energy is the main concern when we try to access data from wireless broadcast. An efficient scheme should allow the mobile device to access its required data by staying active for a minimum amount of time. This would save considerable amount of energy. Since items are distributed based on types and are mapped to suitable locations, we argue that our broadcast deals with clustered data types. The mobile unit has to access a larger major index and a relatively much smaller minor index to get information about the time of arrival of data. This is in contrast to the exponential scheme where the indexes are of equal sizes. The example discussed and Algorithm 1 reveals that to access any data, we need to access the major index only once followed by one or more accesses to the minor index. The number of minor index access depends on the number of internal locations. As the number of internal locations vary for item to item (for example,
Weather is generally associated with a City whereas traffic is granulated up to major and minor roads of a city), we argue that the structure of the location mapped information may be visualized as a forest which is a collection of general trees, the number of general trees depending on the types of information broadcasted and depth of a tree depending on the granularity of the location information associated with the information.
For our experiments, we assume the forest as a collection of balanced M-ary trees. We further assume the M-ary trees to be full by assuming the presence of dummy nodes in different levels of a tree.
Thus, if the number of data items is d and the height of the tree is m, then n= (m*d-1)/(m-1) where n is the number of vertices in the tree and i= (d-1)/(m-1) where i is the number of internal vertices.
Tuning time for a data item involves 1 unit of time required to access the major index plus time required to access the data items present in the leaves of the tree.
Thus, tuning time with d data items is t = logmd+1 We can say that tuning time is bounded by O(logmd).
We compare our scheme with the distributed indexing and exponential scheme. We assume a flat broadcast and number of pages varying from 5000 to 25000. The various simulation parameters are shown in Table 1.
Figure 5-8 shows the relative tuning times of three indexing algorithms, ie, the LBIS, exponential scheme and the distributed tree scheme. Figure 5 shows the result for number of internal location nodes = 3. We can see that LBIS significantly outperforms both the other schemes. The tuning time in LBIS ranges from approx 6.8 to 8. This large tuning time is due to the fact that after reaching the lowest minor index, the MU may have to access few buckets sequentially to get the required data bucket.
We can see that the tuning time tends to become stable as the length of broadcast increases. In figure 6 we consider m= 4. Here we can see that the exponential and the distributed perform almost similarly, though the former seems to perform slightly better as the broadcast length increases. A very interesting pattern is visible in figure 7. For smaller broadcast size, the LBIS seems to have larger tuning time than the other two schemes. But as the length of broadcast increases, it is clearly visible the LBIS outperforms the other two schemes. The Distributed tree indexing shows similar behavior like the LBIS. The tuning time in LBIS remains low because the algorithm allows the MU to skip some intermediate Minor Indexes. This allows the MU to move into lower levels directly after coming into active mode, thus saving valuable energy. This action is not possible in the distributed tree indexing and hence we can observe that its tuning time is more than the LBIS scheme, although it performs better than the exponential scheme. Figure 8, in contrast, shows us that the tuning time in LBIS, though less than the other two schemes, tends to increase sharply as the broadcast length becomes greater than the 15000 pages. This may be attributed both due to increase in time required to scan the intermediate Minor Indexes and the length of the broadcast. But we can observe that the slope of the LBIS curve is significantly less than the other two curves.
Table 1 Simulation Parameters P Definition Values N Number of data Items 5000 - 25000 m Number of internal location nodes 3, 4, 5, 6 B Capacity of bucket without index (for exponential index) 10,64,128,256 i Index base for exponential index 2,4,6,8 k Index size for distributed tree 8 bytes The simulation results establish some facts about our location based indexing scheme. The scheme performs better than the other two schemes in terms of tuning time in most of the cases. As the length of the broadcast increases, after a certain point, though the tuning time increases as a result of factors which we have described before, the scheme always performs better than the other two schemes. Due to the prescribed limit of the number of pages in the paper, we are unable to show more results. But these omitted results show similar trend as the results depicted in figure 5-8.
In this paper we have presented a scheme for mapping of wireless broadcast data with their locations. We have presented an example to show how the hierarchical structure of the location tree maps with the data to create LDD. We have presented a scheme called LBIS to index this LDD. We have used the containment property of LDD in the scheme that limits the search to a narrow range of data in the broadcast, thus saving valuable energy in the device.
The mapping of data with locations and the indexing scheme will be used in our DAYS project to create the push based architecture. The LBIS has been compared with two other prominent indexing schemes, i.e., the distributed tree indexing scheme and the exponential indexing scheme. We showed in our simulations that the LBIS scheme has the lowest tuning time for broadcasts having large number of pages, thus saving valuable battery power in the MU. 22 In the future work we try to incorporate pull based architecture in our DAYS project. Data from the server is available for access by the global users. This may be done by putting a request to the source server. The query in this case is a global query. It is transferred from the user"s source server to the destination server through the use of LEO satellites. We intend to use our LDD scheme and data staging architecture in the pull based architecture.
We will show that the LDD scheme together with the data staging architecture significantly improves the latency for global as well as local query.
[1] Acharya, S., Alonso, R. Franklin, M and Zdonik S. Broadcast disk: Data management for asymmetric communications environments. In Proceedings of ACM SIGMOD Conference on Management of Data, pages 199-210, San Jose, CA, May
[2] Chen, M.S.,Wu, K.L. and Yu, P. S. Optimizing index allocation for sequential data broadcasting in wireless mobile computing. IEEE Transactions on Knowledge and Data Engineering (TKDE), 15(1):161-173, January/February 2003.
Figure 5. Broadcast Size (# buckets) Dist tree Expo LBIS Figure 6. Broadcast Size (# buckets) Dist tree Expo LBIS Figure 7. Broadcast Size (# buckets) Dist tree Expo LBIS Figure 8. Broadcast Size (# buckets) Dist tree Expo LBIS Averagetuningtime Averagetuningtime Averagetuningtime Averagetuningtime 23 [3] Hu, Q. L., Lee, D. L. and Lee, W.C. Performance evaluation of a wireless hierarchical data dissemination system. In Proceedings of the 5th Annual ACM International Conference on Mobile Computing and Networking (MobiCom"99), pages 163-173, Seattle, WA, August 1999. [4] Hu, Q. L. Lee, W.C. and Lee, D. L. Power conservative multi-attribute queries on data broadcast. In Proceedings of the 16th International Conference on Data Engineering (ICDE"00), pages 157-166, San Diego, CA, February 2000. [5] Imielinski, T., Viswanathan, S. and Badrinath. B. R. Power efficient filtering of data on air. In Proceedings of the 4th International Conference on Extending Database Technology (EDBT"94), pages 245-258, Cambridge, UK, March 1994. [6] Imielinski, T., Viswanathan, S. and Badrinath. B. R. Data on air - Organization and access. IEEE Transactions on Knowledge and Data Engineering (TKDE), 9(3):353-372,
May/June 1997. [7] Shih, E., Bahl, P. and Sinclair, M. J. Wake on wireless: An event driven energy saving strategy for battery operated devices. In Proceedings of the 8th Annual ACM International Conference on Mobile Computing and Networking (MobiCom"02), pages 160-171, Atlanta, GA, September
[8] Shivakumar N. and Venkatasubramanian, S. Energy-efficient indexing for information dissemination in wireless systems.
ACM/Baltzer Journal of Mobile Networks and Applications (MONET), 1(4):433-446, December 1996. [9] Tan K. L. and Yu, J. X. Energy efficient filtering of non uniform broadcast. In Proceedings of the 16th International Conference on Distributed Computing Systems (ICDCS"96), pages 520-527, Hong Kong, May 1996. [10] Viredaz, M. A., Brakmo, L. S. and Hamburgen, W. R. Energy management on handheld devices. ACM Queue, 1(7):44-52,
October 2003. [11] Garg, N. Kumar, V., & Dunham, M.H. Information Mapping and Indexing in DAYS, 6th International Workshop on Mobility in Databases and Distributed Systems, in conjunction with the 14th International Conference on Database and Expert Systems Applications September 1-5,
Prague, Czech Republic, 2003. [12] Acharya D., Kumar, V., & Dunham, M.H. InfoSpace: Hybrid and Adaptive Public Data Dissemination System for Ubiquitous Computing. Accepted for publication in the special issue of Pervasive Computing. Wiley Journal for Wireless Communications and Mobile Computing, 2004. [13] Acharya D., Kumar, V., & Prabhu, N. Discovering and using Web Services in M-Commerce, Proceedings for 5th VLDB Workshop on Technologies for E-Services, Toronto,
Canada,2004. [14] Acharya D., Kumar, V. Indexing Location Dependent Data in broadcast environment. Accepted for publication, JDIM special issue on Distributed Data Management, 2005. [15] Flinn, J., Sinnamohideen, S., & Satyanarayan, M. Data Staging on Untrusted Surrogates, Intel Research, Pittsburg,
Unpublished Report, 2003. [16] Seydim, A.Y., Dunham, M.H. & Kumar, V. Location dependent query processing, Proceedings of the 2nd ACM international workshop on Data engineering for wireless and mobile access, p.47-53, Santa Barbara, California, USA,
[17] Xu, J., Lee, W.C., Tang., X. Exponential Index: A Parameterized Distributed Indexing Scheme for Data on Air.

The GovStat Project is a joint effort of the University of North Carolina Interaction Design Lab and the University of Maryland Human-Computer Interaction Lab1 .
Citing end-user difficulty in finding governmental information (especially statistical data) online, the project seeks to create an integrated model of user access to US government statistical information that is rooted in realistic data models and innovative user interfaces. To enable such models and interfaces, we propose a data-driven approach, based on data mining and machine learning techniques. In particular, our work analyzes a particular digital library-the website of the Bureau of Labor Statistics2 (BLS)-in efforts to discover a small number of linguistically meaningful concepts, or bins, that collectively summarize the semantic domain of the site.
The project goal is to classify the site"s web content according to these inferred concepts as an initial step towards data filtering via active user interfaces (cf. [13]). Many digital libraries already make use of content classification, both explicitly and implicitly; they divide their resources manually by topical relation; they organize content into hierarchically oriented file systems. The goal of the present 1 http://www.ils.unc.edu/govstat 2 http://www.bls.gov 151 research is to develop another means of browsing the content of these collections. By analyzing the distribution of terms across documents, our goal is to supplement the agency"s pre-existing information structures. Statistical learning technologies are appealing in this context insofar as they stand to define a data-driven-as opposed to an agency-drivennavigational structure for a site.
Our approach combines supervised and unsupervised learning techniques. A pure document clustering [12] approach to such a large, diverse collection as BLS led to poor results in early tests [6]. But strictly supervised techniques [5] are inappropriate, too. Although BLS designers have defined high-level subject headings for their collections, as we discuss in Section 2, this scheme is less than optimal. Thus we hope to learn an additional set of concepts by letting the data speak for themselves.
The remainder of this paper describes the details of our concept discovery efforts and subsequent evaluation. In Section 2 we describe the previously existing, human-created conceptual structure of the BLS website. This section also describes evidence that this structure leaves room for improvement. Next (Sections 3-5), we turn to a description of the concepts derived via content clustering under three document representations: keyword, title only, and full-text.
Section 6 describes a two-part evaluation of the derived conceptual structures. Finally, we conclude in Section 7 by outlining upcoming work on the project.
WEBSITE The Bureau of Labor Statistics is a federal government agency charged with compiling and publishing statistics pertaining to labor and production in the US and abroad. Given this broad mandate, the BLS publishes a wide array of information, intended for diverse audiences. The agency"s website acts as a clearinghouse for this process. With over 15,000 text/html documents (and many more documents if spreadsheets and typeset reports are included), providing access to the collection provides a steep challenge to information architects.
The starting point of this work is the notion that access to information in the BLS website could be improved by the addition of a dynamic interface such as the relation browser described by Marchionini and Brunk [13]. The relation browser allows users to traverse complex data sets by iteratively slicing the data along several topics. In Figure
applied to the FedStats website3 .
The relation browser supports information seeking by allowing users to form queries in a stepwise fashion, slicing and re-slicing the data as their interests dictate. Its motivation is in keeping with Shneiderman"s suggestion that queries and their results should be tightly coupled [2]. Thus in Fig3 http://www.fedstats.gov Figure 1: Relation Browser Prototype ure 1, users might limit their search set to those documents about energy. Within this subset of the collection, they might further eliminate documents published more than a year ago. Finally, they might request to see only documents published in PDF format.
As Marchionini and Brunk discuss, capturing the publication date and format of documents is trivial. But successful implementations of the relation browser also rely on topical classification. This presents two stumbling blocks for system designers: • Information architects must define the appropriate set of topics for their collection • Site maintainers must classify each document into its appropriate categories These tasks parallel common problems in the metadata community: defining appropriate elements and marking up documents to support metadata-aware information access.
Given a collection of over 15,000 documents, these hurdles are especially daunting, and automatic methods of approaching them are highly desirable.
Prior to our involvement with the project, designers at BLS created a shallow classificatory structure for the most important documents in their website. As seen in Figure 2, the BLS home page organizes 65 top-level documents into
Unemployment, Productivity, and Inflation and Spending. 152 Figure 2: The BLS Home Page We hoped initially that these pre-defined categories could be used to train a 15-way document classifier, thus automating the process of populating the relation browser altogether.
However, this approach proved unsatisfactory. In personal meetings, BLS officials voiced dissatisfaction with the existing topics. Their form, it was argued, owed as much to the institutional structure of BLS as it did to the inherent topology of the website"s information space. In other words, the topics reflected official divisions rather than semantic clusters. The BLS agents suggested that re-designing this classification structure would be desirable.
The agents" misgivings were borne out in subsequent analysis. The BLS topics comprise a shallow classificatory structure; each of the 15 top-level categories is linked to a small number of related pages. Thus there are 7 pages associated with Inflation. Altogether, the link structure of this classificatory system contains 65 documents; that is, excluding navigational links, there are 65 documents linked from the BLS home page, where each hyperlink connects a document to a topic (pages can be linked to multiple topics). Based on this hyperlink structure, we defined M, a symmetric 65×65 matrix, where mij counts the number of topics in which documents i and j are both classified on the BLS home page. To analyze the redundancy inherent in the pre-existing structure, we derived the principal components of M (cf. [11]).
Figure 3 shows the resultant scree plot4 .
Because all 65 documents belong to at least one BLS topic, 4 A scree plot shows the magnitude of the kth eigenvalue versus its rank. During principal component analysis scree plots visualize the amount of variance captured by each component. m00M0M 0 1010M10M 10 2020M20M 20 3030M30M 30 4040M40M 40 5050M50M 50 6060M60M 60 m00M0M 0 22M2M 2 44M4M 4 66M6M 6 88M8M 8 1010M10M 10 1212M12M 12 1414M14M 14 Eigenvalue RankMEigenvalue RankM Eigenvalue Rank Eigenvlue MagnitudeMEigenvlue MagnitudeM EigenvlueMagnitude Figure 3: Scree Plot of BLS Categories the rank of M is guaranteed to be less than or equal to
about Figure 3, however, is the precipitous decline in magnitude among the first four eigenvalues. The four largest eigenvlaues account for 62.2% of the total variance in the data. This fact suggests a high degree of redundancy among the topics. Topical redundancy is not in itself problematic.
However, the documents in this very shallow classificatory structure are almost all gateways to more specific information. Thus the listing of the Producer Price Index under three categories could be confusing to the site"s users. In light of this potential for confusion and the agency"s own request for redesign, we undertook the task of topic discovery described in the following sections.
DISCOVERY To aid in the discovery of a new set of high-level topics for the BLS website, we turned to unsupervised machine learning methods. In efforts to let the data speak for themselves, we desired a means of concept discovery that would be based not on the structure of the agency, but on the content of the material. To begin this process, we crawled the BLS website, downloading all documents of MIME type text/html.
This led to a corpus of 15,165 documents. Based on this corpus, we hoped to derive k ≈ 10 topical categories, such that each document di is assigned to one or more classes. 153 Document clustering (cf. [16]) provided an obvious, but only partial solution to the problem of automating this type of high-level information architecture discovery. The problems with standard clustering are threefold.
identifying the topical content of documents, since documents may be about many subjects.
BLS collection (tables, lists, surveys, etc.), many documents" terms provide noisy topical information.
small number (k ≈ 10) of topics. Without significant data reduction, term-based clustering tends to deliver clusters at too fine a level of granularity.
In light of these problems, we take a hybrid approach to topic discovery. First, we limit the clustering process to a sample of the entire collection, described in Section 4.
Working on a focused subset of the data helps to overcome problems two and three, listed above. To address the problem of mutual exclusivity, we combine unsupervised with supervised learning methods, as described in Section 5.
DOCUMENTS To derive empirically evidenced topics we initially turned to cluster analysis. Let A be the n×p data matrix with n observations in p variables. Thus aij shows the measurement for the ith observation on the jth variable. As described in [12], the goal of cluster analysis is to assign each of the n observations to one of a small number k groups, each of which is characterized by high intra-cluster correlation and low inter-cluster correlation. Though the algorithms for accomplishing such an arrangement are legion, our analysis focuses on k-means clustering5 , during which, each observation oi is assigned to the cluster Ck whose centroid is closest to it, in terms of Euclidean distance. Readers interested in the details of the algorithm are referred to [12] for a thorough treatment of the subject.
Clustering by k-means is well-studied in the statistical literature, and has shown good results for text analysis (cf. [8, 16]). However, k-means clustering requires that the researcher specify k, the number of clusters to define. When applying k-means to our 15,000 document collection, indicators such as the gap statistic [17] and an analysis of the mean-squared distance across values of k suggested that k ≈ 80 was optimal. This paramterization led to semantically intelligible clusters. However, 80 clusters are far too many for application to an interface such as the relation 5 We have focused on k-means as opposed to other clustering algorithms for several reasons. Chief among these is the computational efficiency enjoyed by the k-means approach.
Because we need only a flat clustering there is little to be gained by the more expensive hierarchical algorithms. In future work we will turn to model-based clustering [7] as a more principled method of selecting the number of clusters and of representing clusters. browser. Moreover, the granularity of these clusters was unsuitably fine. For instance, the 80-cluster solution derived a cluster whose most highly associated words (in terms of log-odds ratio [1]) were drug, pharmacy, and chemist. These words are certainly related, but they are related at a level of specificity far below what we sought.
To remedy the high dimensionality of the data, we resolved to limit the algorithm to a subset of the collection.
In consultation with employees of the BLS, we continued our analysis on documents that form a series titled From the Editor"s Desk6 . These are brief articles, written by BLS employees. BLS agents suggested that we focus on the Editor"s Desk because it is intended to span the intellectual domain of the agency. The column is published daily, and each entry describes an important current issue in the BLS domain. The Editor"s Desk column has been written daily (five times per week) since 1998. As such, we operated on a set of N = 1279 documents.
Limiting attention to these 1279 documents not only reduced the dimensionality of the problem. It also allowed the clustering process to learn on a relatively clean data set.
While the entire BLS collection contains a great deal of nonprose text (i.e. tables, lists, etc.), the Editor"s Desk documents are all written in clear, journalistic prose. Each document is highly topical, further aiding the discovery of termtopic relations. Finally, the Editor"s Desk column provided an ideal learning environment because it is well-supplied with topical metadata. Each of the 1279 documents contains a list of one or more keywords. Additionally, a subset of the documents (1112) contained a subject heading. This metadata informed our learning and evaluation, as described in Section 6.1.
UNSUPERVISED LEARNING FORTOPIC DISCOVERY To derive suitably general topics for the application of a dynamic interface to the BLS collection, we combined document clustering with text classification techniques.
Specifically, using k-means, we clustered each of the 1279 documents into one of k clusters, with the number of clusters chosen by analyzing the within-cluster mean squared distance at different values of k (see Section 6.1).
Constructing mutually exclusive clusters violates our assumption that documents may belong to multiple classes. However, these clusters mark only the first step in a two-phase process of topic identification. At the end of the process, documentcluster affinity is measured by a real-valued number.
Once the Editor"s Desk documents were assigned to clusters, we constructed a k-way classifier that estimates the strength of evidence that a new document di is a member of class Ck. We tested three statistical classification techniques: probabilistic Rocchio (prind), naive Bayes, and support vector machines (SVMs). All were implemented using McCallum"s BOW text classification library [14]. Prind is a probabilistic version of the Rocchio classification algorithm [9]. Interested readers are referred to Joachims" article for 6 http://www.bls.gov/opub/ted 154 further details of the classification method. Like prind, naive Bayes attempts to classify documents into the most probable class. It is described in detail in [15]. Finally, support vector machines were thoroughly explicated by Vapnik [18], and applied specifically to text in [10]. They define a decision boundary by finding the maximally separating hyperplane in a high-dimensional vector space in which document classes become linearly separable.
Having clustered the documents and trained a suitable classifier, the remaining 14,000 documents in the collection are labeled by means of automatic classification. That is, for each document di we derive a k-dimensional vector, quantifying the association between di and each class C1 . . . Ck.
Deriving topic scores via naive Bayes for the entire 15,000document collection required less than two hours of CPU time. The output of this process is a score for every document in the collection on each of the automatically discovered topics. These scores may then be used to populate a relation browser interface, or they may be added to a traditional information retrieval system. To use these weights in the relation browser we currently assign to each document the two topics on which it scored highest. In future work we will adopt a more rigorous method of deriving documenttopic weight thresholds. Also, evaluation of the utility of the learned topics for users will be undertaken.
DISCOVERY Prior to implementing a relation browser interface and undertaking the attendant user studies, it is of course important to evaluate the quality of the inferred concepts, and the ability of the automatic classifier to assign documents to the appropriate subjects. To evaluate the success of the two-stage approach described in Section 5, we undertook two experiments. During the first experiment we compared three methods of document representation for the clustering task. The goal here was to compare the quality of document clusters derived by analysis of full-text documents, documents represented only by their titles, and documents represented by human-created keyword metadata. During the second experiment, we analyzed the ability of the statistical classifiers to discern the subject matter of documents from portions of the database in addition to the Editor"s Desk.
Documents from The Editor"s Desk column came supplied with human-generated keyword metadata.
Additionally, The titles of the Editor"s Desk documents tend to be germane to the topic of their respective articles. With such an array of distilled evidence of each document"s subject matter, we undertook a comparison of document representations for topic discovery by clustering. We hypothesized that keyword-based clustering would provide a useful model.
But we hoped to see whether comparable performance could be attained by methods that did not require extensive human indexing, such as the title-only or full-text representations. To test this hypothesis, we defined three modes of document representation-full-text, title-only, and keyword only-we generated three sets of topics, Tfull, Ttitle, and Tkw, respectively.
Topics based on full-text documents were derived by application of k-means clustering to the 1279 Editor"s Desk documents, where each document was represented by a 1908dimensional vector. These 1908 dimensions captured the TF.IDF weights [3] of each term ti in document dj, for all terms that occurred at least three times in the data. To arrive at the appropriate number of clusters for these data, we inspected the within-cluster mean-squared distance for each value of k = 1 . . . 20. As k approached 10 the reduction in error with the addition of more clusters declined notably, suggesting that k ≈ 10 would yield good divisions. To select a single integer value, we calculated which value of k led to the least variation in cluster size. This metric stemmed from a desire to suppress the common result where one large cluster emerges from the k-means algorithm, accompanied by several accordingly small clusters. Without reason to believe that any single topic should have dramatically high prior odds of document membership, this heuristic led to kfull = 10.
Clusters based on document titles were constructed similarly. However, in this case, each document was represented in the vector space spanned by the 397 terms that occur at least twice in document titles. Using the same method of minimizing the variance in cluster membership ktitle-the number of clusters in the title-based representation-was also set to 10.
The dimensionality of the keyword-based clustering was very similar to that of the title-based approach. There were
median number of keywords per document was 7, where a keyword is understood to be either a single word, or a multiword term such as consumer price index. It is worth noting that the keywords were not drawn from any controlled vocabulary; they were assigned to documents by publishers at the BLS. Using the keywords, the documents were clustered into 10 classes.
To evaluate the clusters derived by each method of document representation, we used the subject headings that were included with 1112 of the Editor"s Desk documents. Each of these 1112 documents was assigned one or more subject headings, which were withheld from all of the cluster applications. Like the keywords, subject headings were assigned to documents by BLS publishers. Unlike the keywords, however, subject headings were drawn from a controlled vocabulary. Our analysis began with the assumption that documents with the same subject headings should cluster together. To facilitate this analysis, we took a conservative approach; we considered multi-subject classifications to be unique. Thus if document di was assigned to a single subject prices, while document dj was assigned to two subjects, international comparisons, prices, documents di and dj are not considered to come from the same class.
Table 1 shows all Editor"s Desk subject headings that were assigned to at least 10 documents. As noted in the table, 155 Table 1: Top Editor"s Desk Subject Headings Subject Count prices 92 unemployment 55 occupational safety & health 53 international comparisons, prices 48 manufacturing, prices 45 employment 44 productivity 40 consumer expenditures 36 earnings & wages 27 employment & unemployment 27 compensation costs 25 earnings & wages, metro. areas 18 benefits, compensation costs 18 earnings & wages, occupations 17 employment, occupations 14 benefits 14 earnings & wage, regions 13 work stoppages 12 earnings & wages, industries 11 Total 609 Table 2: Contingecy Table for Three Document Representations Representation Right Wrong Accuracy Full-text 392 217 0.64 Title 441 168 0.72 Keyword 601 8 0.98 there were 19 such subject headings, which altogether covered 609 (54%) of the documents with subjects assigned.
These document-subject pairings formed the basis of our analysis. Limiting analysis to subjects with N > 10 kept the resultant χ2 tests suitably robust.
The clustering derived by each document representation was tested by its ability to collocate documents with the same subjects. Thus for each of the 19 subject headings in Table 1, Si, we calculated the proportion of documents assigned to Si that each clustering co-classified. Further, we assumed that whichever cluster captured the majority of documents for a given class constituted the right answer for that class. For instance, There were 92 documents whose subject heading was prices. Taking the BLS editors" classifications as ground truth, all 92 of these documents should have ended up in the same cluster. Under the full-text representation 52 of these documents were clustered into category 5, while 35 were in category 3, and 5 documents were in category 6. Taking the majority cluster as the putative right home for these documents, we consider the accuracy of this clustering on this subject to be 52/92 = 0.56. Repeating this process for each topic across all three representations led to the contingency table shown in Table 2.
The obvious superiority of the keyword-based clustering evidenced by Table 2 was borne out by a χ2 test on the accuracy proportions. Comparing the proportion right and Table 3: Keyword-Based Clusters benefits costs international jobs plans compensation import employment benefits costs prices jobs employees benefits petroleum youth occupations prices productivity safety workers prices productivity safety earnings index output health operators inflation nonfarm occupational spending unemployment expenditures unemployment consumer mass spending jobless wrong achieved by keyword and title-based clustering led to p 0.001. Due to this result, in the remainder of this paper, we focus our attention on the clusters derived by analysis of the Editor"s Desk keywords. The ten keyword-based clusters are shown in Table 3, represented by the three terms most highly associated with each cluster, in terms of the log-odds ratio. Additionally, each cluster has been given a label by the researchers.
Evaluating the results of clustering is notoriously difficult.
In order to lend our analysis suitable rigor and utility, we made several simplifying assumptions. Most problematic is the fact that we have assumed that each document belongs in only a single category. This assumption is certainly false.
However, by taking an extremely rigid view of what constitutes a subject-that is, by taking a fully qualified and often multipart subject heading as our unit of analysis-we mitigate this problem. Analogically, this is akin to considering the location of books on a library shelf. Although a given book may cover many subjects, a classification system should be able to collocate books that are extremely similar, say books about occupational safety and health. The most serious liability with this evaluation, then, is the fact that we have compressed multiple subject headings, say prices : international into single subjects. This flattening obscures the multivalence of documents. We turn to a more realistic assessment of document-class relations in Section 6.2.
Although the keyword-based clusters appear to classify the Editor"s Desk documents very well, their discovery only solved half of the problem required for the successful implementation of a dynamic user interface such as the relation browser. The matter of roughly fourteen thousand unclassified documents remained to be addressed. To solve this problem, we trained the statistical classifiers described above in Section 5. For each document in the collection di, these classifiers give pi, a k-vector of probabilities or distances (depending on the classification method used), where pik quantifies the strength of association between the ith document and the kth class. All classifiers were trained on the full text of each document, regardless of the representation used to discover the initial clusters. The different training sets were thus constructed simply by changing the 156 Table 4: Cross Validation Results for 3 Classifiers Method Av. Percent Accuracy SE Prind 59.07 1.07 Naive Bayes 75.57 0.4 SVM 75.08 0.68 class variable for each instance (document) to reflect its assigned cluster under a given model.
To test the ability of each classifier to locate documents correctly, we first performed a 10-fold cross validation on the Editor"s Desk documents. During cross-validation the data are split randomly into n subsets (in this case n = 10).
The process proceeds by iteratively holding out each of the n subsets as a test collection for a model trained on the remaining n − 1 subsets. Cross validation is described in [15]. Using this methodology, we compared the performance of the three classification models described above. Table 4 gives the results from cross validation.
Although naive Bayes is not significantly more accurate for these data than the SVM classifier, we limit the remainder of our attention to analysis of its performance. Our selection of naive Bayes is due to the fact that it appears to work comparably to the SVM approach for these data, while being much simpler, both in theory and implementation.
Because we have only 1279 documents and 10 classes, the number of training documents per class is relatively small.
In addition to models fitted to the Editor"s Desk data, then, we constructed a fourth model, supplementing the training sets of each class by querying the Google search engine7 and applying naive Bayes to the augmented test set. For each class, we created a query by submitting the three terms with the highest log-odds ratio with that class. Further, each query was limited to the domain www.bls.gov. For each class we retrieved up to 400 documents from Google (the actual number varied depending on the size of the result set returned by Google). This led to a training set of 4113 documents in the augmented model, as we call it below8 . Cross validation suggested that the augmented model decreased classification accuracy (accuracy= 58.16%, with standard error= 0.32). As we discuss below, however, augmenting the training set appeared to help generalization during our second experiment.
The results of our cross validation experiment are encouraging. However, the success of our classifiers on the Editor"s Desk documents that informed the cross validation study may not be good predictors of the models" performance on the remainder to the BLS website. To test the generality of the naive Bayes classifier, we solicited input from 11 human judges who were familiar with the BLS website. The sample was chosen by convenience, and consisted of faculty and graduate students who work on the GovStat project.
However, none of the reviewers had prior knowledge of the outcome of the classification before their participation. For the experiment, a random sample of 100 documents was drawn from the entire BLS collection. On average each re7 http://www.google.com 8 A more formal treatment of the combination of labeled and unlabeled data is available in [4].
Table 5: Human-Model Agreement on 100 Sample Docs.
Human Judge 1st Choice Model Model 1st Choice Model 2nd Choice N. Bayes (aug.) 14 24 N. Bayes 24 1 Human Judge 2nd Choice Model Model 1st Choice Model 2nd Choice N. Bayes (aug.) 14 21 N. Bayes 21 4 viewer classified 83 documents, placing each document into as many of the categories shown in Table 3 as he or she saw fit.
Results from this experiment suggest that room for improvement remains with respect to generalizing to the whole collection from the class models fitted to the Editor"s Desk documents. In Table 5, we see, for each classifier, the number of documents for which it"s first or second most probable class was voted best or second best by the 11 human judges.
In the context of this experiment, we consider a first- or second-place classification by the machine to be accurate because the relation browser interface operates on a multiway classification, where each document is classified into multiple categories. Thus a document with the correct class as its second choice would still be easily available to a user. Likewise, a correct classification on either the most popular or second most popular category among the human judges is considered correct in cases where a given document was classified into multiple classes. There were 72 multiclass documents in our sample, as seen in Figure 4. The remaining 28 documents were assigned to 1 or 0 classes.
Under this rationale, The augmented naive Bayes classifier correctly grouped 73 documents, while the smaller model (not augmented by a Google search) correctly classified 50.
The resultant χ2 test gave p = 0.001, suggesting that increasing the training set improved the ability of the naive Bayes model to generalize from the Editor"s Desk documents to the collection as a whole. However, the improvement afforded by the augmented model comes at some cost. In particular, the augmented model is significantly inferior to the model trained solely on Editor"s Desk documents if we concern ourselves only with documents selected by the majority of human reviewers-i.e. only first-choice classes. Limiting the right answers to the left column of Table 5 gives p = 0.02 in favor of the non-augmented model. For the purposes of applying the relation browser to complex digital library content (where documents will be classified along multiple categories), the augmented model is preferable. But this is not necessarily the case in general.
It must also be said that 73% accuracy under a fairly liberal test condition leaves room for improvement in our assignment of topics to categories. We may begin to understand the shortcomings of the described techniques by consulting Figure 5, which shows the distribution of categories across documents given by humans and by the augmented naive Bayes model. The majority of reviewers put 157 Number of Human-Assigned ClassesMNumber of Human-Assigned ClassesM Number of Human-Assigned Classes FrequencyMFrequencyM Frequency m00M0M 0 11M1M 1 22M2M 2 33M3M 3 44M4M 4 55M5M 5 66M6M 6 77M7M 7 m00M0M 055M5M 51010M10M 101515M15M 152020M20M 202525M25M 253030M30M 303535M35M 35 Figure 4: Number of Classes Assigned to Documents by Judges documents into only three categories, jobs, benefits, and occupations. On the other hand, the naive Bayes classifier distributed classes more evenly across the topics. This behavior suggests areas for future improvement. Most importantly, we observed a strong correlation among the three most frequent classes among the human judges (for instance, there was 68% correlation between benefits and occupations). This suggests that improving the clustering to produce topics that were more nearly orthogonal might improve performance.
Many developers and maintainers of digital libraries share the basic problem pursued here. Given increasingly large, complex bodies of data, how may we improve access to collections without incurring extraordinary cost, and while also keeping systems receptive to changes in content over time?
Data mining and machine learning methods hold a great deal of promise with respect to this problem. Empirical methods of knowledge discovery can aid in the organization and retrieval of information. As we have argued in this paper, these methods may also be brought to bear on the design and implementation of advanced user interfaces.
This study explored a hybrid technique for aiding information architects as they implement dynamic interfaces such as the relation browser. Our approach combines unsupervised learning techniques, applied to a focused subset of the BLS website. The goal of this initial stage is to discover the most basic and far-reaching topics in the collection. Based mjobsjobsMjobsM jobs benefitsunemploymentpricespricesMpricesM prices safetyinternationalspendingspendingMspendingM spending occupationscostscostsMcostsM costs productivityHuman ClassificationsMHuman ClassificationsM Human Classifications m0.000.00M0.00M
jobs benefitsunemploymentpricespricesMpricesM prices safetyinternationalspendingspendingMspendingM spending occupationscostscostsMcostsM costs productivityMachine ClassificationsMMachine ClassificationsM Machine Classifications m0.000.00M0.00M
Figure 5: Distribution of Classes Across Documents on a statistical model of these topics, the second phase of our approach uses supervised learning (in particular, a naive Bayes classifier, trained on individual words), to assign topical relations to the remaining documents in the collection.
In the study reported here, this approach has demonstrated promise. In its favor, our approach is highly scalable.
It also appears to give fairly good results. Comparing three modes of document representation-full-text, title only, and keyword-we found 98% accuracy as measured by collocation of documents with identical subject headings. While it is not surprising that editor-generated keywords should give strong evidence for such learning, their superiority over fulltext and titles was dramatic, suggesting that even a small amount of metadata can be very useful for data mining.
However, we also found evidence that learning topics from a subset of the collection may lead to overfitted models.
After clustering 1279 Editor"s Desk documents into 10 categories, we fitted a 10-way naive Bayes classifier to categorize the remaining 14,000 documents in the collection. While we saw fairly good results (classification accuracy of 75% with respect to a small sample of human judges), this experiment forced us to reconsider the quality of the topics learned by clustering. The high correlation among human judgments in our sample suggests that the topics discovered by analysis of the Editor"s Desk were not independent. While we do not desire mutually exclusive categories in our setting, we do desire independence among the topics we model.
Overall, then, the techniques described here provide an encouraging start to our work on acquiring subject metadata for dynamic interfaces automatically. It also suggests that a more sophisticated modeling approach might yield 158 better results in the future. In upcoming work we will experiment with streamlining the two-phase technique described here. Instead of clustering documents to find topics and then fitting a model to the learned clusters, our goal is to expand the unsupervised portion of our analysis beyond a narrow subset of the collection, such as The Editor"s Desk.
In current work we have defined algorithms to identify documents likely to help the topic discovery task. Supplied with a more comprehensive training set, we hope to experiment with model-based clustering, which combines the clustering and classification processes into a single modeling procedure.
Topic discovery and document classification have long been recognized as fundamental problems in information retrieval and other forms of text mining. What is increasingly clear, however, as digital libraries grow in scope and complexity, is the applicability of these techniques to problems at the front-end of systems such as information architecture and interface design. Finally, then, in future work we will build on the user studies undertaken by Marchionini and Brunk in efforts to evaluate the utility of automatically populated dynamic interfaces for the users of digital libraries.
[1] A. Agresti. An Introduction to Categorical Data Analysis. Wiley, New York, 1996. [2] C. Ahlberg, C. Williamson, and B. Shneiderman.
Dynamic queries for information exploration: an implementation and evaluation. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 619-626, 1992. [3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. ACM Press, 1999. [4] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92-100. ACM Press, 1998. [5] H. Chen and S. Dumais. Hierarchical classification of web content. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 256-263,
[6] M. Efron, G. Marchionini, and J. Zhang. Implications of the recursive representation problem for automatic concept identification in on-line governmental information. In Proceedings of the ASIST Special Interest Group on Classification Research (ASIST SIG-CR), 2003. [7] C. Fraley and A. E. Raftery. How many clusters? which clustering method? answers via model-based cluster analysis. The Computer Journal, 41(8):578-588, 1998. [8] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Computing Surveys, 31(3):264-323, September 1999. [9] T. Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. In D. H. Fisher, editor, Proceedings of ICML-97, 14th International Conference on Machine Learning, pages 143-151, Nashville, US, 1997. Morgan Kaufmann Publishers, San Francisco, US. [10] T. Joachims. Text categorization with support vector machines: learning with many relevant features. In C. N´edellec and C. Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137-142, Chemnitz, DE, 1998.
Springer Verlag, Heidelberg, DE. [11] I. T. Jolliffe. Principal Component Analysis. Springer, 2nd edition, 2002. [12] L. Kaufman and P. J. Rosseeuw. Finding Groups in Data: an Introduction to Cluster Analysis. Wiley,
[13] G. Marchionini and B. Brunk. Toward a general relation browser: a GUI for information architects.
Journal of Digital Information, 4(1), 2003. http://jodi.ecs.soton.ac.uk/Articles/v04/i01/Marchionini/. [14] A. K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/˜mccallum/bow,
[15] T. Mitchell. Machine Learning. McGraw Hill, 1997. [16] E. Rasmussen. Clustering algorithms. In W. B. Frakes and R. Baeza-Yates, editors, Information Retrieval: Data Structures and Algorithms, pages 419-442.

Despite numerous refinements and optimizations, general purpose search engines still fail to find relevant results for many queries. As a new trend, vertical search has shown promise because it can leverage domain-specific knowledge and is more effective in connecting users with the information they want. There are many vertical search engines, including some for paper search (e.g. Libra [21], Citeseer [7] and Google Scholar [4]), product search (e.g. Froogle [5]), movie search [6], image search [1, 8], video search [6], local search [2], as well as news search [3]. We believe the vertical search engine trend will continue to grow.
Essentially, building vertical search engines includes data crawling, information extraction, object identification and integration, and object-level Web information retrieval (or Web object ranking) [20], among which ranking is one of the most important factors. This is because it deals with the core problem of how to combine and rank objects coming from multiple communities.
Although object-level ranking has been well studied in building vertical search engines, there are still some kinds of vertical domains in which objects cannot be effectively ranked. For example, algorithms that evolved from PageRank [22], PopRank [21] and LinkFusion [27] were proposed to rank objects coming from multiple communities, but can only work on well-defined graphs of heterogeneous data.
Well-defined means that like objects (e.g. authors in paper search) can be identified in multiple communities (e.g. conferences). This allows heterogeneous objects to be well linked to form a graph through leveraging all the relationships (e.g. cited-by, authored-by and published-by) among the multiple communities.
However, this assumption does not always stand for some domains. High-quality photo search, movie search and news search are exceptions. For example, a photograph forum 377 website usually includes three kinds of objects: photos, authors and reviewers. Yet different photo forums seem to lack any relationships, as there are no cited-by relationships.
This makes it difficult to judge whether two authors cited are the same author, or two photos are indeed identical photos. Consequently, although each photo has a rating score in a forum, it is non-trivial to rank photos coming from different photo forums. Similar problems also exist in movie search and news search. Although two movie titles can be identified as the same one by title and director in different movie discussion groups, it is non-trivial to combine rating scores from different discussion groups and rank movies effectively. We call such non-trivial object relationship in which identification is difficult, incomplete relationships.
Other related work includes rank aggregation for the Web [13, 14], and learning algorithm for rank, such as RankBoost [15], RankSVM [17, 19], and RankNet [12]. We will contrast differences of these methods with the proposed methods after we have described the problem and our methods.
We will specifically focus on Web object-ranking problem in cases that lack object relationships or have with incomplete object relationships, and take high-quality photo search as the test bed for this investigation. In the following, we will introduce rationale for building high-quality photo search.
In the past ten years, the Internet has grown to become an incredible resource, allowing users to easily access a huge number of images. However, compared to the more than 1 billion images indexed by commercial search engines, actual queries submitted to image search engines are relatively minor, and occupy only 8-10 percent of total image and text queries submitted to commercial search engines [24]. This is partially because user requirements for image search are far less than those for general text search. On the other hand, current commercial search engines still cannot well meet various user requirements, because there is no effective and practical solution to understand image content.
To better understand user needs in image search, we conducted a query log analysis based on a commercial search engine. The result shows that more than 20% of image search queries are related to nature and places and daily life categories. Users apparently are interested in enjoying high-quality photos or searching for beautiful images of locations or other kinds. However, such user needs are not well supported by current image search engines because of the difficulty of the quality assessment problem.
Ideally, the most critical part of a search engine - the ranking function - can be simplified as consisting of two key factors: relevance and quality. For the relevance factor, search in current commercial image search engines provide most returned images that are quite relevant to queries, except for some ambiguity. However, as to quality factor, there is still no way to give an optimal rank to an image.
Though content-based image quality assessment has been investigated over many years [23, 25, 26], it is still far from ready to provide a realistic quality measure in the immediate future.
Seemingly, it really looks pessimistic to build an image search engine that can fulfill the potentially large requirement of enjoying high-quality photos. Various proliferating Web communities, however, notices us that people today have created and shared a lot of high-quality photos on the Web on virtually any topics, which provide a rich source for building a better image search engine.
In general, photos from various photo forums are of higher quality than personal photos, and are also much more appealing to public users than personal photos. In addition, photos uploaded to photo forums generally require rich metadata about title, camera setting, category and description to be provide by photographers. These metadata are actually the most precise descriptions for photos and undoubtedly can be indexed to help search engines find relevant results.
More important, there are volunteer users in Web communities actively providing valuable ratings for these photos.
The rating information is generally of great value in solving the photo quality ranking problem.
Motivated by such observations, we have been attempting to build a vertical photo search engine by extracting rich metadata and integrating information form various photo Web forums. In this paper, we specifically focus on how to rank photos from multiple Web forums.
Intuitively, the rating scores from different photo forums can be empirically normalized based on the number of photos and the number of users in each forum. However, such a straightforward approach usually requires large manual effort in both tedious parameter tuning and subjective results evaluation, which makes it impractical when there are tens or hundreds of photo forums to combine. To address this problem, we seek to build relationships/links between different photo forums. That is, we first adopt an efficient algorithm to find duplicate photos which can be considered as hidden links connecting multiple forums. We then formulate the ranking challenge as an optimization problem, which eventually results in an optimal ranking function.
The main contributions of this paper are:
engine by leveraging rich metadata from various photo forum Web sites to meet user requirements of searching for and enjoying high-quality photos, which is impossible in traditional image search engines.
algorithms for photos with incomplete relationships, which can automatically and efficiently integrate as many as possible Web communities with rating information and achieves an equal qualitative result compared with the manually tuned fusion scheme.
The rest of this paper is organized as follows. In Section 2, we present in detail the proposed solutions to the ranking problem, including how to find hidden links between different forums, normalize rating scores, obtain the optimal ranking function, and contrast our methods with some other related research. In Section 3, we describe the experimental setting and experiments and user studies conducted to evaluate our algorithm. Our conclusion and a discussion of future work is in Section 4.
It is worth noting that although we treat vertical photo search as the test bed in this paper, the proposed ranking algorithm can also be applied to rank other content that includes video clips, poems, short stories, drawings, sculptures, music, and so on. 378
The difficulty of integrating multiple Web forums is in their different rating systems, where there are generally two kinds of freedom. The first kind of freedom is the rating interval or rating scale including the minimal and maximal ratings for each Web object. For example, some forums use a 5-point rating scale whereas other forums use 3-point or 10-point rating scales. It seems easy to fix this freedom, but detailed analysis of the data and experiments show that it is a non-trivial problem.
The second kind of freedom is the varying rating criteria found in different Web forums. That is, the same score does not mean the same quality in different forums. Intuitively, if we can detect same photographers or same photographs, we can build relationships between any two photo forums and therefore can standardize the rating criterion by score normalization and transformation. Fortunately, we find that quite a number of duplicate photographs exist in various Web photo forums. This fact is reasonable when considering that photographers sometimes submit a photo to more than one forum to obtain critiques or in hopes of widespread publicity. In this work, we adopt an efficient duplicate photo detection algorithm [10] to find these photos.
The proposed methods below are based on the following considerations. Faced with the need to overcome a ranking problem, a standardized rating criterion rather than a reasonable rating criterion is needed. Therefore, we can take a large scale forum as the reference forum, and align other forums by taking into account duplicate Web objects (duplicate photos in this work). Ideally, the scores of duplicate photos should be equal even though they are in different forums. Yet we can deem that scores in different forumsexcept for the reference forum - can vary in a parametric space. This can be determined by minimizing the objective function defined by the sum of squares of the score differences. By formulating the ranking problem as an optimization problem that attempts to make the scores of duplicate photos in non-reference forums as close as possible to those in the reference forum, we can effectively solve the ranking problem.
For convenience, the following notations are employed.
Ski and ¯Ski denote the total score and mean score of ith Web object (photo) in the kth Web site, respectively. The total score refers to the sum of the various rating scores (e.g., novelty rating and aesthetic rating), and the mean score refers to the mean of the various rating scores. Suppose there are a total of K Web sites. We further use {Skl i |i = 1, ..., Ikl; k, l = 1, ..., K; k = l} to denote the set of scores for Web objects (photos) in kth Web forums that are duplicate with the lth Web forums, where Ikl is the total number of duplicate Web objects between these two Web sites. In general, score fusion can be seen as the procedure of finding K transforms ψk(Ski) = eSki, k = 1, ..., K such that eSki can be used to rank Web objects from different Web sites. The objective function described in the above Figure 1: Web community integration. Each Web community forms a subgraph, and all communities are linked together by some hidden links (dashed lines). paragraph can then be formulated as min {ψk|k=2,...,K} KX k=2 Ik1X i=1 ¯wk i  S1k i − ψk(Sk1 i ) 2 (1) where we use k = 1 as the reference forum and thus ψ1(S1i) = S1i. ¯wk i (≥ 0) is the weight coefficient that can be set heuristically according to the numbers of voters (reviewers or commenters) in both the reference forum and the non-reference forum. The more reviewers, the more popular the photo is and the larger the corresponding weight ¯wk i should be. In this work, we do not inspect the problem of how to choose ¯wk i and simply set them to one. But we believe the proper use of ¯wk i , which leverages more information, can significantly improve the results.
Figure 1 illustrates the aforementioned idea. The Web Community 1 is the reference community. The dashed lines are links indicating that the two linked Web objects are actually the same. The proposed algorithm will try to find the best ψk(k = 2, ..., K), which has certain parametric forms according to certain models. So as to minimize the cost function defined in Eq. 1, the summation is taken on all the red dashed lines.
We will first discuss the score normalization methods in Section 2.2, which serves as the basis for the following work.
Before we describe the proposed ranking algorithms, we first introduce a manually tuned method in Section 2.3, which is laborious and even impractical when the number of communities become large. In Section 2.4, we will briefly explain how to precisely find duplicate photos between Web forums.
Then we will describe the two proposed methods: Linear fusion and Non-linear fusion, and a performance measure for result evaluation in Section 2.5. Finally, in Section 2.6 we will discuss the relationship of the proposed methods with some other related work.
Since different Web (photo) forums on the Web usually have different rating criteria, it is necessary to normalize them before applying different kinds of fusion methods. In addition, as there are many kinds of ratings, such as ratings for novelty, ratings for aesthetics etc, it is reasonable to choose a common one - total score or average scorethat can always be extracted in any Web forum or calculated by corresponding ratings. This allows the normaliza379 tion method on the total score or average score to be viewed as an impartial rating method between different Web forums.
It is straightforward to normalize average scores by linearly transforming them to a fixed interval. We call this kind of score as Scaled Mean Score. The difficulty, however, of using this normalization method is that, if there are only a few users rating an object, say a photo in a photo forum, the average score for the object is likely to be spammed or skewed.
Total score can avoid such drawbacks that contain more information such as a Web object"s quality and popularity.
The problem is thus how to normalize total scores in different Web forums. The simplest way may be normalization by the maximal and minimal scores. The drawback of this normalization method is it is non robust, or in other words, it is sensitive to outliers.
To make the normalization insensitive to unusual data, we propose the Mode-90% Percentile normalization method.
Here, the mode score represents the total score that has been assigned to more photos than any other total score. And The high percentile score (e.g.,90%) represents the total score for which the high percentile of images have a lower total score.
This normalization method utilizes the mode and 90% percentile as two reference points to align two rating systems, which makes the distributions of total scores in different forums more consistent. The underlying assumption, for example in different photo forums, is that even the qualities of top photos in different forums may vary greatly and be less dependent on the forum quality, the distribution of photos of middle-level quality (from mode to 90% percentile) should be almost of the same quality up to the freedom which reflects the rating criterion (strictness) of Web forums.
Photos of this middle-level in a Web forum usually occupy more than 70 % of total photos in that forum.
We will give more detailed analysis of the scores in Section
The Web movie forum, IMDB [16], proposed to use a Bayesian-ranking function to normalize rating scores within one community. Motivated by this ranking function, we propose this manual fusion method: For the kth Web site, we use the following formula eSki = αk · „ nk · ¯Ski nk + n∗ k + n∗ k · S∗ k nk + n∗ k « (2) to rank photos, where nk is the number of votes and n∗ k,
S∗ k and αk are three parameters. This ranking function first takes a balance between the original mean score ¯Ski and a reference score S∗ k to get a weighted mean score which may be more reliable than ¯Ski. Then the weighted mean score is scaled by αk to get the final score fSki.
For n Web communities, there are then about 3n parameters in {(αk, n∗ k, S∗ k)|k = 1, ..., n} to tune. Though this method can achieves pretty good results after careful and thorough manual tuning on these parameters, when n becomes increasingly large, say there are tens or hundreds of Web communities crawled and indexed, this method will become more and more laborious and will eventually become impractical. It is therefore desirable to find an effective fusion method whose parameters can be automatically determined.
We use Dedup [10], an efficient and effective duplicate image detection algorithm, to find duplicate photos between any two photo forums. This algorithm uses hash function to map a high dimensional feature to a 32 bits hash code (see below for how to construct the hash code). Its computational complexity to find all the duplicate images among n images is about O(n log n). The low-level visual feature for each photo is extracted on k × k regular grids. Based on all features extracted from the image database, a PCA model is built. The visual features are then transformed to a relatively low-dimensional and zero mean PCA space, or
photo is built as follows: each dimension is transformed to one, if the value in this dimension is greater than 0, and 0 otherwise. Photos in the same bucket are deemed potential duplicates and are further filtered by a threshold in terms of Euclidean similarity in the visual feature space.
Figure 2 illustrates the hashing procedure, where visual features - mean gray values - are extracted on both 6 × 6 and 7×7 grids. The 85-dimensional features are transformed to a 32-dimensional vector, and the hash code is generated according to the signs.
Figure 2: Hashing procedure for duplicate photo dectection
In this section, we will present two solutions on score fusion based on different parametric form assumptions of ψk in Eq. 1.
Intuitively, the most straightforward way to factor out the uncertainties caused by the different criterion is to scale, rel380 ative to a given center, the total scores of each unreferenced Web photo forum with respect to the reference forum. More strictly, we assume ψk has the following form ψk(Ski) = αkSki + tk, k = 2, ..., K (3) ψ1(S1i) = S1i (4) which means that the scores of k(= 1)th forum should be scaled by αk relative to the center tk 1−αk as shown in Figure
Then, if we substitute above ψk to Eq. 1, we get the following objective function, min {αk,tk|k=2,...,K} KX k=2 Ik1X i=1 ¯wk i h S1k i − αkSk1 i − tk i2 . (5) By solving the following set of functions, ( ∂f ∂αk = = 0 ∂f ∂tk = 0 , k = 1, ..., K where f is the objective function defined in Eq. 5, we get the closed form solution as: „ αk tk « = A−1 k Lk (6) where Ak = „ P i ¯wi(Sk1 i )2 P i ¯wiSk1 iP i ¯wiSk1 i P i ¯wi « (7) Lk = „ P i ¯wiS1k i Sk1 iP i ¯wiS1k i « (8) and k = 2, ..., K.
This is a linear fusion method. It enjoys simplicity and excellent performance in the following experiments.
Figure 3: Linear Fusion method
Sometimes we want a method which can adjust scores on intervals with two endpoints unchanged. As illustrated in Figure 4, the method can tune scores between [C0, C1] while leaving scores C0 and C1 unchanged. This kind of fusion method is then much finer than the linear ones and contains many more parameters to tune and expect to further improve the results.
Here, we propose a nonlinear fusion solution to satisfy such constraints. First, we introduce a transform: ηc0,c1,α(x) = ( x−c0 c1−c0 α (c1 − c0) + c0, if x ∈ (c0, c1] x otherwise where α > 0. This transform satisfies that for x ∈ [c0, c1], ηc0,c1,α(x) ∈ [c0, c1] with ηc0,c1,α(c0) = c0 and ηc0,c1,α(c1) = c1. Then we can utilize this nonlinear transform to adjust the scores in certain interval, say (M, T], ψk(Ski) = ηM,T,α(Ski) . (9) Figure 4: Nonlinear Fusion method. We intent to finely adjust the shape of the curves in each segment.
Even there is no closed-form solution for the following optimization problem, min {αk|k∈[2,K]} KX k=2 Ik1X i=1 ¯wk i h S1k i − ηM,T,α(Ski) i2 it is not hard to get the numeric one. Under the same assumptions made in Section 2.2, we can use this method to adjust scores of the middle-level (from the mode point to the 90 % percentile).
This more complicated non-linear fusion method is expected to achieve better results than the linear one.
However, difficulties in evaluating the rank results block us from tuning these parameters extensively. The current experiments in Section 3.5 do not reveal any advantages over the simple linear model.
Since our objective function is to make the scores of the same Web objects (e.g. duplicate photos) between a nonreference forum and the reference forum as close as possible, it is natural to investigate how close they become to each other and how the scores of the same Web objects change between the two non-reference forums before and after score fusion.
Taken Figure 1 as an example, the proposed algorithms minimize the score differences of the same Web objects in two Web forums: the reference forum (the Web Community 1) and a non-reference forum, which corresponds to minimizing the objective function on the red dashed (hidden) links. After the optimization, we must ask what happens to the score differences of the same Web objects in two nonreference forums? Or, in other words, whether the scores of two objects linked by the green dashed (hidden) links become more consistent?
We therefore define the following performance measureδ measure - to quantify the changes for scores of the same Web objects in different Web forums as δkl = Sim(Slk , Skl ) − Sim(Slk ∗ , Skl ∗ ) (10) 381 where Skl = (Skl
Ikl )T , Skl ∗ = (eSkl
Ikl )T and Sim(a, b) = a · b ||a||||b|| . δkl > 0 means after score fusion, scores on the same Web objects between kth and lth Web forum become more consistent, which is what we expect. On the contrary, if δkl < 0, those scores become more inconsistent.
Although we cannot rely on this measure to evaluate our final fusion results as ranking photos by their popularity and qualities is such a subjective process that every person can have its own results, it can help us understand the intermediate ranking results and provide insights into the final performances of different ranking methods.
We have already mentioned the differences of the proposed methods with the traditional methods, such as PageRank [22], PopRank [21], and LinkFusion [27] algorithms in Section 1. Here, we discuss some other related works.
The current problem can also be viewed as a rank aggregation one [13, 14] as we deal with the problem of how to combine several rank lists. However, there are fundamental differences between them. First of all, unlike the Web pages, which can be easily and accurately detected as the same pages, detecting the same photos in different Web forums is a non-trivial work, and can only be implemented by some delicate algorithms while with certain precision and recall. Second, the numbers of the duplicate photos from different Web forums are small relative to the whole photo sets (see Table 1). In another words, the top K rank lists of different Web forums are almost disjointed for a given query. Under this condition, both the algorithms proposed in [13] and their measurements - Kendall tau distance or Spearman footrule distance - will degenerate to some trivial cases.
Another category of rank fusion (aggregation) methods is based on machine learning algorithms, such as RankSVM [17, 19], RankBoost [15], and RankNet [12]. All of these methods entail some labelled datasets to train a model. In current settings, it is difficult or even impossible to get these datasets labelled as to their level of professionalism or popularity, since the photos are too vague and subjective to rank.
Instead, the problem here is how to combine several ordered sub lists to form a total order list.
In this section, we carry out our research on high-quality photo search. We first briefly introduce the newly proposed vertical image search engine - EnjoyPhoto in section 3.1.
Then we focus on how to rank photos from different Web forums. In order to do so, we first normalize the scores (ratings) for photos from different multiple Web forums in section 3.2. Then we try to find duplicate photos in section
in section 3.4. Finally a set of user studies is carried out carefully to justify our proposed method in section 3.5.
Engine In order to meet user requirement of enjoying high-quality photos, we propose and build a high-quality photo search engine - EnjoyPhoto, which accounts for the following three key issues: 1. how to crawl and index photos, 2. how to determine the qualities of each photo and 3. how to display the search results in order to make the search process enjoyable. For a given text based query, this system ranks the photos based on certain combination of relevance of the photo to this query (Issue 1) and the quality of the photo (Issue 2), and finally displays them in an enjoyable manner (Issue 3).
As for Issue 3, we devise the interface of the system deliberately in order to smooth the users" process of enjoying high-quality photos. Techniques, such as Fisheye and slides show, are utilized in current system. Figure 5 shows the interface. We will not talk more about this issue as it is not an emphasis of this paper.
Figure 5: EnjoyPhoto: an enjoyable high-quality photo search engine, where 26,477 records are returned for the query fall in about 0.421 seconds As for Issue 1, we extracted from a commercial search engine a subset of photos coming from various photo forums all over the world, and explicitly parsed the Web pages containing these photos. The number of photos in the data collection is about 2.5 million. After the parsing, each photo was associated with its title, category, description, camera setting, EXIF data 1 (when available for digital images), location (when available in some photo forums), and many kinds of ratings. All these metadata are generally precise descriptions or annotations for the image content, which are then indexed by general text-based search technologies [9, 18, 11]. In current system, the ranking function was specifically tuned to emphasize title, categorization, and rating information.
Issue 2 is essentially dealt with in the following sections which derive the quality of photos by analyzing ratings provided by various Web photo forums. Here we chose six photo forums to study the ranking problem and denote them as Web-A, Web-B, Web-C, Web-D, Web-E and Web-F.
Detailed analysis of different score normalization methods are analyzed in this section. In this analysis, the zero 1 Digital cameras save JPEG (.jpg) files with EXIF (Exchangeable Image File) data. Camera settings and scene information are recorded by the camera into the image file. www.digicamhelp.com/what-is-exif/ 382
0 1000 2000 3000 4000 Normalized Score TotalNumber (a) Web-A
0
1
2
3 x 10 4 Normalized Score TotalNumber (b) Web-B
0
1
2 x 10 5 Normalized Score TotalNumber (c) Web-C
0 2 4 6 8 10 x 10 4 Normalized Score TotalNumber (d) Web-D
0 2000 4000 6000 8000 10000 12000 14000 Normalized Score TotalNumber (e) Web-E
0 1 2 3 4 5 6 x 10 4 Normalized Score TotalNumber (f) Web-F Figure 6: Distributions of mean scores normalized to [0, 10] scores that usually occupy about than 30% of the total number of photos for some Web forums are not currently taken into account. How to utilize these photos is left for future explorations.
In Figure 6, we list the distributions of the mean score, which is transformed to a fixed interval [0, 10]. The distributions of the average scores of these Web forums look quite different. Distributions in Figure 6(a), 6(b), and 6(e) looks like Gaussian distributions, while those in Figure 6(d) and 6(f) are dominated by the top score. The reason of these eccentric distributions for Web-D and Web-F lies in their coarse rating systems. In fact, Web-D and Web-F use 2 or
point rating scales. Therefore, it will be problematic if we directly use these averaged scores. Furthermore the average score is very likely to be spammed, if there are only a few users rating a photo.
Figure 7 shows the total score normalization method by maximal and minimal scores, which is one of our base line system. All the total scores of a given Web forum are normalized to [0, 100] according to the maximal score and minimal score of corresponding Web forum. We notice that total score distribution of Web-A in Figure 7(a) has two larger tails than all the others. To show the shape of the distributions more clearly, we only show the distributions on [0, 25] in Figure 7(b),7(c),7(d),7(e), and 7(f).
Figure 8 shows the Mode-90% Percentile normalization method, where the modes of the six distributions are normalized to 5 and the 90% percentile to 8. We can see that this normalization method makes the distributions of total scores in different forums more consistent. The two proposed algorithms are all based on these normalization methods.
Targeting at computational efficiency, the Dedup algorithm may lose some recall rate, but can achieve a high precision rate. We also focus on finding precise hidden links rather than all hidden links. Figure 9 shows some duplicate detection examples. The results are shown in Table 1 and verify that large numbers of duplicate photos exist in any two Web forums even with the strict condition for Dedup where we chose first 29 bits as the hash code. Since there are only a few parameters to estimate in the proposed fusion methods, the numbers of duplicate photos shown Table 1 are
0 100 200 300 400 500 600 Normalized Score TotalNumber (a) Web-A
0 1 2 3 4 5 x 10 4 Normalized Score TotalNumber (b) Web-B
0 1 2 3 4 5 x 10 5 Normalized Score TotalNumber (c) Web-C
0
1
2
x 10 4 Normalized Score TotalNumber (d) Web-D
0 2000 4000 6000 8000 10000 Normalized Score TotalNumber (e) Web-E
0
1
2
3 x 10 4 Normalized Score TotalNumber (f) Web-F Figure 7: Maxmin Normalization
0 200 400 600 800 1000 1200 1400 Normalized Score TotalNumber (a) Web-A
0 1 2 3 4 5 x 10 4 Normalized Score TotalNumber (b) Web-B
0 2 4 6 8 10 12 14 x 10 4 Normalized Score TotalNumber (c) Web-C
0
1
2
x 10 4 Normalized Score TotalNumber (d) Web-D
0 2000 4000 6000 8000 10000 12000 Normalized Score TotalNumber (e) Web-E
0 2000 4000 6000 8000 10000 Normalized Score TotalNumber (f) Web-F Figure 8: Mode-90% Percentile Normalization sufficient to determine these parameters. The last table column lists the total number of photos in the corresponding Web forums.
The parameters of the proposed linear and nonlinear algorithms are calculated using the duplicate data shown in Table 1, where the Web-C is chosen as the reference Web forum since it shares the most duplicate photos with other forums.
Table 2 and 3 show the δ measure on the linear model and nonlinear model. As δkl is symmetric and δkk = 0, we only show the upper triangular part. The NaN values in both tables lie in that no duplicate photos have been detected by the Dedup algorithm as reported in Table 1.
The linear model guarantees that the δ measures related Table 1: Number of duplicate photos between each pair of Web forums A B C D E F Scale A 0 316 1,386 178 302 0 130k B 316 0 14,708 909 8,023 348 675k C 1,386 14,708 0 1,508 19,271 1,083 1,003k D 178 909 1,508 0 1,084 21 155k E 302 8,023 19,271 1,084 0 98 448k F 0 348 1,083 21 98 0 122k 383 Figure 9: Some results of duplicate photo detection Table 2: δ measure on the linear model.
Web-B Web-C Web-D Web-E Web-F Web-A 0.0659 0.0911 0.0956 0.0928 NaN Web-B - 0.0672 0.0578 0.0791 0.4618 Web-C - - 0.0105 0.0070 0.2220 Web-D - - - 0.0566 0.0232 Web-E - - - - 0.6525 to the reference community should be no less than 0 theoretically. It is indeed the case (see the underlined numbers in Table 2). But this model can not guarantee that the δ measures on the non-reference communities can also be no less than 0, as the normalization steps are based on duplicate photos between the reference community and a nonreference community. Results shows that all the numbers in the δ measure are greater than 0 (see all the non-underlined numbers in Table 2), which indicates that it is probable that this model will give optimal results.
On the contrary, the nonlinear model does not guarantee that δ measures related to the reference community should be no less than 0, as not all duplicate photos between the two Web forums can be used when optimizing this model.
In fact, the duplicate photos that lie in different intervals will not be used in this model. It is these specific duplicate photos that make the δ measure negative. As a result, there are both negative and positive items in Table 3, but overall the number of positive ones are greater than negative ones (9:5), that indicates the model may be better than the normalization only method (see next subsection) which has an all-zero δ measure, and worse than the linear model.
Because it is hard to find an objective criterion to evaluate Table 3: δ measure on the nonlinear model.
Web-B Web-C Web-D Web-E Web-F Web-A 0.0559 0.0054 -0.0185 -0.0054 NaN Web-B - -0.0162 -0.0345 -0.0301 0.0466 Web-C - - 0.0136 0.0071 0.1264 Web-D - - - 0.0032 0.0143 Web-E - - - - 0.214 which ranking function is better, we chose to employ user studies for subjective evaluations. Ten subjects were invited to participate in the user study. They were recruited from nearby universities. As search engines of both text search and image search are familiar to university students, there was no prerequisite criterion for choosing students.
We conducted user studies using Internet Explorer 6.0 on Windows XP with 17-inch LCD monitors set at 1,280 pixels by 1,024 pixels in 32-bit color. Data was recorded with server logs and paper-based surveys after each task.
Figure 10: User study interface We specifically device an interface for user study as shown in Figure 10. For each pair of fusion methods, participants were encouraged to try any query they wished. For those without specific ideas, two combo boxes (category list and query list) were listed on the bottom panel, where the top 1,000 image search queries from a commercial search engine were provided. After a participant submitted a query, the system randomly selected the left or right frame to display each of the two ranking results. The participant were then required to judge which ranking result was better of the two ranking results, or whether the two ranking results were of equal quality, and submit the judgment by choosing the corresponding radio button and clicking the Submit button.
For example, in Figure 10, query sunset is submitted to the system. Then, 79,092 photos were returned and ranked by the Minmax fusion method in the left frame and linear fusion method in the right frame. A participant then compares the two ranking results (without knowing the ranking methods) and submits his/her feedback by choosing answers in the Your option.
Table 4: Results of user study Norm.Only Manually Linear Linear 29:13:10 14:22:15Nonlinear 29:15:9 12:27:12 6:4:45 Table 4 shows the experimental results, where Linear denotes the linear fusion method, Nonlinear denotes the non linear fusion method, Norm. Only means Maxmin normalization method, Manually means the manually tuned method. The three numbers in each item, say 29:13:10, mean that 29 judgments prefer the linear fusion results, 10 384 judgments prefer the normalization only method, and 13 judgments consider these two methods as equivalent.
We conduct the ANOVA analysis, and obtain the following conclusions:
better than the Norm. Only method with respective P-values 0.00165(< 0.05) and 0.00073(<< 0.05). This result is consistent with the δ-measure evaluation result. The Norm. Only method assumes that the top 10% photos in different forums are of the same quality. However, this assumption does not stand in general. For example, a top 10% photo in a top tier photo forum is generally of higher quality than a top 10% photo in a second-tier photo forum. This is similar to that, those top 10% students in a top-tier university and those in a second-tier university are generally of different quality. Both linear and nonlinear fusion methods acknowledge the existence of such differences and aim at quantizing the differences. Therefore, they perform better than the Norm. Only method.
the nonlinear one with P-value 1.195 × 10−10 . This result is rather surprising as this more complicated ranking method is expected to tune the ranking more finely than the linear one. The main reason for this result may be that it is difficult to find the best intervals where the nonlinear tuning should be carried out and yet simply the middle part of the Mode-90% Percentile Normalization method was chosen. The timeconsuming and subjective evaluation methods - user studies - blocked us extensively tuning these parameters.
almost the same with or slightly better than the manually tuned method. Given that the linear/nonlinear fusion methods are fully automatic approaches, they are considered practical and efficient solutions when more communities (e.g. dozens of communities) need to be integrated.
In this paper, we studied the Web object-ranking problem in the cases of lacking object relationships where traditional ranking algorithms are no longer valid, and took high-quality photo search as the test bed for this investigation. We have built a vertical high-quality photo search engine, and proposed score fusion methods which can automatically integrate as many data sources (Web forums) as possible. The proposed fusion methods leverage the hidden links discovered by duplicate photo detection algorithm, and minimize score differences of duplicate photos in different forums. Both the intermediate results and the user studies show that the proposed fusion methods are a practical and efficient solution to Web object ranking in the aforesaid relationships. Though the experiments were conducted on high-quality photo ranking, the proposed algorithms are also applicable to other kinds of Web objects including video clips, poems, short stories, music, drawings, sculptures, and so on.
Current system is far from being perfect. In order to make this system more effective, more delicate analysis for the vertical domain (e.g., Web photo forums) are needed. The following points, for example, may improve the searching results and will be our future work: 1. more subtle analysis and then utilization of different kinds of ratings (e.g., novelty ratings, aesthetic ratings); 2. differentiating various communities who may have different interests and preferences or even distinct culture understandings; 3. incorporating more useful information, including photographers" and reviewers" information, to model the photos in a heterogeneous data space instead of the current homogeneous one.
We will further utilize collaborative filtering to recommend relevant high-quality photos to browsers.
One open problem is whether we can find an objective and efficient criterion for evaluating the ranking results, instead of employing subjective and inefficient user studies, which blocked us from trying more ranking algorithms and tuning parameters in one algorithm.
We thank Bin Wang and Zhi Wei Li for providing Dedup codes to detect duplicate photos; Zhen Li for helping us design the interface of EnjoyPhoto; Ming Jing Li, Longbin Chen, Changhu Wang, Yuanhao Chen, and Li Zhuang etc. for useful discussions. Special thanks go to Dwight Daniels for helping us revise the language of this paper.
[1] Google image search. http://images.google.com. [2] Google local search. http://local.google.com/. [3] Google news search. http://news.google.com. [4] Google paper search. http://Scholar.google.com. [5] Google product search. http://froogle.google.com. [6] Google video search. http://video.google.com. [7] Scientific literature digital library. http://citeseer.ist.psu.edu. [8] Yahoo image search. http://images.yahoo.com. [9] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. New York: ACM Press; Harlow, England: Addison-Wesley, 1999. [10] W. Bin, L. Zhiwei, L. Ming Jing, and M. Wei-Ying.
Large-scale duplicate detection for web image search.
In Proceedings of the International Conference on Multimedia and Expo, page 353, 2006. [11] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Computer Networks, volume 30, pages 107-117, 1998. [12] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pages 89 - 96, 2005. [13] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.
Rank aggregation methods for the web. In Proceedings 10th International Conference on World Wide Web, pages 613 - 622, Hong-Kong, 2001. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. SIAM Journal on Discrete Mathematics, 17(1):134 - 160, 2003. [15] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. 385 Journal of Machine Learning Research, 4(1):933-969(37), 2004. [16] IMDB. Formula for calculating the top rated 250 titles in imdb. http://www.imdb.com/chart/top. [17] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133 - 142, 2002. [18] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604-632, 1999. [19] R. Nallapati. Discriminative models for information retrieval. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 64 - 71,
[20] Z. Nie, Y. Ma, J.-R. Wen, and W.-Y. Ma. Object-level web information retrieval. In Technical Report of Microsoft Research, volume MSR-TR-2005-11, 2005. [21] Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma.
Object-level ranking: Bringing order to web objects.
In Proceedings of the 14th international conference on World Wide Web, pages 567 - 574, Chiba, Japan,
[22] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web.
In Technical report, Stanford Digital Libraries, 1998. [23] A. Savakis, S. Etz, and A. Loui. Evaluation of image appeal in consumer photography. In SPIE Human Vision and Electronic Imaging, pages 111-120, 2000. [24] D. Sullivan. Hitwise search engine ratings. Search Engine Watch Articles, http://searchenginewatch. com/reports/article.php/3099931, August 23, 2005. [25] S. Susstrunk and S. Winkler. Color image quality on the internet. In IS&T/SPIE Electronic Imaging 2004: Internet Imaging V, volume 5304, pages 118-131,
[26] H. Tong, M. Li, Z. H.J., J. He, and Z. C.S.

Personalization is the future of the Web, and it has achieved great success in industrial applications. For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a user"s history. Recent offerings such as My MSN, My Yahoo!,
My Google, and Google News have attracted much attention due to their potential ability to infer a user"s interests from his/her history.
One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 . These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual user"s interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems.
A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the cold start problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.
There has been much research on improving classification accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].
Another approach is using domain knowledge. Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier. The third approach is borrowing training data from other resources [5][7]. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.
One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].
In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data. A mature recommendation system usually works for millions of users. It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering. In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.
This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm. The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters. This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results.
The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.
Section 7 summarizes and offers concluding remarks.
Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970"s. The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user. The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the user"s profile using relevance feedback retrieval models (e.g. Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g. Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]). Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.
Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].
This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25]. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback. Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined. However, this is beyond the scope of this paper and thus not discussed here.
REGRESSION Assume there are M users in the system. The task of the system is to recommend documents that are relevant to each user. For each user, the system learns a user model from the user"s history. In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user. M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m.
Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth user"s jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.
The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.
Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.
Figure 1 shows the graphical representation of a Bayesian hierarchical model. In this graph, each user model is represented by a random vector wm. We assume a user model is sampled randomly from a prior distribution P(w|Φ). The system can predict the user label y of a document x given an estimation of wm (or wm"s distribution) using a function y = f(x, w). The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression. To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).
Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].
Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ. Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian. Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively. I is the K dimensional identity matrix, and a, b, and c are real numbers.
With these settings, we have the following model for the system:
respectively. 2 The first dimension of x is a dummy variable that always equals to 1.
Figure 1: Illustration of dependencies of variables in the hierarchical model. The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ).
Normal distribution: wm ∼ N(µ, Σ2 )
Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).
Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated. The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system.
If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression. Therefore, we will focus on estimating Φ. The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables.
Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w.
This kind of optimization problem is usually solved by the EM algorithm.
Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters. For space considerations, we omit the derivation in this paper since it is not the focus of our work.
E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.
However, we are estimating the posterior distribution of the variables at the E step. This avoids overfitting wm to a particular user"s data, which may be small and noisy. A detailed discussion about this subject appears in [10].
Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive. In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable. The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.
First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible. For simplicity, and as a common practice in IR, we do not model the correlation between features. Thus we approximate these matrices with K dimensional diagonal matrices. In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application. For example, let us consider a movie recommendation system, with the input variable x representing a particular movie. For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k). Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension. If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .
One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8). Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent. Intuitively, he is a good director and the weight for him (µk) should be high. Before the EM iteration, the initial value of µ is usually set to 0. Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially. Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7). It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much. This makes the convergence of the standard EM algorithm very slow.
Now let"s look at whether we can improve the learning speed of the algorithm. Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm. It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well. Thus the corresponding kth dimension of the user model"s mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.
At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7). However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user. A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs. Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs. The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices. To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.
There are two major benefits of the new algorithm. First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated. Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm.
To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB). MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to
measurement of how relevant the document representing the corresponding movie is to the user. We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user. MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users. On average, each user rated 151 movies, of these 87 were judged to be relevant. The average score for a document was 3.58. Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13]. Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).
Table 1: Data Set Statistics. On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.
Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19]. Netflix publicly provides the relevance judgments of 480,189 anonymous customers.
There are around 100 million rating on a scale of 1 to 5 for 17,770 documents. Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.
This number was reduced to 1000 customers through random sampling. The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant. The average score for documents is
Reuters Data: This is the Reuters Corpus, Volume 1. It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997. Only the first 100,000 news were used in our experiments. The Reuters corpus comes with a topic hierarchy. Each document is assigned to one of several locations on the hierarchical tree. The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C,
ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance. All the user profiles on a sub-tree are supposed to share the same prior model distribution. Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant.
We designed the experiments to answer the following three questions:
approach and learn a prior from other users?
EM algorithm for learning the Bayesian hierarchical linear model?
models?
To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models. In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration. To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better. To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.
For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative. We first evaluated the performance on each individual user, and then estimated the macro average over all users. Statistical tests (t-tests) were carried out to see whether the results are significant.
For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing. On Reuters" data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.
For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually.
Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration. Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets. This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected. However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.
Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets. This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.
Figure 4 shows that the two algorithms work similarly on the Reuters-E data set. The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration. The general patterns are very similar on other Reuters" subsets. Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.
Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set. Thus the two learning algorithms perform similarly. The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM. However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.
Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users. The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10. Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1
Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10
Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users. The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21
1
2
3
Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21
Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles. Performances on Reuters-C, Reuters-M,
Reuters-G are similar. 1 2 3 4 5
Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5
Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?
Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation. We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz). The system finished one modified EM iteration in about 4 hours. This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix.
Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.
This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.
Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a scenario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.
The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.
The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model parameters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems.
We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper. Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors.
[1] C. Basu, H. Hirsh, and W. Cohen. Recommendation as classification: Using social and content-based information in recommendation. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive algorithms for collaborative filtering. Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan. Document filtering with inference networks.
In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269,
[4] N. Cancedda, N. Cesa-Bianchi, A. Conconi,
C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M.
Renders, J. S. Taylor, and A. Vinokourov. Kernel method for document filtering. In The Eleventh Text REtrieval Conference (TREC11). National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero. Adaptation of maximum entropy capitalizer: Little data can help a lot. In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.
Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors. Language Modeling for Information Retrieval. Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin. Constructing informative prior distributions from domain knowledge in text classification. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006. ACM Press. [8] J. Delgado and N. Ishii. Memory-based weightedmajority prediction for recommender systems. In ACM SIGIR"99 Workshop on Recommender Systems, 1999. [9] GroupLens. Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman. A tutorial on learning with bayesian networks. In M. Jordan, editor, Learning in Graphical Models. Kluwer Academic, 1998. [11] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An algorithmic framework for performing collaborative filtering. In SIGIR "99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999. ACM Press. [12] T. Hofmann and J. Puzicha. Latent class models for collaborative filtering. In IJCAI "99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco,
CA, USA, 1999. Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB). Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si. An automatic weighting scheme for collaborative filtering. In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY,
USA, 2004. ACM Press. [15] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker,
L. R. Gordon, and J. Riedl. GroupLens: Applying collaborative filtering to Usenet news.
Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis. Applying support vector machines to the TREC-2001 batch filtering and routing tasks. In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu. Text classification by labeling words. In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.
Content-boosted collaborative filtering for improved recommendations. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix. Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones. Relevance weighting of search terms. In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.
Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY,
USA, 2006. ACM Press. [22] X. Wu and R. K. Srihari. Incorporating prior knowledge with weighted margin support vector machines. In Proc. ACM Knowledge Discovery Data Mining Conf.(ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness of adaptive filtering methods in a cross-benchmark evaluation. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian processes from multiple tasks. In ICML "05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY,
USA, 2005. ACM Press. [25] K. Yu, V. Tresp, and S. Yu. A nonparametric hierarchical bayesian framework for information filtering. In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.
ACM Press, 2004. [26] X. Zhu. Semi-supervised learning literature survey.
Technical report, University of Wisconsin - Madison,

Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing. Distributed information retrieval, also known as federated search [1,4,7,11,14,22] is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database. For example, Hidden Web contents (also called invisible or deep Web contents) are information on the Web that cannot be accessed by the conventional search engines.
Hidden web contents have been estimated to be 2-50 [19] times larger than the contents that can be searched by conventional search engines. Therefore, it is very important to search this type of valuable information.
The architecture of distributed search solution is highly influenced by different environmental characteristics. In a small local area network such as small company environments, the information providers may cooperate to provide corpus statistics or use the same type of search engines. Early distributed information retrieval research focused on this type of cooperative environments [1,8]. On the other side, in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required. Even if they are willing to cooperate in these environments, it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required.
Many applications fall into the latter type of uncooperative environments such as the Mind project [16] which integrates non-cooperating digital libraries or the QProber system [9] which supports browsing and searching of uncooperative hidden Web databases. In this paper, we focus mainly on uncooperative environments that contain multiple types of independent search engines.
There are three important sub-problems in distributed information retrieval. First, information about the contents of each individual database must be acquired (resource representation) [1,8,21]. Second, given a query, a set of resources must be selected to do the search (resource selection) [5,7,21]. Third, the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user (retrieval and results merging) [1,5,20,22].
Many types of solutions exist for distributed information retrieval. Invisible-web.net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics. A database recommendation system goes a step further than a browsing system like Invisible-web.net by recommending most relevant information sources to users" queries. It is composed of the resource description and the resource selection components. This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically. Distributed document retrieval is a more sophisticated task. It selects relevant information sources for users" queries as the database recommendation system does.
Furthermore, users" queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users.
The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible, which we call a high-recall goal. On the other side, the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list, which we call a high-precision goal. Prior research indicated that these two goals are related but not identical [4,21].
However, most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods [1,4,21].
This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals.
First, a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling [1]; database size statistics are also estimated [21]. A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance.
Second, after a new query is submitted, the query can be used to search the centralized sample database which produces a score for each sampled document. The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each document"s score. Then, the probabilities of relevance of all the (mostly unseen) documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates.
For the task of resource selection for a database recommendation system, the databases can be ranked by the expected number of relevant documents to meet the high-recall goal. For resource selection for a distributed document retrieval system, databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance. This selection criterion meets the high-precision goal of distributed document retrieval application. Furthermore, the Semi-supervised learning (SSL) [20,22] algorithm is applied to merge the returned documents into a final ranked list.
The unified utility framework makes very few assumptions and works in uncooperative environments. Two key features make it a more solid model for distributed information retrieval: i) It formalizes the resource selection problems of different applications as various utility functions, and optimizes the utility functions to achieve the optimal results accordingly; and ii) It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases. Specifically, the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model. The human effort (relevance judgment) required to train the single centralized logistic model does not scale with the number of databases. This is a large advantage over previous research, which required the amount of human effort to be linear with the number of databases [7,15].
The unified utility framework is not only more theoretically solid but also very effective. Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations.
The next section discusses related work. Section 3 describes the new unified utility maximization model. Section 4 explains our experimental methodology. Sections 5 and 6 present our experimental results for resource selection and document retrieval. Section 7 concludes.
There has been considerable research on all the sub-problems of distributed information retrieval. We survey the most related work in this section.
The first problem of distributed information retrieval is resource representation. The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments [8].
However, in uncooperative environments, even the databases are willing to share their information, it is not easy to judge whether the information they provide is accurate or not. Furthermore, it is not easy to coordinate the databases to provide resource representations that are compatible with each other. Thus, in uncooperative environments, one common choice is query-based sampling, which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions. As the sampled documents are selected by random queries, query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic.
Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents [1].
Many resource selection algorithms such as gGlOSS/vGlOSS [8] and CORI [1] have been proposed in the last decade. The CORI algorithm represents each database by its terms, the document frequencies and a small number of corpus statistics (details in [1]). As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms [1,17,18], we use it as a baseline algorithm in this work. The relevant document distribution estimation (ReDDE [21]) resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly. Although the ReDDE algorithm has been shown to be effective, it relies on heuristic constants that are set empirically [21].
The last step of the document retrieval sub-problem is results merging, which is the process of transforming database-specific 33 document scores into comparable database-independent document scores. The semi supervised learning (SSL) [20,22] result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific, query-specific merging models. These linear models are used to convert the database-specific document scores into the approximated centralized document scores. The SSL algorithm has been shown to be effective [22]. It serves as an important component of our unified utility maximization framework (Section 3).
In order to achieve accurate document retrieval results, many previous methods simply use resource selection algorithms that are effective of database recommendation system. But as pointed out above, a good resource selection algorithm optimized for high-recall may not work well for document retrieval, which targets the high-precision goal. This type of inconsistency has been observed in previous research [4,21].
The research in [21] tried to solve the problem with a heuristic method.
The research most similar to what we propose here is the decision-theoretic framework (DTF) [7,15]. This framework computes a selection that minimizes the overall costs (e.g., retrieval quality, time) of document retrieval system and several methods [15] have been proposed to estimate the retrieval quality. However, two points distinguish our research from the DTF model. First, the DTF is a framework designed specifically for document retrieval, but our new model integrates two distinct applications with different requirements (database recommendation and distributed document retrieval) into the same unified framework. Second, the DTF builds a model for each database to calculate the probabilities of relevance. This requires human relevance judgments for the results retrieved from each database. In contrast, our approach only builds one logistic model for the centralized sample database. The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model, thus the probabilities of relevance of documents in different databases can be estimated. This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases.
FRAMEWORK The Unified Utility Maximization (UUM) framework is based on estimating the probabilities of relevance of the (mostly unseen) documents available in the distributed search environment. In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model. We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system.
As pointed out above, the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision. In order to meet these diverse goals, the key issue is to estimate the probabilities of relevance of the documents in various databases.
This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling. Our strategy is to make full use of all the available information to calculate the probability estimates.
In the resource description step, the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method [21]. At the same time, an effective retrieval algorithm (Inquery [2]) is applied on the centralized sample database with a small number (e.g., 50) of training queries. For each training query, the CORI resource selection algorithm [1] is applied to select some number (e.g., 10) of databases and retrieve 50 document ids from each database. The SSL results merging algorithm [20,22] is used to merge the results. Then, we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database. The centralized scores are further normalized (divided by the maximum centralized score for each query), as this method has been suggested to improve estimation accuracy in previous research [15]. Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows: ( ) ))(exp(1 ))(exp( |)( _ _ dSba dSba drelPdR ccc ccc ++ + == (1) where )( _ dSc is the normalized centralized document score and ac and bc are the two parameters of the logistic model. These two parameters are estimated by maximizing the probabilities of relevance of the training queries. The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores.
When the user submits a new query, the centralized document scores of the documents in the centralized sample database are calculated. However, in order to calculate the probabilities of relevance, we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents. This goal is accomplished using: the centralized scores of the documents in the centralized sample database, and the database size statistics.
We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows: ^ _ i i i db db db samp N SF N = (2) where ^ idbN is the estimated database size and _idb sampN is the number of documents from the ith database in the centralized sample database. The intuition behind the database scale factor is that, for a database whose scale factor is 50, if one document from this database in the centralized sample database has a centralized document score of 0.5, we may guess that there are about 50 documents in that database which have scores of about
interpolation method to estimate the centralized document score curve for each database. Formally, we rank all the sampled documents from the ith database by their centralized document 34 scores to get the sampled centralized document score list {Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith database; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list, the top document in the sampled list would have rank SFdbi/2, the second document in the sampled list would rank SFdbi3/2, and so on. Therefore, the data points of sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)), (SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. Piecewise linear interpolation is applied to estimate the centralized document score curve, as illustrated in Figure 1. The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as: ],1[,)(S ^^ c idbij Njd ∈ .
It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves. However, for databases with large database scale ratios, this kind of linear interpolation may be rather inaccurate, especially for the top ranked (e.g., [1, SFdbi/2]) documents.
Therefore, an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios (e.g., larger than 100).
Specifically, a logistic model is built for each of these databases.
The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores. ))()(exp(1 ))()(exp( )( 22110 22110 ^ 1 iciicii iciicii ic dsSdsS dsSdsS dS ααα ααα +++ ++ = (3) 0iα , 1iα and 2iα are the parameters of the logistic model. For each training query, the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated. Together with the scores of the top two sampled documents, these parameters can be estimated.
After the centralized score of the top document is estimated, an exponential function is fitted for the top part ([1, SFdbi/2]) of the centralized document score curve as: ]2/,1[)*exp()( 10 ^ idbiiijc SFjjdS ∈+= ββ (4) ^
)12/( ))(log()((log( ^ 11 1 − − = idb icic i SF dSdsS β (6) The two parameters 0iβ and 1iβ are fitted to make sure the exponential function passes through the two points (1, ^ 1)( ic dS ) and (SFdbi/2, Sc(dsi1)). The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above. The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results.
From the centralized document score curves, we can estimate the complete centralized document score lists accordingly for all the available databases. After the estimated centralized document scores are normalized, the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1. Formally for the ith database, the complete list of probabilities of relevance is: ],1[,)(R ^^ idbij Njd ∈ .
In this section, we formally define the new unified utility maximization model, which optimizes the resource selection problems for two goals of high-recall (database recommendation) and high-precision (distributed document retrieval) in the same framework.
In the task of database recommendation, the system needs to decide how to rank databases. In the task of document retrieval, the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database. We generalize the database recommendation selection process, which implicitly recommends all documents in every selected database, as a special case of the selection decision for the document retrieval task. Formally, we denote di as the number of documents we would like to retrieve from the ith database and ,.....},{ 21 ddd = as a selection action for all the databases.
The database selection decision is made based on the complete lists of probabilities of relevance for all the databases. The complete lists of probabilities of relevance are inferred from all the available information specifically sR , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample; cS stands for the centralized document scores of the documents in the centralized sample database.
If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable, then the most probable complete lists of probabilities of relevance can be derived and we denote them as 1 ^ ^ * 1{(R( ), [1, ]),dbjd j Nθ = ∈ 2 ^ ^ 2(R( ), [1, ]),.......}dbjd j N∈ . Random vector   denotes an arbitrary set of complete lists of probabilities of relevance and ),|( cs SRP θ as the probability of generating this set of lists.
Finally, to each selection action d and a set of complete lists of Figure 1. Linear interpolation construction of the complete centralized document score list (database scale factor is 50). 35 probabilities of relevance θ , we associate a utility function ),( dU θ which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are θ .
Therefore, the selection decision defined by the Bayesian framework is: θθθ θ dSRPdUd cs d ).|(),(maxarg * = (7) One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation. In other words, we only need to calculate ),( * dU θ and Equation 7 is simplified as follows: ),(maxarg * * θdUd d = (8) This equation serves as the basic model for both the database recommendation system and the document retrieval system.
High-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation. The goal is to select a small set of resources (e.g., less than Nsdb databases) that contain as many relevant documents as possible, which can be formally defined as: = = i N j iji idb ddIdU ^ 1 ^ * )(R)(),( θ (9) I(di) is the indicator function, which is 1 when the ith database is selected and 0 otherwise. Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following: sdb i i i N j iji d NdItoSubject ddId idb = = = )(: )(R)(maxarg ^ 1 ^* (10) The solution of this optimization problem is very simple. We can calculate the expected number of relevant documents for each database as follows: = = idb i N j ijRd dN ^ 1 ^^ )(R (11) The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal. We call this the UUM/HR algorithm (Unified Utility Maximization for High-Recall).
High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval. It is measured by the Precision at the top part of the final merged document list. This high-precision criterion is realized by the following utility function, which measures the Precision of retrieved documents from the selected databases. = = i d j iji i ddIdU 1 ^ * )(R)(),( θ (12) Note that the key difference between Equation 12 and Equation
the documents in a database, while Equation 12 only considers a much smaller part of the ranking. Specifically, we can calculate the optimal selection decision by: = = i d j iji d i ddId 1 ^* )(R)(maxarg (13) Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem. The most common one is to select a fixed number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of documents from each selected database, formally defined as: 0, )(: )(R)(maxarg 1 ^* ≠= = = = irdoci sdb i i i d j iji d difNd NdItoSubject ddId i (14) This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each database"s complete list of probabilities of relevance: = = rdoc i N j ijRdTop dN 1 ^^ _ )(R (15) Then the databases can be ranked by these values and selected.
We call this the UUM/HP-FL algorithm (Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database).
A more complex situation is to vary the number of retrieved documents from each selected database. More specifically, we allow different selected databases to return different numbers of documents. For simplification, the result list lengths are required to be multiples of a baseline number 10. (This value can also be varied, but for simplification it is set to 10 in this paper.) This restriction is set to simulate the behavior of commercial search engines on the Web. (Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page.) This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 (more detail is discussed latterly). For further simplification, we restrict to select at most
selection optimization problem is formalized as follows: ]10..,,2,1,0[,*10 )(: )(R)(maxarg _ 1 ^* ∈= = = = = kkd Nd NdItoSubject ddId i rdocTotal i i sdb i i i d j iji d i (16) NTotal_rdoc is the total number of documents to be retrieved.
Unfortunately, there is no simple solution for this optimization problem as there are for Equations 10 and 14. However, a 36 dynamic programming algorithm can be applied to calculate the optimal solution. The basic steps of this dynamic programming method are described in Figure 2. As this algorithm allows retrieving result lists of varying lengths from each selected database, it is called UUM/HP-VL algorithm.
After the selection decisions are made, the selected databases are searched and the corresponding document ids are retrieved from each database. The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm. It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly, which is consistent with all our selection procedures where documents with higher probabilities of relevance (thus higher centralized document scores) are selected.
It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications.
The TREC Web collections WT2g or WT10g [4,13] provide a way to partition documents by different Web servers. In this way, a large number (O(1000)) of databases with rather diverse contents could be created, which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web. However, two weakness of this testbed are: i) Each database contains only a small amount of document (259 documents by average for WT2g) [4]; and ii) The contents of WT2g or WT10g are arbitrarily crawled from the Web. It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all. These types of web pages are contained in the WT2g/WT10g datasets. Therefore, the noisy Web data is not similar with that of high-quality hidden Web database contents, which are usually organized by domain experts.
Another choice is the TREC news/government data [1,15,17, 18,21]. TREC news/government data is concentrated on relatively narrow topics. Compared with TREC Web data: i) The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page, ii) A database in this testbed is larger than that of TREC Web data. By average a database contains thousands of documents, which is more realistic than a database of TREC Web data with about 250 documents. As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database, it is a good candidate to simulate the distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web sites, such as West that provides access to legal, financial and news text databases [3]. As most current distributed information retrieval systems are developed for the environments of large organizations (companies) or domainspecific hidden Web other than open domain hidden Web,
TREC news/government testbed was chosen in this work.
Trec123-100col-bysource testbed is one of the most used TREC news/government testbed [1,15,17,21]. It was chosen in this work. Three testbeds in [21] with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments.
Trec123-100col-bysource: 100 databases were created from TREC CDs 1, 2 and 3. They were organized by source and publication date [1]. The sizes of the databases are not skewed.
Details are in Table 1.
Three testbeds built in [21] were based on the trec123-100colbysource testbed. Each testbed contains many small databases and two large databases created by merging about 10-20 small databases together.
Input: Complete lists of probabilities of relevance for all the |DB| databases.
Output: Optimal selection solution for Equation 16. i) Create the three-dimensional array: Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb) Each Sel (x, y, z) is associated with a selection decision xyzd , which represents the best selection decision in the condition: only databases from number 1 to number x are considered for selection; totally y*10 documents will be retrieved; only z databases are selected out of the x database candidates. And Sel (x, y, z) is the corresponding utility value by choosing the best selection. ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the estimated relevance information of the 1st database. iii) Iterate the current database candidate i from 2 to |DB| For each entry Sel (i, y, z): Find k such that: )10,min(1: ))()1,,1((maxarg *10 ^ * yktosubject dRzkyiSelk kj ij k ≤≤ +−−−= ≤ ),,1())()1,,1(( * *10 ^ * zyiSeldRzkyiSelIf kj ij −>+−−− ≤ This means that we should retrieve *
from the ith database, otherwise we should not select this database and the previous best solution Sel (i-1, y, z) should be kept.
Then set the value of iyzd and Sel (i, y, z) accordingly. iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd and the corresponding utility value is Sel (|DB|,
NTotal_rdoc/10, Nsdb).
Figure 2. The dynamic programming optimization procedure for Equation 16.
Table1: Testbed statistics.
Number of documents Size (MB) Testbed Size (GB) Min Avg Max Min Avg Max Trec123 3.2 752 10782 39713 28 32 42 Table2: Query set statistics.
Name TREC Topic Set TREC Topic Field Average Length (Words) Trec123 51-150 Title 3.1 37 Trec123-2ldb-60col (representative): The databases in the trec123-100col-bysource were sorted with alphabetical order.
Two large databases were created by merging 20 small databases with the round-robin method. Thus, the two large databases have more relevant documents due to their large sizes, even though the densities of relevant documents are roughly the same as the small databases.
Trec123-AP-WSJ-60col (relevant): The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall. The other 60 collections were left unchanged. The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases. Thus, the two large databases have many more relevant documents than the small databases.
Trec123-FR-DOE-81col (nonrelevant): The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall. The other 80 collections were left unchanged. The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases, even though they are much larger.
51-150. The queries 101-150 were used as training queries and the queries 51-100 were used as test queries (details in Table 2).
In the uncooperative distributed information retrieval environments of large organizations (companies) or domainspecific hidden Web, different databases may use different types of search engine. To simulate the multiple type-engine environment, three different types of search engines were used in the experiments: INQUERY [2], a unigram statistical language model with linear smoothing [12,20] and a TFIDF retrieval algorithm with ltc weight [12,20]. All these algorithms were implemented with the Lemur toolkit [12].
These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner.
DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.
The resource descriptions were created using query-based sampling. About 80 queries were sent to each database to download 300 unique documents. The database size statistics were estimated by the sample-resample method [21]. Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1). Another 50 queries (51-100) were used as test data.
Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21]. Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm. And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.
The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR. The UUM/HR algorithm is described in Section 3.3. It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm. The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds. This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.
It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.
We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.
More training data or a more sophisticated model may help to solve this minor puzzle.
Collections Selected. Collections Selected.
Trec123-100Col Testbed. Representative Testbed.
Collection Selected. Collection Selected.
Relevant Testbed. Nonrelevant Testbed.
Figure 3. Resource selection experiments on the four testbeds. 38
EFFECTIVENESS For document retrieval, the selected databases are searched and the returned results are merged into a single final list. In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm. This version of the SSL algorithm [22] is allowed to download a small number of returned document texts on the fly to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores. It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database [22]. This is a common scenario in operational environments and was the case for our experiments.
Document retrieval effectiveness was measured by Precision at the top part of the final document list. The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms, namely the CORI,
ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.
The last three algorithms were proposed in Section 3. All the first four algorithms selected 3 or 5 databases, and 50 documents were retrieved from each selected database. The UUM/HP-FL algorithm also selected 3 or 5 databases, but it was allowed to adjust the number of documents to retrieve from each selected database; the number retrieved was constrained to be from 10 to 100, and a multiple of 10.
The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quite Table 5. Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
Table 6. Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
Table 3. Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
Table 4. Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
39 a lot worse than the other algorithms. Tables 3 and 4 show the results on the Trec123-100Col testbed, and Tables 5 and 6 show the results on the representative testbed.
On the Trec123-100Col testbed, the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms (Tables 3 and 4).
The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms. One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before: The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves, while the ReDDE algorithm [21] uses a heuristic method, assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves. This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance. Therefore, the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval.
The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do.
It can be seen that on this testbed, the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms. This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16.
On the representative testbed, CORI is much less effective than other algorithms for distributed document retrieval (Tables 5 and 6). The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm. On this testbed the three UUM algorithms are about equally effective. Detailed analysis shows that the overlap of the selected databases between the UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed, since all of them tend to select the two large databases. This explains why they are about equally effective for document retrieval.
In real operational environments, databases may return no document scores and report only ranked lists of results. As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance, it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores. The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks (In a ranked list of 50 documents, the first one has a score of 1, the second has a score of 0.98 etc) ,which has been studied in [22]. The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7. The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids. It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives. Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms.
The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval. The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments.
Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets. Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application. We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical. This kind of inconsistency has also been observed in previous work, but the prior solutions either used heuristic methods or assumed cooperation by individual databases (e.g., all the databases used the same kind of search engines), which is frequently not true in the uncooperative environment.
In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework. In this framework, the selection decisions are obtained by optimizing different objective functions. As far as we know, this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner.
The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database. A single logistic model was trained on the centralized Table 7. Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores) Precision at Doc Rank CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores, while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model.
Therefore, the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment, which is much more efficient than previous methods that build a separate model for each database.
This framework is not only more theoretically solid but also very effective. One algorithm for resource selection (UUM/HR) and two algorithms for document retrieval (UUM/HP-FL and UUM/HP-VL) are derived from this framework. Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations (companies) or domain-specific hidden Web. Furthermore, the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores. Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art, and sometimes considerably better. Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks.
The unified utility maximization framework is open for different extensions. When cost is associated with searching the online databases, the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs. Another extension of the framework is to consider the retrieval effectiveness of the online databases, which is an important issue in the operational environments. All of these are the directions of future research.
ACKNOWLEDGEMENT This research was supported by NSF grants EIA-9983253 and IIS-0118767. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors", and do not necessarily reflect those of the sponsor.
REFERENCES [1] J. Callan. (2000). Distributed information retrieval. In W.B.
Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers. (pp. 127-150). [2] J. Callan, W.B. Croft, and J. Broglio. (1995). TREC and TIPSTER experiments with INQUERY. Information Processing and Management, 31(3). (pp. 327-343). [3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou. (2002). Database selection using actual physical and acquired logical collection resources in a massive domainspecific operational environment. Distributed search over the hidden web: Hierarchical database sampling and selection. In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [4] N. Craswell. (2000). Methods for distributed information retrieval. Ph. D. thesis, The Australian Nation University. [5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).
Merging results from isolated search engines. In Proceedings of 10th Australasian Database Conference. [6] D. D'Souza, J. Thom, and J. Zobel. (2000). A comparison of techniques for selecting text collections. In Proceedings of the 11th Australasian Database Conference. [7] N. Fuhr. (1999). A Decision-Theoretic approach to database selection in networked IR. ACM Transactions on Information Systems, 17(3). (pp. 229-249). [8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke. (1997). STARTS: Stanford proposal for internet metasearching. In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data. [9] L. Gravano, P. Ipeirotis and M. Sahami. (2003). QProber: A System for Automatic Classification of Hidden-Web Databases. ACM Transactions on Information Systems, 21(1). [10] P. Ipeirotis and L. Gravano. (2002). Distributed search over the hidden web: Hierarchical database sampling and selection. In Proceedings of the 28th International Conference on Very Large Databases (VLDB). [11] InvisibleWeb.com. http://www.invisibleweb.com [12] The lemur toolkit. http://www.cs.cmu.edu/~lemur [13] J. Lu and J. Callan. (2003). Content-based information retrieval in peer-to-peer networks. In Proceedings of the 12th International Conference on Information and Knowledge Management. [14] W. Meng, C.T. Yu and K.L. Liu. (2002) Building efficient and effective metasearch engines. ACM Comput. Surv. 34(1). [15] H. Nottelmann and N. Fuhr. (2003). Evaluating different method of estimating retrieval quality for resource selection. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. [16] H., Nottelmann and N., Fuhr. (2003). The MIND architecture for heterogeneous multimedia federated digital libraries. ACM SIGIR 2003 Workshop on Distributed Information Retrieval. [17] A.L. Powell, J.C. French, J. Callan, M. Connell, and C.L.

Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26]. Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.
Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task.
There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper.
General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint.
We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.
In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.
In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.
Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).
We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.
The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.
Section 6 gives our experimental results. We make concluding remarks in section 7.
Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.
The proposed methods fall into two categories: the rule based approach and the machine learning based approach.
Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript. They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes. Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies. Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.
The rule-based approach can achieve high performance. However, it also has disadvantages. It is less adaptive and robust when compared with the machine learning approach.
Han et al. [10], for instance, conducted metadata extraction with the machine learning approach. They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier. They mainly used linguistic information as features.
They reported high extraction accuracy from research papers in terms of precision and recall.
Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.
Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19].
Title information is useful for document retrieval.
In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].
In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as ‘titles" of the pages [5]. Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].
To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146
SETTING We consider the issue of automatically extracting titles from general documents.
By general documents, we mean documents that belong to one of any number of specific genres. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.
General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.
Figure 1 shows an estimate on distributions of file formats on intranet and internet [15]. Office and PDF are the main file formats on the intranet. Even on the internet, the documents in the formats are still not negligible, given its extremely large size. In this paper, without loss of generality, we take Office documents as an example.
Figure 1. Distributions of file formats in internet and intranet.
For Office documents, users can define titles as file properties using a feature provided by Office. We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate. That is to say, titles in file properties are usually inconsistent with the ‘true" titles in the file bodies that are created by the authors and are visible to readers.
We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details). A number of reasons can be considered. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.
In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google. We found that nearly all the titles presented in the search results were from the file properties of the documents. However, only 0.272 of them were correct.
Actually, ‘true" titles usually exist at the beginnings of the bodies of documents. If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing. This is exactly the problem we address in this paper.
More specifically, given a Word document, we are to extract the title from the top region of the first page. Given a PowerPoint document, we are to extract the title from the first slide. A title sometimes consists of a main title and one or two subtitles. We only consider extraction of the main title.
As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.
Figure 2. Title extraction from Word document.
Figure 3. Title extraction from PowerPoint document.
Next, we define a ‘specification" for human judgments in title data annotation. The annotated data will be used in training and testing of the title extraction methods.
Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. However, there are many cases in which the identification is not easy. There are some rules defined in the specification that guide identification for such cases. The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like ‘draft", 147 ‘whitepaper", etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.) Figures 2 and 3 show examples of Office documents from which we conduct title extraction. In Figure 2, ‘Differences in Win32 API Implementations among Windows Operating Systems" is the title of the Word document. ‘Microsoft Windows" on the top of this page is a picture and thus is ignored. In Figure 3, ‘Building Competitive Advantages through an Agile Infrastructure" is the title of the PowerPoint document.
We have developed a tool for annotation of titles by human annotators. Figure 4 shows a snapshot of the tool.
Figure 4. Title annotation tool.
Title extraction based on machine learning consists of training and extraction. The same pre-processing step occurs before training and extraction.
During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. If a line (lines are separated by ‘return" symbols) only has a single format, then the line will become a unit. If a line has several parts and each of them has its own format, then each part will become a unit. Each unit will be treated as an instance in learning. A unit contains not only content information (linguistic information) but also formatting information. The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).
Figure 5 shows the units obtained from the document in Figure 2.
Figure 5. Example of units.
In learning, the input is sequences of units where each sequence corresponds to a document. We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other. We employ four types of models: Perceptron,
Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).
In extraction, the input is a sequence of units from one document.
We employ one type of model to identify whether a unit is title_begin, title_end, or other. We then extract units from the unit labeled with ‘title_begin" to the unit labeled with ‘title_end". The result is the extracted title of the document.
The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction. This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers.
The four models actually can be considered in the same metadata extraction framework. That is why we apply them together to our current problem.
Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ). Recall that an instance here represents a unit. A label represents title_begin, title_end, or other. Here, k is the number of units in a document.
In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).
Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. Metadata extraction model.
We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.
We train the classifiers locally using the labeled data. As the classifier, we employ the Perceptron or Maximum Entropy model.
We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers. However, the classifiers are conditioned on the previous label. When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively. That is to say, the two models are more precise.
In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.
For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. Specifically, we first identify the most likely title_begin. Then we find the most likely title_end within three units after the title_begin. Finally, we extract as a title the units between the title_begin and the title_end.
For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.
In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13]. This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.
We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2]. In addition, in training, the parameters of the model are updated globally rather than locally.
There are two types of features: format features and linguistic features. We mainly use the former. The features are used for both the title-begin and the title-end classifiers.
Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).
If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0. If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0. If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.
It is necessary to conduct normalization on font sizes. For example, in one document the largest font size might be ‘12pt", while in another the smallest one might be ‘18pt".
Boldface: This binary feature represents whether or not the current unit is in boldface.
Alignment: There are four binary features that respectively represent the location of the current unit: ‘left", ‘center", ‘right", and ‘unknown alignment".
The following format features with respect to ‘context" play an important role in title extraction.
Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.
Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.
Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.
Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit.
The linguistic features are based on key words.
Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. The positive words include ‘title:", ‘subject:", ‘subject line:" For example, in some documents the lines of titles and authors have the same formats. However, if lines begin with one of the positive words, then it is likely that they are title lines.
Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words. The negative words include ‘To", ‘By", ‘created by", ‘updated by", etc.
There are more negative words than positive words. The above linguistic features are language dependent.
Word Count: A title should not be too long. We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval. If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.
Ending Character: This feature represents whether the unit ends with ‘:", ‘-", or other special characters. A title usually does not end with such a character.
We describe our method of document retrieval using extracted titles.
Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. A ranking function in search can use different weights for different fields of 149 the document. Also, titles are typically assigned high weights, indicating that they are important for document retrieval. As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document. By doing this, we attempt to improve the overall precision.
In this paper, we employ a modification of BM25 that allows field weighting [21]. As fields, we make use of body, title, extracted title and anchor. First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field. Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk . Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was
We used two data sets in our experiments.
First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft. We call it MS hereafter.
Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.
Figure 7 shows the distributions of the genres of the documents.
We see that the documents are indeed ‘general documents" as we define them.
Figure 7. Distributions of document genres.
Third, a data set in Chinese was also downloaded from the internet.
It includes 500 Word documents and 500 PowerPoint documents in Chinese.
We manually labeled the titles of all the documents, on the basis of our specification.
Not all the documents in the two data sets have titles. Table 1 shows the percentages of the documents having titles. We see that DotCom and DotGov have more PowerPoint documents with titles than MS. This might be because PowerPoint documents published on the internet are more formal than those on the intranet.
Table 1. The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.
Table 2. Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D
We test the accuracies of the two baselines described in section
respectively.
We investigate how many titles in the file properties of the documents are reliable. We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles. We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation). This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).
Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3. Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181
We conducted title extraction from the first data set (Word and PowerPoint in MS). As the model, we used Perceptron.
We conduct 4-fold cross validation. Thus, all the results reported here are those averaged over 4 trials. Tables 4 and 5 show the results. We see that Perceptron significantly outperforms the baselines. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.
Table 4. Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5. Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction. For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines. For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.
We conduct significance tests. The results are shown in Table 6.
Here, ‘Largest" denotes the baseline of using the largest font size, ‘First" denotes the baseline of using the first line. The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6. Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction. However, it is also obvious that using only these two features is not enough. There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like ‘Confidential", ‘White paper", etc. For those cases, the ‘largest font size" method cannot work well. For similar reasons, the ‘first line" method alone cannot work well, either. With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.
We investigate the performance of solely using linguistic features.
We found that it does not work well. It seems that the format features play important roles and the linguistic features are supplements..
Figure 8. An example Word document.
Figure 9. An example PowerPoint document.
We conducted an error analysis on the results of Perceptron. We found that the errors fell into three categories. (1) About one third of the errors were related to ‘hard cases". In these documents, the layouts of the first pages were difficult to understand, even for humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets. Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3). Confusions between main titles and subtitles were another type of error. Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect. This type of error does little harm to document processing like search, however.
To compare the performance of different machine learning models, we conducted another experiment. Again, we perform 4-fold cross 151 validation on the first data set (MS). Table 7, 8 shows the results of all the four models.
It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. In general, the Markovian models perform better than or as well as their classifier counterparts. This seems to be because the Markovian models are trained globally, while the classifiers are trained locally. The Perceptron based models perform better than the ME based counterparts. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.
Table 7. Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8. Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759
We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov). Tables 9-12 show the results.
Table 9. Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10. Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11. Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12. Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well. There is almost no drop in accuracy. The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information.
We apply the model trained with the data in English (MS) to the data set in Chinese.
Tables 13-14 show the results.
Table 13. Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.
There are only small drops in accuracy. Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible. The results indicate that the patterns of title formats exist across different languages.
From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents.
We performed experiments on using title extraction for document retrieval. As a baseline, we employed BM25 without using extracted titles. The ranking mechanism was as described in Section 5. The weights were heuristically set. We did not conduct optimization on the weights.
The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranet"s search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results. In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0
P@10 P@5 Reciprocal P@10 P@5 Reciprocal
BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10. Search ranking results.
Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.
With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval.
In this paper, we have investigated the problem of automatically extracting titles from general documents. We have tried using a machine learning approach to address the problem.
Previous work showed that the machine learning approach can work well for metadata extraction from research papers. In this paper, we showed that the approach can work for extraction from general documents as well. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information. It appeared that using formatting information is a key for successfully conducting title extraction from general documents.
We tried different machine learning models including Perceptron,
Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron. We found that the performance of the Perceptorn models was the best. We applied models constructed in one domain to another domain and applied models trained in one language to another language. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic. We also attempted to use the extracted titles in document retrieval. We observed a significant improvement in document ranking performance for search when using extracted title information. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach.
We thank Chunyu Wei and Bojuan Zhao for their work on data annotation. We acknowledge Jinzhu Li for his assistance in conducting the experiments. We thank Ming Zhou, John Chen,
Jun Xu, and the anonymous reviewers of JCDL"05 for their valuable comments on this paper.
[1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A maximum entropy approach to natural language processing.
Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.
In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H.,
Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files. In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A. Automatic document metadata extraction using support vector machines. In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48,
[11] Kobayashi, M., and Takeda, K. Information retrieval on the Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289,
[13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.
In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S.,
Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J. Automatic Metadata generation & evaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A. Effective enterprise information retrieval across new content formats. In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html,
[16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials. In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.
In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.

Over the past decade, the Web has grown exponentially in size.
Unfortunately, this growth has not been isolated to good-quality pages. The number of incorrect, spamming, and malicious (e.g., phishing) sites has also grown rapidly. The sheer number of both good and bad pages on the Web has led to an increasing reliance on search engines for the discovery of useful information. Users rely on search engines not only to return pages related to their search query, but also to separate the good from the bad, and order results so that the best pages are suggested first.
To date, most work on Web page ranking has focused on improving the ordering of the results returned to the user (querydependent ranking, or dynamic ranking). However, having a good query-independent ranking (static ranking) is also crucially important for a search engine. A good static ranking algorithm provides numerous benefits: • Relevance: The static rank of a page provides a general indicator to the overall quality of the page. This is a useful input to the dynamic ranking algorithm. • Efficiency: Typically, the search engine"s index is ordered by static rank. By traversing the index from highquality to low-quality pages, the dynamic ranker may abort the search when it determines that no later page will have as high of a dynamic rank as those already found. The more accurate the static rank, the better this early-stopping ability, and hence the quicker the search engine may respond to queries. • Crawl Priority: The Web grows and changes as quickly as search engines can crawl it. Search engines need a way to prioritize their crawl-to determine which pages to recrawl, how frequently, and how often to seek out new pages. Among other factors, the static rank of a page is used to determine this prioritization. A better static rank thus provides the engine with a higher quality, more upto-date index.
Google is often regarded as the first commercially successful search engine. Their ranking was originally based on the PageRank algorithm [5][27]. Due to this (and possibly due to Google"s promotion of PageRank to the public), PageRank is widely regarded as the best method for the static ranking of Web pages.
Though PageRank has historically been thought to perform quite well, there has yet been little academic evidence to support this claim. Even worse, there has recently been work showing that PageRank may not perform any better than other simple measures on certain tasks. Upstill et al. have found that for the task of finding home pages, the number of pages linking to a page and the type of URL were as, or more, effective than PageRank [32]. They found similar results for the task of finding high quality companies [31]. PageRank has also been used in systems for TREC"s very large collection and Web track competitions, but with much less success than had been expected [17]. Finally,
Amento et al. [1] found that simple features, such as the number of pages on a site, performed as well as PageRank.
Despite these, the general belief remains among many, both academic and in the public, that PageRank is an essential factor for a good static rank. Failing this, it is still assumed that using the link structure is crucial, in the form of the number of inlinks or the amount of anchor text.
In this paper, we show there are a number of simple url- or pagebased features that significantly outperform PageRank (for the purposes of statically ranking Web pages) despite ignoring the structure of the Web. We combine these and other static features using machine learning to achieve a ranking system that is significantly better than PageRank (in pairwise agreement with human labels).
A machine learning approach for static ranking has other advantages besides the quality of the ranking. Because the measure consists of many features, it is harder for malicious users to manipulate it (i.e., to raise their page"s static rank to an undeserved level through questionable techniques, also known as Web spamming). This is particularly true if the feature set is not known. In contrast, a single measure like PageRank can be easier to manipulate because spammers need only concentrate on one goal: how to cause more pages to point to their page. With an algorithm that learns, a feature that becomes unusable due to spammer manipulation will simply be reduced or removed from the final computation of rank. This flexibility allows a ranking system to rapidly react to new spamming techniques.
A machine learning approach to static ranking is also able to take advantage of any advances in the machine learning field. For example, recent work on adversarial classification [12] suggests that it may be possible to explicitly model the Web page spammer"s (the adversary) actions, adjusting the ranking model in advance of the spammer"s attempts to circumvent it. Another example is the elimination of outliers in constructing the model, which helps reduce the effect that unique sites may have on the overall quality of the static rank. By moving static ranking to a machine learning framework, we not only gain in accuracy, but also gain in the ability to react to spammer"s actions, to rapidly add new features to the ranking algorithm, and to leverage advances in the rapidly growing field of machine learning.
Finally, we believe there will be significant advantages to using this technique for other domains, such as searching a local hard drive or a corporation"s intranet. These are domains where the link structure is particularly weak (or non-existent), but there are other domain-specific features that could be just as powerful. For example, the author of an intranet page and his/her position in the organization (e.g., CEO, manager, or developer) could provide significant clues as to the importance of that page. A machine learning approach thus allows rapid development of a good static algorithm in new domains.
This paper"s contribution is a systematic study of static features, including PageRank, for the purposes of (statically) ranking Web pages. Previous studies on PageRank typically used subsets of the Web that are significantly smaller (e.g., the TREC VLC2 corpus, used by many, contains only 19 million pages). Also, the performance of PageRank and other static features has typically been evaluated in the context of a complete system for dynamic ranking, or for other tasks such as question answering. In contrast, we explore the use of PageRank and other features for the direct task of statically ranking Web pages.
We first briefly describe the PageRank algorithm. In Section 3 we introduce RankNet, the machine learning technique used to combine static features into a final ranking. Section 4 describes the static features. The heart of the paper is in Section 5, which presents our experiments and results. We conclude with a discussion of related and future work.
The basic idea behind PageRank is simple: a link from a Web page to another can be seen as an endorsement of that page. In general, links are made by people. As such, they are indicative of the quality of the pages to which they point - when creating a page, an author presumably chooses to link to pages deemed to be of good quality. We can take advantage of this linkage information to order Web pages according to their perceived quality.
Imagine a Web surfer who jumps from Web page to Web page, choosing with uniform probability which link to follow at each step. In order to reduce the effect of dead-ends or endless cycles the surfer will occasionally jump to a random page with some small probability α, or when on a page with no out-links. If averaged over a sufficient number of steps, the probability the surfer is on page j at some point in time is given by the formula: ∑∈ + − = ji i iP N jP B F )()1( )( α α (1) Where Fi is the set of pages that page i links to, and Bj is the set of pages that link to page j. The PageRank score for node j is defined as this probability: PR(j)=P(j). Because equation (1) is recursive, it must be iteratively evaluated until P(j) converges (typically, the initial distribution for P(j) is uniform). The intuition is, because a random surfer would end up at the page more frequently, it is likely a better page. An alternative view for equation (1) is that each page is assigned a quality, P(j). A page gives an equal share of its quality to each page it points to.
PageRank is computationally expensive. Our collection of 5 billion pages contains approximately 370 billion links. Computing PageRank requires iterating over these billions of links multiple times (until convergence). It requires large amounts of memory (or very smart caching schemes that slow the computation down even further), and if spread across multiple machines, requires significant communication between them. Though much work has been done on optimizing the PageRank computation (see e.g., [25] and [6]), it remains a relatively slow, computationally expensive property to compute.
Much work in machine learning has been done on the problems of classification and regression. Let X={xi} be a collection of feature vectors (typically, a feature is any real valued number), and Y={yi} be a collection of associated classes, where yi is the class of the object described by feature vector xi. The classification problem is to learn a function f that maps yi=f(xi), for all i. When yi is real-valued as well, this is called regression.
Static ranking can be seen as a regression problem. If we let xi represent features of page i, and yi be a value (say, the rank) for each page, we could learn a regression function that mapped each page"s features to their rank. However, this over-constrains the problem we wish to solve. All we really care about is the order of the pages, not the actual value assigned to them.
Recent work on this ranking problem [7][13][18] directly attempts to optimize the ordering of the objects, rather than the value assigned to them. For these, let Z={<i,j>} be a collection of pairs of items, where item i should be assigned a higher value than item j. The goal of the ranking problem, then, is to learn a function f such that, )()(,, ji ffji xxZ >∈∀ 708 Note that, as with learning a regression function, the result of this process is a function (f) that maps feature vectors to real values.
This function can still be applied anywhere that a regressionlearned function could be applied. The only difference is the technique used to learn the function. By directly optimizing the ordering of objects, these methods are able to learn a function that does a better job of ranking than do regression techniques.
We used RankNet [7], one of the aforementioned techniques for learning ranking functions, to learn our static rank function.
RankNet is a straightforward modification to the standard neural network back-prop algorithm. As with back-prop, RankNet attempts to minimize the value of a cost function by adjusting each weight in the network according to the gradient of the cost function with respect to that weight. The difference is that, while a typical neural network cost function is based on the difference between the network output and the desired output, the RankNet cost function is based on the difference between a pair of network outputs. That is, for each pair of feature vectors <i,j> in the training set, RankNet computes the network outputs oi and oj.
Since vector i is supposed to be ranked higher than vector j, the larger is oj-oi, the larger the cost.
RankNet also allows the pairs in Z to be weighted with a confidence (posed as the probability that the pair satisfies the ordering induced by the ranking function). In this paper, we used a probability of one for all pairs. In the next section, we will discuss the features used in our feature vectors, xi.
To apply RankNet (or other machine learning techniques) to the ranking problem, we needed to extract a set of features from each page. We divided our feature set into four, mutually exclusive, categories: page-level (Page), domain-level (Domain), anchor text and inlinks (Anchor), and popularity (Popularity). We also optionally used the PageRank of a page as a feature. Below, we describe each of these feature categories in more detail.
PageRank We computed PageRank on a Web graph of 5 billion crawled pages (and 20 billion known URLs linked to by these pages).
This represents a significant portion of the Web, and is approximately the same number of pages as are used by Google, Yahoo, and MSN for their search engines.
Because PageRank is a graph-based algorithm, it is important that it be run on as large a subset of the Web as possible. Most previous studies on PageRank used subsets of the Web that are significantly smaller (e.g. the TREC VLC2 corpus, used by many, contains only 19 million pages) We computed PageRank using the standard value of 0.85 for α.
Popularity Another feature we used is the actual popularity of a Web page, measured as the number of times that it has been visited by users over some period of time. We have access to such data from users who have installed the MSN toolbar and have opted to provide it to MSN. The data is aggregated into a count, for each Web page, of the number of users who viewed that page.
Though popularity data is generally unavailable, there are two other sources for it. The first is from proxy logs. For example, a university that requires its students to use a proxy has a record of all the pages they have visited while on campus.
Unfortunately, proxy data is quite biased and relatively small.
Another source, internal to search engines, are records of which results their users clicked on. Such data was used by the search engine Direct Hit, and has recently been explored for dynamic ranking purposes [20]. An advantage of the toolbar data over this is that it contains information about URL visits that are not just the result of a search.
The raw popularity is processed into a number of features such as the number of times a page was viewed and the number of times any page in the domain was viewed. More details are provided in section 5.5.
Anchor text and inlinks These features are based on the information associated with links to the page in question. It includes features such as the total amount of text in links pointing to the page (anchor text), the number of unique words in that text, etc.
Page This category consists of features which may be determined by looking at the page (and its URL) alone. We used only eight, simple features such as the number of words in the body, the frequency of the most common term, etc.
Domain This category contains features that are computed as averages across all pages in the domain. For example, the average number of outlinks on any page and the average PageRank.
Many of these features have been used by others for ranking Web pages, particularly the anchor and page features. As mentioned, the evaluation is typically for dynamic ranking, and we wish to evaluate the use of them for static ranking. Also, to our knowledge, this is the first study on the use of actual page visitation popularity for static ranking. The closest similar work is on using click-through behavior (that is, which search engine results the users click on) to affect dynamic ranking (see e.g., [20]).
Because we use a wide variety of features to come up with a static ranking, we refer to this as fRank (for feature-based ranking). fRank uses RankNet and the set of features described in this section to learn a ranking function for Web pages. Unless otherwise specified, fRank was trained with all of the features.
In this section, we will demonstrate that we can out perform PageRank by applying machine learning to a straightforward set of features. Before the results, we first discuss the data, the performance metric, and the training method.
In order to evaluate the quality of a static ranking, we needed a gold standard defining the correct ordering for a set of pages.
For this, we employed a dataset which contains human judgments for 28000 queries. For each query, a number of results are manually assigned a rating, from 0 to 4, by human judges. The rating is meant to be a measure of how relevant the result is for the query, where 0 means poor and 4 means excellent. There are approximately 500k judgments in all, or an average of 18 ratings per query.
The queries are selected by randomly choosing queries from among those issued to the MSN search engine. The probability that a query is selected is proportional to its frequency among all 709 of the queries. As a result, common queries are more likely to be judged than uncommon queries. As an example of how diverse the queries are, the first four queries in the training set are chef schools, chicagoland speedway, eagles fan club, and Turkish culture. The documents selected for judging are those that we expected would, on average, be reasonably relevant (for example, the top ten documents returned by MSN"s search engine). This provides significantly more information than randomly selecting documents on the Web, the vast majority of which would be irrelevant to a given query.
Because of this process, the judged pages tend to be of higher quality than the average page on the Web, and tend to be pages that will be returned for common search queries. This bias is good when evaluating the quality of static ranking for the purposes of index ordering and returning relevant documents. This is because the most important portion of the index to be well-ordered and relevant is the portion that is frequently returned for search queries. Because of this bias, however, the results in this paper are not applicable to crawl prioritization. In order to obtain experimental results on crawl prioritization, we would need ratings on a random sample of Web pages.
To convert the data from query-dependent to query-independent, we simply removed the query, taking the maximum over judgments for a URL that appears in more than one query. The reasoning behind this is that a page that is relevant for some query and irrelevant for another is probably a decent page and should have a high static rank. Because we evaluated the pages on queries that occur frequently, our data indicates the correct index ordering, and assigns high value to pages that are likely to be relevant to a common query.
We randomly assigned queries to a training, validation, or test set, such that they contained 84%, 8%, and 8% of the queries, respectively. Each set contains all of the ratings for a given query, and no query appears in more than one set. The training set was used to train fRank. The validation set was used to select the model that had the highest performance. The test set was used for the final results.
This data gives us a query-independent ordering of pages. The goal for a static ranking algorithm will be to reproduce this ordering as closely as possible. In the next section, we describe the measure we used to evaluate this.
We chose to use pairwise accuracy to evaluate the quality of a static ranking. The pairwise accuracy is the fraction of time that the ranking algorithm and human judges agree on the ordering of a pair of Web pages.
If S(x) is the static ranking assigned to page x, and H(x) is the human judgment of relevance for x, then consider the following sets: )}()(:,{ yHxHyx >=pH and )}()(:,{ ySxSyx >=pS The pairwise accuracy is the portion of Hp that is also contained in Sp: p pp H SH ∩ =accuracypairwise This measure was chosen for two reasons. First, the discrete human judgments provide only a partial ordering over Web pages, making it difficult to apply a measure such as the Spearman rank order correlation coefficient (in the pairwise accuracy measure, a pair of documents with the same human judgment does not affect the score). Second, the pairwise accuracy has an intuitive meaning: it is the fraction of pairs of documents that, when the humans claim one is better than the other, the static rank algorithm orders them correctly.
We trained fRank (a RankNet based neural network) using the following parameters. We used a fully connected 2 layer network.
The hidden layer had 10 hidden nodes. The input weights to this layer were all initialized to be zero. The output layer (just a single node) weights were initialized using a uniform random distribution in the range [-0.1, 0.1]. We used tanh as the transfer function from the inputs to the hidden layer, and a linear function from the hidden layer to the output. The cost function is the pairwise cross entropy cost function as discussed in section 3.
The features in the training set were normalized to have zero mean and unit standard deviation. The same linear transformation was then applied to the features in the validation and test sets.
For training, we presented the network with 5 million pairings of pages, where one page had a higher rating than the other. The pairings were chosen uniformly at random (with replacement) from all possible pairings. When forming the pairs, we ignored the magnitude of the difference between the ratings (the rating spread) for the two URLs. Hence, the weight for each pair was constant (one), and the probability of a pair being selected was independent of its rating spread.
We trained the network for 30 epochs. On each epoch, the training pairs were randomly shuffled. The initial training rate was
the error had increased, then we decreased the training rate, under the hypothesis that the network had probably overshot. The training rate at each epoch was thus set to: Training rate = 1+ε κ Where κ is the initial rate (0.001), and ε is the number of times the training set error has increased. After each epoch, we measured the performance of the neural network on the validation set, using 1 million pairs (chosen randomly with replacement).
The network with the highest pairwise accuracy on the validation set was selected, and then tested on the test set. We report the pairwise accuracy on the test set, calculated using all possible pairs.
These parameters were determined and fixed before the static rank experiments in this paper. In particular, the choice of initial training rate, number of epochs, and training rate decay function were taken directly from Burges et al [7].
Though we had the option of preprocessing any of the features before they were input to the neural network, we refrained from doing so on most of them. The only exception was the popularity features. As with most Web phenomenon, we found that the distribution of site popularity is Zipfian. To reduce the dynamic range, and hopefully make the feature more useful, we presented the network with both the unpreprocessed, as well as the logarithm, of the popularity features (As with the others, the logarithmic feature values were also normalized to have zero mean and unit standard deviation). 710 Applying fRank to a document is computationally efficient, taking time that is only linear in the number of input features; it is thus within a constant factor of other simple machine learning methods such as naïve Bayes. In our experiments, computing the fRank for all five billion Web pages was approximately 100 times faster than computing the PageRank for the same set.
As Table 1 shows, fRank significantly outperforms PageRank for the purposes of static ranking. With a pairwise accuracy of 67.4%, fRank more than doubles the accuracy of PageRank (relative to the baseline of 50%, which is the accuracy that would be achieved by a random ordering of Web pages). Note that one of fRank"s input features is the PageRank of the page, so we would expect it to perform no worse than PageRank. The significant increase in accuracy implies that the other features (anchor, popularity, etc.) do in fact contain useful information regarding the overall quality of a page.
Table 1: Basic Results Technique Accuracy (%) None (Baseline) 50.00 PageRank 56.70 fRank 67.43 There are a number of decisions that go into the computation of PageRank, such as how to deal with pages that have no outlinks, the choice of α, numeric precision, convergence threshold, etc.
We were able to obtain a computation of PageRank from a completely independent implementation (provided by Marc Najork) that varied somewhat in these parameters. It achieved a pairwise accuracy of 56.52%, nearly identical to that obtained by our implementation. We thus concluded that the quality of the PageRank is not sensitive to these minor variations in algorithm, nor was PageRank"s low accuracy due to problems with our implementation of it.
We also wanted to find how well each feature set performed. To answer this, for each feature set, we trained and tested fRank using only that set of features. The results are shown in Table 2.
As can be seen, every single feature set individually outperformed PageRank on this test. Perhaps the most interesting result is that the Page-level features had the highest performance out of all the feature sets. This is surprising because these are features that do not depend on the overall graph structure of the Web, nor even on what pages point to a given page. This is contrary to the common belief that the Web graph structure is the key to finding a good static ranking of Web pages.
Table 2: Results for individual feature sets.
Feature Set Accuracy (%) PageRank 56.70 Popularity 60.82 Anchor 59.09 Page 63.93 Domain 59.03 All Features 67.43 Because we are using a two-layer neural network, the features in the learned network can interact with each other in interesting, nonlinear ways. This means that a particular feature that appears to have little value in isolation could actually be very important when used in combination with other features. To measure the final contribution of a feature set, in the context of all the other features, we performed an ablation study. That is, for each set of features, we trained a network to contain all of the features except that set. We then compared the performance of the resulting network to the performance of the network with all of the features.
Table 3 shows the results of this experiment, where the decrease in accuracy is the difference in pairwise accuracy between the network trained with all of the features, and the network missing the given feature set.
Table 3: Ablation study. Shown is the decrease in accuracy when we train a network that has all but the given set of features. The last line is shows the effect of removing the anchor, PageRank, and domain features, hence a model containing no network or link-based information whatsoever.
Feature Set Decrease in Accuracy PageRank 0.18 Popularity 0.78 Anchor 0.47 Page 5.42 Domain Anchor, PageRank & Domain
The results of the ablation study are consistent with the individual feature set study. Both show that the most important feature set is the Page-level feature set, and the second most important is the popularity feature set.
Finally, we wished to see how the performance of fRank improved as we added features; we wanted to find at what point adding more feature sets became relatively useless. Beginning with no features, we greedily added the feature set that improved performance the most. The results are shown in Table 4. For example, the fourth line of the table shows that fRank using the page, popularity, and anchor features outperformed any network that used the page, popularity, and some other feature set, and that the performance of this network was 67.25%.
Table 4: fRank performance as feature sets are added. At each row, the feature set that gave the greatest increase in accuracy was added to the list of features (i.e., we conducted a greedy search over feature sets).
Feature Set Accuracy (%) None 50.00 +Page 63.93 +Popularity 66.83 +Anchor 67.25 +PageRank 67.31 +Domain 67.43 711 Finally, we present a qualitative comparison of PageRank vs. fRank. In Table 5 are the top ten URLs returned for PageRank and for fRank. PageRank"s results are heavily weighted towards technology sites. It contains two QuickTime URLs (Apple"s video playback software), as well as Internet Explorer and FireFox URLs (both of which are Web browsers). fRank, on the other hand, contains more consumer-oriented sites such as American Express, Target, Dell, etc. PageRank"s bias toward technology can be explained through two processes. First, there are many pages with buttons at the bottom suggesting that the site is optimized for Internet Explorer, or that the visitor needs QuickTime. These generally link back to, in these examples, the Internet Explorer and QuickTime download sites. Consequently, PageRank ranks those pages highly. Though these pages are important, they are not as important as it may seem by looking at the link structure alone. One fix for this is to add information about the link to the PageRank computation, such as the size of the text, whether it was at the bottom of the page, etc.
The other bias comes from the fact that the population of Web site authors is different than the population of Web users. Web authors tend to be technologically-oriented, and thus their linking behavior reflects those interests. fRank, by knowing the actual visitation popularity of a site (the popularity feature set), is able to eliminate some of that bias. It has the ability to depend more on where actual Web users visit rather than where the Web site authors have linked.
The results confirm that fRank outperforms PageRank in pairwise accuracy. The two most important feature sets are the page and popularity features. This is surprising, as the page features consisted only of a few (8) simple features. Further experiments found that, of the page features, those based on the text of the page (as opposed to the URL) performed the best. In the next section, we explore the popularity feature in more detail.
As mentioned in section 4, our popularity data came from MSN toolbar users. For privacy reasons, we had access only to an aggregate count of, for each URL, how many times it was visited by any toolbar user. This limited the possible features we could derive from this data. For possible extensions, see section 6.3, future work.
For each URL in our train and test sets, we provided a feature to fRank which was how many times it had been visited by a toolbar user. However, this feature was quite noisy and sparse, particularly for URLs with query parameters (e.g., http://search.msn.com/results.aspx?q=machine+learning&form=QBHP). One solution was to provide an additional feature which was the number of times any URL at the given domain was visited by a toolbar user. Adding this feature dramatically improved the performance of fRank.
We took this one step further and used the built-in hierarchical structure of URLs to construct many levels of backoff between the full URL and the domain. We did this by using the set of features shown in Table 6.
Table 6: URL functions used to compute the Popularity feature set.
Function Example Exact URL cnn.com/2005/tech/wikipedia.html?v=mobile No Params cnn.com/2005/tech/wikipedia.html Page wikipedia.html URL-1 cnn.com/2005/tech URL-2 cnn.com/2005 … Domain cnn.com Domain+1 cnn.com/2005 … Each URL was assigned one feature for each function shown in the table. The value of the feature was the count of the number of times a toolbar user visited a URL, where the function applied to that URL matches the function applied to the URL in question.
For example, a user"s visit to cnn.com/2005/sports.html would increment the Domain and Domain+1 features for the URL cnn.com/2005/tech/wikipedia.html.
As seen in Table 7, adding the domain counts significantly improved the quality of the popularity feature, and adding the numerous backoff functions listed in Table 6 improved the accuracy even further.
Table 7: Effect of adding backoff to the popularity feature set Features Accuracy (%) URL count 58.15 URL and Domain counts 59.31 All backoff functions (Table 6) 60.82 Table 5: Top ten URLs for PageRank vs. fRank PageRank fRank google.com google.com apple.com/quicktime/download yahoo.com amazon.com americanexpress.com yahoo.com hp.com microsoft.com/windows/ie target.com apple.com/quicktime bestbuy.com mapquest.com dell.com ebay.com autotrader.com mozilla.org/products/firefox dogpile.com ftc.gov bankofamerica.com 712 Backing off to subsets of the URL is one technique for dealing with the sparsity of data. It is also informative to see how the performance of fRank depends on the amount of popularity data that we have collected. In Figure 1 we show the performance of fRank trained with only the popularity feature set vs. the amount of data we have for the popularity feature set. Each day, we receive additional popularity data, and as can be seen in the plot, this increases the performance of fRank. The relation is logarithmic: doubling the amount of popularity data provides a constant improvement in pairwise accuracy.
In summary, we have found that the popularity features provide a useful boost to the overall fRank accuracy. Gathering more popularity data, as well as employing simple backoff strategies, improve this boost even further.
The experiments provide a number of conclusions. First, fRank performs significantly better than PageRank, even without any information about the Web graph. Second, the page level and popularity features were the most significant contributors to pairwise accuracy. Third, by collecting more popularity data, we can continue to improve fRank"s performance.
The popularity data provides two benefits to fRank. First, we see that qualitatively, fRank"s ordering of Web pages has a more favorable bias than PageRank"s. fRank"s ordering seems to correspond to what Web users, rather than Web page authors, prefer. Second, the popularity data is more timely than PageRank"s link information. The toolbar provides information about which Web pages people find interesting right now, whereas links are added to pages more slowly, as authors find the time and interest.
Since the original PageRank paper, there has been work on improving it. Much of that work centers on speeding up and parallelizing the computation [15][25].
One recognized problem with PageRank is that of topic drift: A page about dogs will have high PageRank if it is linked to by many pages that themselves have high rank, regardless of their topic. In contrast, a search engine user looking for good pages about dogs would likely prefer to find pages that are pointed to by many pages that are themselves about dogs. Hence, a link that is on topic should have higher weight than a link that is not.
Richardson and Domingos"s Query Dependent PageRank [29] and Haveliwala"s Topic-Sensitive PageRank [16] are two approaches that tackle this problem.
Other variations to PageRank include differently weighting links for inter- vs. intra-domain links, adding a backwards step to the random surfer to simulate the back button on most browsers [24] and modifying the jump probability (α) [3]. See Langville and Meyer [23] for a good survey of these, and other modifications to PageRank.
PageRank is not the only link analysis algorithm used for ranking Web pages. The most well-known other is HITS [22], which is used by the Teoma search engine [30]. HITS produces a list of hubs and authorities, where hubs are pages that point to many authority pages, and authorities are pages that are pointed to by many hubs. Previous work has shown HITS to perform comparably to PageRank [1].
One field of interest is that of static index pruning (see e.g.,
Carmel et al. [8]). Static index pruning methods reduce the size of the search engine"s index by removing documents that are unlikely to be returned by a search query. The pruning is typically done based on the frequency of query terms. Similarly, Pandey and Olston [28] suggest crawling pages frequently if they are likely to incorrectly appear (or not appear) as a result of a search.
Similar methods could be incorporated into the static rank (e.g., how many frequent queries contain words found on this page).
Others have investigated the effect that PageRank has on the Web at large [9]. They argue that pages with high PageRank are more likely to be found by Web users, thus more likely to be linked to, and thus more likely to maintain a higher PageRank than other pages. The same may occur for the popularity data. If we increase the ranking for popular pages, they are more likely to be clicked on, thus further increasing their popularity. Cho et al. [10] argue that a more appropriate measure of Web page quality would depend on not only the current link structure of the Web, but also on the change in that link structure. The same technique may be applicable to popularity data: the change in popularity of a page may be more informative than the absolute popularity.
One interesting related work is that of Ivory and Hearst [19].
Their goal was to build a model of Web sites that are considered high quality from the perspective of content, structure and navigation, visual design, functionality, interactivity, and overall experience. They used over 100 page level features, as well as features encompassing the performance and structure of the site.
This let them qualitatively describe the qualities of a page that make it appear attractive (e.g., rare use of italics, at least 9 point font, …), and (in later work) to build a system that assists novel Web page authors in creating quality pages by evaluating it according to these features. The primary differences between this work and ours are the goal (discovering what constitutes a good Web page vs. ordering Web pages for the purposes of Web search), the size of the study (they used a dataset of less than 6000 pages vs. our set of 468,000), and our comparison with PageRank. y = 0.577Ln(x) + 58.283 R 2 = 0.9822 58
59
60
61
Days of Toolbar Data PairwiseAccuracy Figure 1: Relation between the amount of popularity data and the performance of the popularity feature set. Note the x-axis is a logarithmic scale. 713 Nevertheless, their work provides insights to additional useful static features that we could incorporate into fRank in the future.
Recent work on incorporating novel features into dynamic ranking includes that by Joachims et al. [21], who investigate the use of implicit feedback from users, in the form of which search engine results are clicked on. Craswell et al. [11] present a method for determining the best transformation to apply to query independent features (such as those used in this paper) for the purposes of improving dynamic ranking. Other work, such as Boyan et al. [4] and Bartell et al. [2] apply machine learning for the purposes of improving the overall relevance of a search engine (i.e., the dynamic ranking). They do not apply their techniques to the problem of static ranking.
There are many ways in which we would like to extend this work.
First, fRank uses only a small number of features. We believe we could achieve even more significant results with more features. In particular the existence, or lack thereof, of certain words could prove very significant (for instance, under construction probably signifies a low quality page). Other features could include the number of images on a page, size of those images, number of layout elements (tables, divs, and spans), use of style sheets, conforming to W3C standards (like XHTML 1.0 Strict), background color of a page, etc.
Many pages are generated dynamically, the contents of which may depend on parameters in the URL, the time of day, the user visiting the site, or other variables. For such pages, it may be useful to apply the techniques found in [26] to form a static approximation for the purposes of extracting features. The resulting grammar describing the page could itself be a source of additional features describing the complexity of the page, such as how many non-terminal nodes it has, the depth of the grammar tree, etc. fRank allows one to specify a confidence in each pairing of documents. In the future, we will experiment with probabilities that depend on the difference in human judgments between the two items in the pair. For example, a pair of documents where one was rated 4 and the other 0 should have a higher confidence than a pair of documents rated 3 and 2.
The experiments in this paper are biased toward pages that have higher than average quality. Also, fRank with all of the features can only be applied to pages that have already been crawled.
Thus, fRank is primarily useful for index ordering and improving relevance, not for directing the crawl. We would like to investigate a machine learning approach for crawl prioritization as well. It may be that a combination of methods is best: for example, using PageRank to select the best 5 billion of the 20 billion pages on the Web, then using fRank to order the index and affect search relevancy.
Another interesting direction for exploration is to incorporate fRank and page-level features directly into the PageRank computation itself. Work on biasing the PageRank jump vector [16], and transition matrix [29], have demonstrated the feasibility and advantages of such an approach. There is reason to believe that a direct application of [29], using the fRank of a page for its relevance, could lead to an improved overall static rank.
Finally, the popularity data can be used in other interesting ways.
The general surfing and searching habits of Web users varies by time of day. Activity in the morning, daytime, and evening are often quite different (e.g., reading the news, solving problems, and accessing entertainment, respectively). We can gain insight into these differences by using the popularity data, divided into segments of the day. When a query is issued, we would then use the popularity data matching the time of query in order to do the ranking of Web pages. We also plan to explore popularity features that use more than just the counts of how often a page was visited.
For example, how long users tended to dwell on a page, did they leave the page by clicking a link or by hitting the back button, etc.
Fox et al. did a study that showed that features such as this can be valuable for the purposes of dynamic ranking [14]. Finally, the popularity data could be used as the label rather than as a feature.
Using fRank in this way to predict the popularity of a page may useful for the tasks of relevance, efficiency, and crawl priority.
There is also significantly more popularity data than human labeled data, potentially enabling more complex machine learning methods, and significantly more features.
A good static ranking is an important component for today"s search engines and information retrieval systems. We have demonstrated that PageRank does not provide a very good static ranking; there are many simple features that individually out perform PageRank. By combining many static features, fRank achieves a ranking that has a significantly higher pairwise accuracy than PageRank alone. A qualitative evaluation of the top documents shows that fRank is less technology-biased than PageRank; by using popularity data, it is biased toward pages that Web users, rather than Web authors, visit. The machine learning component of fRank gives it the additional benefit of being more robust against spammers, and allows it to leverage further developments in the machine learning community in areas such as adversarial classification. We have only begun to explore the options, and believe that significant strides can be made in the area of static ranking by further experimentation with additional features, other machine learning techniques, and additional sources of data.
Thank you to Marc Najork for providing us with additional PageRank computations and to Timo Burkard for assistance with the popularity data. Many thanks to Chris Burges for providing code and significant support in using training RankNets. Also, we thank Susan Dumais and Nick Craswell for their edits and suggestions.
[1] B. Amento, L. Terveen, and W. Hill. Does authority mean quality? Predicting expert quality ratings of Web documents.
In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2000. [2] B. Bartell, G. Cottrell, and R. Belew. Automatic combination of multiple ranked retrieval systems. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1994. [3] P. Boldi, M. Santini, and S. Vigna. PageRank as a function of the damping factor. In Proceedings of the International World Wide Web Conference, May 2005. 714 [4] J. Boyan, D. Freitag, and T. Joachims. A machine learning architecture for optimizing web search engines. In AAAI Workshop on Internet Based Information Systems, August
[5] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proceedings of the Seventh International Wide Web Conference, Brisbane,
Australia, 1998. Elsevier. [6] A. Broder, R. Lempel, F. Maghoul, and J. Pederson.
Efficient PageRank approximation via graph aggregation. In Proceedings of the International World Wide Web Conference, May 2004. [7] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N.
Hamilton, G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [8] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y.
S. Maarek, and A. Soffer. Static index pruning for information retrieval systems. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 43-50,
New Orleans, Louisiana, USA, September 2001. [9] J. Cho and S. Roy. Impact of search engines on page popularity. In Proceedings of the International World Wide Web Conference, May 2004. [10]J. Cho, S. Roy, R. Adams. Page Quality: In search of an unbiased web ranking. In Proceedings of the ACM SIGMOD
[11]N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor.
Relevance weighting for query independent evidence. In Proceedings of the 28th Annual Conference on Research and Development in Information Retrieval (SIGIR), August,
[12]N. Dalvi, P. Domingos, Mausam, S. Sanghai, D. Verma.
Adversarial Classification. In Proceedings of the Tenth International Conference on Knowledge Discovery and Data Mining (pp. 99-108), Seattle, WA, 2004. [13]O. Dekel, C. Manning, and Y. Singer. Log-linear models for label-ranking. In Advances in Neural Information Processing Systems 16. Cambridge, MA: MIT Press, 2003. [14]S. Fox, K S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White (2005). Evaluating implicit measures to improve the search experiences. In the ACM Transactions on Information Systems, 23(2), pp. 147-168. April 2005. [15]T. Haveliwala. Efficient computation of PageRank. Stanford University Technical Report, 1999. [16]T. Haveliwala. Topic-sensitive PageRank. In Proceedings of the International World Wide Web Conference, May 2002. [17]D. Hawking and N. Craswell. Very large scale retrieval and Web search. In D. Harman and E. Voorhees (eds), The TREC Book. MIT Press. [18]R. Herbrich, T. Graepel, and K. Obermayer. Support vector learning for ordinal regression. In Proceedings of the Ninth International Conference on Artificial Neural Networks, pp. 97-102. 1999. [19]M. Ivory and M. Hearst. Statistical profiles of highly-rated Web sites. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 2002. [20]T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2002. [21]T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G.
Gay. Accurately Interpreting Clickthrough Data as Implicit Feedback. In Proceedings of the Conference on Research and Development in Information Retrieval (SIGIR), 2005. [22]J. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM 46:5, pp. 604-32. 1999. [23]A. Langville and C. Meyer. Deeper inside PageRank.
Internet Mathematics 1(3):335-380, 2004. [24]F. Matthieu and M. Bouklit. The effect of the back button in a random walk: application for PageRank. In Alternate track papers and posters of the Thirteenth International World Wide Web Conference, 2004. [25]F. McSherry. A uniform approach to accelerated PageRank computation. In Proceedings of the International World Wide Web Conference, May 2005. [26]Y. Minamide. Static approximation of dynamically generated Web pages. In Proceedings of the International World Wide Web Conference, May 2005. [27]L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citation ranking: Bringing order to the web.
Technical report, Stanford University, Stanford, CA, 1998. [28]S. Pandey and C. Olston. User-centric Web crawling. In Proceedings of the International World Wide Web Conference, May 2005. [29]M. Richardson and P. Domingos. The intelligent surfer: probabilistic combination of link and content information in PageRank. In Advances in Neural Information Processing Systems 14, pp. 1441-1448. Cambridge, MA: MIT Press,

Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task?
Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments?
Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them. The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].
This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time. As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17],
Carterette et al. [8], and Aslam et al. [4], among others.
As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.
Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be reusable. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one. We need a more careful definition of reusability.
Specifically, the question of reusability is not how accurately we can evaluate new systems. A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree.
Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time.
Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation. By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents. Our evaluation should be robust to missing judgments.
In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8]. This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure. Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.
We therefore see confidence as a probability estimate. One of the questions we must ask about a probability estimate is what it means. What does it mean to have 75% confidence that system A is better than system B? As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change. If this is what it means, we can trust the confidence estimates. But do we know it has that meaning?
Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant. This assumption is almost certainly not realistic in most IR applications. As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted. Before elaborating on this, we formally define confidence.
Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall). It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i. Let Xi be a random variable indicating the relevance of document i. If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .
Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}. Using aij instead of 1/i allows us to number the documents arbitrarily. To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2. Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1. Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.
We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents. This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.
All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1). Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7. We can then compute e.g. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.
Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2. As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking. Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.
Since topics are independent, we can easily extend this to mean average precision (MAP). MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0).
Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).
Let Z be the set of all pairs of ranked results for a common set of topics. Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence. Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm . For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document. If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.
If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate. The assumptions they are based on are the probabilities of relevance pi. We need these to be realistic.
We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions. This is known as the principle of maximum entropy [13].
The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).
This has found a wide array of uses in computer science and information retrieval. The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form. The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.
Theorem 1. If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above,
Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now). We forgo the proof for the time being, but it is quite simple.
This says that the better the estimates of relevance, the more accurate the evaluation. The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.
The theorem and its proof say nothing whatsoever about the evaluation metric. The probability estimates are entirely indepedent of the measure we are interested in. This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.
Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1. If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.
The task therefore becomes the imputation of the missing values of relevance. The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness.
In our statement of Theorem 1, we left the nature of the information I unspecified. One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance. A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on. If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.
This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking. Aslam et al. [3] previously identified a connection between evaluation and metasearch. Our problem has two key differences:
can plug into Eq. 1; metasearch algorithms have no such requirement.
proceed with the evaluation and are able to re-estimate relevance given each new judgment.
In light of (1) above, we introduce a probabilistic model for expert combination.
Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1). The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik). The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).
As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5]. Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression. Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters. We include a beta prior for p(λj) with parameters α, β. This can be seen as a type of smoothing to account for the fact that the training data is highly biased.
This model has the advantage of including the statistical dependence between the experts. A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10]. A similar maximumentropy-motivated approach has been used for expert aggregation [15]. Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.
Where do the qij s come from? Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics. A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.
We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence. Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well. We could use a hierarchical model [12], but that will not generalize to unseen topics. Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.
Thus our model takes into account not only the dependence between experts, but also the dependence between experts" performances on different tasks (topics).
Each expert gives us a score and a rank for each document.
We need to convert these to probabilities. A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance. The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.
Let q∗ ij be expert j"s self-reported probability that document i is relevant. Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).
The pairwise preference model can handle these two requirements easily, so we will use it. Let θrj (i) be the relevance coefficient of the document at rank rj(i). We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively. Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.
After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.
Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD. Therefore we only have to solve this once for each topic.
The above model gives topic-independent probabilities for each document. But suppose an expert who reports 90% probability is only right 50% of the time. Its opinion should be discounted based on its observed performance.
Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the expert"s ability to retrieve relevant documents given the judgments that have been made to this point.
Platt"s SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.
Once we have the calibration function, it is applied to adjust the experts" predictions to their actual performance.
The calibrated probabilities are plugged into model (2) to find the document probabilities.
Figure 1: Conceptual diagram of our aggregation model. Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2. The first step is to obtain q∗ ij. Next is calibration to true performance to find qij . Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · .
Our model has three components that differ by the data they take as input and what they produce as output. A conceptual diagram is shown in Figure 1.
gives us q∗ ij, expert j"s self-reported probability of the relevance of document i. This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters).
This gives us qij = Cj (q∗ ij), expert j"s calibrated probability of the relevance of document i. This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks.
gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .
This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.
Although the model appears rather complex, it is really just three successive applications of logistic regression. As such, it can be implemented in a statistical programming language such as R in a few lines of code. The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line. Our code is available at http://ciir.cs.umass.edu/~carteret/.
Three hypotheses are under consideration. The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.
The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8]. These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents.
We obtained full ad-hoc runs submitted to TRECs 3 through 8. Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4). Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.
These are the tracks that have replaced the ad-hoc track since its end in 1999. Statistics are shown in Table 1.
We set aside the TREC 4 (ad-hoc 95) set for training,
TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.
We use the qrels files assembled by NIST as truth. The number of relevance judgments made and relevant documents found for each track are listed in Table 1.
For computational reasons, we truncate ranked lists at 100 documents. There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming. Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100.
We will compare three algorithms for acquiring relevance judgments. The baseline is a variation of TREC pooling that we will call incremental pooling (IP). This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged. It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.
The second algorithm is that presented in Carterette et al. [8] (Algorithm 1). Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists. For this approach pi = 0.5 for all i; there is no estimation of probabilities. We will call this MTC for minimal test collection.
The third algorithm augments MTC with updated estimates of probabilities of relevance. We will call this RTC for robust test collection. It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.
RTC has smoothing (prior distribution) parameters that must be set. We trained using the ad-hoc 95 set. We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.
Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.
For expert aggregation, the prior parameters are α = β = 1.
First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems. For each experimental trial:
probabilities of relevance for all documents retrieved by all k runs.
for all pairs of runs.
We do the same for MTC, but omit step 4. Note that after evaluating the first c systems, we make no additional relevance judgments.
To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems. We will then generalize to a set of k = 10 (of which those two are a subset).
As we run more trials, we obtain the data we need to test all three of our hypotheses.
Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.
One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin. We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.
Since summary statistics are useful, we devised the following metric. Suppose we are a bookmaker taking bets on whether ΔMAP < 0. We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) . Suppose a bettor wagers $1 on ΔMAP ≥ 0. If it turns out that ΔMAP < 0, we win the dollar. Otherwise, we pay out O. If our confidence estimates are perfectly accurate, we break even. If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.
Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.
However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.
The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0). The summary statistic is W, the mean of Wi.
Note that as Pi increases, we lose more for being wrong.
This is as it should be: the penalty should be great for missing the high probability predictions. However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.
For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.
The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand. The distribution is therefore highly skewed, and the mean strongly affected by those outliers.
Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendall"s τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments. Kendall"s τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.
It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.
As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability. We include it for completeness.
Running multiple trials allows the use of statistical hypothesis testing to compare algorithms. Using the same sets of systems allows the use of paired tests.
As we stated above, we are more interested in the median number of judgments than the mean. A test for difference in median is the Wilcoxon sign rank test. We can also use a paired t-test to test for a difference in mean.
For rank correlation, we can use a paired t-test to test for a difference in τ.
The comparison between MTC and RTC is shown in Table 2. With MTC and uniform probabilities of relevance, the results are far from robust. We cannot reuse the relevance judgments with much confidence. But with RTC, the results are very robust. There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy. Mean Wi shows that RTC is much closer to 0 than MTC. The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates. The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.
Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.
More detailed results for both algorithms are shown in Figure 2. The solid line is the ideal result that would give W = 0. RTC is on or above this line at all points until confidence reaches about 0.97. After that there is a slight dip in accuracy which we discuss below. Note that both MTC RTC confidence % in bin accuracy % in bin accuracy
W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.
Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets. RTC is much more robust than MTC.
W is defined in Section 4.4; closer to 0 is better.
Median judged is the number of judgments to reach 95% confidence on the first two systems. Mean τ is the average rank correlation for all 10 systems.
1
accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC. The solid line is the perfect result that would give W = 0; performance should be on or above this line. Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence
sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].
Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic. The median required by RTC is 235, about 4.7 per topic. Although the numbers are close, RTC"s median is significantly lower by a paired Wilcoxon test (p < 0.0001). For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.
The difference in means is much greater. MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.) This difference is significant by a paired t-test (p < 0.0001).
Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic). Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least
For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.
Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates. The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.
This difference is significant by a paired t-test (p < 0.0001).
Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments. It is more important that we estimate confidence in each pairwise comparison correctly.
We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant). We calculated the τ correlation to the true ranking. The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC. Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.
Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable. We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.
In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.
Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences. Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.
Pairwise Comparisons: Our pairwise comparisons fall into one of three groups:
are acquired;
Table 3 shows confidence vs. accuracy results for each of these three groups. Interestingly, performance is worst when comparing one of the original runs to one of the additional runs. This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.
Nevertheless, performance is quite good on all three subsets.
Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common. If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.
A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2]. We calculated this for all pairs, then looked at performance on pairs with low similarity. Results are shown in accuracy confidence two original one original no original
W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs. RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3
W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%). RTC is robust in all three cases.
Table 4. Performance is in fact very robust even when similarity is low. When the two runs share very few documents in common, W is actually positive.
MTC and IP both performed quite poorly in these cases.
When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.
By Data Set: All the previous results have only been on the ad-hoc collections. We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies. The results in Table 5 show everything about each set, including binned accuracy,
W, mean τ, and median number of judgments to reach 95% confidence on the first two systems. The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set.
In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.
The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05
W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.
We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing. It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].
The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed). In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration.
Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it. In the meantime, capping confidence estimates at 95% is a hack that solves the problem.
We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.
Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor.

The MPEG-7 standard defines - among others - a set of descriptors for visual media. Each descriptor consists of a feature extraction mechanism, a description (in binary and XML format) and guidelines that define how to apply the descriptor on different kinds of media (e.g. on temporal media). The MPEG-7 descriptors have been carefully designed to meet - partially complementaryrequirements of different application domains: archival, browsing, retrieval, etc. [9]. In the following, we will exclusively deal with the visual MPEG-7 descriptors in the context of media retrieval.
The visual MPEG-7 descriptors fall in five groups: colour, texture, shape, motion and others (e.g. face description) and sum up to 16 basic descriptors. For retrieval applications, a rule for each descriptor is mandatory that defines how to measure the similarity of two descriptions. Common rules are distance functions, like the Euclidean distance and the Mahalanobis distance. Unfortunately, the MPEG-7 standard does not include distance measures in the normative part, because it was not designed to be (and should not exclusively understood to be) retrieval-specific. However, the MPEG-7 authors give recommendations, which distance measure to use on a particular descriptor. These recommendations are based on accurate knowledge of the descriptors' behaviour and the description structures.
In the present study a large number of successful distance measures from different areas (statistics, psychology, medicine, social and economic sciences, etc.) were implemented and applied on MPEG-7 data vectors to verify whether or not the recommended MPEG-7 distance measures are really the best for any reasonable class of media objects. From the MPEG-7 tests and the recommendations it does not become clear, how many and which distance measures have been tested on the visual descriptors and the MPEG-7 test datasets. The hypothesis is that analytically derived distance measures may be good in general but only a quantitative analysis is capable to identify the best distance measure for a specific feature extraction method.
The paper is organised as follows. Section 2 gives a minimum of background information on the MPEG-7 descriptors and distance measurement in visual information retrieval (VIR, see [3], [16]).
Section 3 gives an overview over the implemented distance measures. Section 4 describes the test setup, including the test data and the implemented evaluation methods. Finally, Section 5 presents the results per descriptor and over all descriptors.
The visual part of the MPEG-7 standard defines several descriptors. Not all of them are really descriptors in the sense that they extract properties from visual media. Some of them are just structures for descriptor aggregation or localisation. The basic descriptors are Color Layout, Color Structure, Dominant Color,
Scalable Color, Edge Histogram, Homogeneous Texture, Texture Browsing, Region-based Shape, Contour-based Shape, Camera Motion, Parametric Motion and Motion Activity.
Other descriptors are based on low-level descriptors or semantic information: Group-of-Frames/Group-of-Pictures Color (based on Scalable Color), Shape 3D (based on 3D mesh information),
Motion Trajectory (based on object segmentation) and Face Recognition (based on face extraction).
Descriptors for spatiotemporal aggregation and localisation are: Spatial 2D Coordinates, Grid Layout, Region Locator (spatial),
Time Series, Temporal Interpolation (temporal) and SpatioTemporal Locator (combined). Finally, other structures exist for colour spaces, colour quantisation and multiple 2D views of 3D objects.
These additional structures allow combining the basic descriptors in multiple ways and on different levels. But they do not change the characteristics of the extracted information. Consequently, structures for aggregation and localisation were not considered in the work described in this paper.
Generally, similarity measurement on visual information aims at imitating human visual similarity perception. Unfortunately, human perception is much more complex than any of the existing similarity models (it includes perception, recognition and subjectivity).
The common approach in visual information retrieval is measuring dis-similarity as distance. Both, query object and candidate object are represented by their corresponding feature vectors. The distance between these objects is measured by computing the distance between the two vectors. Consequently, the process is independent of the employed querying paradigm (e.g. query by example). The query object may be natural (e.g. a real object) or artificial (e.g. properties of a group of objects).
Goal of the measurement process is to express a relationship between the two objects by their distance. Iteration for multiple candidates allows then to define a partial order over the candidates and to address those in a (to be defined) neighbourhood being similar to the query object. At this point, it has to be mentioned that in a multi-descriptor environmentespecially in MPEG-7 - we are only half way towards a statement on similarity. If multiple descriptors are used (e.g. a descriptor scheme), a rule has to be defined how to combine all distances to a global value for each object. Still, distance measurement is the most important first step in similarity measurement.
Obviously, the main task of good distance measures is to reorganise descriptor space in a way that media objects with the highest similarity are nearest to the query object. If distance is defined minimal, the query object is always in the origin of distance space and similar candidates should form clusters around the origin that are as large as possible. Consequently, many well known distance measures are based on geometric assumptions of descriptor space (e.g. Euclidean distance is based on the metric axioms). Unfortunately, these measures do not fit ideally with human similarity perception (e.g. due to human subjectivity). To overcome this shortage, researchers from different areas have developed alternative models that are mostly predicate-based (descriptors are assumed to contain just binary elements, e.g.
Tversky's Feature Contrast Model [17]) and fit better with human perception. In the following distance measures of both groups of approaches will be considered.
The distance measures used in this work have been collected from various areas (Subsection 3.1). Because they work on differently quantised data, Subsection 3.2 sketches a model for unification on the basis of quantitative descriptions. Finally, Subsection 3.3 introduces the distance measures as well as their origin and the idea they implement.
Distance measurement is used in many research areas such as psychology, sociology (e.g. comparing test results), medicine (e.g. comparing parameters of test persons), economics (e.g. comparing balance sheet ratios), etc. Naturally, the character of data available in these areas differs significantly. Essentially, there are two extreme cases of data vectors (and distance measures): predicatebased (all vector elements are binary, e.g. {0, 1}) and quantitative (all vector elements are continuous, e.g. [0, 1]).
Predicates express the existence of properties and represent highlevel information while quantitative values can be used to measure and mostly represent low-level information. Predicates are often employed in psychology, sociology and other human-related sciences and most predicate-based distance measures were therefore developed in these areas. Descriptions in visual information retrieval are nearly ever (if they do not integrate semantic information) quantitative. Consequently, mostly quantitative distance measures are used in visual information retrieval.
The goal of this work is to compare the MPEG-7 distance measures with the most powerful distance measures developed in other areas. Since MPEG-7 descriptions are purely quantitative but some of the most sophisticated distance measures are defined exclusively on predicates, a model is mandatory that allows the application of predicate-based distance measures on quantitative data. The model developed for this purpose is presented in the next section.
The goal of the quantisation model is to redefine the set operators that are usually used in predicate-based distance measures on continuous data. The first in visual information retrieval to follow this approach were Santini and Jain, who tried to apply Tversky's Feature Contrast Model [17] to content-based image retrieval [12], [13]. They interpreted continuous data as fuzzy predicates and used fuzzy set operators. Unfortunately, their model suffered from several shortcomings they described in [12], [13] (for example, the quantitative model worked only for one specific version of the original predicate-based measure).
The main idea of the presented quantisation model is that set operators are replaced by statistical functions. In [5] the authors could show that this interpretation of set operators is reasonable.
The model offers a solution for the descriptors considered in the evaluation. It is not specific to one distance measure, but can be applied to any predicate-based measure. Below, it will be shown that the model does not only work for predicate data but for quantitative data as well. Each measure implementing the model can be used as a substitute for the original predicate-based measure.
Generally, binary properties of two objects (e.g. media objects) can exist in both objects (denoted as a), in just one (b, c) or in none of them (d). The operator needed for these relationships are UNION, MINUS and NOT. In the quantisation model they are replaced as follows (see [5] for further details). 131 ∑     ≤ + − + ==∩= k jkikjkik kkji else xx Mif xx ssXXa 0 22, 1ε ( ) ( ) ∑ ∑ ∑     ≤ ++ −==¬∩¬=    ≤−−− ==−=    ≤−−− ==−= k jkikjkik kkji k ikjkikjk kkij k jkikjkik kkji else xx if xx MssXXd else xxMifxx ssXXc else xxMifxx ssXXb 0 22, 0 , 0 , 1 2 2 ε ε ε with: ( ) [ ] ( ) { }0\ . 0 1 . 0 1 , 2 2 1 minmax maxmin + ∈ − =     ≥      − = =     ≥      − = −= ∈= ∑ ∑ ∑ ∑ Rp ki x where else pif p M ki x where else pif p M xxM xxxwithxX i k ik i k ik ikiki µ σ σ σ ε µ µ µ ε a selects properties that are present in both data vectors (Xi, Xj representing media objects), b and c select properties that are present in just one of them and d selects properties that are present in neither of the two data vectors. Every property is selected by the extent to which it is present (a and d: mean, b and c: difference) and only if the amount to which it is present exceeds a certain threshold (depending on the mean and standard deviation over all elements of descriptor space).
The implementation of these operators is based on one assumption.
It is assumed that vector elements measure on interval scale. That means, each element expresses that the measured property is "more or less" present ("0": not at all, "M": fully present). This is true for most visual descriptors and all MPEG-7 descriptors. A natural origin as it is assumed here ("0") is not needed.
Introducing p (called discriminance-defining parameter) for the thresholds 21 ,εε has the positive consequence that a, b, c, d can then be controlled through a single parameter. p is an additional criterion for the behaviour of a distance measure and determines the thresholds used in the operators. It expresses how accurate data items are present (quantisation) and consequently, how accurate they should be investigated. p can be set by the user or automatically. Interesting are the limits:
In this case, all elements (=properties) are assumed to be continuous (high quantisation). In consequence, all properties of a descriptor are used by the operators. Then, the distance measure is not discriminant for properties.
In this case, all properties are assumed to be predicates. In consequence, only binary elements (=predicates) are used by the operators (1-bit quantisation). The distance measure is then highly discriminant for properties.
Between these limits, a distance measure that uses the quantisation model is - depending on p - more or less discriminant for properties. This means, it selects a subset of all available description vector elements for distance measurement.
For both predicate data and quantitative data it can be shown that the quantisation model is reasonable. If description vectors consist of binary elements only, p should be used as follows (for example, p can easily be set automatically): ( )σµεε ,min..,0,0 21 ==⇒→ pgep In this case, a, b, c, d measure like the set operators they replace.
For example, Table 1 shows their behaviour for two onedimensional feature vectors Xi and Xj. As can be seen, the statistical measures work like set operators. Actually, the quantisation model works accurate on predicate data for any p≠∞.
To show that the model is reasonable for quantitative data the following fact is used. It is easy to show that for predicate data some quantitative distance measures degenerate to predicatebased measures. For example, the L1 metric (Manhattan metric) degenerates to the Hamming distance (from [9], without weights): distanceHammingcbxxL k jkik =+≡−= ∑1 If it can be shown that the quantisation model is able to reconstruct the quantitative measure from the degenerated predicate-based measure, the model is obviously able to extend predicate-based measures to the quantitative domain. This is easy to illustrate. For purely quantitative feature vectors, p should be used as follows (again, p can easily be set automatically): 1, 21 =⇒∞→ εεp Then, a and d become continuous functions: ∑ ∑ + −==⇒≡≤ + + ==⇒≡≤ + − k jkik kk jkik k jkik kk jkik xx MswheresdtrueM xx xx swheresatrueM xx M 22 22 b and c can be made continuous for the following expressions: ( ) ( ) ∑ ∑ ∑ −==+⇒    ≥−− ==⇒ ≥−≡≤−−    ≥−− ==⇒ ≥−≡≤−− k jkikkk k ikjkikjk kk ikjkikjk k jkikjkik kk jkikjkik xxswherescb else xxifxx swheresc xxMxxM else xxifxx swheresb xxMxxM 0 0 0 0 0 0 Table 1. Quantisation model on predicate vectors.
Xi Xj a b c d (1) (1) 1 0 0 0 (1) (0) 0 1 0 0 (0) (1) 0 0 1 0 (0) (0) 0 0 0 1 132 ∑ ∑ −==− −==− k ikjkkk k jkikkk xxswheresbc xxswherescb This means, for sufficiently high p every predicate-based distance measure that is either not using b and c or just as b+c, b-c or c-b, can be transformed into a continuous quantitative distance measure. For example, the Hamming distance (again, without weights): 1 Lxxxxswherescb k jkik k jkikkk =−=−==+ ∑∑ The quantisation model successfully reconstructs the L1 metric and no distance measure-specific modification has to be made to the model. This demonstrates that the model is reasonable. In the following it will be used to extend successful predicate-based distance measures on the quantitative domain.
The major advantages of the quantisation model are: (1) it is application domain independent, (2) the implementation is straightforward, (3) the model is easy to use and finally, (4) the new parameter p allows to control the similarity measurement process in a new way (discriminance on property level).
For the evaluation described in this work next to predicate-based (based on the quantisation model) and quantitative measures, the distance measures recommended in the MPEG-7 standard were implemented (all together 38 different distance measures).
Table 2 summarises those predicate-based measures that performed best in the evaluation (in sum 20 predicate-based measures were investigated). For these measures, K is the number of predicates in the data vectors Xi and Xj. In P1, the sum is used for Tversky's f() (as Tversky himself does in [17]) and α, β are weights for element b and c. In [5] the author's investigated Tversky's Feature Contrast Model and found α=1, β=0 to be the optimum parameters.
Some of the predicate-based measures are very simple (e.g. P2,
P4) but have been heavily exploited in psychological research.
Pattern difference (P6) - a very powerful measure - is used in the statistics package SPSS for cluster analysis. P7 is a correlation coefficient for predicates developed by Pearson.
Table 3 shows the best quantitative distance measures that were used. Q1 and Q2 are metric-based and were implemented as representatives for the entire group of Minkowski distances. The wi are weights. In Q5, ii σµ , are mean and standard deviation for the elements of descriptor Xi. In Q6, m is 2 M (=0.5). Q3, the Canberra metric, is a normalised form of Q1. Similarly, Q4,
Clark's divergence coefficient is a normalised version of Q2. Q6 is a further-developed correlation coefficient that is invariant against sign changes. This measure is used even though its particular properties are of minor importance for this application domain.
Finally, Q8 is a measure that takes the differences between adjacent vector elements into account. This makes it structurally different from all other measures.
Obviously, one important distance measure is missing. The Mahalanobis distance was not considered, because different descriptors would require different covariance matrices and for some descriptors it is simply impossible to define a covariance matrix. If the identity matrix was used in this case, the Mahalanobis distance would degenerate to a Minkowski distance.
Additionally, the recommended MPEG-7 distances were implemented with the following parameters: In the distance measure of the Color Layout descriptor all weights were set to "1" (as in all other implemented measures). In the distance measure of the Dominant Color descriptor the following parameters were used: 20,1,3.0,7.0 21 ==== dTww α (as recommended). In the Homogeneous Texture descriptor's distance all ( )kα were set to "1" and matching was done rotation- and scale-invariant.
Important! Some of the measures presented in this section are distance measures while others are similarity measures. For the tests, it is important to notice, that all similarity measures were inverted to distance measures.
Subsection 4.1 describes the descriptors (including parameters) and the collections (including ground truth information) that were used in the evaluation. Subsection 4.2 discusses the evaluation method that was implemented and Subsection 4.3 sketches the test environment used for the evaluation process.
For the evaluation eight MPEG-7 descriptors were used. All colour descriptors: Color Layout, Color Structure, Dominant Color, Scalable Color, all texture descriptors: Edge Histogram,
Homogeneous Texture, Texture Browsing and one shape descriptor: Region-based Shape. Texture Browsing was used even though the MPEG-7 standard suggests that it is not suitable for retrieval. The other basic shape descriptor, Contour-based Shape, was not used, because it produces structurally different descriptions that cannot be transformed to data vectors with elements measuring on interval-scales. The motion descriptors were not used, because they integrate the temporal dimension of visual media and would only be comparable, if the basic colour, texture and shape descriptors would be aggregated over time. This was not done. Finally, no high-level descriptors were used (Localisation, Face Recognition, etc., see Subsection 2.1), because - to the author's opinion - the behaviour of the basic descriptors on elementary media objects should be evaluated before conclusions on aggregated structures can be drawn.
Table 2. Predicate-based distance measures.
No. Measure Comment P1 cba .. βα −− Feature Contrast Model,
Tversky 1977 [17] P2 a No. of co-occurrences P3 cb + Hamming distance P4 K a Russel 1940 [14] P5 cb a + Kulczvnski 1927 [14] P6 2 K bc Pattern difference [14] P7 ( )( )( )( )dcdbcaba bcad ++++ − Pearson 1926 [11] 133 The Texture Browsing descriptions had to be transformed from five bins to an eight bin representation in order that all elements of the descriptor measure on an interval scale. A Manhattan metric was used to measure proximity (see [6] for details).
Descriptor extraction was performed using the MPEG-7 reference implementation. In the extraction process each descriptor was applied on the entire content of each media object and the following extraction parameters were used. Colour in Color Structure was quantised to 32 bins. For Dominant Color colour space was set to YCrCb, 5-bit default quantisation was used and the default value for spatial coherency was used. Homogeneous Texture was quantised to 32 components. Scalable Color values were quantised to sizeof(int)-3 bits and 64 bins were used. Finally,
Texture Browsing was used with five components.
These descriptors were applied on three media collections with image content: the Brodatz dataset (112 images, 512x512 pixel), a subset of the Corel dataset (260 images, 460x300 pixel, portrait and landscape) and a dataset with coats-of-arms images (426 images, 200x200 pixel). Figure 1 shows examples from the three collections.
Designing appropriate test sets for a visual evaluation is a highly difficult task (for example, see the TREC video 2002 report [15]).
Of course, for identifying the best distance measure for a descriptor, it should be tested on an infinite number of media objects. But this is not the aim of this study. It is just evaluated if - for likely image collections - better proximity measures than those suggested by the MPEG-7 group can be found. Collections of this relatively small size were used in the evaluation, because the applied evaluation methods are above a certain minimum size invariant against collection size and for smaller collections it is easier to define a high-quality ground truth. Still, the average ratio of ground truth size to collection size is at least 1:7. Especially, no collection from the MPEG-7 dataset was used in the evaluation because the evaluations should show, how well the descriptors and the recommended distance measures perform on "unknown" material.
When the descriptor extraction was finished, the resulting XML descriptions were transformed into a data matrix with 798 lines (media objects) and 314 columns (descriptor elements). To be usable with distance measures that do not integrate domain knowledge, the elements of this data matrix were normalised to [0, 1].
For the distance evaluation - next to the normalised data matrixhuman similarity judgement is needed. In this work, the ground truth is built of twelve groups of similar images (four for each dataset). Group membership was rated by humans based on semantic criterions. Table 4 summarises the twelve groups and the underlying descriptions. It has to be noticed, that some of these groups (especially 5, 7 and 10) are much harder to find with lowlevel descriptors than others.
Usually, retrieval evaluation is performed based on a ground truth with recall and precision (see, for example, [3], [16]). In multidescriptor environments this leads to a problem, because the resulting recall and precision values are strongly influenced by the method used to merge the distance values for one media object.
Even though it is nearly impossible to say, how big the influence of a single distance measure was on the resulting recall and precision values, this problem has been almost ignored so far.
In Subsection 2.2 it was stated that the major task of a distance measure is to bring the relevant media objects as close to the origin (where the query object lies) as possible. Even in a multidescriptor environment it is then simple to identify the similar objects in a large distance space. Consequently, it was decided to Table 3. Quantitative distance measures.
No. Measure Comment No. Measure Comment Q1 ∑ − k jkiki xxw City block distance (L1 ) Q2 ( )∑ − k jkiki xxw 2 Euclidean distance (L2 ) Q3 ∑ + − k jkik jkik xx xx Canberra metric,
Lance, Williams
Q4 ( ) ∑ + − k jkik jkik xx xx K 2 1 Divergence coefficient,
Clark 1952 [1] Q5 ( )( ) ( ) ( )∑ ∑ ∑ −− −− k k jjkiik k jjkiik xx xx 22 µµ µµ Correlation coefficient Q6       −+      −−       +−− ∑∑∑ ∑ ∑∑ k ik k jkik k ik k k jk k ikjkik xmKmxxmKmx xxmKmxx
Cohen 1969 [2] Q7 ∑ ∑ ∑ k k jkik k jkik xx xx 22 Angular distance,
Gower 1967 [7] Q8 ( ) ( )( )∑ − ++ −−− 1 2 11 K k jkjkikik xxxx Meehl Index [10] Table 4. Ground truth information.
Coll. No Images Description
Brodatz
Corel
Arms
134 use indicators measuring the distribution in distance space of candidates similar to the query object for this evaluation instead of recall and precision. Identifying clusters of similar objects (based on the given ground truth) is relatively easy, because the resulting distance space for one descriptor and any distance measure is always one-dimensional. Clusters are found by searching from the origin of distance space to the first similar object, grouping all following similar objects in the cluster, breaking off the cluster with the first un-similar object and so forth.
For the evaluation two indicators were defined. The first measures the average distance of all cluster means to the origin: distanceavgclustersno sizecluster distanceclustersno i i sizecluster j ij d i _._ _ _ _ ∑ ∑ =µ where distanceij is the distance value of the j-th element in the i-th cluster, ∑ ∑ ∑ = CLUSTERS i i CLUSTERS i sizecluster j ij sizecluster distance distanceavg i _ _ _ , no_clusters is the number of found clusters and cluster_sizei is the size of the i-th cluster. The resulting indicator is normalised by the distribution characteristics of the distance measure (avg_distance).
Additionally, the standard deviation is used. In the evaluation process this measure turned out to produce valuable results and to be relatively robust against parameter p of the quantisation model.
In Subsection 3.2 we noted that p affects the discriminance of a predicate-based distance measure: The smaller p is set the larger are the resulting clusters because the quantisation model is then more discriminant against properties and less elements of the data matrix are used. This causes a side-effect that is measured by the second indicator: more and more un-similar objects come out with exactly the same distance value as similar objects (a problem that does not exist for large p's) and become indiscernible from similar objects. Consequently, they are (false) cluster members. This phenomenon (conceptually similar to the "false negatives" indicator) was named "cluster pollution" and the indicator measures the average cluster pollution over all clusters: clustersno doublesno cp clustersno i sizecluster j ij i _ _ _ _ ∑ ∑ = where no_doublesij is the number of indiscernible un-similar objects associated with the j-th element of cluster i.
Remark: Even though there is a certain influence, it could be proven in [5] that no significant correlation exists between parameter p of the quantisation model and cluster pollution.
As pointed out above, to generate the descriptors, the MPEG-7 reference implementation in version 5.6 was used (provided by TU Munich). Image processing was done with Adobe Photoshop and normalisation and all evaluations were done with Perl. The querying process was performed in the following steps: (1) random selection of a ground truth group, (2) random selection of a query object from this group, (3) distance comparison for all other objects in the dataset, (4) clustering of the resulting distance space based on the ground truth and finally, (5) evaluation.
For each combination of dataset and distance measure 250 queries were issued and evaluations were aggregated over all datasets and descriptors. The next section shows the - partially surprisingresults.
In the results presented below the first indicator from Subsection
parameter p had to be set in a way that all measures are equally discriminant. Distance measurement is fair if the following condition holds true for any predicate-based measure dP and any continuous measure dC: ( ) ( )CP dcppdcp ≈,
Then, it is guaranteed that predicate-based measures do not create larger clusters (with a higher number of similar objects) for the price of higher cluster pollution. In more than 1000 test queries the optimum value was found to be p=1.
Results are organised as follows: Subsection 5.1 summarises the Figure 1. Test datasets. Left: Brodatz dataset, middle: Corel dataset, right: coats-of-arms dataset. 135 best distance measures per descriptor, Section 5.2 shows the best overall distance measures and Section 5.3 points out other interesting results (for example, distance measures that work particularly good on specific ground truth groups).
Figure 2 shows the evaluation results for the first indicator. For each descriptor the best measure and the performance of the MPEG-7 recommendation are shown. The results are aggregated over the tested datasets.
On first sight, it becomes clear that the MPEG-7 recommendations are mostly relatively good but never the best.
For Color Layout the difference between MP7 and the best measure, the Meehl index (Q8), is just 4% and the MPEG-7 measure has a smaller standard deviation. The reason why the Meehl index is better may be that this descriptors generates descriptions with elements that have very similar variance.
Statistical analysis confirmed that (see [6]).
For Color Structure, Edge Histogram, Homogeneous Texture,
Region-based Shape and Scalable Color by far the best measure is pattern difference (P6). Psychological research on human visual perception has revealed that in many situation differences between the query object and a candidate weigh much stronger than common properties. The pattern difference measure implements this insight in the most consequent way. In the author's opinion, the reason why pattern difference performs so extremely well on many descriptors is due to this fact. Additional advantages of pattern difference are that it usually has a very low variance andbecause it is a predicate-based measure - its discriminance (and cluster structure) can be tuned with parameter p.
The best measure for Dominant Color turned out to be Clark's Divergence coefficient (Q4). This is a similar measure to pattern difference on the continuous domain. The Texture Browsing descriptor is a special problem. In the MPEG-7 standard it is recommended to use it exclusively for browsing. After testing it for retrieval on various distance measures the author supports this opinion. It is very difficult to find a good distance measure for Texture Browsing. The proposed Manhattan metric, for example, performs very bad. The best measure is predicate-based (P7). It works on common properties (a, d) but produces clusters with very high cluster pollution. For this descriptor the second indicator is up to eight times higher than for predicate-based measures on other descriptors.
Figure 3 summarises the results over all descriptors and media collections. The diagram should give an indication on the general potential of the investigated distance measures for visual information retrieval.
It can be seen that the best overall measure is a predicate-based one. The top performance of pattern difference (P6) proves that the quantisation model is a reasonable method to extend predicate-based distance measures on the continuous domain. The second best group of measures are the MPEG-7 recommendations, which have a slightly higher mean but a lower standard deviation than pattern difference. The third best measure is the Meehl index (Q8), a measure developed for psychological applications but because of its characteristic properties tailormade for certain (homogeneous) descriptors.
Minkowski metrics are also among the best measures: the average mean and variance of the Manhattan metric (Q1) and the Euclidean metric (Q2) are in the range of Q8. Of course, these measures do not perform particularly well for any of the descriptors. Remarkably for a predicate-based measure, Tversky's Feature Contrast Model (P1) is also in the group of very good measures (even though it is not among the best) that ends with Q5, the correlation coefficient. The other measures either have a significantly higher mean or a very large standard deviation.
Distance measures that perform in average worse than others may in certain situations (e.g. on specific content) still perform better.
For Color Layout, for example, Q7 is a very good measure on colour photos. It performs as good as Q8 and has a lower standard deviation. For artificial images the pattern difference and the Hamming distance produce comparable results as well.
If colour information is available in media objects, pattern difference performs well on Dominant Color (just 20% worse Q4) and in case of difficult ground truth (group 5, 7, 10) the Meehl index is as strong as P6. 0,000 0,001 0,002 0,003 0,004 0,005 0,006 0,007 0,008 Q8 MP7 P6 MP7 Q4 MP7 P6 MP7 P6 MP7 P6 MP7 P6 MP7 P7 Q2 Color Layout Color Structure Dominant Color Edge Histogram Homog.
Texture Region Shape Scalable Color Texture Browsing Figure 2. Results per measure and descriptor. The horizontal axis shows the best measure and the performance of the MPEG-7 recommendation for each descriptor. The vertical axis shows the values for the first indicator (smaller value = better cluster structure).
Shades have the following meaning: black=µ-σ (good cases), black + dark grey=µ (average) and black + dark grey + light grey=µ+σ (bad). 136
The evaluation presented in this paper aims at testing the recommended distance measures and finding better ones for the basic visual MPEG-7 descriptors. Eight descriptors were selected,
created and assessed, performance indicators were defined and more than 22500 tests were performed. To be able to use predicate-based distance measures next to quantitative measures a quantisation model was defined that allows the application of predicate-based measures on continuous data.
In the evaluation the best overall distance measures for visual content - as extracted by the visual MPEG-7 descriptors - turned out to be the pattern difference measure and the Meehl index (for homogeneous descriptions). Since these two measures perform significantly better than the MPEG-7 recommendations they should be further tested on large collections of image and video content (e.g. from [15]).
The choice of the right distance function for similarity measurement depends on the descriptor, the queried media collection and the semantic level of the user's idea of similarity.
This work offers suitable distance measures for various situations.
In consequence, the distance measures identified as the best will be implemented in the open MPEG-7 based visual information retrieval framework VizIR [4].
ACKNOWLEDGEMENTS The author would like to thank Christian Breiteneder for his valuable comments and suggestions for improvement. The work presented in this paper is part of the VizIR project funded by the Austrian Scientific Research Fund FWF under grant no. P16111.
REFERENCES [1] Clark, P.S. An extension of the coefficient of divergence for use with multiple characters. Copeia, 2 (1952), 61-64. [2] Cohen, J. A profile similarity coefficient invariant over variable reflection. Psychological Bulletin, 71 (1969),
[3] Del Bimbo, A. Visual information retrieval. Morgan Kaufmann Publishers, San Francisco CA, 1999. [4] Eidenberger, H., and Breiteneder, C. A framework for visual information retrieval. In Proceedings Visual Information Systems Conference (HSinChu Taiwan, March 2002), LNCS 2314, Springer Verlag, 105-116. [5] Eidenberger, H., and Breiteneder, C. Visual similarity measurement with the Feature Contrast Model. In Proceedings SPIE Storage and Retrieval for Media Databases Conference (Santa Clara CA, January 2003), SPIE Vol. 5021, 64-76. [6] Eidenberger, H., How good are the visual MPEG-7 features?
In Proceedings SPIE Visual Communications and Image Processing Conference (Lugano Switzerland, July 2003),
SPIE Vol. 5150, 476-488. [7] Gower, J.G. Multivariate analysis and multidimensional geometry. The Statistician, 17 (1967),13-25. [8] Lance, G.N., and Williams, W.T. Mixed data classificatory programs. Agglomerative Systems Australian Comp. Journal,
[9] Manjunath, B.S., Ohm, J.R., Vasudevan, V.V., and Yamada,
A. Color and texture descriptors. In Special Issue on MPEG7. IEEE Transactions on Circuits and Systems for Video Technology, 11/6 (June 2001), 703-715. [10] Meehl, P. E. The problem is epistemology, not statistics: Replace significance tests by confidence intervals and quantify accuracy of risky numerical predictions. In Harlow,
L.L., Mulaik, S.A., and Steiger, J.H. (Eds.). What if there were no significance tests? Erlbaum, Mahwah NJ, 393-425. [11] Pearson, K. On the coefficients of racial likeness. Biometrica,
[12] Santini, S., and Jain, R. Similarity is a geometer. Multimedia Tools and Application, 5/3 (1997), 277-306. [13] Santini, S., and Jain, R. Similarity measures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21/9 (September 1999), 871-883. [14] Sint, P.P. Similarity structures and similarity measures.

Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12]. In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.
These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).
According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7]. In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web. Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7]. For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.
In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them. Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links). We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.
Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users. Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result. By making the Hidden-Web pages searchable at a central location, we can significantly reduce the user"s wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28]. Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28]. According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort. Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.
Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface. The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented. Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.
Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward. We exhaustively issue all possible queries, one query at a time. When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries. In this case, what queries should we pick? Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?
In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically. We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.
In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions. Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy. Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites. Our experiments will show the relative advantages of various crawling policies and demonstrate their potential. The results from our experiments are very promising. In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries.
In this section, we present a formal framework for the study of the Hidden-Web crawling problem. In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites. Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2. Finally in Section 2.3, we formalize the Hidden-Web crawling problem.
There exists a variety of Hidden Web sources that provide information on a multitude of topics. Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database. A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]). Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1). In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=‘Harry Potter", author=‘J.K. Rowling" and isbn=‘0590353403") and supports multi-attribute search interfaces (Figure 2). In this paper, we will mainly focus on textual databases that support single-attribute keyword queries. We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.
Typically, the users need to take the following steps in order to access pages in a Hidden-Web database:
search interface provided by the Web site (such as the one shown in Figure 1).
with a result index page. That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a).
the pages that look interesting and follows the links. Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at.
Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section. That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages. In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.
In Figure 4 we show the generic algorithm for a Hidden-Web crawler. For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)). Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)). This same process is repeated until all the available resources are used up (Step (1)).
Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next. If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources. In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages. Therefore, how the crawler selects the next query can greatly affect its effectiveness. In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.
Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.
Figure 3: Pages from the PubMed Web site.
ALGORITHM 2.1. Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.
S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem.
Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5). We represent each Web page in S as a point (dots in Figure 5). Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site. Each subset is associated with a weight that represents the cost of issuing the query. Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost). This problem is equivalent to the set-covering problem in graph theory [16].
There are two main difficulties that we need to address in this formalization. First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance. Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage. Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.
In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.
Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned. Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site. We present our prediction method and our query selection algorithm in Section 3.
Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.
Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site. For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3. We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)). Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).
We also use Cost(qi) to represent the cost of issuing the query qi. Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these. As we will see later, our proposed algorithms are independent of the exact cost function.
In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)). We assume that submitting a query incurs a fixed cost of cq. The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed. Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries. In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.
Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.
Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper. When we need a concrete cost function, however, we will use Equation 2.
Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1. Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t.
Here, t is the maximum download resource that the crawler has.
How should a crawler select the queries to issue? Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database. The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword. Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result. We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources. The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents. Based on this analysis, we issue the most promising query, and repeat the process.
Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst. Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic. The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.
We will experimentally compare these three policies in Section 4.
While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy. We address this issue in the rest of this section.
In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query. That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value. In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1. Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi). We may consider a number of different ways to estimate P(qi), including the following:
term qi is independent of the terms q1, . . . , qi−1. That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1).
estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus. Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25]. That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc.), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.
Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset. For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].
After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi). In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table. In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site.
The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources. Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi. For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj. Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable. Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).
Cost(qi) represents the cost of issuing the query qi.
Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site. However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1. Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi. Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value. By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents. In Figure 6, we show the query selection function that uses the concept of efficiency. In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.
We can estimate the efficiency of every query using the estimation method described in Section 3.1. That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3. We can also estimate Cost(qi) similarly. For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi).
In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi. This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm. In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.
The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.
We record these counts in a table, as shown in Figure 7(a). The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term. For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents. Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.
We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents. This update can be done efficiently as we illustrate in the following example.
EXAMPLE 1. After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi. From the new query qi = computer, we downloaded
Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\/ ... \/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk. The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.
We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b). The result is shown on Figure 7(c). For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi. According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3.
results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages. For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query. Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.
First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages. Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1). That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1. However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler. Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries. However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure. Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8). That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.
Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8). Now we need to update our query statistics table so that it has accurate information for the next step. That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3. Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1. The term P(qi+1 ∧ qi) however is unknown and we need to estimate it. Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi).
In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper. Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases. Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.
The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2. To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant. That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries. Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model. In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages. Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26]. Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution). In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M). The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.
The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.
The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2]. According to the information on PubMed"s Web site, its collection contains approximately 14 million abstracts of biomedical articles. We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm. Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed. The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.
In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.
The querying to Amazon is performed through the Software Developer"s Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form. The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply. Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection. Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books. It"s also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.
As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to
The links are searchable through a keyword-search interface. We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying. On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list. As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0
1
fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0
1
fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into. The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed. Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries.
The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites. That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above? More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.
In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively. On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number. A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms. In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.
Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0
1
fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0
1
fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific. For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,. For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites. The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.
Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.
Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies. For each keyword, we present the number of the iteration, along with the number of results that it returned. As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.
In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency. It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results
Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords. On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.
For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11). A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results. Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance. We are currently running another experiment to verify whether this is indeed the case. Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.
As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query. Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.
In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.
When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results . On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g. Amazon). Finally, the random policy performs poorly in general, and should not be preferred.
An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations. The choice of this keyword is not done by the selection of the 0
1
fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet. Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.
For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million. These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low. We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles. As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query. Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query. In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers. Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance. We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site. Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query. Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries.
While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites. In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query. In Figure 14 we plot the coverage for the two policies as a function of the number of queries. As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.
For example, when the result limit was 10,000, the adaptive pol107 0
1
FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000. On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.
The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000. Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit. In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries.
cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost. In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process. As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages. For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website. The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger. We believe that these values are reasonable for the PubMed Web site.
Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process. The horizontal axis is the amount of resources used, and the vertical axis is the coverage. As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.
However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9. The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost. Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before. That 0
1
FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost. Still, we observe noticeable savings from the adaptive policy. At the total cost of 8000, for example, the coverage of the adaptive policy is roughly
In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler. The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically. The potential queries are either provided manually by users or collected from the query interfaces. In contrast, our main focus is to generate queries automatically without any human intervention.
The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.
For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database. In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them. In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes. In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection. Our work differs from the previous studies in two ways. First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work. Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.
Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form. This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.
Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database. In [3] the authors study query-based techniques that can extract relational data from large text databases. Again, these works study orthogonal issues and are complementary to our work.
In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]). These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents. Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.
There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18]. This body of work is often referred to as meta-searching or database selection problem over the Hidden Web. For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category. Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location.
Traditional crawlers normally follow links on the Web to discover and download pages. Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces. In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it. We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site. Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential. In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries. Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search.
We briefly discuss some future-research avenues.
Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases. While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes. For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.
Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = ‘Harry Potter", author = ‘J.K. Rowling", etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one. The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute. Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.
Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler. For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites. But how can the crawler discover the query interfaces? The method proposed in [15] may be a good starting point. In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results. In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically. Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day). In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site. Page similarity detection algorithms may be useful for this purpose [9, 13].

Localized search engines are small-scale search engines that index only a single community of the web. Such communities can be site-specific domains, such as pages within the cs.utexas.edu domain, or topic-related communitiesfor example, political websites. Compared to the web graph crawled and indexed by large-scale search engines, the size of such local communities is typically orders of magnitude smaller. Consequently, the computational resources needed to build such a search engine are also similarly lighter. By restricting themselves to smaller, more manageable sections of the web, localized search engines can also provide more precise and complete search capabilities over their respective domains.
One drawback of localized indexes is the lack of global information needed to compute link-based rankings. The PageRank algorithm [3], has proven to be an effective such measure. In general, the PageRank of a given page is dependent on pages throughout the entire web graph. In the context of a localized search engine, if the PageRanks are computed using only the local subgraph, then we would expect the resulting PageRanks to reflect the perceived popularity within the local community and not of the web as a whole. For example, consider a localized search engine that indexes political pages with conservative views. A person wishing to research the opinions on global warming within the conservative political community may encounter numerous such opinions across various websites. If only local PageRank values are available, then the search results will reflect only strongly held beliefs within the community. However, if global PageRanks are also available, then the results can additionally reflect outsiders" views of the conservative community (those documents that liberals most often access within the conservative community).
Thus, for many localized search engines, incorporating global PageRanks can improve the quality of search results.
However, the number of pages a local search engine indexes is typically orders of magnitude smaller than the number of pages indexed by their large-scale counterparts. Localized search engines do not have the bandwidth, storage capacity, or computational power to crawl, download, and compute the global PageRanks of the entire web. In this work, we present a method of approximating the global PageRanks of a local domain while only using resources of the same order as those needed to compute the PageRanks of the local subgraph.
Our proposed method looks for a supergraph of our local subgraph such that the local PageRanks within this supergraph are close to the true global PageRanks. We construct this supergraph by iteratively crawling global pages on the current web frontier-i.e., global pages with inlinks from pages that have already been crawled. In order to provide 116 Research Track Paper a good approximation to the global PageRanks, care must be taken when choosing which pages to crawl next; in this paper, we present a well-motivated page selection algorithm that also performs well empirically. This algorithm is derived from a well-defined problem objective and has a running time linear in the number of local nodes.
We experiment across several types of local subgraphs, including four topic related communities and several sitespecific domains. To evaluate performance, we measure the difference between the current global PageRank estimate and the global PageRank, as a function of the number of pages crawled. We compare our algorithm against several heuristics and also against a baseline algorithm that chooses pages at random, and we show that our method outperforms these other methods. Finally, we empirically demonstrate that, given a local domain of size n, we can provide good approximations to the global PageRank values by crawling at most n or 2n additional pages.
The paper is organized as follows. Section 2 gives an overview of localized search engines and outlines their advantages over global search. Section 3 provides background on the PageRank algorithm. Section 4 formally defines our problem, and section 5 presents our page selection criteria and derives our algorithms. Section 6 provides experimental results, section 7 gives an overview of related work, and, finally, conclusions are given in section 8.
Localized search engines index a single community of the web, typically either a site-specific community, or a topicspecific community. Localized search engines enjoy three major advantages over their large-scale counterparts: they are relatively inexpensive to build, they can offer more precise search capability over their local domain, and they can provide a more complete index.
The resources needed to build a global search engine are enormous. A 2003 study by Lyman et al. [13] found that the ‘surface web" (publicly available static sites) consists of
approximately 18.7 kilobytes. To download a crawl of this size, approximately 167 terabytes of space is needed. For a researcher who wishes to build a search engine with access to a couple of workstations or a small server, storage of this magnitude is simply not available. However, building a localized search engine over a web community of a hundred thousand pages would only require a few gigabytes of storage. The computational burden required to support search queries over a database this size is more manageable as well.
We note that, for topic-specific search engines, the relevant community can be efficiently identified and downloaded by using a focused crawler [21, 4].
For site-specific domains, the local domain is readily available on their own web server. This obviates the need for crawling or spidering, and a complete and up-to-date index of the domain can thus be guaranteed. This is in contrast to their large-scale counterparts, which suffer from several shortcomings. First, crawling dynamically generated pages-pages in the ‘hidden web"-has been the subject of research [20] and is a non-trivial task for an external crawler.
Second, site-specific domains can enable the robots exclusion policy. This prohibits external search engines" crawlers from downloading content from the domain, and an external search engine must instead rely on outside links and anchor text to index these restricted pages.
By restricting itself to only a specific domain of the internet, a localized search engine can provide more precise search results. Consider the canonical ambiguous search query, ‘jaguar", which can refer to either the car manufacturer or the animal. A scientist trying to research the habitat and evolutionary history of a jaguar may have better success using a finely tuned zoology-specific search engine than querying Google with multiple keyword searches and wading through irrelevant results. A method to learn better ranking functions for retrieval was recently proposed by Radlinski and Joachims [19] and has been applied to various local domains, including Cornell University"s website [8].
The PageRank algorithm defines the importance of web pages by analyzing the underlying hyperlink structure of a web graph. The algorithm works by building a Markov chain from the link structure of the web graph and computing its stationary distribution. One way to compute the stationary distribution of a Markov chain is to find the limiting distribution of a random walk over the chain. Thus, the PageRank algorithm uses what is sometimes referred to as the ‘random surfer" model. In each step of the random walk, the ‘surfer" either follows an outlink from the current page (i.e. the current node in the chain), or jumps to a random page on the web.
We now precisely define the PageRank problem. Let U be an m × m adjacency matrix for a given web graph such that Uji = 1 if page i links to page j and Uji = 0 otherwise.
We define the PageRank matrix PU to be: PU = αUD−1 U + (1 − α)veT , (1) where DU is the (unique) diagonal matrix such that UD−1 U is column stochastic, α is a given scalar such that 0 ≤ α ≤ 1, e is the vector of all ones, and v is a non-negative,
L1normalized vector, sometimes called the ‘random surfer" vector. Note that the matrix D−1 U is well-defined only if each column of U has at least one non-zero entry-i.e., each page in the webgraph has at least one outlink. In the presence of such ‘dangling nodes" that have no outlinks, one commonly used solution, proposed by Brin et al. [3], is to replace each zero column of U by a non-negative, L1-normalized vector.
The PageRank vector r is the dominant eigenvector of the PageRank matrix, r = PU r. We will assume, without loss of generality, that r has an L1-norm of one. Computationally, r can be computed using the power method. This method first chooses a random starting vector r(0) , and iteratively multiplies the current vector by the PageRank matrix PU ; see Algorithm 1. In general, each iteration of the power method can take O(m2 ) operations when PU is a dense matrix. However, in practice, the number of links in a web graph will be of the order of the number of pages. By exploiting the sparsity of the PageRank matrix, the work per iteration can be reduced to O(km), where k is the average number of links per web page. It has also been shown that the total number of iterations needed for convergence is proportional to α and does not depend on the size of the web graph [11, 7]. Finally, the total space needed is also O(km), mainly to store the matrix U. 117 Research Track Paper Algorithm 1: A linear time (per iteration) algorithm for computing PageRank.
ComputePR(U) Input: U: Adjacency matrix.
Output: r: PageRank vector.
Choose (randomly) an initial non-negative vector r(0) such that r(0)
i ← 0 repeat i ← i + 1 ν ← αUD−1 U r(i−1) {α is the random surfing probability} r(i) ← ν + (1 − α)v {v is the random surfer vector.} until r(i) − r(i−1) < δ {δ is the convergence threshold.} r ← r(i)
Given a local domain L, let G be an N × N adjacency matrix for the entire connected component of the web that contains L, such that Gji = 1 if page i links to page j and Gji = 0 otherwise. Without loss of generality, we will partition G as: G = L Gout Lout Gwithin , (2) where L is the n × n local subgraph corresponding to links inside the local domain, Lout is the subgraph that corresponds to links from the local domain pointing out to the global domain, Gout is the subgraph containing links from the global domain into the local domain, and Gwithin contains links within the global domain. We assume that when building a localized search engine, only pages inside the local domain are crawled, and the links between these pages are represented by the subgraph L. The links in Lout are also known, as these point from crawled pages in the local domain to uncrawled pages in the global domain.
As defined in equation (1), PG is the PageRank matrix formed from the global graph G, and we define the global PageRank vector of this graph to be g. Let the n-length vector p∗ be the L1-normalized vector corresponding to the global PageRank of the pages in the local domain L: p∗ = EL g ELg 1 , where EL = [ I | 0 ] is the restriction matrix that selects the components from g corresponding to nodes in L. Let p denote the PageRank vector constructed from the local domain subgraph L. In practice, the observed local PageRank p and the global PageRank p∗ will be quite different. One would expect that as the size of local matrix L approaches the size of global matrix G, the global PageRank and the observed local PageRank will become more similar. Thus, one approach to estimating the global PageRank is to crawl the entire global domain, compute its PageRank, and extract the PageRanks of the local domain.
Typically, however, n N , i.e., the number of global pages is much larger than the number of local pages.
Therefore, crawling all global pages will quickly exhaust all local resources (computational, storage, and bandwidth) available to create the local search engine. We instead seek a supergraph ˆF of our local subgraph L with size O(n). Our goal Algorithm 2: The FindGlobalPR algorithm.
FindGlobalPR(L, Lout, T, k) Input: L: zero-one adjacency matrix for the local domain, Lout: zero-one outlink matrix from L to global subgraph as in (2), T: number of iterations, k: number of pages to crawl per iteration.
Output: ˆp: an improved estimate of the global PageRank of L.
F ← L Fout ← Lout f ← ComputePR(F ) for (i = 1 to T) {Determine which pages to crawl next} pages ← SelectNodes(F , Fout, f, k) Crawl pages, augment F and modify Fout {Update PageRanks for new local domain} f ← ComputePR(F ) end {Extract PageRanks of original local domain & normalize} ˆp ← ELf ELf 1 is to find such a supergraph ˆF with PageRank ˆf, so that ˆf when restricted to L is close to p∗ . Formally, we seek to minimize GlobalDiff( ˆf) = EL ˆf EL ˆf 1 − p∗ 1 . (3) We choose the L1 norm for measuring the error as it does not place excessive weight on outliers (as the L2 norm does, for example), and also because it is the most commonly used distance measure in the literature for comparing PageRank vectors, as well as for detecting convergence of the algorithm [3].
We propose a greedy framework, given in Algorithm 2, for constructing ˆF . Initially, F is set to the local subgraph L, and the PageRank f of this graph is computed. The algorithm then proceeds as follows. First, the SelectNodes algorithm (which we discuss in the next section) is called and it returns a set of k nodes to crawl next from the set of nodes in the current crawl frontier, Fout. These selected nodes are then crawled to expand the local subgraph, F , and the PageRanks of this expanded graph are then recomputed.
These steps are repeated for each of T iterations. Finally, the PageRank vector ˆp, which is restricted to pages within the original local domain, is returned. Given our computation, bandwidth, and memory restrictions, we will assume that the algorithm will crawl at most O(n) pages. Since the PageRanks are computed in each iteration of the algorithm, which is an O(n) operation, we will also assume that the number of iterations T is a constant. Of course, the main challenge here is in selecting which set of k nodes to crawl next. In the next section, we formally define the problem and give efficient algorithms.
In this section, we present node selection algorithms that operate within the greedy framework presented in the previous section. We first give a well-defined criteria for the page selection problem and provide experimental evidence that this criteria can effectively identify pages that optimize our problem objective (3). We then present our main al118 Research Track Paper gorithmic contribution of the paper, a method with linear running time that is derived from this page selection criteria. Finally, we give an intuitive analysis of our algorithm in terms of ‘leaks" and ‘flows". We show that if only the ‘flow" is considered, then the resulting method is very similar to a widely used page selection heuristic [6].
For a given page j in the global domain, we define the expanded local graph Fj: Fj = F s uT j 0 , (4) where uj is the zero-one vector containing the outlinks from F into page j, and s contains the inlinks from page j into the local domain. Note that we do not allow self-links in this framework. In practice, self-links are often removed, as they only serve to inflate a given page"s PageRank.
Observe that the inlinks into F from node j are not known until after node j is crawled. Therefore, we estimate this inlink vector as the expectation over inlink counts among the set of already crawled pages, s = F T e F T e 1 . (5) In practice, for any given page, this estimate may not reflect the true inlinks from that page. Furthermore, this expectation is sampled from the set of links within the crawled domain, whereas a better estimate would also use links from the global domain. However, the latter distribution is not known to a localized search engine, and we contend that the above estimate will, on average, be a better estimate than the uniform distribution, for example.
Let the PageRank of F be f. We express the PageRank f+ j of the expanded local graph Fj as f+ j = (1 − xj)fj xj , (6) where xj is the PageRank of the candidate global node j, and fj is the L1-normalized PageRank vector restricted to the pages in F .
Since directly optimizing our problem goal requires knowing the global PageRank p∗ , we instead propose to crawl those nodes that will have the greatest influence on the PageRanks of pages in the original local domain L: influence(j) = k∈L |fj[k] − f[k]| (7) = EL (fj − f) 1.
Experimentally, the influence score is a very good predictor of our problem objective (3). For each candidate global node j, figure 1(a) shows the objective function value Global Diff(fj) as a function of the influence of page j. The local domain used here is a crawl of conservative political pages (we will provide more details about this dataset in section 6); we observed similar results in other domains. The correlation is quite strong, implying that the influence criteria can effectively identify pages that improve the global PageRank estimate. As a baseline, figure 1(b) compares our objective with an alternative criteria, outlink count. The outlink count is defined as the number of outlinks from the local domain to page j. The correlation here is much weaker. .00001 .0001 .001 .01
Influence Objective
Outlink Count Objective (a) (b) Figure 1: (a) The correlation between our influence page selection criteria (7) and the actual objective function (3) value is quite strong. (b) This is in contrast to other criteria, such as outlink count, which exhibit a much weaker correlation.
As described, for each candidate global page j, the influence score (7) must be computed. If fj is computed exactly for each global page j, then the PageRank algorithm would need to be run for each of the O(n) such global pages j we consider, resulting in an O(n2 ) computational cost for the node selection method. Thus, computing the exact value of fj will lead to a quadratic algorithm, and we must instead turn to methods of approximating this vector.
The algorithm we present works by performing one power method iteration used by the PageRank algorithm (Algorithm 1). The convergence rate for the PageRank algorithm has been shown to equal the random surfer probability α [7, 11]. Given a starting vector x(0) , if k PageRank iterations are performed, the current PageRank solution x(k) satisfies: x(k) − x∗
x(0) − x∗ 1), (8) where x∗ is the desired PageRank vector. Therefore, if only one iteration is performed, choosing a good starting vector is necessary to achieve an accurate approximation.
We partition the PageRank matrix PFj , corresponding to the × subgraph Fj as: PFj = ˜F ˜s ˜uT j w , (9) where ˜F = αF (DF + diag(uj))−1 + (1 − α) e + 1 eT , ˜s = αs + (1 − α) e + 1 , ˜uj = α(DF + diag(uj))−1 uj + (1 − α) e + 1 , w =
+ 1 , and diag(uj) is the diagonal matrix with the (i, i)th entry equal to one if the ith element of uj equals one, and is zero otherwise. We have assumed here that the random surfer vector is the uniform vector, and that L has no ‘dangling links". These assumptions are not necessary and serve only to simplify discussion and analysis.
A simple approach for estimating fj is the following. First, estimate the PageRank f+ j of Fj by computing one PageRank iteration over the matrix PFj , using the starting vector ν = f 0 . Then, estimate fj by removing the last 119 Research Track Paper component from our estimate of f+ j (i.e., the component corresponding to the added node j), and renormalizing.
The problem with this approach is in the starting vector.
Recall from (6) that xj is the PageRank of the added node j. The difference between the actual PageRank f+ j of PFj and the starting vector ν is ν − f+ j 1 = xj + f − (1 − xj)fj 1 ≥ xj + | f 1 − (1 − xj) fj 1| = xj + |xj| = 2xj.
Thus, by (8), after one PageRank iteration, we expect our estimate of f+ j to still have an error of about 2αxj. In particular, for candidate nodes j with relatively high PageRank xj, this method will yield more inaccurate results. We will next present a method that eliminates this bias and runs in O(n) time.
Since f+ j , as given in (6) is the PageRank of the matrix PFj , we have: fj(1 − xj) xj = ˜F ˜s ˜uT j w fj(1 − xj) xj = ˜F fj(1 − xj) + ˜sxj ˜uT j fj(1 − xj) + wxj .
Solving the above system for fj can be shown to yield fj = ( ˜F + (1 − w)−1 ˜s˜uT j )fj. (10) The matrix S = ˜F +(1−w)−1 ˜s˜uT j is known as the stochastic complement of the column stochastic matrix PFj with respect to the sub matrix ˜F . The theory of stochastic complementation is well studied, and it can be shown the stochastic complement of an irreducible matrix (such as the PageRank matrix) is unique. Furthermore, the stochastic complement is also irreducible and therefore has a unique stationary distribution as well. For an extensive study, see [15].
It can be easily shown that the sub-dominant eigenvalue of S is at most +1 α, where is the size of F . For sufficiently large , this value will be very close to α. This is important, as other properties of the PageRank algorithm, notably the algorithm"s sensitivity, are dependent on this value [11].
In this method, we estimate the length vector fj by computing one PageRank iteration over the × stochastic complement S, starting at the vector f: fj ≈ Sf. (11) This is in contrast to the simple method outlined in the previous section, which first iterates over the ( + 1) × ( + 1) matrix PFj to estimate f+ j , and then removes the last component from the estimate and renormalizes to approximate fj. The problem with the latter method is in the choice of the ( + 1) length starting vector, ν. Consequently, the PageRank estimate given by the simple method differs from the true PageRank by at least 2αxj, where xj is the PageRank of page j. By using the stochastic complement, we can establish a tight lower bound of zero for this difference.
To see this, consider the case in which a node k is added to F to form the augmented local subgraph Fk, and that the PageRank of this new graph is (1 − xk)f xk .
Specifically, the addition of page k does not change the PageRanks of the pages in F , and thus fk = f. By construction of the stochastic complement, fk = Sfk, so the approximation given in equation (11) will yield the exact solution.
Next, we present the computational details needed to efficiently compute the quantity fj −f 1 over all known global pages j. We begin by expanding the difference fj −f, where the vector fj is estimated as in (11), fj − f ≈ Sf − f = αF (DF + diag(uj))−1 f + (1 − α) e + 1 eT f +(1 − w)−1 (˜uT j f)˜s − f. (12) Note that the matrix (DF +diag(uj))−1 is diagonal. Letting o[k] be the outlink count for page k in F , we can express the kth diagonal element as: (DF + diag(uj))−1 [k, k] = 1 o[k]+1 if uj[k] = 1 1 o[k] if uj[k] = 0 Noting that (o[k] + 1)−1 = o[k]−1 − (o[k](o[k] + 1))−1 and rewriting this in matrix form yields (DF +diag(uj))−1 = D−1 F −D−1 F (DF +diag(uj))−1 diag(uj). (13) We use the same identity to express e + 1 = e − e ( + 1) . (14) Recall that, by definition, we have PF = αF D−1 F +(1−α)e .
Substituting (13) and (14) in (12) yields fj − f ≈ (PF f − f) −αF D−1 F (DF + diag(uj))−1 diag(uj)f −(1 − α) e ( + 1) + (1 − w)−1 (˜uT j f)˜s = x + y + (˜uT j f)z, (15) noting that by definition, f = PF f, and defining the vectors x, y, and z to be x = −αF D−1 F (DF + diag(uj))−1 diag(uj)f (16) y = −(1 − α) e ( + 1) (17) z = (1 − w)−1 ˜s. (18) The first term x is a sparse vector, and takes non-zero values only for local pages k that are siblings of the global page j. We define (i, j) ∈ F if and only if F [j, i] = 1 (equivalently, page i links to page j) and express the value of the component x[k ] as: x[k ] = −α k:(k,k )∈F ,uj [k]=1 f[k] o[k](o[k] + 1) , (19) where o[k], as before, is the number of outlinks from page k in the local domain. Note that the last two terms, y and z are not dependent on the current global node j. Given the function hj(f) = y + (˜uT j f)z 1, the quantity fj − f 1 120 Research Track Paper can be expressed as fj − f 1 = k x[k] + y[k] + (˜uT j f)z[k] = k:x[k]=0 y[k] + (˜uT j f)z[k] + k:x[k]=0 x[k] + y[k] + (˜uT j f)z[k] = hj(f) − k:x[k]=0 y[k] + (˜uT j f)z[k] + k:x[k]=0 x[k] + y[k] + (˜uT j f)z[k] .(20) If we can compute the function hj in linear time, then we can compute each value of fj − f 1 using an additional amount of time that is proportional to the number of nonzero components in x. These optimizations are carried out in Algorithm 3. Note that (20) computes the difference between all components of f and fj, whereas our node selection criteria, given in (7), is restricted to the components corresponding to nodes in the original local domain L.
Let us examine Algorithm 3 in more detail. First, the algorithm computes the outlink counts for each page in the local domain. The algorithm then computes the quantity ˜uT j f for each known global page j. This inner product can be written as (1 − α) 1 + 1 + α k:(k,j)∈Fout f[k] o[k] + 1 , where the second term sums over the set of local pages that link to page j. Since the total number of edges in Fout was assumed to have size O( ) (recall that is the number of pages in F ), the running time of this step is also O( ).
The algorithm then computes the vectors y and z, as given in (17) and (18), respectively. The L1NormDiff method is called on the components of these vectors which correspond to the pages in L, and it estimates the value of EL(y + (˜uT j f)z) 1 for each page j. The estimation works as follows. First, the values of ˜uT j f are discretized uniformly into c values {a1, ..., ac}. The quantity EL(y + aiz) 1 is then computed for each discretized value of ai and stored in a table. To evaluate EL (y + az) 1 for some a ∈ [a1, ac], the closest discretized value ai is determined, and the corresponding entry in the table is used. The total running time for this method is linear in and the discretization parameter c (which we take to be a constant). We note that if exact values are desired, we have also developed an algorithm that runs in O( log ) time that is not described here.
In the main loop, we compute the vector x, as defined in equation (16). The nested loops iterate over the set of pages in F that are siblings of page j. Typically, the size of this set is bounded by a constant. Finally, for each page j, the scores vector is updated over the set of non-zero components k of the vector x with k ∈ L. This set has size equal to the number of local siblings of page j, and is a subset of the total number of siblings of page j. Thus, each iteration of the main loop takes constant time, and the total running time of the main loop is O( ). Since we have assumed that the size of F will not grow larger than O(n), the total running time for the algorithm is O(n).
Algorithm 3: Node Selection via Stochastic Complementation.
SC-Select(F , Fout, f, k) Input: F : zero-one adjacency matrix of size corresponding to the current local subgraph, Fout: zero-one outlink matrix from F to global subgraph, f: PageRank of F , k: number of pages to return Output: pages: set of k pages to crawl next {Compute outlink sums for local subgraph} foreach (page j ∈ F ) o[j] ← k:(j,k)∈F F[j, k] end {Compute scalar ˜uT j f for each global node j } foreach (page j ∈ Fout) g[j] ← (1 − α) 1 +1 foreach (page k : (k, j) ∈ Fout) g[j] ← g[j] + α f[k] o[k]+1 end end {Compute vectors y and z as in (17) and (18) } y ← −(1 − α) e ( +1) z ← (1 − w)−1 ˜s {Approximate y + g[j] ∗ z 1 for all values g[j]} norm diffs ←L1NormDiffs(g, ELy, ELz) foreach (page j ∈ Fout) {Compute sparse vector x as in (19)} x ← 0 foreach (page k : (k, j) ∈ Fout) foreach (page k : (k, k ) ∈ F )) x[k ] ← x[k ] − f[k] o[k](o[k]+1) end end x ← αx scores[j] ← norm diffs[j] foreach (k : x[k] > 0 and page k ∈ L) scores[j] ← scores[j] − |y[k] + g[j] ∗ z[k]| +|x[k]+y[k]+g[j]∗z[k])| end end Return k pages with highest scores
We now present an intuitive analysis of the stochastic complementation method by decomposing the change in PageRank in terms of ‘leaks" and ‘flows". This analysis is motivated by the decomposition given in (15). PageRank ‘flow" is the increase in the local PageRanks originating from global page j. The flows are represented by the non-negative vector (˜uT j f)z (equations (15) and (18)). The scalar ˜uT j f can be thought of as the total amount of PageRank flow that page j has available to distribute. The vector z dictates how the flow is allocated to the local domain; the flow that local page k receives is proportional to (within a constant factor due to the random surfer vector) the expected number of its inlinks.
The PageRank ‘leaks" represent the decrease in PageRank resulting from the addition of page j. The leakage can be quantified in terms of the non-positive vectors x and y (equations (16) and (17)). For vector x, we can see from equation (19) that the amount of PageRank leaked by a local page is proportional to the weighted sum of the Page121 Research Track Paper Ranks of its siblings. Thus, pages that have siblings with higher PageRanks (and low outlink counts) will experience more leakage. The leakage caused by y is an artifact of the random surfer vector.
We will next show that if only the ‘flow" term, (˜uT j f)z, is considered, then the resulting method is very similar to a heuristic proposed by Cho et al. [6] that has been widely used for the Crawling Through URL Ordering problem.
This heuristic is computationally cheaper, but as we will see later, not as effective as the Stochastic Complementation method.
Our node selection strategy chooses global nodes that have the largest influence (equation (7)). If this influence is approximated using only ‘flows", the optimal node j∗ is: j∗ = argmaxj EL ˜uT j fz 1 = argmaxj ˜uT j f EL z 1 = argmaxj ˜uT j f = argmaxj α(DF + diag(uj))−1 uj + (1 − α) e + 1 , f = argmaxjfT (DF + diag(uj))−1 uj.
The resulting page selection score can be expressed as a sum of the PageRanks of each local page k that links to j, where each PageRank value is normalized by o[k]+1. Interestingly, the normalization that arises in our method differs from the heuristic given in [6], which normalizes by o[k]. The algorithm PF-Select, which is omitted due to lack of space, first computes the quantity fT (DF +diag(uj))−1 uj for each global page j, and then returns the pages with the k largest scores. To see that the running time for this algorithm is O(n), note that the computation involved in this method is a subset of that needed for the SC-Select method (Algorithm 3), which was shown to have a running time of O(n).
In this section, we provide experimental evidence to verify the effectiveness of our algorithms. We first outline our experimental methodology and then provide results across a variety of local domains.
Given the limited resources available at an academic institution, crawling a section of the web that is of the same magnitude as that indexed by Google or Yahoo! is clearly infeasible. Thus, for a given local domain, we approximate the global graph by crawling a local neighborhood around the domain that is several orders of magnitude larger than the local subgraph. Even though such a graph is still orders of magnitude smaller than the ‘true" global graph, we contend that, even if there exist some highly influential pages that are very far away from our local domain, it is unrealistic for any local node selection algorithm to find them. Such pages also tend to be highly unrelated to pages within the local domain.
When explaining our node selection strategies in section 5, we made the simplifying assumption that our local graph contained no dangling nodes. This assumption was only made to ease our analysis. Our implementation efficiently handles dangling links by replacing each zero column of our adjacency matrix with the uniform vector. We evaluate the algorithm using the two node selection strategies given in Section 5.2, and also against the following baseline methods: • Random: Nodes are chosen uniformly at random among the known global nodes. • OutlinkCount: Global nodes with the highest number of outlinks from the local domain are chosen.
At each iteration of the FindGlobalPR algorithm, we evaluate performance by computing the difference between the current PageRank estimate of the local domain, ELf ELf 1 , and the global PageRank of the local domain ELg ELg 1 . All PageRank calculations were performed using the uniform random surfer vector. Across all experiments, we set the random surfer parameter α, to be .85, and used a convergence threshold of 10−6 . We evaluate the difference between the local and global PageRank vectors using three different metrics: the L1 and L∞ norms, and Kendall"s tau. The L1 norm measures the sum of the absolute value of the differences between the two vectors, and the L∞ norm measures the absolute value of the largest difference. Kendall"s tau metric is a popular rank correlation measure used to compare PageRanks [2, 11]. This metric can be computed by counting the number of pairs of pairs that agree in ranking, and subtracting from that the number of pairs of pairs that disagree in ranking. The final value is then normalized by the total number of n 2 such pairs, resulting in a [−1, 1] range, where a negative score signifies anti-correlation among rankings, and values near one correspond to strong rank correlation.
Our experiments are based on two large web crawls and were downloaded using the web crawler that is part of the Nutch open source search engine project [18]. All crawls were restricted to only ‘http" pages, and to limit the number of dynamically generated pages that we crawl, we ignored all pages with urls containing any of the characters ‘?", ‘*", ‘@", or ‘=". The first crawl, which we will refer to as the ‘edu" dataset, was seeded by homepages of the top
rated by the US News and World Report [16], and also by the home pages of their respective institutions. A crawl of depth 5 was performed, restricted to pages within the ‘.edu" domain, resulting in a graph with approximately 4.7 million pages and 22.9 million links. The second crawl was seeded by the set of pages under the ‘politics" hierarchy in the dmoz open directory project[17]. We crawled all pages up to four links away, which yielded a graph with 4.4 million pages and
Within the ‘edu" crawl, we identified the five site-specific domains corresponding to the websites of the top five graduate computer science departments, as ranked by the US News and World Report. This yielded local domains of various sizes, from 10,626 (UIUC) to 59,895 (Berkeley). For each of these site-specific domains with size n, we performed 50 iterations of the FindGlobalPR algorithm to crawl a total of 2n additional nodes. Figure 2(a) gives the (L1) difference from the PageRank estimate at each iteration to the global PageRank, for the Berkeley local domain.
The performance of this dataset was representative of the typical performance across the five computer science sitespecific local domains. Initially, the L1 difference between the global and local PageRanks ranged from .0469 (Stanford) to .149 (MIT). For the first several iterations, the 122 Research Track Paper
Number of Iterations GlobalandLocalPageRankDifference(L1) Stochastic Complement PageRank Flow Outlink Count Random
0
Number of Iterations GlobalandLocalPageRankDifference(L1) Stochastic Complement PageRank Flow Outlink Count Random
Number of Iterations GlobalandLocalPageRankDifference(L1) Stochastic Complement PageRank Flow Outlink Count Random (a) www.cs.berkeley.edu (b) www.enterstageright.com (c) Politics Figure 2: L1 difference between the estimated and true global PageRanks for (a) Berkeley"s computer science website, (b) the site-specific domain, www.enterstageright.com, and (c) the ‘politics" topic-specific domain. The stochastic complement method outperforms all other methods across various domains. three link-based methods all outperform the random selection heuristic. After these initial iterations, the random heuristic tended to be more competitive with (or even outperform, as in the Berkeley local domain) the outlink count and PageRank flow heuristics. In all tests, the stochastic complementation method either outperformed, or was competitive with, the other methods. Table 1 gives the average difference between the final estimated global PageRanks and the true global PageRanks for various distance measures.
Algorithm L1 L∞ Kendall Stoch. Comp. .0384 .00154 .9257 PR Flow .0470 .00272 .8946 Outlink .0419 .00196 .9053 Random .0407 .00204 .9086 Table 1: Average final performance of various node selection strategies for the five site-specific computer science local domains. Note that Kendall"s Tau measures similarity, while the other metrics are dissimilarity measures. Stochastic Complementation clearly outperforms the other methods in all metrics.
Within the ‘politics" dataset, we also performed two sitespecific tests for the largest websites in the crawl: www.adamsmith.org, the website for the London based Adam Smith Institute, and www.enterstageright.com, an online conservative journal. As with the ‘edu" local domains, we ran our algorithm for 50 iterations, crawling a total of 2n nodes.
Figure 2 (b) plots the results for the www.enterstageright.com domain. In contrast to the ‘edu" local domains, the Random and OutlinkCount methods were not competitive with either the SC-Select or the PF-Select methods. Among all datasets and all node selection methods, the stochastic complementation method was most impressive in this dataset, realizing a final estimate that differed only .0279 from the global PageRank, a ten-fold improvement over the initial local PageRank difference of .299. For the Adam Smith local domain, the initial difference between the local and global PageRanks was .148, and the final estimates given by the SC-Select, PF-Select, OutlinkCount, and Random methods were .0208, .0193, .0222, and .0356, respectively.
Within the ‘politics" dataset, we constructed four topicspecific local domains. The first domain consisted of all pages in the dmoz politics category, and also all pages within each of these sites up to two links away. This yielded a local domain of 90,811 pages, and the results are given in figure 2 (c). Because of the larger size of the topic-specific domains, we ran our algorithm for only 25 iterations to crawl a total of n nodes.
We also created topic-specific domains from three political sub-topics: liberalism, conservatism, and socialism. The pages in these domains were identified by their corresponding dmoz categories. For each sub-topic, we set the local domain to be all pages within three links from the corresponding dmoz category pages. Table 2 summarizes the performance of these three topic-specific domains, and also the larger political domain.
To quantify a global page j"s effect on the global PageRank values of pages in the local domain, we define page j"s impact to be its PageRank value, g[j], normalized by the fraction of its outlinks pointing to the local domain: impact(j) = oL[j] o[j] · g[j], where, oL[j] is the number of outlinks from page j to pages in the local domain L, and o[j] is the total number of j"s outlinks. In terms of the random surfer model, the impact of page j is the probability that the random surfer (1) is currently at global page j in her random walk and (2) takes an outlink to a local page, given that she has already decided not to jump to a random page.
For the politics local domain, we found that many of the pages with high impact were in fact political pages that should have been included in the dmoz politics topic, but were not. For example, the two most influential global pages were the political search engine www.askhenry.com, and the home page of the online political magazine, www.policyreview.com. Among non-political pages, the home page of the journal Education Next was most influential. The journal is freely available online and contains articles regarding various aspect of K-12 education in America. To provide some anecdotal evidence for the effectiveness of our page selection methods, we note that the SC-Select method chose
PF-Select method discovered 7 such pages, while the OutlinkCount and Random methods found only 6 pages each.
For the conservative political local domain, the socialist website www.ornery.org had a very high impact score. This 123 Research Track Paper All Politics: Algorithm L1 L2 Kendall Stoch. Comp. .1253 .000700 .8671 PR Flow .1446 .000710 .8518 Outlink .1470 .00225 .8642 Random .2055 .00203 .8271 Conservativism: Algorithm L1 L2 Kendall Stoch. Comp. .0496 .000990 .9158 PR Flow .0554 .000939 .9028 Outlink .0602 .00527 .9144 Random .1197 .00102 .8843 Liberalism: Algorithm L1 L2 Kendall Stoch. Comp. .0622 .001360 .8848 PR Flow .0799 .001378 .8669 Outlink .0763 .001379 .8844 Random .1127 .001899 .8372 Socialism: Algorithm L1 L∞ Kendall Stoch. Comp. .04318 .00439 .9604 PR Flow .0450 .004251 .9559 Outlink .04282 .00344 .9591 Random .0631 .005123 .9350 Table 2: Final performance among node selection strategies for the four political topic-specific crawls.
Note that Kendall"s Tau measures similarity, while the other metrics are dissimilarity measures. was largely due to a link from the front page of this site to an article regarding global warming published by the National Center for Public Policy Research, a conservative research group in Washington, DC. Not surprisingly, the global PageRank of this article (which happens to be on the home page of the NCCPR, www.nationalresearch.com), was approximately .002, whereas the local PageRank of this page was only .00158. The SC-Select method yielded a global PageRank estimate of approximately .00182, the PFSelect method estimated a value of .00167, and the Random and OutlinkCount methods yielded values of .01522 and .00171, respectively.
The node selection framework we have proposed is similar to the url ordering for crawling problem proposed by Cho et al. in [6]. Whereas our framework seeks to minimize the difference between the global and local PageRank, the objective used in [6] is to crawl the most highly (globally) ranked pages first. They propose several node selection algorithms, including the outlink count heuristic, as well as a variant of our PF-Select algorithm which they refer to as the ‘PageRank ordering metric". They found this method to be most effective in optimizing their objective, as did a recent survey of these methods by Baeza-Yates et al. [1]. Boldi et al. also experiment within a similar crawling framework in [2], but quantify their results by comparing Kendall"s rank correlation between the PageRanks of the current set of crawled pages and those of the entire global graph. They found that node selection strategies that crawled pages with the highest global PageRank first actually performed worse (with respect to Kendall"s Tau correlation between the local and global PageRanks) than basic depth first or breadth first strategies. However, their experiments differ from our work in that our node selection algorithms do not use (or have access to) global PageRank values.
Many algorithmic improvements for computing exact PageRank values have been proposed [9, 10, 14]. If such algorithms are used to compute the global PageRanks of our local domain, they would all require O(N) computation, storage, and bandwidth, where N is the size of the global domain. This is in contrast to our method, which approximates the global PageRank and scales linearly with the size of the local domain.
Wang and Dewitt [22] propose a system where the set of web servers that comprise the global domain communicate with each other to compute their respective global PageRanks. For a given web server hosting n pages, the computational, bandwidth, and storage requirements are also linear in n. One drawback of this system is that the number of distinct web servers that comprise the global domain can be very large. For example, our ‘edu" dataset contains websites from over 3,200 different universities; coordinating such a system among a large number of sites can be very difficult.
Gan, Chen, and Suel propose a method for estimating the PageRank of a single page [5] which uses only constant bandwidth, computation, and space. Their approach relies on the availability of a remote connectivity server that can supply the set of inlinks to a given page, an assumption not used in our framework. They experimentally show that a reasonable estimate of the node"s PageRank can be obtained by visiting at most a few hundred nodes. Using their algorithm for our problem would require that either the entire global domain first be downloaded or a connectivity server be used, both of which would lead to very large web graphs.
The internet is growing exponentially, and in order to navigate such a large repository as the web, global search engines have established themselves as a necessity. Along with the ubiquity of these large-scale search engines comes an increase in search users" expectations. By providing complete and isolated coverage of a particular web domain, localized search engines are an effective outlet to quickly locate content that could otherwise be difficult to find. In this work, we contend that the use of global PageRank in a localized search engine can improve performance.
To estimate the global PageRank, we have proposed an iterative node selection framework where we select which pages from the global frontier to crawl next. Our primary contribution is our stochastic complementation page selection algorithm. This method crawls nodes that will most significantly impact the local domain and has running time linear in the number of nodes in the local domain.
Experimentally, we validate these methods across a diverse set of local domains, including seven site-specific domains and four topic-specific domains. We conclude that by crawling an additional n or 2n pages, our methods find an estimate of the global PageRanks that is up to ten times better than just using the local PageRanks. Furthermore, we demonstrate that our algorithm consistently outperforms other existing heuristics. 124 Research Track Paper Often times, topic-specific domains are discovered using a focused web crawler which considers a page"s content in conjunction with link anchor text to decide which pages to crawl next [4]. Although such crawlers have proven to be quite effective in discovering topic-related content, many irrelevant pages are also crawled in the process. Typically, these pages are deleted and not indexed by the localized search engine. These pages can of course provide valuable information regarding the global PageRank of the local domain. One way to integrate these pages into our framework is to start the FindGlobalPR algorithm with the current subgraph F equal to the set of pages that were crawled by the focused crawler.
The global PageRank estimation framework, along with the node selection algorithms presented, all require O(n) computation per iteration and bandwidth proportional to the number of pages crawled, Tk. If the number of iterations T is relatively small compared to the number of pages crawled per iteration, k, then the bottleneck of the algorithm will be the crawling phase. However, as the number of iterations increases (relative to k), the bottleneck will reside in the node selection computation. In this case, our algorithms would benefit from constant factor optimizations. Recall that the FindGlobalPR algorithm (Algorithm 2) requires that the PageRanks of the current expanded local domain be recomputed in each iteration. Recent work by Langville and Meyer [12] gives an algorithm to quickly recompute PageRanks of a given webgraph if a small number of nodes are added. This algorithm was shown to give speedup of five to ten times on some datasets. We plan to investigate this and other such optimizations as future work.
In this paper, we have objectively evaluated our methods by measuring how close our global PageRank estimates are to the actual global PageRanks. To determine the benefit of using global PageRanks in a localized search engine, we suggest a user study in which users are asked to rate the quality of search results for various search queries. For some queries, only the local PageRanks are used in ranking, and for the remaining queries, local PageRanks and the approximate global PageRanks, as computed by our algorithms, are used. The results of such a study can then be analyzed to determine the added benefit of using the global PageRanks computed by our methods, over just using the local PageRanks.
Acknowledgements. This research was supported by NSF grant CCF-0431257, NSF Career Award ACI-0093404, and a grant from Sabre, Inc.
[1] R. Baeza-Yates, M. Marin, C. Castillo, and A. Rodriguez. Crawling a country: better strategies than breadth-first for web page ordering. World-Wide Web Conference, 2005. [2] P. Boldi, M. Santini, and S. Vigna. Do your worst to make the best: paradoxical effects in pagerank incremental computations. Workshop on Web Graphs, 3243:168-180, 2004. [3] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 33(1-7):107-117, 1998. [4] S. Chakrabarti, M. van den Berg, and B. Dom.
Focused crawling: a new approach to topic-specific web resource discovery. World-Wide Web Conference,
[5] Y. Chen, Q. Gan, and T. Suel. Local methods for estimating pagerank values. Conference on Information and Knowledge Management, 2004. [6] J. Cho, H. Garcia-Molina, and L. Page. Efficient crawling through url ordering. World-Wide Web Conference, 1998. [7] T. H. Haveliwala and S. D. Kamvar. The second eigenvalue of the Google matrix. Technical report,
Stanford University, 2003. [8] T. Joachims, F. Radlinski, L. Granka, A. Cheng,
C. Tillekeratne, and A. Patel. Learning retrieval functions from implicit feedback. http://www.cs.cornell.edu/People/tj/career. [9] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H. Golub. Exploiting the block structure of the web for computing pagerank. World-Wide Web Conference, 2003. [10] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H. Golub. Extrapolation methods for accelerating pagerank computation. World-Wide Web Conference,
[11] A. N. Langville and C. D. Meyer. Deeper inside pagerank. Internet Mathematics, 2004. [12] A. N. Langville and C. D. Meyer. Updating the stationary vector of an irreducible markov chain with an eye on Google"s pagerank. SIAM Journal on Matrix Analysis, 2005. [13] P. Lyman, H. R. Varian, K. Swearingen, P. Charles,
N. Good, L. L. Jordan, and J. Pal. How much information 2003? School of Information Management and System, University of California at Berkely, 2003. [14] F. McSherry. A uniform approach to accelerated pagerank computation. World-Wide Web Conference,
[15] C. D. Meyer. Stochastic complementation, uncoupling markov chains, and the theory of nearly reducible systems. SIAM Review, 31:240-272, 1989. [16] US News and World Report. http://www.usnews.com. [17] Dmoz open directory project. http://www.dmoz.org. [18] Nutch open source search engine. http://www.nutch.org. [19] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005. [20] S. Raghavan and H. Garcia-Molina. Crawling the hidden web. In Proceedings of the Twenty-seventh International Conference on Very Large Databases,
[21] T. Tin Tang, D. Hawking, N. Craswell, and K. Griffiths. Focused crawling for both topical relevance and quality of medical information.

News forms a major portion of information disseminated in the world everyday. Common people and news analysts alike are very interested in keeping abreast of new things that happen in the news, but it is becoming very difficult to cope with the huge volumes of information that arrives each day. Hence there is an increasing need for automatic techniques to organize news stories in a way that helps users interpret and analyze them quickly. This problem is addressed by a research program called Topic Detection and Tracking (TDT) [3] that runs an open annual competition on standardized tasks of news organization.
One of the shortcomings of current TDT evaluation is its view of news topics as flat collection of stories. For example, the detection task of TDT is to arrange a collection of news stories into clusters of topics. However, a topic in news is more than a mere collection of stories: it is characterized by a definite structure of inter-related events. This is indeed recognized by TDT which defines a topic as ‘a set of news stories that are strongly related by some seminal realworld event" where an event is defined as ‘something that happens at a specific time and location" [3]. For example, when a bomb explodes in a building, that is the seminal event that triggers the topic. Other events in the topic may include the rescue attempts, the search for perpetrators, arrests and trials and so on. We see that there is a pattern of dependencies between pairs of events in the topic. In the above example, the event of rescue attempts is ‘influenced" by the event of bombing and so is the event of search for perpetrators.
In this work we investigate methods for modeling the structure of a topic in terms of its events. By structure, we mean not only identifying the events that make up a topic, but also establishing dependencies-generally causal-among them. We call the process of recognizing events and identifying dependencies among them event threading, an analogy to email threading that shows connections between related email messages. We refer to the resulting interconnected structure of events as the event model of the topic. Although this paper focuses on threading events within an existing news topic, we expect that such event based dependency structure more accurately reflects the structure of news than strictly bounded topics do. From a user"s perspective, we believe that our view of a news topic as a set of interconnected events helps him/her get a quick overview of the topic and also allows him/her navigate through the topic faster.
The rest of the paper is organized as follows. In section 2, we discuss related work. In section 3, we define the problem and use an example to illustrate threading of events within a news topic. In section 4, we describe how we built the corpus for our problem.
Section 5 presents our evaluation techniques while section 6 describes the techniques we use for modeling event structure. In section 7 we present our experiments and results. Section 8 concludes the paper with a few observations on our results and comments on future work. 446
The process of threading events together is related to threading of electronic mail only by name for the most part. Email usually incorporates a strong structure of referenced messages and consistently formatted subject headings-though information retrieval techniques are useful when the structure breaks down [7]. Email threading captures reference dependencies between messages and does not attempt to reflect any underlying real-world structure of the matter under discussion.
Another area of research that looks at the structure within a topic is hierarchical text classification of topics [9, 6]. The hierarchy within a topic does impose a structure on the topic, but we do not know of an effort to explore the extent to which that structure reflects the underlying event relationships.
Barzilay and Lee [5] proposed a content structure modeling technique where topics within text are learnt using unsupervised methods, and a linear order of these topics is modeled using hidden Markov models. Our work differs from theirs in that we do not constrain the dependency to be linear. Also their algorithms are tuned to work on specific genres of topics such as earthquakes, accidents, etc., while we expect our algorithms to generalize over any topic.
In TDT, researchers have traditionally considered topics as flatclusters [1]. However, in TDT-2003, a hierarchical structure of topic detection has been proposed and [2] made useful attempts to adopt the new structure. However this structure still did not explicitly model any dependencies between events.
In a work closest to ours, Makkonen [8] suggested modeling news topics in terms of its evolving events. However, the paper stopped short of proposing any models to the problem. Other related work that dealt with analysis within a news topic includes temporal summarization of news topics [4].
In this work, we have adhered to the definition of event and topic as defined in TDT. We present some definitions (in italics) and our interpretations (regular-faced) below for clarity.
to users. In TDT, a story is assumed to refer to only a single topic. In this work, we also assume that each story discusses a single event. In other words, a story is the smallest atomic unit in the hierarchy (topic event story). Clearly, both the assumptions are not necessarily true in reality, but we accept them for simplicity in modeling.
time and place [10]. In our work, we represent an event by a set of stories that discuss it. Following the assumption of atomicity of a story, this means that any set of distinct events can be represented by a set of non-overlapping clusters of news stories.
event. We expand on this definition and interpret a topic as a series of related events. Thus a topic can be represented by clusters of stories each representing an event and a set of (directed or undirected) edges between pairs of these clusters representing the dependencies between these events. We will describe this representation of a topic in more detail in the next section.
detects clusters of stories that discuss the same topic; Topic tracking detects stories that discuss a previously known topic [3].
Thus TDT concerns itself mainly with clustering stories into topics that discuss them.
topic, and also captures the dependencies among the events.
Thus the main difference between event threading and TDT is that we focus our modeling effort on microscopic events rather than larger topics. Additionally event threading models the relatedness or dependencies between pairs of events in a topic while TDT models topics as unrelated clusters of stories.
We first define our problem and representation of our model formally and then illustrate with the help of an example. We are given a set of Ò news stories Ë ×½ ¡ ¡ ¡ ×Ò on a given topic Ì and their time of publication. We define a set of events ½ ¡ ¡ ¡ Ñ with the following constraints: ¾ ¾ Ë (1) × Ø (2) × ¾ × Ø × ¾ (3) While the first constraint says that each event is an element in the power set of S, the second constraint ensures that each story can belong to at most one event. The last constraint tells us that every story belongs to one of the events in . In fact this allows us to define a mapping function from stories to events as follows: ´× µ iff × ¾ (4) Further, we also define a set of directed edges ´ µ which denote dependencies between events. It is important to explain what we mean by this directional dependency: While the existence of an edge itself represents relatedness of two events, the direction could imply causality or temporal-ordering. By causal dependency we mean that the occurrence of event B is related to and is a consequence of the occurrence of event A. By temporal ordering, we mean that event B happened after event A and is related to A but is not necessarily a consequence of A. For example, consider the following two events: ‘plane crash" (event A) and ‘subsequent investigations" (event B) in a topic on a plane crash incident.
Clearly, the investigations are a result of the crash. Hence an arrow from A to B falls under the category of causal dependency.
Now consider the pair of events ‘Pope arrives in Cuba"(event A) and ‘Pope meets Castro"(event B) in a topic that discusses Pope"s visit to Cuba. Now events A and B are closely related through their association with the Pope and Cuba but event B is not necessarily a consequence of the occurrence of event A. An arrow in such scenario captures what we call time ordering. In this work, we do not make an attempt to distinguish between these two kinds of dependencies and our models treats them as identical. A simpler (and hence less controversial) choice would be to ignore direction in the dependencies altogether and consider only undirected edges. This choice definitely makes sense as a first step but we chose the former since we believe directional edges make more sense to the user as they provide a more illustrative flow-chart perspective to the topic.
To make the idea of event threading more concrete, consider the example of TDT3 topic 30005, titled ‘Osama bin Laden"s Indictment" (in the 1998 news). This topic has 23 stories which form 5 events. An event model of this topic can be represented as in figure
indictment. The occurrence of event 2, namely ‘Trial and Indictment of Osama" is dependent on the event of ‘evidence gathered by CIA", i.e., event 1. Similarly, event 2 influences the occurrences of events 3, 4 and 5, namely ‘Threats from Militants", ‘Reactions 447 from Muslim World" and ‘announcement of reward". Thus all the dependencies in the example are causal.
Extending our notation further, we call an event A a parent of B and B the child of A, if ´ µ ¾ . We define an event model Å ´ µ to be a tuple of the set of events and set of dependencies.
Trial and (5) (3) (4) CIA announces reward Muslim world Reactions from Islamic militants Threats from (2) (1) Osama Indictment of CIA gathered by Evidence Figure 1: An event model of TDT topic ‘Osama bin Laden"s indictment".
Event threading is strongly related to topic detection and tracking, but also different from it significantly. It goes beyond topics, and models the relationships between events. Thus, event threading can be considered as a further extension of topic detection and tracking and is more challenging due to at least the following difficulties.
metrics and benchmark data is available.
In the next few sections, we will describe our attempts to tackle these problems.
We picked 28 topics from the TDT2 corpus and 25 topics from the TDT3 corpus. The criterion we used for selecting a topic is that it should contain at least 15 on-topic stories from CNN headline news. If the topic contained more than 30 CNN stories, we picked only the first 30 stories to keep the topic short enough for annotators. The reason for choosing only CNN as the source is that the stories from this source tend to be short and precise and do not tend to digress or drift too far away from the central theme. We believe modeling such stories would be a useful first step before dealing with more complex data sets.
We hired an annotator to create truth data. Annotation includes defining the event membership for each story and also the dependencies. We supervised the annotator on a set of three topics that we did our own annotations on and then asked her to annotate the
In identifying events in a topic, the annotator was asked to broadly follow the TDT definition of an event, i.e., ‘something that happens at a specific time and location". The annotator was encouraged to merge two events A and B into a single event C if any of the stories discusses both A and B. This is to satisfy our assumption that each story corresponds to a unique event. The annotator was also encouraged to avoid singleton events, events that contain a single news story, if possible. We realized from our own experience that people differ in their perception of an event especially when the number of stories in that event is small. As part of the guidelines, we instructed the annotator to assign titles to all the events in each topic. We believe that this would help make her understanding of the events more concrete. We however, do not use or model these titles in our algorithms.
In defining dependencies between events, we imposed no restrictions on the graph structure. Each event could have single, multiple or no parents. Further, the graph could have cycles or orphannodes. The annotator was however instructed to assign a dependency from event A to event B if and only if the occurrence of B is ‘either causally influenced by A or is closely related to A and follows A in time".
From the annotated topics, we created a training set of 26 topics and a test set of 27 topics by merging the 28 topics from TDT2 and
training and test sets have fairly similar statistics.
Feature Training set Test set Num. topics 26 27 Avg. Num. Stories/Topic 28.69 26.74 Avg. Doc. Len. 64.60 64.04 Avg. Num. Stories/Event 5.65 6.22 Avg. Num. Events/Topic 5.07 4.29 Avg. Num. Dependencies/Topic 3.07 2.92 Avg. Num. Dependencies/Event 0.61 0.68 Avg. Num. Days/Topic 30.65 34.48 Table 1: Statistics of annotated data
A system can generate some event model Å¼ ´ ¼ ¼µ using certain algorithms, which is usually different from the truth model Å ´ µ (we assume the annotator did not make any mistake). Comparing a system event model Å¼ with the true model Å requires comparing the entire event models including their dependency structure. And different event granularities may bring huge discrepancy between Å¼ and Å. This is certainly non-trivial as even testing whether two graphs are isomorphic has no known polynomial time solution. Hence instead of comparing the actual structure we examine a pair of stories at a time and verify if the system and true labels agree on their event-memberships and dependencies. Specifically, we compare two kinds of story pairs: ¯ Cluster pairs ( ´Åµ): These are the complete set of unordered pairs ´× × µ of stories × and × that fall within the same event given a model Å. Formally, ´Åµ ´× × µ × × ¾ Ë ´× µ ´× µ (5) where is the function in Å that maps stories to events as defined in equation 4. ¯ Dependency pairs ( ´Åµ): These are the set of all ordered pairs of stories ´× × µ such that there is a dependency from the event of × to the event of × in the model Å. ´Åµ ´× × µ ´ ´× µ ´× µµ ¾ (6) Note the story pair is ordered here, so ´× × µ is not equivalent to ´× × µ. In our evaluation, a correct pair with wrong 448 (B->D) Cluster pairs (A,C) Dependency pairs (A->B) (C->B) (B->D) D,E D,E (D,E) (D,E) (A->C) (A->E) (B->C) (B->E) (B->E) Cluster precision: 1/2 Cluster Recall: 1/2 Dependency Recall: 2/6 Dependency Precision: 2/4 (A->D) True event model System event model A,B C A,C B Cluster pairs (A,B) Dependency pairs Figure 2: Evaluation measures direction will be considered a mistake. As we mentioned earlier in section 3, ignoring the direction may make the problem simpler, but we will lose the expressiveness of our representation.
Given these two sets of story pairs corresponding to the true event model Å and the system event model Å¼, we define recall and precision for each category as follows. ¯ Cluster Precision (CP): It is the probability that two randomly selected stories × and × are in the same true-event given that they are in the same system event.
È È´ ´× µ ´× µ ¼´× µ ¼´× µµ ´Åµ ´Å¼µ ´Å¼µ (7) where ¼ is the story-event mapping function corresponding to the model Å¼. ¯ Cluster Recall(CR): It is the probability that two randomly selected stories × and × are in the same system-event given that they are in the same true event.
Ê È´ ¼´× µ ¼´× µ ´× µ ´× µµ ´Åµ ´Å¼µ ´Åµ (8) ¯ Dependency Precision(DP): It is the probability that there is a dependency between the events of two randomly selected stories × and × in the true model Å given that they have a dependency in the system model Å¼. Note that the direction of dependency is important in comparison.
È È´´ ´× µ ´× µµ ¾ ´ ¼´× µ ¼´× µµ ¾ ¼µ ´Åµ ´Å¼µ ´Å¼µ (9) ¯ Dependency Recall(DR): It is the probability that there is a dependency between the events of two randomly selected stories × and × in the system model Å¼ given that they have a dependency in the true model Å. Again, the direction of dependency is taken into consideration.
Ê È´´ ¼´× µ ¼´× µµ ¾ ¼ ´ ´× µ ´× µµ ¾ µ ´Åµ ´Å¼µ ´Åµ (10) The measures are illustrated by an example in figure 2. We also combine these measures using the well known F1-measure commonly used in text classification and other research areas as shown below. ¾ ¢ È ¢ Ê È · Ê ¾ ¢ È ¢ Ê È · Ê Â ¾ ¢ ¢ · (11) where and are the cluster and dependency F1-measures respectively and Â is the Joint F1-measure (Â ) that we use to measure the overall performance.
The task of event modeling can be split into two parts: clustering the stories into unique events in the topic and constructing dependencies among them. In the following subsections, we describe techniques we developed for each of these sub-tasks.
Each topic is composed of multiple events, so stories must be clustered into events before we can model the dependencies among them. For simplicity, all stories in the same topic are assumed to be available at one time, rather than coming in a text stream. This task is similar to traditional clustering but features other than word distributions may also be critical in our application.
In many text clustering systems, the similarity between two stories is the inner product of their tf-idf vectors, hence we use it as one of our features. Stories in the same event tend to follow temporal locality, so the time stamp of each story can be a useful feature.
Additionally, named-entities such as person and location names are another obvious feature when forming events. Stories in the same event tend to be related to the same person(s) and locations(s).
In this subsection, we present an agglomerative clustering algorithm that combines all these features. In our experiments, however, we study the effect of each feature on the performance separately using modified versions of this algorithm.
time decay (ACDT) We initialize our events to singleton events (clusters), i.e., each cluster contains exactly one story. So the similarity between two events, to start with, is exactly the similarity between the corresponding stories. The similarity Û×ÙÑ´×½ ×¾µ between two stories ×½ and ×¾ is given by the following formula: Û×ÙÑ´×½ ×¾µ ½ Ó×´×½ ×¾µ · ¾ÄÓ ´×½ ×¾µ · ¿È Ö´×½ ×¾µ (12) Here ½, ¾, ¿ are the weights on different features. In this work, we determined them empirically, but in the future, one can consider more sophisticated learning techniques to determine them.
Ó×´×½ ×¾µ is the cosine similarity of the term vectors. ÄÓ ´×½ ×¾µ is 1 if there is some location that appears in both stories, otherwise it is 0. È Ö´×½ ×¾µ is similarly defined for person name.
We use time decay when calculating similarity of story pairs, i.e., the larger time difference between two stories, the smaller their similarities. The time period of each topic differs a lot, from a few days to a few months. So we normalize the time difference using the whole duration of that topic. The time decay adjusted similarity 449 × Ñ´×½ ×¾µ is given by × Ñ´×½ ×¾µ Û×ÙÑ´×½ ×¾µ  « Ø½ Ø¾ Ì (13) where Ø½ and Ø¾ are the time stamps for story 1 and 2 respectively.
T is the time difference between the earliest and the latest story in the given topic. « is the time decay factor.
In each iteration, we find the most similar event pair and merge them. We have three different ways to compute the similarity between two events Ù and Ú: ¯ Average link: In this case the similarity is the average of the similarities of all pairs of stories between Ù and Ú as shown below: × Ñ´ Ù Ú µ È×Ù¾ Ù È×Ú¾ Ú × Ñ´×Ù ×Ú µ Ù Ú (14) ¯ Complete link: The similarity between two events is given by the smallest of the pair-wise similarities. × Ñ´ Ù Ú µ Ñ Ò ×Ù¾ Ù ×Ú¾ Ú × Ñ´×Ù ×Ú µ (15) ¯ Single link: Here the similarity is given by the best similarity between all pairs of stories. × Ñ´ Ù Ú µ Ñ Ü ×Ù¾ Ù ×Ú¾ Ú × Ñ´×Ù ×Ú µ (16) This process continues until the maximum similarity falls below the threshold or the number of clusters is smaller than a given number.
Capturing dependencies is an extremely hard problem because it may require a ‘deeper understanding" of the events in question.
A human annotator decides on dependencies not just based on the information in the events but also based on his/her vast repertoire of domain-knowledge and general understanding of how things operate in the world. For example, in Figure 1 a human knows ‘Trial and indictment of Osama" is influenced by ‘Evidence gathered by CIA" because he/she understands the process of law in general.
We believe a robust model should incorporate such domain knowledge in capturing dependencies, but in this work, as a first step, we will rely on surface-features such as time-ordering of news stories and word distributions to model them. Our experiments in later sections demonstrate that such features are indeed useful in capturing dependencies to a large extent.
In this subsection, we describe the models we considered for capturing dependencies. In the rest of the discussion in this subsection, we assume that we are already given the mapping ¼ Ë and we focus only on modeling the edges ¼. First we define a couple of features that the following models will employ.
First we define a 1-1 time-ordering function Ø Ë ½ ¡ ¡ ¡ Ò that sorts stories in ascending order by their time of publication.
Now, the event-time-ordering function Ø is defined as follows.
Ø ½ ¡ ¡ ¡ Ñ × Ø Ù Ú ¾ Ø ´ Ùµ Ø ´ Úµ ´µ Ñ Ò ×Ù¾ Ù Ø´×Ùµ Ñ Ò ×Ú¾ Ú Ø´×Úµ (17) In other words, Ø time-orders events based on the time-ordering of their respective first stories.
We will also use average cosine similarity between two events as a feature and it is defined as follows.
Ú Ë Ñ´ Ù Ú µ È×Ù¾ Ù È×Ú¾ Ú Ó×´×Ù ×Ú µ Ù Ú (18)
In this model, we assume that there are dependencies between all pairs of events. The direction of dependency is determined by the time-ordering of the first stories in the respective events. Formally, the system edges are defined as follows. ¼ ´ Ù Ú µ Ø ´ Ùµ Ø ´ Ú µ (19) where Ø is the event-time-ordering function. In other words, the dependency edge is directed from event Ù to event Ú , if the first story in event Ù is earlier than the first story in event Ú . We point out that this is not to be confused with the complete-link algorithm in clustering. Although we use the same names, it will be clear from the context which one we refer to.
This model is an extension of the complete link model with an additional constraint that there is a dependency between any two events Ù and Ú only if the average cosine similarity between event Ù and event Ú is greater than a threshold Ì. Formally, ¼ ´ Ù Úµ Ú Ë Ñ´ Ù Ú µ Ì Ø ´ Ùµ Ø ´ Ú µ (20)
In this model, we assume that each event can have at most one parent. We define the set of dependencies as follows. ¼ ´ Ù Úµ Ú Ë Ñ´ Ù Ú µ Ì Ø ´ Úµ Ø ´ Ùµ · ½ (21) Thus, for each event Ú , the nearest parent model considers only the event preceding it as defined by Ø as a potential candidate. The candidate is assigned as the parent only if the average similarity exceeds a pre-defined threshold Ì.
This model also assumes that each event can have at most one parent. An event Ú is assigned a parent Ù if and only if Ù is the most similar earlier event to Ú and the similarity exceeds a threshold Ì. Mathematically, this can be expressed as: ¼ ´ Ù Ú µ Ú Ë Ñ´ Ù Úµ Ì Ù Ö Ñ Ü Û Ø ´ Ûµ Ø ´ Úµ Ú Ë Ñ´ Û Ú µ (22)
In this model, we first build a maximum spanning tree (MST) using a greedy algorithm on the following fully connected weighted, undirected graph whose vertices are the events and whose edges are defined as follows: ´ Ù Ú µ Û´ Ù Ú µ Ú Ë Ñ´ Ù Úµ (23) Let ÅËÌ´ µ be the set of edges in the maximum spanning tree of ¼. Now our directed dependency edges are defined as follows. ¼ ´ Ù Ú µ ´ Ù Ú µ ¾ ÅËÌ´ µ Ø ´ Ùµ Ø ´ Úµ Ú Ë Ñ´ Ù Ú µ Ì (24) 450 Thus in this model, we assign dependencies between the most similar events in the topic.
Our experiments consists of three parts. First we modeled only the event clustering part (defining the mapping function ¼) using clustering algorithms described in section 6.1. Then we modeled only the dependencies by providing to the system the true clusters and running only the dependency algorithms of section 6.2. Finally, we experimented with combinations of clustering and dependency algorithms to produce the complete event model. This way of experimentation allows us to compare the performance of our algorithms in isolation and in association with other components. The following subsections present the three parts of our experimentation.
We have tried several variations of the Ì algorithm to study the effects of various features on the clustering performance. All the parameters are learned by tuning on the training set. We also tested the algorithms on the test set with parameters fixed at their optimal values learned from training. We used agglomerative clusModel best T CP CR CF P-value cos+1-lnk 0.15 0.41 0.56
cos+N(T)+avg-lnk - 0.41 0.62 0.48 7.5e-2 cos+N(T)+T+avg-lnk 0.03 0.42 0.62 0.49 2.4e-2* cos+TD+N(T)+avg-lnk - 0.44 0.66 0.52 7.0e-3* cos+TD+N(T)+T+avg-lnk 0.03 0.47 0.64 0.53 1.1e-3* Baseline(cos+avg-lnk) 0.05 0.39 0.67
(training set) tering based on only cosine similarity as our clustering baseline.
The results on the training and test sets are in Table 2 and 3 respectively. We use the Cluster F1-measure (CF) averaged over all topics as our evaluation criterion.
Model CP CR CF P-value cos+1-lnk 0.43 0.49
cos+N(T)+avg-lnk 0.41 0.71 0.51 0.31 cos+N(T)+T+avg-lnk 0.43 0.69* 0.52 0.14 cos+TD+N(T)+avg-lnk 0.43 0.76 0.54 0.025* cos+TD+N(T)+T+avg-lnk 0.47 0.69 0.54 0.0095* Baseline(cos+avg-lnk) 0.44 0.67
(test set) P-value marked with a £ means that it is a statistically significant improvement over the baseline (95% confidence level, one tailed T-test). The methods shown in table 2 and 3 are: ¯ Baseline: tf-idf vector weight, cosine similarity, average link in clustering. In equation 12, ½ ½, ¾ ¿ ¼. And « ¼ in equation 13. This F-value is the maximum obtained by tuning the threshold. ¯ cos+1-lnk: Single link comparison (see equation 16) is used where similarity of two clusters is the maximum of all story pairs, other configurations are the same as the baseline run. ¯ cos+all-lnk: Complete link algorithm of equation 15 is used.
Similar to single link but it takes the minimum similarity of all story pairs. ¯ cos+Loc+avg-lnk: Location names are used when calculating similarity. ¾ ¼ ¼ in equation 12. All algorithms starting from this one use average link (equation 14), since single link and complete link do not show any improvement of performance. ¯ cos+Per+avg-lnk: ¿ ¼ ¼ in equation 12, i.e., we put some weight on person names in the similarity. ¯ cos+TD+avg-lnk: Time Decay coefficient « ½ in equation 13, which means the similarity between two stories will be decayed to ½ if they are at different ends of the topic. ¯ cos+N(T)+avg-lnk: Use the number of true events to control the agglomerative clustering algorithm. When the number of clusters is fewer than that of truth events, stop merging clusters. ¯ cos+N(T)+T+avg-lnk: similar to N(T) but also stop agglomeration if the maximal similarity is below the threshold Ì. ¯ cos+TD:+N(T)+avg-lnk: similar to N(T) but the similarities are decayed, « ½ in equation 13. ¯ cos+TD+N(T)+T+avg-lnk: similar to TD+N(Truth) but calculation halts when the maximal similarity is smaller than the threshold Ì.
Our experiments demonstrate that single link and complete link similarities perform worse than average link, which is reasonable since average link is less sensitive to one or two story pairs. We had expected locations and person names to improve the result, but it is not the case. Analysis of topics shows that many on-topic stories share the same locations or persons irrespective of the event they belong to, so these features may be more useful in identifying topics rather than events. Time decay is successful because events are temporally localized, i.e., stories discussing the same event tend to be adjacent to each other in terms of time. Also we noticed that providing the number of true events improves the performance since it guides the clustering algorithm to get correct granularity.
However, for most applications, it is not available. We used it only as a cheat experiment for comparison with other algorithms. On the whole, time decay proved to the most powerful feature besides cosine similarity on both training and test sets.
In this subsection, our goal is to model only dependencies. We use the true mapping function and by implication the true events Î . We build our dependency structure ¼ using all the five models described in section 6.2. We first train our models on the 26 training topics. Training involves learning the best threshold Ì for each of the models. We then test the performances of all the trained models on the 27 test topics. We evaluate our performance 451 using the average values of Dependency Precision (DP),
Dependency Recall (DR) and Dependency F-measure (DF). We consider the complete-link model to be our baseline since for each event, it trivially considers all earlier events to be parents.
Table 4 lists the results on the training set. We see that while all the algorithms except MST outperform the baseline complete-link algorithm , the nearest Parent algorithm is statistically significant from the baseline in terms of its DF-value using a one-tailed paired T-test at 95% confidence level.
Model best Ì DP DR DF P-value Nearest Parent 0.025 0.55 0.62 0.56 0.04* Best Similarity 0.02 0.51 0.62 0.53 0.24 MST 0.0 0.46 0.58
Complete-link - 0.36 0.93
of the threshold Ì. * indicates the corresponding model is statistically significant compared to the baseline using a one-tailed, paired T-test at 95% confidence level.
In table 5 we present the comparison of the models on the test set. Here, we do not use any tuning but set the threshold to the corresponding optimal values learned from the training set. The results throw some surprises: The nearest parent model, which was significantly better than the baseline on training set, turns out to be worse than the baseline on the test set. However all the other models are better than the baseline including the best similarity which is statistically significant. Notice that all the models that perform better than the baseline in terms of DF, actually sacrifice their recall performance compared to the baseline, but improve on their precision substantially thereby improving their performance on the DF-measure.
We notice that both simple-thresholding and best similarity are better than the baseline on both training and test sets although the improvement is not significant. On the whole, we observe that the surface-level features we used capture the dependencies to a reasonable level achieving a best value of 0.72 DF on the test set.
Although there is a lot of room for improvement, we believe this is a good first step.
Model DP DR DF P-value Nearest Parent 0.61 0.60
MST 0.70 0.68 0.69 0.22 Simple Thresh. 0.57 0.75 0.64 0.24 Baseline (Complete-link) 0.50 0.94
Now that we have studied the clustering and dependency algorithms in isolation, we combine the best performing algorithms and build the entire event model. Since none of the dependency algorithms has been shown to be consistently and significantly better than the others, we use all of them in our experimentation. From the clustering techniques, we choose the best performing Cos+TD.
As a baseline, we use a combination of the baselines in each components, i.e., cos for clustering and complete-link for dependencies.
Note that we need to retrain all the algorithms on the training set because our objective function to optimize is now JF, the joint F-measure. For each algorithm, we need to optimize both the clustering threshold and the dependency threshold. We did this empirically on the training set and the optimal values are listed in table
The results on the training set, also presented in table 6, indicate that cos+TD+Simple-Thresholding is significantly better than the baseline in terms of the joint F-value JF, using a one-tailed paired Ttest at 95% confidence level. On the whole, we notice that while the clustering performance is comparable to the experiments in section
performance. Unlike our experiments in section 7.2 where we had provided the true clusters to the system, in this case, the system has to deal with deterioration in the cluster quality. Hence the performance of the dependency algorithms has suffered substantially thereby lowering the overall performance.
The results on the test set present a very similar story as shown in table 7. We also notice a fair amount of consistency in the performance of the combination algorithms. cos+TD+Simple-Thresholding outperforms the baseline significantly. The test set results also point to the fact that the clustering component remains a bottleneck in achieving an overall good performance.
In this paper, we have presented a new perspective of modeling news topics. Contrary to the TDT view of topics as flat collection of news stories, we view a news topic as a relational structure of events interconnected by dependencies. In this paper, we also proposed a few approaches for both clustering stories into events and constructing dependencies among them. We developed a timedecay based clustering approach that takes advantage of temporallocalization of news stories on the same event and showed that it performs significantly better than the baseline approach based on cosine similarity. Our experiments also show that we can do fairly well on dependencies using only surface-features such as cosinesimilarity and time-stamps of news stories as long as true events are provided to the system. However, the performance deteriorates rapidly if the system has to discover the events by itself. Despite that discouraging result, we have shown that our combined algorithms perform significantly better than the baselines.
Our results indicate modeling dependencies can be a very hard problem especially when the clustering performance is below ideal level. Errors in clustering have a magnifying effect on errors in dependencies as we have seen in our experiments. Hence, we should focus not only on improving dependencies but also on clustering at the same time.
As part of our future work, we plan to investigate further into the data and discover new features that influence clustering as well as dependencies. And for modeling dependencies, a probabilistic framework should be a better choice since there is no definite answer of yes/no for the causal relations among some events. We also hope to devise an iterative algorithm which can improve clustering and dependency performance alternately as suggested by one of the reviewers. We also hope to expand our labeled corpus further to include more diverse news sources and larger and more complex event structures.
Acknowledgments We would like to thank the three anonymous reviewers for their valuable comments. This work was supported in part by the Center 452 Model Cluster T Dep. T CP CR CF DP DR DF JF P-value cos+TD+Nearest-Parent 0.055 0.02 0.51 0.53 0.49 0.21 0.19 0.19
Baseline (cos+Complete-link) 0.10 - 0.58 0.31 0.38 0.20 0.67 0.30
Model CP CR CF DP DR DF JF P-value cos+TD+Nearest Parent 0.57 0.50 0.50 0.27 0.19 0.21
Baseline (cos+Complete-link) 0.66 0.27 0.36 0.30 0.72 0.43
for Intelligent Information Retrieval and in part by SPAWARSYSCENSD grant number N66001-02-1-8903. Any opinions, findings and conclusions or recommendations expressed in this material are the authors" and do not necessarily reflect those of the sponsor.
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. Topic detection and tracking pilot study: Final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, pages 194-218,
[2] J. Allan, A. Feng, and A. Bolivar. Flexible intrinsic evaluation of hierarchical clustering for tdt. volume In the Proc. of the ACM Twelfth International Conference on Information and Knowledge Management, pages 263-270,
Nov 2003. [3] James Allan, editor. Topic Detection and Tracking:Event based Information Organization. Kluwer Academic Publishers, 2000. [4] James Allan, Rahul Gupta, and Vikas Khandelwal. Temporal summaries of new topics. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10-18. ACM Press, 2001. [5] Regina Barzilay and Lillian Lee. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics(HLT-NAACL), pages 113-120, 2004. [6] D. Lawrie and W. B. Croft. Discovering and comparing topic hierarchies. In Proceedings of RIAO 2000 Conference, pages 314-330, 1999. [7] David D. Lewis and Kimberly A. Knowles. Threading electronic mail: a preliminary study. Inf. Process. Manage., 33(2):209-217, 1997. [8] Juha Makkonen. Investigations on event evolution in tdt. In Proceedings of HLT-NAACL 2003 Student Workshop, pages 43-48, 2004. [9] Aixin Sun and Ee-Peng Lim. Hierarchical text classification and evaluation. In Proceedings of the 2001 IEEE International Conference on Data Mining, pages 521-528.
IEEE Computer Society, 2001. [10] Yiming Yang, Jaime Carbonell, Ralf Brown, Thomas Pierce,

Relevance measurement is crucial to web search and to information retrieval in general. Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs. However, explicit human ratings are expensive and difficult to obtain. At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.
Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.
However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks. Therefore, it is not clear whether these techniques will work for general real-world web search. A significant distinction is that web search is not controlled. Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting. By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.
Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).
We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper.
Ranking search results is a fundamental problem in information retrieval. The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20]. A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features. Current search engines are commonly tuned on human relevance judgments. Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated. Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.
Several research groups have evaluated the relationship between implicit measures and user interest. In these studies, both reading time and explicit ratings of interest are collected.
Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a user"s interest levels. Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system. Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems. More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.
Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web. The authors hypothesized correlations between a high degree of page activity and a user"s interest. While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest. Claypool et al. [6] studied how several implicit measures related to the interests of the user. They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited. Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest. Fox et al. [7] explored the relationship between implicit and explicit measures in Web search. They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions. They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.
Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. More recently,
Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting. A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].
Unfortunately, the extent to which existing research applies to real-world web search is unclear. In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting.
As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences. Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces. Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.
Our general idea is to model the deviations from the expected user behavior. Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information. We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2).
As we discussed, we aggregate statistics across many user sessions. A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately. In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments. The data set is described in more detail in Section 5.2. For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs. For these queries we aggregate click data over more than 120,000 searches performed over a three week period. We also have explicit relevance judgments for the top 10 results for each query.
Figure 3.1 shows the relative clickthrough frequency as a function of result position. The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p). These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1. The resulting distribution agrees with previous observations that users click more often on top-ranked results.
This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0
1
result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.
First we consider the distribution of clicks for the relevant documents for these queries. Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR). While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.
For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.
Nevertheless, many users still click on the non-relevant results in position 1 for such queries. This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0
1
result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0
result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).
If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results. Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges.
Clicks on search results comprise only a small fraction of the post-search activities typically performed by users. We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.
Although clickthrough distributions are heavily biased towards top results, we have just shown how the ‘relevance-driven" click distribution can be recovered by correcting for the prior, background distribution. We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted. Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.
More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 . In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair. This aggregation gives additional robustness of not relying on individual noisy user interactions.
In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.
We now describe the actual features we use to represent user behavior.
Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result. Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize. This information was obtained via opt-in client-side instrumentation from users of a major web search engine.
This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7]. An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior. Furthermore, we include derived, distributional features computed as described above.
The features we use to represent user search interactions are summarized in Table 3.1. For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.
Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary. To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text. These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.
Browsing features: Simple aspects of the user web page interactions can be captured and quantified. These features are used to characterize interactions with pages beyond the results page. For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query. These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries). We include both the direct features and the derived features described above.
Clickthrough features: Clicks are a special case of user interaction with the search engine. We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4. For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove). The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.
Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL
Having described our features, we now turn to the actual method of mapping the features to user preferences. We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights. We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence. Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences. The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.
For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.
We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.
To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items. More specifically, for each judged query we check if a result link has been judged. If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result. These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.
RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments.
In our experiments, we explore several models for predicting user preferences. These models range from using no implicit user feedback to using all available implicit user feedback.
Ranking search results to predict user preferences is a fundamental problem in information retrieval. Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1). At the same time, user interactions with a search engine provide a wealth of information. A commonly considered type of interaction is user clicks on search results. Previous work [9], as described above, also examined which results were skipped (e.g., ‘skip above" and ‘skip next") and other related strategies to induce preference judgments from the users" skipping over results and not clicking on following results. We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.
As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3). We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine.
A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker. For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine. Hence, we will call this system Current for the subsequent discussion. While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality. The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections.
If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10]. By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies. We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.
Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p.
In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one. Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking. Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above. For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.
We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper. These strategies are motivated and empirically tested for individual users in a laboratory setting. As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users" behavior.
The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency. We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model. For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .
Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.
The choice of d selects the tradeoff between recall and precision. While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.
However, for informational queries, multiple results may be clicked, with varying frequency. Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results. We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.
As in CD, the choice of m selects the tradeoff between recall and precision. The pairs may be preferred in the original order or in reverse of it. Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other. Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.
Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.
CDiff and CD are complimentary. CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD. Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.
Other variations of the above strategies were considered, but these five methods cover the range of observed performance.
The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies. As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results. We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).
The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.
Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.
Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.
This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond. As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences.
We now describe our experimental setup. We first describe the methodology used, including our evaluation metrics (Section
methods we compared in this study (Section 5.3).
Our evaluation focuses on the pairwise agreement between preferences for results. This allows us to compare to previous work [9,10]. Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9]. The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.
We discuss other applications of our models beyond web search ranking in Section 7.
To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels. To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).
In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20]. While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.
Specifically, we report the average query recall and precision.
For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.
The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively. A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.
We discuss this issue further when we consider extensions to the current work in Section 7.
For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine. For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort. In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.
The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine. This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31. These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.
In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).
These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings. Furthermore, the data size is order of magnitude larger than any study reported in the literature.
We considered a number of methods for comparison. We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1). Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page. In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions. This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1). Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.
Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method. To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%. The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).
It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results. Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above. To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets.
We now turn to experimental evaluation of predicting relevance preference of web search results. Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2). The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively. Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence. In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).
SA+N SA
Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff,
CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.
Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08. In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels. Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies. However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.
But first, we consider the best performing strategy,
UserBehavior. Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared. Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.
Furthermore, Browse is able to achieve high recall (as high as
the baseline ranking.
To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation. Figure 6.2 reports Precision vs.
Recall for each feature group. Interestingly, Query-text alone has low accuracy (only marginally better than Random). Furthermore,
Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model. Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values.
Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually.
Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs. Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).
Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier. For example, the clickthrough-trained classifier achieves
achieved by the CD+CDiff strategy.
Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data. We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query. Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least
query. Not surprisingly, CD+CDiff improves with more clicks.
This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.
Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff. For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0
Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).
Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall. In contrast, the current search engine always makes a prediction for every result for a given query. As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available. Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time. We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625). As expected, Recall of both strategies improves quickly with more days of interaction data examined.
We now briefly summarize our experimental results. We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences. Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results). Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.
Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone. Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods.
Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting. We showed that our robust models result in higher prediction accuracy than previously published techniques.
We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries. Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios. Our methods" predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions. We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features. By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.
Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies. Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles. For example, the user behavior model on intranet search may be different from the web search behavior. Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.
A natural application of our preference prediction models is to improve web search ranking [1]. In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking. For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels. Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.
While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.
For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.
Hence, clustering queries and learning different predictive models for each query type is a promising research direction. Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.
Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.
As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting. Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods. The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience.
[1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan. HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents. In Proceedings of TREC 2003, 24-37,
[3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,. In Proceedings of WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N.
Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.
Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick. Learning users" interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay,
Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical. Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.

Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.
The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.
None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].
Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper.
We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.
Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.
This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.
In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].
Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14]. It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.
The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively.
Section 5 reports the experiments and results. Section 6 concludes the main findings in this study.
We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets.
TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]. The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.
TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].
TDT3 was the evaluation benchmark in the TDT2001 dry run1 .
The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT,
CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.
Machine-translated versions of the non-English stories (Xinhua,
Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT.
TDT5 was the evaluation benchmark in TDT2004 [4]. The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper.
The TDT topics differ from TREC topics both conceptually and statistically. Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.
The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0
Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics
To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation.
Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max
CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002). For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging).
The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).
To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( ,
N DB TP + =− )(1 ,
CA C Pmiss + = ,
DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.
In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric. To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.
Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01
From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.
The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.
More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.
That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w . However, this is not true if
on average for the test corpus. Using TDT3 as an example, the true percentage is:
37770
)( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: )1.010( 1
)3.7937770(37770
))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10
)( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows:
991,207/3.71
)( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.
The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.
Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. This was a challenging part of the TDT2004 evaluation for AF.
Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge.
We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| ' |)(| )()( )(')( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights. The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.
The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the system"s prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype. Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF). To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.
The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].
It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.
Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.
This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).
Results of more complex variants of Rocchio are also discussed when relevant.
Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].
Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.
We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix. Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5]. How to find an effective μ r is an open issue for research, depending on the user"s belief about the parameter space and the optimal range.
The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ .
We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback.
The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004. Multiple research teams participated and multiple runs from each team were allowed.
Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure
with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison.
Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better)
0
Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.) We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better)
0
1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).
CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004
0
1
Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made. Such a setting has been conventional for the Topic Tracking task in TDT until
each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers.
How much the strong performance of our systems depends on parameter tuning is an important question.
Both Rocchio and LR have parameters that must be prespecified before the AF process. The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed.
Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004. The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.
Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.
Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ. Table 2 summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0. This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.
Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers" parameters do not.
Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.
Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964
3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0
Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.
The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between
performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.
Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments.
How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.
To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.
Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.
Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF.
After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback.
We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.
For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.
Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.

The representation of documents in XML provides an opportunity for information retrieval systems to take advantage of document structure, returning individual document components when appropriate, rather than complete documents in all circumstances. In response to a user query, an XML information retrieval system might return a mixture of paragraphs, sections, articles, bibliographic entries and other components. This facility is of particular benefit when a collection contains very long documents, such as product manuals or books, where the user should be directed to the most relevant portions of these documents. <article> <fm> <atl>Text Compression for Dynamic Document Databases</atl> <au>Alistair Moffat</au> <au>Justin Zobel</au> <au>Neil Sharman</au> <abs><p><b>Abstract</b> For ...</p></abs> </fm> <bdy> <sec><st>INTRODUCTION</st> <ip1>Modern document databases...</ip1> <p>There are good reasons to compress...</p> </sec> <sec><st>REDUCING MEMORY REQUIREMENTS</st>... <ss1><st>2.1 Method A</st>... </sec> ... </bdy> </article> Figure 1: A journal article encoded in XML.
Figure 1 provides an example of a journal article encoded in XML, illustrating many of the important characteristics of XML documents. Tags indicate the beginning and end of each element, with elements varying widely in size, from one word to thousands of words. Some elements, such as paragraphs and sections, may be reasonably presented to the user as retrieval results, but others are not appropriate. Elements overlap each other - articles contain sections, sections contain subsections, and subsections contain paragraphs. Each of these characteristics affects the design of an XML IR system, and each leads to fundamental problems that must be solved in an successful system. Most of these fundamental problems can be solved through the careful adaptation of standard IR techniques, but the problems caused by overlap are unique to this area [4,11] and form the primary focus of this paper.
The article of figure 1 may be viewed as an XML tree, as illustrated in figure 2. Formally, a collection of XML documents may be represented as a forest of ordered, rooted trees, consisting of a set of nodes N and a set of directed edges E connecting these nodes. For each node x ∈ N , the notation x.parent refers to the parent node of x, if one exists, and the notation x.children refers to the set of child nodes sec bdyfm atl au au au abs p b st ip1 sec st ss1 st article p Figure 2: Example XML tree. of x. Since an element may be represented by the node at its root, the output of an XML IR system may be viewed as a ranked list of the top-m nodes.
The direct application of a standard relevance ranking technique to a set of XML elements can produce a result in which the top ranks are dominated by many structurally related elements. A high scoring section is likely to contain several high scoring paragraphs and to be contained in an high scoring article. For example, many of the elements in figure 2 would receive a high score on the keyword query text index compression algorithms. If each of these elements are presented to a user as an individual and separate result, she may waste considerable time reviewing and rejecting redundant content.
One possible solution is to report only the highest scoring element along a given path in the tree, and to remove from the lower ranks any element containing it, or contained within it. Unfortunately, this approach destroys some of the possible benefits of XML IR. For example, an outer element may contain a substantial amount of information that does not appear in an inner element, but the inner element may be heavily focused on the query topic and provide a short overview of the key concepts. In such cases, it is reasonable to report elements which contain, or are contained in, higher ranking elements. Even when an entire book is relevant, a user may still wish to have the most important paragraphs highlighted, to guide her reading and to save time [6].
This paper presents a method for controlling overlap.
Starting with an initial element ranking, a re-ranking algorithm adjusts the scores of lower ranking elements that contain, or are contained within, higher ranking elements, reflecting the fact that this information may now be redundant. For example, once an element representing a section appears in the ranking, the scores for the paragraphs it contains and the article that contains it are reduced. The inspiration for this strategy comes partially from recent work on structured documents retrieval, where terms appearing in different fields, such as the title and body, are given different weights [20].
Extending that approach, the re-ranking algorithm varies weights dynamically as elements are processed.
The remainder of the paper is organized as follows: After a discussion of background work and evaluation methodology, a baseline retrieval method is presented in section 4.
This baseline method represents a reasonable adaptation of standard IR technology to XML. Section 5 then outlines a strategy for controlling overlap, using the baseline method as a starting point. A re-ranking algorithm implementing this strategy is presented in section 6 and evaluated in section 7.
Section 8 discusses an extended version of the algorithm.
This section provides a general overview of XML information retrieval and discusses related work, with an emphasis on the fundamental problems mentioned in the introduction.
Much research in the area of XML retrieval views it from a traditional database perspective, being concerned with such problems as the implementation of structured query languages [5] and the processing of joins [1]. Here, we take a content oriented IR perceptive, focusing on XML documents that primarily contain natural language data and queries that are primarily expressed in natural language.
We assume that these queries indicate only the nature of desired content, not its structure, and that the role of the IR system is to determine which elements best satisfy the underlying information need. Other IR research has considered mixed queries, in which both content and structural requirements are specified [2,6,14,17,23].
In traditional information retrieval applications the standard unit of retrieval is taken to be the document.
Depending on the application, this term might be interpreted to encompass many different objects, including web pages, newspaper articles and email messages.
When applying standard relevance ranking techniques in the context of XML IR, a natural approach is to treat each element as a separate document, with term statistics available for each [16]. In addition, most ranking techniques require global statistics (e.g. inverse document frequency) computed over the collection as a whole. If we consider this collection to include all elements that might be returned by the system, a specific occurrence of a term may appear in several different documents, perhaps in elements representing a paragraph, a subsection, a section and an article.
It is not appropriate to compute inverse document frequency under the assumption that the term is contained in all of these elements, since the number of elements that contain a term depends entirely on the structural arrangement of the documents [13,23].
While an XML IR system might potentially retrieve any element, many elements may not be appropriate as retrieval results. This is usually the case when elements contain very little text [10]. For example, a section title containing only the query terms may receive a high score from a ranking algorithm, but alone it would be of limited value to a user, who might prefer the actual section itself. Other elements may reflect the document"s physical, rather than logical, structure, which may have little or no meaning to a user. An effective XML IR system must return only those elements that have sufficient content to be usable and are able to stand alone as independent objects [15,18]. Standard document components such as paragraphs, sections, subsections, and abstracts usually meet these requirements; titles, italicized phrases, and individual metadata fields often do not.
Over the past three years, the INitiative for the Evaluation of XML Retrieval (INEX) has encouraged research into XML information retrieval technology [7,8]. INEX is an experimental conference series, similar to TREC, with groups from different institutions completing one or more experimental tasks using their own tools and systems, and comparing their results at the conference itself. Over 50 groups participated in INEX 2004, and the conference has become as influential in the area of XML IR as TREC is in other IR areas. The research described in this paper, as well as much of the related work it cites, depends on the test collections developed by INEX.
Overlap causes considerable problems with retrieval evaluation, and the INEX organizers and participants have wrestled with these problems since the beginning. While substantial progress has been made, these problem are still not completely solved. Kazai et al. [11] provide a detailed exposition of the overlap problem in the context of INEX retrieval evaluation and discuss both current and proposed evaluation metrics. Many of these metrics are applied to evaluate the experiments reported in this paper, and they are briefly outlined in the next section.
Space limitations prevent the inclusion of more than a brief summary of INEX 2004 tasks and evaluation methodology. For detailed information, the proceedings of the conference itself should be consulted [8].
For the main experimental tasks, INEX 2004 participants were provided with a collection of 12,107 articles taken from the IEEE Computer Societies magazines and journals between 1995 and 2002. Each document is encoded in XML using a common DTD, with the document of figures 1 and 2 providing one example.
At INEX 2004, the two main experimental tasks were both adhoc retrieval tasks, investigating the performance of systems searching a static collection using previously unseen topics. The two tasks differed in the types of topics they used. For one task, the content-only or CO task, the topics consist of short natural language statements with no direct reference to the structure of the documents in the collection. For this task, the IR system is required to select the elements to be returned. For the other task, the contentand-structure or CAS task, the topics are written in an XML query language [22] and contain explicit references to document structure, which the IR system must attempt to satisfy. Since the work described in this paper is directed at the content-only task, where the IR system receives no guidance regarding the elements to return, the CAS task is ignored in the remainder of our description.
In 2004, 40 new CO topics were selected by the conference organizers from contributions provided by the conference participants. Each topic includes a short keyword query, which is executed over the collection by each participating group on their own XML IR system. Each group could submit up to three experimental runs consisting of the top m = 1500 elements for each topic.
Since XML IR is concerned with locating those elements that provide complete coverage of a topic while containing as little extraneous information as possible, simple relevant vs. not relevant judgments are not sufficient. Instead, the INEX organizers adopted two dimensions for relevance assessment: The exhaustivity dimension reflects the degree to which an element covers the topic, and the specificity dimension reflects the degree to which an element is focused on the topic. A four-point scale is used in both dimensions. Thus, a (3,3) element is highly exhaustive and highly specific, a (1,3) element is marginally exhaustive and highly specific, and a (0,0) element is not relevant. Additional information on the assessment methodology may be found in Piwowarski and Lalmas [19], who provide a detailed rationale.
The principle evaluation metric used at INEX 2004 is a version of mean average precision (MAP), adjusted by various quantization functions to give different weights to different elements, depending on their exhaustivity and specificity values. One variant, the strict quantization function gives a weight of 1 to (3,3) elements and a weight of 0 to all others.
This variant is essentially the familiar MAP value, with (3,3) elements treated as relevant and all other elements treated as not relevant. Other quantization functions are designed to give partial credit to elements which are near misses, due to a lack or exhaustivity and/or specificity. Both the generalized quantization function and the specificity-oriented generalization (sog) function credit elements according to their degree of relevance [11], with the second function placing greater emphasis on specificity. This paper reports results of this metric using all three of these quantization functions. Since this metric was first introduced at INEX 2002, it is generally referred as the inex-2002 metric.
The inex-2002 metric does not penalize overlap. In particular, both the generalized and sog quantization functions give partial credit to a near miss even when a (3,3) element overlapping it is reported at a higher rank. To address this problem, Kazai et al. [11] propose an XML cumulated gain metric, which compares the cumulated gain [9] of a ranked list to an ideal gain vector. This ideal gain vector is constructed from the relevance judgments by eliminating overlap and retaining only best element along a given path. Thus, the XCG metric rewards retrieval runs that avoid overlap. While XCG was not used officially at INEX 2004, a version of it is likely to be used in the future.
At INEX 2003, yet another metric was introduced to ameliorate the perceived limitations of the inex-2002 metric.
This inex-2003 metric extends the definitions of precision and recall to consider both the size of reported components and the overlap between them. Two versions were created, one that considered only component size and another that considered both size and overlap. While the inex-2003 metric exhibits undesirable anomalies [11], and was not used in 2004, values are reported in the evaluation section to provide an additional instrument for investigating overlap.
This section provides an overview of baseline XML information retrieval method currently used in the MultiText IR system, developed by the Information Retrieval Group at the University of Waterloo [3]. This retrieval method results from the adaptation and tuning of the Okapi BM25 measure [21] to the XML information retrieval task. The MultiText system performed respectably at INEX 2004, placing in the top ten under all of the quantization functions, and placing first when the quantization function emphasized exhaustivity.
To support retrieval from XML and other structured document types, the system provides generalized queries of the form: rank X by Y where X is a sub-query specifying a set of document elements to be ranked and Y is a vector of sub-queries specifying individual retrieval terms.
For our INEX 2004 runs, the sub-query X specified a list of retrievable elements as those with tag names as follows: abs app article bb bdy bm fig fm ip1 li p sec ss1 ss2 vt This list includes bibliographic entries (bb) and figure captions (fig) as well as paragraphs, sections and subsections.
Prior to INEX 2004, the INEX collection and the INEX 2003 relevance judgments were manually analyzed to select these tag names. Tag names were selected on the basis of their frequency in the collection, the average size of their associated elements, and the relative number of positive relevance judgments they received. Automating this selection process is planned as future work.
For INEX 2004, the term vector Y was derived from the topic by splitting phrases into individual words, eliminating stopwords and negative terms (those starting with -), and applying a stemmer. For example, keyword field of topic 166 +"tree edit distance" + XML -image became the four-term query "$tree" "$edit" "$distance" "$xml" where the $ operator within a quoted string stems the term that follows it.
Our implementation of Okapi BM25 is derived from the formula of Robertson et al. [21] by setting parameters k2 = 0 and k3 = ∞. Given a term set Q, an element x is assigned the score   t∈Q w(1) qt (k1 + 1)xt K + xt (1) where w(1) = log ¡ D − Dt + 0.5 Dt + 0.5 ¢ D = number of documents in the corpus Dt = number of documents containing t qt = frequency that t occurs in the topic xt = frequency that t occurs in x K = k1((1 − b) + b · lx/lavg) lx = length of x lavg = average document length
MeanAveragePrecision(inex-2002) k1 strict generalized sog Figure 3: Impact of k1 on inex-2002 mean average precision with b = 0.75 (INEX 2003 CO topics).
Prior to INEX 2004, the INEX 2003 topics and judgments were used to tune the b and k1 parameters, and the impact of this tuning is discussed later in this section.
For the purposes of computing document-level statistics (D, Dt and lavg) a document is defined to be an article.
These statistics are used for ranking all element types.
Following the suggestion of Kamps et al. [10], the retrieval results are filtered to eliminate very short elements, those less than 25 words in length.
The use of article statistics for all element types might be questioned. This approach may be justified by viewing the collection as a set of articles to be searched using standard document-oriented techniques, where only articles may be returned. The score computed for an element is essentially the score it would receive if it were added to the collection as a new document, ignoring the minor adjustments needed to the document-level statistics. Nonetheless, we plan to examine this issue again in the future.
In our experience, the performance of BM25 typically benefits from tuning the b and k1 parameters to the collection, whenever training queries are available for this purpose. Prior to INEX 2004, we trained the MultiText system using the INEX 2003 queries. As a starting point we used the values b = 0.75 and k1 = 1.2, which perform well on TREC adhoc collections and are used as default values in our system. The results were surprising. Figure 3 shows the result of varying k1 with b = 0.75 on the MAP values under three quantization functions. In our experience, optimal values for k1 are typically in the range 0.0 to 2.0. In this case, large values are required for good performance. Between k1 = 1.0 and k1 = 6.0 MAP increases by over 15% under the strict quantization. Similar improvements are seen under the generalized and sog quantizations. In contrast, our default value of b = 0.75 works well under all quantization functions (figure 4). After tuning over a wide range of values under several quantization functions, we selected values of k = 10.0 and b = 0.80 for our INEX 2004 experiments, and these values are used for the experiments reported in section 7.
MeanAveragePrecision(inex-2002) b strict generalized sog Figure 4: Impact of b on inex-2002 mean average precision with k1 = 10 (INEX 2003 CO topics).
Starting with an element ranking generated by the baseline method described in the previous section, elements are re-ranked to control overlap by iteratively adjusting the scores of those elements containing or contained in higher ranking elements. At a conceptual level, re-ranking proceeds as follows:
One approach to adjusting the scores of unreported elements in step 2 might be based on the Okapi BM25 scores of the involved elements. For example, assume a paragraph with score p is reported in step 1. In step 2, the section containing the paragraph might then have its score s lowered by an amount α · p to reflect the reduced contribution the paragraph should make to the section"s score.
In a related context, Robertson et al. [20] argue strongly against the linear combination of Okapi scores in this fashion. That work considers the problem of assigning different weights to different document fields, such as the title and body associated with Web pages. A common approach to this problem scores the title and body separately and generates a final score as a linear combination of the two.
Robertson et al. discuss the theoretical flaws in this approach and demonstrate experimentally that it can actually harm retrieval effectiveness. Instead, they apply the weights at the term frequency level, with an occurrence of a query term t in the title making a greater contribution to the score than an occurrence in the body. In equation 1, xt becomes α0 · yt + α1 · zt, where yt is the number of times t occurs in the title and zt is the number of times t occurs in the body.
Translating this approach to our context, the contribution of terms appearing in elements is dynamically reduced as they are reported. The next section presents and analysis a simple re-ranking algorithm that follows this strategy.
The algorithm is evaluated experimentally in section 7. One limitation of the algorithm is that the contribution of terms appearing in reported elements is reduced by the same factor regardless of the number of reported elements in which it appears. In section 8 the algorithm is extended to apply increasing weights, lowering the score, when a term appears in more than one reported element.
The re-ranking algorithm operates over XML trees, such as the one appearing in figure 2. Input to the algorithm is a list of n elements ranked according to their initial BM25 scores. During the initial ranking the XML tree is dynamically re-constructed to include only those nodes with nonzero BM25 scores, so n may be considerably less than |N |.
Output from the algorithm is a list of the top m elements, ranked according to their adjusted scores.
An element is represented by the node x ∈ N at its root.
Associated with this node are fields storing the length of element, term frequencies, and other information required by the re-ranking algorithm, as follows: x.f - term frequency vector x.g - term frequency adjustments x.l - element length x.score - current Okapi BM25 score x.reported - boolean flag, initially false x.children - set of child nodes x.parent - parent node, if one exists These fields are populated during the initial ranking process, and updated as the algorithm progresses. The vector x.f contains term frequency information corresponding to each term in the query. The vector x.g is initially zero and is updated by the algorithm as elements are reported.
The score field contains the current BM25 score for the element, which will change as the values in x.g change. The score is computed using equation 1, with the xt value for each term determined by a combination of the values in x.f and x.g. Given a term t ∈ Q, let ft be the component of x.f corresponding to t, and let gt be the component of x.g corresponding to t, then: xt = ft − α · gt (2) For processing by the re-ranking algorithm, nodes are stored in priority queues, ordered by decreasing score. Each priority queue PQ supports three operations: PQ.front() - returns the node with greatest score PQ.add (x) - adds node x to the queue PQ.remove(x) - removes node x from the queue When implemented using standard data structures, the front operation requires O(1) time, and the other operations require O(log n) time, where n is the size of the queue.
The core of the re-ranking algorithm is presented in figure 5. The algorithm takes as input the priority queue S containing the initial ranking, and produces the top-m reranked nodes in the priority queue F. After initializing F to be empty on line 1, the algorithm loops m times over lines 215, transferring at least one node from S to F during each iteration. At the start of each iteration, the unreported node at the front of S has the greatest adjusted score, and it is removed and added to F. The algorithm then traverses the
7
11
Figure 5: Re-Ranking Algorithm - As input, the algorithm takes a priority queue S, containing XML nodes ranked by their initial scores, and returns its results in priority queue F, ranked by adjusted scores.
9
Figure 6: Tree traversal routines called by the reranking algorithm.
MeanAveragePrecision(inex-2002) XMLCumulatedGain(XCG) alpha MAP (strict) MAP (generalized) MAP (sog) XCG (sog2) Figure 7: Impact of α on XCG and inex-2002 MAP (INEX 2004 CO topics; assessment set I). node"s ancestors (lines 8-10) and descendants (lines 12-14) adjusting the scores of these nodes.
The tree traversal routines, Up and Down are given in figure 6. The Up routine removes each ancestor node from S, adjusts its term frequency values, recomputes its score, and adds it back into S. The adjustment of the term frequency values (line 3) adds to y.g only the previously unreported term occurrences in x. Re-computation of the score on line 4 uses equations 1 and 2. The Down routine performs a similar operation on each descendant. However, since the contents of each descendant are entirely contained in a reported element its final score may be computed, and it is removed from S and added to F.
In order to determine the time complexity of the algorithm, first note that a node may be an argument to Down at most once. Thereafter, the reported flag of its parent is true. During each call to Down a node may be moved from S to F, requiring O(log n) time. Thus, the total time for all calls to Down is O(n log n), and we may temporarily ignore lines 8-10 of figure 5 when considering the time complexity of the loop over lines 2-15. During each iteration of this loop, a node and each of its ancestors are removed from a priority queue and then added back into a priority queue. Since a node may have at most h ancestors, where h is the maximum height of any tree in the collection, each of the m iterations requires O(h log n) time. Combining these observations produces an overall time complexity of O((n + mh) log n).
In practice, re-ranking an INEX result set requires less than 200ms on a three-year-old desktop PC.
None of the metrics described in section 3.3 is a close fit with the view of overlap advocated by this paper.
Nonetheless, when taken together they provide insight into the behaviour of the re-ranking algorithm. The INEX evaluation packages (inex_eval and inex_eval_ng) were used to compute values for the inex-2002 and inex-2003 metrics. Values for the XCG metrics were computed using software supplied by its inventors [11].
Figure 7 plots the three variants of inex-2002 MAP metric together with the XCG metric. Values for these metrics
MeanAveragePrecision(inex-2003) alpha strict, overlap not considered strict, overlap considered generalized, overlap not considered generalized, overlap considered Figure 8: Impact of α on inex-2003 MAP (INEX
are plotted for values of α between 0.0 and 1.0. Recalling that the XCG metric is designed to penalize overlap, while the inex-2002 metric ignores overlap, the conflict between the metrics is obvious. The MAP values at one extreme (α = 0.0) and the XCG value at the other extreme (α =
systems at INEX 2004 [8,12].
Figure 8 plots values of the inex-2003 MAP metric for two quantizations, with and without consideration of overlap.
Once again, conflict is apparent, with the influence of α substantially lessened when overlap is considered.
One limitation of the re-ranking algorithm is that a single weight α is used to adjust the scores of both the ancestors and descendants of reported elements. An obvious extension is to use different weights in these two cases. Furthermore, the same weight is used regardless of the number of times an element is contained in a reported element. For example, a paragraph may form part of a reported section and then form part of a reported article. Since the user may now have seen this paragraph twice, its score should be further lowered by increasing the value of the weight.
Motivated by these observations, the re-ranking algorithm may be extended with a series of weights
where βj is the weight applied to a node that has been a descendant of a reported node j times. Note that an upper bound on M is h, the maximum height of any XML tree in the collection. However, in practice M is likely to be relatively small (perhaps 3 or 4).
Figure 9 presents replacements for the Up and Down routines of figure 6, incorporating this series of weights. One extra field is required in each node, as follows: x.j - down count The value of x.j is initially set to zero in all nodes and is incremented each time Down is called with x as its argument.
When computing the score of node, the value of x.j selects
12
Figure 9: Extended tree traversal routines. the weight to be applied to the node by adjusting the value of xt in equation 1, as follows: xt = βx.j · (ft − α · gt) (3) where ft and gt are the components of x.f and x.g corresponding to term t.
A few additional changes are required to extend Up and Down. The Up routine returns immediately (line 2) if its argument has already been reported, since term frequencies have already been adjusted in its ancestors. The Down routine does not report its argument, but instead recomputes its score and adds it back into S.
A node cannot be an argument to Down more than M +1 times, which in turn implies an overall time complexity of O((nM + mh) log n). Since M ≤ h and m ≤ n, the time complexity is also O(nh log n).
When generating retrieval results over an XML collection, some overlap in the results should be tolerated, and may be beneficial. For example, when a highly exhaustive and fairly specific (3,2) element contains a much smaller (2,3) element, both should be reported to the user, and retrieval algorithms and evaluation metrics should respect this relationship. The algorithm presented in this paper controls overlap by weighting the terms occurring in reported elements to reflect their reduced importance.
Other approaches may also help to control overlap. For example, when XML retrieval results are presented to users it may be desirable to cluster structurally related elements together, visually illustrating the relationships between them.
While this style of user interface may help a user cope with overlap, the strategy presented in this paper continues to be applicable, by determining the best elements to include in each cluster.
At Waterloo, we continue to develop and test our ideas for INEX 2005. In particular, we are investigating methods for learning the α and βj weights. We are also re-evaluating our approach to document statistics and examining appropriate adjustments to the k1 parameter as term weights change [20].
Thanks to Gabriella Kazai and Arjen de Vries for providing an early version of their software for computing the XCG metric, and thanks to Phil Tilker and Stefan B¨uttcher for their help with the experimental evaluation. In part, funding for this project was provided by IBM Canada through the National Institute for Software Research.
[1] N. Bruno, N. Koudas, and D. Srivastava. Holistic twig joins: Optimal XML pattern matching. In Proceedings of the 2002 ACM SIGMOD International Conference on the Management of Data, pages 310-321, Madison,
Wisconsin, June 2002. [2] D. Carmel, Y. S. Maarek, M. Mandelbrod, Y. Mass, and A. Soffer. Searching XML documents via XML fragments. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 151-158, Toronto, Canada, 2003. [3] C. L. A. Clarke and P. L. Tilker. MultiText experiments for INEX 2004. In INEX 2004 Workshop Proceedings, 2004. Published in LNCS 3493 [8]. [4] A. P. de Vries, G. Kazai, and M. Lalmas. Tolerance to irrelevance: A user-effort oriented evaluation of retrieval systems without predefined retrieval unit. In RIAO 2004 Conference Proceedings, pages 463-473,
Avignon, France, April 2004. [5] D. DeHaan, D. Toman, M. P. Consens, and M. T. ¨Ozsu. A comprehensive XQuery to SQL translation using dynamic interval encoding. In Proceedings of the
Management of Data, San Diego, June 2003. [6] N. Fuhr and K. Großjohann. XIRQL: A query language for information retrieval in XML documents.
In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 172-180, New Orleans,
September 2001. [7] N. Fuhr, M. Lalmas, and S. Malik, editors. Initiative for the Evaluation of XML Retrieval. Proceedings of the Second Workshop (INEX 2003), Dagstuhl,
Germany, December 2003. [8] N. Fuhr, M. Lalmas, S. Malik, and Zolt´an Szl´avik, editors. Initiative for the Evaluation of XML Retrieval. Proceedings of the Third Workshop (INEX 2004), Dagstuhl, Germany, December 2004. Published as Advances in XML Information Retrieval, Lecture Notes in Computer Science, volume 3493, Springer,
[9] K. J¨avelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422-446, 2002. [10] J. Kamps, M. de Rijke, and B. Sigurbj¨ornsson. Length normalization in XML retrieval. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 80-87, Sheffield, UK, July 2004. [11] G. Kazai, M. Lalmas, and A. P. de Vries. The overlap problem in content-oriented XML retrieval evaluation.
In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 72-79, Sheffield, UK,
July 2004. [12] G. Kazai, M. Lalmas, and A. P. de Vries. Reliability tests for the XCG and inex-2002 metrics. In INEX
[13] J. Kek¨al¨ainen, M. Junkkari, P. Arvola, and T. Aalto.
TRIX 2004 - Struggling with the overlap. In INEX
[14] S. Liu, Q. Zou, and W. W. Chu. Configurable indexing and ranking for XML information retrieval.
In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 88-95, Sheffield, UK,
July 2004. [15] Y. Mass and M. Mandelbrod. Retrieving the most relevant XML components. In INEX 2003 Workshop Proceedings, Dagstuhl, Germany, December 2003. [16] Y. Mass and M. Mandelbrod. Component ranking and automatic query refinement for XML retrieval. In INEX 2004 Workshop Proceedings, 2004. Published in LNCS 3493 [8]. [17] P. Ogilvie and J. Callan. Hierarchical language models for XML component retrieval. In INEX 2004 Workshop Proceedings, 2004. Published in LNCS
[18] J. Pehcevski, J. A. Thom, and A. Vercoustre. Hybrid XML retrieval re-visited. In INEX 2004 Workshop Proceedings, 2004. Published in LNCS 3493 [8]. [19] B. Piwowarski and M. Lalmas. Providing consistent and exhaustive relevance assessments for XML retrieval evaluation. In Proceedings of the 13th ACM Conference on Information and Knowledge Management, pages 361-370, Washington, DC,
November 2004. [20] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In Proceedings of the 13th ACM Conference on Information and Knowledge Management, pages 42-50, Washington, DC, November 2004. [21] S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: Automatic ad-hoc, filtering, VLC and interactive track. In Proceedings of the Seventh Text REtrieval Conference, Gaithersburg, MD, November
[22] A. Trotman and B. Sigurbj¨ornsson. NEXI, now and next. In INEX 2004 Workshop Proceedings, 2004.

The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.
Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.
The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.
However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group. For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team. Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.
As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28]. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].
However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the user"s perspective. For example, users are often interested in finding either phone codes or zip codes when entering the query area codes. But the clusters discovered by the current methods may partition the results into local codes and international codes. Such clusters would not be very useful for users; even the best cluster would still have a low precision.
Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem: (1) The clusters are not corresponding to a user"s interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms. For example, the ambiguous query jaguar may mean an animal or a car. A cluster may be labeled as panthera onca. Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.
In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar. More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).
Such aspects can be very useful for organizing future search results about car. Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a user"s perspective, even though the generated clusters are coherent and meaningful in other ways.
Second, we will generate more meaningful cluster labels using past query words entered by users. Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects. Thus they can be better labels than those extracted from the ordinary contents of search results.
To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.
We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.
The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7.
Our work is closely related to the study of clustering search results. In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters. The system Grouper was described in [26, 27]. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22]. However, in all these works, the clusters are generated solely based on the search results.
Thus the obtained clusters do not necessarily reflect users" preferences and the generated labels may not be informative from a user"s viewpoint.
Methods of organizing search results based on text categorization are studied in [6, 8]. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query.
Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4]. Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3],
Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1]. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query. Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner.
Search engine logs record the activities of Web users, which reflect the actual users" needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ...
Table 1: Sample entries of search engine logs.
Different ID"s mean different sessions.
Web search. They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked. Search engine logs are separated by sessions. A session includes a single query and all the URLs that a user clicked after issuing the query [24]. A small sample of search log data is shown in Table 1.
Our idea of using search engine logs is to treat these logs as past history, learn users" interests using this history data automatically, and represent their interests by representative queries. For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car. Different users are probably interested in different aspects of car.
Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio. By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a user"s perspective. As an example, the following is some aspects about car learned from our search log data (see Section 5).
rental, ...
In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection. As shown above, search engine logs consist of sessions. Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks. However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately. To gather rich information, we enrich each URL with additional text content. Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session. All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.
Different sessions may contain the same queries. Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant. In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together. That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.
The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents. All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section.
Our approach is to organize search results by aspects learned from search engine logs. Given an input query, the general procedure of our approach is:
All the information forms a working set.
These aspects correspond to users" interests given the input query. Each aspect is labeled with a representative query.
query according to the aspects learned above.
We now give a detailed presentation of each step.
Given a query q, a search engine will return a ranked list of Web pages. To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.
Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }. Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3. To find q"s related queries in H, a natural way is to use a text retrieval algorithm. Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods. Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively,
IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.
Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in. Each document in H corresponds to a past query, and thus the top ranked documents correspond to q"s related past queries.
Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in. In this subsection, we propose to use a clustering method to discover these aspects.
Any clustering algorithm could be applied here. In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2]. A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally. We describe the star clustering algorithm below.
Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18]. Then the clusters are formed by dense subgraphs that are star-shaped. These clusters form a cover of the similarity graph. Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector. Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .
A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ. Each document di is a vertex of Gσ. If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices. After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows:
unmarked.
the highest degree and let it be u.
that are not marked as center. Mark all the selected neighbors as satellites.
marked.
Each cluster is star-shaped, which consists a single center and several satellites. There is only one parameter σ in the star clustering algorithm. A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small. On the other hand, a small σ will make the clusters big and less coherent. We will study the impact of this parameter in our experiments.
A good feature of the star clustering algorithm is that it outputs a center for each cluster. In the past query collection Hq, each document corresponds to a query. This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally. All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users.
In order to organize the search results according to users" interests, we use the learned aspects from the related past queries to categorize the search results. Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.
In principle, any categorization algorithm can be used here. Here we use a simple centroid-based method for categorization. Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.
Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.
All these pi"s are used to categorize the search results.
Specifically, for any search result sj, we build a TF-IDF vector.
The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi. We then assign sj to the aspect with which it has the highest cosine similarity score.
All the aspects are finally ranked according to the number of search results they have. Within each aspect, the search results are ranked according to their original search engine ranking.
We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].
In total, this log data spans 31 days from 05/01/2006 to 05/31/2006. There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.
To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.
In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters ‘a", ‘b", ..., ‘z", and space, and appear more than 5 times). After cleaning, we get 169,057 unique queries in our history data collection totally. On average, each query has 3.5 distinct clicks. We build the pseudo-documents for all these queries as described in Section 3. The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.
We construct our test data from the last 1/3 data.
According to the time, we separate this data into two test sets equally for cross-validation to set parameters. For each test set, we use every session as a test case. Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases. Different test cases may have the same queries but possibly different clicks.) Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents. Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.
Organizing search results into different aspects is expected to help informational queries. It thus makes sense to focus on the informational queries in our evaluation. For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query. Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.
Finally, we obtain 172 and 177 test cases in the first and second test sets respectively. On average, we have 6.23 and
In the section, we describe our experiments on the search result organization based past search engine logs.
We use two baseline methods to evaluate the proposed method for organizing search results. For each test case, the first method is the default ranked list from a search engine (baseline). The second method is to organize the search results by clustering them (cluster-based). For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering). That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters. We compare our method (log-based) with the two baseline methods in the following experiments. For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.
To compare different result organization methods, we adopt a similar method as in the paper [9]. That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.
Organizing search results into clusters is to help users navigate into relevant documents quickly. The above metric is to simulate a scenario when users always choose the right cluster and look into it. Specifically, we download and organize the top 100 search results into aspects for each test case. We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods. P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents. We also use Mean Reciprocal Rank (MRR) as another metric. MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.
To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect. The number of aspects is fixed at 10 in all the following experiments.
The star clustering algorithm can output different number of clusters for different input. To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates. We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid. In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric.
Our main hypothesis is that organizing search results based on the users" interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results. In the following, we test our hypothesis from two perspectives - organization and labeling.
Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5. We also show the percentage of relative improvement in the lower part.
Comparison Test set 1 Test set 2 Impr./Decr. Impr./Decr.
Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5"s are improved versus decreased w.r.t the baseline.
We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5. We optimize the parameter σ"s for each collection individually based on P@5 values. This shows the best performance that each method can achieve. In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods. For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is
accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement). The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method. Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy. This is because cluster-based method organizes the search results only based on the contents. Thus it could organize the results differently from users" preferences. This confirms our hypothesis of the bias of the cluster-based method. Comparing our method with the cluster-based method, we achieve significant improvement on both test collections. The p-values of the significance tests based on P@5 on both collections are 0.01 and
effective to learn users" preferences from the past query history, and thus it can organize the search results in a more useful way to users.
We showed the optimal results above. To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set. We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1. We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.
However, our method still performs much better than the optimal results of cluster-based method on both test collections.
Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection. We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.
In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased. We can see that our method improves more test cases compared with the other two methods. In the next section, we show more detailed analysis to see what types of test cases can be improved by our method.
To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty. All the analysis below is based on test set 1.
Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters. In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity. If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse. In this case, we would expect our method to help more. The results are shown in Figure 2. In this figure, we partition the ratios into 4 bins. The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.) In each bin, we count the numbers of test cases whose P@5"s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure. We can observe that when the ratio is smaller, the log-based method can improve more test cases. But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline. For example, in bin 1, 48 test cases are improved and 34 are decreased. But in bin 4, all the 4 test cases are decreased. This confirms our hypothesis that our method can help more if the query has more diverse results.
This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).
Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5]. Here we analyze the effectiveness of our method in helping difficult queries. We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case. We then order the 172 test cases in test set 1 in an increasing order of MAP values. We partition the test cases into 4 bins with each having a roughly equal number of test cases. A small MAP means that the utility of the original ranking is low. Bin 1 contains those test cases with the lowest MAP"s and bin 4 contains those test cases with the highest MAP"s.
For each bin, we compute the numbers of test cases whose P@5"s are improved versus decreased. Figure 3 shows the results. Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20). This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries. This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries.
We examine parameter sensitivity in this section. For the star clustering algorithm, we study the similarity threshold parameter σ. For the OKAPI retrieval function, we study the parameters k1 and b. We also study the impact of the number of past queries retrieved in our log-based method.
Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets. We vary σ from 0.05 to 0.3 with step 0.05. Figure 4 shows that the performance is not very sensitive to the parameter σ. We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.
In Table 4, we show the impact of OKAPI parameters.
We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2. From this table, it is clear that P@5 is also not very sensitive to the parameter setting. Most of the values are larger than 0.35. The default values k1 = 1.2 and b = 0.8 give approximately optimal results.
We further study the impact of the amount of history
P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods. We show the result on both test collections. b
k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465
Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects. The results on both test collections are shown in Figure 5. We can see that the performance gradually increases as we enlarge the number of past queries retrieved. Thus our method could potentially learn more as we accumulate more history. More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries.
We use the query area codes to show the difference in the results of the log-based method and the cluster-based method. This query may mean phone codes or zip codes.
Table 5 shows the representative keywords extracted from the three biggest clusters of both methods. In the clusterbased method, the results are partitioned based on locations: local or international. In the log-based method, the results are disambiguated into two senses: phone codes or zip codes. While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively. Therefore our log-based method is more effective in helping users to navigate into their desired results.
Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method
1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved.
We now compare the labels between the cluster-based method and log-based method. The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster. Our log-based method can avoid this difficulty by taking advantage of queries.
Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label. For log-based method, we use the center of each star cluster as the label for the corresponding cluster.
In general, it is not easy to quantify the readability of a cluster label automatically. We use examples to show the difference between the cluster-based and the log-based methods. In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple. For the cluster-based method, we separate keywords by commas since they do not form a phrase. From this table, we can see that our log-based method gives more readable labels because it generates labels based on users" queries. This is another advantage of our way of organizing search results over the clustering approach.
Label comparison for query jaguar Log-based method Cluster-based method
Label comparison for query apple Log-based method Cluster-based method
Table 6: Cluster label comparison.
In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users" perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.
Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.
There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view). It would thus be interesting to study how to further improve the organization of the results based on such feedback information.
Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user.
We thank the anonymous reviewers for their valuable comments. This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933.
[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR, pages 19-26, 2006. [2] J. A. Aslam, E. Pelekov, and D. Rus. The star clustering algorithm for static and dynamic information organization. Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Applications of web query mining.
In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger. Agglomerative clustering of a search engine query log. In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.
What makes a query difficult? In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais. Bringing order to the web: automatically categorizing search results. In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.
Predicting query performance. In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/gather on retrieval results.
In SIGIR, pages 76-84, 1996. [10] T. Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133-142, 2002. [11] T. Joachims. Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96. Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I.
Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.
Generating query substitutions. In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. A hierarchical monothetic document clustering algorithm for summarization and browsing search results. In WWW, pages 658-665,
[14] Microsoft Live Labs. Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.
Scatter/gather browsing communicates the topic structure of a very large text collection. In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using implicit feedback. In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen. Information Retrieval, second edition. Butterworths, London, 1979. [21] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Latent semantic analysis for multiple-type interrelated data objects. In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user queries of a search engine. In WWW, pages 162-168,
[25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.
Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni. Web document clustering: A feasibility demonstration. In SIGIR, pages 46-54,
[27] O. Zamir and O. Etzioni. Grouper: A dynamic clustering interface to web search results. Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.

In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.
From a single query, however, the retrieval system can only have very limited clue about the user"s information need. An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available. Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].
There are many kinds of context that we can exploit. Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy. However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents. Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information. Thus the effectiveness of relevance feedback may be limited in real applications.
For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12]. In general, the retrieval results using the user"s initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8]. For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied. In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading). We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.
A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort. For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia. As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island. However, any particular user is unlikely searching for both types of documents. Such an ambiguity can be resolved by exploiting history information. For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.
Implicit feedback was studied in several previous works. In [11],
Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people. In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated. In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user. Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].
While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval. Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy. We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information. We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.
One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation. We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models. To the best of our knowledge, this is the first test set for implicit feedback. We evaluate the proposed models using this data set. The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.
The remaining sections are organized as follows. In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later. In Section 3, we propose several implicit feedback models based on statistical language models. In Section 4, we describe how we create the data set for implicit feedback experiments. In Section 5, we evaluate different implicit feedback models on the created data set. Section 6 is our conclusions and future work.
There are two kinds of context information we can use for implicit feedback. One is short-term context, which is the immediate surrounding information which throws light on a user"s current information need in a single session. A session can be considered as a period consisting of all interactions for the same information need.
The category of a user"s information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context. Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search. In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session. The other kind of context is long-term context, which refers to information such as a user"s education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time. Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session. In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.
In a single search session, a user may interact with the search system several times. During interactions, the user would continuously modify the query. Therefore for the current query Qk (except for the first query of a search session) , there is a query history,
HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session. Note that we assume that the session boundaries are known in this paper.
In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16]. Traditionally, the retrieval system only uses the current query Qk to do retrieval. But the short-term query history clearly may provide useful clues about the user"s current information need as seen in the java example given in the previous section. Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.
In addition to the query history, there may be other short-term context information available. For example, a user would presumably frequently click some documents to view. We refer to data associated with these actions as clickthrough history. The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.
Although it is not clear whether a viewed document is actually relevant to the user"s information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the user"s information need. Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval. In general, we may have a history of clicked summaries C1, ..., Ck−1.
We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk. Previous work has also shown positive results using similar clickthrough information [11, 17].
Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them. In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models.
CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk. An important research question is how we can exploit such information effectively. We propose to use statistical language models to model a user"s information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model.
We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method. According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.
One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.
Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk. Let HC = (C1, ..., Ck−1) be the clickthrough history. Note that Ci is the concatenation of all clicked documents" summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally. Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC . We now describe several different language models for exploiting HQ and HC to estimate p(w|θk). We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked document"s summary or any other text. We will use |X| to denote the length of text X or the total number of words in X.
Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ). Then we linearly interpolate these two history models to obtain the history model p(w|H). Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk). These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.
If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ).
One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries. But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history. To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.
The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ). We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length. Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α. Later we will show that such an adaptive α empirically performs better than a fixed α.
Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries. This means that all previous queries are treated equally and so are all clicked summaries.
However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better. Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable. Interestingly, if we incrementally update our belief about the user"s information need after seeing each query, we could naturally obtain decaying weights on the previous queries. Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.
In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents. In order to rank documents, the system must have some model for the user"s information need. In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query. A principled way of updating the query model is to use Bayesian estimation, which we discuss below.
We first discuss how we apply Bayesian estimation to update a query model in general. Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary). To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior. We use Dirichlet prior because it is a conjugate prior for multinomial distributions. With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T.
Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T.
We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.
In general, we assume that the retrieval system maintains a current query model φi at any moment. As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.
Initially, before we see any user query, we may already have some information about the user. For example, we may have some information about what documents the user has viewed in the past.
We use such information to define a prior on the query model, which is denoted by φ0. After we observe the first query Q1, we can update the query model based on the new observed data Q1.
The updated query model φ1 can then be used for ranking documents in response to Q1. As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1. As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2. In general, we may repeat such an updating process to iteratively update the query model.
Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci. In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data. Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.
If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci). On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model. Thus the model remains the same as if we do not observe any new text evidence. In general, the parameters µi and νi may have different values for different i. For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi. But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.
Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents. This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.
To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk)
If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight. This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document. One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries. The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries. As in OnlineUp, we set all µi"s and νi"s to the same value. And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk)
In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic. Since there is no such data set available to us, we have to create one. There are two choices. One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine). But the problem is that we have no relevance judgments on such data. The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file. Unfortunately, there are no query history and clickthrough history data. We decide to augment a TREC data set by collecting query history and clickthrough history data.
We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments. There are altogether 242918 news articles and the average document length is 416 words. Most articles have titles. If not, we select the first sentence of the text as the title. For the preprocessing, we only do case folding and do not do stopword removal or stemming.
We select 30 relatively difficult topics from TREC topics 1-150.
These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20]. The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user. In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.
We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles. We use 3 subjects to do experiments to collect query history and clickthrough history data.
Each subject is assigned 10 topics and given the topic descriptions provided by TREC. For each topic, the first query is the title of the topic given in the original TREC topic description. After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject. The subject will browse the results and maybe click one or more results to browse the full text of article(s). The subject may also modify the query to do another search. For each topic, the subject composes at least 4 queries. In our experiment, only the first 4 queries for each topic are used. The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study. We use a relational database to store user interactions, including the submitted queries and clicked documents. For each query, we store the query terms and the associated result pages.
And for each clicked document, we store the summary as shown on the search result page. The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).
Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words. Altogether there are 91 documents clicked to view. So on average, there are around 3 clicks per topic. The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words. Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file. This data set is publicly available 1 .
Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy. In particular, the search context can provide extra information to help us estimate a better query model than using just the current query. So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.
Since we collected four versions of queries for each topic, we make such comparisons for each version of queries. We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents. In all cases, the reported figure is the average over all of the 30 topics.
We evaluate the four models for exploiting search context (i.e.,
FixInt, BayesInt, OnlineUp, and BatchUp). Each model has precisely two parameters (α and β for FixInt; µ and ν for others).
Note that µ and ν may need to be interpreted differently for different methods. We vary these parameters and identify the optimal performance for each method. We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters.
We compare the optimal performances of four models with those using the current query only in Table 1. A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context. We can make several observations from this table:
reformulated queries are better than the previous queries with the performance of q4 being the best. Users generally formulate better and better queries.
when the context is rich. This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2. Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse. When the search context is rich, the performance improvement can be quite substantial. For example, BatchUp achieves
make the relative improvement deceptively high, though.)
of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp. Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense. The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries. Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.
While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient. Overall, BatchUp appears to be the best method when we vary the parameter settings.
We have two different kinds of search context - query history and clickthrough data. We now look into the contribution of each kind of context.
In each of four models, we can turn off the clickthrough history data by setting parameters appropriately. This allows us to evaluate the effect of using query history alone. We use the same parameter setting for query history as in Table 1. The results are shown in Table 2. Here we see that in general, the benefit of using query history is very limited with mixed results. This is different from what is reported in a previous study [15], where using query history is consistently helpful. Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4. This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations. Yet another observation is that when using query history only, the BayesInt model appears to be better than other models. Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm. The displayed results thus reflect the variation caused by parameter µ. A smaller setting of 2.0 is seen better than a larger value of 5.0. A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.
The value of µ can be interpreted as how many words we regard the query history is worth. A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich. Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2. As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information. This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.
The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection.
We now turn off the query history and only use the clicked summaries plus the current query. The results are shown in Table 4. We see that the benefit of using clickthrough information is much more significant than that of using query history. We see an overall positive effect, often with significant improvement over the baseline. It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve. Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.
These results show that the clicked summary text is in general quite useful for inferring a user"s information need. Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant.
query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.
However, such improvement is really not beneficial for the user as the user has already seen these relevant documents. To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file. The results are shown in Table 5. Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results. From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.
Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant. To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4. The results are shown in Table 6. We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries. These results should be interpreted as very encouraging as they are based on only
click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.
FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.
Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data
By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information. In Table 7, we show this effect for the BatchUp method.
All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model. In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others. BatchUp has two parameters µ and ν.
We first look at µ. When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query. If we increase µ, we will gradually incorporate more information from the previous queries. In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved. We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.
The pattern is also similar when we set ν to other values.
In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.
The best performance is generally achieved when µ is around
about 2 words in the current query. Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.
We now turn to the other parameter ν. When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query. With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9. We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15. This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.
Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust.
In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance. Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt,
OnlineUp and BatchUp. We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models. Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.
The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information. It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history. For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query. Second, the proposed models can be implemented in any practical systems. We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms. We will also do a user study to evaluate effectiveness of these models in the real web search. Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models.
This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472. We thank the anonymous reviewers for their useful comments.
[1] E. Adar and D. Karger. Haystack: Per-user information environments. In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al. Challenges in information retrieval and language modeling. Workshop at University of Amherst,
[3] K. Bharat. Searchpad: Explicit capture of search context to support web search. In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.
Relevance feedback and personalization: A language modeling perspective. In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit queries (IQ) for contextualized search (demo description). In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
G. Wolfman, and E. Ruppin. Placing search in context: The concept revisited. In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang. Query session based term suggestion for interactive web search. In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic web log session identification with statistical language models. Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom. Scaling personalized web search. In Proceeding of WWW 2003, 2003. [11] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin. Display time as implicit feedback: Understanding task effects. In Proceedings of SIGIR 2004,
[13] D. Kelly and J. Teevan. Implicit feedback for inferring user preference. SIGIR Forum, 32(2), 2003. [14] J. Rocchio. Relevance feedback information retrieval. In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.
Prentice-Hall. [15] X. Shen and C. Zhai. Exploiting query history for document ranking in interactive information retrieval (poster). In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai. A session-based search engine (poster). In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based on user profile constructed without any effort from users. In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven. A simulated study of implicit feedback models.

Millions of users interact with search engines daily. They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions. These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.
Implicit relevance feedback for ranking and personalization has become an active area of research. Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process. Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval. How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank? While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.
Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned. To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.
The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).
We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper.
Ranking search results is a fundamental problem in information retrieval. Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].
However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.
Implicit relevance measures have been studied by several research groups. An overview of implicit measures is compiled in Kelly and Teevan [14]. This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.
Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions. This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior. However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions. Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.
More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting. Unfortunately, the extent to which previous research applies to real-world web search is unclear. At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.
We build on existing research to develop robust user behavior interpretation techniques for the real web search setting. Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections.
FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm. We describe the two general ranking approaches next. The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5.
Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions. Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.
While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers" scores, and instead simply merge the rank orders.
The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.
We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets). We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers. For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result. We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.
The query results are ordered in by decreasing values of SM to produce the final ranking. One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline. Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.
The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features. We now relax this assumption by integrating implicit feedback features directly into the ranking process.
Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain). In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.
Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm. During training or tuning, the ranker can be tuned as before but with additional features. At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair. This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available. We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback.
A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]). In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results. Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.
RankNet is one such algorithm. It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences. While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods. An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.
We use a 2-layer implementation of RankNet in order to model non-linear relationships between features. Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques. Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives.
Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine. Interpreting implicit feedback in real web search setting is not an easy task.
We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.
The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results. We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments.
We model observed web search behaviors as a combination of a ``background"" component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance"" component (i.e., query-specific behavior indicative of relevance of a result to a query). We design our features to take advantage of aggregated user behavior. The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.
The features used to represent user interactions with web search results are summarized in Table 4.1. This information was obtained via opt-in client-side instrumentation from users of a major web search engine.
We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position. We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time. Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust. For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary. To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.
Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.
Having described our feature set, we briefly review our general method for deriving a user behavior model.
To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries. We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.
Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.
These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model. This approach is particularly attractive as it does not require heuristics beyond feature engineering. The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below.
The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.
Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.
In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions. We describe this dataset in detail next. Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section
We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs. The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution. On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad. Overall, there were over 83,000 results with explicit relevance judgments. In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.
Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.
The user interactions were collected over a period of 8 weeks using voluntary opt-in information. In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine. The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted. These actions were aggregated across users and search sessions and converted to features in Table 4.1.
To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and
there was no overlap in training, validation, and test queries.
We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)),
Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP). Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant. In our setting, we require a relevant document to be labeled Good or higher. The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10]. For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j. Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions. NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved. The final MAP value is defined as the mean of average precisions of all queries in the test set. This metric is the most commonly used single-value summary of a run over a set of queries.
Recall that our goal is to quantify the effectiveness of implicit behavior for real web search. One dimension is to compare the utility of implicit feedback with other information available to a web search engine. Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27]. BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline. The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth). The scoring function and field-specific tuning is described in detail in [23]. Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result. This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries. A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above. Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance. This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d.
In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks). The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results. This method serves as our baseline implicit feedback reranking method.
BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).
This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2). At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced. The merged ranking is computed as described in Section 3.1. Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2). We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).
The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN). As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents.
Implicit feedback for web search ranking can be exploited in a number of ways. We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features). We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2. The results were averaged over three random splits of the overall dataset. Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint. We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings). We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.
We first experimented with different methods of re-ranking the output of the BM25F search results. Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1). Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone. The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to
from 0.5 to 0.63. Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback.
NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT,
BM25F-Rerank-All, and BM25F+All for varying K
K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT,
BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1. While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0
1
Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).
If users considered only the relevance of a result to their query, they would click on the topmost relevant results. Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically. Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant. Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure. For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5. As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.
We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.
RN incorporates BM25F, link-based features, and hundreds of other features. Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings. In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set.
NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods. This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine.
K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1. We report the Mean Average Precision (MAP) score for each system. While not intuitive to interpret, MAP allows quantitative comparison on a single metric. The gains marked with * are significant at p=0.01 level using two tailed t-test.
MAP Gain P(1) Gain BM25F 0.184 -
BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 -
Table 6.1: Mean Average Precision (MAP) for all strategies.
So far we reported results averaged across all queries in the test set. Unfortunately, less than half had sufficient interactions to attempt reranking. Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least
the user). This is not surprising: web search is heavy-tailed, and there are many unique queries. We now consider the performance on the queries for which user interactions were available. Figure
implicit feedback features. The gains at top 1 are dramatic. The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set.
NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users. When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system.
Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F,
BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2. MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.
Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful. Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6. Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1). Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score. One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350
-0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0
Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines. Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest. Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking. The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant.
In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking. We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.
We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function. Our experiments showed significant improvement over methods that do not consider implicit feedback.
The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.
Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).
One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries. As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.
ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments. We also thank Robert Ragno for his valuable suggestions and many discussions.
[1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T.
White. Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick. Learning users" interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay,
Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.
Advances in Kernel Methods, in Support Vector Learning,
MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl. GroupLens: Applying collaborative filtering to usenet news. In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.
Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim. Implicit feedback for recommender systems. In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim. Modeling information content using observable behavior. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin. The SST method: a tool for analyzing web information search processes. In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web. In Working with Technology in Mind: Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, and W.G. Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S.

Search engine queries are often associated with geographical locations, either explicitly (i.e. a location reference is given as part of the query) or implicitly (i.e. the location reference is not present in the query string, but the query clearly has a local intent [17]). One of the concerns of geographical information retrieval (GIR) lies in appropriately handling such queries, bringing better targeted search results and improving user satisfaction.
Nowadays, GIR is getting increasing attention. Systems that access resources on the basis of geographic context are starting to appear, both in the academic and commercial domains [4, 7].
Accurately and effectively detecting location references in search engine queries is a crucial aspect of these systems, as they are generally based on interpreting geographical terms differently from the others. Detecting locations in queries is also important for generalpropose search engines, as this information can be used to improve ranking algorithms. Queries with a local intent are best answered with localized pages, while queries without any geographical references are best answered with broad pages [5].
Text mining methods have been successfully used in GIR to detect and disambiguate geographical references in text [9], or even to infer geographic scopes for documents [1, 13]. However, this body of research has been focused on processing Web pages and full-text documents. Search engine queries are more difficult to handle, in the sense that they are very short and with implicit and subjective user intents. Moreover, the data is also noisier and more versatile in form, and we have to deal with misspellings, multilingualism and acronyms. How to automatically understand what the user intended, given a search query, without putting the burden in the user himself, remains an open text mining problem.
Key challenges in handling locations over search engine queries include their detection and disambiguation, the ranking of possible candidates, the detection of false positives (i.e not all contained location names refer to geographical locations), and the detection of implied locations by the context of the query (i.e. when the query does not explicitly contain a place reference but it is nonetheless geographical). Simple named entity recognition (NER) algorithms, based on dictionary look-ups for geographical names, may introduce high false positives for queries whose location names do not constitute place references. For example the query Denzel Washington contains the place name Washington, but the query is not geographical. Queries can also be geographic without containing any explicit reference to locations at the dictionary. In these cases, place name extraction and disambiguation does not give any results, and we need to access other sources of information.
This paper proposes simple and yet effective techniques for handling place references over queries. Each query is split into a triple < what,relation,where >, where what specifies the non-geographic aspect of the information need, where specifies the geographic areas of interest, and relation specifies a spatial relationship connecting what and where. When this is not possible, i.e. the query does not contain any place references, we try using information from documents matching the query, exploiting geographic scopes previously assigned to these documents.
Disambiguating place references is one of the most important aspects. We use a search procedure that combines textual patterns with geographical names defined at an ontology, and we use heuristics to disambiguate the discovered references (e.g. more important places are preferred). Disambiguation results in having the where term, from the triple above, associated with the most likely corresponding concepts from the ontology. When we cannot detect any locations, we attempt to use geographical scopes previously inferred for the documents at the top search results. By doing this, we assume that the most frequent geographical scope in the results should correspond to the geographical context implicit in the query.
Experiments with CLEF topics [4] and sample queries from a Web search engine show that the proposed methods are accurate, and may have applications in improving search results.
The rest of this paper is organized as follows. We first formalize the problem and describe related work to our research. Next, we describe our approach for handling place names in queries, starting with the general approach for disambiguating place references over textual strings, then presenting the method for splitting a query into a < what,relation,where > triple, and finally discussing the technique for exploiting geographic scopes previously assigned to documents in the result set. Section 4 presents evaluation results.
Finally, we give some conclusions and directions for future research.
Search engine performance depends on the ability to capture the most likely meaning of a query as intended by the user [16].
Previous studies showed that a significant portion of the queries submitted to search engines are geographic [8, 14]. A recent enhancement to search engine technology is the addition of geographic reasoning, combining geographic information systems and information retrieval in order to build search engines that find information associated with given locations. The ability to recognize and reason about the geographical terminology, given in the text documents and user queries, is a crucial aspect of these geographical information retrieval (GIR) systems [4, 7].
Extracting and distinguishing different types of entities in text is usually referred to as Named Entity Recognition (NER). For at least a decade, this has been an important text mining task, and a key feature of the Message Understanding Conferences (MUC) [3]. NER has been successfully automated with near-human performance, but the specific problem of recognizing geographical references presents additional challenges [9]. When handling named entities with a high level of detail, ambiguity problems arise more frequently. Ambiguity in geographical references is bi-directional [15].
The same name can be used for more than one location (referent ambiguity), and the same location can have more than one name (reference ambiguity). The former has another twist, since the same name can be used for locations as well as for other class of entities, such as persons or company names (referent class ambiguity).
Besides the recognition of geographical expressions, GIR also requires that the recognized expressions be classified and grounded to unique identifiers [11]. Grounding the recognized expressions (e.g. associating them to coordinates or concepts at an ontology) assures that they can be used in more advanced GIR tasks.
Previous works have addressed the tagging and grounding of locations in Web pages, as well as the assignment of geographic scopes to these documents [1, 7, 13]. This is a complementary aspect to the techniques described in this paper, since if we have the Web pages tagged with location information, a search engine can conveniently return pages with a geographical scope related to the scope of the query. The task of handling geographical references over documents is however considerably different from that of handling geographical references over queries. In our case, queries are usually short and often do not constitute proper sentences. Text mining techniques that make use of context information are difficult to apply for high accuracy.
Previous studies have also addressed the use of text mining and automated classification techniques over search engine queries [16, 10]. However, most of these works did not consider place references or geographical categories. Again, these previously proposed methods are difficult to apply to the geographic domain.
Gravano et. al. studied the classification of Web queries into two types, namely local and global [5]. They defined a query as local if its best matches on a Web search engine are likely to be local pages, such as houses for sale. A number of classification algorithms have been evaluated using search engine queries. However, their experimental results showed that only a rather low precision and recall could be achieved. The problem addressed in this paper is also slightly different, since we are trying not only to detect local queries but also to disambiguate the local of interest.
Wang et. al. proposed to go further than detecting local queries, by also disambiguating the implicit local of interest [17]. The proposed approach works for both queries containing place references and queries not containing them, by looking for dominant geographic references over query logs and text from search results.
In comparison, we propose simpler techniques based on matching names from a geographic ontology. Our approach looks for spatial relationships at the query string, and it also associates the place references to ontology concepts. In the case of queries not containing explicit place references, we use geographical scopes previously assigned to the documents, whereas Wang et. al. proposed to extract locations from the text of the top search results.
There are nowadays many geocoding, reverse-geocoding, and mapping services on the Web that can be easily integrated with other applications. Geocoding is the process of locating points on the surface of the Earth from alphanumeric addressing data. Taking a string with an address, a geocoder queries a geographical information system and returns interpolated coordinate values for the given location. Instead of computing coordinates for a given place reference, the technique described in this paper aims at assigning references to the corresponding ontology concepts. However, if each concept at the ontology contains associated coordinate information, the approach described here could also be used to build a geocoding service. Most of such existing services are commercial in nature, and there are no technical publications describing them.
A number of commercial search services have also started to support location-based searches. Google Local, for instance, initially required the user to specify a location qualifier separately from the search query. More recently, it added location look-up capabilities that extract locations from query strings. For example, in a search for Pizza Seattle, Google Local returns local results for pizza near Seattle, WA. However, the intrinsics of their solution are not published, and their approach also does not handle locationimplicit queries. Moreover, Google Local does not take spatial relations into account.
In sum, there are already some studies on tagging geographical references, but Web queries pose additional challenges which have not been addressed. In this paper, we explain the proposed solutions for the identified problems.
Most GIR queries can be parsed to < what,relation,where > triple, where the what term is used to specify the general nongeographical aspect of the information need, the where term is used to specify the geographical areas of interest, and the relation term is used to specify a spatial relationship connecting what and where.
While the what term can assume any form, in order to reflect any information need, the relation and where terms should be part of a controlled vocabulary. In particular, the relation term should refer to a well-known geographical relation that the underlying GIR system can interpret (e.g. near or contained at), and the where term should be disambiguated into a set of unique identifiers, corresponding to concepts at the ontology.
Different systems can use alternative schemes to take input queries from the users. Three general strategies can be identified, and GIR systems often support more than one of the following schemes: Figure 1: Strategies for processing queries in Geographical Information Retrieval systems.
hardest case, since we need to separate the query into the three different components, and then we need to disambiguate the where term into a set of unique identifiers.
concerning the what term, and the other concerning the where.
The relation term can be either fixed (e.g. always assume the near relation), specified together with the where string, or provided separately from the users from a set of possible choices. Although there is no need for separating query string into the different components, we still need to disambiguate the where term into a set of unique identifiers.
together with an unambiguous description of the geographical area of interest (e.g. a sketch in a map, spatial coordinates or a selection from a set of possible choices). No disambiguation is required, and therefore the techniques described in this paper do not have to be applied.
The first two schemes depend on place name disambiguation.
Figure 1 illustrates how we propose to handle geographic queries in these first two schemes. A common component is the algorithm for disambiguating place references into corresponding ontology concepts, which is described next.
A required task in handling GIR queries consists of associating a string containing a geographical reference with the set of corresponding concepts at the geographic ontology. We propose to do this according to the pseudo-code listed at Algorithm 1.
The algorithm considers the cases where a second (or even more than one) location is given to qualify a first (e.g. Paris, France).
It makes recursive calls to match each location, and relies on hierarchical part-of relations to detect if two locations share a common hierarchy path. One of the provided locations should be more general and the other more specific, in the sense that there must exist a part-of relationship among the associated concepts at the ontology (either direct or transitive). The most specific location is a sub-region of the most general, and the algorithm returns the most specific one (i.e. for Paris, France the algorithm returns the ontology concept associated with Paris, the capital city of France).
We also consider the cases where a geographical type expression is used to qualify a given name (e.g. city of Lisbon or state of New York). For instance the name Lisbon can correspond to many different concepts at a geographical ontology, and type Algorithm 1 Matching a place name with ontology concepts Require: O = A geographic ontology Require: GN = A string with the geographic name to be matched 1: L = An empty list 2: INDEX = The position in GN for the first occurrence of a comma, semi-colon or bracket character 3: if INDEX is defined then 4: GN1 = The substring of GN from position 0 to INDEX 5: GN2 = The substring of GN from INDEX +1 to length(GN) 6: L1 = Algorithm1(O,GN1) 7: L2 = Algorithm1(O,GN2) 8: for each C1 in L1 do 9: for each C2 in L2 do 10: if C1 is an ancestor of C2 at O then 11: L = The list L after adding element C2 12: else if C1 is a descendant of C2 at O then 13: L = The list L after adding element C1 14: end if 15: end for 16: end for 17: else 18: GN = The string GN after removing case and diacritics 19: if GN contains a geographic type qualifier then 20: T = The substring of GN containing the type qualifier 21: GN = The substring of GN with the type qualifier removed 22: L = The list of concepts from O with name GN and type T 23: else 24: L = The list of concepts from O with name GN 25: end if 26: end if 27: return The list L qualifiers can provide useful information for disambiguation. The considered type qualifiers should also described at the ontologies (e.g. each geographic concept should be associated to a type that is also defined at the ontology, such as country, district or city).
Ideally, the geographical reference provided by the user should be disambiguated into a single ontology concept. However, this is not always possible, since the user may not provide all the required information (i.e. a type expression or a second qualifying location).
The output is therefore a list with the possible concepts being referred to by the user. In a final step, we propose to sort this list, so that if a single concept is required as output, we can use the one that is ranked higher. The sorting procedure reflects the likelihood of each concept being indeed the one referred to. We propose to rank concepts according to the following heuristics:
ontology concept. For the same name, a country is more likely to be referenced than a city, and in turn a city more likely to be referenced than a street.
ontology tend to be more general, and are therefore more likely to be referenced in search engine queries.
and therefore more likely to be referenced in queries.
Subregions of highly populated places are better known, and also more likely to be referenced in search engine queries.
counts) for the geographical names. Places names that occur more frequently over Web documents are also more likely to be referenced in search engine queries.
more sub-regions tend to be more general, and are therefore more likely to be mentioned in search engine queries.
likely to be mentioned in search engine queries.
Algorithm 1, plus the ranking procedure, can already handle GIR queries where the where term is given separately from the what and relation terms. However, if the query is given in a single string, we require the identification of the associated < what,relation,where > triple, before disambiguating the where term into the corresponding ontology concepts. This is described in the following Section.
Algorithm 2 provides the mechanism for separating a query string into a < what,relation,where > triple. It uses Algorithm 1 to find the where term, disambiguating it into a set of ontology concepts.
The algorithm starts by tokenizing the query string into individual words, also taking care of removing case and diacritics. We have a simple tokenizer that uses the space character as a word delimiter, but we could also have a tokenization approach similar to the proposal of Wang et. al. which relies on Web occurrence statistics to avoid breaking collocations [17]. In the future, we plan on testing if this different tokenization scheme can improve results.
Next, the algorithm tests different possible splits of the query, building the what, relation and where terms through concatenations of the individual tokens. The relation term is matched against a list of possible values (e.g. near, at, around, or south of), corresponding to the operators that are supported by the GIR system. Note that is also the responsibility of the underlying GIR system to interpret the actual meaning of the different spatial relations. Algorithm 1 is used to check whether a where term constitutes a geographical reference or not. We also check if the last word in the what term belongs to a list of exceptions, containing for instance first names of people in different languages. This ensures that a query like Denzel Washington is appropriately handled.
If the algorithm succeeds in finding valid relation and where terms, then the corresponding triple is returned. Otherwise, we return a triple with the what term equaling the query string, and the relation and where terms set as empty. If the entire query string constitutes a geographical reference, we return a triple with the what term set to empty, the where term equaling the query string, and the relation term set the DEFINITION (i.e. these queries should be answered with information about the given place references). The algorithm also handles query strings where more than one geographical reference is provided, using and or an equivalent preposition, together with a recursive call to Algorithm
Algorithm 2 Get < what,relation,where > from a query string Require: O = A geographical ontology Require: Q = A non-empty string with the query 1: Q = The string Q after removing case and diacritics 2: TOKENS[0..N] = An array of strings with the individual words of Q 3: N = The size of the TOKENS array 4: for INDEX = 0 to N do 5: if INDEX = 0 then 6: WHAT = Concatenation of TOKENS[0..INDEX −1] 7: LASTWHAT = TOKENS[INDEX −1] 8: else 9: WHAT = An empty string 10: LASTWHAT = An empty string 11: end if 12: WHERE = Concatenation of TOKENS[INDEX..N] 13: RELATION = An empty string 14: for INDEX2 = INDEX to N −1 do 15: RELATION2 = Concatenation of TOKENS[INDEX..INDEX2] 16: if RELATION2 is a valid geographical relation then 17: WHERE = Concatenation of S[INDEX2 +1..N] 18: RELATION = RELATION2; 19: end if 20: end for 21: if RELATION = empty AND LASTWHAT in an exception then 22: TESTGEO = FALSE 23: else 24: TESTGEO = TRUE 25: end if 26: if TESTGEO AND Algorithm1(WHERE) <> EMPTY then 27: if WHERE ends with AND SURROUNDINGS then 28: RELATION = The string NEAR; 29: WHERE = The substring of WHERE with AND SURROUNDINGS removed 30: end if 31: if WHAT ends with AND or similar) then 32: < WHAT,RELATION,WHERE2 >= Algorithm2(WHAT) 33: WHERE = Concatenation of WHERE with WHERE2 34: end if 35: if RELATION = An empty string then 36: if WHAT = An empty string then 37: RELATION = The string DEFINITION 38: else 39: RELATION = The string CONTAINED-AT 40: end if 41: end if 42: else 43: WHAT = The string Q 44: WHERE = An empty string 45: RELATION = An empty string 46: end if 47: end for 48: return < WHAT,RELATION,WHERE > therefore appropriately handled. Finally, if the geographical reference in the query is complemented with an expression similar to and its surroundings, the spatial relation (which is assumed to be CONTAINED-AT if none is provided) is changed to NEAR.
The procedures given so far are appropriate for handling queries where a place reference is explicitly mentioned. However, the fact that a query can be associated with a geographical context may not be directly observable in the query itself, but rather from the results returned. For instance, queries like recommended hotels for SIGIR 2006 or SeaFair 2006 lodging can be seen to refer to the city of Seattle. Although they do not contain an explicit place reference, we expect results to be about hotels in Seattle.
In the cases where a query does not contain place references, we start by assuming that the top results from a search engine represent the most popular and correct context and usage for the query. We Topic What Relation Where TGN concepts ML concepts Vegetable Exporters of Europe Vegetable Exporters CONTAINED-AT Europe 1 1 Trade Unions in Europe Trade Unions CONTAINED-AT Europe 1 1 Roman cities in the UK and Germany Roman cities CONTAINED-AT UK and Germany 6 2 Cathedrals in Europe Cathedrals CONTAINED-AT Europe 1 1 Car bombings near Madrid Car bombings NEAR Madrid 14 2 Volcanos around Quito Volcanos NEAR Quito 4 1 Cities within 100km of Frankfurt Cities NEAR Frankfurt 3 1 Russian troops in south(ern) Caucasus Russian troops in south(ern) CONTAINED-AT Caucasus 2 1 Cities near active volcanoes (This topic could not be appropriately handled - the relation and where terms are returned empty) Japanese rice imports (This topic could not be appropriately handled - the relation and where terms are returned empty) Table 1: Example topics from the GeoCLEF evaluation campaigns and the corresponding < what,relation,where > triples. then propose to use the distributional characteristics of geographical scopes previously assigned to the documents corresponding to these top results. In a previous work, we presented a text mining approach for assigning documents with corresponding geographical scopes, defined at an ontology, that worked as an offline preprocessing stage in a GIR system [13]. This pre-processing step is a fundamental stage of GIR, and it is reasonable to assume that this kind of information would be available on any system. Similarly to Wang et. al., we could also attempt to process the results on-line, in order to detect place references in the documents [17]. However, a GIR system already requires the offline stage.
For the top N documents given at the results, we check the geographic scopes that were assigned to them. If a significant portion of the results are assigned to the same scope, than the query can be seen to be related to the corresponding geographic concept. This assumption could even be relaxed, for instance by checking if the documents belong to scopes that are hierarchically related.
We used three different ontologies in evaluation experiments, namely the Getty thesaurus of geographic names (TGN) [6] and two specific resources developed at our group, here referred to as the PT and ML ontologies [2]. TGN and ML include global geographical information in multiple languages (although TGN is considerably larger), while the PT ontology focuses on the Portuguese territory with a high detail. Place types are also different across ontologies, as for instance PT includes street names and postal addresses, whereas ML only goes to the level of cities. The reader should refer to [2, 6] for a complete description of these resources.
Our initial experiments used Portuguese and English topics from the GeoCLEF 2005 and 2006 evaluation campaigns. Topics in GeoCLEF correspond to query strings that can be used as input to a GIR system [4]. ImageCLEF 2006 also included topics specifying place references, and participants were encouraged to run their GIR systems on them. Our experiments also considered this dataset. For each topic, we measured if Algorithm 2 was able to find the corresponding < what,relation,where > triple. The ontologies used in this experiment were the TGN and ML, as topics were given in multiple languages and covered the whole globe.
Dataset Number of Correct Triples Time per Query Queries ML TGN ML TGN GeoCLEF05 EN 25 19 20 GeoCLEF05 PT 25 20 18 288.1 334.5 GeoCLEF06 EN 32 28 19 msec msec GeoCLEF06 PT 25 23 11 ImgCLEF06 EN 24 16 18 Table 2: Summary of results over CLEF topics.
Table 1 illustrates some of the topics, and Table 2 summarizes the obtained results. The tables show that the proposed technique adequately handles most of these queries. A manual inspection of the ontology concepts that were returned for each case also revealed that the where term was being correctly disambiguated. Note that the TGN ontology indeed added some ambiguity, as for instance names like Madrid can correspond to many different places around the globe. It should also be noted that some of the considered topics are very hard for an automated system to handle. Some of them were ambiguous (e.g. in Japanese rice imports, the query can be said to refer either rice imports in Japan or imports of Japanese rice), and others contained no direct geographical references (e.g. cities near active volcanoes). Besides these very hard cases, we also missed some topics due to their usage of place adjectives and specific regions that are not defined at the ontologies (e.g. environmental concerns around the Scottish Trossachs).
In a second experiment, we used a sample of around 100,000 real search engine queries. The objective was to see if a significant number of these queries were geographical in nature, also checking if the algorithm did not produce many mistakes by classifying a query as geographical when that was not the case. The Portuguese ontology was used in this experiment, and queries were taken from the logs of a Portuguese Web search engine available at www.tumba.pt. Table 3 summarizes the obtained results. Many queries were indeed geographical (around 3.4%, although previous studies reported values above 14% [8]). A manual inspection showed that the algorithm did not produce many false positives, and the geographical queries were indeed correctly split into correct < what,relation,where > triple. The few mistakes we encountered were related to place names that were more frequently used in other contexts (e.g. in Teófilo Braga we have the problem that Braga is a Portuguese district, and Teófilo Braga was a well known Portuguese writer and politician). The addition of more names to the exception list can provide a workaround for most of these cases.
Value Num. Queries 110,916 Num. Queries without Geographical References 107,159 (96.6%) Num. Queries with Geographical References 3,757 (3.4%) Table 3: Results from an experiment with search engine logs.
We also tested the procedure for detecting queries that are implicitly geographical with a small sample of queries from the logs.
For instance, for the query Estádio do Dragão (e.g. home stadium for a soccer team from Porto), the correct geographical context can be discovered from the analysis of the results (more than 75% of the top 20 results are assigned with the scope Porto). For future work, we plan on using a larger collection of queries to evaluate this aspect. Besides queries from the search engine logs, we also plan on using the names of well-known buildings, monuments and other landmarks, as they have a strong geographical connotation.
Finally, we also made a comparative experiment with 2 popular geocoders, Maporama and Microsoft"s Mappoint. The objective was to compare Algorithm 1 with other approaches, in terms of being able to correctly disambiguate a string with a place reference.
Civil Parishes from Lisbon Maporama Mappoint Ours Coded refs. (out of 53) 9 (16.9%) 30 (56,6%) 15 (28.3%) Avg. Time per ref. (msec) 506.23 1235.87 143.43 Civil Parishes from Porto Maporama Mappoint Ours Coded refs. (out of 15) 0 (0%) 2 (13,3%) 5 (33.3%) Avg. Time per ref. (msec) 514.45 991.88 132.14 Table 4: Results from a comparison with geocoding services.
The Portuguese ontology was used in this experiment, taking as input the names of civil parishes from the Portuguese municipalities of Lisbon and Porto, and checking if the systems were able to disambiguate the full name (e.g. Campo Grande, Lisboa or Foz do Douro, Porto) into the correct geocode. We specifically measured whether our approach was better at unambiguously returning geocodes given the place reference (i.e. return the single correct code), and providing results rapidly. Table 4 shows the obtained results, and the accuracy of our method seems comparable to the commercial geocoders. Note that for Maporama and Mappoint, the times given at Table 4 include fetching results from the Web, but we have no direct way of accessing the geocoding algorithms (in both cases, fetching static content from the Web servers takes around
return the correct geocode in most cases (only 20 out of a total of
disambiguate (e.g. for Madalena, Lisboa we return both a street and a civil parish), as opposed to the other systems that often did not produce results. Moreover, if we consider the top geocode according to the ranking procedure described in Section 3.1, or if we use a type qualifier in the name (e.g. civil parish of Campo Grande,
Lisboa), our algorithm always returns the correct geocode.
This paper presented simple approaches for handling place references in search engine queries. This is a hard text mining problem, as queries are often ambiguous or underspecify information needs.
However, our initial experiments indicate that for many queries, the referenced places can be determined effectively. Unlike the techniques proposed by Wang et. al. [17], we mainly focused on recognizing spatial relations and associating place names to ontology concepts. The proposed techniques were employed in the prototype system that we used for participating in GeoCLEF 2006. In queries where a geographical reference is not explicitly mentioned, we propose to use the results for the query, exploiting geographic scopes previously assigned to these documents. In the future, we plan on doing a careful evaluation of this last approach. Another idea that we would like to test involves the integration of a spelling correction mechanism [12] into Algorithm 1, so that incorrectly spelled place references can be matched to ontology concepts.
The proposed techniques for handling geographic queries can have many applications in improving GIR systems or even general purpose search engines. After place references are appropriately disambiguated into ontology concepts, a GIR system can use them to retrieve relevant results, through the use of appropriate index structures (e.g. indexing the spatial coordinates associated with ontology concepts) and provided that the documents are also assigned to scopes corresponding to ontology concepts. A different GIR strategy can involve query expansion, by taking the where terms from the query and using the ontology to add names from neighboring locations. In a general purpose search engine, and if a local query is detected, we can forward users to a GIR system, which should be better suited for properly handling the query. The regular Google search interface already does this, by presenting a link to Google Local when it detects a geographical query.
[1] E. Amitay, N. Har"El, R. Sivan, and A. Soffer. Web-a-Where: Geotagging Web content. In Proceedings of SIGIR-04, the 27th Conference on research and development in information retrieval, 2004. [2] M. Chaves, M. J. Silva, and B. Martins. A Geographic Knowledge Base for Semantic Web Applications. In Proceedings of SBBD-05, the 20th Brazilian Symposium on Databases, 2005. [3] N. A. Chinchor. Overview of MUC-7/MET-2. In Proceedings of MUC-7, the 7th Message Understanding Conference, 1998. [4] F. Gey, R. Larson, M. Sanderson, H. Joho, and P. Clough.
GeoCLEF: the CLEF 2005 cross-language geographic information retrieval track. In Working Notes for the CLEF
[5] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.
Categorizing Web queries according to geographical locality.
In Proceedings of CIKM-03, the 12th Conference on Information and knowledge management, 2003. [6] P. Harpring. Proper words in proper places: The thesaurus of geographic names. MDA Information, 3, 1997. [7] C. Jones, R. Purves, A. Ruas, M. Sanderson, M. Sester,
M. van Kreveld, and R. Weibel. Spatial information retrieval and geographical ontologies: An overview of the SPIRIT project. In Proceedings of SIGIR-02, the 25th Conference on Research and Development in Information Retrieval, 2002. [8] J. Kohler. Analyzing search engine queries for the use of geographic terms, 2003. (MSc Thesis). [9] A. Kornai and B. Sundheim, editors. Proceedings of the NAACL-HLT Workshop on the Analysis of Geographic References, 2003. [10] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: Facing a great challenge. SIGKDD Explorations, 7, 2006. [11] D. Manov, A. Kiryakov, B. Popov, K. Bontcheva,
D. Maynard, and H. Cunningham. Experiments with geographic knowledge for information extraction. In Proceedings of the NAACL-HLT Workshop on the Analysis of Geographic References, 2003. [12] B. Martins and M. J. Silva. Spelling correction for search engine queries. In Proceedings of EsTAL-04, España for Natural Language Processing, 2004. [13] B. Martins and M. J. Silva. A graph-ranking algorithm for geo-referencing documents. In Proceedings of ICDM-05, the 5th IEEE International Conference on Data Mining, 2005. [14] L. Souza, C. J. Davis, K. Borges, T. Delboni, and A. Laender. The role of gazetteers in geographic knowledge discovery on the web. In Proceedings of LA-Web-05, the 3rd Latin American Web Congress, 2005. [15] E. Tjong, K. Sang, and F. D. Meulder. Introduction to the CoNLL-2003 shared task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL-2003, the 7th Conference on Natural Language Learning, 2003. [16] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen,

Information Retrieval (IR) systems are designed to help searchers solve problems. In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection. However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.
As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6]. Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification. However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].
Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact. IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5]. IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].
In this paper we present a study into the use and effectiveness of IRF in an online search environment. The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages? This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.
The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13]. In this paper we use data derived from that experiment to study factors affecting the utility of IRF.
In this section we describe the user study conducted to address our research questions.
Our study used two systems both of which suggested new query terms to the user. One system suggested terms based on the user"s interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material. Both systems used the same term suggestion algorithm, [15], and used a common interface.
In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time. We used the Web as the test collection in this study and Google1 as the underlying search engine.
Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence). Each summary sentence and top-ranking sentence is regarded as a representation of the document. The default display contains the list of top-ranking sentences and the list of the first ten document titles. Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.
This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16]. In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information. Both systems provide an interactive query expansion feature by suggesting new query terms to the user. The searcher has the responsibility for choosing which, if any, of these terms to add to the query. The searcher can also add or remove terms from the query at will.
This version of the system implements explicit RF. Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant. Only the representations marked relevant by the user are used for suggesting new query terms.
This system was used as a baseline against which the IRF system could be compared.
This system makes inferences about searcher interests based on the information with which they interact. As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document. To the searcher this is a way they can find out more information from a potentially interesting source. To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant. The query modification terms are selected using the same algorithm as in the Explicit RF system. Therefore the only difference between the systems is how relevance is communicated to the system.
The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness.
Search tasks were designed to encourage realistic search behaviour by our subjects. The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance. We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects. These subjects were not involved in the main experiment.
For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity. As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks. By developing tasks of different complexity we can assess how the nature of the task affects the subjects" interactive behaviour and hence the evidence supplied to IRF algorithms. Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task. In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.
Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 . They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task. Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.
HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source. Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.
MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends" guests is complaining about the price of petrol and the factors that cause it. Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.
LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol. However, as you have not been driving for long, you are unaware of any major changes in price. You decide to find out how the price of petrol has changed in the UK in recent years.
Figure 1. Varying task complexity (Petrol Prices topic).
subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers). Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.
The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science. The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience. The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed. All were familiar with Web searching, and some with searching in other domains.
The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity. Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface. Each subject carried out three tasks, one on each system. We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once. The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.
Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4]. System logging was also used to record subject interaction.
A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system. Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires. Subjects were offered a 5 minute break after the first hour. In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms. This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.
This contained questions about the subject"s education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet. No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once. Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search. The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.
Subjects were told that their interaction may be used by the IRF system to help them as they searched. They were not told which behaviours would be used or how it would be used.
We now describe the findings of our analysis.
In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF. We present our findings per research question. Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated. We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate. All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement. The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively. The highest, or most positive, values in each table are shown in bold. Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems.
Searchers attempted three search tasks of varying complexity, each on a different experimental system. In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities. We present our findings in terms of the RF provided by subjects and the terms recommended by the systems.
We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks. In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on:
to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful"/not useful.
to the system made you feel: comfortable/uncomfortable, in control/not in control.
The average obtained differential values are shown in Table 1 for IRF and each task category. The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement. This gives some overall understanding of the subjects" feelings which can be useful as the subjects may not answer individual differentials very precisely. The values for ERF are included for reference in this table and all other tables and figures in the Findings section. Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.
Table 1. Subject perceptions of RF method (lower = better).
Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc). Kruskal-Wallis Tests were applied to each differential for each type of RF3 . Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials. This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.
Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 . Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 . To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearman"s Rank Order Correlation Coefficient to participant responses. The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .
On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 . Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding. In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.
We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided. To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess. In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant. In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher. This proportion measures the searcher"s level of interaction with a document, we take it to measure the user"s interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.
There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context. Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess. Table 2 shows proportion of representations provided as RF by subjects.
Table 2. Feedback and documents viewed.
Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback
Documents Viewed
For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document. This suggests a pattern where users are investigating retrieved documents in more depth. It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunn"s post-hoc tests (multiple comparison using rank sums); all Z ≥
6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥
feedback varies based on the complexity of the search task. Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide. This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.
Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document). This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change. In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF.
The same RF algorithm was used to select query modification terms in all systems [16]. We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks. To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful. Table 3 presents average responses grouped by search task.
Table 3. Subject perceptions of system terms (lower = better).
Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF. The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task. For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 . This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity. Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .
As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested. Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query. In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearson"s Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.
Table 4. Term Acceptance (percentage of top six terms).
Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .
As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks . Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer. This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases. For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 . Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted. Differences may reside in the nature of the terms accepted; future work will investigate this issue.
In this section we have presented an investigation on the effect of search task complexity on the utility of IRF. From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks. Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted.
Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers. As such, levels of search experience may affect searchers" use and perceptions of IRF.
In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed. In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects. The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task.
We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…). These differentials elicited opinion from experimental subjects about the RF method used. In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5. Subject perceptions of RF method (lower = better).
The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant. For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .
Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 . As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction. Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 . It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.
In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems. Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 .
We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects. Table 6 shows the average differential responses obtained from both subject groups.
Table 6. Subject perceptions of system terms (lower = better).
Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp.
Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .
Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp.
Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects. This finding was supported by the proportion of query modification terms these subjects accepted. In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.
Table 7 shows the average number of accepted terms per subject group.
Table 7. Term Acceptance (percentage of top six terms).
Explicit RF Implicit RFProportion of terms Inexp. Exp. Inexp. Exp.
Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF. However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 .
In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF. The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful. We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries. Search experience appears to affect how subjects use the terms recommended as a result of the RF process.
From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search. To test this, our third research question concerned the use and usefulness of IRF during the course of a search. In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are. For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes. We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.
In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search
The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices. This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results. Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end. In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search. The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%). To simplify the statistical analysis and comparison we use the grouping of start, middle and end.
0
1
2
3
4
Slice Search"precision"(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2. Distribution of RF provision per search task.
Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm. These are essentially differences in the way users are assessing documents. In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .
When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations. At this stage the subjects are perhaps concentrating more on reading the retrieved results. Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search. This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.
Figure 2 also shows the proportion of feedback for tasks of different complexity. The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task. More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point. This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunn"s post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test)
The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items. That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification. Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search. In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search. The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.
Table 8. Term Acceptance (proportion of top six terms).
Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept. Search stage affects term acceptance in IRF but not in ERF26 . The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ). A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 .
The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user. Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted.
In this section we discuss the implications of the findings presented in the previous section for each research question.
The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks. From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest). When the search was more complex subjects rarely found results they regarded as completely relevant. Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥
27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.
In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.
The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves. It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias. Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher. For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents. Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit.
We analysed the affect of search experience on the utility of IRF. Our analysis revealed a general preference across all subjects for IRF over ERF. That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF. However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).
All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 . These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively.
We also analysed the effects of search stage on the use and usefulness of IRF. Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method. The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end. The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point. Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.
The findings suggest that searchers interact differently for IRF and ERF. Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF. The development of such a system represents part of our ongoing work in this area.
In this paper we have presented an investigation of Implicit Relevance Feedback (IRF). We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF. These factors were search task complexity, the subjects" search experience and the stage in the search. Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.
Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.
Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not. It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems.
[1] Bell, D.J. and Ruthven, I. (2004). Searchers' assessments of task complexity for web searching. Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000). Experimental components for the evaluation of interactive information retrieval systems. Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).
Strategic help for user interfaces for information retrieval.
Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980). Research methods in librarianship: Techniques and interpretation. Library and information science series. New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996). The ostensive model of developing information needs. Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992). Relevance feedback and other query modification techniques. In Information retrieval: Data structures and algorithms. New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003). Implicit feedback for inferring user preference. SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996). A case for interaction: A study of interactive information retrieval behavior and effectiveness. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984). Statistics using ranks: A unified approach.
Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994). Information filtering based on user behavior analysis and best match text retrieval.
Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990). Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988). Nonparametric statistics for the behavioural sciences. 2nd ed. Singapore: McGraw-Hill. [13] White, R.W. (2004). Implicit feedback for interactive information retrieval. Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005). An implicit feedback approach for interactive information retrieval,
Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004). A simulated study of implicit feedback models.
Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000). The impact of fluid documents on reading and browsing: An observational study. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.
Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.
Appendix A. Interface to Implicit RF system.

E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage. This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All,
I"d like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m. The current schedule looks like this: + 9:30 a.m. Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m. Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m. Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance. As a result, I will need each of your parts by Wednesday.
Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipient"s attention or action. the receiver"s attention or action. Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.
Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent. We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions. The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasn"t dropped the proverbial ball by forgetting to address an action request.
Action-item detection differs from standard text classification in two important ways. First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body. In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22]. Second, action-item detection attempts to recover the email sender"s intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below. Instead we find that we need more information-laden features such as higher-order n-grams. Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17]. In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14]. Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20]. We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.
We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.
Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level. From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem. We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task. Finally, we summarize this paper"s contributions and consider interesting directions for future work.
Several other researchers have considered very similar text classification tasks. Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts. We consider action-items to be an important specific type of speech act that falls within their more general classification. While they provide results for several classification methods, their methods only make use of human judgments at the document-level. In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.
Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List. This classification task is very similar to ours except they do not consider simple factual questions to belong to this category. We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?. From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.
Additionally, they do not study alternative choices or approaches to the classification task. Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list. In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.
Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature. For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail. In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action.
In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments. Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification. Finally, we focus on the representation necessary to achieve the most competitive performance.
In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items. Therefore, there are three basic problems:
not it contains an action-item.
documents containing action-items occur as high as possible in the ranking.
to whether or not it is an action-item.
As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application. In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users. In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail. This can be the case for crisis managers during disaster management. Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the user"s required time to gist the message.
As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest. We term the human judgments a phrase-labeling since the user"s view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries. Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.
To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data. The document-level view treats each e-mail as a learning instance with an associated class-label. Then, the document can be converted to a feature-value vector and learning progresses as usual. Applying a document-level classifier to document detection and ranking is straightforward. In order to apply it to sentence detection, one must make additional steps. For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated. The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.
In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label. Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance. Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction. This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership.
Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you. Each of these phrases consists of common words that occur in many e-mails. However, when they occur in the same sentence, they are far more indicative of an action-item.
Additionally, order can be important: consider have you versus you have. Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification. Therefore, we consider all n-grams up to size 4.
When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it. We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall. Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.
Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.
Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence. Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation. In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document. This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence.
In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level. We do not address how to use a document-level classifier to make predictions at the sentence-level.
In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4]. Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item. When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth. Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods. If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive. The metric would need to punish overly long true predictions as well as too short predictions. Our criteria for converting to labeled instances implicitly includes both criteria. Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item. Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item. Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.
Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score. We use the simple policy of predicting positive when any of the sentences is predicted positive. In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifier"s 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document. In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold. When no sentence is predicted positive, the document score is the maximum sentence score normalized by length. As in other text problems, we are more likely to emit false positives for documents with more words or sentences.
Thus we include a length normalization factor.
Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements. The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.
After identity anonymization, the corpora has three basic versions. Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail. Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant. To isolate the effects of quoted material, we have three versions of the corpora. The raw form contains the basic messages.
The auto-stripped version contains the messages after quoted material has been automatically removed. The hand-stripped version contains the messages after quoted material has been removed by a human. Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message. The studies reported here are performed with the hand-stripped version. This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it. Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.
Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document.
Two human annotators labeled each message as to whether or not it contained an action-item. In addition, they identified each segment of the e-mail which contained an action-item. A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence. They were instructed that an action item is an explicit request for information that requires the recipient"s attention or a required action and told to highlight the phrases or sentences that make up the request.
Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.
Annotator Two labeled 327 messages as containing action items.
The agreement of the human annotators is shown in Tables 1 and
both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same. At the document-level, the annotators agreed 93% of the time. The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R
A is the empirical estimate of the probability of agreement. R is the empirical estimate of the probability of random agreement given the empirical class priors. A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.
At the document-level, the kappa statistic for inter-annotator agreement is 0.85. This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].
In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.
This allows us to compare agreement over no judgments. We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc. Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well. This only reduces the number of no agreements. This leaves 6301 automatically segmented sentences.
At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.
In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion. The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements. For example, If you would like to keep your job, come to tomorrow"s meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrow"s meeting does not.
The first would be an action-item in most contexts while the second would not. Of course, many conditional statements are not so clearly interpretable. After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems. Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments. Two messages have four action-item segments, and one message has six action-item segments. Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators" individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.
In terms of message characteristics, there were on average 132 content tokens in the body after stripping. For action-item messages, there were 115. However, by examining Figure 2 we see the length distributions are nearly identical. As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has
For this experiment, we have selected a variety of standard text classification algorithms. In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager. We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand. This is important since it is easy to improve a strawman classifier by introducing a new representation. By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly.
We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19]. We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it. In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set. The value of k is set to be 2( log2 N + 1) where N is the number of training points. This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8]. In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure.
We use a standard multinomial na¨ıve Bayes classifier [16]. In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160
NumberofMessages Number of Tokens All Messages Action-Item Messages 0
PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length. A bin size of 20 words was used. Only tokens in the body after hand-stripping were counted. After stripping, the majority of words left are usually actual message content.
Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics. The best performance for each classifier is shown in bold.
We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].
All default settings were used.
Like the SVM, the Voted Perceptron is a kernel-based learning method. We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm. The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct. With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron. The output of the classifier uses the weights on the perceptra to make a final voted classification. When used in an offline-manner, multiple passes can be made through the training data. Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.
Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].
Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin. Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation.
To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy. The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives .
We perform standard 10-fold cross-validation on the set of documents. For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold. For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.
Feature selection was performed using the chi-squared statistic. Different levels of feature selection were considered for each classifier. Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. There are approximately 4700 unigram tokens without feature selection. In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier. For this study, only the body of each e-mail message was used. Feature selection is always applied to all candidate features. That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method.
The results for document-level classification are given in Table
are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram). Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes. This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).
Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting. In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes. More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.
As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small. This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams. Further improvement would signify that the order of the words matter even when only considering a small sentence-size window. Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.
Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers. When the F1 result is statistically significant, it is shown in bold. When the accuracy result is significant, it is shown with a † .
F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.
When the result is statistically significant, it is shown in bold.
Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve. In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall. This would mean that a tenth of the user"s action-items would be placed at the top of their action-item sorted inbox. Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score. Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.
Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here. This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance. The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.
Sentence detection results are presented in Table 6. With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem. That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare. Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no. Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.
Figure 4: Users find action-items quicker when assisted by a classification system.
Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user. In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used. There were three distinct sets of e-mail in which users had to find action-items. These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the
1
Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help). In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifier"s reordering.
Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets. This is typically handled by varying the ordering of the sets across users so that the means are comparable. While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.
Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes. Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting. As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection. For example, highlighting the wrong sentence near an actual action-item hurts the user"s trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss. Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy.
In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know. Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase. Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document. As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.
Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items. A few examples are terms such as org, bob, and gov. We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions. This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on. We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining.
While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers. Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis. Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task. We are currently pursuing some of these avenues to see what additional gains these offer.
Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers. Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier. Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline.
The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value. Further experiments are needed to see how this interacts with the amount of training data available. Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items. This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.
In this work, we examined how action-items can be effectively detected in e-mails. Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments. When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.
Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).
We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments. We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhower"s support of the text preprocessing package. Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic.

Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking. For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time. This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time. Furthermore, the costs can be changed without retraining the model.
Additionally, probability estimates are often used as the basis of deciding which document"s label to request next during active learning [17, 23]. Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17]. Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3]. However, in all of these tasks, the quality of the probability estimates is crucial.
Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data. Since many text classification tasks often have very little training data, we focus on parametric methods. However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable. While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes. We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models. Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.
We first review related work on improving probability estimates and score modeling in information retrieval. Then, we discuss in further detail the need for asymmetric models. After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores. We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods. Finally, we summarize our contributions and discuss future directions.
Parametric models have been employed to obtain probability estimates in several areas of information retrieval. Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning. Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines. They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here. They also survey the long history of modeling the relevance scores of search engines.
Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.
Focus on improving probability estimates has been growing lately.
Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes. In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM. While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results. Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue. In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.
There is a variety of other work that this paper extends. Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.
His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better). Finally, Bennett [1] obtained moderate gains by applying Platt"s method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.
Recalibrating poorly calibrated classifiers is not a new problem.
Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24].
Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines.
The general problem we are concerned with is highlighted in Figure 1. A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).
We assume throughout there are only two classes: the positive and the negative (or irrelevant) class ("+" and "-" respectively).
There are two general types of parametric approaches. The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes" RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey. The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)). The second type of approach breaks the problem down as shown in the grey box of Figure 1. An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes" rule and the class priors are used to obtain the estimate for P(+|s(d)).
Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters. We can visualize this as depicted in Figure 2. Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).
A B C 0
1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen. Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).
Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished. This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3). As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.
Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.
Furthermore, no examples fall between θ− and θ+. The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity. Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved. This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.
Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes. Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples). Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.
A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian. An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.
We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0
-300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians. A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.
We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters. To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr). While these distributions are composed of halves, the resulting function is a single continuous distribution.
These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters. We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive). In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way. Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).
To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval. Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task.
Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions. In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.
Because of the simplicity of analysis in the latter alternative, we choose this method.
For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ). Now, we fix θ and compute the maximum likelihood for that choice of θ. Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.
The complete derivation is omitted because of space but is available in [2]. We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.
Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ. Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ. The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).
By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform. Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half). Dr = 0 is handled similarly.
Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable. Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time. In addition, if the scores are sorted, then we can perform the whole process quite efficiently. Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately. Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left. Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time).
For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ). The MLEs can be worked out similar to the above.
We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.
The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp. Dr2 = 0), we assign σl = 0 (resp. σr = 0). Again, the same computational complexity analysis applies to estimating these parameters.
For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e. P(c) = |c|+1 N+2 where N is the number of documents. For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes" rule as described above. All of the methods below are fit using maximum likelihood estimates.
For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifier"s estimate as s(d). The log-odds are defined to be log P (+|d) P (−|d) . The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e. P(+|d) = P(−|d) = 0.5).
Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19]. Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors. In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.
Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates. This method is denoted in the tables below as Gauss.
Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested. This method is denoted as A. Gauss.
Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form. The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14]. We denote this method as Laplace below.
Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs. This method is denoted as A. Laplace below.
Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)). Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels. As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier. When this is desired, these methods may be more appropriate. The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it. Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.
Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning. The model they use is: P(+|s(d)) = exp(a + b s(d))
. (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d). Instead of using this below, we will use the logodds ratio. This does not affect the model as it simply shifts all of the scores by a constant determined by the priors. We refer to this method as LogReg below.
Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.
This model differs from the LogReg model only in how the parameters are estimated. The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.
The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes" rule to infer the probability of incorrect label).
Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness. We refer to this method below as LR+Noise.
We examined several corpora, including the MSN Web Directory,
Reuters, and TREC-AP.
MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified. We used the same train/test split of 50078/10024 documents as that reported in [9]. The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.
The class proportions in the training set vary from 1.15% to 22.29%.
In the testing set, they range from 1.14% to 21.54%. The classes are general subjects such as Health & Fitness and Travel & Vacation.
Human indexers assigned the documents to zero or more categories.
For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.
Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987. For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents). The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects. There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22]. The class proportions in the training set vary from 1.88% to 29.96%. In the testing set, they range from 1.7% to 32.95%.
For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.
TREC-AP The TREC-AP corpus is a collection of AP news stories from
documents that was used in [18]. As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.
The title and body fields are used in the experiments below. There are twenty categories in total. The class proportions in the training set vary from 0.06% to 2.03%. In the testing set, they range from
For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents.
We selected two classifiers for evaluation. A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.
After accounting for the variance, that evaluation also supported the claims made here.
SVM For linear SVMs, we use the Smox toolkit which is based on Platt"s Sequential Minimal Optimization algorithm. The features were represented as continuous values. We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22]. The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.
Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].
We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.
We use the log-odds estimated by the classifier as s(d). The normal decision threshold is at zero.
We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates. For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise. The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 . When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero. When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one. Thus, both measures assess how close an estimate comes to correctly predicting the item"s class but vary in how harshly incorrect predictions are penalized.
We report only the sum of these measures and omit the averages for space. Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.
In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities. This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5. Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates. It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.
We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures. Only pairs that the methods disagree on are used in the sign test. This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed. We use a significance level of p = 0.01.
As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.
In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data. We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result. Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier.
The results for recalibrating na¨ıve Bayes are given in Table 1a.
Table 1b gives results for producing probabilistic outputs for SVMs.
Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡
726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right). The best entry for a corpus is in bold. Entries that are statistically significantly better than all other entries are underlined. A † denotes the method is significantly better than all other methods except for na¨ıve Bayes. A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left). The reason for this distinction in significance tests is described in the text.
We start with general observations that result from examining the performance of these methods over the various corpora. The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods. There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.
With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.
However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.
In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters. Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly. Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ). Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.
We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on. Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).
In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins. For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.
No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.
The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17]. This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here). The totals of the squared error and log-loss bear out the previous observation that when it"s wrong it"s really wrong.
There are several interesting points about the performance of the asymmetric distributions as well. First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount. This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails). While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate. Figure
a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class. Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0
-600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0
-15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.
Also shown is the fit of the asymmetric Laplace distribution to the training score distribution. The positive class (i.e. Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily. There are enough such cases overall that it seems clearly inferior to the top three methods.
However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential). As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.
Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes. Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.
Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.
Finally, we can make a few observations about the usefulness of the various performance metrics. First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate. Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice. Secondly, squared error has a weakness in the other direction. That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates. For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesn"t quite reach if you use smoothing). This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome. For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced. These observations are straightforward from the definitions but are underscored by the evaluation.
A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes). From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.
Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.
This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.
Finally, extending these methods to the outputs of other discriminative classifiers is an open area. We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11]. By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − .
We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier. In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function. We have given an efficient way to estimate the parameters of these distributions.
While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.
In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression. Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.
Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox. Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work. Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper.

